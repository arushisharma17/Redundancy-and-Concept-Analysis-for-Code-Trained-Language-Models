utf-8 def get_vid_from_url ( url ) : 
return match1 ( url , r'youtu\\.be/([^?/]+)' ) or match1 ( url , r'youtube\\.com/embed/([^/?]+)' ) or match1 ( url , r'youtube\\.com/v/([^/?]+)' ) or match1 ( url , r'youtube\\.com/watch/([^/?]+)' ) or parse_query_param ( url , 'v' ) or parse_query_param ( parse_query_param ( url , 'u' ) , 'v' ) 
~~
~~ def sina_xml_to_url_list ( xml_data ) : 
rawurl = [ ] 
dom = parseString ( xml_data ) 
for node in dom . getElementsByTagName ( 'durl' ) : 
~~~ url = node . getElementsByTagName ( 'url' ) [ 0 ] 
rawurl . append ( url . childNodes [ 0 ] . data ) 
~~ return rawurl 
~~ def makeMimi ( upid ) : 
strSeed = "gGddgPfeaf_gzyr" 
prehash = upid + "_" + strSeed 
return md5 ( prehash . encode ( 'utf-8' ) ) . hexdigest ( ) 
~~ def fc2video_download ( url , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : 
~~~ """wrapper""" 
#'http://video.fc2.com/en/content/20151021bTVKnbEw' 
#'http://xiaojiadianvideo.asia/content/20151021bTVKnbEw' 
#'http://video.fc2.com/ja/content/20151021bTVKnbEw' 
#'http://video.fc2.com/tw/content/20151021bTVKnbEw' 
hostname = urlparse ( url ) . hostname 
if not ( 'fc2.com' in hostname or 'xiaojiadianvideo.asia' in hostname ) : 
~~~ return False 
~~ upid = match1 ( url , r'.+/content/(\\w+)' ) 
fc2video_download_by_upid ( upid , output_dir , merge , info_only ) 
~~ def dailymotion_download ( url , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : 
html = get_content ( rebuilt_url ( url ) ) 
info = json . loads ( match1 ( html , r\ ) ) 
title = match1 ( html , r\ ) or match1 ( html , r\ ) 
title = unicodize ( title ) 
for quality in [ '1080' , '720' , '480' , '380' , '240' , '144' , 'auto' ] : 
~~~ try : 
~~~ real_url = info [ quality ] [ 1 ] [ "url" ] 
if real_url : 
~~~ break 
~~ ~~ except KeyError : 
~~~ pass 
~~ ~~ mime , ext , size = url_info ( real_url ) 
print_info ( site_info , title , mime , size ) 
if not info_only : 
~~~ download_urls ( [ real_url ] , title , ext , size , output_dir = output_dir , merge = merge ) 
~~ ~~ def dictify ( r , root = True ) : 
~~~ """http://stackoverflow.com/a/30923963/2946714""" 
if root : 
~~~ return { r . tag : dictify ( r , False ) } 
~~ d = copy ( r . attrib ) 
if r . text : 
~~~ d [ "_text" ] = r . text 
~~ for x in r . findall ( "./*" ) : 
~~~ if x . tag not in d : 
~~~ d [ x . tag ] = [ ] 
~~ d [ x . tag ] . append ( dictify ( x , False ) ) 
~~ return d 
~~ def ucas_download_single ( url , output_dir = '.' , merge = False , info_only = False , ** kwargs ) : 
html = get_content ( url ) 
resourceID = re . findall ( r\ , html ) [ 0 ] 
title = match1 ( html , r\ ) 
url_lists = _ucas_get_url_lists_by_resourceID ( resourceID ) 
for k , part in enumerate ( url_lists ) : 
~~~ part_title = title + '_' + str ( k ) 
print_info ( site_info , part_title , 'flv' , 0 ) 
~~~ download_urls ( part , part_title , 'flv' , total_size = None , output_dir = output_dir , merge = merge ) 
~~ ~~ ~~ def ucas_download_playlist ( url , output_dir = '.' , merge = False , info_only = False , ** kwargs ) : 
parts = re . findall ( r\ , html ) 
for part_path in parts : 
~~~ ucas_download ( 'http://v.ucas.ac.cn/course/' + part_path , output_dir = output_dir , merge = merge , info_only = info_only ) 
~~ ~~ def sina_download_by_vid ( vid , title = None , output_dir = '.' , merge = True , info_only = False ) : 
xml = api_req ( vid ) 
urls , name , size = video_info ( xml ) 
if urls is None : 
~~~ log . wtf ( name ) 
~~ title = name 
print_info ( site_info , title , 'flv' , size ) 
~~~ download_urls ( urls , title , 'flv' , size , output_dir = output_dir , merge = merge ) 
~~ ~~ def sina_download_by_vkey ( vkey , title = None , output_dir = '.' , merge = True , info_only = False ) : 
url = 'http://video.sina.com/v/flvideo/%s_0.flv' % vkey 
type , ext , size = url_info ( url ) 
~~~ download_urls ( [ url ] , title , 'flv' , size , output_dir = output_dir , merge = merge ) 
~~ ~~ def sina_download ( url , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : 
if 'news.sina.com.cn/zxt' in url : 
~~~ sina_zxt ( url , output_dir = output_dir , merge = merge , info_only = info_only , ** kwargs ) 
return 
~~ vid = match1 ( url , r'vid=(\\d+)' ) 
if vid is None : 
~~~ video_page = get_content ( url ) 
vid = hd_vid = match1 ( video_page , r'hd_vid\\s*:\\s*\\'([^\\']+)\\'' ) 
if hd_vid == '0' : 
~~~ vids = match1 ( video_page , r'[^\\w]vid\\s*:\\s*\\'([^\\']+)\\'' ) . split ( '|' ) 
vid = vids [ - 1 ] 
~~ ~~ if vid is None : 
~~~ vid = match1 ( video_page , r\ ) 
~~ if vid : 
~~~ sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) 
~~ else : 
~~~ vkey = match1 ( video_page , r\ ) 
if vkey is None : 
~~~ vid = match1 ( url , r'#(\\d+)' ) 
sina_download_by_vid ( vid , output_dir = output_dir , merge = merge , info_only = info_only ) 
~~ title = match1 ( video_page , r\ ) 
sina_download_by_vkey ( vkey , title = title , output_dir = output_dir , merge = merge , info_only = info_only ) 
~~ ~~ def yixia_download ( url , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : 
if 'n.miaopai.com' == hostname : 
~~~ smid = match1 ( url , r'n\\.miaopai\\.com/media/([^.]+)' ) 
miaopai_download_by_smid ( smid , output_dir , merge , info_only ) 
~~ elif 'miaopai.com' in hostname : #Miaopai 
~~~ yixia_download_by_scid = yixia_miaopai_download_by_scid 
scid = match1 ( url , r'miaopai\\.com/show/channel/([^.]+)\\.htm' ) or match1 ( url , r'miaopai\\.com/show/([^.]+)\\.htm' ) or match1 ( url , r'm\\.miaopai\\.com/show/channel/([^.]+)\\.htm' ) or match1 ( url , r'm\\.miaopai\\.com/show/channel/([^.]+)' ) 
~~ elif 'xiaokaxiu.com' in hostname : #Xiaokaxiu 
~~~ yixia_download_by_scid = yixia_xiaokaxiu_download_by_scid 
if re . match ( r'http://v.xiaokaxiu.com/v/.+\\.html' , url ) : #PC 
~~~ scid = match1 ( url , r'http://v.xiaokaxiu.com/v/(.+)\\.html' ) 
~~ elif re . match ( r'http://m.xiaokaxiu.com/m/.+\\.html' , url ) : #Mobile 
~~~ scid = match1 ( url , r'http://m.xiaokaxiu.com/m/(.+)\\.html' ) 
~~ ~~ else : 
~~ yixia_download_by_scid ( scid , output_dir , merge , info_only ) 
~~ def veoh_download ( url , output_dir = '.' , merge = False , info_only = False , ** kwargs ) : 
if re . match ( r'http://www.veoh.com/watch/\\w+' , url ) : 
~~~ item_id = match1 ( url , r'http://www.veoh.com/watch/(\\w+)' ) 
~~ elif re . match ( r'http://www.veoh.com/m/watch.php\\?v=\\.*' , url ) : 
~~~ item_id = match1 ( url , r'http://www.veoh.com/m/watch.php\\?v=(\\w+)' ) 
~~ veoh_download_by_id ( item_id , output_dir = '.' , merge = False , info_only = info_only , ** kwargs ) 
~~ def veoh_download_by_id ( item_id , output_dir = '.' , merge = False , info_only = False , ** kwargs ) : 
webpage_url = 'http://www.veoh.com/m/watch.php?v={item_id}&quality=1' . format ( item_id = item_id ) 
a = get_content ( webpage_url , decoded = True ) 
url = match1 ( a , r\ ) 
title = match1 ( a , r\ ) 
type_ , ext , size = url_info ( url ) 
print_info ( site_info , title , type_ , size ) 
~~~ download_urls ( [ url ] , title , ext , total_size = None , output_dir = output_dir , merge = merge ) 
~~ ~~ def download_by_id ( self , vid = '' , title = None , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : 
assert vid 
self . prepare ( vid = vid , title = title , ** kwargs ) 
self . extract ( ** kwargs ) 
self . download ( output_dir = output_dir , 
merge = merge , 
info_only = info_only , ** kwargs ) 
~~ def get_vid_from_url ( self , url ) : 
hit = re . search ( r'live.qq.com/(\\d+)' , url ) 
if hit is not None : 
~~~ return hit . group ( 1 ) 
~~ hit = re . search ( r'live.qq.com/directory/match/(\\d+)' , url ) 
~~~ return self . get_room_id_from_url ( hit . group ( 1 ) ) 
~~ html = get_content ( url ) 
room_id = match1 ( html , r\ ) 
if room_id is None : 
~~ return room_id 
~~ def sprint ( text , * colors ) : 
return "\\33[{}m{content}\\33[{}m" . format ( ";" . join ( [ str ( color ) for color in colors ] ) , RESET , content = text ) if IS_ANSI_TERMINAL and colors else text 
~~ def print_log ( text , * colors ) : 
~~ def e ( message , exit_code = None ) : 
print_log ( message , YELLOW , BOLD ) 
if exit_code is not None : 
~~~ sys . exit ( exit_code ) 
~~ ~~ def wtf ( message , exit_code = 1 ) : 
print_log ( message , RED , BOLD ) 
~~ ~~ def detect_os ( ) : 
syst = system ( ) . lower ( ) 
os = 'unknown' 
if 'cygwin' in syst : 
~~~ os = 'cygwin' 
~~ elif 'darwin' in syst : 
~~~ os = 'mac' 
~~ elif 'linux' in syst : 
~~~ os = 'linux' 
try : 
~~~ with open ( '/proc/version' , 'r' ) as f : 
~~~ if 'microsoft' in f . read ( ) . lower ( ) : 
~~~ os = 'wsl' 
~~ ~~ ~~ except : pass 
~~ elif 'windows' in syst : 
~~~ os = 'windows' 
~~ elif 'bsd' in syst : 
~~~ os = 'bsd' 
~~ return os 
~~ def miaopai_download_by_fid ( fid , output_dir = '.' , merge = False , info_only = False , ** kwargs ) : 
page_url = 'http://video.weibo.com/show?fid=' + fid + '&type=mp4' 
mobile_page = get_content ( page_url , headers = fake_headers_mobile ) 
url = match1 ( mobile_page , r\ ) 
if url is None : 
~~~ wb_mp = re . search ( r\ , mobile_page ) . group ( 2 ) 
return miaopai_download_by_wbmp ( wb_mp , fid , output_dir = output_dir , merge = merge , 
info_only = info_only , total_size = None , ** kwargs ) 
~~ title = match1 ( mobile_page , r'<title>((.|\\n)+?)</title>' ) 
if not title : 
~~~ title = fid 
~~ title = title . replace ( '\\n' , '_' ) 
ext , size = 'mp4' , url_info ( url ) [ 2 ] 
print_info ( site_info , title , ext , size ) 
~~ ~~ def vimeo_download_by_channel ( url , output_dir = '.' , merge = False , info_only = False , ** kwargs ) : 
~~~ """str->None""" 
channel_id = match1 ( url , r'http://vimeo.com/channels/(\\w+)' ) 
vimeo_download_by_channel_id ( channel_id , output_dir , merge , info_only , ** kwargs ) 
~~ def vimeo_download_by_channel_id ( channel_id , output_dir = '.' , merge = False , info_only = False , ** kwargs ) : 
~~~ """str/int->None""" 
html = get_content ( 'https://api.vimeo.com/channels/{channel_id}/videos?access_token={access_token}' . format ( channel_id = channel_id , access_token = access_token ) ) 
data = loads ( html ) 
id_list = [ ] 
#print(data) 
for i in data [ 'data' ] : 
~~~ id_list . append ( match1 ( i [ 'uri' ] , r'/videos/(\\w+)' ) ) 
~~ for id in id_list : 
~~~ vimeo_download_by_id ( id , None , output_dir , merge , info_only , ** kwargs ) 
~~ except urllib . error . URLError as e : 
~~ ~~ ~~ def vimeo_download_by_id ( id , title = None , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : 
site = VimeoExtractor ( ) 
site . download_by_vid ( id , info_only = info_only , output_dir = output_dir , merge = merge , ** kwargs ) 
~~ def ckplayer_get_info_by_xml ( ckinfo ) : 
e = ET . XML ( ckinfo ) 
video_dict = { 'title' : '' , 
'links' : [ ] , 
'size' : 0 , 
'flashvars' : '' , } 
dictified = dictify ( e ) [ 'ckplayer' ] 
if 'info' in dictified : 
~~~ if '_text' in dictified [ 'info' ] [ 0 ] [ 'title' ] [ 0 ] : #title 
~~~ video_dict [ 'title' ] = dictified [ 'info' ] [ 0 ] [ 'title' ] [ 0 ] [ '_text' ] . strip ( ) 
~~~ video_dict [ 'size' ] = sum ( [ int ( i [ 'size' ] [ 0 ] [ '_text' ] ) for i in dictified [ 'video' ] ] ) 
~~~ video_dict [ 'links' ] = [ i [ 'file' ] [ 0 ] [ '_text' ] . strip ( ) for i in dictified [ 'video' ] ] 
~~ if '_text' in dictified [ 'flashvars' ] [ 0 ] : 
~~~ video_dict [ 'flashvars' ] = dictified [ 'flashvars' ] [ 0 ] [ '_text' ] . strip ( ) 
~~ return video_dict 
~~ def get_video_url_from_video_id ( video_id ) : 
data = [ "" ] * 256 
for index , _ in enumerate ( data ) : 
~~~ t = index 
for i in range ( 8 ) : 
~~~ t = - 306674912 ^ unsigned_right_shitf ( t , 1 ) if 1 & t else unsigned_right_shitf ( t , 1 ) 
~~ data [ index ] = t 
~~ def tmp ( ) : 
~~~ rand_num = random . random ( ) 
path = "/video/urls/v/1/toutiao/mp4/{video_id}?r={random_num}" . format ( video_id = video_id , 
random_num = str ( rand_num ) [ 2 : ] ) 
e = o = r = - 1 
i , a = 0 , len ( path ) 
while i < a : 
~~~ e = ord ( path [ i ] ) 
i += 1 
if e < 128 : 
~~~ r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ e ) ] 
~~~ if e < 2048 : 
~~~ r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 192 | e >> 6 & 31 ) ) ] 
r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & e ) ) ] 
~~~ if 55296 <= e < 57344 : 
~~~ e = ( 1023 & e ) + 64 
o = 1023 & t . url ( i ) 
r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 240 | e >> 8 & 7 ) ) ] 
r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 2 & 63 ) ) ] 
r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | o >> 6 & 15 | ( 3 & e ) << 4 ) ) ] 
r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | 63 & o ) ) ] 
~~~ r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 224 | e >> 12 & 15 ) ) ] 
r = unsigned_right_shitf ( r , 8 ) ^ data [ 255 & ( r ^ ( 128 | e >> 6 & 63 ) ) ] 
~~ ~~ ~~ ~~ return "https://ib.365yg.com{path}&s={param}" . format ( path = path , param = unsigned_right_shitf ( r ^ - 1 , 0 ) ) 
~~ while 1 : 
~~~ url = tmp ( ) 
~~~ return url 
~~ ~~ ~~ def get_vid_from_url ( url ) : 
vid = match1 ( url , 'https?://www.mgtv.com/(?:b|l)/\\d+/(\\d+).html' ) 
if not vid : 
~~~ vid = match1 ( url , 'https?://www.mgtv.com/hz/bdpz/\\d+/(\\d+).html' ) 
~~ return vid 
~~ def get_mgtv_real_url ( url ) : 
content = loads ( get_content ( url ) ) 
m3u_url = content [ 'info' ] 
split = urlsplit ( m3u_url ) 
base_url = "{scheme}://{netloc}{path}/" . format ( scheme = split [ 0 ] , 
netloc = split [ 1 ] , 
path = dirname ( split [ 2 ] ) ) 
segment_list = [ ] 
segments_size = 0 
for i in content . split ( ) : 
~~~ segment_list . append ( base_url + i ) 
~~ elif i . startswith ( '#EXT-MGTV-File-SIZE:' ) : 
~~~ segments_size += int ( i [ i . rfind ( ':' ) + 1 : ] ) 
~~ ~~ return m3u_url , segments_size , segment_list 
~~ def get_head ( repo_path ) : 
~~~ ref = open ( os . path . join ( repo_path , '.git' , 'HEAD' ) , 'r' ) . read ( ) . strip ( ) [ 5 : ] . split ( '/' ) 
branch = ref [ - 1 ] 
commit = open ( os . path . join ( repo_path , '.git' , * ref ) , 'r' ) . read ( ) . strip ( ) [ : 7 ] 
return branch , commit 
~~ except : 
~~~ return None 
~~ ~~ def legitimize ( text , os = detect_os ( ) ) : 
text = text . translate ( { 
0 : None , 
ord ( '/' ) : '-' , 
ord ( '|' ) : '-' , 
} ) 
if os == 'windows' or os == 'cygwin' or os == 'wsl' : 
~~~ text = text . translate ( { 
ord ( ':' ) : '-' , 
ord ( '*' ) : '-' , 
ord ( '?' ) : '-' , 
ord ( '\\\\' ) : '-' , 
ord ( \ ) : '\\'' , 
ord ( '+' ) : '-' , 
ord ( '<' ) : '-' , 
ord ( '>' ) : '-' , 
ord ( '[' ) : '(' , 
ord ( ']' ) : ')' , 
~~~ if os == 'mac' : 
~~ if text . startswith ( "." ) : 
~~~ text = text [ 1 : ] 
return text 
~~ def get_terminal_size ( ) : 
return struct . unpack ( 'hh' , fcntl . ioctl ( 1 , termios . TIOCGWINSZ , '1234' ) ) 
~~~ return ( 40 , 80 ) 
~~ ~~ def cbs_download ( url , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : 
pid = match1 ( html , r'video\\.settings\\.pid\\s*=\\s*\\'([^\\']+)\\'' ) 
theplatform_download_by_pid ( pid , title , output_dir = output_dir , merge = merge , info_only = info_only ) 
~~ def download ( self , ** kwargs ) : 
if 'json_output' in kwargs and kwargs [ 'json_output' ] : 
~~~ json_output . output ( self ) 
~~ elif 'info_only' in kwargs and kwargs [ 'info_only' ] : 
~~~ if 'stream_id' in kwargs and kwargs [ 'stream_id' ] : 
~~~ stream_id = kwargs [ 'stream_id' ] 
if 'index' not in kwargs : 
~~~ self . p ( stream_id ) 
~~~ self . p_i ( stream_id ) 
~~~ if 'index' not in kwargs : 
~~~ self . p ( [ ] ) 
~~~ stream_id = self . streams_sorted [ 0 ] [ 'id' ] if 'id' in self . streams_sorted [ 0 ] else self . streams_sorted [ 0 ] [ 'itag' ] 
self . p_i ( stream_id ) 
~~ ~~ ~~ else : 
~~ if 'index' not in kwargs : 
~~ if stream_id in self . streams : 
~~~ urls = self . streams [ stream_id ] [ 'src' ] 
ext = self . streams [ stream_id ] [ 'container' ] 
total_size = self . streams [ stream_id ] [ 'size' ] 
~~~ urls = self . dash_streams [ stream_id ] [ 'src' ] 
ext = self . dash_streams [ stream_id ] [ 'container' ] 
total_size = self . dash_streams [ stream_id ] [ 'size' ] 
~~ if not urls : 
~~ download_url_ffmpeg ( urls [ 0 ] , self . title , 'mp4' , output_dir = kwargs [ 'output_dir' ] , merge = kwargs [ 'merge' ] , stream = False ) 
if not kwargs [ 'caption' ] : 
~~ for lang in self . caption_tracks : 
~~~ filename = '%s.%s.srt' % ( get_filename ( self . title ) , lang ) 
srt = self . caption_tracks [ lang ] 
with open ( os . path . join ( kwargs [ 'output_dir' ] , filename ) , 
'w' , encoding = 'utf-8' ) as x : 
~~~ x . write ( srt ) 
~~ print ( 'Done.' ) 
~~ ~~ ~~ def acfun_download_by_vid ( vid , title , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : 
info = json . loads ( get_content ( 'http://www.acfun.cn/video/getVideo.aspx?id=' + vid ) ) 
sourceType = info [ 'sourceType' ] 
if 'sourceId' in info : sourceId = info [ 'sourceId' ] 
if sourceType == 'sina' : 
~~~ sina_download_by_vid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) 
~~ elif sourceType == 'youku' : 
~~~ youku_download_by_vid ( sourceId , title = title , output_dir = output_dir , merge = merge , info_only = info_only , ** kwargs ) 
~~ elif sourceType == 'tudou' : 
~~~ tudou_download_by_iid ( sourceId , title , output_dir = output_dir , merge = merge , info_only = info_only ) 
~~ elif sourceType == 'qq' : 
~~~ qq_download_by_vid ( sourceId , title , True , output_dir = output_dir , merge = merge , info_only = info_only ) 
~~ elif sourceType == 'letv' : 
~~~ letvcloud_download_by_vu ( sourceId , '2d8c027396' , title , output_dir = output_dir , merge = merge , info_only = info_only ) 
~~ elif sourceType == 'zhuzhan' : 
~~~ url = 'http://www.acfun.cn/v/ac' + vid 
yk_streams = youku_acfun_proxy ( info [ 'sourceId' ] , info [ 'encode' ] , url ) 
seq = [ 'mp4hd3' , 'mp4hd2' , 'mp4hd' , 'flvhd' ] 
for t in seq : 
~~~ if yk_streams . get ( t ) : 
~~~ preferred = yk_streams [ t ] 
break 
~~ ~~ size = 0 
for url in preferred [ 0 ] : 
~~~ _ , _ , seg_size = url_info ( url ) 
size += seg_size 
~~ if re . search ( r'fid=[0-9A-Z\\-]*.flv' , preferred [ 0 ] [ 0 ] ) : 
~~~ ext = 'flv' 
~~~ ext = 'mp4' 
~~ print_info ( site_info , title , ext , size ) 
~~~ download_urls ( preferred [ 0 ] , title , ext , size , output_dir = output_dir , merge = merge ) 
~~~ raise NotImplementedError ( sourceType ) 
~~ if not info_only and not dry_run : 
~~~ if not kwargs [ 'caption' ] : 
~~ try : 
~~~ title = get_filename ( title ) 
cmt = get_srt_json ( vid ) 
with open ( os . path . join ( output_dir , title + '.cmt.json' ) , 'w' , encoding = 'utf-8' ) as x : 
~~~ x . write ( cmt ) 
~~ ~~ except : 
~~ ~~ ~~ def main_dev ( ** kwargs ) : 
head = git . get_head ( kwargs [ 'repo_path' ] ) 
~~~ opts , args = getopt . getopt ( sys . argv [ 1 : ] , _short_options , _options ) 
~~ except getopt . GetoptError as e : 
~~ if not opts and not args : 
~~~ print ( _help ) 
#gui_main() 
~~~ conf = { } 
for opt , arg in opts : 
~~~ if opt in ( '-h' , '--help' ) : 
~~ elif opt in ( '-V' , '--version' ) : 
~~~ log . println ( "you-get:" , log . BOLD ) 
if head is not None : 
~~ elif opt in ( '-g' , '--gui' ) : 
~~~ conf [ 'gui' ] = True 
~~ elif opt in ( '-f' , '--force' ) : 
~~~ conf [ 'force' ] = True 
~~ elif opt in ( '-l' , '--playlist' , '--playlists' ) : 
~~~ conf [ 'playlist' ] = True 
~~ ~~ if args : 
~~~ if 'gui' in conf and conf [ 'gui' ] : 
~~~ from . gui import gui_main 
gui_main ( * args , ** conf ) 
~~~ from . console import console_main 
console_main ( * args , ** conf ) 
~~ ~~ ~~ ~~ def ffmpeg_download_stream ( files , title , ext , params = { } , output_dir = '.' , stream = True ) : 
output = title + '.' + ext 
if not ( output_dir == '.' ) : 
~~~ output = output_dir + '/' + output 
if stream : 
~~~ ffmpeg_params = [ FFMPEG ] + [ '-y' , '-re' , '-i' ] 
~~~ ffmpeg_params = [ FFMPEG ] + [ '-y' , '-i' ] 
~~~ ffmpeg_params += [ '-c' , 'copy' , output ] 
~~~ ffmpeg_params += [ '-c' , 'copy' , '-bsf:a' , 'aac_adtstoasc' ] 
~~ if params is not None : 
~~~ if len ( params ) > 0 : 
~~~ for k , v in params : 
~~~ ffmpeg_params . append ( k ) 
ffmpeg_params . append ( v ) 
~~ ~~ ~~ ffmpeg_params . append ( output ) 
~~~ a = subprocess . Popen ( ffmpeg_params , stdin = subprocess . PIPE ) 
a . communicate ( ) 
~~ except KeyboardInterrupt : 
~~~ a . stdin . write ( 'q' . encode ( 'utf-8' ) ) 
~~ ~~ return True 
~~ def match1 ( text , * patterns ) : 
if len ( patterns ) == 1 : 
~~~ pattern = patterns [ 0 ] 
match = re . search ( pattern , text ) 
if match : 
~~~ return match . group ( 1 ) 
~~~ ret = [ ] 
for pattern in patterns : 
~~~ match = re . search ( pattern , text ) 
~~~ ret . append ( match . group ( 1 ) ) 
~~ ~~ return ret 
~~ ~~ def matchall ( text , patterns ) : 
ret = [ ] 
~~~ match = re . findall ( pattern , text ) 
ret += match 
~~ return ret 
~~ def parse_query_param ( url , param ) : 
~~~ return parse . parse_qs ( parse . urlparse ( url ) . query ) [ param ] [ 0 ] 
~~ ~~ def ungzip ( data ) : 
from io import BytesIO 
import gzip 
buffer = BytesIO ( data ) 
f = gzip . GzipFile ( fileobj = buffer ) 
return f . read ( ) 
~~ def undeflate ( data ) : 
import zlib 
decompressobj = zlib . decompressobj ( - zlib . MAX_WBITS ) 
return decompressobj . decompress ( data ) + decompressobj . flush ( ) 
~~ def get_content ( url , headers = { } , decoded = True ) : 
req = request . Request ( url , headers = headers ) 
if cookies : 
~~~ cookies . add_cookie_header ( req ) 
req . headers . update ( req . unredirected_hdrs ) 
~~ response = urlopen_with_retry ( req ) 
data = response . read ( ) 
content_encoding = response . getheader ( 'Content-Encoding' ) 
if content_encoding == 'gzip' : 
~~~ data = ungzip ( data ) 
~~ elif content_encoding == 'deflate' : 
~~~ data = undeflate ( data ) 
~~ if decoded : 
~~~ charset = match1 ( 
response . getheader ( 'Content-Type' , '' ) , r'charset=([\\w-]+)' 
) 
if charset is not None : 
~~~ data = data . decode ( charset , 'ignore' ) 
~~~ data = data . decode ( 'utf-8' , 'ignore' ) 
~~ ~~ return data 
~~ def post_content ( url , headers = { } , post_data = { } , decoded = True , ** kwargs ) : 
if kwargs . get ( 'post_data_raw' ) : 
~~ req = request . Request ( url , headers = headers ) 
~~ if kwargs . get ( 'post_data_raw' ) : 
~~~ post_data_enc = bytes ( kwargs [ 'post_data_raw' ] , 'utf-8' ) 
~~~ post_data_enc = bytes ( parse . urlencode ( post_data ) , 'utf-8' ) 
~~ response = urlopen_with_retry ( req , data = post_data_enc ) 
response . getheader ( 'Content-Type' ) , r'charset=([\\w-]+)' 
~~~ data = data . decode ( charset ) 
~~~ data = data . decode ( 'utf-8' ) 
~~ def parse_host ( host ) : 
if re . match ( r'^(\\d+)$' , host ) is not None : 
~~~ return ( "0.0.0.0" , int ( host ) ) 
~~ if re . match ( r'^(\\w+)://' , host ) is None : 
~~~ host = "//" + host 
~~ o = parse . urlparse ( host ) 
hostname = o . hostname or "0.0.0.0" 
port = o . port or 0 
return ( hostname , port ) 
~~ def print_more_compatible ( * args , ** kwargs ) : 
~~~ import builtins as __builtin__ 
if sys . version_info [ : 2 ] >= ( 3 , 3 ) : 
~~~ return __builtin__ . print ( * args , ** kwargs ) 
~~ doFlush = kwargs . pop ( 'flush' , False ) 
ret = __builtin__ . print ( * args , ** kwargs ) 
if doFlush : 
~~~ kwargs . get ( 'file' , sys . stdout ) . flush ( ) 
~~ def showroom_get_roomid_by_room_url_key ( room_url_key ) : 
~~~ """str->str""" 
fake_headers_mobile = { 
'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8' , 
'Accept-Charset' : 'UTF-8,*;q=0.5' , 
'Accept-Encoding' : 'gzip,deflate,sdch' , 
'Accept-Language' : 'en-US,en;q=0.8' , 
} 
webpage_url = 'https://www.showroom-live.com/' + room_url_key 
html = get_content ( webpage_url , headers = fake_headers_mobile ) 
roomid = match1 ( html , r'room\\?room_id\\=(\\d+)' ) 
assert roomid 
return roomid 
~~ def showroom_download_by_room_id ( room_id , output_dir = '.' , merge = False , info_only = False , ** kwargs ) : 
while True : 
~~~ timestamp = str ( int ( time ( ) * 1000 ) ) 
api_endpoint = 'https://www.showroom-live.com/api/live/streaming_url?room_id={room_id}&_={timestamp}' . format ( room_id = room_id , timestamp = timestamp ) 
html = get_content ( api_endpoint ) 
html = json . loads ( html ) 
if len ( html ) >= 1 : 
sleep ( 1 ) 
~~ stream_url = [ i [ 'url' ] for i in html [ 'streaming_url_list' ] if i [ 'is_default' ] and i [ 'type' ] == 'hls' ] [ 0 ] 
assert stream_url 
#title 
title = '' 
profile_api = 'https://www.showroom-live.com/api/room/profile?room_id={room_id}' . format ( room_id = room_id ) 
html = loads ( get_content ( profile_api ) ) 
~~~ title = html [ 'main_name' ] 
~~ except KeyError : 
~~~ title = 'Showroom_{room_id}' . format ( room_id = room_id ) 
~~ type_ , ext , size = url_info ( stream_url ) 
~~~ download_url_ffmpeg ( url = stream_url , title = title , ext = 'mp4' , output_dir = output_dir ) 
~~ ~~ def _wanmen_get_title_by_json_topic_part ( json_content , tIndex , pIndex ) : 
return '_' . join ( [ json_content [ 0 ] [ 'name' ] , 
json_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'name' ] , 
json_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'Parts' ] [ pIndex ] [ 'name' ] ] ) 
~~ def wanmen_download_by_course ( json_api_content , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : 
for tIndex in range ( len ( json_api_content [ 0 ] [ 'Topics' ] ) ) : 
~~~ for pIndex in range ( len ( json_api_content [ 0 ] [ 'Topics' ] [ tIndex ] [ 'Parts' ] ) ) : 
~~~ wanmen_download_by_course_topic_part ( json_api_content , 
tIndex , 
pIndex , 
output_dir = output_dir , 
info_only = info_only , 
** kwargs ) 
~~ ~~ ~~ def wanmen_download_by_course_topic_part ( json_api_content , tIndex , pIndex , output_dir = '.' , merge = True , info_only = False , ** kwargs ) : 
html = json_api_content 
title = _wanmen_get_title_by_json_topic_part ( html , 
pIndex ) 
bokeccID = _wanmen_get_boke_id_by_json_topic_part ( html , 
bokecc_download_by_id ( vid = bokeccID , title = title , output_dir = output_dir , merge = merge , info_only = info_only , ** kwargs ) 
~~ def get_streams_by_id ( account_number , video_id ) : 
endpoint = 'https://edge.api.brightcove.com/playback/v1/accounts/{account_number}/videos/{video_id}' . format ( account_number = account_number , video_id = video_id ) 
fake_header_id = fake_headers 
fake_header_id [ 'Accept' ] = 'application/json;pk=BCpkADawqM1cc6wmJQC2tvoXZt4mrB7bFfi6zGt9QnOzprPZcGLE9OMGJwspQwKfuFYuCjAAJ53JdjI8zGFx1ll4rxhYJ255AXH1BQ10rnm34weknpfG-sippyQ' 
html = get_content ( endpoint , headers = fake_header_id ) 
html_json = json . loads ( html ) 
link_list = [ ] 
for i in html_json [ 'sources' ] : 
~~~ if i [ 'src' ] . startswith ( 'https' ) : 
~~~ link_list . append ( ( str ( i [ 'height' ] ) , i [ 'src' ] ) ) 
~~ ~~ ~~ return link_list 
~~ def vgg11_bn ( pretrained = False , ** kwargs ) : 
if pretrained : 
~~~ kwargs [ 'init_weights' ] = False 
~~ model = VGG ( make_layers ( cfg [ 'A' ] , batch_norm = True ) , ** kwargs ) 
~~~ model . load_state_dict ( model_zoo . load_url ( model_urls [ 'vgg11_bn' ] ) ) 
~~ return model 
~~ def vgg13 ( pretrained = False , ** kwargs ) : 
~~ model = VGG ( make_layers ( cfg [ 'B' ] ) , ** kwargs ) 
~~~ model . load_state_dict ( model_zoo . load_url ( model_urls [ 'vgg13' ] ) ) 
~~ def alexnet ( pretrained = False , ** kwargs ) : 
model = AlexNet ( ** kwargs ) 
~~~ model . load_state_dict ( model_zoo . load_url ( model_urls [ 'alexnet' ] ) ) 
~~ def densenet121 ( pretrained = False , ** kwargs ) : 
model = DenseNet ( num_init_features = 64 , growth_rate = 32 , block_config = ( 6 , 12 , 24 , 16 ) , 
~~~ _load_state_dict ( model , model_urls [ 'densenet121' ] ) 
~~ def to_tensor ( pic ) : 
if not ( _is_pil_image ( pic ) or _is_numpy_image ( pic ) ) : 
~~ if isinstance ( pic , np . ndarray ) : 
~~~ if pic . ndim == 2 : 
~~~ pic = pic [ : , : , None ] 
~~ img = torch . from_numpy ( pic . transpose ( ( 2 , 0 , 1 ) ) ) 
if isinstance ( img , torch . ByteTensor ) : 
~~~ return img . float ( ) . div ( 255 ) 
~~~ return img 
~~ ~~ if accimage is not None and isinstance ( pic , accimage . Image ) : 
~~~ nppic = np . zeros ( [ pic . channels , pic . height , pic . width ] , dtype = np . float32 ) 
pic . copyto ( nppic ) 
return torch . from_numpy ( nppic ) 
~~ if pic . mode == 'I' : 
~~~ img = torch . from_numpy ( np . array ( pic , np . int32 , copy = False ) ) 
~~ elif pic . mode == 'I;16' : 
~~~ img = torch . from_numpy ( np . array ( pic , np . int16 , copy = False ) ) 
~~ elif pic . mode == 'F' : 
~~~ img = torch . from_numpy ( np . array ( pic , np . float32 , copy = False ) ) 
~~ elif pic . mode == '1' : 
~~~ img = 255 * torch . from_numpy ( np . array ( pic , np . uint8 , copy = False ) ) 
~~~ img = torch . ByteTensor ( torch . ByteStorage . from_buffer ( pic . tobytes ( ) ) ) 
~~ if pic . mode == 'YCbCr' : 
~~~ nchannel = 3 
~~~ nchannel = 1 
~~~ nchannel = len ( pic . mode ) 
~~ img = img . view ( pic . size [ 1 ] , pic . size [ 0 ] , nchannel ) 
img = img . transpose ( 0 , 1 ) . transpose ( 0 , 2 ) . contiguous ( ) 
~~ ~~ def to_pil_image ( pic , mode = None ) : 
if not ( isinstance ( pic , torch . Tensor ) or isinstance ( pic , np . ndarray ) ) : 
~~ elif isinstance ( pic , torch . Tensor ) : 
~~~ if pic . ndimension ( ) not in { 2 , 3 } : 
~~ elif pic . ndimension ( ) == 2 : 
~~~ pic = pic . unsqueeze ( 0 ) 
~~ ~~ elif isinstance ( pic , np . ndarray ) : 
~~~ if pic . ndim not in { 2 , 3 } : 
~~ elif pic . ndim == 2 : 
~~~ pic = np . expand_dims ( pic , 2 ) 
~~ ~~ npimg = pic 
if isinstance ( pic , torch . FloatTensor ) : 
~~~ pic = pic . mul ( 255 ) . byte ( ) 
~~ if isinstance ( pic , torch . Tensor ) : 
~~~ npimg = np . transpose ( pic . numpy ( ) , ( 1 , 2 , 0 ) ) 
~~ if not isinstance ( npimg , np . ndarray ) : 
~~ if npimg . shape [ 2 ] == 1 : 
~~~ expected_mode = None 
npimg = npimg [ : , : , 0 ] 
if npimg . dtype == np . uint8 : 
~~~ expected_mode = 'L' 
~~ elif npimg . dtype == np . int16 : 
~~~ expected_mode = 'I;16' 
~~ elif npimg . dtype == np . int32 : 
~~~ expected_mode = 'I' 
~~ elif npimg . dtype == np . float32 : 
~~~ expected_mode = 'F' 
~~ if mode is not None and mode != expected_mode : 
. format ( mode , np . dtype , expected_mode ) ) 
~~ mode = expected_mode 
~~ elif npimg . shape [ 2 ] == 2 : 
~~~ permitted_2_channel_modes = [ 'LA' ] 
if mode is not None and mode not in permitted_2_channel_modes : 
~~ if mode is None and npimg . dtype == np . uint8 : 
~~~ mode = 'LA' 
~~ ~~ elif npimg . shape [ 2 ] == 4 : 
~~~ permitted_4_channel_modes = [ 'RGBA' , 'CMYK' , 'RGBX' ] 
if mode is not None and mode not in permitted_4_channel_modes : 
~~~ mode = 'RGBA' 
~~~ permitted_3_channel_modes = [ 'RGB' , 'YCbCr' , 'HSV' ] 
if mode is not None and mode not in permitted_3_channel_modes : 
~~~ mode = 'RGB' 
~~ ~~ if mode is None : 
~~ return Image . fromarray ( npimg , mode = mode ) 
~~ def normalize ( tensor , mean , std , inplace = False ) : 
if not _is_tensor_image ( tensor ) : 
~~ if not inplace : 
~~~ tensor = tensor . clone ( ) 
~~ mean = torch . as_tensor ( mean , dtype = torch . float32 , device = tensor . device ) 
std = torch . as_tensor ( std , dtype = torch . float32 , device = tensor . device ) 
tensor . sub_ ( mean [ : , None , None ] ) . div_ ( std [ : , None , None ] ) 
return tensor 
~~ def resize ( img , size , interpolation = Image . BILINEAR ) : 
if not _is_pil_image ( img ) : 
~~ if not ( isinstance ( size , int ) or ( isinstance ( size , Iterable ) and len ( size ) == 2 ) ) : 
~~ if isinstance ( size , int ) : 
~~~ w , h = img . size 
if ( w <= h and w == size ) or ( h <= w and h == size ) : 
~~ if w < h : 
~~~ ow = size 
oh = int ( size * h / w ) 
return img . resize ( ( ow , oh ) , interpolation ) 
~~~ oh = size 
ow = int ( size * w / h ) 
~~~ return img . resize ( size [ : : - 1 ] , interpolation ) 
~~ ~~ def pad ( img , padding , fill = 0 , padding_mode = 'constant' ) : 
~~ if not isinstance ( padding , ( numbers . Number , tuple ) ) : 
~~ if not isinstance ( fill , ( numbers . Number , str , tuple ) ) : 
~~ if not isinstance ( padding_mode , str ) : 
~~ if isinstance ( padding , Sequence ) and len ( padding ) not in [ 2 , 4 ] : 
if padding_mode == 'constant' : 
~~~ if img . mode == 'P' : 
~~~ palette = img . getpalette ( ) 
image = ImageOps . expand ( img , border = padding , fill = fill ) 
image . putpalette ( palette ) 
return image 
~~ return ImageOps . expand ( img , border = padding , fill = fill ) 
~~~ if isinstance ( padding , int ) : 
~~~ pad_left = pad_right = pad_top = pad_bottom = padding 
~~ if isinstance ( padding , Sequence ) and len ( padding ) == 2 : 
~~~ pad_left = pad_right = padding [ 0 ] 
pad_top = pad_bottom = padding [ 1 ] 
~~ if isinstance ( padding , Sequence ) and len ( padding ) == 4 : 
~~~ pad_left = padding [ 0 ] 
pad_top = padding [ 1 ] 
pad_right = padding [ 2 ] 
pad_bottom = padding [ 3 ] 
~~ if img . mode == 'P' : 
img = np . asarray ( img ) 
img = np . pad ( img , ( ( pad_top , pad_bottom ) , ( pad_left , pad_right ) ) , padding_mode ) 
img = Image . fromarray ( img ) 
img . putpalette ( palette ) 
return img 
~~ img = np . asarray ( img ) 
if len ( img . shape ) == 3 : 
~~~ img = np . pad ( img , ( ( pad_top , pad_bottom ) , ( pad_left , pad_right ) , ( 0 , 0 ) ) , padding_mode ) 
~~ if len ( img . shape ) == 2 : 
~~~ img = np . pad ( img , ( ( pad_top , pad_bottom ) , ( pad_left , pad_right ) ) , padding_mode ) 
~~ return Image . fromarray ( img ) 
~~ ~~ def crop ( img , i , j , h , w ) : 
~~ return img . crop ( ( j , i , j + w , i + h ) ) 
~~ def resized_crop ( img , i , j , h , w , size , interpolation = Image . BILINEAR ) : 
img = crop ( img , i , j , h , w ) 
img = resize ( img , size , interpolation ) 
~~ def hflip ( img ) : 
~~ return img . transpose ( Image . FLIP_LEFT_RIGHT ) 
~~ def _get_perspective_coeffs ( startpoints , endpoints ) : 
matrix = [ ] 
for p1 , p2 in zip ( endpoints , startpoints ) : 
~~~ matrix . append ( [ p1 [ 0 ] , p1 [ 1 ] , 1 , 0 , 0 , 0 , - p2 [ 0 ] * p1 [ 0 ] , - p2 [ 0 ] * p1 [ 1 ] ] ) 
matrix . append ( [ 0 , 0 , 0 , p1 [ 0 ] , p1 [ 1 ] , 1 , - p2 [ 1 ] * p1 [ 0 ] , - p2 [ 1 ] * p1 [ 1 ] ] ) 
~~ A = torch . tensor ( matrix , dtype = torch . float ) 
B = torch . tensor ( startpoints , dtype = torch . float ) . view ( 8 ) 
res = torch . gels ( B , A ) [ 0 ] 
return res . squeeze_ ( 1 ) . tolist ( ) 
~~ def perspective ( img , startpoints , endpoints , interpolation = Image . BICUBIC ) : 
~~ coeffs = _get_perspective_coeffs ( startpoints , endpoints ) 
return img . transform ( img . size , Image . PERSPECTIVE , coeffs , interpolation ) 
~~ def vflip ( img ) : 
~~ return img . transpose ( Image . FLIP_TOP_BOTTOM ) 
~~ def five_crop ( img , size ) : 
if isinstance ( size , numbers . Number ) : 
~~~ size = ( int ( size ) , int ( size ) ) 
~~ w , h = img . size 
crop_h , crop_w = size 
if crop_w > w or crop_h > h : 
( h , w ) ) ) 
~~ tl = img . crop ( ( 0 , 0 , crop_w , crop_h ) ) 
tr = img . crop ( ( w - crop_w , 0 , w , crop_h ) ) 
bl = img . crop ( ( 0 , h - crop_h , crop_w , h ) ) 
br = img . crop ( ( w - crop_w , h - crop_h , w , h ) ) 
center = center_crop ( img , ( crop_h , crop_w ) ) 
return ( tl , tr , bl , br , center ) 
~~ def ten_crop ( img , size , vertical_flip = False ) : 
~~ first_five = five_crop ( img , size ) 
if vertical_flip : 
~~~ img = vflip ( img ) 
~~~ img = hflip ( img ) 
~~ second_five = five_crop ( img , size ) 
return first_five + second_five 
~~ def adjust_brightness ( img , brightness_factor ) : 
~~ enhancer = ImageEnhance . Brightness ( img ) 
img = enhancer . enhance ( brightness_factor ) 
~~ def adjust_contrast ( img , contrast_factor ) : 
~~ enhancer = ImageEnhance . Contrast ( img ) 
img = enhancer . enhance ( contrast_factor ) 
~~ def adjust_saturation ( img , saturation_factor ) : 
~~ enhancer = ImageEnhance . Color ( img ) 
img = enhancer . enhance ( saturation_factor ) 
~~ def adjust_hue ( img , hue_factor ) : 
if not ( - 0.5 <= hue_factor <= 0.5 ) : 
~~ if not _is_pil_image ( img ) : 
~~ input_mode = img . mode 
if input_mode in { 'L' , '1' , 'I' , 'F' } : 
~~ h , s , v = img . convert ( 'HSV' ) . split ( ) 
np_h = np . array ( h , dtype = np . uint8 ) 
with np . errstate ( over = 'ignore' ) : 
~~~ np_h += np . uint8 ( hue_factor * 255 ) 
~~ h = Image . fromarray ( np_h , 'L' ) 
img = Image . merge ( 'HSV' , ( h , s , v ) ) . convert ( input_mode ) 
~~ def adjust_gamma ( img , gamma , gain = 1 ) : 
~~ if gamma < 0 : 
img = img . convert ( 'RGB' ) 
gamma_map = [ 255 * gain * pow ( ele / 255. , gamma ) for ele in range ( 256 ) ] * 3 
img = img . convert ( input_mode ) 
~~ def rotate ( img , angle , resample = False , expand = False , center = None ) : 
~~ return img . rotate ( angle , resample , expand , center ) 
~~ def affine ( img , angle , translate , scale , shear , resample = 0 , fillcolor = None ) : 
output_size = img . size 
center = ( img . size [ 0 ] * 0.5 + 0.5 , img . size [ 1 ] * 0.5 + 0.5 ) 
matrix = _get_inverse_affine_matrix ( center , angle , translate , scale , shear ) 
kwargs = { "fillcolor" : fillcolor } if PILLOW_VERSION [ 0 ] == '5' else { } 
return img . transform ( output_size , Image . AFFINE , matrix , resample , ** kwargs ) 
~~ def to_grayscale ( img , num_output_channels = 1 ) : 
~~ if num_output_channels == 1 : 
~~~ img = img . convert ( 'L' ) 
~~ elif num_output_channels == 3 : 
np_img = np . array ( img , dtype = np . uint8 ) 
np_img = np . dstack ( [ np_img , np_img , np_img ] ) 
img = Image . fromarray ( np_img , 'RGB' ) 
~~ return img 
~~ def make_grid ( tensor , nrow = 8 , padding = 2 , 
normalize = False , range = None , scale_each = False , pad_value = 0 ) : 
if not ( torch . is_tensor ( tensor ) or 
( isinstance ( tensor , list ) and all ( torch . is_tensor ( t ) for t in tensor ) ) ) : 
~~ if isinstance ( tensor , list ) : 
~~~ tensor = torch . stack ( tensor , dim = 0 ) 
~~~ tensor = tensor . unsqueeze ( 0 ) 
~~~ tensor = torch . cat ( ( tensor , tensor , tensor ) , 0 ) 
~~ tensor = tensor . unsqueeze ( 0 ) 
~~~ tensor = torch . cat ( ( tensor , tensor , tensor ) , 1 ) 
~~ if normalize is True : 
if range is not None : 
~~ def norm_ip ( img , min , max ) : 
~~~ img . clamp_ ( min = min , max = max ) 
img . add_ ( - min ) . div_ ( max - min + 1e-5 ) 
~~ def norm_range ( t , range ) : 
~~~ if range is not None : 
~~~ norm_ip ( t , range [ 0 ] , range [ 1 ] ) 
~~~ norm_ip ( t , float ( t . min ( ) ) , float ( t . max ( ) ) ) 
~~ ~~ if scale_each is True : 
~~~ norm_range ( t , range ) 
~~~ norm_range ( tensor , range ) 
~~ ~~ if tensor . size ( 0 ) == 1 : 
~~~ return tensor . squeeze ( ) 
~~ nmaps = tensor . size ( 0 ) 
xmaps = min ( nrow , nmaps ) 
ymaps = int ( math . ceil ( float ( nmaps ) / xmaps ) ) 
height , width = int ( tensor . size ( 2 ) + padding ) , int ( tensor . size ( 3 ) + padding ) 
grid = tensor . new_full ( ( 3 , height * ymaps + padding , width * xmaps + padding ) , pad_value ) 
k = 0 
for y in irange ( ymaps ) : 
~~~ for x in irange ( xmaps ) : 
~~~ if k >= nmaps : 
~~ grid . narrow ( 1 , y * height + padding , height - padding ) . narrow ( 2 , x * width + padding , width - padding ) . copy_ ( tensor [ k ] ) 
k = k + 1 
~~ ~~ return grid 
~~ def save_image ( tensor , filename , nrow = 8 , padding = 2 , 
from PIL import Image 
grid = make_grid ( tensor , nrow = nrow , padding = padding , pad_value = pad_value , 
normalize = normalize , range = range , scale_each = scale_each ) 
ndarr = grid . mul_ ( 255 ) . add_ ( 0.5 ) . clamp_ ( 0 , 255 ) . permute ( 1 , 2 , 0 ) . to ( 'cpu' , torch . uint8 ) . numpy ( ) 
im = Image . fromarray ( ndarr ) 
im . save ( filename ) 
~~ def _find_classes ( self , dir ) : 
if sys . version_info >= ( 3 , 5 ) : 
~~~ classes = [ d . name for d in os . scandir ( dir ) if d . is_dir ( ) ] 
~~~ classes = [ d for d in os . listdir ( dir ) if os . path . isdir ( os . path . join ( dir , d ) ) ] 
~~ classes . sort ( ) 
class_to_idx = { classes [ i ] : i for i in range ( len ( classes ) ) } 
return classes , class_to_idx 
~~ def read_image_file ( data_dir , image_ext , n ) : 
def PIL2array ( _img ) : 
return np . array ( _img . getdata ( ) , dtype = np . uint8 ) . reshape ( 64 , 64 ) 
~~ def find_files ( _data_dir , _image_ext ) : 
files = [ ] 
for file_dir in os . listdir ( _data_dir ) : 
~~~ if file_dir . endswith ( _image_ext ) : 
~~~ files . append ( os . path . join ( _data_dir , file_dir ) ) 
~~ patches = [ ] 
list_files = find_files ( data_dir , image_ext ) 
for fpath in list_files : 
~~~ img = Image . open ( fpath ) 
for y in range ( 0 , 1024 , 64 ) : 
~~~ for x in range ( 0 , 1024 , 64 ) : 
~~~ patch = img . crop ( ( x , y , x + 64 , y + 64 ) ) 
patches . append ( PIL2array ( patch ) ) 
~~ ~~ ~~ return torch . ByteTensor ( np . array ( patches [ : n ] ) ) 
~~ def read_info_file ( data_dir , info_file ) : 
labels = [ ] 
with open ( os . path . join ( data_dir , info_file ) , 'r' ) as f : 
~~~ labels = [ int ( line . split ( ) [ 0 ] ) for line in f ] 
~~ return torch . LongTensor ( labels ) 
~~ def read_matches_files ( data_dir , matches_file ) : 
matches = [ ] 
with open ( os . path . join ( data_dir , matches_file ) , 'r' ) as f : 
~~~ for line in f : 
~~~ line_split = line . split ( ) 
matches . append ( [ int ( line_split [ 0 ] ) , int ( line_split [ 3 ] ) , 
int ( line_split [ 1 ] == line_split [ 4 ] ) ] ) 
~~ ~~ return torch . LongTensor ( matches ) 
~~ def conv1x1 ( in_planes , out_planes , stride = 1 ) : 
return nn . Conv2d ( in_planes , out_planes , kernel_size = 1 , stride = stride , bias = False ) 
~~ def accuracy ( output , target , topk = ( 1 , ) ) : 
with torch . no_grad ( ) : 
~~~ maxk = max ( topk ) 
batch_size = target . size ( 0 ) 
_ , pred = output . topk ( maxk , 1 , True , True ) 
pred = pred . t ( ) 
correct = pred . eq ( target [ None ] ) 
res = [ ] 
for k in topk : 
~~~ correct_k = correct [ : k ] . flatten ( ) . sum ( dtype = torch . float32 ) 
res . append ( correct_k * ( 100.0 / batch_size ) ) 
~~ return res 
~~ ~~ def setup_for_distributed ( is_master ) : 
import builtins as __builtin__ 
builtin_print = __builtin__ . print 
def print ( * args , ** kwargs ) : 
~~~ force = kwargs . pop ( 'force' , False ) 
if is_master or force : 
~~~ builtin_print ( * args , ** kwargs ) 
~~ ~~ __builtin__ . print = print 
~~ def synchronize_between_processes ( self ) : 
if not is_dist_avail_and_initialized ( ) : 
~~~ return 
~~ t = torch . tensor ( [ self . count , self . total ] , dtype = torch . float64 , device = 'cuda' ) 
dist . barrier ( ) 
dist . all_reduce ( t ) 
t = t . tolist ( ) 
self . count = int ( t [ 0 ] ) 
self . total = t [ 1 ] 
~~ def squeezenet1_1 ( pretrained = False , ** kwargs ) : 
model = SqueezeNet ( version = 1.1 , ** kwargs ) 
~~~ model . load_state_dict ( model_zoo . load_url ( model_urls [ 'squeezenet1_1' ] ) ) 
~~ def makedir_exist_ok ( dirpath ) : 
~~~ os . makedirs ( dirpath ) 
~~ except OSError as e : 
~~~ if e . errno == errno . EEXIST : 
~~~ raise 
~~ ~~ ~~ def download_url ( url , root , filename = None , md5 = None ) : 
from six . moves import urllib 
root = os . path . expanduser ( root ) 
if not filename : 
~~~ filename = os . path . basename ( url ) 
~~ fpath = os . path . join ( root , filename ) 
makedir_exist_ok ( root ) 
if os . path . isfile ( fpath ) and check_integrity ( fpath , md5 ) : 
urllib . request . urlretrieve ( 
url , fpath , 
reporthook = gen_bar_updater ( ) 
~~ except OSError : 
~~~ if url [ : 5 ] == 'https' : 
~~~ url = url . replace ( 'https:' , 'http:' ) 
~~ ~~ ~~ ~~ def list_dir ( root , prefix = False ) : 
directories = list ( 
filter ( 
lambda p : os . path . isdir ( os . path . join ( root , p ) ) , 
os . listdir ( root ) 
if prefix is True : 
~~~ directories = [ os . path . join ( root , d ) for d in directories ] 
~~ return directories 
~~ def list_files ( root , suffix , prefix = False ) : 
files = list ( 
lambda p : os . path . isfile ( os . path . join ( root , p ) ) and p . endswith ( suffix ) , 
~~~ files = [ os . path . join ( root , d ) for d in files ] 
~~ return files 
~~ def download_file_from_google_drive ( file_id , root , filename = None , md5 = None ) : 
import requests 
url = "https://docs.google.com/uc?export=download" 
~~~ filename = file_id 
~~~ session = requests . Session ( ) 
response = session . get ( url , params = { 'id' : file_id } , stream = True ) 
token = _get_confirm_token ( response ) 
if token : 
~~~ params = { 'id' : file_id , 'confirm' : token } 
response = session . get ( url , params = params , stream = True ) 
~~ _save_response_content ( response , fpath ) 
~~ ~~ def get_params ( img , output_size ) : 
w , h = img . size 
th , tw = output_size 
if w == tw and h == th : 
~~~ return 0 , 0 , h , w 
~~ i = random . randint ( 0 , h - th ) 
j = random . randint ( 0 , w - tw ) 
return i , j , th , tw 
~~ def get_params ( width , height , distortion_scale ) : 
half_height = int ( height / 2 ) 
half_width = int ( width / 2 ) 
topleft = ( random . randint ( 0 , int ( distortion_scale * half_width ) ) , 
random . randint ( 0 , int ( distortion_scale * half_height ) ) ) 
topright = ( random . randint ( width - int ( distortion_scale * half_width ) - 1 , width - 1 ) , 
botright = ( random . randint ( width - int ( distortion_scale * half_width ) - 1 , width - 1 ) , 
random . randint ( height - int ( distortion_scale * half_height ) - 1 , height - 1 ) ) 
botleft = ( random . randint ( 0 , int ( distortion_scale * half_width ) ) , 
startpoints = [ ( 0 , 0 ) , ( width - 1 , 0 ) , ( width - 1 , height - 1 ) , ( 0 , height - 1 ) ] 
endpoints = [ topleft , topright , botright , botleft ] 
return startpoints , endpoints 
~~ def get_params ( img , scale , ratio ) : 
area = img . size [ 0 ] * img . size [ 1 ] 
for attempt in range ( 10 ) : 
~~~ target_area = random . uniform ( * scale ) * area 
log_ratio = ( math . log ( ratio [ 0 ] ) , math . log ( ratio [ 1 ] ) ) 
aspect_ratio = math . exp ( random . uniform ( * log_ratio ) ) 
w = int ( round ( math . sqrt ( target_area * aspect_ratio ) ) ) 
h = int ( round ( math . sqrt ( target_area / aspect_ratio ) ) ) 
if w <= img . size [ 0 ] and h <= img . size [ 1 ] : 
~~~ i = random . randint ( 0 , img . size [ 1 ] - h ) 
j = random . randint ( 0 , img . size [ 0 ] - w ) 
return i , j , h , w 
~~ ~~ in_ratio = img . size [ 0 ] / img . size [ 1 ] 
if ( in_ratio < min ( ratio ) ) : 
~~~ w = img . size [ 0 ] 
h = w / min ( ratio ) 
~~ elif ( in_ratio > max ( ratio ) ) : 
~~~ h = img . size [ 1 ] 
w = h * max ( ratio ) 
h = img . size [ 1 ] 
~~ i = ( img . size [ 1 ] - h ) // 2 
j = ( img . size [ 0 ] - w ) // 2 
~~ def get_params ( brightness , contrast , saturation , hue ) : 
transforms = [ ] 
if brightness is not None : 
~~~ brightness_factor = random . uniform ( brightness [ 0 ] , brightness [ 1 ] ) 
transforms . append ( Lambda ( lambda img : F . adjust_brightness ( img , brightness_factor ) ) ) 
~~ if contrast is not None : 
~~~ contrast_factor = random . uniform ( contrast [ 0 ] , contrast [ 1 ] ) 
transforms . append ( Lambda ( lambda img : F . adjust_contrast ( img , contrast_factor ) ) ) 
~~ if saturation is not None : 
~~~ saturation_factor = random . uniform ( saturation [ 0 ] , saturation [ 1 ] ) 
transforms . append ( Lambda ( lambda img : F . adjust_saturation ( img , saturation_factor ) ) ) 
~~ if hue is not None : 
~~~ hue_factor = random . uniform ( hue [ 0 ] , hue [ 1 ] ) 
transforms . append ( Lambda ( lambda img : F . adjust_hue ( img , hue_factor ) ) ) 
~~ random . shuffle ( transforms ) 
transform = Compose ( transforms ) 
return transform 
~~ def get_params ( degrees , translate , scale_ranges , shears , img_size ) : 
angle = random . uniform ( degrees [ 0 ] , degrees [ 1 ] ) 
if translate is not None : 
~~~ max_dx = translate [ 0 ] * img_size [ 0 ] 
max_dy = translate [ 1 ] * img_size [ 1 ] 
translations = ( np . round ( random . uniform ( - max_dx , max_dx ) ) , 
np . round ( random . uniform ( - max_dy , max_dy ) ) ) 
~~~ translations = ( 0 , 0 ) 
~~ if scale_ranges is not None : 
~~~ scale = random . uniform ( scale_ranges [ 0 ] , scale_ranges [ 1 ] ) 
~~~ scale = 1.0 
~~ if shears is not None : 
~~~ shear = random . uniform ( shears [ 0 ] , shears [ 1 ] ) 
~~~ shear = 0.0 
~~ return angle , translations , scale , shear 
~~ def inception_v3 ( pretrained = False , ** kwargs ) : 
~~~ if 'transform_input' not in kwargs : 
~~~ kwargs [ 'transform_input' ] = True 
~~ if 'aux_logits' in kwargs : 
~~~ original_aux_logits = kwargs [ 'aux_logits' ] 
kwargs [ 'aux_logits' ] = True 
~~~ original_aux_logits = True 
~~ model = Inception3 ( ** kwargs ) 
model . load_state_dict ( model_zoo . load_url ( model_urls [ 'inception_v3_google' ] ) ) 
if not original_aux_logits : 
~~~ model . aux_logits = False 
del model . AuxLogits 
~~ return Inception3 ( ** kwargs ) 
~~ def download ( self ) : 
import tarfile 
if self . _check_integrity ( ) : 
~~ download_url ( self . url , self . root , self . filename , self . md5_checksum ) 
with tarfile . open ( os . path . join ( self . root , self . filename ) , 'r:gz' ) as tar : 
~~~ tar . extractall ( path = self . root ) 
~~ with open ( os . path . join ( self . root , 'dataset' , 'SBU_captioned_photo_dataset_urls.txt' ) ) as fh : 
~~~ for line in fh : 
~~~ url = line . rstrip ( ) 
~~~ download_url ( url , os . path . join ( self . root , 'dataset' ) ) 
~~ ~~ ~~ ~~ def googlenet ( pretrained = False , ** kwargs ) : 
~~ if 'aux_logits' not in kwargs : 
~~~ kwargs [ 'aux_logits' ] = False 
~~ if kwargs [ 'aux_logits' ] : 
~~ original_aux_logits = kwargs [ 'aux_logits' ] 
kwargs [ 'init_weights' ] = False 
model = GoogLeNet ( ** kwargs ) 
model . load_state_dict ( model_zoo . load_url ( model_urls [ 'googlenet' ] ) ) 
del model . aux1 , model . aux2 
~~ return GoogLeNet ( ** kwargs ) 
if self . _check_exists ( ) : 
~~ makedir_exist_ok ( self . raw_folder ) 
makedir_exist_ok ( self . processed_folder ) 
for url in self . urls : 
~~~ filename = url . rpartition ( '/' ) [ 2 ] 
file_path = os . path . join ( self . raw_folder , filename ) 
download_url ( url , root = self . raw_folder , filename = filename , md5 = None ) 
self . extract_gzip ( gzip_path = file_path , remove_finished = True ) 
~~ print ( 'Processing...' ) 
training_set = ( 
read_image_file ( os . path . join ( self . raw_folder , 'train-images-idx3-ubyte' ) ) , 
read_label_file ( os . path . join ( self . raw_folder , 'train-labels-idx1-ubyte' ) ) 
test_set = ( 
read_image_file ( os . path . join ( self . raw_folder , 't10k-images-idx3-ubyte' ) ) , 
read_label_file ( os . path . join ( self . raw_folder , 't10k-labels-idx1-ubyte' ) ) 
with open ( os . path . join ( self . processed_folder , self . training_file ) , 'wb' ) as f : 
~~~ torch . save ( training_set , f ) 
~~ with open ( os . path . join ( self . processed_folder , self . test_file ) , 'wb' ) as f : 
~~~ torch . save ( test_set , f ) 
~~ print ( 'Done!' ) 
import shutil 
import zipfile 
filename = self . url . rpartition ( '/' ) [ 2 ] 
download_url ( self . url , root = self . raw_folder , filename = filename , md5 = None ) 
with zipfile . ZipFile ( file_path ) as zip_f : 
~~~ zip_f . extractall ( self . raw_folder ) 
~~ os . unlink ( file_path ) 
gzip_folder = os . path . join ( self . raw_folder , 'gzip' ) 
for gzip_file in os . listdir ( gzip_folder ) : 
~~~ if gzip_file . endswith ( '.gz' ) : 
~~~ self . extract_gzip ( gzip_path = os . path . join ( gzip_folder , gzip_file ) ) 
~~ ~~ for split in self . splits : 
read_image_file ( os . path . join ( gzip_folder , 'emnist-{}-train-images-idx3-ubyte' . format ( split ) ) ) , 
read_label_file ( os . path . join ( gzip_folder , 'emnist-{}-train-labels-idx1-ubyte' . format ( split ) ) ) 
read_image_file ( os . path . join ( gzip_folder , 'emnist-{}-test-images-idx3-ubyte' . format ( split ) ) ) , 
read_label_file ( os . path . join ( gzip_folder , 'emnist-{}-test-labels-idx1-ubyte' . format ( split ) ) ) 
with open ( os . path . join ( self . processed_folder , self . _training_file ( split ) ) , 'wb' ) as f : 
~~ with open ( os . path . join ( self . processed_folder , self . _test_file ( split ) ) , 'wb' ) as f : 
~~ ~~ shutil . rmtree ( gzip_folder ) 
print ( 'Done!' ) 
~~ def request ( method , url , ** kwargs ) : 
time_before_request = time ( ) 
session = SessionSinglePool ( ) 
kwargs [ 'proxies' ] = settings [ 'outgoing' ] . get ( 'proxies' ) or None 
if 'timeout' in kwargs : 
~~~ timeout = kwargs [ 'timeout' ] 
~~~ timeout = getattr ( threadLocal , 'timeout' , None ) 
if timeout is not None : 
~~~ kwargs [ 'timeout' ] = timeout 
~~ ~~ response = session . request ( method = method , url = url , ** kwargs ) 
time_after_request = time ( ) 
start_time = getattr ( threadLocal , 'start_time' , time_before_request ) 
search_duration = time_after_request - start_time 
if search_duration > timeout + timeout_overhead : 
~~~ raise requests . exceptions . Timeout ( response = response ) 
~~ ~~ session . close ( ) 
if hasattr ( threadLocal , 'total_time' ) : 
~~~ threadLocal . total_time += time_after_request - time_before_request 
~~ return response 
~~ def get_current_theme_name ( override = None ) : 
if override and ( override in themes or override == '__common__' ) : 
~~~ return override 
~~ theme_name = request . args . get ( 'theme' , request . preferences . get_value ( 'theme' ) ) 
if theme_name not in themes : 
~~~ theme_name = default_theme 
~~ return theme_name 
~~ def index ( ) : 
output_format = request . form . get ( 'format' , 'html' ) 
if output_format not in [ 'html' , 'csv' , 'json' , 'rss' ] : 
~~~ output_format = 'html' 
~~ if request . form . get ( 'q' ) is None : 
~~~ if output_format == 'html' : 
~~~ return render ( 
'index.html' , 
~~ ~~ search_query = None 
result_container = None 
~~~ search_query = get_search_query_from_webapp ( request . preferences , request . form ) 
search = SearchWithPlugins ( search_query , request . user_plugins , request ) 
result_container = search . search ( ) 
~~ except Exception as e : 
if ( issubclass ( e . __class__ , SearxParameterException ) ) : 
~~~ return index_error ( output_format , e . message ) , 400 
~~ ~~ results = result_container . get_ordered_results ( ) 
number_of_results = result_container . results_number ( ) 
if number_of_results < result_container . results_length ( ) : 
~~~ number_of_results = 0 
~~ advanced_search = request . form . get ( 'advanced_search' , None ) 
for result in results : 
~~~ if 'content' in result and result [ 'content' ] : 
~~~ result [ 'content' ] = highlight_content ( escape ( result [ 'content' ] [ : 1024 ] ) , search_query . query ) 
~~ result [ 'title' ] = highlight_content ( escape ( result [ 'title' ] or u'' ) , search_query . query ) 
~~~ if result . get ( 'content' ) : 
~~~ result [ 'content' ] = html_to_text ( result [ 'content' ] ) . strip ( ) 
~~ result [ 'pretty_url' ] = prettify_url ( result [ 'url' ] ) 
if 'publishedDate' in result : 
~~ except ValueError : 
~~~ result [ 'publishedDate' ] = None 
~~~ if result [ 'publishedDate' ] . replace ( tzinfo = None ) >= datetime . now ( ) - timedelta ( days = 1 ) : 
~~~ timedifference = datetime . now ( ) - result [ 'publishedDate' ] . replace ( tzinfo = None ) 
minutes = int ( ( timedifference . seconds / 60 ) % 60 ) 
hours = int ( timedifference . seconds / 60 / 60 ) 
if hours == 0 : 
~~~ result [ 'publishedDate' ] = format_date ( result [ 'publishedDate' ] ) 
~~ ~~ ~~ ~~ if output_format == 'json' : 
~~~ return Response ( json . dumps ( { 'query' : search_query . query . decode ( 'utf-8' ) , 
'number_of_results' : number_of_results , 
'results' : results , 
'answers' : list ( result_container . answers ) , 
'corrections' : list ( result_container . corrections ) , 
'infoboxes' : result_container . infoboxes , 
'suggestions' : list ( result_container . suggestions ) , 
'unresponsive_engines' : list ( result_container . unresponsive_engines ) } , 
default = lambda item : list ( item ) if isinstance ( item , set ) else item ) , 
mimetype = 'application/json' ) 
~~ elif output_format == 'csv' : 
~~~ csv = UnicodeWriter ( StringIO ( ) ) 
keys = ( 'title' , 'url' , 'content' , 'host' , 'engine' , 'score' ) 
csv . writerow ( keys ) 
for row in results : 
~~~ row [ 'host' ] = row [ 'parsed_url' ] . netloc 
csv . writerow ( [ row . get ( key , '' ) for key in keys ] ) 
~~ csv . stream . seek ( 0 ) 
response = Response ( csv . stream . read ( ) , mimetype = 'application/csv' ) 
cont_disp = 'attachment;Filename=searx_-_{0}.csv' . format ( search_query . query ) 
response . headers . add ( 'Content-Disposition' , cont_disp ) 
return response 
~~ elif output_format == 'rss' : 
~~~ response_rss = render ( 
'opensearch_response_rss.xml' , 
results = results , 
q = request . form [ 'q' ] , 
number_of_results = number_of_results , 
base_url = get_base_url ( ) , 
override_theme = '__common__' , 
return Response ( response_rss , mimetype = 'text/xml' ) 
~~ return render ( 
'results.html' , 
selected_categories = search_query . categories , 
pageno = search_query . pageno , 
time_range = search_query . time_range , 
number_of_results = format_decimal ( number_of_results ) , 
advanced_search = advanced_search , 
suggestions = result_container . suggestions , 
answers = result_container . answers , 
corrections = result_container . corrections , 
infoboxes = result_container . infoboxes , 
paging = result_container . paging , 
unresponsive_engines = result_container . unresponsive_engines , 
current_language = match_language ( search_query . lang , 
LANGUAGE_CODES , 
fallback = settings [ 'search' ] [ 'language' ] ) , 
theme = get_current_theme_name ( ) , 
favicons = global_favicons [ themes . index ( get_current_theme_name ( ) ) ] 
~~ def autocompleter ( ) : 
disabled_engines = request . preferences . engines . get_disabled ( ) 
if PY3 : 
~~~ raw_text_query = RawTextQuery ( request . form . get ( 'q' , b'' ) , disabled_engines ) 
~~~ raw_text_query = RawTextQuery ( request . form . get ( 'q' , u'' ) . encode ( 'utf-8' ) , disabled_engines ) 
~~ raw_text_query . parse_query ( ) 
if not raw_text_query . getSearchQuery ( ) : 
~~~ return '' , 400 
~~ completer = autocomplete_backends . get ( request . preferences . get_value ( 'autocomplete' ) ) 
raw_results = searx_bang ( raw_text_query ) 
if len ( raw_results ) <= 3 and completer : 
~~~ language = request . preferences . get_value ( 'language' ) 
if not language or language == 'all' : 
~~~ language = 'en' 
~~~ language = language . split ( '-' ) [ 0 ] 
~~ raw_results . extend ( completer ( raw_text_query . getSearchQuery ( ) , language ) ) 
~~ results = [ ] 
for result in raw_results : 
~~~ raw_text_query . changeSearchQuery ( result ) 
results . append ( raw_text_query . getFullQuery ( ) ) 
~~ if request . form . get ( 'format' ) == 'x-suggestions' : 
~~~ return Response ( json . dumps ( [ raw_text_query . query , results ] ) , 
~~ return Response ( json . dumps ( results ) , 
~~ def preferences ( ) : 
if request . method == 'POST' : 
~~~ resp = make_response ( redirect ( urljoin ( settings [ 'server' ] [ 'base_url' ] , url_for ( 'index' ) ) ) ) 
~~~ request . preferences . parse_form ( request . form ) 
~~ except ValidationException : 
return resp 
~~ return request . preferences . save ( resp ) 
~~ image_proxy = request . preferences . get_value ( 'image_proxy' ) 
lang = request . preferences . get_value ( 'language' ) 
allowed_plugins = request . preferences . plugins . get_enabled ( ) 
stats = { } 
for c in categories : 
~~~ for e in categories [ c ] : 
~~~ stats [ e . name ] = { 'time' : None , 
'warn_timeout' : False , 
'warn_time' : False } 
if e . timeout > settings [ 'outgoing' ] [ 'request_timeout' ] : 
~~~ stats [ e . name ] [ 'warn_timeout' ] = True 
~~ stats [ e . name ] [ 'supports_selected_language' ] = _is_selected_language_supported ( e , request . preferences ) 
~~ ~~ for engine_stat in get_engines_stats ( ) [ 0 ] [ 1 ] : 
~~~ stats [ engine_stat . get ( 'name' ) ] [ 'time' ] = round ( engine_stat . get ( 'avg' ) , 3 ) 
if engine_stat . get ( 'avg' ) > settings [ 'outgoing' ] [ 'request_timeout' ] : 
~~~ stats [ engine_stat . get ( 'name' ) ] [ 'warn_time' ] = True 
~~ ~~ return render ( 'preferences.html' , 
locales = settings [ 'locales' ] , 
current_locale = get_locale ( ) , 
image_proxy = image_proxy , 
engines_by_category = categories , 
stats = stats , 
answerers = [ { 'info' : a . self_info ( ) , 'keywords' : a . keywords } for a in answerers ] , 
disabled_engines = disabled_engines , 
autocomplete_backends = autocomplete_backends , 
shortcuts = { y : x for x , y in engine_shortcuts . items ( ) } , 
themes = themes , 
plugins = plugins , 
doi_resolvers = settings [ 'doi_resolvers' ] , 
current_doi_resolver = get_doi_resolver ( request . args , request . preferences . get_value ( 'doi_resolver' ) ) , 
allowed_plugins = allowed_plugins , 
preferences_url_params = request . preferences . get_as_url_params ( ) , 
preferences = True ) 
~~ def request ( query , params ) : 
offset = ( params [ 'pageno' ] - 1 ) 
params [ 'url' ] = search_url . format ( offset = offset , query = quote ( query ) ) 
return params 
~~ def response ( resp ) : 
results = [ ] 
dom = html . fromstring ( resp . text ) 
~~~ number_of_results_string = re . sub ( '[^0-9]' , '' , dom . xpath ( 
\ ) [ 0 ] 
results . append ( { 'number_of_results' : int ( number_of_results_string ) } ) 
~~~ logger . debug ( "Couldn\ ) 
pass 
~~ for result in dom . xpath ( \ ) : 
link = result . xpath ( './/h2/a' ) [ 0 ] 
url = link . attrib . get ( 'href' ) 
title = result . xpath ( 'string(.//h2/a)' ) 
content = extract_text ( result . xpath ( './/p' ) ) 
results . append ( { 'url' : url , 
'title' : title , 
'content' : content } ) 
continue 
~~ ~~ return results 
~~ def get_themes ( templates_path ) : 
themes = os . listdir ( templates_path ) 
if '__common__' in themes : 
~~~ themes . remove ( '__common__' ) 
~~ return themes 
~~ def searx_bang ( full_query ) : 
if len ( full_query . getSearchQuery ( ) ) == 0 : 
~~~ return [ ] 
first_char = full_query . getSearchQuery ( ) [ 0 ] 
if first_char == '!' or first_char == '?' : 
~~~ if len ( full_query . getSearchQuery ( ) ) == 1 : 
~~~ results . append ( first_char + "images" ) 
results . append ( first_char + "wikipedia" ) 
results . append ( first_char + "osm" ) 
~~~ engine_query = full_query . getSearchQuery ( ) [ 1 : ] 
for categorie in categories : 
~~~ if categorie . startswith ( engine_query ) : 
~~~ results . append ( first_char + '{categorie}' . format ( categorie = categorie ) ) 
~~ ~~ for engine in engines : 
~~ ~~ for engine_shortcut in engine_shortcuts : 
~~~ if engine_shortcut . startswith ( engine_query ) : 
~~~ results . append ( first_char + '{engine_shortcut}' . format ( engine_shortcut = engine_shortcut ) ) 
~~ ~~ ~~ ~~ elif first_char == ':' : 
~~~ results . append ( ":en" ) 
results . append ( ":en_us" ) 
results . append ( ":english" ) 
results . append ( ":united_kingdom" ) 
for lc in language_codes : 
~~~ lang_id , lang_name , country , english_name = map ( unicode . lower , lc ) 
if lang_id . startswith ( engine_query ) : 
~~~ if len ( engine_query ) <= 2 : 
~~~ results . append ( u':{lang_id}' . format ( lang_id = lang_id . split ( '-' ) [ 0 ] ) ) 
~~~ results . append ( u':{lang_id}' . format ( lang_id = lang_id ) ) 
~~ ~~ if lang_name . startswith ( engine_query ) or english_name . startswith ( engine_query ) : 
~~~ results . append ( u':{lang_name}' . format ( lang_name = lang_name ) ) 
~~ ~~ ~~ ~~ result_set = set ( results ) 
for query_part in full_query . query_parts : 
~~~ if query_part in result_set : 
~~~ result_set . remove ( query_part ) 
~~ ~~ return list ( result_set ) 
json_resp = resp . text [ resp . text . find ( '\\n' ) + 1 : resp . text . rfind ( '\\n' ) - 2 ] 
~~~ conversion_rate = float ( json . loads ( json_resp ) [ 'conversion' ] [ 'converted-amount' ] ) 
~~~ return results 
resp . search_params [ 'amount' ] , 
resp . search_params [ 'from' ] , 
resp . search_params [ 'amount' ] * conversion_rate , 
resp . search_params [ 'to' ] , 
conversion_rate , 
resp . search_params [ 'from_name' ] , 
resp . search_params [ 'to_name' ] , 
url = 'https://duckduckgo.com/js/spice/currency/1/{0}/{1}' . format ( 
resp . search_params [ 'from' ] . upper ( ) , resp . search_params [ 'to' ] ) 
results . append ( { 'answer' : answer , 'url' : url } ) 
return results 
~~ def _call ( self , utterances_batch : list , utterances_ids : Optional [ list ] = None ) -> list : 
batch_size = len ( utterances_batch ) 
ids = utterances_ids or list ( range ( batch_size ) ) 
batch_history = [ self . history [ utt_id ] for utt_id in ids ] 
responses = [ ] 
filtered = self . skills_filter ( utterances_batch , batch_history ) 
for skill_i , ( filtered_utterances , skill ) in enumerate ( zip ( filtered , self . wrapped_skills ) ) : 
~~~ skill_i_utt_indexes = [ utt_index for utt_index , utt_filter in enumerate ( filtered_utterances ) if utt_filter ] 
if skill_i_utt_indexes : 
~~~ skill_i_utt_batch = [ utterances_batch [ i ] for i in skill_i_utt_indexes ] 
skill_i_utt_ids = [ ids [ i ] for i in skill_i_utt_indexes ] 
res = [ ( None , 0. ) ] * batch_size 
predicted , confidence = skill ( skill_i_utt_batch , skill_i_utt_ids ) 
for i , predicted , confidence in zip ( skill_i_utt_indexes , predicted , confidence ) : 
~~~ res [ i ] = ( predicted , confidence ) 
~~ responses . append ( res ) 
~~ ~~ responses = self . skills_processor ( utterances_batch , batch_history , * responses ) 
return responses 
~~ def expand_tile ( units , axis ) : 
assert axis in ( 1 , 2 ) 
n_time_steps = K . int_shape ( units ) [ 1 ] 
repetitions = [ 1 , 1 , 1 , 1 ] 
repetitions [ axis ] = n_time_steps 
if axis == 1 : 
~~~ expanded = Reshape ( target_shape = ( ( 1 , ) + K . int_shape ( units ) [ 1 : ] ) ) ( units ) 
~~~ expanded = Reshape ( target_shape = ( K . int_shape ( units ) [ 1 : 2 ] + ( 1 , ) + K . int_shape ( units ) [ 2 : ] ) ) ( units ) 
~~ return K . tile ( expanded , repetitions ) 
~~ def additive_self_attention ( units , n_hidden = None , n_output_features = None , activation = None ) : 
n_input_features = K . int_shape ( units ) [ 2 ] 
if n_hidden is None : 
~~~ n_hidden = n_input_features 
~~ if n_output_features is None : 
~~~ n_output_features = n_input_features 
~~ exp1 = Lambda ( lambda x : expand_tile ( x , axis = 1 ) ) ( units ) 
exp2 = Lambda ( lambda x : expand_tile ( x , axis = 2 ) ) ( units ) 
units_pairs = Concatenate ( axis = 3 ) ( [ exp1 , exp2 ] ) 
query = Dense ( n_hidden , activation = "tanh" ) ( units_pairs ) 
attention = Dense ( 1 , activation = lambda x : softmax ( x , axis = 2 ) ) ( query ) 
attended_units = Lambda ( lambda x : K . sum ( attention * x , axis = 2 ) ) ( exp1 ) 
output = Dense ( n_output_features , activation = activation ) ( attended_units ) 
return output 
~~ def multiplicative_self_attention ( units , n_hidden = None , n_output_features = None , activation = None ) : 
queries = Dense ( n_hidden ) ( exp1 ) 
keys = Dense ( n_hidden ) ( exp2 ) 
scores = Lambda ( lambda x : K . sum ( queries * x , axis = 3 , keepdims = True ) ) ( keys ) 
attention = Lambda ( lambda x : softmax ( x , axis = 2 ) ) ( scores ) 
mult = Multiply ( ) ( [ attention , exp1 ] ) 
attended_units = Lambda ( lambda x : K . sum ( x , axis = 2 ) ) ( mult ) 
~~ def precompute_future_symbols ( trie , n , allow_spaces = False ) : 
if n == 0 : 
~~ if trie . is_terminated and trie . precompute_symbols : 
~~ for index , final in enumerate ( trie . final ) : 
~~~ trie . data [ index ] = [ set ( ) for i in range ( n ) ] 
~~ for index , ( node_data , final ) in enumerate ( zip ( trie . data , trie . final ) ) : 
~~~ node_data [ 0 ] = set ( trie . _get_letters ( index ) ) 
if allow_spaces and final : 
~~ ~~ for d in range ( 1 , n ) : 
~~~ for index , ( node_data , final ) in enumerate ( zip ( trie . data , trie . final ) ) : 
~~~ children = set ( trie . _get_children ( index ) ) 
for child in children : 
~~~ node_data [ d ] |= trie . data [ child ] [ d - 1 ] 
~~ if allow_spaces and final : 
~~~ node_data [ d ] |= trie . data [ trie . root ] [ d - 1 ] 
~~ ~~ ~~ trie . terminated = True 
~~ def save ( self , outfile ) : 
with open ( outfile , "w" , encoding = "utf8" ) as fout : 
~~~ attr_values = [ getattr ( self , attr ) for attr in Trie . ATTRS ] 
attr_values . append ( any ( x is not None for x in self . data ) ) 
fout . write ( "{}\\n{}\\t{}\\n" . format ( 
self . nodes_number , self . root ) ) 
for index , label in enumerate ( self . final ) : 
~~~ letters = self . _get_letters ( index , return_indexes = True ) 
children = self . _get_children ( index ) 
fout . write ( "{}\\t{}\\n" . format ( 
for elem in zip ( letters , children ) ) ) ) 
~~ if self . precompute_symbols is not None : 
~~~ for elem in self . data : 
~~~ fout . write ( ":" . join ( "," . join ( 
map ( str , symbols ) ) for symbols in elem ) + "\\n" ) 
~~ ~~ ~~ return 
~~ def make_cashed ( self ) : 
self . _descendance_cash = [ dict ( ) for _ in self . graph ] 
self . descend = self . _descend_cashed 
~~ def add ( self , s ) : 
if self . is_terminated : 
~~ if s == "" : 
~~~ self . _set_final ( self . root ) 
~~ curr = self . root 
for i , a in enumerate ( s ) : 
~~~ code = self . alphabet_codes [ a ] 
next = self . graph [ curr ] [ code ] 
if next == Trie . NO_NODE : 
~~~ curr = self . _add_descendant ( curr , s [ i : ] ) 
~~~ curr = next 
~~ ~~ self . _set_final ( curr ) 
return self 
~~ def words ( self ) : 
branch , word , indexes = [ self . root ] , [ ] , [ 0 ] 
letters_with_children = [ self . _get_children_and_letters ( self . root ) ] 
while len ( branch ) > 0 : 
~~~ if self . is_final ( branch [ - 1 ] ) : 
~~~ yield "" . join ( word ) 
~~ while indexes [ - 1 ] == len ( letters_with_children [ - 1 ] ) : 
~~~ indexes . pop ( ) 
letters_with_children . pop ( ) 
branch . pop ( ) 
if len ( indexes ) == 0 : 
~~~ raise StopIteration ( ) 
~~ word . pop ( ) 
~~ next_letter , next_child = letters_with_children [ - 1 ] [ indexes [ - 1 ] ] 
indexes [ - 1 ] += 1 
indexes . append ( 0 ) 
word . append ( next_letter ) 
branch . append ( next_child ) 
letters_with_children . append ( self . _get_children_and_letters ( branch [ - 1 ] ) ) 
~~ ~~ def find_partitions ( self , s , max_count = 1 ) : 
curr_agenda = [ ( self . root , [ ] , 0 ) ] 
~~~ next_agenda = [ ] 
for curr , borders , cost in curr_agenda : 
~~~ if cost >= max_count : 
~~~ continue 
~~ child = self . graph [ curr ] [ self . alphabet_codes [ a ] ] 
if child == Trie . NO_NODE : 
~~ next_agenda . append ( ( child , borders , cost ) ) 
if self . is_final ( child ) : 
~~~ next_agenda . append ( ( self . root , borders + [ i + 1 ] , cost + 1 ) ) 
~~ ~~ curr_agenda = next_agenda 
~~ answer = [ ] 
~~~ if curr == self . root : 
~~~ borders = [ 0 ] + borders 
answer . append ( [ s [ left : borders [ i + 1 ] ] for i , left in enumerate ( borders [ : - 1 ] ) ] ) 
~~ ~~ return answer 
~~ def _add_empty_child ( self , parent , code , final = False ) : 
self . graph [ parent ] [ code ] = self . nodes_number 
self . graph . append ( self . _make_default_node ( ) ) 
self . data . append ( None ) 
self . final . append ( final ) 
self . nodes_number += 1 
return ( self . nodes_number - 1 ) 
~~ def _descend_simple ( self , curr , s ) : 
for a in s : 
~~~ curr = self . graph [ curr ] [ self . alphabet_codes [ a ] ] 
if curr == Trie . NO_NODE : 
~~ ~~ return curr 
~~ def _descend_cashed ( self , curr , s ) : 
if s == "" : 
~~~ return curr 
~~ curr_cash = self . _descendance_cash [ curr ] 
answer = curr_cash . get ( s , None ) 
if answer is not None : 
~~~ return answer 
~~ res = curr 
~~~ res = self . graph [ res ] [ self . alphabet_codes [ a ] ] 
if res == Trie . NO_NODE : 
~~ ~~ curr_cash [ s ] = res 
return res 
~~ def _get_letters ( self , index , return_indexes = False ) : 
if self . dict_storage : 
~~~ answer = list ( self . graph [ index ] . keys ( ) ) 
~~~ answer = [ i for i , elem in enumerate ( self . graph [ index ] ) 
if elem != Trie . NO_NODE ] 
~~ if not return_indexes : 
~~ return answer 
~~ def _get_children ( self , index ) : 
~~~ return list ( self . graph [ index ] . values ( ) ) 
~~~ return [ elem for elem in self . graph [ index ] if elem != Trie . NO_NODE ] 
~~ ~~ def generate_postorder ( self , trie ) : 
order , stack = [ ] , [ ] 
stack . append ( trie . root ) 
colors = [ 'white' ] * len ( trie ) 
while len ( stack ) > 0 : 
~~~ index = stack [ - 1 ] 
color = colors [ index ] 
~~~ colors [ index ] = 'grey' 
for child in trie . _get_children ( index ) : 
~~~ if child != Trie . NO_NODE and colors [ child ] == 'white' : 
~~~ stack . append ( child ) 
~~~ if color == 'grey' : 
~~~ colors [ index ] = 'black' 
order . append ( index ) 
~~ stack = stack [ : - 1 ] 
~~ ~~ return order 
~~ def run_population ( population , evolution , gpus ) : 
population_size = len ( population ) 
for k in range ( population_size // len ( gpus ) + 1 ) : 
~~~ procs = [ ] 
for j in range ( len ( gpus ) ) : 
~~~ i = k * len ( gpus ) + j 
if i < population_size : 
~~~ save_path = expand_path ( 
evolution . get_value_from_config ( parse_config ( population [ i ] ) , 
evolution . path_to_models_save_path ) ) 
save_path . mkdir ( parents = True , exist_ok = True ) 
f_name = save_path / "config.json" 
save_json ( population [ i ] , f_name ) 
with save_path . joinpath ( 'out.txt' ) . open ( 'w' , encoding = 'utf8' ) as outlog , save_path . joinpath ( 'err.txt' ) . open ( 'w' , encoding = 'utf8' ) as errlog : 
~~~ env = dict ( os . environ ) 
if len ( gpus ) > 1 or gpus [ 0 ] != - 1 : 
~~~ env [ 'CUDA_VISIBLE_DEVICES' ] = str ( gpus [ j ] ) 
shell = True , stdout = outlog , stderr = errlog , env = env ) ) 
~~ ~~ ~~ for j , proc in enumerate ( procs ) : 
if proc . wait ( ) != 0 : 
with save_path . joinpath ( 'err.txt' ) . open ( encoding = 'utf8' ) as errlog : 
errlog . read ( ) ) 
~~ ~~ ~~ ~~ return None 
~~ def dot_attention ( inputs , memory , mask , att_size , keep_prob = 1.0 , scope = "dot_attention" ) : 
with tf . variable_scope ( scope ) : 
~~~ BS , IL , IH = tf . unstack ( tf . shape ( inputs ) ) 
BS , ML , MH = tf . unstack ( tf . shape ( memory ) ) 
d_inputs = tf . nn . dropout ( inputs , keep_prob = keep_prob , noise_shape = [ BS , 1 , IH ] ) 
d_memory = tf . nn . dropout ( memory , keep_prob = keep_prob , noise_shape = [ BS , 1 , MH ] ) 
with tf . variable_scope ( "attention" ) : 
~~~ inputs_att = tf . layers . dense ( d_inputs , att_size , use_bias = False , activation = tf . nn . relu ) 
memory_att = tf . layers . dense ( d_memory , att_size , use_bias = False , activation = tf . nn . relu ) 
logits = tf . matmul ( inputs_att , tf . transpose ( memory_att , [ 0 , 2 , 1 ] ) ) / ( att_size ** 0.5 ) 
mask = tf . tile ( tf . expand_dims ( mask , axis = 1 ) , [ 1 , IL , 1 ] ) 
att_weights = tf . nn . softmax ( softmax_mask ( logits , mask ) ) 
outputs = tf . matmul ( att_weights , memory ) 
res = tf . concat ( [ inputs , outputs ] , axis = 2 ) 
~~ with tf . variable_scope ( "gate" ) : 
~~~ dim = res . get_shape ( ) . as_list ( ) [ - 1 ] 
d_res = tf . nn . dropout ( res , keep_prob = keep_prob , noise_shape = [ BS , 1 , IH + MH ] ) 
gate = tf . layers . dense ( d_res , dim , use_bias = False , activation = tf . nn . sigmoid ) 
return res * gate 
~~ ~~ ~~ def simple_attention ( memory , att_size , mask , keep_prob = 1.0 , scope = "simple_attention" ) : 
~~~ BS , ML , MH = tf . unstack ( tf . shape ( memory ) ) 
memory_do = tf . nn . dropout ( memory , keep_prob = keep_prob , noise_shape = [ BS , 1 , MH ] ) 
logits = tf . layers . dense ( tf . layers . dense ( memory_do , att_size , activation = tf . nn . tanh ) , 1 , use_bias = False ) 
logits = softmax_mask ( tf . squeeze ( logits , [ 2 ] ) , mask ) 
att_weights = tf . expand_dims ( tf . nn . softmax ( logits ) , axis = 2 ) 
res = tf . reduce_sum ( att_weights * memory , axis = 1 ) 
~~ ~~ def attention ( inputs , state , att_size , mask , scope = "attention" ) : 
~~~ u = tf . concat ( [ tf . tile ( tf . expand_dims ( state , axis = 1 ) , [ 1 , tf . shape ( inputs ) [ 1 ] , 1 ] ) , inputs ] , axis = 2 ) 
logits = tf . layers . dense ( tf . layers . dense ( u , att_size , activation = tf . nn . tanh ) , 1 , use_bias = False ) 
res = tf . reduce_sum ( att_weights * inputs , axis = 1 ) 
return res , logits 
~~ ~~ def compute_bleu ( reference_corpus , translation_corpus , max_order = 4 , 
smooth = False ) : 
matches_by_order = [ 0 ] * max_order 
possible_matches_by_order = [ 0 ] * max_order 
reference_length = 0 
translation_length = 0 
for ( references , translation ) in zip ( reference_corpus , 
translation_corpus ) : 
~~~ reference_length += min ( len ( r ) for r in references ) 
translation_length += len ( translation ) 
merged_ref_ngram_counts = collections . Counter ( ) 
for reference in references : 
~~~ merged_ref_ngram_counts |= _get_ngrams ( reference , max_order ) 
~~ translation_ngram_counts = _get_ngrams ( translation , max_order ) 
overlap = translation_ngram_counts & merged_ref_ngram_counts 
for ngram in overlap : 
~~~ matches_by_order [ len ( ngram ) - 1 ] += overlap [ ngram ] 
~~ for order in range ( 1 , max_order + 1 ) : 
~~~ possible_matches = len ( translation ) - order + 1 
if possible_matches > 0 : 
~~~ possible_matches_by_order [ order - 1 ] += possible_matches 
~~ ~~ ~~ precisions = [ 0 ] * max_order 
for i in range ( 0 , max_order ) : 
~~~ if smooth : 
~~~ precisions [ i ] = ( ( matches_by_order [ i ] + 1. ) / 
( possible_matches_by_order [ i ] + 1. ) ) 
~~~ if possible_matches_by_order [ i ] > 0 : 
~~~ precisions [ i ] = ( float ( matches_by_order [ i ] ) / 
possible_matches_by_order [ i ] ) 
~~~ precisions [ i ] = 0.0 
~~ ~~ ~~ if min ( precisions ) > 0 : 
~~~ p_log_sum = sum ( ( 1. / max_order ) * math . log ( p ) for p in precisions ) 
geo_mean = math . exp ( p_log_sum ) 
~~~ geo_mean = 0 
~~ ratio = float ( translation_length ) / reference_length 
if ratio > 1.0 : 
~~~ bp = 1. 
~~~ bp = math . exp ( 1 - 1. / ratio ) 
~~ bleu = geo_mean * bp 
return ( bleu , precisions , bp , ratio , translation_length , reference_length ) 
~~ def _get_log_file ( self ) : 
log_dir : Path = Path ( self . config [ 'log_path' ] ) . expanduser ( ) . resolve ( ) / self . agent_name 
log_dir . mkdir ( parents = True , exist_ok = True ) 
log_file_path = Path ( log_dir , f'{self._get_timestamp_utc_str()}_{self.agent_name}.log' ) 
log_file = open ( log_file_path , 'a' , buffering = 1 , encoding = 'utf8' ) 
return log_file 
~~ def _log ( self , utterance : Any , direction : str , dialog_id : Optional [ Hashable ] = None ) : 
if isinstance ( utterance , str ) : 
~~ elif isinstance ( utterance , RichMessage ) : 
~~~ utterance = utterance . json ( ) 
~~ elif isinstance ( utterance , ( list , dict ) ) : 
~~~ utterance = jsonify_data ( utterance ) 
~~~ utterance = str ( utterance ) 
~~ dialog_id = str ( dialog_id ) if not isinstance ( dialog_id , str ) else dialog_id 
if self . log_file . tell ( ) >= self . log_max_size * 1024 : 
~~~ self . log_file . close ( ) 
self . log_file = self . _get_log_file ( ) 
~~~ log_msg = { } 
log_msg [ 'timestamp' ] = self . _get_timestamp_utc_str ( ) 
log_msg [ 'dialog_id' ] = dialog_id 
log_msg [ 'direction' ] = direction 
log_msg [ 'message' ] = utterance 
log_str = json . dumps ( log_msg , ensure_ascii = self . config [ 'ensure_ascii' ] ) 
self . log_file . write ( f'{log_str}\\n' ) 
~~ except IOError : 
~~ ~~ ~~ def log_in ( self , utterance : Any , dialog_id : Optional [ Hashable ] = None ) -> None : 
if self . enabled : 
~~~ self . _log ( utterance , 'in' , dialog_id ) 
~~ ~~ def summary_gradient_updates ( grads , opt , lr ) : 
vars_grads = { } 
for v in tf . trainable_variables ( ) : 
~~~ vars_grads [ v . name ] = [ v , None , None ] 
~~ for g , v in grads : 
~~~ vars_grads [ v . name ] [ 1 ] = g 
vars_grads [ v . name ] [ 2 ] = opt . get_slot ( v , 'accumulator' ) 
~~ ret = [ ] 
for vname , ( v , g , a ) in vars_grads . items ( ) : 
~~~ if g is None : 
~~ if isinstance ( g , tf . IndexedSlices ) : 
~~~ updates = lr * g . values 
if a is not None : 
~~~ updates /= tf . sqrt ( tf . gather ( a , g . indices ) ) 
~~~ updates = lr * g 
~~~ updates /= tf . sqrt ( a ) 
~~ ~~ values_norm = tf . sqrt ( tf . reduce_sum ( v * v ) ) + 1.0e-7 
updates_norm = tf . sqrt ( tf . reduce_sum ( updates * updates ) ) 
ret . append ( tf . summary . scalar ( 'UPDATE/' + vname . replace ( ":" , "_" ) , updates_norm / values_norm ) ) 
~~ def _deduplicate_indexed_slices ( values , indices ) : 
unique_indices , new_index_positions = tf . unique ( indices ) 
summed_values = tf . unsorted_segment_sum ( values , 
new_index_positions , 
tf . shape ( unique_indices ) [ 0 ] ) 
return ( summed_values , unique_indices ) 
~~ def dump_weights ( tf_save_dir , outfile , options ) : 
def _get_outname ( tf_name ) : 
~~~ outname = re . sub ( ':0$' , '' , tf_name ) 
outname = outname . lstrip ( 'lm/' ) 
outname = re . sub ( '/rnn/' , '/RNN/' , outname ) 
outname = re . sub ( '/multi_rnn_cell/' , '/MultiRNNCell/' , outname ) 
outname = re . sub ( '/cell_' , '/Cell' , outname ) 
outname = re . sub ( '/lstm_cell/' , '/LSTMCell/' , outname ) 
if '/RNN/' in outname : 
~~~ if 'projection' in outname : 
~~~ outname = re . sub ( 'projection/kernel' , 'W_P_0' , outname ) 
~~~ outname = re . sub ( '/kernel' , '/W_0' , outname ) 
outname = re . sub ( '/bias' , '/B' , outname ) 
~~ ~~ return outname 
~~ ckpt_file = tf . train . latest_checkpoint ( tf_save_dir ) 
config = tf . ConfigProto ( allow_soft_placement = True ) 
with tf . Graph ( ) . as_default ( ) : 
~~~ with tf . Session ( config = config ) as sess : 
~~~ with tf . variable_scope ( 'lm' ) : 
loader = tf . train . Saver ( ) 
loader . restore ( sess , ckpt_file ) 
~~ with h5py . File ( outfile , 'w' ) as fout : 
~~~ for v in tf . trainable_variables ( ) : 
~~~ if v . name . find ( 'softmax' ) >= 0 : 
~~ outname = _get_outname ( v . name ) 
shape = v . get_shape ( ) . as_list ( ) 
dset = fout . create_dataset ( outname , shape , dtype = 'float32' ) 
values = sess . run ( [ v ] ) [ 0 ] 
dset [ ... ] = values 
~~ ~~ ~~ ~~ ~~ def read_data_by_config ( config : dict ) : 
dataset_config = config . get ( 'dataset' , None ) 
if dataset_config : 
~~~ config . pop ( 'dataset' ) 
ds_type = dataset_config [ 'type' ] 
if ds_type == 'classification' : 
~~~ reader = { 'class_name' : 'basic_classification_reader' } 
iterator = { 'class_name' : 'basic_classification_iterator' } 
config [ 'dataset_reader' ] = { ** dataset_config , ** reader } 
config [ 'dataset_iterator' ] = { ** dataset_config , ** iterator } 
~~ ~~ try : 
~~~ reader_config = dict ( config [ 'dataset_reader' ] ) 
~~ reader = get_model ( reader_config . pop ( 'class_name' ) ) ( ) 
data_path = reader_config . pop ( 'data_path' , '' ) 
if isinstance ( data_path , list ) : 
~~~ data_path = [ expand_path ( x ) for x in data_path ] 
~~~ data_path = expand_path ( data_path ) 
~~ return reader . read ( data_path , ** reader_config ) 
~~ def get_iterator_from_config ( config : dict , data : dict ) : 
iterator_config = config [ 'dataset_iterator' ] 
iterator : Union [ DataLearningIterator , DataFittingIterator ] = from_params ( iterator_config , 
data = data ) 
return iterator 
~~ def train_evaluate_model_from_config ( config : Union [ str , Path , dict ] , 
iterator : Union [ DataLearningIterator , DataFittingIterator ] = None , * , 
to_train : bool = True , 
evaluation_targets : Optional [ Iterable [ str ] ] = None , 
to_validate : Optional [ bool ] = None , 
download : bool = False , 
start_epoch_num : Optional [ int ] = None , 
recursive : bool = False ) -> Dict [ str , Dict [ str , float ] ] : 
config = parse_config ( config ) 
if download : 
~~~ deep_download ( config ) 
~~ if to_train and recursive : 
~~~ for subconfig in get_all_elems_from_json ( config [ 'chainer' ] , 'config_path' ) : 
~~~ log . info ( f\ ) 
train_evaluate_model_from_config ( subconfig , download = False , recursive = True ) 
~~ ~~ import_packages ( config . get ( 'metadata' , { } ) . get ( 'imports' , [ ] ) ) 
if iterator is None : 
~~~ data = read_data_by_config ( config ) 
~~ except ConfigError as e : 
~~~ to_train = False 
~~~ iterator = get_iterator_from_config ( config , data ) 
~~ ~~ if 'train' not in config : 
~~ train_config = config . get ( 'train' ) 
if start_epoch_num is not None : 
~~~ train_config [ 'start_epoch_num' ] = start_epoch_num 
~~ if 'evaluation_targets' not in train_config and ( 'validate_best' in train_config 
or 'test_best' in train_config ) : 
~~~ log . warning ( \ 
\ ) 
train_config [ 'evaluation_targets' ] = [ ] 
if train_config . pop ( 'validate_best' , True ) : 
~~~ train_config [ 'evaluation_targets' ] . append ( 'valid' ) 
~~ if train_config . pop ( 'test_best' , True ) : 
~~~ train_config [ 'evaluation_targets' ] . append ( 'test' ) 
~~ ~~ trainer_class = get_model ( train_config . pop ( 'class_name' , 'nn_trainer' ) ) 
trainer = trainer_class ( config [ 'chainer' ] , ** train_config ) 
if to_train : 
~~~ trainer . train ( iterator ) 
~~ res = { } 
if iterator is not None : 
~~~ if to_validate is not None : 
~~~ if evaluation_targets is None : 
evaluation_targets = [ 'test' ] 
if to_validate : 
~~~ evaluation_targets . append ( 'valid' ) 
~~~ log . warn ( \ 
~~ ~~ res = trainer . evaluate ( iterator , evaluation_targets , print_reports = True ) 
trainer . get_chainer ( ) . destroy ( ) 
~~ res = { k : v [ 'metrics' ] for k , v in res . items ( ) } 
~~ def interact_alice ( agent : Agent ) : 
data = request . get_json ( ) 
text = data [ 'request' ] . get ( 'command' , '' ) . strip ( ) 
payload = data [ 'request' ] . get ( 'payload' ) 
session_id = data [ 'session' ] [ 'session_id' ] 
user_id = data [ 'session' ] [ 'user_id' ] 
message_id = data [ 'session' ] [ 'message_id' ] 
dialog_id = DialogID ( user_id , session_id ) 
response = { 
'response' : { 
'end_session' : True , 
'text' : '' 
} , 
"session" : { 
'session_id' : session_id , 
'message_id' : message_id , 
'user_id' : user_id 
'version' : '1.0' 
agent_response : Union [ str , RichMessage ] = agent ( [ payload or text ] , [ dialog_id ] ) [ 0 ] 
if isinstance ( agent_response , RichMessage ) : 
~~~ response [ 'response' ] [ 'text' ] = '\\n' . join ( [ j [ 'content' ] 
for j in agent_response . json ( ) 
if j [ 'type' ] == 'plain_text' ] ) 
~~~ response [ 'response' ] [ 'text' ] = str ( agent_response ) 
~~ return jsonify ( response ) , 200 
~~ def labels2onehot ( labels : [ List [ str ] , List [ List [ str ] ] , np . ndarray ] , classes : [ list , np . ndarray ] ) -> np . ndarray : 
n_classes = len ( classes ) 
y = [ ] 
for sample in labels : 
~~~ curr = np . zeros ( n_classes ) 
if isinstance ( sample , list ) : 
~~~ for intent in sample : 
~~~ if intent not in classes : 
~~~ curr [ np . where ( np . array ( classes ) == intent ) [ 0 ] ] = 1 
~~~ curr [ np . where ( np . array ( classes ) == sample ) [ 0 ] ] = 1 
~~ y . append ( curr ) 
~~ y = np . asarray ( y ) 
return y 
~~ def proba2labels ( proba : [ list , np . ndarray ] , confident_threshold : float , classes : [ list , np . ndarray ] ) -> List [ List ] : 
for sample in proba : 
~~~ to_add = np . where ( sample > confident_threshold ) [ 0 ] 
if len ( to_add ) > 0 : 
~~~ y . append ( np . array ( classes ) [ to_add ] . tolist ( ) ) 
~~~ y . append ( np . array ( [ np . array ( classes ) [ np . argmax ( sample ) ] ] ) . tolist ( ) ) 
~~ ~~ return y 
~~ def proba2onehot ( proba : [ list , np . ndarray ] , confident_threshold : float , classes : [ list , np . ndarray ] ) -> np . ndarray : 
return labels2onehot ( proba2labels ( proba , confident_threshold , classes ) , classes ) 
~~ def _config_session ( ) : 
config = tf . ConfigProto ( ) 
config . gpu_options . allow_growth = True 
config . gpu_options . visible_device_list = '0' 
return tf . Session ( config = config ) 
~~ def process_event ( self , event_name : str , data : dict ) -> None : 
if event_name == "after_epoch" : 
~~~ self . epochs_done = data [ "epochs_done" ] 
self . batches_seen = data [ "batches_seen" ] 
self . train_examples_seen = data [ "train_examples_seen" ] 
~~ return 
~~ def load ( self ) -> None : 
if self . load_path . exists ( ) : 
~~~ path = str ( self . load_path . resolve ( ) ) 
self . _net . load ( path ) 
~~ ~~ def save ( self ) -> None : 
path = str ( self . save_path . absolute ( ) ) 
self . _net . save ( path ) 
~~ def train_on_batch ( self , * args ) -> None : 
* data , labels = args 
self . _net . train_on_batch ( data , labels ) 
~~ def get_momentum_variable ( self ) : 
optimizer = self . get_optimizer ( ) 
if hasattr ( optimizer , 'rho' ) : 
~~~ return optimizer . rho 
~~ elif hasattr ( optimizer , 'beta_1' ) : 
~~~ return optimizer . beta_1 
~~ return None 
~~ def _update_graph_variables ( self , learning_rate : float = None , momentum : float = None ) : 
if learning_rate is not None : 
~~~ K . set_value ( self . get_learning_rate_variable ( ) , learning_rate ) 
~~ if momentum is not None : 
~~~ K . set_value ( self . get_momentum_variable ( ) , momentum ) 
~~ ~~ def process_event ( self , event_name : str , data : dict ) : 
if ( isinstance ( self . opt . get ( "learning_rate" , None ) , float ) and 
isinstance ( self . opt . get ( "learning_rate_decay" , None ) , float ) ) : 
~~~ if event_name == 'after_train_log' : 
~~~ if ( self . get_learning_rate_variable ( ) is not None ) and ( 'learning_rate' not in data ) : 
~~~ data [ 'learning_rate' ] = float ( K . get_value ( self . get_learning_rate_variable ( ) ) ) 
~~ if ( self . get_momentum_variable ( ) is not None ) and ( 'momentum' not in data ) : 
~~~ data [ 'momentum' ] = float ( K . get_value ( self . get_momentum_variable ( ) ) ) 
~~~ super ( ) . process_event ( event_name , data ) 
~~ ~~ ~~ def round_f1 ( y_true , y_predicted ) : 
~~~ predictions = [ np . round ( x ) for x in y_predicted ] 
~~ except TypeError : 
~~~ predictions = y_predicted 
~~ return f1_score ( y_true , predictions ) 
~~ def round_f1_macro ( y_true , y_predicted ) : 
~~ return f1_score ( np . array ( y_true ) , np . array ( predictions ) , average = "macro" ) 
~~ def process_word ( word : str , to_lower : bool = False , 
append_case : Optional [ str ] = None ) -> Tuple [ str ] : 
if all ( x . isupper ( ) for x in word ) and len ( word ) > 1 : 
~~~ uppercase = "<ALL_UPPER>" 
~~ elif word [ 0 ] . isupper ( ) : 
~~~ uppercase = "<FIRST_UPPER>" 
~~~ uppercase = None 
~~ if to_lower : 
~~~ word = word . lower ( ) 
~~ if word . isdigit ( ) : 
~~~ answer = [ "<DIGIT>" ] 
~~ elif word . startswith ( "http://" ) or word . startswith ( "www." ) : 
~~~ answer = [ "<HTTP>" ] 
~~~ answer = list ( word ) 
~~ if to_lower and uppercase is not None : 
~~~ if append_case == "first" : 
~~~ answer = [ uppercase ] + answer 
~~ elif append_case == "last" : 
~~~ answer = answer + [ uppercase ] 
~~ ~~ return tuple ( answer ) 
~~ def stacked_cnn ( units : tf . Tensor , 
n_hidden_list : List , 
filter_width = 3 , 
use_batch_norm = False , 
use_dilation = False , 
training_ph = None , 
add_l2_losses = False ) : 
l2_reg = tf . nn . l2_loss if add_l2_losses else None 
for n_layer , n_hidden in enumerate ( n_hidden_list ) : 
~~~ if use_dilation : 
~~~ dilation_rate = 2 ** n_layer 
~~~ dilation_rate = 1 
~~ units = tf . layers . conv1d ( units , 
n_hidden , 
filter_width , 
padding = 'same' , 
dilation_rate = dilation_rate , 
kernel_initializer = INITIALIZER ( ) , 
kernel_regularizer = l2_reg ) 
if use_batch_norm : 
~~~ assert training_ph is not None 
units = tf . layers . batch_normalization ( units , training = training_ph ) 
~~ units = tf . nn . relu ( units ) 
~~ return units 
~~ def dense_convolutional_network ( units : tf . Tensor , 
training_ph = None ) : 
units_list = [ units ] 
for n_layer , n_filters in enumerate ( n_hidden_list ) : 
~~~ total_units = tf . concat ( units_list , axis = - 1 ) 
if use_dilation : 
~~ units = tf . layers . conv1d ( total_units , 
n_filters , 
kernel_initializer = INITIALIZER ( ) ) 
~~~ units = tf . layers . batch_normalization ( units , training = training_ph ) 
units_list . append ( units ) 
~~ def bi_rnn ( units : tf . Tensor , 
n_hidden : List , 
cell_type = 'gru' , 
seq_lengths = None , 
trainable_initial_states = False , 
use_peepholes = False , 
name = 'Bi-' ) : 
with tf . variable_scope ( name + '_' + cell_type . upper ( ) ) : 
~~~ if cell_type == 'gru' : 
~~~ forward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden , kernel_initializer = INITIALIZER ( ) ) 
backward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden , kernel_initializer = INITIALIZER ( ) ) 
if trainable_initial_states : 
~~~ initial_state_fw = tf . tile ( tf . get_variable ( 'init_fw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) 
initial_state_bw = tf . tile ( tf . get_variable ( 'init_bw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) 
~~~ initial_state_fw = initial_state_bw = None 
~~ ~~ elif cell_type == 'lstm' : 
~~~ forward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes , initializer = INITIALIZER ( ) ) 
backward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes , initializer = INITIALIZER ( ) ) 
~~~ initial_state_fw = tf . nn . rnn_cell . LSTMStateTuple ( 
tf . tile ( tf . get_variable ( 'init_fw_c' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) , 
tf . tile ( tf . get_variable ( 'init_fw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) ) 
initial_state_bw = tf . nn . rnn_cell . LSTMStateTuple ( 
tf . tile ( tf . get_variable ( 'init_bw_c' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) , 
tf . tile ( tf . get_variable ( 'init_bw_h' , [ 1 , n_hidden ] ) , ( tf . shape ( units ) [ 0 ] , 1 ) ) ) 
~~~ raise RuntimeError ( \ ) 
~~ ( rnn_output_fw , rnn_output_bw ) , ( fw , bw ) = tf . nn . bidirectional_dynamic_rnn ( forward_cell , 
backward_cell , 
units , 
dtype = tf . float32 , 
sequence_length = seq_lengths , 
initial_state_fw = initial_state_fw , 
initial_state_bw = initial_state_bw ) 
~~ kernels = [ var for var in forward_cell . trainable_variables + 
backward_cell . trainable_variables if 'kernel' in var . name ] 
for kernel in kernels : 
~~~ tf . add_to_collection ( tf . GraphKeys . REGULARIZATION_LOSSES , tf . nn . l2_loss ( kernel ) ) 
~~ return ( rnn_output_fw , rnn_output_bw ) , ( fw , bw ) 
~~ def stacked_bi_rnn ( units : tf . Tensor , 
name = 'RNN_layer' ) : 
for n , n_hidden in enumerate ( n_hidden_list ) : 
~~~ with tf . variable_scope ( name + '_' + str ( n ) ) : 
~~~ forward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden ) 
backward_cell = tf . nn . rnn_cell . GRUCell ( n_hidden ) 
~~ elif cell_type == 'lstm' : 
~~~ forward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes ) 
backward_cell = tf . nn . rnn_cell . LSTMCell ( n_hidden , use_peepholes = use_peepholes ) 
sequence_length = seq_lengths ) 
units = tf . concat ( [ rnn_output_fw , rnn_output_bw ] , axis = 2 ) 
if cell_type == 'gru' : 
~~~ last_units = tf . concat ( [ fw , bw ] , axis = 1 ) 
~~~ ( c_fw , h_fw ) , ( c_bw , h_bw ) = fw , bw 
c = tf . concat ( [ c_fw , c_bw ] , axis = 1 ) 
h = tf . concat ( [ h_fw , h_bw ] , axis = 1 ) 
last_units = ( h , c ) 
~~ ~~ ~~ return units , last_units 
~~ def u_shape ( units : tf . Tensor , 
filter_width = 7 , 
units_for_skip_conn = [ ] 
conv_net_params = { 'filter_width' : filter_width , 
'use_batch_norm' : use_batch_norm , 
'training_ph' : training_ph } 
for n_hidden in n_hidden_list : 
~~~ units = stacked_cnn ( units , [ n_hidden ] , ** conv_net_params ) 
units_for_skip_conn . append ( units ) 
units = tf . layers . max_pooling1d ( units , pool_size = 2 , strides = 2 , padding = 'same' ) 
~~ units = stacked_cnn ( units , [ n_hidden ] , ** conv_net_params ) 
for down_step , n_hidden in enumerate ( n_hidden_list [ : : - 1 ] ) : 
~~~ units = tf . expand_dims ( units , axis = 2 ) 
units = tf . layers . conv2d_transpose ( units , n_hidden , filter_width , strides = ( 2 , 1 ) , padding = 'same' ) 
units = tf . squeeze ( units , axis = 2 ) 
skip_units = units_for_skip_conn [ - ( down_step + 1 ) ] 
if skip_units . get_shape ( ) . as_list ( ) [ - 1 ] != n_hidden : 
~~~ skip_units = tf . layers . dense ( skip_units , n_hidden ) 
~~ units = skip_units + units 
units = stacked_cnn ( units , [ n_hidden ] , ** conv_net_params ) 
~~ def stacked_highway_cnn ( units : tf . Tensor , 
~~~ input_units = units 
if input_units . get_shape ( ) . as_list ( ) [ - 1 ] != n_hidden : 
~~~ input_units = tf . layers . dense ( input_units , n_hidden ) 
~~ if use_dilation : 
~~ sigmoid_gate = tf . layers . dense ( input_units , 1 , activation = tf . sigmoid , kernel_initializer = INITIALIZER ( ) ) 
input_units = sigmoid_gate * input_units + ( 1 - sigmoid_gate ) * units 
input_units = tf . nn . relu ( input_units ) 
~~ units = input_units 
return units 
~~ def embedding_layer ( token_indices = None , 
token_embedding_matrix = None , 
n_tokens = None , 
token_embedding_dim = None , 
name : str = None , 
trainable = True ) : 
if token_embedding_matrix is not None : 
~~~ tok_mat = token_embedding_matrix 
if trainable : 
~~~ tok_mat = np . random . randn ( n_tokens , token_embedding_dim ) . astype ( np . float32 ) / np . sqrt ( token_embedding_dim ) 
~~ tok_emb_mat = tf . Variable ( tok_mat , name = name , trainable = trainable ) 
embedded_tokens = tf . nn . embedding_lookup ( tok_emb_mat , token_indices ) 
return embedded_tokens 
~~ def character_embedding_network ( char_placeholder : tf . Tensor , 
n_characters : int = None , 
emb_mat : np . array = None , 
char_embedding_dim : int = None , 
filter_widths = ( 3 , 4 , 5 , 7 ) , 
highway_on_top = False ) : 
if emb_mat is None : 
~~~ emb_mat = np . random . randn ( n_characters , char_embedding_dim ) . astype ( np . float32 ) / np . sqrt ( char_embedding_dim ) 
~~~ char_embedding_dim = emb_mat . shape [ 1 ] 
~~ char_emb_var = tf . Variable ( emb_mat , trainable = True ) 
with tf . variable_scope ( 'Char_Emb_Network' ) : 
~~~ c_emb = tf . nn . embedding_lookup ( char_emb_var , char_placeholder ) 
conv_results_list = [ ] 
for filter_width in filter_widths : 
~~~ conv_results_list . append ( tf . layers . conv2d ( c_emb , 
char_embedding_dim , 
( 1 , filter_width ) , 
kernel_initializer = INITIALIZER ) ) 
~~ units = tf . concat ( conv_results_list , axis = 3 ) 
units = tf . reduce_max ( units , axis = 2 ) 
if highway_on_top : 
~~~ sigmoid_gate = tf . layers . dense ( units , 
1 , 
activation = tf . sigmoid , 
kernel_initializer = INITIALIZER , 
kernel_regularizer = tf . nn . l2_loss ) 
deeper_units = tf . layers . dense ( units , 
tf . shape ( units ) [ - 1 ] , 
units = sigmoid_gate * units + ( 1 - sigmoid_gate ) * deeper_units 
units = tf . nn . relu ( units ) 
~~ ~~ return units 
n_time_steps = tf . shape ( units ) [ 1 ] 
return tf . tile ( tf . expand_dims ( units , axis ) , repetitions ) 
n_input_features = units . get_shape ( ) . as_list ( ) [ 2 ] 
~~ units_pairs = tf . concat ( [ expand_tile ( units , 1 ) , expand_tile ( units , 2 ) ] , 3 ) 
query = tf . layers . dense ( units_pairs , n_hidden , activation = tf . tanh , kernel_initializer = INITIALIZER ( ) ) 
attention = tf . nn . softmax ( tf . layers . dense ( query , 1 ) , dim = 2 ) 
attended_units = tf . reduce_sum ( attention * expand_tile ( units , 1 ) , axis = 2 ) 
output = tf . layers . dense ( attended_units , n_output_features , activation , kernel_initializer = INITIALIZER ( ) ) 
~~ queries = tf . layers . dense ( expand_tile ( units , 1 ) , n_hidden , kernel_initializer = INITIALIZER ( ) ) 
keys = tf . layers . dense ( expand_tile ( units , 2 ) , n_hidden , kernel_initializer = INITIALIZER ( ) ) 
scores = tf . reduce_sum ( queries * keys , axis = 3 , keep_dims = True ) 
attention = tf . nn . softmax ( scores , dim = 2 ) 
~~ def cudnn_gru ( units , n_hidden , n_layers = 1 , trainable_initial_states = False , 
seq_lengths = None , input_initial_h = None , name = 'cudnn_gru' , reuse = False ) : 
with tf . variable_scope ( name , reuse = reuse ) : 
~~~ gru = tf . contrib . cudnn_rnn . CudnnGRU ( num_layers = n_layers , 
num_units = n_hidden ) 
~~~ init_h = tf . get_variable ( 'init_h' , [ n_layers , 1 , n_hidden ] ) 
init_h = tf . tile ( init_h , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) 
~~~ init_h = tf . zeros ( [ n_layers , tf . shape ( units ) [ 0 ] , n_hidden ] ) 
~~ initial_h = input_initial_h or init_h 
h , h_last = gru ( tf . transpose ( units , ( 1 , 0 , 2 ) ) , ( initial_h , ) ) 
h = tf . transpose ( h , ( 1 , 0 , 2 ) ) 
if seq_lengths is not None : 
~~~ indices = tf . stack ( [ tf . range ( tf . shape ( h ) [ 0 ] ) , seq_lengths - 1 ] , axis = 1 ) 
h_last = tf . gather_nd ( h , indices ) 
~~ return h , h_last 
~~ ~~ def cudnn_compatible_gru ( units , n_hidden , n_layers = 1 , trainable_initial_states = False , 
~~~ if trainable_initial_states : 
with tf . variable_scope ( 'cudnn_gru' , reuse = reuse ) : 
~~~ def single_cell ( ) : return tf . contrib . cudnn_rnn . CudnnCompatibleGRUCell ( n_hidden ) 
cell = tf . nn . rnn_cell . MultiRNNCell ( [ single_cell ( ) for _ in range ( n_layers ) ] ) 
units = tf . transpose ( units , ( 1 , 0 , 2 ) ) 
h , h_last = tf . nn . dynamic_rnn ( cell = cell , inputs = units , time_major = True , 
initial_state = tuple ( tf . unstack ( initial_h , axis = 0 ) ) ) 
~~ ~~ ~~ def cudnn_lstm ( units , n_hidden , n_layers = 1 , trainable_initial_states = None , seq_lengths = None , initial_h = None , 
initial_c = None , name = 'cudnn_lstm' , reuse = False ) : 
~~~ lstm = tf . contrib . cudnn_rnn . CudnnLSTM ( num_layers = n_layers , 
init_c = tf . get_variable ( 'init_c' , [ n_layers , 1 , n_hidden ] ) 
init_c = tf . tile ( init_c , ( 1 , tf . shape ( units ) [ 0 ] , 1 ) ) 
~~~ init_h = init_c = tf . zeros ( [ n_layers , tf . shape ( units ) [ 0 ] , n_hidden ] ) 
~~ initial_h = initial_h or init_h 
initial_c = initial_c or init_c 
h , ( h_last , c_last ) = lstm ( tf . transpose ( units , ( 1 , 0 , 2 ) ) , ( initial_h , initial_c ) ) 
h_last = h_last [ - 1 ] 
c_last = c_last [ - 1 ] 
~~ return h , ( h_last , c_last ) 
~~ ~~ def cudnn_compatible_lstm ( units , n_hidden , n_layers = 1 , trainable_initial_states = None , seq_lengths = None , initial_h = None , 
with tf . variable_scope ( 'cudnn_lstm' , reuse = reuse ) : 
~~~ def single_cell ( ) : return tf . contrib . cudnn_rnn . CudnnCompatibleLSTMCell ( n_hidden ) 
init = tuple ( [ tf . nn . rnn_cell . LSTMStateTuple ( ic , ih ) for ih , ic in 
zip ( tf . unstack ( initial_h , axis = 0 ) , tf . unstack ( initial_c , axis = 0 ) ) ] ) 
h , state = tf . nn . dynamic_rnn ( cell = cell , inputs = units , time_major = True , initial_state = init ) 
h_last = state [ - 1 ] . h 
c_last = state [ - 1 ] . c 
~~ ~~ ~~ def cudnn_bi_gru ( units , 
n_layers = 1 , 
name = 'cudnn_bi_gru' , 
reuse = False ) : 
~~~ if seq_lengths is None : 
~~~ seq_lengths = tf . ones ( [ tf . shape ( units ) [ 0 ] ] , dtype = tf . int32 ) * tf . shape ( units ) [ 1 ] 
~~ with tf . variable_scope ( 'Forward' ) : 
~~~ h_fw , h_last_fw = cudnn_gru_wrapper ( units , 
n_layers = n_layers , 
trainable_initial_states = trainable_initial_states , 
seq_lengths = seq_lengths , 
reuse = reuse ) 
~~ with tf . variable_scope ( 'Backward' ) : 
~~~ reversed_units = tf . reverse_sequence ( units , seq_lengths = seq_lengths , seq_dim = 1 , batch_dim = 0 ) 
h_bw , h_last_bw = cudnn_gru_wrapper ( reversed_units , 
h_bw = tf . reverse_sequence ( h_bw , seq_lengths = seq_lengths , seq_dim = 1 , batch_dim = 0 ) 
~~ ~~ return ( h_fw , h_bw ) , ( h_last_fw , h_last_bw ) 
~~ def cudnn_bi_lstm ( units , 
~~~ h_fw , ( h_fw_last , c_fw_last ) = cudnn_lstm_wrapper ( units , 
seq_lengths = seq_lengths ) 
h_bw , ( h_bw_last , c_bw_last ) = cudnn_lstm_wrapper ( reversed_units , 
~~ return ( h_fw , h_bw ) , ( ( h_fw_last , c_fw_last ) , ( h_bw_last , c_bw_last ) ) 
~~ ~~ def cudnn_stacked_bi_gru ( units , 
n_stacks = 2 , 
keep_prob = 1.0 , 
concat_stacked_outputs = False , 
name = 'cudnn_stacked_bi_gru' , 
if seq_lengths is None : 
~~ outputs = [ units ] 
~~~ for n in range ( n_stacks ) : 
~~~ if n == 0 : 
~~~ inputs = outputs [ - 1 ] 
~~~ inputs = variational_dropout ( outputs [ - 1 ] , keep_prob = keep_prob ) 
~~ ( h_fw , h_bw ) , _ = cudnn_bi_gru ( inputs , n_hidden , seq_lengths , 
name = '{}_cudnn_bi_gru' . format ( n ) , 
outputs . append ( tf . concat ( [ h_fw , h_bw ] , axis = 2 ) ) 
~~ ~~ if concat_stacked_outputs : 
~~~ return tf . concat ( outputs [ 1 : ] , axis = 2 ) 
~~ return outputs [ - 1 ] 
~~ def variational_dropout ( units , keep_prob , fixed_mask_dims = ( 1 , ) ) : 
units_shape = tf . shape ( units ) 
noise_shape = [ units_shape [ n ] for n in range ( len ( units . shape ) ) ] 
for dim in fixed_mask_dims : 
~~~ noise_shape [ dim ] = 1 
~~ return tf . nn . dropout ( units , keep_prob , noise_shape ) 
~~ def build ( self ) : 
word_inputs = kl . Input ( shape = ( None , MAX_WORD_LENGTH + 2 ) , dtype = "int32" ) 
inputs = [ word_inputs ] 
word_outputs = self . _build_word_cnn ( word_inputs ) 
if len ( self . word_vectorizers ) > 0 : 
~~~ additional_word_inputs = [ kl . Input ( shape = ( None , input_dim ) , dtype = "float32" ) 
for input_dim , dense_dim in self . word_vectorizers ] 
inputs . extend ( additional_word_inputs ) 
additional_word_embeddings = [ kl . Dense ( dense_dim ) ( additional_word_inputs [ i ] ) 
for i , ( _ , dense_dim ) in enumerate ( self . word_vectorizers ) ] 
word_outputs = kl . Concatenate ( ) ( [ word_outputs ] + additional_word_embeddings ) 
~~ outputs , lstm_outputs = self . _build_basic_network ( word_outputs ) 
compile_args = { "optimizer" : ko . nadam ( lr = 0.002 , clipnorm = 5.0 ) , 
"loss" : "categorical_crossentropy" , "metrics" : [ "accuracy" ] } 
self . model_ = Model ( inputs , outputs ) 
self . model_ . compile ( ** compile_args ) 
if self . verbose > 0 : 
~~~ self . model_ . summary ( print_fn = log . info ) 
~~ return self 
~~ def _build_word_cnn ( self , inputs ) : 
inputs = kl . Lambda ( kb . one_hot , arguments = { "num_classes" : self . symbols_number_ } , 
output_shape = lambda x : tuple ( x ) + ( self . symbols_number_ , ) ) ( inputs ) 
char_embeddings = kl . Dense ( self . char_embeddings_size , use_bias = False ) ( inputs ) 
conv_outputs = [ ] 
self . char_output_dim_ = 0 
for window_size , filters_number in zip ( self . char_window_size , self . char_filters ) : 
~~~ curr_output = char_embeddings 
curr_filters_number = ( min ( self . char_filter_multiple * window_size , 200 ) 
if filters_number is None else filters_number ) 
for _ in range ( self . char_conv_layers - 1 ) : 
~~~ curr_output = kl . Conv2D ( curr_filters_number , ( 1 , window_size ) , 
padding = "same" , activation = "relu" , 
data_format = "channels_last" ) ( curr_output ) 
if self . conv_dropout > 0.0 : 
~~~ curr_output = kl . Dropout ( self . conv_dropout ) ( curr_output ) 
~~ ~~ curr_output = kl . Conv2D ( curr_filters_number , ( 1 , window_size ) , 
conv_outputs . append ( curr_output ) 
self . char_output_dim_ += curr_filters_number 
~~ if len ( conv_outputs ) > 1 : 
~~~ conv_output = kl . Concatenate ( axis = - 1 ) ( conv_outputs ) 
~~~ conv_output = conv_outputs [ 0 ] 
~~ highway_input = kl . Lambda ( kb . max , arguments = { "axis" : - 2 } ) ( conv_output ) 
if self . intermediate_dropout > 0.0 : 
~~~ highway_input = kl . Dropout ( self . intermediate_dropout ) ( highway_input ) 
~~ for i in range ( self . char_highway_layers - 1 ) : 
~~~ highway_input = Highway ( activation = "relu" ) ( highway_input ) 
if self . highway_dropout > 0.0 : 
~~~ highway_input = kl . Dropout ( self . highway_dropout ) ( highway_input ) 
~~ ~~ highway_output = Highway ( activation = "relu" ) ( highway_input ) 
return highway_output 
~~ def _build_basic_network ( self , word_outputs ) : 
if self . word_dropout > 0.0 : 
~~~ lstm_outputs = kl . Dropout ( self . word_dropout ) ( word_outputs ) 
~~~ lstm_outputs = word_outputs 
~~ for j in range ( self . word_lstm_layers - 1 ) : 
~~~ lstm_outputs = kl . Bidirectional ( 
kl . LSTM ( self . word_lstm_units [ j ] , return_sequences = True , 
dropout = self . lstm_dropout ) ) ( lstm_outputs ) 
~~ lstm_outputs = kl . Bidirectional ( 
kl . LSTM ( self . word_lstm_units [ - 1 ] , return_sequences = True , 
pre_outputs = kl . TimeDistributed ( 
kl . Dense ( self . tags_number_ , activation = "softmax" , 
activity_regularizer = self . regularizer ) , 
name = "p" ) ( lstm_outputs ) 
return pre_outputs , lstm_outputs 
~~ def train_on_batch ( self , data : List [ Iterable ] , labels : Iterable [ list ] ) -> None : 
X , Y = self . _transform_batch ( data , labels ) 
self . model_ . train_on_batch ( X , Y ) 
~~ def predict_on_batch ( self , data : Union [ list , tuple ] , 
return_indexes : bool = False ) -> List [ List [ str ] ] : 
X = self . _transform_batch ( data ) 
objects_number , lengths = len ( X [ 0 ] ) , [ len ( elem ) for elem in data [ 0 ] ] 
Y = self . model_ . predict_on_batch ( X ) 
labels = np . argmax ( Y , axis = - 1 ) 
answer : List [ List [ str ] ] = [ None ] * objects_number 
for i , ( elem , length ) in enumerate ( zip ( labels , lengths ) ) : 
~~~ elem = elem [ : length ] 
answer [ i ] = elem if return_indexes else self . tags . idxs2toks ( elem ) 
~~ def _make_sent_vector ( self , sent : List , bucket_length : int = None ) -> np . ndarray : 
bucket_length = bucket_length or len ( sent ) 
answer = np . zeros ( shape = ( bucket_length , MAX_WORD_LENGTH + 2 ) , dtype = np . int32 ) 
for i , word in enumerate ( sent ) : 
~~~ answer [ i , 0 ] = self . tags . tok2idx ( "BEGIN" ) 
m = min ( len ( word ) , MAX_WORD_LENGTH ) 
for j , x in enumerate ( word [ - m : ] ) : 
~~~ answer [ i , j + 1 ] = self . symbols . tok2idx ( x ) 
~~ answer [ i , m + 1 ] = self . tags . tok2idx ( "END" ) 
answer [ i , m + 2 : ] = self . tags . tok2idx ( "PAD" ) 
~~ def _make_tags_vector ( self , tags , bucket_length = None ) -> np . ndarray : 
bucket_length = bucket_length or len ( tags ) 
answer = np . zeros ( shape = ( bucket_length , ) , dtype = np . int32 ) 
for i , tag in enumerate ( tags ) : 
~~~ answer [ i ] = self . tags . tok2idx ( tag ) 
~~ def bleu_advanced ( y_true : List [ Any ] , y_predicted : List [ Any ] , 
weights : Tuple = ( 1 , ) , smoothing_function = SMOOTH . method1 , 
auto_reweigh = False , penalty = True ) -> float : 
bleu_measure = sentence_bleu ( [ y_true ] , y_predicted , weights , smoothing_function , auto_reweigh ) 
hyp_len = len ( y_predicted ) 
hyp_lengths = hyp_len 
ref_lengths = closest_ref_length ( [ y_true ] , hyp_len ) 
bpenalty = brevity_penalty ( ref_lengths , hyp_lengths ) 
if penalty is True or bpenalty == 0 : 
~~~ return bleu_measure 
~~ return bleu_measure / bpenalty 
~~ def verify_sc_url ( url : str ) -> bool : 
parsed = urlsplit ( url ) 
scheme : str = parsed . scheme 
netloc : str = parsed . netloc 
path : str = parsed . path 
~~~ port = parsed . port 
~~~ port = None 
~~ result = ( scheme . lower ( ) == 'https' and 
netloc . lower ( ) . split ( ':' ) [ 0 ] == 's3.amazonaws.com' and 
path . startswith ( '/echo.api/' ) and 
( port == 443 or port is None ) ) 
return result 
~~ def extract_certs ( certs_txt : str ) -> List [ crypto . X509 ] : 
certs_txt = re . findall ( pattern , certs_txt , flags = re . DOTALL ) 
certs = [ crypto . load_certificate ( crypto . FILETYPE_PEM , cert_txt ) for cert_txt in certs_txt ] 
return certs 
~~ def verify_sans ( amazon_cert : crypto . X509 ) -> bool : 
cert_extentions = [ amazon_cert . get_extension ( i ) for i in range ( amazon_cert . get_extension_count ( ) ) ] 
subject_alt_names = '' 
for extention in cert_extentions : 
~~~ if 'subjectAltName' in str ( extention . get_short_name ( ) ) : 
~~~ subject_alt_names = extention . __str__ ( ) 
~~ ~~ result = 'echo-api.amazon.com' in subject_alt_names 
~~ def verify_certs_chain ( certs_chain : List [ crypto . X509 ] , amazon_cert : crypto . X509 ) -> bool : 
store = crypto . X509Store ( ) 
for cert in certs_chain : 
~~~ store . add_cert ( cert ) 
~~ default_verify_paths = ssl . get_default_verify_paths ( ) 
default_verify_file = default_verify_paths . cafile 
default_verify_file = Path ( default_verify_file ) . resolve ( ) if default_verify_file else None 
default_verify_path = default_verify_paths . capath 
default_verify_path = Path ( default_verify_path ) . resolve ( ) if default_verify_path else None 
ca_files = [ ca_file for ca_file in default_verify_path . iterdir ( ) ] if default_verify_path else [ ] 
if default_verify_file : 
~~~ ca_files . append ( default_verify_file ) 
~~ for ca_file in ca_files : 
~~~ ca_file : Path 
if ca_file . is_file ( ) : 
~~~ with ca_file . open ( 'r' , encoding = 'ascii' ) as crt_f : 
~~~ ca_certs_txt = crt_f . read ( ) 
ca_certs = extract_certs ( ca_certs_txt ) 
for cert in ca_certs : 
~~ ~~ ~~ ~~ ssl_context = ssl . create_default_context ( ) 
der_certs = ssl_context . get_ca_certs ( binary_form = True ) 
pem_certs = '\\n' . join ( [ ssl . DER_cert_to_PEM_cert ( der_cert ) for der_cert in der_certs ] ) 
ca_certs = extract_certs ( pem_certs ) 
for ca_cert in ca_certs : 
~~~ store . add_cert ( ca_cert ) 
~~ store_context = crypto . X509StoreContext ( store , amazon_cert ) 
~~~ store_context . verify_certificate ( ) 
result = True 
~~ except crypto . X509StoreContextError : 
~~~ result = False 
~~ return result 
~~ def verify_signature ( amazon_cert : crypto . X509 , signature : str , request_body : bytes ) -> bool : 
signature = base64 . b64decode ( signature ) 
~~~ crypto . verify ( amazon_cert , signature , request_body , 'sha1' ) 
~~ except crypto . Error : 
~~ def verify_cert ( signature_chain_url : str ) -> Optional [ crypto . X509 ] : 
~~~ certs_chain_get = requests . get ( signature_chain_url ) 
~~ except requests . exceptions . ConnectionError as e : 
return None 
~~ certs_chain_txt = certs_chain_get . text 
certs_chain = extract_certs ( certs_chain_txt ) 
amazon_cert : crypto . X509 = certs_chain . pop ( 0 ) 
sc_url_verification = verify_sc_url ( signature_chain_url ) 
if not sc_url_verification : 
~~ expired_verification = not amazon_cert . has_expired ( ) 
if not expired_verification : 
~~ sans_verification = verify_sans ( amazon_cert ) 
if not sans_verification : 
~~ chain_verification = verify_certs_chain ( certs_chain , amazon_cert ) 
if not chain_verification : 
~~ result = ( sc_url_verification and expired_verification and sans_verification and chain_verification ) 
return amazon_cert if result else None 
~~ def json ( self ) -> list : 
json_controls = [ control . json ( ) for control in self . controls ] 
return json_controls 
~~ def ms_bot_framework ( self ) -> list : 
ms_bf_controls = [ control . ms_bot_framework ( ) for control in self . controls ] 
return ms_bf_controls 
~~ def telegram ( self ) -> list : 
telegram_controls = [ control . telegram ( ) for control in self . controls ] 
return telegram_controls 
~~ def alexa ( self ) -> list : 
alexa_controls = [ control . alexa ( ) for control in self . controls ] 
return alexa_controls 
~~ def main ( ) : 
args = parser . parse_args ( ) 
path = get_settings_path ( ) 
if args . default : 
~~~ if populate_settings_dir ( force = True ) : 
~~ ~~ def _graph_wrap ( func , graph ) : 
@ wraps ( func ) 
def _wrapped ( * args , ** kwargs ) : 
~~~ with graph . as_default ( ) : 
~~~ return func ( * args , ** kwargs ) 
~~ ~~ return _wrapped 
~~ def _keras_wrap ( func , graph , session ) : 
import keras . backend as K 
~~~ K . set_session ( session ) 
return func ( * args , ** kwargs ) 
~~ def roc_auc_score ( y_true : Union [ List [ List [ float ] ] , List [ List [ int ] ] , np . ndarray ] , 
y_pred : Union [ List [ List [ float ] ] , List [ List [ int ] ] , np . ndarray ] ) -> float : 
~~~ return sklearn . metrics . roc_auc_score ( np . squeeze ( np . array ( y_true ) ) , 
np . squeeze ( np . array ( y_pred ) ) , average = "macro" ) 
~~~ return 0. 
~~ ~~ def hash_ ( token : str , hash_size : int ) -> int : 
return murmurhash3_32 ( token , positive = True ) % hash_size 
~~ def accuracy ( y_true : [ list , np . ndarray ] , y_predicted : [ list , np . ndarray ] ) -> float : 
examples_len = len ( y_true ) 
correct = sum ( [ y1 == y2 for y1 , y2 in zip ( y_true , y_predicted ) ] ) 
return correct / examples_len if examples_len else 0 
~~ def round_accuracy ( y_true , y_predicted ) : 
predictions = [ round ( x ) for x in y_predicted ] 
correct = sum ( [ y1 == y2 for y1 , y2 in zip ( y_true , predictions ) ] ) 
~~ def _pretrained_initializer ( varname , weight_file , embedding_weight_file = None ) : 
weight_name_map = { } 
for i in range ( 2 ) : 
~~~ root = 'RNN_{}/RNN/MultiRNNCell/Cell{}' . format ( i , j ) 
weight_name_map [ root + '/rnn/lstm_cell/kernel' ] = root + '/LSTMCell/W_0' 
weight_name_map [ root + '/rnn/lstm_cell/bias' ] = root + '/LSTMCell/B' 
weight_name_map [ root + '/rnn/lstm_cell/projection/kernel' ] = root + '/LSTMCell/W_P_0' 
~~ ~~ varname_in_file = varname [ 5 : ] 
if varname_in_file . startswith ( 'RNN' ) : 
~~~ varname_in_file = weight_name_map [ varname_in_file ] 
~~ if varname_in_file == 'embedding' : 
~~~ with h5py . File ( embedding_weight_file , 'r' ) as fin : 
~~~ embed_weights = fin [ varname_in_file ] [ ... ] 
weights = np . zeros ( 
( embed_weights . shape [ 0 ] + 1 , embed_weights . shape [ 1 ] ) , 
dtype = DTYPE 
weights [ 1 : , : ] = embed_weights 
~~~ with h5py . File ( weight_file , 'r' ) as fin : 
~~~ if varname_in_file == 'char_embed' : 
~~~ char_embed_weights = fin [ varname_in_file ] [ ... ] 
( char_embed_weights . shape [ 0 ] + 1 , 
char_embed_weights . shape [ 1 ] ) , 
weights [ 1 : , : ] = char_embed_weights 
~~~ weights = fin [ varname_in_file ] [ ... ] 
~~ ~~ ~~ def ret ( shape , ** kwargs ) : 
~~~ if list ( shape ) != list ( weights . shape ) : 
~~~ raise ValueError ( 
varname_in_file , shape , weights . shape ) 
~~ return weights 
~~ def weight_layers ( name , bilm_ops , l2_coef = None , 
use_top_only = False , do_layer_norm = False , reuse = False ) : 
def _l2_regularizer ( weights ) : 
~~~ if l2_coef is not None : 
~~~ return l2_coef * tf . reduce_sum ( tf . square ( weights ) ) 
~~~ return 0.0 
~~ ~~ lm_embeddings = bilm_ops [ 'lm_embeddings' ] 
mask = bilm_ops [ 'mask' ] 
n_lm_layers = int ( lm_embeddings . get_shape ( ) [ 1 ] ) 
lm_dim = int ( lm_embeddings . get_shape ( ) [ 3 ] ) 
with tf . control_dependencies ( [ lm_embeddings , mask ] ) : 
~~~ mask_float = tf . cast ( mask , 'float32' ) 
broadcast_mask = tf . expand_dims ( mask_float , axis = - 1 ) 
def _do_ln ( x ) : 
~~~ x_masked = x * broadcast_mask 
N = tf . reduce_sum ( mask_float ) * lm_dim 
mean = tf . reduce_sum ( x_masked ) / N 
variance = tf . reduce_sum ( ( ( x_masked - mean ) * broadcast_mask ) ** 2 ) / N 
return tf . nn . batch_normalization ( 
x , mean , variance , None , None , 1E-12 
~~ if use_top_only : 
~~~ layers = tf . split ( lm_embeddings , n_lm_layers , axis = 1 ) 
sum_pieces = tf . squeeze ( layers [ - 1 ] , squeeze_dims = 1 ) 
reg = 0.0 
~~~ with tf . variable_scope ( "aggregation" , reuse = reuse ) : 
~~~ W = tf . get_variable ( 
'{}_ELMo_W' . format ( name ) , 
shape = ( n_lm_layers , ) , 
initializer = tf . zeros_initializer , 
regularizer = _l2_regularizer , 
trainable = True , 
~~ normed_weights = tf . split ( 
tf . nn . softmax ( W + 1.0 / n_lm_layers ) , n_lm_layers 
layers = tf . split ( lm_embeddings , n_lm_layers , axis = 1 ) 
pieces = [ ] 
for w , t in zip ( normed_weights , layers ) : 
~~~ if do_layer_norm : 
~~~ pieces . append ( w * _do_ln ( tf . squeeze ( t , squeeze_dims = 1 ) ) ) 
~~~ pieces . append ( w * tf . squeeze ( t , squeeze_dims = 1 ) ) 
~~ ~~ sum_pieces = tf . add_n ( pieces ) 
reg = [ 
r for r in tf . get_collection ( tf . GraphKeys . REGULARIZATION_LOSSES ) 
if r . name . find ( '{}_ELMo_W/' . format ( name ) ) >= 0 
] 
if len ( reg ) != 1 : 
~~~ raise ValueError 
~~ ~~ with tf . variable_scope ( "aggregation" , reuse = reuse ) : 
~~~ gamma = tf . get_variable ( 
'{}_ELMo_gamma' . format ( name ) , 
shape = ( 1 , ) , 
initializer = tf . ones_initializer , 
regularizer = None , 
~~ weighted_lm_layers = sum_pieces * gamma 
weighted_lm_layers_masked = sum_pieces * broadcast_mask 
weighted_lm_layers_sum = tf . reduce_sum ( weighted_lm_layers_masked , 1 ) 
mask_sum = tf . reduce_sum ( mask_float , 1 ) 
mask_sum = tf . maximum ( mask_sum , [ 1 ] ) 
weighted_lm_layers_mean = weighted_lm_layers_sum / tf . expand_dims ( mask_sum , - 1 ) 
word_emb_2n = tf . squeeze ( layers [ 0 ] , [ 1 ] ) 
lstm_outputs1 = tf . squeeze ( layers [ 1 ] , [ 1 ] ) 
lstm_outputs2 = tf . squeeze ( layers [ 2 ] , [ 1 ] ) 
ret = { 'weighted_op' : weighted_lm_layers , 
'mean_op' : weighted_lm_layers_mean , 
'regularization_op' : reg , 
'word_emb' : word_emb_1n , 
'lstm_outputs1' : lstm_outputs1 , 
'lstm_outputs2' : lstm_outputs2 , } 
~~ def _build_word_char_embeddings ( self ) : 
projection_dim = self . options [ 'lstm' ] [ 'projection_dim' ] 
cnn_options = self . options [ 'char_cnn' ] 
filters = cnn_options [ 'filters' ] 
n_filters = sum ( f [ 1 ] for f in filters ) 
max_chars = cnn_options [ 'max_characters_per_token' ] 
char_embed_dim = cnn_options [ 'embedding' ] [ 'dim' ] 
n_chars = cnn_options [ 'n_characters' ] 
if n_chars != 262 : 
~~ if cnn_options [ 'activation' ] == 'tanh' : 
~~~ activation = tf . nn . tanh 
~~ elif cnn_options [ 'activation' ] == 'relu' : 
~~~ activation = tf . nn . relu 
~~ with tf . device ( "/cpu:0" ) : 
~~~ self . embedding_weights = tf . get_variable ( "char_embed" , [ n_chars , char_embed_dim ] , 
dtype = DTYPE , 
initializer = tf . random_uniform_initializer ( - 1.0 , 1.0 ) ) 
self . char_embedding = tf . nn . embedding_lookup ( self . embedding_weights , 
self . ids_placeholder ) 
~~ def make_convolutions ( inp ) : 
~~~ with tf . variable_scope ( 'CNN' ) : 
~~~ convolutions = [ ] 
for i , ( width , num ) in enumerate ( filters ) : 
~~~ if cnn_options [ 'activation' ] == 'relu' : 
~~~ w_init = tf . random_uniform_initializer ( 
minval = - 0.05 , maxval = 0.05 ) 
~~ elif cnn_options [ 'activation' ] == 'tanh' : 
~~~ w_init = tf . random_normal_initializer ( 
mean = 0.0 , 
stddev = np . sqrt ( 1.0 / ( width * char_embed_dim ) ) 
~~ w = tf . get_variable ( 
"W_cnn_%s" % i , 
[ 1 , width , char_embed_dim , num ] , 
initializer = w_init , 
dtype = DTYPE ) 
b = tf . get_variable ( 
"b_cnn_%s" % i , [ num ] , dtype = DTYPE , 
initializer = tf . constant_initializer ( 0.0 ) ) 
conv = tf . nn . conv2d ( inp , w , 
strides = [ 1 , 1 , 1 , 1 ] , 
padding = "VALID" ) + b 
conv = tf . nn . max_pool ( conv , [ 1 , 1 , max_chars - width + 1 , 1 ] , 
[ 1 , 1 , 1 , 1 ] , 'VALID' ) 
conv = activation ( conv ) 
conv = tf . squeeze ( conv , squeeze_dims = [ 2 ] ) 
convolutions . append ( conv ) 
~~ ~~ return tf . concat ( convolutions , 2 ) 
~~ embedding = make_convolutions ( self . char_embedding ) 
n_highway = cnn_options . get ( 'n_highway' ) 
use_highway = n_highway is not None and n_highway > 0 
use_proj = n_filters != projection_dim 
if use_highway or use_proj : 
~~~ batch_size_n_tokens = tf . shape ( embedding ) [ 0 : 2 ] 
embedding = tf . reshape ( embedding , [ - 1 , n_filters ] ) 
~~ if use_proj : 
~~~ assert n_filters > projection_dim 
with tf . variable_scope ( 'CNN_proj' ) : 
~~~ W_proj_cnn = tf . get_variable ( 
"W_proj" , [ n_filters , projection_dim ] , 
initializer = tf . random_normal_initializer ( 
mean = 0.0 , stddev = np . sqrt ( 1.0 / n_filters ) ) , 
b_proj_cnn = tf . get_variable ( 
"b_proj" , [ projection_dim ] , 
initializer = tf . constant_initializer ( 0.0 ) , 
~~ ~~ def high ( x , ww_carry , bb_carry , ww_tr , bb_tr ) : 
~~~ carry_gate = tf . nn . sigmoid ( tf . matmul ( x , ww_carry ) + bb_carry ) 
transform_gate = tf . nn . relu ( tf . matmul ( x , ww_tr ) + bb_tr ) 
return carry_gate * transform_gate + ( 1.0 - carry_gate ) * x 
~~ if use_highway : 
~~~ highway_dim = n_filters 
for i in range ( n_highway ) : 
~~~ with tf . variable_scope ( 'CNN_high_%s' % i ) : 
~~~ W_carry = tf . get_variable ( 
'W_carry' , [ highway_dim , highway_dim ] , 
mean = 0.0 , stddev = np . sqrt ( 1.0 / highway_dim ) ) , 
b_carry = tf . get_variable ( 
'b_carry' , [ highway_dim ] , 
initializer = tf . constant_initializer ( - 2.0 ) , 
W_transform = tf . get_variable ( 
'W_transform' , [ highway_dim , highway_dim ] , 
b_transform = tf . get_variable ( 
'b_transform' , [ highway_dim ] , 
~~ embedding = high ( embedding , W_carry , b_carry , 
W_transform , b_transform ) 
~~ ~~ if use_proj : 
~~~ embedding = tf . matmul ( embedding , W_proj_cnn ) + b_proj_cnn 
~~ if use_highway or use_proj : 
~~~ shp = tf . concat ( [ batch_size_n_tokens , [ projection_dim ] ] , axis = 0 ) 
embedding = tf . reshape ( embedding , shp ) 
~~ self . embedding = embedding 
~~ def read ( self , data_path : str , * args , ** kwargs ) -> Dict [ str , List [ Tuple [ Any , Any ] ] ] : 
raise NotImplementedError 
~~ def make_hello_bot_agent ( ) -> DefaultAgent : 
agent = DefaultAgent ( [ skill_hello , skill_bye , skill_fallback ] , skills_processor = HighestConfidenceSelector ( ) ) 
return agent 
~~ def to_one_hot ( x , k ) : 
unit = np . eye ( k , dtype = int ) 
return unit [ x ] 
~~ def prettify_metrics ( metrics : List [ Tuple [ str , float ] ] , precision : int = 4 ) -> OrderedDict : 
prettified_metrics = OrderedDict ( ) 
for key , value in metrics : 
~~~ value = round ( value , precision ) 
prettified_metrics [ key ] = value 
~~ return prettified_metrics 
~~ def populate_settings_dir ( force : bool = False ) -> bool : 
res = False 
if _default_settings_path == _settings_path : 
~~~ return res 
~~ for src in list ( _default_settings_path . glob ( '**/*.json' ) ) : 
~~~ dest = _settings_path / src . relative_to ( _default_settings_path ) 
if not force and dest . exists ( ) : 
~~ res = True 
dest . parent . mkdir ( parents = True , exist_ok = True ) 
shutil . copy ( src , dest ) 
~~ def update_state ( self , 
slots : Union [ List [ Tuple [ str , Any ] ] , Dict [ str , Any ] ] ) -> 'Tracker' : 
~~ def predict_with_model ( config_path : [ Path , str ] ) -> List [ Optional [ List [ str ] ] ] : 
config = parse_config ( config_path ) 
reader_config = config [ 'dataset_reader' ] 
reader = get_model ( reader_config [ 'class_name' ] ) ( ) 
data_path = expand_path ( reader_config . get ( 'data_path' , '' ) ) 
read_params = { k : v for k , v in reader_config . items ( ) if k not in [ 'class_name' , 'data_path' ] } 
data : Dict = reader . read ( data_path , ** read_params ) 
iterator : MorphoTaggerDatasetIterator = from_params ( iterator_config , data = data ) 
model = build_model ( config , load_trained = True ) 
answers = [ None ] * len ( iterator . test ) 
batch_size = config [ 'predict' ] . get ( "batch_size" , - 1 ) 
for indexes , ( x , _ ) in iterator . gen_batches ( 
batch_size = batch_size , data_type = "test" , shuffle = False , return_indexes = True ) : 
~~~ y = model ( x ) 
for i , elem in zip ( indexes , y ) : 
~~~ answers [ i ] = elem 
~~ ~~ outfile = config [ 'predict' ] . get ( "outfile" ) 
if outfile is not None : 
~~~ outfile = Path ( outfile ) 
if not outfile . exists ( ) : 
~~~ outfile . parent . mkdir ( parents = True , exist_ok = True ) 
~~ with open ( outfile , "w" , encoding = "utf8" ) as fout : 
~~~ for elem in answers : 
~~~ fout . write ( elem + "\\n" ) 
~~ ~~ ~~ return answers 
~~ def run_alexa_server ( agent_generator : callable , 
multi_instance : bool = False , 
stateful : bool = False , 
port : Optional [ int ] = None , 
https : bool = False , 
ssl_key : str = None , 
ssl_cert : str = None ) -> None : 
server_config_path = Path ( get_settings_path ( ) , SERVER_CONFIG_FILENAME ) . resolve ( ) 
server_params = read_json ( server_config_path ) 
host = server_params [ 'common_defaults' ] [ 'host' ] 
port = port or server_params [ 'common_defaults' ] [ 'port' ] 
alexa_server_params = server_params [ 'alexa_defaults' ] 
alexa_server_params [ 'multi_instance' ] = multi_instance or server_params [ 'common_defaults' ] [ 'multi_instance' ] 
alexa_server_params [ 'stateful' ] = stateful or server_params [ 'common_defaults' ] [ 'stateful' ] 
alexa_server_params [ 'amazon_cert_lifetime' ] = AMAZON_CERTIFICATE_LIFETIME 
if https : 
~~~ ssh_key_path = Path ( ssl_key or server_params [ 'https_key_path' ] ) . resolve ( ) 
if not ssh_key_path . is_file ( ) : 
log . error ( e ) 
raise e 
~~ ssh_cert_path = Path ( ssl_cert or server_params [ 'https_cert_path' ] ) . resolve ( ) 
if not ssh_cert_path . is_file ( ) : 
~~ ssl_context = ssl . SSLContext ( ssl . PROTOCOL_TLSv1_2 ) 
ssl_context . load_cert_chain ( ssh_cert_path , ssh_key_path ) 
~~~ ssl_context = None 
~~ input_q = Queue ( ) 
output_q = Queue ( ) 
bot = Bot ( agent_generator , alexa_server_params , input_q , output_q ) 
bot . start ( ) 
endpoint_description = { 
'parameters' : [ 
{ 
'name' : 'Signature' , 
'in' : 'header' , 
'required' : 'true' , 
'type' : 'string' , 
'example' : 'Z5H5wqd06ExFVPNfJiqhKvAFjkf+cTVodOUirucHGcEVAMO1LfvgqWUkZ/X1ITDZbI0w+SMwVkEQZlkeThbVS/54M22StNDUtfz4Ua20xNDpIPwcWIACAmZ38XxbbTEFJI5WwqrbilNcfzqiGrIPfdO5rl+/xUjHFUdcJdUY/QzBxXsceytVYfEiR9MzOCN2m4C0XnpThUavAu159KrLj8AkuzN0JF87iXv+zOEeZRgEuwmsAnJrRUwkJ4yWokEPnSVdjF0D6f6CscfyvRe9nsWShq7/zRTa41meweh+n006zvf58MbzRdXPB22RI4AN0ksWW7hSC8/QLAKQE+lvaw==' , 
'name' : 'Signaturecertchainurl' , 
'example' : 'https://s3.amazonaws.com/echo.api/echo-api-cert-6-ats.pem' , 
'name' : 'data' , 
'in' : 'body' , 
'example' : { 
'version' : '1.0' , 
'session' : { 
'new' : False , 
'sessionId' : 'amzn1.echo-api.session.3c6ebffd-55b9-4e1a-bf3c-c921c1801b63' , 
'application' : { 
'applicationId' : 'amzn1.ask.skill.8b17a5de-3749-4919-aa1f-e0bbaf8a46a6' 
'attributes' : { 
'sessionId' : 'amzn1.echo-api.session.3c6ebffd-55b9-4e1a-bf3c-c921c1801b63' 
'user' : { 
'userId' : 'amzn1.ask.account.AGR4R2LOVHMNMNOGROBVNLU7CL4C57X465XJF2T2F55OUXNTLCXDQP3I55UXZIALEKKZJ6Q2MA5MEFSMZVPEL5NVZS6FZLEU444BVOLPB5WVH5CHYTQAKGD7VFLGPRFZVHHH2NIB4HKNHHGX6HM6S6QDWCKXWOIZL7ONNQSBUCVPMZQKMCYXRG5BA2POYEXFDXRXCGEVDWVSMPQ' 
'context' : { 
'System' : { 
'device' : { 
'deviceId' : 'amzn1.ask.device.AFQAMLYOYQUUACSE7HFVYS4ZI2KUB35JPHQRUPKTDCAU3A47WESP5L57KSWT5L6RT3FVXWH4OA2DNPJRMZ2VGEIACF3PJEIDCOUWUBC4W5RPJNUB3ZVT22J4UJN5UL3T2UBP36RVHFJ5P4IPT2HUY3P2YOY33IOU4O33HUAG7R2BUNROEH4T2' , 
'supportedInterfaces' : { } 
'apiEndpoint' : 'https://api.amazonalexa.com' , 
'apiAccessToken' : 'eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsImtpZCI6IjEifQ.eyJhdWQiOiJodHRwczovL2FwaS5hbWF6b25hbGV4YS5jb20iLCJpc3MiOiJBbGV4YVNraWxsS2l0Iiwic3ViIjoiYW16bjEuYXNrLnNraWxsLjhiMTdhNWRlLTM3NDktNDkxOS1hYTFmLWUwYmJhZjhhNDZhNiIsImV4cCI6MTU0NTIyMzY1OCwiaWF0IjoxNTQ1MjIwMDU4LCJuYmYiOjE1NDUyMjAwNTgsInByaXZhdGVDbGFpbXMiOnsiY29uc2VudFRva2VuIjpudWxsLCJkZXZpY2VJZCI6ImFtem4xLmFzay5kZXZpY2UuQUZRQU1MWU9ZUVVVQUNTRTdIRlZZUzRaSTJLVUIzNUpQSFFSVVBLVERDQVUzQTQ3V0VTUDVMNTdLU1dUNUw2UlQzRlZYV0g0T0EyRE5QSlJNWjJWR0VJQUNGM1BKRUlEQ09VV1VCQzRXNVJQSk5VQjNaVlQyMko0VUpONVVMM1QyVUJQMzZSVkhGSjVQNElQVDJIVVkzUDJZT1kzM0lPVTRPMzNIVUFHN1IyQlVOUk9FSDRUMiIsInVzZXJJZCI6ImFtem4xLmFzay5hY2NvdW50LkFHUjRSMkxPVkhNTk1OT0dST0JWTkxVN0NMNEM1N1g0NjVYSkYyVDJGNTVPVVhOVExDWERRUDNJNTVVWFpJQUxFS0taSjZRMk1BNU1FRlNNWlZQRUw1TlZaUzZGWkxFVTQ0NEJWT0xQQjVXVkg1Q0hZVFFBS0dEN1ZGTEdQUkZaVkhISDJOSUI0SEtOSEhHWDZITTZTNlFEV0NLWFdPSVpMN09OTlFTQlVDVlBNWlFLTUNZWFJHNUJBMlBPWUVYRkRYUlhDR0VWRFdWU01QUSJ9fQ.jcomYhBhU485T4uoe2NyhWnL-kZHoPQKpcycFqa-1sy_lSIitfFGup9DKrf2NkN-I9lZ3xwq9llqx9WRN78fVJjN6GLcDhBDH0irPwt3n9_V7_5bfB6KARv5ZG-JKOmZlLBqQbnln0DAJ10D8HNiytMARNEwduMBVDNK0A5z6YxtRcLYYFD2-Ieg_V8Qx90eE2pd2U5xOuIEL0pXfSoiJ8vpxb8BKwaMO47tdE4qhg_k7v8ClwyXg3EMEhZFjixYNqdW1tCrwDGj58IWMXDyzZhIlRMh6uudMOT6scSzcNVD0v42IOTZ3S_X6rG01B7xhUDlZXMqkrCuzOyqctGaPw' 
'Viewport' : { 
'experiences' : [ 
'arcMinuteWidth' : 246 , 
'arcMinuteHeight' : 144 , 
'canRotate' : False , 
'canResize' : False 
] , 
'shape' : 'RECTANGLE' , 
'pixelWidth' : 1024 , 
'pixelHeight' : 600 , 
'dpi' : 160 , 
'currentPixelWidth' : 1024 , 
'currentPixelHeight' : 600 , 
'touch' : [ 
'SINGLE' 
'request' : { 
'type' : 'IntentRequest' , 
'requestId' : 'amzn1.echo-api.request.388d0f6e-04b9-4450-a687-b9abaa73ac6a' , 
'timestamp' : '2018-12-19T11:47:38Z' , 
'locale' : 'en-US' , 
'intent' : { 
'name' : 'AskDeepPavlov' , 
'confirmationStatus' : 'NONE' , 
'slots' : { 
'raw_input' : { 
'name' : 'raw_input' , 
'resolutions' : { 
'resolutionsPerAuthority' : [ 
'authority' : 'amzn1.er-authority.echo-sdk.amzn1.ask.skill.8b17a5de-3749-4919-aa1f-e0bbaf8a46a6.GetInput' , 
'status' : { 
'code' : 'ER_SUCCESS_NO_MATCH' 
'source' : 'USER' 
'responses' : { 
"200" : { 
@ app . route ( '/' ) 
def index ( ) : 
~~~ return redirect ( '/apidocs/' ) 
~~ @ app . route ( '/interact' , methods = [ 'POST' ] ) 
@ swag_from ( endpoint_description ) 
def handle_request ( ) : 
~~~ request_body : bytes = request . get_data ( ) 
signature_chain_url : str = request . headers . get ( 'Signaturecertchainurl' ) 
signature : str = request . headers . get ( 'Signature' ) 
alexa_request : dict = request . get_json ( ) 
request_dict = { 
'request_body' : request_body , 
'signature_chain_url' : signature_chain_url , 
'signature' : signature , 
'alexa_request' : alexa_request 
bot . input_queue . put ( request_dict ) 
response : dict = bot . output_queue . get ( ) 
response_code = 400 if 'error' in response . keys ( ) else 200 
return jsonify ( response ) , response_code 
~~ app . run ( host = host , port = port , threaded = True , ssl_context = ssl_context ) 
~~ def load ( self , exclude_scopes : tuple = ( 'Optimizer' , ) ) -> None : 
if not hasattr ( self , 'sess' ) : 
~~ path = str ( self . load_path . resolve ( ) ) 
if tf . train . checkpoint_exists ( path ) : 
var_list = self . _get_saveable_variables ( exclude_scopes ) 
saver = tf . train . Saver ( var_list ) 
saver . restore ( self . sess , path ) 
~~ ~~ def save ( self , exclude_scopes : tuple = ( 'Optimizer' , ) ) -> None : 
~~ path = str ( self . save_path . resolve ( ) ) 
saver . save ( self . sess , path ) 
~~ def get_train_op ( self , 
loss , 
learning_rate , 
optimizer = None , 
clip_norm = None , 
learnable_scopes = None , 
optimizer_scope_name = None , 
** kwargs ) : 
if optimizer_scope_name is None : 
~~~ opt_scope = tf . variable_scope ( 'Optimizer' ) 
~~~ opt_scope = tf . variable_scope ( optimizer_scope_name ) 
~~ with opt_scope : 
~~~ if learnable_scopes is None : 
~~~ variables_to_train = tf . get_collection ( tf . GraphKeys . TRAINABLE_VARIABLES ) 
~~~ variables_to_train = [ ] 
for scope_name in learnable_scopes : 
~~~ variables_to_train . extend ( tf . get_collection ( tf . GraphKeys . TRAINABLE_VARIABLES , scope = scope_name ) ) 
~~ ~~ if optimizer is None : 
~~~ optimizer = tf . train . AdamOptimizer 
~~ extra_update_ops = tf . get_collection ( tf . GraphKeys . UPDATE_OPS ) 
with tf . control_dependencies ( extra_update_ops ) : 
~~~ def clip_if_not_none ( grad ) : 
~~~ if grad is not None : 
~~~ return tf . clip_by_norm ( grad , clip_norm ) 
~~ ~~ opt = optimizer ( learning_rate , ** kwargs ) 
grads_and_vars = opt . compute_gradients ( loss , var_list = variables_to_train ) 
if clip_norm is not None : 
~~~ grads_and_vars = [ ( clip_if_not_none ( grad ) , var ) 
for grad , var in grads_and_vars ] 
~~ train_op = opt . apply_gradients ( grads_and_vars ) 
~~ ~~ return train_op 
~~ def print_number_of_parameters ( ) : 
variables = tf . trainable_variables ( ) 
blocks = defaultdict ( int ) 
for var in variables : 
~~~ block_name = var . name . split ( '/' ) [ 0 ] 
number_of_parameters = np . prod ( var . get_shape ( ) . as_list ( ) ) 
blocks [ block_name ] += number_of_parameters 
~~ for block_name , cnt in blocks . items ( ) : 
~~ total_num_parameters = np . sum ( list ( blocks . values ( ) ) ) 
~~ def _precompute_absense_costs ( dictionary , removal_costs , insertion_costs , n , 
allow_spaces = False ) : 
answer = [ dict ( ) for node in dictionary . data ] 
~~ curr_alphabet = copy . copy ( dictionary . alphabet ) 
if allow_spaces : 
~~ for l , ( costs_in_node , node ) in enumerate ( zip ( answer , dictionary . data ) ) : 
~~~ curr_node_removal_costs = np . empty ( dtype = np . float64 , shape = ( n , ) ) 
if len ( node [ 0 ] ) > 0 : 
~~~ curr_node_removal_costs [ 0 ] = min ( removal_costs [ symbol ] for symbol in node [ 0 ] ) 
for j , symbols in enumerate ( node [ 1 : ] , 1 ) : 
~~~ if len ( symbols ) == 0 : 
~~~ curr_node_removal_costs [ j : ] = curr_node_removal_costs [ j - 1 ] 
~~ curr_cost = min ( removal_costs [ symbol ] for symbol in symbols ) 
curr_node_removal_costs [ j ] = min ( curr_node_removal_costs [ j - 1 ] , curr_cost ) 
~~~ curr_node_removal_costs [ : ] = np . inf 
~~ for a in curr_alphabet : 
~~~ curr_symbol_costs = np . empty ( dtype = np . float64 , shape = ( n , ) ) 
curr_symbol_costs . fill ( insertion_costs [ a ] ) 
for j , symbols in enumerate ( node ) : 
~~~ if a in symbols : 
~~~ curr_symbol_costs [ j : ] = 0.0 
~~ curr_symbol_costs [ j ] = min ( curr_symbol_costs [ j ] , curr_node_removal_costs [ j ] ) 
~~ costs_in_node [ a ] = curr_symbol_costs 
~~ def search ( self , word , d , allow_spaces = True , return_cost = True ) : 
if not all ( ( c in self . alphabet 
~~ return self . _trie_search ( 
word , d , allow_spaces = allow_spaces , return_cost = return_cost ) 
~~ def _trie_search ( self , word , d , transducer = None , 
allow_spaces = True , return_cost = True ) : 
if transducer is None : 
~~~ transducer = self . transducer . inverse ( ) 
~~ allow_spaces &= self . allow_spaces 
trie = self . dictionary 
used_agenda_keys = set ( ) 
agenda = SortedListWithKey ( key = ( lambda x : x [ 1 ] ) ) 
h = self . h_func ( word , trie . root ) 
key , value = ( "" , 0 , trie . root ) , ( 0.0 , 0.0 , h ) 
agenda . add ( ( key , value ) ) 
answer = dict ( ) 
while len ( agenda ) > 0 : 
~~~ key , value = agenda . pop ( 0 ) 
if key in used_agenda_keys : 
~~ used_agenda_keys . add ( key ) 
low , pos , index = key 
cost , g , h = value 
k += 1 
max_upperside_length = min ( len ( word ) - pos , transducer . max_up_length ) 
for upperside_length in range ( max_upperside_length + 1 ) : 
~~~ new_pos = pos + upperside_length 
curr_up = word [ pos : new_pos ] 
if curr_up not in transducer . operation_costs : 
~~ for curr_low , curr_cost in transducer . operation_costs [ curr_up ] . items ( ) : 
~~~ new_g = g + curr_cost 
~~~ if allow_spaces and trie . is_final ( index ) : 
~~~ new_index = trie . root 
~~~ new_index = Trie . NO_NODE 
~~~ new_index = trie . descend ( index , curr_low ) 
~~ if new_index is Trie . NO_NODE : 
~~ new_low = low + curr_low 
new_h = self . h_func ( word [ new_pos : ] , new_index ) 
new_cost = new_g + new_h 
if new_cost > d : 
~~ new_key = ( new_low , new_pos , new_index ) 
new_value = ( new_cost , new_g , new_h ) 
if new_pos == len ( word ) and trie . is_final ( new_index ) : 
~~~ old_g = answer . get ( new_low , None ) 
if old_g is None or new_g < old_g : 
~~~ answer [ new_low ] = new_g 
~~ ~~ agenda . add ( ( new_key , new_value ) ) 
~~ ~~ ~~ answer = sorted ( answer . items ( ) , key = ( lambda x : x [ 1 ] ) ) 
if return_cost : 
~~~ return [ elem [ 0 ] for elem in answer ] 
~~ ~~ def _precompute_euristics ( self ) : 
if self . euristics is None : 
~~ removal_costs = { a : np . inf for a in self . alphabet } 
insertion_costs = { a : np . inf for a in self . alphabet } 
if self . allow_spaces : 
~~ for up , costs in self . transducer . operation_costs . items ( ) : 
~~~ for low , cost in costs . items ( ) : 
~~~ if up == low : 
~~ if up != '' : 
~~~ removal_cost = cost / len ( up ) 
for a in up : 
~~~ removal_costs [ a ] = min ( removal_costs [ a ] , removal_cost ) 
~~ ~~ if low != '' : 
~~~ insertion_cost = cost / len ( low ) 
for a in low : 
~~~ insertion_costs [ a ] = min ( insertion_costs [ a ] , insertion_cost ) 
~~ ~~ ~~ ~~ self . _absense_costs_by_node = _precompute_absense_costs ( 
self . dictionary , removal_costs , insertion_costs , 
self . euristics , self . allow_spaces ) 
self . _temporary_euristics = [ dict ( ) for i in range ( len ( self . dictionary ) ) ] 
~~ def _euristic_h_function ( self , suffix , index ) : 
if self . euristics > 0 : 
~~~ suffix = suffix [ : self . euristics ] 
~~ index_temporary_euristics = self . _temporary_euristics [ index ] 
cost = index_temporary_euristics . get ( suffix , None ) 
if cost is not None : 
~~~ return cost 
~~ absense_costs = self . _absense_costs_by_node [ index ] 
data = self . dictionary . data [ index ] 
costs = np . zeros ( dtype = np . float64 , shape = ( self . euristics , ) ) 
for i , a in enumerate ( suffix ) : 
~~~ costs [ i : ] += absense_costs [ a ] [ i : ] 
~~ cost = max ( costs ) 
index_temporary_euristics [ suffix ] = cost 
return cost 
~~ def get_operation_cost ( self , up , low ) : 
up_costs = self . operation_costs . get ( up , None ) 
if up_costs is None : 
~~~ return np . inf 
~~ cost = up_costs . get ( low , np . inf ) 
~~ def inverse ( self ) : 
inversed_transducer = SegmentTransducer ( self . alphabet , operation_costs = dict ( ) ) 
inversed_transducer . operation_costs = self . _reversed_operation_costs 
inversed_transducer . _reversed_operation_costs = self . operation_costs 
inversed_transducer . max_low_length = self . max_up_length 
inversed_transducer . max_up_length = self . max_low_length 
inversed_transducer . max_low_lengths_by_up = self . max_up_lengths_by_low 
inversed_transducer . max_up_lengths_by_low = self . max_low_lengths_by_up 
return inversed_transducer 
~~ def distance ( self , first , second , return_transduction = False ) : 
if return_transduction : 
~~~ add_pred = ( lambda x , y : ( y == np . inf or x < y ) ) 
~~~ add_pred = ( lambda x , y : ( y == np . inf or x <= y ) ) 
~~ clear_pred = ( lambda x , y : ( y < np . inf and x < y ) ) 
update_func = lambda x , y : min ( x , y ) 
costs , backtraces = self . _fill_levenshtein_table ( first , second , 
update_func , add_pred , clear_pred ) 
final_cost = costs [ - 1 ] [ - 1 ] 
if final_cost == np . inf : 
~~~ transductions = [ None ] 
~~ elif return_transduction : 
~~~ transductions = self . _backtraces_to_transductions ( first , second , backtraces , 
final_cost , return_cost = False ) 
~~ if return_transduction : 
~~~ return final_cost , transductions 
~~~ return final_cost 
~~ ~~ def transduce ( self , first , second , threshold ) : 
add_pred = ( lambda x , y : x <= threshold ) 
clear_pred = ( lambda x , y : False ) 
update_func = ( lambda x , y : min ( x , y ) ) 
update_func , add_pred , clear_pred , 
threshold = threshold ) 
result = self . _backtraces_to_transductions ( first , second , 
backtraces , threshold , return_cost = True ) 
~~ def lower_transductions ( self , word , max_cost , return_cost = True ) : 
prefixes = [ [ ] for i in range ( len ( word ) + 1 ) ] 
prefixes [ 0 ] . append ( ( ( ) , 0.0 ) ) 
for pos in range ( len ( prefixes ) ) : 
~~~ prefixes [ pos ] = self . _perform_insertions ( prefixes [ pos ] , max_cost ) 
max_upperside_length = min ( len ( word ) - pos , self . max_up_length ) 
for upperside_length in range ( 1 , max_upperside_length + 1 ) : 
~~~ up = word [ pos : pos + upperside_length ] 
for low , low_cost in self . operation_costs . get ( up , dict ( ) ) . items ( ) : 
~~~ for transduction , cost in prefixes [ pos ] : 
~~~ new_cost = cost + low_cost 
if new_cost <= max_cost : 
~~~ new_transduction = transduction + ( up , low ) 
prefixes [ pos + upperside_length ] . append ( ( new_transduction , new_cost ) ) 
~~ ~~ ~~ ~~ ~~ answer = sorted ( prefixes [ - 1 ] , key = ( lambda x : x [ 0 ] ) ) 
~~ ~~ def _fill_levenshtein_table ( self , first , second , update_func , add_pred , clear_pred , 
threshold = None ) : 
m , n = len ( first ) , len ( second ) 
if threshold is None : 
~~~ threshold = 0.0 
for a , b in zip ( first , second ) : 
~~~ threshold += self . get_operation_cost ( a , b ) 
~~ if m > n : 
~~~ for a in first [ n : ] : 
~~~ threshold += self . get_operation_cost ( a , '' ) 
~~ ~~ elif m < n : 
~~~ for b in second [ m : ] : 
~~~ threshold += self . get_operation_cost ( '' , b ) 
~~ ~~ threshold *= 2 
~~ costs = np . zeros ( shape = ( m + 1 , n + 1 ) , dtype = np . float64 ) 
costs [ : ] = np . inf 
backtraces = [ None ] * ( m + 1 ) 
for i in range ( m + 1 ) : 
~~~ backtraces [ i ] = [ [ ] for j in range ( n + 1 ) ] 
~~ costs [ 0 ] [ 0 ] = 0.0 
~~~ for i_right in range ( i , min ( i + self . max_up_length , m ) + 1 ) : 
~~~ up = first [ i : i_right ] 
max_low_length = self . max_low_lengths_by_up . get ( up , - 1 ) 
~~ up_costs = self . operation_costs [ up ] 
for j in range ( n + 1 ) : 
~~~ if costs [ i ] [ j ] > threshold : 
~~ if len ( backtraces [ i ] [ j ] ) == 0 and i + j > 0 : 
~~ for j_right in range ( ( j if i_right > i else j + 1 ) , 
min ( j + max_low_length , n ) + 1 ) : 
~~~ low = second [ j : j_right ] 
curr_cost = up_costs . get ( low , np . inf ) 
old_cost = costs [ i_right ] [ j_right ] 
new_cost = costs [ i ] [ j ] + curr_cost 
if new_cost > threshold : 
~~ if add_pred ( new_cost , old_cost ) : 
~~~ if clear_pred ( new_cost , old_cost ) : 
~~~ backtraces [ i_right ] [ j_right ] = [ ] 
~~ costs [ i_right ] [ j_right ] = update_func ( new_cost , old_cost ) 
backtraces [ i_right ] [ j_right ] . append ( ( i , j ) ) 
~~ ~~ ~~ ~~ ~~ return costs , backtraces 
~~ def _make_reversed_operation_costs ( self ) : 
_reversed_operation_costs = dict ( ) 
for up , costs in self . operation_costs . items ( ) : 
~~~ if low not in _reversed_operation_costs : 
~~~ _reversed_operation_costs [ low ] = dict ( ) 
~~ _reversed_operation_costs [ low ] [ up ] = cost 
~~ ~~ self . _reversed_operation_costs = _reversed_operation_costs 
~~ def _make_maximal_key_lengths ( self ) : 
self . max_up_length = ( max ( len ( up ) for up in self . operation_costs ) 
if len ( self . operation_costs ) > 0 else - 1 ) 
self . max_low_length = ( max ( len ( low ) for low in self . _reversed_operation_costs ) 
if len ( self . _reversed_operation_costs ) > 0 else - 1 ) 
self . max_low_lengths_by_up , self . max_up_lengths_by_low = dict ( ) , dict ( ) 
~~~ self . max_low_lengths_by_up [ up ] = max ( len ( low ) for low in costs ) if len ( costs ) > 0 else - 1 
~~ for low , costs in self . _reversed_operation_costs . items ( ) : 
~~~ self . max_up_lengths_by_low [ low ] = max ( len ( up ) for up in costs ) if len ( costs ) > 0 else - 1 
~~ ~~ def _backtraces_to_transductions ( self , first , second , backtraces , threshold , return_cost = False ) : 
agenda = [ None ] * ( m + 1 ) 
~~~ agenda [ i ] = [ [ ] for j in range ( n + 1 ) ] 
~~ agenda [ m ] [ n ] = [ ( ( ) , 0.0 ) ] 
for i_right in range ( m , - 1 , - 1 ) : 
~~~ for j_right in range ( n , - 1 , - 1 ) : 
~~~ current_agenda = agenda [ i_right ] [ j_right ] 
if len ( current_agenda ) == 0 : 
~~ for ( i , j ) in backtraces [ i_right ] [ j_right ] : 
~~~ up , low = first [ i : i_right ] , second [ j : j_right ] 
add_cost = self . operation_costs [ up ] [ low ] 
for elem , cost in current_agenda : 
~~~ new_cost = cost + add_cost 
~~~ agenda [ i ] [ j ] . append ( ( ( ( up , low ) , ) + elem , new_cost ) ) 
~~ ~~ ~~ ~~ ~~ if return_cost : 
~~~ return agenda [ 0 ] [ 0 ] 
~~~ return [ elem [ 0 ] for elem in agenda [ 0 ] [ 0 ] ] 
~~ ~~ def _perform_insertions ( self , initial , max_cost ) : 
queue = list ( initial ) 
final = initial 
while len ( queue ) > 0 : 
~~~ transduction , cost = queue [ 0 ] 
queue = queue [ 1 : ] 
for string , string_cost in self . operation_costs [ "" ] . items ( ) : 
~~~ new_cost = cost + string_cost 
~~~ new_transduction = transduction + ( "" , string ) 
final . append ( ( new_transduction , new_cost ) ) 
queue . append ( ( new_transduction , new_cost ) ) 
~~ ~~ ~~ return final 
~~ def _make_default_operation_costs ( self , allow_spaces = False ) : 
self . operation_costs = dict ( ) 
for a in self . alphabet : 
~~~ current_costs = { c : 1.0 for c in self . alphabet } 
current_costs [ a ] = 0.0 
current_costs [ "" ] = 1.0 
~~ self . operation_costs [ a ] = current_costs 
~~ for a , b in itertools . permutations ( self . alphabet , 2 ) : 
~~~ self . operation_costs [ a + b ] = { b + a : 1.0 } 
~~ if allow_spaces : 
~~ ~~ def _start_timer ( self ) -> None : 
self . timer = Timer ( self . config [ 'conversation_lifetime' ] , self . self_destruct_callback ) 
self . timer . start ( ) 
~~ def handle_request ( self , request : dict ) -> dict : 
request_type = request [ 'request' ] [ 'type' ] 
request_id = request [ 'request' ] [ 'requestId' ] 
if request_type in self . handled_requests . keys ( ) : 
~~~ response : dict = self . handled_requests [ request_type ] ( request ) 
~~~ response : dict = self . handled_requests [ '_unsupported' ] ( request ) 
~~ self . _rearm_self_destruct ( ) 
~~ def _act ( self , utterance : str ) -> list : 
if self . stateful : 
~~~ utterance = [ [ utterance ] , [ self . key ] ] 
~~~ utterance = [ [ utterance ] ] 
~~ agent_response : list = self . agent ( * utterance ) 
return agent_response 
~~ def _generate_response ( self , response : dict , request : dict ) -> dict : 
response_template = deepcopy ( self . response_template ) 
response_template [ 'sessionAttributes' ] [ 'sessionId' ] = request [ 'session' ] [ 'sessionId' ] 
for key , value in response_template . items ( ) : 
~~~ if key not in response . keys ( ) : 
~~~ response [ key ] = value 
~~ ~~ return response 
~~ def _handle_intent ( self , request : dict ) -> dict : 
intent_name = self . config [ 'intent_name' ] 
slot_name = self . config [ 'slot_name' ] 
request_intent : dict = request [ 'request' ] [ 'intent' ] 
if intent_name != request_intent [ 'name' ] : 
~~ if slot_name not in request_intent [ 'slots' ] . keys ( ) : 
~~ utterance = request_intent [ 'slots' ] [ slot_name ] [ 'value' ] 
agent_response = self . _act ( utterance ) 
if not agent_response : 
~~ prediction : RichMessage = agent_response [ 0 ] 
prediction : list = prediction . alexa ( ) 
if not prediction : 
~~ response = self . _generate_response ( prediction [ 0 ] , request ) 
~~ def _handle_launch ( self , request : dict ) -> dict : 
'shouldEndSession' : False , 
'outputSpeech' : { 
'type' : 'PlainText' , 
'text' : self . config [ 'start_message' ] 
'card' : { 
'type' : 'Simple' , 
'content' : self . config [ 'start_message' ] 
response = self . _generate_response ( response , request ) 
~~ def _handle_unsupported ( self , request : dict ) -> dict : 
'text' : self . config [ 'unsupported_message' ] 
'content' : self . config [ 'unsupported_message' ] 
~~ def _repr_pretty_ ( self , p , cycle ) : 
if cycle : 
~~~ p . text ( 'Struct(...)' ) 
~~~ with p . group ( 7 , 'Struct(' , ')' ) : 
~~~ p . pretty ( self . _asdict ( ) ) 
~~ ~~ ~~ def elmo_loss2ppl ( losses : List [ np . ndarray ] ) -> float : 
avg_loss = np . mean ( losses ) 
return float ( np . exp ( avg_loss ) ) 
~~ def _build_loss ( self , lstm_outputs ) : 
batch_size = self . options [ 'batch_size' ] 
unroll_steps = self . options [ 'unroll_steps' ] 
n_tokens_vocab = self . options [ 'n_tokens_vocab' ] 
def _get_next_token_placeholders ( suffix ) : 
~~~ name = 'next_token_id' + suffix 
id_placeholder = tf . placeholder ( DTYPE_INT , 
shape = ( batch_size , unroll_steps ) , 
name = name ) 
return id_placeholder 
~~ self . next_token_id = _get_next_token_placeholders ( '' ) 
if self . bidirectional : 
~~~ self . next_token_id_reverse = _get_next_token_placeholders ( 
'_reverse' ) 
~~ softmax_dim = self . options [ 'lstm' ] [ 'projection_dim' ] 
if self . share_embedding_softmax : 
~~~ self . softmax_W = self . embedding_weights 
~~ with tf . variable_scope ( 'softmax' ) , tf . device ( '/cpu:0' ) : 
~~~ softmax_init = tf . random_normal_initializer ( 0.0 , 1.0 / np . sqrt ( softmax_dim ) ) 
if not self . share_embedding_softmax : 
~~~ self . softmax_W = tf . get_variable ( 
'W' , [ n_tokens_vocab , softmax_dim ] , 
initializer = softmax_init 
~~ self . softmax_b = tf . get_variable ( 
'b' , [ n_tokens_vocab ] , 
~~ self . individual_train_losses = [ ] 
self . individual_eval_losses = [ ] 
~~~ next_ids = [ self . next_token_id , self . next_token_id_reverse ] 
~~~ next_ids = [ self . next_token_id ] 
~~ for id_placeholder , lstm_output_flat in zip ( next_ids , lstm_outputs ) : 
~~~ next_token_id_flat = tf . reshape ( id_placeholder , [ - 1 , 1 ] ) 
with tf . control_dependencies ( [ lstm_output_flat ] ) : 
~~~ sampled_losses = tf . nn . sampled_softmax_loss ( self . softmax_W , self . softmax_b , 
next_token_id_flat , lstm_output_flat , 
self . options [ 'n_negative_samples_batch' ] , 
self . options [ 'n_tokens_vocab' ] , 
num_true = 1 ) 
output_scores = tf . matmul ( 
lstm_output_flat , 
tf . transpose ( self . softmax_W ) 
) + self . softmax_b 
losses = tf . nn . sparse_softmax_cross_entropy_with_logits ( 
logits = output_scores , 
labels = tf . squeeze ( next_token_id_flat , squeeze_dims = [ 1 ] ) 
~~ sampled_losses = tf . reshape ( sampled_losses , [ self . options [ 'batch_size' ] , - 1 ] ) 
losses = tf . reshape ( losses , [ self . options [ 'batch_size' ] , - 1 ] ) 
self . individual_train_losses . append ( tf . reduce_mean ( sampled_losses , axis = 1 ) ) 
self . individual_eval_losses . append ( tf . reduce_mean ( losses , axis = 1 ) ) 
~~ if self . bidirectional : 
~~~ self . total_train_loss = 0.5 * ( self . individual_train_losses [ 0 ] + self . individual_train_losses [ 1 ] ) 
self . total_eval_loss = 0.5 * ( self . individual_eval_losses [ 0 ] + self . individual_eval_losses [ 1 ] ) 
~~~ self . total_train_loss = self . individual_train_losses [ 0 ] 
self . total_eval_loss = self . individual_eval_losses [ 0 ] 
~~ ~~ def build_model ( config : Union [ str , Path , dict ] , mode : str = 'infer' , 
load_trained : bool = False , download : bool = False , 
serialized : Optional [ bytes ] = None ) -> Chainer : 
if serialized : 
~~~ serialized : list = pickle . loads ( serialized ) 
~~ if download : 
~~ import_packages ( config . get ( 'metadata' , { } ) . get ( 'imports' , [ ] ) ) 
model_config = config [ 'chainer' ] 
model = Chainer ( model_config [ 'in' ] , model_config [ 'out' ] , model_config . get ( 'in_y' ) ) 
for component_config in model_config [ 'pipe' ] : 
~~~ if load_trained and ( 'fit_on' in component_config or 'in_y' in component_config ) : 
~~~ component_config [ 'load_path' ] = component_config [ 'save_path' ] 
. format ( component_config . get ( 'class_name' , component_config . get ( 'ref' , 'UNKNOWN' ) ) ) ) 
~~ ~~ if serialized and 'in' in component_config : 
~~~ component_serialized = serialized . pop ( 0 ) 
~~~ component_serialized = None 
~~ component = from_params ( component_config , mode = mode , serialized = component_serialized ) 
if 'in' in component_config : 
~~~ c_in = component_config [ 'in' ] 
c_out = component_config [ 'out' ] 
in_y = component_config . get ( 'in_y' , None ) 
main = component_config . get ( 'main' , False ) 
model . append ( component , c_in , c_out , in_y , main ) 
~~ ~~ return model 
~~ def interact_model ( config : Union [ str , Path , dict ] ) -> None : 
model = build_model ( config ) 
~~~ args = [ ] 
for in_x in model . in_x : 
~~~ args . append ( ( input ( '{}::' . format ( in_x ) ) , ) ) 
if args [ - 1 ] [ 0 ] in { 'exit' , 'stop' , 'quit' , 'q' } : 
~~ ~~ pred = model ( * args ) 
if len ( model . out_params ) > 1 : 
~~~ pred = zip ( * pred ) 
~~ print ( '>>' , * pred ) 
~~ ~~ def predict_on_stream ( config : Union [ str , Path , dict ] , batch_size : int = 1 , file_path : Optional [ str ] = None ) -> None : 
if file_path is None or file_path == '-' : 
~~~ if sys . stdin . isatty ( ) : 
~~ f = sys . stdin 
~~~ f = open ( file_path , encoding = 'utf8' ) 
~~ model : Chainer = build_model ( config ) 
args_count = len ( model . in_x ) 
~~~ batch = list ( ( l . strip ( ) for l in islice ( f , batch_size * args_count ) ) ) 
if not batch : 
~~ args = [ ] 
for i in range ( args_count ) : 
~~~ args . append ( batch [ i : : args_count ] ) 
~~ res = model ( * args ) 
if len ( model . out_params ) == 1 : 
~~~ res = [ res ] 
~~ for res in zip ( * res ) : 
~~~ res = json . dumps ( res , ensure_ascii = False ) 
print ( res , flush = True ) 
~~ ~~ if f is not sys . stdin : 
~~~ f . close ( ) 
~~ ~~ def read_infile ( infile : Union [ Path , str ] , from_words = False , 
word_column : int = WORD_COLUMN , pos_column : int = POS_COLUMN , 
tag_column : int = TAG_COLUMN , max_sents : int = - 1 , 
read_only_words : bool = False ) -> List [ Tuple [ List , Union [ List , None ] ] ] : 
answer , curr_word_sent , curr_tag_sent = [ ] , [ ] , [ ] 
if from_words : 
~~~ word_column , read_only_words = 0 , True 
~~ with open ( infile , "r" , encoding = "utf8" ) as fin : 
~~~ for line in fin : 
~~~ line = line . strip ( ) 
if line . startswith ( "#" ) : 
~~ if line == "" : 
~~~ if len ( curr_word_sent ) > 0 : 
~~~ if read_only_words : 
~~~ curr_tag_sent = None 
~~ answer . append ( ( curr_word_sent , curr_tag_sent ) ) 
~~ curr_tag_sent , curr_word_sent = [ ] , [ ] 
if len ( answer ) == max_sents : 
~~ continue 
~~ splitted = line . split ( "\\t" ) 
index = splitted [ 0 ] 
if not from_words and not index . isdigit ( ) : 
~~ curr_word_sent . append ( splitted [ word_column ] ) 
if not read_only_words : 
~~~ pos , tag = splitted [ pos_column ] , splitted [ tag_column ] 
tag = pos if tag == "_" else "{},{}" . format ( pos , tag ) 
curr_tag_sent . append ( tag ) 
~~ ~~ if len ( curr_word_sent ) > 0 : 
~~ def preprocess_data ( data : List [ Tuple [ List [ str ] , List [ str ] ] ] , to_lower : bool = True , 
append_case : str = "first" ) -> List [ Tuple [ List [ Tuple [ str ] ] , List [ str ] ] ] : 
new_data = [ ] 
for words , tags in data : 
~~~ new_words = [ process_word ( word , to_lower = to_lower , append_case = append_case ) 
for word in words ] 
new_tags = tags 
new_data . append ( ( new_words , new_tags ) ) 
~~ return new_data 
~~ def fn_from_str ( name : str ) -> Callable [ ... , Any ] : 
~~~ module_name , fn_name = name . split ( ':' ) 
. format ( name ) ) 
~~ return getattr ( importlib . import_module ( module_name ) , fn_name ) 
~~ def register_metric ( metric_name : str ) -> Callable [ ... , Any ] : 
def decorate ( fn ) : 
~~~ fn_name = fn . __module__ + ':' + fn . __name__ 
if metric_name in _REGISTRY and _REGISTRY [ metric_name ] != fn_name : 
. format ( metric_name ) ) 
~~ _REGISTRY [ metric_name ] = fn_name 
return fn 
~~ return decorate 
~~ def get_metric_by_name ( name : str ) -> Callable [ ... , Any ] : 
if name not in _REGISTRY : 
~~~ raise ConfigError ( f\ ) 
~~ return fn_from_str ( _REGISTRY [ name ] ) 
~~ def from_str ( cls , label : str ) -> int : 
label_norm = label . replace ( '1' , 'one' ) . upper ( ) 
if label_norm in cls . __members__ : 
~~~ return DecayType [ label_norm ] 
~~~ raise NotImplementedError 
~~ ~~ def fit ( self , * args ) : 
data = list ( zip ( * args ) ) 
self . save ( ) 
if self . _fit_batch_size is None : 
~~ bs = int ( self . _fit_batch_size ) 
data_len = len ( data ) 
num_batches = self . _fit_max_batches or ( ( data_len - 1 ) // bs + 1 ) 
avg_loss = 0. 
best_loss = float ( 'inf' ) 
lrs , losses = [ ] , [ ] 
_lr_find_schedule = DecayScheduler ( start_val = self . _fit_learning_rate [ 0 ] , 
end_val = self . _fit_learning_rate [ 1 ] , 
dec_type = "exponential" , 
num_it = num_batches ) 
self . _lr = _lr_find_schedule . start_val 
self . _mom = 0. 
self . _update_graph_variables ( learning_rate = self . _lr , momentum = self . _mom ) 
best_lr = _lr_find_schedule . start_val 
for i in range ( num_batches ) : 
~~~ batch_start = ( i * bs ) % data_len 
batch_end = batch_start + bs 
report = self . train_on_batch ( * zip ( * data [ batch_start : batch_end ] ) ) 
if not isinstance ( report , dict ) : 
~~~ report = { 'loss' : report } 
~~ avg_loss = self . _fit_beta * avg_loss + ( 1 - self . _fit_beta ) * report [ 'loss' ] 
smoothed_loss = avg_loss / ( 1 - self . _fit_beta ** ( i + 1 ) ) 
lrs . append ( self . _lr ) 
losses . append ( smoothed_loss ) 
if math . isnan ( smoothed_loss ) or ( smoothed_loss > 4 * best_loss ) : 
~~ if ( smoothed_loss < best_loss ) and ( i >= self . _fit_min_batches ) : 
~~~ best_loss = smoothed_loss 
best_lr = self . _lr 
~~ self . _lr = _lr_find_schedule . next_val ( ) 
self . _update_graph_variables ( learning_rate = self . _lr ) 
if i >= num_batches : 
~~ ~~ end_val = self . _get_best ( lrs , losses ) 
start_val = end_val 
if self . _lr_schedule . dec_type in ( DecayType . ONECYCLE , DecayType . TRAPEZOID ) : 
~~~ start_val = end_val / self . _fit_learning_rate_div 
~~ elif self . _lr_schedule . dec_type in ( DecayType . POLYNOMIAL , DecayType . EXPONENTIAL , 
DecayType . LINEAR , DecayType . COSINE ) : 
~~~ start_val = end_val 
end_val = end_val / self . _fit_learning_rate_div 
~~ self . _lr_schedule = DecayScheduler ( start_val = start_val , 
end_val = end_val , 
num_it = self . _lr_schedule . nb , 
dec_type = self . _lr_schedule . dec_type , 
extra = self . _lr_schedule . extra ) 
self . load ( ) 
self . _lr = self . _lr_schedule . start_val 
self . _mom = self . _mom_schedule . start_val 
return { 'smoothed_loss' : losses , 'learning_rate' : lrs } 
~~ def _get_best ( values : List [ float ] , losses : List [ float ] , 
max_loss_div : float = 0.9 , min_val_div : float = 10.0 ) -> float : 
min_ind = np . argmin ( losses ) 
for i in range ( min_ind - 1 , 0 , - 1 ) : 
~~~ if ( losses [ i ] * max_loss_div > losses [ min_ind ] ) or ( values [ i ] * min_val_div < values [ min_ind ] ) : 
~~~ return values [ i + 1 ] 
~~ ~~ return values [ min_ind ] / min_val_div 
if event_name == "after_validation" : 
~~~ if data [ 'impatience' ] > self . _learning_rate_last_impatience : 
~~~ self . _learning_rate_cur_impatience += 1 
~~~ self . _learning_rate_cur_impatience = 0 
~~ self . _learning_rate_last_impatience = data [ 'impatience' ] 
if ( self . _learning_rate_drop_patience is not None ) and ( self . _learning_rate_cur_impatience >= 
self . _learning_rate_drop_patience ) : 
self . _learning_rate_cur_div *= self . _learning_rate_drop_div 
self . _lr /= self . _learning_rate_drop_div 
~~ ~~ if event_name == 'after_batch' : 
~~~ if ( self . _lr is not None ) and self . _lr_update_on_batch : 
~~~ self . _lr = self . _lr_schedule . next_val ( ) / self . _learning_rate_cur_div 
~~ if ( self . _mom is not None ) and self . _mom_update_on_batch : 
~~~ self . _mom = min ( 1. , max ( 0. , self . _mom_schedule . next_val ( ) ) ) 
self . _update_graph_variables ( momentum = self . _mom ) 
~~ ~~ if event_name == 'after_epoch' : 
~~~ if ( self . _lr is not None ) and not self . _lr_update_on_batch : 
~~ if ( self . _mom is not None ) and not self . _mom_update_on_batch : 
~~ ~~ if event_name == 'after_train_log' : 
~~~ if ( self . _lr is not None ) and ( 'learning_rate' not in data ) : 
~~~ data [ 'learning_rate' ] = self . _lr 
~~ if ( self . _mom is not None ) and ( 'momentum' not in data ) : 
~~~ data [ 'momentum' ] = self . _mom 
~~ ~~ ~~ def _encode ( self , tokens : List [ str ] , mean : bool ) -> Union [ List [ np . ndarray ] , np . ndarray ] : 
embedded_tokens = [ ] 
for t in tokens : 
~~~ emb = self . tok2emb [ t ] 
~~~ emb = self . _get_word_vector ( t ) 
~~~ emb = np . zeros ( self . dim , dtype = np . float32 ) 
~~ self . tok2emb [ t ] = emb 
~~ embedded_tokens . append ( emb ) 
~~ if mean is None : 
~~~ mean = self . mean 
~~ if mean : 
~~~ filtered = [ et for et in embedded_tokens if np . any ( et ) ] 
if filtered : 
~~~ return np . mean ( filtered , axis = 0 ) 
~~ return np . zeros ( self . dim , dtype = np . float32 ) 
~~ return embedded_tokens 
~~ def read_requirements ( ) : 
reqs_path = os . path . join ( __location__ , 'requirements.txt' ) 
with open ( reqs_path , encoding = 'utf8' ) as f : 
~~~ reqs = [ line . strip ( ) for line in f if not line . strip ( ) . startswith ( '#' ) ] 
~~ names = [ ] 
links = [ ] 
for req in reqs : 
~~~ if '://' in req : 
~~~ links . append ( req ) 
~~~ names . append ( req ) 
~~ ~~ return { 'install_requires' : names , 'dependency_links' : links } 
~~ def detokenize ( tokens ) : 
step3 = re . sub ( r\ , r"\\1\\2" , step2 ) 
return step6 . strip ( ) 
~~ def ngramize ( items : List [ str ] , ngram_range = ( 1 , 1 ) ) -> Generator [ List [ str ] , Any , None ] : 
ngrams = [ ] 
ranges = [ ( 0 , i ) for i in range ( ngram_range [ 0 ] , ngram_range [ 1 ] + 1 ) ] 
for r in ranges : 
~~~ ngrams += list ( zip ( * [ items [ j : ] for j in range ( * r ) ] ) ) 
yield formatted_ngrams 
~~ def sk_log_loss ( y_true : Union [ List [ List [ float ] ] , List [ List [ int ] ] , np . ndarray ] , 
y_predicted : Union [ List [ List [ float ] ] , List [ List [ int ] ] , np . ndarray ] ) -> float : 
return log_loss ( y_true , y_predicted ) 
~~ def make_module_spec ( options , weight_file ) : 
def module_fn ( ) : 
_bos_id = 256 
_eos_id = 257 
_bow_id = 258 
_eow_id = 259 
_pad_id = 260 
_max_word_length = 50 
_parallel_iterations = 10 
_max_batch_size = 1024 
id_dtype = tf . int32 
id_nptype = np . int32 
max_word_length = tf . constant ( _max_word_length , dtype = id_dtype , name = 'max_word_length' ) 
version = tf . constant ( 'from_dp_1' , dtype = tf . string , name = 'version' ) 
def _make_bos_eos ( c ) : 
~~~ r = np . zeros ( [ _max_word_length ] , dtype = id_nptype ) 
r [ : ] = _pad_id 
r [ 0 ] = _bow_id 
r [ 1 ] = c 
r [ 2 ] = _eow_id 
return tf . constant ( r , dtype = id_dtype ) 
~~ bos_ids = _make_bos_eos ( _bos_id ) 
eos_ids = _make_bos_eos ( _eos_id ) 
def token2ids ( token ) : 
~~~ with tf . name_scope ( "token2ids_preprocessor" ) : 
~~~ char_ids = tf . decode_raw ( token , tf . uint8 , name = 'decode_raw2get_char_ids' ) 
char_ids = tf . cast ( char_ids , tf . int32 , name = 'cast2int_token' ) 
char_ids = tf . strided_slice ( char_ids , [ 0 ] , [ max_word_length - 2 ] , 
[ 1 ] , name = 'slice2resized_token' ) 
ids_num = tf . shape ( char_ids ) [ 0 ] 
fill_ids_num = ( _max_word_length - 2 ) - ids_num 
pads = tf . fill ( [ fill_ids_num ] , _pad_id ) 
bow_token_eow_pads = tf . concat ( [ [ _bow_id ] , char_ids , [ _eow_id ] , pads ] , 
0 , name = 'concat2bow_token_eow_pads' ) 
return bow_token_eow_pads 
~~ ~~ def sentence_tagging_and_padding ( sen_dim ) : 
~~~ with tf . name_scope ( "sentence_tagging_and_padding_preprocessor" ) : 
~~~ sen = sen_dim [ 0 ] 
dim = sen_dim [ 1 ] 
extra_dim = tf . shape ( sen ) [ 0 ] - dim 
sen = tf . slice ( sen , [ 0 , 0 ] , [ dim , max_word_length ] , name = 'slice2sen' ) 
bos_sen_eos = tf . concat ( [ [ bos_ids ] , sen , [ eos_ids ] ] , 0 , name = 'concat2bos_sen_eos' ) 
bos_sen_eos_plus_one = bos_sen_eos + 1 
bos_sen_eos_pads = tf . pad ( bos_sen_eos_plus_one , [ [ 0 , extra_dim ] , [ 0 , 0 ] ] , 
"CONSTANT" , name = 'pad2bos_sen_eos_pads' ) 
return bos_sen_eos_pads 
~~ ~~ tokens = tf . placeholder ( shape = ( None , None ) , dtype = tf . string , name = 'ph2tokens' ) 
sequence_len = tf . placeholder ( shape = ( None , ) , dtype = tf . int32 , name = 'ph2sequence_len' ) 
tok_shape = tf . shape ( tokens ) 
line_tokens = tf . reshape ( tokens , shape = [ - 1 ] , name = 'reshape2line_tokens' ) 
with tf . device ( '/cpu:0' ) : 
~~~ tok_ids = tf . map_fn ( 
token2ids , 
line_tokens , 
dtype = tf . int32 , back_prop = False , parallel_iterations = _parallel_iterations , 
name = 'map_fn2get_tok_ids' ) 
~~ tok_ids = tf . reshape ( tok_ids , [ tok_shape [ 0 ] , tok_shape [ 1 ] , - 1 ] , name = 'reshape2tok_ids' ) 
~~~ sen_ids = tf . map_fn ( 
sentence_tagging_and_padding , 
( tok_ids , sequence_len ) , 
name = 'map_fn2get_sen_ids' ) 
~~ bilm = BidirectionalLanguageModel ( options , str ( weight_file ) , 
max_batch_size = _max_batch_size ) 
embeddings_op = bilm ( sen_ids ) 
elmo_output = weight_layers ( 'elmo_output' , embeddings_op , l2_coef = 0.0 ) 
weighted_op = elmo_output [ 'weighted_op' ] 
mean_op = elmo_output [ 'mean_op' ] 
word_emb = elmo_output [ 'word_emb' ] 
lstm_outputs1 = elmo_output [ 'lstm_outputs1' ] 
lstm_outputs2 = elmo_output [ 'lstm_outputs2' ] 
hub . add_signature ( "tokens" , { "tokens" : tokens , "sequence_len" : sequence_len } , 
{ "elmo" : weighted_op , 
"default" : mean_op , 
"word_emb" : word_emb , 
"lstm_outputs1" : lstm_outputs1 , 
"lstm_outputs2" : lstm_outputs2 , 
"version" : version } ) 
def_strings = tf . placeholder ( shape = ( None ) , dtype = tf . string ) 
def_tokens_sparse = tf . string_split ( def_strings ) 
def_tokens_dense = tf . sparse_to_dense ( sparse_indices = def_tokens_sparse . indices , 
output_shape = def_tokens_sparse . dense_shape , 
sparse_values = def_tokens_sparse . values , 
default_value = '' 
def_mask = tf . not_equal ( def_tokens_dense , '' ) 
def_int_mask = tf . cast ( def_mask , dtype = tf . int32 ) 
def_sequence_len = tf . reduce_sum ( def_int_mask , axis = - 1 ) 
def_tok_shape = tf . shape ( def_tokens_dense ) 
def_line_tokens = tf . reshape ( def_tokens_dense , shape = [ - 1 ] , name = 'reshape2line_tokens' ) 
~~~ def_tok_ids = tf . map_fn ( 
def_line_tokens , 
~~ def_tok_ids = tf . reshape ( def_tok_ids , [ def_tok_shape [ 0 ] , def_tok_shape [ 1 ] , - 1 ] , name = 'reshape2tok_ids' ) 
~~~ def_sen_ids = tf . map_fn ( 
( def_tok_ids , def_sequence_len ) , 
~~ def_embeddings_op = bilm ( def_sen_ids ) 
def_elmo_output = weight_layers ( 'elmo_output' , def_embeddings_op , l2_coef = 0.0 , reuse = True ) 
def_weighted_op = def_elmo_output [ 'weighted_op' ] 
def_mean_op = def_elmo_output [ 'mean_op' ] 
def_word_emb = def_elmo_output [ 'word_emb' ] 
def_lstm_outputs1 = def_elmo_output [ 'lstm_outputs1' ] 
def_lstm_outputs2 = def_elmo_output [ 'lstm_outputs2' ] 
hub . add_signature ( "default" , { "strings" : def_strings } , 
{ "elmo" : def_weighted_op , 
"default" : def_mean_op , 
"word_emb" : def_word_emb , 
"lstm_outputs1" : def_lstm_outputs1 , 
"lstm_outputs2" : def_lstm_outputs2 , 
~~ return hub . create_module_spec ( module_fn ) 
~~ def export2hub ( weight_file , hub_dir , options ) : 
spec = make_module_spec ( options , str ( weight_file ) ) 
~~~ with tf . Graph ( ) . as_default ( ) : 
~~~ module = hub . Module ( spec ) 
with tf . Session ( ) as sess : 
~~~ sess . run ( tf . global_variables_initializer ( ) ) 
if hub_dir . exists ( ) : 
~~~ shutil . rmtree ( hub_dir ) 
~~ module . export ( str ( hub_dir ) , sess ) 
~~ ~~ ~~ finally : 
~~ ~~ def show_details ( item_data : Dict [ Any , Any ] ) -> str : 
txt = "" 
for key , value in item_data . items ( ) : 
~~ return txt 
~~ def make_agent ( ) -> EcommerceAgent : 
config_path = find_config ( 'tfidf_retrieve' ) 
skill = build_model ( config_path ) 
agent = EcommerceAgent ( skills = [ skill ] ) 
run_ms_bot_framework_server ( agent_generator = make_agent , 
app_id = args . ms_id , 
app_secret = args . ms_secret , 
stateful = True ) 
~~ def _call ( self , utterances_batch : List [ str ] , utterances_ids : List [ int ] = None ) -> List [ RichMessage ] : 
rich_message = RichMessage ( ) 
for utt_id , utt in enumerate ( utterances_batch ) : 
~~~ if utterances_ids : 
~~~ id_ = utterances_ids [ utt_id ] 
if utt == "/start" : 
rich_message . add_control ( PlainText ( welcome ) ) 
~~ if utt [ 0 ] == "@" : 
~~~ command , * parts = utt . split ( ":" ) 
if command == "@details" : 
rich_message . add_control ( PlainText ( show_details ( 
self . history [ id_ ] [ batch_index ] [ item_index ] ) ) ) 
~~ if command == "@entropy" : 
~~~ state = self . history [ id_ ] [ int ( parts [ 0 ] ) ] 
state [ parts [ 1 ] ] = parts [ 2 ] 
state [ "start" ] = 0 
state [ "stop" ] = 5 
utt = state [ 'query' ] 
self . states [ id_ ] = state 
~~ if command == "@next" : 
state [ 'start' ] = state [ 'stop' ] 
state [ 'stop' ] = state [ 'stop' ] + 5 
~~~ if id_ not in self . states : 
~~~ self . states [ id_ ] = { } 
~~ self . states [ id_ ] [ "start" ] = 0 
self . states [ id_ ] [ "stop" ] = 5 
~~ responses_batch , confidences_batch , state_batch = self . skills [ 0 ] ( 
[ utt ] , self . history [ id_ ] , [ self . states [ id_ ] ] ) 
self . states [ id_ ] = state_batch [ 0 ] 
self . states [ id_ ] [ "query" ] = utt 
items_batch , entropy_batch = responses_batch 
for batch_idx , items in enumerate ( items_batch ) : 
~~~ self . history [ id_ ] . append ( items ) 
self . history [ id_ ] . append ( self . states [ id_ ] ) 
for idx , item in enumerate ( items ) : 
~~~ rich_message . add_control ( _draw_item ( item , idx , self . history [ id_ ] ) ) 
~~ if len ( items ) == self . states [ id_ ] [ 'stop' ] - self . states [ id_ ] [ 'start' ] : 
~~~ buttons_frame = _draw_tail ( entropy_batch [ batch_idx ] , self . history [ id_ ] ) 
rich_message . add_control ( buttons_frame ) 
~~ ~~ ~~ return [ rich_message ] 
~~ def TemporalDropout ( inputs , dropout = 0.0 ) : 
if dropout == 0.0 : 
~~~ return inputs 
~~ inputs_func = lambda x : kb . ones_like ( inputs [ : , : , 0 : 1 ] ) 
inputs_mask = kl . Lambda ( inputs_func ) ( inputs ) 
inputs_mask = kl . Dropout ( dropout ) ( inputs_mask ) 
tiling_shape = [ 1 , 1 , kb . shape ( inputs ) [ 2 ] ] + [ 1 ] * ( kb . ndim ( inputs ) - 3 ) 
inputs_mask = kl . Lambda ( kb . tile , arguments = { "n" : tiling_shape } , 
output_shape = inputs . _keras_shape [ 1 : ] ) ( inputs_mask ) 
answer = kl . Multiply ( ) ( [ inputs , inputs_mask ] ) 
return answer 
~~ def positions_func ( inputs , pad = 0 ) : 
position_inputs = kb . cumsum ( kb . ones_like ( inputs , dtype = "float32" ) , axis = 1 ) 
position_inputs *= kb . cast ( kb . not_equal ( inputs , pad ) , "float32" ) 
return kb . log ( 1.0 + position_inputs ) 
~~ def download ( dest_file_path : [ List [ Union [ str , Path ] ] ] , source_url : str , force_download = True ) : 
if isinstance ( dest_file_path , list ) : 
~~~ dest_file_paths = [ Path ( path ) for path in dest_file_path ] 
~~~ dest_file_paths = [ Path ( dest_file_path ) . absolute ( ) ] 
~~ if not force_download : 
~~~ to_check = list ( dest_file_paths ) 
dest_file_paths = [ ] 
for p in to_check : 
~~~ if p . exists ( ) : 
~~~ dest_file_paths . append ( p ) 
~~ ~~ ~~ if dest_file_paths : 
~~~ cache_dir = os . getenv ( 'DP_CACHE_DIR' ) 
cached_exists = False 
if cache_dir : 
~~~ first_dest_path = Path ( cache_dir ) / md5 ( source_url . encode ( 'utf8' ) ) . hexdigest ( ) [ : 15 ] 
cached_exists = first_dest_path . exists ( ) 
~~~ first_dest_path = dest_file_paths . pop ( ) 
~~ if not cached_exists : 
~~~ first_dest_path . parent . mkdir ( parents = True , exist_ok = True ) 
simple_download ( source_url , first_dest_path ) 
~~ for dest_path in dest_file_paths : 
~~~ dest_path . parent . mkdir ( parents = True , exist_ok = True ) 
shutil . copy ( str ( first_dest_path ) , str ( dest_path ) ) 
~~ ~~ ~~ def untar ( file_path , extract_folder = None ) : 
file_path = Path ( file_path ) 
if extract_folder is None : 
~~~ extract_folder = file_path . parent 
~~ extract_folder = Path ( extract_folder ) 
tar = tarfile . open ( file_path ) 
tar . extractall ( extract_folder ) 
tar . close ( ) 
~~ def ungzip ( file_path , extract_path : Path = None ) : 
CHUNK = 16 * 1024 
extract_path = extract_path or file_path . with_suffix ( '' ) 
with gzip . open ( file_path , 'rb' ) as fin , extract_path . open ( 'wb' ) as fout : 
~~~ while True : 
~~~ block = fin . read ( CHUNK ) 
if not block : 
~~ fout . write ( block ) 
~~ ~~ ~~ def download_decompress ( url : str , download_path : [ Path , str ] , extract_paths = None ) : 
file_name = Path ( urlparse ( url ) . path ) . name 
download_path = Path ( download_path ) 
if extract_paths is None : 
~~~ extract_paths = [ download_path ] 
~~ elif isinstance ( extract_paths , list ) : 
~~~ extract_paths = [ Path ( path ) for path in extract_paths ] 
~~~ extract_paths = [ Path ( extract_paths ) ] 
~~ cache_dir = os . getenv ( 'DP_CACHE_DIR' ) 
extracted = False 
~~~ cache_dir = Path ( cache_dir ) 
url_hash = md5 ( url . encode ( 'utf8' ) ) . hexdigest ( ) [ : 15 ] 
arch_file_path = cache_dir / url_hash 
extracted_path = cache_dir / ( url_hash + '_extracted' ) 
extracted = extracted_path . exists ( ) 
if not extracted and not arch_file_path . exists ( ) : 
~~~ simple_download ( url , arch_file_path ) 
~~~ arch_file_path = download_path / file_name 
simple_download ( url , arch_file_path ) 
extracted_path = extract_paths . pop ( ) 
~~ if not extracted : 
extracted_path . mkdir ( parents = True , exist_ok = True ) 
if file_name . endswith ( '.tar.gz' ) : 
~~~ untar ( arch_file_path , extracted_path ) 
~~ elif file_name . endswith ( '.gz' ) : 
~~~ ungzip ( arch_file_path , extracted_path / Path ( file_name ) . with_suffix ( '' ) . name ) 
~~ elif file_name . endswith ( '.zip' ) : 
~~~ with zipfile . ZipFile ( arch_file_path , 'r' ) as zip_ref : 
~~~ zip_ref . extractall ( extracted_path ) 
~~ if not cache_dir : 
~~~ arch_file_path . unlink ( ) 
~~ ~~ for extract_path in extract_paths : 
~~~ for src in extracted_path . iterdir ( ) : 
~~~ dest = extract_path / src . name 
if src . is_dir ( ) : 
~~~ copytree ( src , dest ) 
~~~ extract_path . mkdir ( parents = True , exist_ok = True ) 
shutil . copy ( str ( src ) , str ( dest ) ) 
~~ ~~ ~~ ~~ def update_dict_recursive ( editable_dict : dict , editing_dict : dict ) -> None : 
for k , v in editing_dict . items ( ) : 
~~~ if isinstance ( v , collections . Mapping ) : 
~~~ update_dict_recursive ( editable_dict . get ( k , { } ) , v ) 
~~~ editable_dict [ k ] = v 
~~ ~~ ~~ def path_set_md5 ( url ) : 
scheme , netloc , path , query_string , fragment = urlsplit ( url ) 
path += '.md5' 
return urlunsplit ( ( scheme , netloc , path , query_string , fragment ) ) 
~~ def set_query_parameter ( url , param_name , param_value ) : 
query_params = parse_qs ( query_string ) 
query_params [ param_name ] = [ param_value ] 
new_query_string = urlencode ( query_params , doseq = True ) 
return urlunsplit ( ( scheme , netloc , path , new_query_string , fragment ) ) 
~~ def alexa ( self ) -> dict : 
'text' : self . content } , 
'content' : self . content 
~~ def json ( self ) -> dict : 
content = { } 
content [ 'name' ] = self . name 
content [ 'callback' ] = self . callback 
self . control_json [ 'content' ] = content 
return self . control_json 
~~ def ms_bot_framework ( self ) -> dict : 
card_action = { } 
card_action [ 'type' ] = 'postBack' 
card_action [ 'title' ] = self . name 
card_action [ 'value' ] = self . callback = self . callback 
return card_action 
if self . text : 
~~~ content [ 'text' ] = self . text 
~~ content [ 'controls' ] = [ control . json ( ) for control in self . content ] 
rich_card = { } 
buttons = [ button . ms_bot_framework ( ) for button in self . content ] 
rich_card [ 'buttons' ] = buttons 
~~~ rich_card [ 'title' ] = self . text 
~~ attachments = [ 
"contentType" : "application/vnd.microsoft.card.thumbnail" , 
"content" : rich_card 
out_activity = { } 
out_activity [ 'type' ] = 'message' 
out_activity [ 'attachments' ] = attachments 
return out_activity 
~~ def squad_v2_exact_match ( y_true : List [ List [ str ] ] , y_predicted : List [ str ] ) -> float : 
EM_total = sum ( normalize_answer ( prediction ) in map ( normalize_answer , ground_truth ) 
for ground_truth , prediction in zip ( y_true , y_predicted ) ) 
return 100 * EM_total / len ( y_true ) if len ( y_true ) > 0 else 0 
~~ def squad_v1_exact_match ( y_true : List [ List [ str ] ] , y_predicted : List [ str ] ) -> float : 
EM_total = 0 
count = 0 
for ground_truth , prediction in zip ( y_true , y_predicted ) : 
~~~ if len ( ground_truth [ 0 ] ) == 0 : 
~~ count += 1 
EMs = [ int ( normalize_answer ( gt ) == normalize_answer ( prediction ) ) for gt in ground_truth ] 
EM_total += max ( EMs ) 
~~ return 100 * EM_total / count if count > 0 else 0 
~~ def squad_v2_f1 ( y_true : List [ List [ str ] ] , y_predicted : List [ str ] ) -> float : 
f1_total = 0.0 
~~~ prediction_tokens = normalize_answer ( prediction ) . split ( ) 
f1s = [ ] 
for gt in ground_truth : 
~~~ gt_tokens = normalize_answer ( gt ) . split ( ) 
if len ( gt_tokens ) == 0 or len ( prediction_tokens ) == 0 : 
~~~ f1s . append ( float ( gt_tokens == prediction_tokens ) ) 
~~ common = Counter ( prediction_tokens ) & Counter ( gt_tokens ) 
num_same = sum ( common . values ( ) ) 
if num_same == 0 : 
~~~ f1s . append ( 0.0 ) 
~~ precision = 1.0 * num_same / len ( prediction_tokens ) 
recall = 1.0 * num_same / len ( gt_tokens ) 
f1 = ( 2 * precision * recall ) / ( precision + recall ) 
f1s . append ( f1 ) 
~~ f1_total += max ( f1s ) 
~~ return 100 * f1_total / len ( y_true ) if len ( y_true ) > 0 else 0 
~~ def recall_at_k ( y_true : List [ int ] , y_pred : List [ List [ np . ndarray ] ] , k : int ) : 
num_examples = float ( len ( y_pred ) ) 
predictions = np . array ( y_pred ) 
predictions = np . flip ( np . argsort ( predictions , - 1 ) , - 1 ) [ : , : k ] 
num_correct = 0 
for el in predictions : 
~~~ if 0 in el : 
~~~ num_correct += 1 
~~ ~~ return float ( num_correct ) / num_examples 
~~ def check_gpu_existence ( ) : 
global _gpu_available 
if _gpu_available is None : 
~~~ sess_config = tf . ConfigProto ( ) 
sess_config . gpu_options . allow_growth = True 
~~~ with tf . Session ( config = sess_config ) : 
~~~ device_list = device_lib . list_local_devices ( ) 
_gpu_available = any ( device . device_type == 'GPU' for device in device_list ) 
~~ ~~ except AttributeError as e : 
_gpu_available = False 
~~ ~~ return _gpu_available 
~~ def _parse_config_property ( item : _T , variables : Dict [ str , Union [ str , Path , float , bool , None ] ] ) -> _T : 
if isinstance ( item , str ) : 
~~~ return item . format ( ** variables ) 
~~ elif isinstance ( item , list ) : 
~~~ return [ _parse_config_property ( item , variables ) for item in item ] 
~~ elif isinstance ( item , dict ) : 
~~~ return { k : _parse_config_property ( v , variables ) for k , v in item . items ( ) } 
~~~ return item 
~~ ~~ def parse_config ( config : Union [ str , Path , dict ] ) -> dict : 
if isinstance ( config , ( str , Path ) ) : 
~~~ config = read_json ( find_config ( config ) ) 
~~ variables = { 
'DEEPPAVLOV_PATH' : os . getenv ( f'DP_DEEPPAVLOV_PATH' , Path ( __file__ ) . parent . parent . parent ) 
for name , value in config . get ( 'metadata' , { } ) . get ( 'variables' , { } ) . items ( ) : 
~~~ env_name = f'DP_{name}' 
if env_name in os . environ : 
~~~ value = os . getenv ( env_name ) 
~~ variables [ name ] = value . format ( ** variables ) 
~~ return _parse_config_property ( config , variables ) 
~~ def expand_path ( path : Union [ str , Path ] ) -> Path : 
return Path ( path ) . expanduser ( ) . resolve ( ) 
~~ def from_params ( params : Dict , mode : str = 'infer' , serialized : Any = None , ** kwargs ) -> Component : 
config_params = { k : _resolve ( v ) for k , v in params . items ( ) } 
if 'ref' in config_params : 
~~~ component = _refs [ config_params [ 'ref' ] ] 
if serialized is not None : 
~~~ component . deserialize ( serialized ) 
~~ return component 
~~~ e = ConfigError ( \ 
. format ( id = config_params [ 'ref' ] ) ) 
log . exception ( e ) 
~~ ~~ elif 'config_path' in config_params : 
~~~ from deeppavlov . core . commands . infer import build_model 
refs = _refs . copy ( ) 
_refs . clear ( ) 
config = parse_config ( expand_path ( config_params [ 'config_path' ] ) ) 
model = build_model ( config , serialized = serialized ) 
_refs . update ( refs ) 
~~~ _refs [ config_params [ 'id' ] ] = model 
~~ cls_name = config_params . pop ( 'class_name' , None ) 
if not cls_name : 
~~ cls = get_model ( cls_name ) 
config_params = { k : _init_param ( v , mode ) for k , v in config_params . items ( ) } 
~~~ spec = inspect . getfullargspec ( cls ) 
if 'mode' in spec . args + spec . kwonlyargs or spec . varkw is not None : 
~~~ kwargs [ 'mode' ] = mode 
~~ component = cls ( ** dict ( config_params , ** kwargs ) ) 
~~~ _refs [ config_params [ 'id' ] ] = component 
~~ ~~ except Exception : 
raise 
~~ if serialized is not None : 
~~ def run ( self ) -> None : 
~~~ request = self . input_queue . get ( ) 
response = self . _handle_request ( request ) 
self . output_queue . put ( response ) 
~~ ~~ def _del_conversation ( self , conversation_key : str ) -> None : 
if conversation_key in self . conversations . keys ( ) : 
~~~ del self . conversations [ conversation_key ] 
~~ ~~ def _refresh_valid_certs ( self ) -> None : 
self . timer = Timer ( REFRESH_VALID_CERTS_PERIOD_SECS , self . _refresh_valid_certs ) 
expired_certificates = [ ] 
for valid_cert_url , valid_cert in self . valid_certificates . items ( ) : 
~~~ valid_cert : ValidatedCert = valid_cert 
cert_expiration_time : datetime = valid_cert . expiration_timestamp 
if datetime . utcnow ( ) > cert_expiration_time : 
~~~ expired_certificates . append ( valid_cert_url ) 
~~ ~~ for expired_cert_url in expired_certificates : 
~~~ del self . valid_certificates [ expired_cert_url ] 
~~ ~~ def _verify_request ( self , signature_chain_url : str , signature : str , request_body : bytes ) -> bool : 
if signature_chain_url not in self . valid_certificates . keys ( ) : 
~~~ amazon_cert : X509 = verify_cert ( signature_chain_url ) 
if amazon_cert : 
~~~ amazon_cert_lifetime : timedelta = self . config [ 'amazon_cert_lifetime' ] 
expiration_timestamp = datetime . utcnow ( ) + amazon_cert_lifetime 
validated_cert = ValidatedCert ( cert = amazon_cert , expiration_timestamp = expiration_timestamp ) 
self . valid_certificates [ signature_chain_url ] = validated_cert 
return False 
~~~ validated_cert : ValidatedCert = self . valid_certificates [ signature_chain_url ] 
amazon_cert : X509 = validated_cert . cert 
~~ if verify_signature ( amazon_cert , signature , request_body ) : 
~~~ result = True 
~~~ log . error ( f\ ) 
result = False 
~~ def _handle_request ( self , request : dict ) -> dict : 
request_body : bytes = request [ 'request_body' ] 
signature_chain_url : str = request [ 'signature_chain_url' ] 
signature : str = request [ 'signature' ] 
alexa_request : dict = request [ 'alexa_request' ] 
if not self . _verify_request ( signature_chain_url , signature , request_body ) : 
~~ timestamp_str = alexa_request [ 'request' ] [ 'timestamp' ] 
timestamp_datetime = datetime . strptime ( timestamp_str , '%Y-%m-%dT%H:%M:%SZ' ) 
now = datetime . utcnow ( ) 
delta = now - timestamp_datetime if now >= timestamp_datetime else timestamp_datetime - now 
if abs ( delta . seconds ) > REQUEST_TIMESTAMP_TOLERANCE_SECS : 
~~ conversation_key = alexa_request [ 'session' ] [ 'user' ] [ 'userId' ] 
if conversation_key not in self . conversations . keys ( ) : 
~~~ if self . config [ 'multi_instance' ] : 
~~~ conv_agent = self . _init_agent ( ) 
~~~ conv_agent = self . agent 
~~ self . conversations [ conversation_key ] = Conversation ( config = self . config , 
agent = conv_agent , 
conversation_key = conversation_key , 
self_destruct_callback = lambda : self . _del_conversation ( conversation_key ) ) 
~~ conversation = self . conversations [ conversation_key ] 
response = conversation . handle_request ( alexa_request ) 
~~ def csoftmax_for_slice ( input ) : 
[ ten , u ] = input 
shape_t = ten . shape 
shape_u = u . shape 
ten -= tf . reduce_mean ( ten ) 
q = tf . exp ( ten ) 
active = tf . ones_like ( u , dtype = tf . int32 ) 
mass = tf . constant ( 0 , dtype = tf . float32 ) 
found = tf . constant ( True , dtype = tf . bool ) 
def loop ( q_ , mask , mass_ , found_ ) : 
~~~ q_list = tf . dynamic_partition ( q_ , mask , 2 ) 
p = q_list [ 1 ] * ( 1.0 - mass_ ) / tf . reduce_sum ( q_list [ 1 ] ) 
p_new = tf . dynamic_stitch ( condition_indices , [ q_list [ 0 ] , p ] ) 
condition_indices = tf . dynamic_partition ( tf . range ( tf . shape ( p_new ) [ 0 ] ) , less_mask , 
split_p_new = tf . dynamic_partition ( p_new , less_mask , 2 ) 
split_u = tf . dynamic_partition ( u , less_mask , 2 ) 
alpha = tf . dynamic_stitch ( condition_indices , [ split_p_new [ 0 ] , split_u [ 1 ] ] ) 
mass_ += tf . reduce_sum ( split_u [ 1 ] ) 
mask = mask * ( tf . ones_like ( less_mask ) - less_mask ) 
found_ = tf . cond ( tf . equal ( tf . reduce_sum ( less_mask ) , 0 ) , 
lambda : False , 
lambda : True ) 
alpha = tf . reshape ( alpha , q_ . shape ) 
return alpha , mask , mass_ , found_ 
~~ ( csoft , mask_ , _ , _ ) = tf . while_loop ( cond = lambda _0 , _1 , _2 , f : f , 
body = loop , 
loop_vars = ( q , active , mass , found ) ) 
return [ csoft , mask_ ] 
~~ def csoftmax ( tensor , inv_cumulative_att ) : 
shape_ten = tensor . shape 
shape_cum = inv_cumulative_att . shape 
merge_tensor = [ tensor , inv_cumulative_att ] 
return cs 
~~ def attention_gen_step ( hidden_for_sketch , hidden_for_attn_alignment , sketch , key , cum_att ) : 
with tf . name_scope ( 'attention_step' ) : 
~~~ sketch_dims = hidden_for_sketch . get_shape ( ) . as_list ( ) 
batch_size = sketch_dims [ 0 ] 
num_tokens = sketch_dims [ 1 ] 
hidden_size = sketch_dims [ 2 ] 
attn_alignment_dims = hidden_for_attn_alignment . get_shape ( ) . as_list ( ) 
attn_alignment_hidden_size = attn_alignment_dims [ 2 ] 
repeated_sketch = tf . tile ( tf . reshape ( sketch , [ - 1 , 1 , hidden_size ] ) , ( 1 , num_tokens , 1 ) ) 
concat_mem = tf . concat ( [ hidden_for_sketch , repeated_sketch ] , - 1 ) 
reduce_mem = tf . layers . dense ( concat_mem , hidden_size ) 
projected_key = tf . layers . dense ( key , hidden_size ) 
t_key = tf . reshape ( projected_key , [ - 1 , hidden_size , 1 ] ) 
score = tf . reshape ( tf . matmul ( reduce_mem , t_key ) , [ - 1 , num_tokens ] ) 
inv_cum_att = tf . reshape ( tf . ones_like ( cum_att ) - cum_att , [ - 1 , num_tokens ] ) 
att = csoftmax ( score , inv_cum_att ) 
t_reduce_mem = tf . transpose ( reduce_mem , [ 0 , 2 , 1 ] ) 
t_hidden_for_attn_alignment = tf . transpose ( hidden_for_attn_alignment , [ 0 , 2 , 1 ] ) 
r_att = tf . reshape ( att , [ - 1 , num_tokens , 1 ] ) 
next_sketch = tf . squeeze ( tf . matmul ( t_reduce_mem , r_att ) , - 1 ) 
aligned_hidden_sketch = tf . squeeze ( tf . matmul ( t_hidden_for_attn_alignment , r_att ) , - 1 ) 
~~ return next_sketch , att , aligned_hidden_sketch 
~~ def attention_gen_block ( hidden_for_sketch , hidden_for_attn_alignment , key , attention_depth ) : 
with tf . name_scope ( 'attention_block' ) : 
~~~ sketch_dims = tf . shape ( hidden_for_sketch ) 
attn_alignment_dims = tf . shape ( hidden_for_attn_alignment ) 
sketches = [ tf . zeros ( shape = [ batch_size , hidden_size ] , dtype = tf . float32 ) ] 
aligned_hiddens = [ ] 
for i in range ( attention_depth ) : 
~~~ sketch , cum_att_ , aligned_hidden = attention_gen_step ( hidden_for_sketch , hidden_for_attn_alignment , sketches [ - 1 ] , key , cum_att ) 
sketches . append ( sketch ) #sketch 
aligned_hiddens . append ( aligned_hidden ) #sketch 
cum_att += cum_att_ 
~~ final_aligned_hiddens = tf . reshape ( tf . transpose ( tf . stack ( aligned_hiddens ) , [ 1 , 0 , 2 ] ) , [ 1 , attention_depth , attn_alignment_hidden_size ] ) 
~~ return final_aligned_hiddens 
~~ def cls_from_str ( name : str ) -> type : 
~~~ module_name , cls_name = name . split ( ':' ) 
~~ return getattr ( importlib . import_module ( module_name ) , cls_name ) 
~~ def register ( name : str = None ) -> type : 
def decorate ( model_cls : type , reg_name : str = None ) -> type : 
~~~ model_name = reg_name or short_name ( model_cls ) 
global _REGISTRY 
cls_name = model_cls . __module__ + ':' + model_cls . __name__ 
if model_name in _REGISTRY and _REGISTRY [ model_name ] != cls_name : 
~~~ logger . warning ( \ . format ( model_name ) ) 
~~ _REGISTRY [ model_name ] = cls_name 
return model_cls 
~~ return lambda model_cls_name : decorate ( model_cls_name , name ) 
~~ def get_model ( name : str ) -> type : 
~~~ if ':' not in name : 
~~ return cls_from_str ( name ) 
~~ return cls_from_str ( _REGISTRY [ name ] ) 
~~ def general_attention ( key , context , hidden_size , projected_align = False ) : 
if hidden_size % 2 != 0 : 
~~ batch_size = tf . shape ( context ) [ 0 ] 
max_num_tokens , token_size = context . get_shape ( ) . as_list ( ) [ - 2 : ] 
r_context = tf . reshape ( context , shape = [ - 1 , max_num_tokens , token_size ] ) 
projected_key = tf . layers . dense ( key , hidden_size , kernel_initializer = xav ( ) ) 
r_projected_key = tf . reshape ( projected_key , shape = [ - 1 , hidden_size , 1 ] ) 
lstm_fw_cell = tf . nn . rnn_cell . LSTMCell ( hidden_size // 2 ) 
lstm_bw_cell = tf . nn . rnn_cell . LSTMCell ( hidden_size // 2 ) 
( output_fw , output_bw ) , states = tf . nn . bidirectional_dynamic_rnn ( cell_fw = lstm_fw_cell , 
cell_bw = lstm_bw_cell , 
inputs = r_context , 
dtype = tf . float32 ) 
bilstm_output = tf . concat ( [ output_fw , output_bw ] , - 1 ) 
attn = tf . nn . softmax ( tf . matmul ( bilstm_output , r_projected_key ) , dim = 1 ) 
if projected_align : 
t_context = tf . transpose ( bilstm_output , [ 0 , 2 , 1 ] ) 
output = tf . reshape ( tf . matmul ( t_context , attn ) , 
shape = [ batch_size , - 1 , hidden_size ] ) 
t_context = tf . transpose ( r_context , [ 0 , 2 , 1 ] ) 
shape = [ batch_size , - 1 , token_size ] ) 
~~ return output 
~~ def light_general_attention ( key , context , hidden_size , projected_align = False ) : 
batch_size = tf . shape ( context ) [ 0 ] 
projected_context = tf . layers . dense ( r_context , hidden_size , kernel_initializer = xav ( ) ) 
attn = tf . nn . softmax ( tf . matmul ( projected_context , r_projected_key ) , dim = 1 ) 
t_context = tf . transpose ( projected_context , [ 0 , 2 , 1 ] ) 
~~ def light_bahdanau_attention ( key , context , hidden_size , projected_align = False ) : 
r_projected_key = tf . tile ( tf . reshape ( projected_key , shape = [ - 1 , 1 , hidden_size ] ) , 
[ 1 , max_num_tokens , 1 ] ) 
concat_h_state = tf . concat ( [ projected_context , r_projected_key ] , - 1 ) 
projected_state = tf . layers . dense ( concat_h_state , hidden_size , use_bias = False , 
kernel_initializer = xav ( ) ) 
score = tf . layers . dense ( tf . tanh ( projected_state ) , units = 1 , use_bias = False , 
attn = tf . nn . softmax ( score , dim = 1 ) 
~~ def cs_bahdanau_attention ( key , context , hidden_size , depth , projected_align = False ) : 
projected_context = tf . layers . dense ( r_context , token_size , 
kernel_initializer = xav ( ) , 
name = 'projected_context' ) 
projected_key = tf . layers . dense ( key , hidden_size , kernel_initializer = xav ( ) , 
name = 'projected_key' ) 
inputs = projected_context , 
concat_h_state = tf . concat ( [ r_projected_key , output_fw , output_bw ] , - 1 ) 
h_state_for_attn_alignment = bilstm_output 
aligned_h_state = csoftmax_attention . attention_bah_block ( 
concat_h_state , h_state_for_attn_alignment , depth ) 
output = tf . reshape ( aligned_h_state , shape = [ batch_size , - 1 , depth * hidden_size ] ) 
h_state_for_attn_alignment = projected_context 
output = tf . reshape ( aligned_h_state , shape = [ batch_size , - 1 , depth * token_size ] ) 
~~ def update_site_forward ( apps , schema_editor ) : 
Site = apps . get_model ( "sites" , "Site" ) 
Site . objects . update_or_create ( 
id = settings . SITE_ID , 
defaults = { 
"domain" : "{{cookiecutter.domain_name}}" , 
"name" : "{{cookiecutter.project_name}}" , 
~~ def generate_random_string ( 
length , using_digits = False , using_ascii_letters = False , using_punctuation = False 
) : 
if not using_sysrandom : 
~~ symbols = [ ] 
if using_digits : 
~~~ symbols += string . digits 
~~ if using_ascii_letters : 
~~~ symbols += string . ascii_letters 
~~ if using_punctuation : 
~~~ all_punctuation = set ( string . punctuation ) 
unsuitable = { "\ , \ , "\\\\" , "$" } 
suitable = all_punctuation . difference ( unsuitable ) 
symbols += "" . join ( suitable ) 
~~ return "" . join ( [ random . choice ( symbols ) for _ in range ( length ) ] ) 
~~ def get_tweets ( user , pages = 25 ) : 
url = f'https://twitter.com/i/profiles/show/{user}/timeline/tweets?include_available_features=1&include_entities=1&include_new_items_bar=true' 
headers = { 
'Referer' : f'https://twitter.com/{user}' , 
'X-Twitter-Active-User' : 'yes' , 
'X-Requested-With' : 'XMLHttpRequest' , 
'Accept-Language' : 'en-US' 
def gen_tweets ( pages ) : 
~~~ r = session . get ( url , headers = headers ) 
while pages > 0 : 
~~~ html = HTML ( html = r . json ( ) [ 'items_html' ] , 
url = 'bunk' , default_encoding = 'utf-8' ) 
f\ ) 
~~ comma = "," 
dot = "." 
tweets = [ ] 
~~~ text = tweet . find ( '.tweet-text' ) [ 0 ] . full_text 
~~ tweet_id = tweet . find ( '.js-permalink' ) [ 0 ] . attrs [ 'data-conversation-id' ] 
time = datetime . fromtimestamp ( int ( tweet . find ( '._timestamp' ) [ 0 ] . attrs [ 'data-time-ms' ] ) / 1000.0 ) 
interactions = [ 
x . text 
for x in tweet . find ( '.ProfileTweet-actionCount' ) 
replies = int ( 
or interactions [ 3 ] 
retweets = int ( 
or interactions [ 4 ] 
or interactions [ 5 ] 
likes = int ( 
or interactions [ 6 ] 
or interactions [ 7 ] 
hashtags = [ 
hashtag_node . full_text 
for hashtag_node in tweet . find ( '.twitter-hashtag' ) 
urls = [ 
url_node . attrs [ 'data-expanded-url' ] 
for url_node in tweet . find ( 'a.twitter-timeline-link:not(.u-hidden)' ) 
photos = [ 
photo_node . attrs [ 'data-image-url' ] 
for photo_node in tweet . find ( '.AdaptiveMedia-photoContainer' ) 
videos = [ ] 
video_nodes = tweet . find ( ".PlayableMedia-player" ) 
for node in video_nodes : 
~~~ styles = node . attrs [ 'style' ] . split ( ) 
for style in styles : 
~~~ if style . startswith ( 'background' ) : 
~~~ tmp = style . split ( '/' ) [ - 1 ] 
video_id = tmp [ : tmp . index ( '.jpg' ) ] 
videos . append ( { 'id' : video_id } ) 
~~ ~~ ~~ tweets . append ( { 
'tweetId' : tweet_id , 
'time' : time , 
'text' : text , 
'replies' : replies , 
'retweets' : retweets , 
'likes' : likes , 
'entries' : { 
'hashtags' : hashtags , 'urls' : urls , 
'photos' : photos , 'videos' : videos 
~~ last_tweet = html . find ( '.stream-item' ) [ - 1 ] . attrs [ 'data-item-id' ] 
for tweet in tweets : 
~~~ if tweet : 
yield tweet 
~~ ~~ r = session . get ( url , params = { 'max_position' : last_tweet } , headers = headers ) 
pages += - 1 
~~ ~~ yield from gen_tweets ( pages ) 
~~ def monochrome ( I , color , vmin = None , vmax = None ) : 
if vmin is None : 
~~~ vmin = np . nanmin ( I ) 
~~ if vmax is None : 
~~~ vmax = np . nanmax ( I ) 
~~ normalized = ( I - vmin ) / ( vmax - vmin ) 
return np . clip ( normalized [ ... , np . newaxis ] , 0 , 1 ) * np . array ( color ) 
~~ def polychrome ( I , colors , vmin = None , vmax = None , axis = - 1 ) : 
axes_length = len ( I . shape ) 
allaxes = list ( range ( axes_length ) ) 
otheraxes = list ( allaxes ) 
otheraxes . remove ( ( axis + axes_length ) % axes_length ) 
otheraxes = tuple ( otheraxes ) 
~~~ vmin = np . nanmin ( I , axis = otheraxes ) 
~~~ vmax = np . nanmax ( I , axis = otheraxes ) 
return np . clip ( normalized , 0 , 1 ) . dot ( colors ) 
~~ def parallelize ( cores = None , fork = True , flatten = False , info = False , infoclass = InfoThreadProgressBar , init = None , * args , ** kwargs ) : 
if cores == None : 
~~~ cores = multiprocessing . cpu_count ( ) 
~~ def wrapper ( f ) : 
~~~ def execute ( * multiargs ) : 
~~~ results = [ ] 
len ( list ( zip ( * multiargs ) ) ) 
N = len ( multiargs [ 0 ] ) 
if info : 
~~ taskQueue = queue . Queue ( len ( multiargs [ 0 ] ) ) 
#\ttaskQueue.put(timenr) 
for tasknr , _args in enumerate ( zip ( * multiargs ) ) : 
~~~ taskQueue . put ( ( tasknr , list ( _args ) ) ) 
#\tresults.append(result) 
~~ executions = [ Execution ( taskQueue , fork , f , init , corenr , args , kwargs ) for corenr in range ( cores ) ] 
~~~ infoobj = infoclass ( len ( multiargs [ 0 ] ) , executions ) 
infoobj . start ( ) 
~~ for i , execution in enumerate ( executions ) : 
~~~ execution . setName ( "T-%d" % i ) 
execution . start ( ) 
#\twatchdog.start() 
~~ error = False 
for execution in executions : 
~~~ log ( "joining:" , execution . getName ( ) ) 
~~~ execution . join ( ) 
~~ except BaseException : 
~~~ error = True 
~~ results . extend ( execution . results ) 
if execution . error : 
~~ ~~ if info : 
~~~ infoobj . join ( ) 
~~ if error : 
~~~ print ( "error" , file = sys . stderr ) 
results = None 
~~~ results . sort ( cmp = lambda a , b : cmp ( a [ 0 ] , b [ 0 ] ) ) 
results = [ k [ 1 ] for k in results ] 
if flatten : 
~~~ flatresults = [ ] 
~~~ flatresults . extend ( result ) 
~~ results = flatresults 
~~ return execute 
~~ return wrapper 
~~ def export_hdf5 ( dataset , path , column_names = None , byteorder = "=" , shuffle = False , selection = False , progress = None , virtual = True , sort = None , ascending = True ) : 
if selection : 
~~~ selection = "default" 
~~ ~~ with h5py . File ( path , "w" ) as h5file_output : 
~~~ h5table_output = h5file_output . require_group ( "/table" ) 
h5table_output . attrs [ "type" ] = "table" 
h5columns_output = h5file_output . require_group ( "/table/columns" ) 
N = len ( dataset ) if not selection else dataset . selected_length ( selection ) 
if N == 0 : 
~~ logger . debug ( "virtual=%r" , virtual ) 
column_names = column_names or dataset . get_column_names ( virtual = virtual , strings = True ) 
sparse_groups = collections . defaultdict ( list ) 
for column_name in list ( column_names ) : 
~~~ sparse_matrix = dataset . _sparse_matrix ( column_name ) 
if sparse_matrix is not None : 
~~~ sparse_groups [ id ( sparse_matrix ) ] . append ( column_name ) 
sparse_matrices [ id ( sparse_matrix ) ] = sparse_matrix 
~~ dtype = dataset . dtype ( column_name ) 
if column_name in dataset . get_column_names ( virtual = False ) : 
~~~ column = dataset . columns [ column_name ] 
shape = ( N , ) + column . shape [ 1 : ] 
~~~ shape = ( N , ) 
~~ h5column_output = h5columns_output . require_group ( column_name ) 
if dtype == str_type : 
~~~ byte_length = dataset [ column_name ] . str . byte_length ( ) . sum ( selection = selection ) 
if byte_length > max_int32 : 
~~~ dtype_indices = 'i8' 
~~~ dtype_indices = 'i4' 
~~ data_shape = ( byte_length , ) 
indices_shape = ( N + 1 , ) 
array = h5column_output . require_dataset ( 'data' , shape = data_shape , dtype = 'S1' ) 
index_array = h5column_output . require_dataset ( 'indices' , shape = indices_shape , dtype = dtype_indices ) 
null_value_count = N - dataset . count ( column_name , selection = selection ) 
if null_value_count > 0 : 
null_bitmap_array = h5column_output . require_dataset ( 'null_bitmap' , shape = null_shape , dtype = 'u1' ) 
~~ array . attrs [ "dtype" ] = 'str' 
~~~ if dtype . kind in 'mM' : 
~~~ array = h5column_output . require_dataset ( 'data' , shape = shape , dtype = np . int64 ) 
array . attrs [ "dtype" ] = dtype . name 
~~ elif dtype . kind == 'U' : 
~~~ char_length = dtype . itemsize // 4 
shape = ( N , char_length ) 
array = h5column_output . require_dataset ( 'data' , shape = shape , dtype = np . uint8 ) 
array . attrs [ "dtype" ] = 'utf32' 
array . attrs [ "dlength" ] = char_length 
~~~ array = h5column_output . require_dataset ( 'data' , shape = shape , dtype = dtype . newbyteorder ( byteorder ) ) 
del h5columns_output [ column_name ] 
column_names . remove ( column_name ) 
data = dataset . evaluate ( column_name , 0 , 1 ) 
if np . ma . isMaskedArray ( data ) : 
~~~ mask = h5column_output . require_dataset ( 'mask' , shape = shape , dtype = np . bool ) 
~~ ~~ ~~ random_index_name = None 
if shuffle : 
~~~ random_index_name = "random_index" 
while random_index_name in dataset . get_column_names ( ) : 
~~~ random_index_name += "_new" 
~~ shuffle_array = h5columns_output . require_dataset ( random_index_name + "/data" , shape = ( N , ) , dtype = byteorder + "i8" ) 
shuffle_array [ 0 ] = shuffle_array [ 0 ] 
sparse_index = 0 
for sparse_matrix in sparse_matrices . values ( ) : 
~~~ columns = sorted ( sparse_groups [ id ( sparse_matrix ) ] , key = lambda col : dataset . columns [ col ] . column_index ) 
name = "sparse" + str ( sparse_index ) 
sparse_index += 1 
sparse_group = h5columns_output . require_group ( name ) 
sparse_group . attrs [ 'type' ] = 'csr_matrix' 
ar = sparse_group . require_dataset ( 'data' , shape = ( len ( sparse_matrix . data ) , ) , dtype = sparse_matrix . dtype ) 
ar [ 0 ] = ar [ 0 ] 
ar = sparse_group . require_dataset ( 'indptr' , shape = ( len ( sparse_matrix . indptr ) , ) , dtype = sparse_matrix . indptr . dtype ) 
ar = sparse_group . require_dataset ( 'indices' , shape = ( len ( sparse_matrix . indices ) , ) , dtype = sparse_matrix . indices . dtype ) 
for i , column_name in enumerate ( columns ) : 
~~~ h5column = sparse_group . require_group ( column_name ) 
h5column . attrs [ 'column_index' ] = i 
~~ ~~ ~~ dataset_output = vaex . hdf5 . dataset . Hdf5MemoryMapped ( path , write = True ) 
column_names = vaex . export . _export ( dataset_input = dataset , dataset_output = dataset_output , path = path , random_index_column = random_index_name , 
column_names = column_names , selection = selection , shuffle = shuffle , byteorder = byteorder , 
progress = progress , sort = sort , ascending = ascending ) 
import getpass 
import datetime 
user = getpass . getuser ( ) 
date = str ( datetime . datetime . now ( ) ) 
source = dataset . path 
if dataset . description : 
~~ dataset_output . copy_metadata ( dataset ) 
dataset_output . description = description 
dataset_output . write_meta ( ) 
dataset_output . close_files ( ) 
~~ def arrow_table_from_vaex_df ( ds , column_names = None , selection = None , strings = True , virtual = False ) : 
names = [ ] 
arrays = [ ] 
for name , array in ds . to_items ( column_names = column_names , selection = selection , strings = strings , virtual = virtual ) : 
~~~ names . append ( name ) 
arrays . append ( arrow_array_from_numpy_array ( array ) ) 
~~ return pyarrow . Table . from_arrays ( arrays , names ) 
~~ def patch ( f ) : 
name = f . __name__ 
Dataset . __hidden__ [ name ] = f 
return f 
~~ def add_virtual_columns_eq2ecl ( self , long_in = "ra" , lat_in = "dec" , long_out = "lambda_" , lat_out = "beta" , name_prefix = "__celestial_eq2ecl" , radians = False ) : 
self . add_virtual_columns_celestial ( long_in , lat_in , long_out , lat_out , name_prefix = name_prefix , radians = radians , _matrix = 'eq2ecl' ) 
~~ def add_virtual_columns_distance_from_parallax ( self , parallax = "parallax" , distance_name = "distance" , parallax_uncertainty = None , uncertainty_postfix = "_uncertainty" ) : 
import astropy . units 
unit = self . unit ( parallax ) 
distance_expression = "1/%s" % ( parallax ) 
self . ucds [ distance_name ] = "pos.distance" 
if unit : 
~~~ if unit == astropy . units . milliarcsecond : 
~~~ self . units [ distance_name ] = astropy . units . kpc 
~~ if unit == astropy . units . arcsecond : 
~~~ self . units [ distance_name ] = astropy . units . parsec 
~~ ~~ self . add_virtual_column ( distance_name , distance_expression ) 
if parallax_uncertainty : 
name = distance_name + uncertainty_postfix 
distance_uncertainty_expression = "{parallax_uncertainty}/({parallax})**2" . format ( ** locals ( ) ) 
self . add_virtual_column ( name , distance_uncertainty_expression ) 
self . ucds [ name ] = "stat.error;pos.distance" 
~~ ~~ def add_virtual_columns_cartesian_velocities_to_pmvr ( self , x = "x" , y = "y" , z = "z" , vx = "vx" , vy = "vy" , vz = "vz" , vr = "vr" , pm_long = "pm_long" , pm_lat = "pm_lat" , distance = None ) : 
if distance is None : 
~~~ distance = "sqrt({x}**2+{y}**2+{z}**2)" . format ( ** locals ( ) ) 
~~ k = 4.74057 
self . add_variable ( "k" , k , overwrite = False ) 
self . add_virtual_column ( vr , "({x}*{vx}+{y}*{vy}+{z}*{vz})/{distance}" . format ( ** locals ( ) ) ) 
self . add_virtual_column ( pm_long , "-({vx}*{y}-{x}*{vy})/sqrt({x}**2+{y}**2)/{distance}/k" . format ( ** locals ( ) ) ) 
~~ def add_virtual_columns_proper_motion_eq2gal ( self , long_in = "ra" , lat_in = "dec" , pm_long = "pm_ra" , pm_lat = "pm_dec" , pm_long_out = "pm_l" , pm_lat_out = "pm_b" , 
name_prefix = "__proper_motion_eq2gal" , 
right_ascension_galactic_pole = 192.85 , 
declination_galactic_pole = 27.12 , 
propagate_uncertainties = False , 
radians = False , inverse = False ) : 
long_in_original = long_in = self . _expr ( long_in ) 
lat_in_original = lat_in = self . _expr ( lat_in ) 
pm_long = self . _expr ( pm_long ) 
pm_lat = self . _expr ( pm_lat ) 
if not radians : 
~~~ long_in = long_in * np . pi / 180 
lat_in = lat_in * np . pi / 180 
~~ c1_name = name_prefix + "_C1" 
c2_name = name_prefix + "_C2" 
right_ascension_galactic_pole = math . radians ( right_ascension_galactic_pole ) 
declination_galactic_pole = math . radians ( declination_galactic_pole ) 
self [ c1_name ] = c1 = np . sin ( declination_galactic_pole ) * np . cos ( lat_in ) - np . cos ( declination_galactic_pole ) * np . sin ( lat_in ) * np . cos ( long_in - right_ascension_galactic_pole ) 
self [ c2_name ] = c2 = np . cos ( declination_galactic_pole ) * np . sin ( long_in - right_ascension_galactic_pole ) 
c1 = self [ c1_name ] 
c2 = self [ c2_name ] 
if inverse : 
~~~ self [ pm_long_out ] = ( c1 * pm_long + - c2 * pm_lat ) / np . sqrt ( c1 ** 2 + c2 ** 2 ) 
self [ pm_lat_out ] = ( c2 * pm_long + c1 * pm_lat ) / np . sqrt ( c1 ** 2 + c2 ** 2 ) 
~~~ self [ pm_long_out ] = ( c1 * pm_long + c2 * pm_lat ) / np . sqrt ( c1 ** 2 + c2 ** 2 ) 
self [ pm_lat_out ] = ( - c2 * pm_long + c1 * pm_lat ) / np . sqrt ( c1 ** 2 + c2 ** 2 ) 
~~ if propagate_uncertainties : 
~~~ self . propagate_uncertainties ( [ self [ pm_long_out ] , self [ pm_lat_out ] ] ) 
~~ ~~ def add_virtual_columns_proper_motion_gal2eq ( self , long_in = "ra" , lat_in = "dec" , pm_long = "pm_l" , pm_lat = "pm_b" , pm_long_out = "pm_ra" , pm_lat_out = "pm_dec" , 
name_prefix = "__proper_motion_gal2eq" , 
radians = False ) : 
kwargs = dict ( ** locals ( ) ) 
kwargs . pop ( 'self' ) 
kwargs [ 'inverse' ] = True 
self . add_virtual_columns_proper_motion_eq2gal ( ** kwargs ) 
~~ def add_virtual_columns_lbrvr_proper_motion2vcartesian ( self , long_in = "l" , lat_in = "b" , distance = "distance" , pm_long = "pm_l" , pm_lat = "pm_b" , 
vr = "vr" , vx = "vx" , vy = "vy" , vz = "vz" , 
center_v = ( 0 , 0 , 0 ) , 
propagate_uncertainties = False , radians = False ) : 
k = 4.74057 
a , d , distance = self . _expr ( long_in , lat_in , distance ) 
pm_long , pm_lat , vr = self . _expr ( pm_long , pm_lat , vr ) 
~~~ a = a * np . pi / 180 
d = d * np . pi / 180 
~~ A = [ [ np . cos ( a ) * np . cos ( d ) , - np . sin ( a ) , - np . cos ( a ) * np . sin ( d ) ] , 
[ np . sin ( a ) * np . cos ( d ) , np . cos ( a ) , - np . sin ( a ) * np . sin ( d ) ] , 
[ np . sin ( d ) , d * 0 , np . cos ( d ) ] ] 
self . add_virtual_columns_matrix3d ( vr , k * pm_long * distance , k * pm_lat * distance , vx , vy , vz , A , translation = center_v ) 
if propagate_uncertainties : 
~~~ self . propagate_uncertainties ( [ self [ vx ] , self [ vy ] , self [ vz ] ] ) 
~~ ~~ def add_virtual_columns_equatorial_to_galactic_cartesian ( self , alpha , delta , distance , xname , yname , zname , radians = True , alpha_gp = np . radians ( 192.85948 ) , delta_gp = np . radians ( 27.12825 ) , l_omega = np . radians ( 32.93192 ) ) : 
~~~ alpha = "pi/180.*%s" % alpha 
delta = "pi/180.*%s" % delta 
~~ def add_virtual_columns_proper_motion2vperpendicular ( self , distance = "distance" , pm_long = "pm_l" , pm_lat = "pm_b" , 
vl = "vl" , vb = "vb" , 
self . add_virtual_column ( vl , "k*{pm_long}*{distance}" . format ( ** locals ( ) ) ) 
~~~ self . propagate_uncertainties ( [ self [ vl ] , self [ vb ] ] ) 
~~ ~~ def add_virtual_columns_cartesian_angular_momenta ( self , x = 'x' , y = 'y' , z = 'z' , 
vx = 'vx' , vy = 'vy' , vz = 'vz' , 
Lx = 'Lx' , Ly = 'Ly' , Lz = 'Lz' , 
propagate_uncertainties = False ) : 
x , y , z , vx , vy , vz = self . _expr ( x , y , z , vx , vy , vz ) 
self . add_virtual_column ( Lx , y * vz - z * vy ) 
self . add_virtual_column ( Ly , z * vx - x * vz ) 
self . add_virtual_column ( Lz , x * vy - y * vx ) 
~~~ self . propagate_uncertainties ( [ self [ Lx ] , self [ Ly ] , self [ Lz ] ] ) 
~~ ~~ def _recompute_transform ( self ) : 
center = ( self . convert_xunits ( self . center [ 0 ] ) , 
self . convert_yunits ( self . center [ 1 ] ) ) 
width = self . width #self.convert_xunits(self.width) 
height = self . height #self.convert_yunits(self.height) 
trans = artist . Artist . get_transform ( self ) 
self . _patch_transform = transforms . Affine2D ( ) . scale ( width * 0.5 * self . scale , height * 0.5 * self . scale ) . rotate_deg ( self . angle ) . translate ( * trans . transform ( center ) ) 
~~ def _graph ( self ) : 
expression = self . expression 
def walk ( node ) : 
~~~ if isinstance ( node , six . string_types ) : 
~~~ if node in self . ds . virtual_columns : 
~~~ ex = Expression ( self . ds , self . ds . virtual_columns [ node ] ) 
return [ node , None , None , [ ex . _graph ( ) ] ] 
~~~ return node 
~~~ fname , node_repr , deps = node 
~~ deps = [ walk ( dep ) for dep in deps ] 
obj = self . ds . functions . get ( fname ) 
if isinstance ( obj , Function ) : 
~~~ obj = obj . f 
~~ if isinstance ( obj , FunctionSerializablePickle ) : 
~~ return [ node_repr , fname , obj , deps ] 
~~ ~~ return walk ( expresso . _graph ( expression ) ) 
~~ def _graphviz ( self , dot = None ) : 
from graphviz import Graph , Digraph 
node = self . _graph ( ) 
dot = dot or Digraph ( comment = self . expression ) 
~~~ dot . node ( node , node ) 
return node , node 
~~~ node_repr , fname , fobj , deps = node 
node_id = node_repr 
dot . node ( node_id , node_repr ) 
for dep in deps : 
~~~ dep_id , dep = walk ( dep ) 
dot . edge ( node_id , dep_id ) 
~~ return node_id , node 
~~ ~~ walk ( node ) 
return dot 
~~ def min ( self , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , progress = None ) : 
kwargs = dict ( locals ( ) ) 
del kwargs [ 'self' ] 
kwargs [ 'expression' ] = self . expression 
return self . ds . min ( ** kwargs ) 
~~ def value_counts ( self , dropna = False , dropnull = True , ascending = False , progress = False ) : 
from pandas import Series 
dtype = self . dtype 
transient = self . transient or self . ds . filtered or self . ds . is_masked ( self . expression ) 
if self . dtype == str_type and not transient : 
~~~ ar = self . ds . columns [ self . expression ] 
if not isinstance ( ar , ColumnString ) : 
~~~ transient = True 
~~ ~~ counter_type = counter_type_from_dtype ( self . dtype , transient ) 
counters = [ None ] * self . ds . executor . thread_pool . nthreads 
def map ( thread_index , i1 , i2 , ar ) : 
~~~ if counters [ thread_index ] is None : 
~~~ counters [ thread_index ] = counter_type ( ) 
~~ if dtype == str_type : 
~~~ previous_ar = ar 
ar = _to_string_sequence ( ar ) 
if not transient : 
~~~ assert ar is previous_ar . string_sequence 
~~ ~~ if np . ma . isMaskedArray ( ar ) : 
~~~ mask = np . ma . getmaskarray ( ar ) 
counters [ thread_index ] . update ( ar , mask ) 
~~~ counters [ thread_index ] . update ( ar ) 
~~ return 0 
~~ def reduce ( a , b ) : 
~~~ return a + b 
~~ self . ds . map_reduce ( map , reduce , [ self . expression ] , delay = False , progress = progress , name = 'value_counts' , info = True , to_numpy = False ) 
counters = [ k for k in counters if k is not None ] 
counter0 = counters [ 0 ] 
for other in counters [ 1 : ] : 
~~~ counter0 . merge ( other ) 
~~ value_counts = counter0 . extract ( ) 
index = np . array ( list ( value_counts . keys ( ) ) ) 
counts = np . array ( list ( value_counts . values ( ) ) ) 
order = np . argsort ( counts ) 
if not ascending : 
~~~ order = order [ : : - 1 ] 
~~ counts = counts [ order ] 
index = index [ order ] 
if not dropna or not dropnull : 
~~~ index = index . tolist ( ) 
counts = counts . tolist ( ) 
if not dropna and counter0 . nan_count : 
~~~ index = [ np . nan ] + index 
counts = [ counter0 . nan_count ] + counts 
~~ if not dropnull and counter0 . null_count : 
~~~ index = [ 'null' ] + index 
counts = [ counter0 . null_count ] + counts 
~~ ~~ return Series ( counts , index = index ) 
~~ def map ( self , mapper , nan_mapping = None , null_mapping = None ) : 
df = self . ds 
mapper_keys = np . array ( list ( mapper . keys ( ) ) ) 
key_set = df . _set ( self . expression ) 
found_keys = key_set . keys ( ) 
mapper_has_nan = any ( [ key != key for key in mapper_keys ] ) 
if not set ( mapper_keys ) . issuperset ( found_keys ) : 
~~~ missing = set ( found_keys ) . difference ( mapper_keys ) 
missing0 = list ( missing ) [ 0 ] 
~~ ~~ choices = [ mapper [ key ] for key in found_keys ] 
if key_set . has_nan : 
~~~ if mapper_has_nan : 
~~~ choices = [ mapper [ np . nan ] ] + choices 
~~~ choices = [ nan_mapping ] + choices 
~~ ~~ if key_set . has_null : 
~~~ choices = [ null_mapping ] + choices 
~~ choices = np . array ( choices ) 
key_set_name = df . add_variable ( 'map_key_set' , key_set , unique = True ) 
choices_name = df . add_variable ( 'map_choices' , choices , unique = True ) 
return Expression ( df , expr ) 
~~ def app ( * args , ** kwargs ) : 
import vaex . ui . main 
return vaex . ui . main . VaexApp ( ) 
~~ def _convert_name ( filenames , shuffle = False ) : 
if not isinstance ( filenames , ( list , tuple ) ) : 
~~~ filenames = [ filenames ] 
~~ base = filenames [ 0 ] 
~~~ base += '-shuffle' 
~~ if len ( filenames ) > 1 : 
~~~ return base + "_and_{}_more.hdf5" . format ( len ( filenames ) - 1 ) 
~~~ return base + ".hdf5" 
~~ ~~ def open ( path , convert = False , shuffle = False , copy_index = True , * args , ** kwargs ) : 
import vaex 
~~~ if path in aliases : 
~~~ path = aliases [ path ] 
~~~ server , DataFrame = path . rsplit ( "/" , 1 ) 
server = vaex . server ( server , ** kwargs ) 
DataFrames = server . DataFrames ( as_dict = True ) 
if DataFrame not in DataFrames : 
~~ return DataFrames [ DataFrame ] 
~~ if path . startswith ( "cluster" ) : 
~~~ import vaex . distributed 
return vaex . distributed . open ( path , * args , ** kwargs ) 
~~~ import vaex . file 
import glob 
filenames = list ( sorted ( glob . glob ( path ) ) ) 
ds = None 
if len ( filenames ) == 0 : 
~~ filename_hdf5 = _convert_name ( filenames , shuffle = shuffle ) 
filename_hdf5_noshuffle = _convert_name ( filenames , shuffle = False ) 
if len ( filenames ) == 1 : 
~~~ path = filenames [ 0 ] 
ext = os . path . splitext ( path ) [ 1 ] 
~~~ if convert : 
~~~ ds = vaex . file . open ( filename_hdf5 ) 
~~~ ds = vaex . file . open ( filename_hdf5 , * args , ** kwargs ) 
~~~ ds = from_csv ( path , copy_index = copy_index , ** kwargs ) 
~~~ ds = vaex . file . open ( path , * args , ** kwargs ) 
~~ if convert : 
~~~ ds . export_hdf5 ( filename_hdf5 , shuffle = shuffle ) 
~~ ~~ if ds is None : 
~~~ if os . path . exists ( path ) : 
~~ if os . path . exists ( path ) : 
~~ ~~ ~~ elif len ( filenames ) > 1 : 
~~~ if convert not in [ True , False ] : 
~~~ filename_hdf5 = convert 
~~~ filename_hdf5 = _convert_name ( filenames , shuffle = shuffle ) 
~~~ ds = open ( filename_hdf5 ) 
~~~ DataFrames = [ ] 
for filename in filenames : 
~~~ DataFrames . append ( open ( filename , convert = bool ( convert ) , shuffle = shuffle , ** kwargs ) ) 
~~ ds = vaex . dataframe . DataFrameConcatenated ( DataFrames ) 
ds = vaex . file . open ( filename_hdf5 , * args , ** kwargs ) 
~~ ~~ ~~ if ds is None : 
~~ return ds 
~~ ~~ def open_many ( filenames ) : 
dfs = [ ] 
~~~ filename = filename . strip ( ) 
if filename and filename [ 0 ] != "#" : 
~~~ dfs . append ( open ( filename ) ) 
~~ ~~ return vaex . dataframe . DataFrameConcatenated ( dfs = dfs ) 
~~ def from_samp ( username = None , password = None ) : 
import vaex . samp 
t = vaex . samp . single_table ( username = username , password = password ) 
return from_astropy_table ( t . to_table ( ) ) 
~~ def from_astropy_table ( table ) : 
import vaex . file . other 
return vaex . file . other . DatasetAstropyTable ( table = table ) 
~~ def from_items ( * items ) : 
import numpy as np 
df = vaex . dataframe . DataFrameArrays ( "array" ) 
for name , array in items : 
~~~ df . add_column ( name , np . asanyarray ( array ) ) 
~~ return df 
~~ def from_arrays ( ** arrays ) : 
import six 
from . column import Column 
for name , array in arrays . items ( ) : 
~~~ if isinstance ( array , Column ) : 
~~~ df . add_column ( name , array ) 
~~~ array = np . asanyarray ( array ) 
df . add_column ( name , array ) 
~~ ~~ return df 
~~ def from_scalars ( ** kwargs ) : 
return from_arrays ( ** { k : np . array ( [ v ] ) for k , v in kwargs . items ( ) } ) 
~~ def from_pandas ( df , name = "pandas" , copy_index = True , index_name = "index" ) : 
vaex_df = vaex . dataframe . DataFrameArrays ( name ) 
def add ( name , column ) : 
~~~ values = column . values 
~~~ vaex_df . add_column ( name , values ) 
~~~ values = values . astype ( "S" ) 
vaex_df . add_column ( name , values ) 
~~ ~~ ~~ for name in df . columns : 
~~~ add ( name , df [ name ] ) 
~~ if copy_index : 
~~~ add ( index_name , df . index ) 
~~ return vaex_df 
~~ def from_ascii ( path , seperator = None , names = True , skip_lines = 0 , skip_after = 0 , ** kwargs ) : 
import vaex . ext . readcol as rc 
ds = vaex . dataframe . DataFrameArrays ( path ) 
if names not in [ True , False ] : 
~~~ namelist = names 
names = False 
~~~ namelist = None 
~~ data = rc . readcol ( path , fsep = seperator , asdict = namelist is None , names = names , skipline = skip_lines , skipafter = skip_after , ** kwargs ) 
if namelist : 
~~~ for name , array in zip ( namelist , data . T ) : 
~~~ ds . add_column ( name , array ) 
~~~ for name , array in data . items ( ) : 
~~ ~~ return ds 
~~ def from_csv ( filename_or_buffer , copy_index = True , ** kwargs ) : 
import pandas as pd 
return from_pandas ( pd . read_csv ( filename_or_buffer , ** kwargs ) , copy_index = copy_index ) 
~~ def read_csv_and_convert ( path , shuffle = False , copy_index = True , ** kwargs ) : 
from concurrent . futures import ProcessPoolExecutor 
filenames = glob . glob ( path ) 
if len ( filenames ) > 1 : 
if not os . path . exists ( filename_hdf5 ) : 
~~~ if not os . path . exists ( filename_hdf5_noshuffle ) : 
~~~ for filename in filenames : 
~~~ read_csv_and_convert ( filename , shuffle = shuffle , copy_index = copy_index , ** kwargs ) 
~~ ds = open_many ( [ _convert_name ( k , shuffle = shuffle ) for k in filenames ] ) 
~~~ ds = open ( filename_hdf5_noshuffle ) 
~~ ds . export_hdf5 ( filename_hdf5 , shuffle = shuffle ) 
~~ return open ( filename_hdf5 ) 
~~~ filename = filenames [ 0 ] 
filename_hdf5 = _convert_name ( filename , shuffle = shuffle ) 
filename_hdf5_noshuffle = _convert_name ( filename , shuffle = False ) 
~~~ df = pd . read_csv ( filename , ** kwargs ) 
ds = from_pandas ( df , copy_index = copy_index ) 
~~ ~~ def server ( url , ** kwargs ) : 
from vaex . remote import ServerRest 
url = urlparse ( url ) 
if url . scheme == "ws" : 
~~~ websocket = True 
~~~ websocket = False 
~~ assert url . scheme in [ "ws" , "http" ] 
port = url . port 
base_path = url . path 
hostname = url . hostname 
return vaex . remote . ServerRest ( hostname , base_path = base_path , port = port , websocket = websocket , ** kwargs ) 
~~ def example ( download = True ) : 
from . import utils 
path = utils . get_data_file ( "helmi-dezeeuw-2000-10p.hdf5" ) 
if path is None and download : 
~~~ return vaex . datasets . helmi_de_zeeuw_10percent . fetch ( ) 
~~ return open ( path ) if path else None 
~~ def zeldovich ( dim = 2 , N = 256 , n = - 2.5 , t = None , scale = 1 , seed = None ) : 
import vaex . file 
return vaex . file . other . Zeldovich ( dim = dim , N = N , n = n , t = t , scale = scale ) 
~~ def concat ( dfs ) : 
ds = reduce ( ( lambda x , y : x . concat ( y ) ) , dfs ) 
return ds 
~~ def vrange ( start , stop , step = 1 , dtype = 'f8' ) : 
from . column import ColumnVirtualRange 
return ColumnVirtualRange ( start , stop , step , dtype ) 
~~ def main ( argv = sys . argv [ 1 : ] ) : 
~~~ global main_thread 
global vaex 
global app 
global kernel 
global ipython_console 
global current 
vaex . set_log_level_warning ( ) 
if app is None : 
~~~ app = QtGui . QApplication ( argv ) 
~~~ import vaex . ui . icons 
icon = QtGui . QIcon ( vaex . ui . icons . iconfile ( 'vaex128' ) ) 
app . setWindowIcon ( icon ) 
~~ ~~ main_thread = QtCore . QThread . currentThread ( ) 
def qt_exception_hook ( exctype , value , traceback ) : 
sys . __excepthook__ ( exctype , value , traceback ) 
qt_exception ( None , exctype , value , traceback ) 
~~ sys . excepthook = qt_exception_hook 
vaex . promise . Promise . unhandled = staticmethod ( qt_exception_hook ) 
vaex_app = VaexApp ( argv , open_default = True ) 
def plot ( * args , ** kwargs ) : 
~~~ vaex_app . plot ( * args , ** kwargs ) 
~~ def select ( * args , ** kwargs ) : 
~~~ vaex_app . select ( * args , ** kwargs ) 
sys . exit ( app . exec_ ( ) ) 
~~ def open ( self , path ) : 
if path . startswith ( "http" ) or path . startswith ( "ws" ) : 
~~~ dataset = vaex . open ( path , thread_mover = self . call_in_main_thread ) 
~~~ dataset = vaex . open ( path ) 
~~ self . add_recently_opened ( path ) 
self . dataset_selector . add ( dataset ) 
return dataset 
~~ def evaluate ( self , expression , i1 = None , i2 = None , out = None , selection = None , delay = False ) : 
~~~ expression = _ensure_strings_from_expressions ( expression ) 
result = self . server . _call_dataset ( "evaluate" , self , expression = expression , i1 = i1 , i2 = i2 , selection = selection , delay = delay ) 
~~ def delayed ( f ) : 
def wrapped ( * args , ** kwargs ) : 
~~~ key_promise = list ( [ ( key , promisify ( value ) ) for key , value in kwargs . items ( ) ] ) 
arg_promises = list ( [ promisify ( value ) for value in args ] ) 
kwarg_promises = list ( [ promise for key , promise in key_promise ] ) 
promises = arg_promises + kwarg_promises 
for promise in promises : 
~~~ def echo_error ( exc , promise = promise ) : 
~~ def echo ( value , promise = promise ) : 
~~ ~~ allarguments = aplus . listPromise ( * promises ) 
def call ( _ ) : 
~~~ kwargs_real = { key : promise . get ( ) for key , promise in key_promise } 
args_real = list ( [ promise . get ( ) for promise in arg_promises ] ) 
return f ( * args_real , ** kwargs_real ) 
~~ def error ( exc ) : 
~~~ print ( "error" , exc ) 
raise exc 
~~ return allarguments . then ( call , error ) 
~~ return wrapped 
~~ def _depending_columns ( self , ds ) : 
depending = set ( ) 
for expression in self . expressions : 
depending |= expression . variables ( ) 
~~ if self . previous_selection : 
~~~ depending |= self . previous_selection . _depending_columns ( ds ) 
~~ return depending 
~~ def limits ( self , value , square = False ) : 
if isinstance ( value , six . string_types ) : 
~~~ import re 
match = re . match ( r"(\\d*)(\\D*)" , value ) 
if match is None : 
~~~ value , type = match . groups ( ) 
import ast 
value = ast . literal_eval ( value ) 
type = type . strip ( ) 
if type in [ "s" , "sigma" ] : 
~~~ return self . limits_sigma ( value ) 
~~ elif type in [ "ss" , "sigmasquare" ] : 
~~~ return self . limits_sigma ( value , square = True ) 
~~ elif type in [ "%" , "percent" ] : 
~~~ return self . limits_percentage ( value ) 
~~ elif type in [ "%s" , "%square" , "percentsquare" ] : 
~~~ return self . limits_percentage ( value , square = True ) 
~~ ~~ ~~ if value is None : 
~~~ return self . limits_percentage ( square = square ) 
~~~ return value 
~~ ~~ def plot ( self , grid = None , size = 256 , limits = None , square = False , center = None , weight = None , weight_stat = "mean" , figsize = None , 
aspect = "auto" , f = "identity" , axes = None , xlabel = None , ylabel = None , 
group_by = None , group_limits = None , group_colors = 'jet' , group_labels = None , group_count = None , 
vmin = None , vmax = None , 
cmap = "afmhot" , 
import pylab 
f = _parse_f ( f ) 
limits = self . limits ( limits ) 
if limits is None : 
~~~ limits = self . limits_sigma ( ) 
~~ if group_limits is None and group_by : 
~~~ group_limits = tuple ( self . df ( group_by ) . minmax ( ) [ 0 ] ) + ( group_count , ) 
~~ if figsize is not None : 
~~~ pylab . figure ( num = None , figsize = figsize , dpi = 80 , facecolor = 'w' , edgecolor = 'k' ) 
~~ if axes is None : 
~~~ axes = pylab . gca ( ) 
~~ fig = pylab . gcf ( ) 
pylab . xlabel ( xlabel or self . expressions [ 0 ] ) 
pylab . ylabel ( ylabel or self . expressions [ 1 ] ) 
rgba8 = self . image_rgba ( grid = grid , size = size , limits = limits , square = square , center = center , weight = weight , weight_stat = weight_stat , 
f = f , axes = axes , 
group_by = group_by , group_limits = group_limits , group_colors = group_colors , group_count = group_count , 
vmin = vmin , vmax = vmax , 
cmap = cmap ) 
import matplotlib 
if group_by : 
~~~ if isinstance ( group_colors , six . string_types ) : 
~~~ group_colors = matplotlib . cm . get_cmap ( group_colors ) 
~~ if isinstance ( group_colors , matplotlib . colors . Colormap ) : 
~~~ group_count = group_limits [ 2 ] 
colors = [ group_colors ( k / float ( group_count - 1. ) ) for k in range ( group_count ) ] 
~~~ colors = [ matplotlib . colors . colorConverter . to_rgba ( k ) for k in group_colors ] 
~~ colormap = matplotlib . colors . ListedColormap ( colors ) 
delta = ( gmax - gmin ) / ( group_count - 1. ) 
norm = matplotlib . colors . Normalize ( gmin - delta / 2 , gmax + delta / 2 ) 
sm = matplotlib . cm . ScalarMappable ( norm , colormap ) 
colorbar = fig . colorbar ( sm ) 
if group_labels : 
~~~ colorbar . set_ticks ( np . arange ( gmin , gmax + delta / 2 , delta ) ) 
colorbar . set_ticklabels ( group_labels ) 
colorbar . set_ticklabels ( map ( lambda x : "%f" % x , np . arange ( gmin , gmax + delta / 2 , delta ) ) ) 
~~ colorbar . ax . set_ylabel ( group_by ) 
im = axes . imshow ( rgba8 , extent = np . array ( limits ) . flatten ( ) , origin = "lower" , aspect = aspect , ** kwargs ) 
~~~ norm = matplotlib . colors . Normalize ( 0 , 23 ) 
sm = matplotlib . cm . ScalarMappable ( norm , cmap ) 
colorbar = None 
~~ return im , colorbar 
~~ def plot1d ( self , grid = None , size = 64 , limits = None , weight = None , figsize = None , f = "identity" , axes = None , xlabel = None , ylabel = None , ** kwargs ) : 
~~ if grid is None : 
~~~ grid = self . histogram ( limits = limits , size = size , weight = weight ) 
~~ pylab . xlabel ( xlabel or self . expressions [ 0 ] ) 
pylab . ylabel ( "counts" or ylabel ) 
N = len ( grid ) 
xmin , xmax = limits [ 0 ] 
return pylab . plot ( np . arange ( N ) / ( N - 1.0 ) * ( xmax - xmin ) + xmin , f ( grid , ) , drawstyle = "steps" , ** kwargs ) 
~~ def bounded_by_sigmas ( self , sigmas = 3 , square = False ) : 
bounds = self . limits_sigma ( sigmas = sigmas , square = square ) 
return SubspaceBounded ( self , bounds ) 
~~ def _task ( self , task , progressbar = False ) : 
if self . delay : 
~~~ return self . executor . schedule ( task ) 
~~~ import vaex . utils 
callback = None 
~~~ if progressbar == True : 
~~~ def update ( fraction ) : 
~~~ bar . update ( fraction ) 
return True 
~~ bar = vaex . utils . progressbar ( task . name ) 
callback = self . executor . signal_progress . connect ( update ) 
~~ elif progressbar : 
~~~ callback = self . executor . signal_progress . connect ( progressbar ) 
~~ result = self . executor . run ( task ) 
if progressbar == True : 
~~~ bar . finish ( ) 
sys . stdout . write ( '\\n' ) 
~~ finally : 
~~~ if callback : 
~~~ self . executor . signal_progress . disconnect ( callback ) 
~~ ~~ ~~ ~~ def sort ( self , Ncol , order ) : 
self . emit ( QtCore . SIGNAL ( "layoutAboutToBeChanged()" ) ) 
if Ncol == 0 : 
sortlist = list ( zip ( self . pairs , list ( range ( len ( self . pairs ) ) ) ) ) 
print ( sortlist ) 
sortlist . sort ( key = operator . itemgetter ( 0 ) ) 
self . indices = list ( map ( operator . itemgetter ( 1 ) , sortlist ) ) 
print ( ( self . indices ) ) 
~~ if Ncol == 1 : 
~~~ if None not in self . ranking : 
~~~ sortlist = list ( zip ( self . ranking , list ( range ( len ( self . pairs ) ) ) ) ) 
~~~ self . indices = list ( range ( len ( self . pairs ) ) ) 
~~ print ( ( self . indices ) ) 
~~ if order == QtCore . Qt . DescendingOrder : 
~~~ self . indices . reverse ( ) 
self . emit ( QtCore . SIGNAL ( "layoutChanged()" ) ) 
~~ def export_hdf5_v1 ( dataset , path , column_names = None , byteorder = "=" , shuffle = False , selection = False , progress = None , virtual = True ) : 
~~~ h5data_output = h5file_output . require_group ( "data" ) 
for column_name in column_names : 
~~~ if column_name in dataset . get_column_names ( strings = True ) : 
dtype = column . dtype 
~~~ dtype = np . float64 ( ) . dtype 
shape = ( N , ) 
~~ if dtype . type == np . datetime64 : 
~~~ array = h5file_output . require_dataset ( "/data/%s" % column_name , shape = shape , dtype = np . int64 ) 
~~~ array = h5file_output . require_dataset ( "/data/%s" % column_name , shape = shape , dtype = dtype . newbyteorder ( byteorder ) ) 
~~ random_index_name = None 
~~ shuffle_array = h5file_output . require_dataset ( "/data/" + random_index_name , shape = ( N , ) , dtype = byteorder + "i8" ) 
~~ dataset_output = vaex . hdf5 . dataset . Hdf5MemoryMapped ( path , write = True ) 
column_names = vaex . export . _export ( dataset_input = dataset , 
dataset_output = dataset_output , 
path = path , 
random_index_column = random_index_name , 
column_names = column_names , 
selection = selection , 
shuffle = shuffle , 
byteorder = byteorder , 
progress = progress ) 
~~ def getinfo ( filename , seek = None ) : 
keys = ( 'Npart' , 'Massarr' , 'Time' , 'Redshift' , 'FlagSfr' , 'FlagFeedback' , 'Nall' , 'FlagCooling' , 'NumFiles' , 'BoxSize' , 'Omega0' , 'OmegaLambda' , 'HubbleParam' , 'FlagAge' , 'FlagMetals' , 'NallHW' , 'flag_entr_ics' , 'filename' ) 
f = open ( filename , 'rb' ) 
firstbytes = struct . unpack ( 'I' , f . read ( 4 ) ) 
if firstbytes [ 0 ] == 8 : 
~~~ gtype = 2 
~~~ gtype = 1 
~~ if gtype == 2 : 
~~~ f . seek ( 16 ) 
~~~ f . seek ( 0 ) 
~~ if seek is not None : 
~~~ f . seek ( seek ) 
~~ raw = struct . unpack ( HEAD , f . read ( 264 ) ) [ 1 : - 1 ] 
values = ( raw [ : 6 ] , raw [ 6 : 12 ] ) + raw [ 12 : 16 ] + ( raw [ 16 : 22 ] , ) + raw [ 22 : 30 ] + ( raw [ 30 : 36 ] , raw [ 36 ] , filename ) 
header = dict ( list ( zip ( keys , values ) ) ) 
f . close ( ) 
if gtype == 2 : 
~~~ posoffset = ( 2 * 16 + ( 8 + 256 ) ) 
~~~ posoffset = ( 8 + 256 ) 
~~ Npart = sum ( header [ 'Npart' ] ) 
~~~ veloffset = 3 * 16 + ( 8 + 256 ) + ( 8 + 3 * 4 * Npart ) 
~~~ veloffset = ( 8 + 256 ) + ( 8 + 3 * 4 * Npart ) 
~~ return Npart , posoffset + 4 , veloffset + 4 , header 
~~ def _export ( dataset_input , dataset_output , random_index_column , path , column_names = None , byteorder = "=" , shuffle = False , selection = False , progress = None , virtual = True , sort = None , ascending = True ) : 
~~ ~~ N = len ( dataset_input ) if not selection else dataset_input . selected_length ( selection ) 
~~ if shuffle and sort : 
~~ if shuffle : 
~~~ shuffle_array = dataset_output . columns [ random_index_column ] 
~~ partial_shuffle = shuffle and len ( dataset_input ) != N 
order_array = None 
order_array_inverse = None 
has_strings = any ( [ dataset_input . dtype ( k ) == str_type for k in column_names ] ) 
if partial_shuffle : 
~~~ shuffle_array_full = np . random . choice ( len ( dataset_input ) , len ( dataset_input ) , replace = False ) 
shuffle_array [ : ] = shuffle_array_full [ shuffle_array_full < N ] 
del shuffle_array_full 
order_array = shuffle_array 
~~ elif shuffle : 
~~~ shuffle_array_memory = np . random . choice ( N , N , replace = False ) 
shuffle_array [ : ] = shuffle_array_memory 
~~ if order_array is not None : 
~~~ indices_r = np . zeros_like ( order_array ) 
indices_r [ order_array ] = np . arange ( len ( order_array ) ) 
order_array_inverse = indices_r 
del indices_r 
~~ if sort : 
~~~ if selection : 
~~ logger . info ( "sorting..." ) 
indices = np . argsort ( dataset_input . evaluate ( sort ) ) 
indices_r = np . zeros_like ( indices ) 
indices_r [ indices ] = np . arange ( len ( indices ) ) 
if has_strings : 
~~~ order_array_inverse = indices if ascending else indices [ : - - 1 ] 
~~~ del indices 
~~ order_array = indices_r if ascending else indices_r [ : : - 1 ] 
~~ if progress == True : 
~~~ progress = vaex . utils . progressbar_callable ( title = "exporting" ) 
~~ progress = progress or ( lambda value : True ) 
progress_total = len ( column_names ) * len ( dataset_input ) 
progress_status = ProgressStatus ( ) 
progress_status . cancelled = False 
progress_status . value = 0 
~~~ full_mask = dataset_input . evaluate_selection_mask ( selection ) 
~~~ full_mask = None 
~~ sparse_groups = collections . defaultdict ( list ) 
string_columns = [ ] 
futures = [ ] 
thread_pool = concurrent . futures . ThreadPoolExecutor ( max_workers = 1 ) 
if True : 
~~~ for column_name in column_names : 
~~~ sparse_matrix = dataset_output . _sparse_matrix ( column_name ) 
future = thread_pool . submit ( _export_column , dataset_input , dataset_output , column_name , full_mask , 
shuffle , sort , selection , N , order_array , order_array_inverse , progress_status ) 
futures . append ( future ) 
~~ ~~ done = False 
while not done : 
~~~ done = True 
for future in futures : 
~~~ future . result ( 0.1 / 4 ) 
~~ except concurrent . futures . TimeoutError : 
~~~ done = False 
~~ ~~ if not done : 
~~~ if not progress ( progress_status . value / float ( progress_total ) ) : 
~~~ progress_status . cancelled = True 
~~ ~~ ~~ for sparse_matrix_id , column_names in sparse_groups . items ( ) : 
~~~ sparse_matrix = sparse_matrices [ sparse_matrix_id ] 
~~~ assert not shuffle 
assert selection in [ None , False ] 
column = dataset_output . columns [ column_name ] 
column . matrix . data [ : ] = dataset_input . columns [ column_name ] . matrix . data 
column . matrix . indptr [ : ] = dataset_input . columns [ column_name ] . matrix . indptr 
column . matrix . indices [ : ] = dataset_input . columns [ column_name ] . matrix . indices 
~~ ~~ return column_names 
~~ def export_fits ( dataset , path , column_names = None , shuffle = False , selection = False , progress = None , virtual = True , sort = None , ascending = True ) : 
~~ ~~ column_names = column_names or dataset . get_column_names ( virtual = virtual , strings = True ) 
data_types = [ ] 
data_shapes = [ ] 
ucds = [ ] 
units = [ ] 
if dataset . dtype ( column_name ) == str_type : 
~~~ max_length = dataset [ column_name ] . apply ( lambda x : len ( x ) ) . max ( selection = selection ) 
dtype = np . dtype ( 'S' + str ( int ( max_length ) ) ) 
~~ ucds . append ( dataset . ucds . get ( column_name ) ) 
units . append ( dataset . units . get ( column_name ) ) 
data_types . append ( dtype ) 
data_shapes . append ( shape ) 
~~~ column_names . append ( random_index_name ) 
data_types . append ( np . int64 ( ) . dtype ) 
data_shapes . append ( ( N , ) ) 
ucds . append ( None ) 
units . append ( None ) 
~~~ random_index_name = None 
~~ null_values = { key : dataset . columns [ key ] . fill_value for key in dataset . get_column_names ( ) if dataset . is_masked ( key ) and dataset . dtype ( key ) . kind != "f" } 
vaex . file . colfits . empty ( path , N , column_names , data_types , data_shapes , ucds , units , null_values = null_values ) 
~~~ del column_names [ - 1 ] 
del data_types [ - 1 ] 
del data_shapes [ - 1 ] 
~~ dataset_output = vaex . file . other . FitsBinTable ( path , write = True ) 
_export ( dataset_input = dataset , dataset_output = dataset_output , path = path , random_index_column = random_index_name , 
column_names = column_names , selection = selection , shuffle = shuffle , 
~~ def clear ( self , event ) : 
if self . useblit : 
~~~ self . background = ( 
self . canvas . copy_from_bbox ( self . canvas . figure . bbox ) ) 
~~ for line in self . vlines + self . hlines : 
~~~ line . set_visible ( False ) 
~~ self . ellipse . set_visible ( False ) 
~~ def _wait ( self ) : 
self . _plot_event = threading . Event ( ) 
self . queue_update . _wait ( ) 
self . queue_replot . _wait ( ) 
self . queue_redraw . _wait ( ) 
qt_app = QtCore . QCoreApplication . instance ( ) 
sleep = 10 
while not self . _plot_event . is_set ( ) : 
qt_app . processEvents ( ) 
QtTest . QTest . qSleep ( sleep ) 
~~ def _update_step2 ( self , layers ) : 
for dimension in range ( self . dimensions ) : 
~~~ if self . state . ranges_viewport [ dimension ] is None : 
~~~ vmin = min ( [ layer . state . ranges_grid [ dimension ] [ 0 ] for layer in layers ] ) 
vmax = max ( [ layer . state . ranges_grid [ dimension ] [ 1 ] for layer in layers ] ) 
self . state . ranges_viewport [ dimension ] = [ vmin , vmax ] 
self . check_aspect ( 0 ) 
for layer in layers : 
~~~ for d in range ( layer . dimensions ) : 
~~~ layer . set_range ( self . state . ranges_viewport [ d ] [ 0 ] , self . state . ranges_viewport [ d ] [ 1 ] , d ) 
~~ ~~ promises = [ layer . add_tasks_histograms ( ) for layer in layers ] 
executors = list ( set ( [ layer . dataset . executor for layer in layers ] ) ) 
for executor in executors : 
~~~ executor . execute ( ) 
~~ promises_histograms_done = vaex . promise . listPromise ( promises ) 
promises_histograms_done . then ( self . _update_step3 , self . on_error_or_cancel ) . end ( ) 
~~ def subdivide ( length , parts = None , max_length = None ) : 
if max_length : 
~~~ i1 = 0 
done = False 
~~~ i2 = min ( length , i1 + max_length ) 
yield i1 , i2 
i1 = i2 
if i1 == length : 
~~~ part_length = int ( math . ceil ( float ( length ) / parts ) ) 
for index in range ( parts ) : 
~~~ i1 , i2 = index * part_length , min ( length , ( index + 1 ) * part_length ) 
~~ ~~ ~~ def os_open ( document ) : 
osname = platform . system ( ) . lower ( ) 
if osname == "darwin" : 
~~ if osname == "linux" : 
os . system ( cmd ) 
~~ if osname == "windows" : 
~~ ~~ def write_to ( f , mode ) : 
if hasattr ( f , 'write' ) : 
~~~ yield f 
~~~ f = open ( f , mode ) 
yield f 
~~ ~~ def _split_and_combine_mask ( arrays ) : 
masks = [ np . ma . getmaskarray ( block ) for block in arrays if np . ma . isMaskedArray ( block ) ] 
arrays = [ block . data if np . ma . isMaskedArray ( block ) else block for block in arrays ] 
mask = None 
if masks : 
~~~ mask = masks [ 0 ] . copy ( ) 
for other in masks [ 1 : ] : 
~~~ mask |= other 
~~ ~~ return arrays , mask 
~~ def plot2d_contour ( self , x = None , y = None , what = "count(*)" , limits = None , shape = 256 , 
selection = None , f = "identity" , figsize = None , 
xlabel = None , ylabel = None , 
aspect = "auto" , levels = None , fill = False , 
colorbar = False , colorbar_label = None , 
colormap = None , colors = None , linewidths = None , linestyles = None , 
grid = None , show = None , ** kwargs ) : 
f = vaex . dataset . _parse_f ( f ) 
binby = [ ] 
x = vaex . dataset . _ensure_strings_from_expressions ( x ) 
y = vaex . dataset . _ensure_strings_from_expressions ( y ) 
for expression in [ y , x ] : 
~~~ if expression is not None : 
~~~ binby = [ expression ] + binby 
~~ ~~ shape = vaex . dataset . _expand_shape ( shape , 2 ) 
limits = self . limits ( binby , limits ) 
if grid is None : 
~~~ if what : 
~~~ if isinstance ( what , ( vaex . stat . Expression ) ) : 
~~~ grid = what . calculate ( self , binby = binby , limits = limits , shape = shape , selection = selection ) 
~~~ what = what . strip ( ) 
index = what . index ( "(" ) 
groups = re . match ( "(.*)\\((.*)\\)" , what ) . groups ( ) 
if groups and len ( groups ) == 2 : 
~~~ function = groups [ 0 ] 
arguments = groups [ 1 ] . strip ( ) 
functions = [ "mean" , "sum" , "std" , "count" ] 
if function in functions : 
~~~ grid = getattr ( vaex . stat , function ) ( arguments ) . calculate ( self , binby = binby , limits = limits , shape = shape , selection = selection ) 
~~ elif function == "count" and arguments == "*" : 
~~~ grid = self . count ( binby = binby , shape = shape , limits = limits , selection = selection ) 
~~ elif function == "cumulative" and arguments == "*" : 
grid = np . cumsum ( grid ) 
~~~ grid = self . histogram ( binby , size = shape , limits = limits , selection = selection ) 
~~ ~~ fgrid = f ( grid ) 
if figsize is not None : 
~~~ fig = plt . figure ( num = None , figsize = figsize , dpi = 80 , facecolor = 'w' , edgecolor = 'k' ) 
~~ fig = plt . gcf ( ) 
plt . xlabel ( xlabel or x ) 
plt . ylabel ( ylabel or y ) 
if fill == False : 
~~~ value = plt . contour ( fgrid . T , origin = "lower" , extent = np . array ( limits ) . ravel ( ) . tolist ( ) , 
linestyles = linestyles , linewidths = linewidths , levels = levels , 
colors = colors , cmap = colormap , vmin = vmin , vmax = vmax , ** kwargs ) 
~~~ value = plt . contourf ( fgrid . T , origin = "lower" , extent = np . array ( limits ) . ravel ( ) . tolist ( ) , 
linestyles = linestyles , levels = levels , colors = colors , 
cmap = colormap , vmin = vmin , vmax = vmax , ** kwargs ) 
~~ if colorbar : 
~~~ plt . colorbar ( label = colorbar_label or what ) 
~~ if show : 
~~~ plt . show ( ) 
~~ return value 
~~ def _export_table ( dataset , column_names = None , byteorder = "=" , shuffle = False , selection = False , progress = None , virtual = True , sort = None , ascending = True ) : 
for name in column_names : 
~~~ if name not in dataset . columns : 
~~ ~~ N = len ( dataset ) if not selection else dataset . selected_length ( selection ) 
~~~ random_index_column = "random_index" 
while random_index_column in dataset . get_column_names ( ) : 
~~~ random_index_column += "_new" 
~~ ~~ partial_shuffle = shuffle and len ( dataset ) != N 
~~~ shuffle_array_full = np . random . choice ( len ( dataset ) , len ( dataset ) , replace = False ) 
shuffle_array = shuffle_array_full [ shuffle_array_full < N ] 
~~~ shuffle_array = np . random . choice ( N , N , replace = False ) 
indices = np . argsort ( dataset . evaluate ( sort ) ) 
order_array = indices if ascending else indices [ : : - 1 ] 
~~ if selection : 
~~~ full_mask = dataset . evaluate_selection_mask ( selection ) 
~~ arrow_arrays = [ ] 
~~~ mask = full_mask 
~~~ values = dataset . evaluate ( column_name , filtered = False ) 
values = values [ mask ] 
~~~ values = dataset . evaluate ( column_name ) 
if shuffle or sort : 
~~~ indices = order_array 
values = values [ indices ] 
~~ ~~ arrow_arrays . append ( arrow_array_from_numpy_array ( values ) ) 
~~~ arrow_arrays . append ( arrow_array_from_numpy_array ( order_array ) ) 
column_names = column_names + [ random_index_column ] 
~~ table = pa . Table . from_arrays ( arrow_arrays , column_names ) 
return table 
~~ def nop ( self , expression , progress = False , delay = False ) : 
expression = _ensure_string_from_expression ( expression ) 
def map ( ar ) : 
~~ return self . map_reduce ( map , reduce , [ expression ] , delay = delay , progress = progress , name = 'nop' , to_numpy = False ) 
~~ def mutual_information ( self , x , y = None , mi_limits = None , mi_shape = 256 , binby = [ ] , limits = None , shape = default_shape , sort = False , selection = False , delay = False ) : 
if y is None : 
~~~ waslist , [ x , ] = vaex . utils . listify ( x ) 
~~~ waslist , [ x , y ] = vaex . utils . listify ( x , y ) 
x = list ( zip ( x , y ) ) 
if mi_limits : 
~~~ mi_limits = [ mi_limits ] 
~~ ~~ limits = self . limits ( binby , limits , delay = True ) 
mi_limits = self . limits ( x , mi_limits , delay = True ) 
@ delayed 
def calculate ( counts ) : 
~~~ counts = counts . astype ( np . float64 ) 
fullshape = _expand_shape ( shape , len ( binby ) ) 
out = np . zeros ( ( fullshape ) , dtype = float ) 
if len ( fullshape ) == 0 : 
~~~ out = vaex . kld . mutual_information ( counts ) 
~~ elif len ( fullshape ) == 1 : 
~~~ for i in range ( fullshape [ 0 ] ) : 
~~~ out [ i ] = vaex . kld . mutual_information ( counts [ ... , i ] ) 
~~ ~~ elif len ( fullshape ) == 2 : 
~~~ for j in range ( fullshape [ 1 ] ) : 
~~~ out [ i , j ] = vaex . kld . mutual_information ( counts [ ... , i , j ] ) 
~~ ~~ ~~ elif len ( fullshape ) == 3 : 
~~~ for k in range ( fullshape [ 2 ] ) : 
~~~ out [ i , j , k ] = vaex . kld . mutual_information ( counts [ ... , i , j , k ] ) 
~~ ~~ ~~ ~~ else : 
~~ return out 
~~ @ delayed 
def has_limits ( limits , mi_limits ) : 
~~~ if not _issequence ( binby ) : 
~~~ limits = [ list ( limits ) ] 
~~ values = [ ] 
for expressions , expression_limits in zip ( x , mi_limits ) : 
~~~ total_shape = _expand_shape ( mi_shape , len ( expressions ) ) + _expand_shape ( shape , len ( binby ) ) 
counts = self . count ( binby = list ( expressions ) + list ( binby ) , limits = list ( expression_limits ) + list ( limits ) , 
shape = total_shape , delay = True , selection = selection ) 
values . append ( calculate ( counts ) ) 
~~ return values 
def finish ( mi_list ) : 
~~~ if sort : 
~~~ mi_list = np . array ( mi_list ) 
indices = np . argsort ( mi_list ) [ : : - 1 ] 
sorted_x = list ( [ x [ k ] for k in indices ] ) 
return mi_list [ indices ] , sorted_x 
~~~ return np . array ( vaex . utils . unlistify ( waslist , mi_list ) ) 
~~ ~~ values = finish ( delayed_list ( has_limits ( limits , mi_limits ) ) ) 
return self . _delay ( delay , values ) 
~~ def count ( self , expression = None , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , edges = False , progress = None ) : 
return self . _compute_agg ( 'count' , expression , binby , limits , shape , selection , delay , edges , progress ) 
~~ def first ( self , expression , order_expression , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , edges = False , progress = None ) : 
return self . _compute_agg ( 'first' , expression , binby , limits , shape , selection , delay , edges , progress , extra_expressions = [ order_expression ] ) 
expression = _ensure_strings_from_expressions ( expression ) 
order_expression = _ensure_string_from_expression ( order_expression ) 
binby = _ensure_strings_from_expressions ( binby ) 
waslist , [ expressions , ] = vaex . utils . listify ( expression ) 
def finish ( * counts ) : 
~~~ counts = np . asarray ( counts ) 
return vaex . utils . unlistify ( waslist , counts ) 
~~ progressbar = vaex . utils . progressbars ( progress ) 
limits = self . limits ( binby , limits , delay = True , shape = shape ) 
stats = [ self . _first_calculation ( expression , order_expression , binby = binby , limits = limits , shape = shape , selection = selection , edges = edges , progressbar = progressbar ) for expression in expressions ] 
var = finish ( * stats ) 
return self . _delay ( delay , var ) 
~~ def mean ( self , expression , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , progress = None , edges = False ) : 
return self . _compute_agg ( 'mean' , expression , binby , limits , shape , selection , delay , edges , progress ) 
selection = _ensure_strings_from_expressions ( selection ) 
def calculate ( expression , limits ) : 
~~~ task = tasks . TaskStatistic ( self , binby , shape , limits , weight = expression , op = tasks . OP_ADD_WEIGHT_MOMENTS_01 , selection = selection ) 
self . executor . schedule ( task ) 
return task 
def finish ( * stats_args ) : 
~~~ stats = np . array ( stats_args ) 
counts = stats [ ... , 0 ] 
with np . errstate ( divide = 'ignore' , invalid = 'ignore' ) : 
~~~ mean = stats [ ... , 1 ] / counts 
~~ return vaex . utils . unlistify ( waslist , mean ) 
~~ waslist , [ expressions , ] = vaex . utils . listify ( expression ) 
progressbar = vaex . utils . progressbars ( progress ) 
limits = self . limits ( binby , limits , delay = True ) 
stats = [ calculate ( expression , limits ) for expression in expressions ] 
~~ def sum ( self , expression , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , progress = None , edges = False ) : 
return self . _compute_agg ( 'sum' , expression , binby , limits , shape , selection , delay , edges , progress ) 
def finish ( * sums ) : 
~~~ return vaex . utils . unlistify ( waslist , sums ) 
~~ expression = _ensure_strings_from_expressions ( expression ) 
sums = [ self . _sum_calculation ( expression , binby = binby , limits = limits , shape = shape , selection = selection , progressbar = progressbar ) for expression in expressions ] 
s = finish ( * sums ) 
return self . _delay ( delay , s ) 
~~ def std ( self , expression , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , progress = None ) : 
def finish ( var ) : 
~~~ return var ** 0.5 
~~ return self . _delay ( delay , finish ( self . var ( expression , binby = binby , limits = limits , shape = shape , selection = selection , delay = True , progress = progress ) ) ) 
~~ def covar ( self , x , y , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , progress = None ) : 
def cov ( mean_x , mean_y , mean_xy ) : 
~~~ return mean_xy - mean_x * mean_y 
~~ waslist , [ xlist , ylist ] = vaex . utils . listify ( x , y ) 
limits = self . limits ( binby , limits , selection = selection , delay = True ) 
def calculate ( limits ) : 
for x , y in zip ( xlist , ylist ) : 
~~~ mx = self . mean ( x , binby = binby , limits = limits , shape = shape , selection = selection , delay = True , progress = progressbar ) 
my = self . mean ( y , binby = binby , limits = limits , shape = shape , selection = selection , delay = True , progress = progressbar ) 
cxy = self . mean ( "(%s)*(%s)" % ( x , y ) , binby = binby , limits = limits , shape = shape , selection = selection , 
delay = True , progress = progressbar ) 
results . append ( cov ( mx , my , cxy ) ) 
~~ return results 
covars = calculate ( limits ) 
def finish ( covars ) : 
~~~ value = np . array ( vaex . utils . unlistify ( waslist , covars ) ) 
return value 
~~ return self . _delay ( delay , finish ( delayed_list ( covars ) ) ) 
~~ def correlation ( self , x , y = None , binby = [ ] , limits = None , shape = default_shape , sort = False , sort_key = np . abs , selection = False , delay = False , progress = None ) : 
def corr ( cov ) : 
~~~ return cov [ ... , 0 , 1 ] / ( cov [ ... , 0 , 0 ] * cov [ ... , 1 , 1 ] ) ** 0.5 
~~ ~~ if y is None : 
~~~ if not isinstance ( x , ( tuple , list ) ) : 
~~ if _issequence ( x ) and not _issequence ( x [ 0 ] ) and len ( x ) == 2 : 
~~~ x = [ x ] 
~~ if not ( _issequence ( x ) and all ( [ _issequence ( k ) and len ( k ) == 2 for k in x ] ) ) : 
~~ waslist = True 
xlist , ylist = zip ( * x ) 
~~~ waslist , [ xlist , ylist ] = vaex . utils . listify ( x , y ) 
~~ limits = self . limits ( binby , limits , selection = selection , delay = True ) 
def echo ( limits ) : 
~~ echo ( limits ) 
~~~ task = self . cov ( x , y , binby = binby , limits = limits , shape = shape , selection = selection , delay = True , 
progress = progressbar ) 
results . append ( corr ( task ) ) 
correlations = calculate ( limits ) 
def finish ( correlations ) : 
~~~ correlations = np . array ( correlations ) 
indices = np . argsort ( sort_key ( correlations ) if sort_key else correlations ) [ : : - 1 ] 
return correlations [ indices ] , sorted_x 
~~ value = np . array ( vaex . utils . unlistify ( waslist , correlations ) ) 
~~ return self . _delay ( delay , finish ( delayed_list ( correlations ) ) ) 
~~ def cov ( self , x , y = None , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , progress = None ) : 
~~~ if not _issequence ( x ) : 
~~ expressions = x 
~~~ expressions = [ x , y ] 
~~ N = len ( expressions ) 
binby = _ensure_list ( binby ) 
shape = _expand_shape ( shape , len ( binby ) ) 
def calculate ( expressions , limits ) : 
~~~ task = tasks . TaskStatistic ( self , binby , shape , limits , weights = expressions , op = tasks . OP_COV , selection = selection ) 
def finish ( values ) : 
~~~ N = len ( expressions ) 
counts = values [ ... , : N ] 
sums = values [ ... , N : 2 * N ] 
~~~ means = sums / counts 
~~ meansxy = means [ ... , None ] * means [ ... , None , : ] 
counts = values [ ... , 2 * N : 2 * N + N ** 2 ] 
sums = values [ ... , 2 * N + N ** 2 : ] 
shape = counts . shape [ : - 1 ] + ( N , N ) 
counts = counts . reshape ( shape ) 
sums = sums . reshape ( shape ) 
~~~ moments2 = sums / counts 
~~ cov_matrix = moments2 - meansxy 
return cov_matrix 
values = calculate ( expressions , limits ) 
cov_matrix = finish ( values ) 
return self . _delay ( delay , cov_matrix ) 
~~ def minmax ( self , expression , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , progress = None ) : 
def finish ( * minmax_list ) : 
~~~ value = vaex . utils . unlistify ( waslist , np . array ( minmax_list ) ) 
value = value . astype ( dtype0 ) 
~~~ task = tasks . TaskStatistic ( self , binby , shape , limits , weight = expression , op = tasks . OP_MIN_MAX , selection = selection ) 
dtypes = [ self . dtype ( expr ) for expr in expressions ] 
dtype0 = dtypes [ 0 ] 
if not all ( [ k . kind == dtype0 . kind for k in dtypes ] ) : 
~~ progressbar = vaex . utils . progressbars ( progress , name = "minmaxes" ) 
all_tasks = [ calculate ( expression , limits ) for expression in expressions ] 
result = finish ( * all_tasks ) 
return self . _delay ( delay , result ) 
~~ def min ( self , expression , binby = [ ] , limits = None , shape = default_shape , selection = False , delay = False , progress = None , edges = False ) : 
return self . _compute_agg ( 'min' , expression , binby , limits , shape , selection , delay , edges , progress ) 
def finish ( result ) : 
~~~ return result [ ... , 0 ] 
~~ return self . _delay ( delay , finish ( self . minmax ( expression , binby = binby , limits = limits , shape = shape , selection = selection , delay = delay , progress = progress ) ) ) 
~~ def median_approx ( self , expression , percentage = 50. , binby = [ ] , limits = None , shape = default_shape , percentile_shape = 256 , percentile_limits = "minmax" , selection = False , delay = False ) : 
return self . percentile_approx ( expression , 50 , binby = binby , limits = limits , shape = shape , percentile_shape = percentile_shape , percentile_limits = percentile_limits , selection = selection , delay = delay ) 
~~ def percentile_approx ( self , expression , percentage = 50. , binby = [ ] , limits = None , shape = default_shape , percentile_shape = 1024 , percentile_limits = "minmax" , selection = False , delay = False ) : 
if not isinstance ( binby , ( tuple , list ) ) : 
~~~ binby = [ binby ] 
~~~ binby = binby 
def calculate ( expression , shape , limits ) : 
~~~ return self . count ( binby = list ( binby ) + [ expression ] , shape = shape , limits = limits , selection = selection , delay = True , edges = True ) 
def finish ( percentile_limits , counts_list ) : 
for i , counts in enumerate ( counts_list ) : 
~~~ counts = counts . astype ( np . float ) 
nonnans = list ( [ slice ( 2 , - 1 , None ) for k in range ( len ( counts . shape ) - 1 ) ] ) 
nonnans = tuple ( nonnans ) 
totalcounts = np . sum ( counts . __getitem__ ( nonnans ) , - 1 ) 
empty = totalcounts == 0 
original_shape = counts . shape 
counts = np . sum ( counts , - 1 ) 
edges_floor = np . zeros ( shape [ : - 1 ] + ( 2 , ) , dtype = np . int64 ) 
edges_ceil = np . zeros ( shape [ : - 1 ] + ( 2 , ) , dtype = np . int64 ) 
values [ empty ] = 0 
floor_values = np . array ( np . floor ( values ) ) 
ceil_values = np . array ( np . ceil ( values ) ) 
vaex . vaexfast . grid_find_edges ( cumulative_grid , floor_values , edges_floor ) 
vaex . vaexfast . grid_find_edges ( cumulative_grid , ceil_values , edges_ceil ) 
def index_choose ( a , indices ) : 
~~~ out = np . zeros ( a . shape [ : - 1 ] ) 
for i in np . ndindex ( out . shape ) : 
~~~ out [ i ] = a [ i + ( indices [ i ] , ) ] 
~~ def calculate_x ( edges , values ) : 
~~~ left , right = edges [ ... , 0 ] , edges [ ... , 1 ] 
left_value = index_choose ( cumulative_grid , left ) 
right_value = index_choose ( cumulative_grid , right ) 
u = np . array ( ( values - left_value ) / ( right_value - left_value ) ) 
xleft , xright = percentile_limits [ i ] [ 0 ] + ( left - 0.5 ) * ( percentile_limits [ i ] [ 1 ] - percentile_limits [ i ] [ 0 ] ) / ( shape [ - 1 ] - 3 ) , percentile_limits [ i ] [ 0 ] + ( right - 0.5 ) * ( percentile_limits [ i ] [ 1 ] - percentile_limits [ i ] [ 0 ] ) / ( shape [ - 1 ] - 3 ) 
return x 
~~ x1 = calculate_x ( edges_floor , floor_values ) 
x2 = calculate_x ( edges_ceil , ceil_values ) 
u = values - floor_values 
x = x1 + ( x2 - x1 ) * u 
results . append ( x ) 
~~ shape = _expand_shape ( shape , len ( binby ) ) 
percentile_shapes = _expand_shape ( percentile_shape , len ( expressions ) ) 
if percentile_limits : 
~~~ percentile_limits = _expand_limits ( percentile_limits , len ( expressions ) ) 
percentile_limits = self . limits ( expressions , percentile_limits , selection = selection , delay = True ) 
def calculation ( limits , percentile_limits ) : 
~~~ tasks = [ calculate ( expression , tuple ( shape ) + ( percentile_shape , ) , list ( limits ) + [ list ( percentile_limit ) ] ) 
for percentile_shape , percentile_limit , expression 
in zip ( percentile_shapes , percentile_limits , expressions ) ] 
return finish ( percentile_limits , delayed_args ( * tasks ) ) 
~~ result = calculation ( limits , percentile_limits ) 
def finish2 ( grid ) : 
~~~ value = vaex . utils . unlistify ( waslist , np . array ( grid ) ) 
~~ return self . _delay ( delay , finish2 ( result ) ) 
~~ def limits_percentage ( self , expression , percentage = 99.73 , square = False , delay = False ) : 
import scipy 
limits = [ ] 
for expr in expressions : 
~~~ subspace = self ( expr ) 
limits_minmax = subspace . minmax ( ) 
vmin , vmax = limits_minmax [ 0 ] 
size = 1024 * 16 
counts = subspace . histogram ( size = size , limits = limits_minmax ) 
cumcounts = np . concatenate ( [ [ 0 ] , np . cumsum ( counts ) ] ) 
cumcounts /= cumcounts . max ( ) 
f = ( 1 - percentage / 100. ) / 2 
x = np . linspace ( vmin , vmax , size + 1 ) 
l = scipy . interp ( [ f , 1 - f ] , cumcounts , x ) 
limits . append ( l ) 
~~ return vaex . utils . unlistify ( waslist , limits ) 
~~ def limits ( self , expression , value = None , square = False , selection = None , delay = False , shape = None ) : 
if expression == [ ] : 
~~~ return [ ] if shape is None else ( [ ] , [ ] ) 
expressions = _ensure_strings_from_expressions ( expressions ) 
if value is None : 
~~~ value = "99.73%" 
~~ if _is_limit ( value ) or not _issequence ( value ) : 
~~~ values = ( value , ) * len ( expressions ) 
~~~ values = value 
~~ initial_expressions , initial_values = expressions , values 
expression_values = dict ( ) 
expression_shapes = dict ( ) 
for i , ( expression , value ) in enumerate ( zip ( expressions , values ) ) : 
~~~ if _issequence ( expression ) : 
~~~ expressions = expression 
nested = True 
~~~ expressions = [ expression ] 
nested = False 
~~ for j , ( expression , value ) in enumerate ( zip ( expressions , values ) ) : 
~~~ if shape is not None : 
~~~ if _issequence ( shape ) : 
~~~ shapes = shape 
~~~ shapes = ( shape , ) * ( len ( expressions ) if nested else len ( initial_expressions ) ) 
~~ ~~ shape_index = j if nested else i 
~~~ expression_values [ ( expression , value ) ] = None 
~~ if self . is_category ( expression ) : 
~~~ N = self . _categories [ _ensure_string_from_expression ( expression ) ] [ 'N' ] 
expression_shapes [ expression ] = min ( N , shapes [ shape_index ] if shape is not None else default_shape ) 
~~~ expression_shapes [ expression ] = shapes [ shape_index ] if shape is not None else default_shape 
~~ ~~ ~~ limits_list = [ ] 
for expression , value in expression_values . keys ( ) : 
~~~ if self . is_category ( expression ) : 
limits = [ - 0.5 , N - 0.5 ] 
~~~ if isinstance ( value , six . string_types ) : 
~~~ if value == "minmax" : 
~~~ limits = self . minmax ( expression , selection = selection , delay = True ) 
~~~ match = re . match ( r"([\\d.]*)(\\D*)" , value ) 
~~~ number , type = match . groups ( ) 
number = ast . literal_eval ( number ) 
~~~ limits = self . limits_sigma ( number ) 
~~~ limits = self . limits_sigma ( number , square = True ) 
~~~ limits = self . limits_percentage ( expression , number , delay = False ) 
~~~ limits = self . limits_percentage ( expression , number , square = True , delay = True ) 
~~ ~~ ~~ ~~ elif value is None : 
~~~ limits = self . limits_percentage ( expression , square = square , delay = True ) 
~~~ limits = value 
~~ ~~ limits_list . append ( limits ) 
~~ expression_values [ ( expression , value ) ] = limits 
~~ limits_list = delayed_args ( * limits_list ) 
def finish ( limits_list ) : 
~~~ limits_outer = [ ] 
shapes_list = [ ] 
for expression , value in zip ( initial_expressions , initial_values ) : 
waslist2 = True 
waslist2 = False 
~~ limits = [ ] 
shapes = [ ] 
for expression , value in zip ( expressions , values ) : 
~~~ if not _is_limit ( value ) : 
~~~ value = expression_values [ ( expression , value ) ] 
if not _is_limit ( value ) : 
~~~ value = value . get ( ) 
~~ ~~ limits . append ( value ) 
shapes . append ( expression_shapes [ expression ] ) 
~~ if waslist2 : 
~~~ limits_outer . append ( limits ) 
shapes_list . append ( shapes ) 
~~~ limits_outer . append ( limits [ 0 ] ) 
shapes_list . append ( shapes [ 0 ] ) 
~~ ~~ if shape : 
~~~ return vaex . utils . unlistify ( waslist , limits_outer ) , vaex . utils . unlistify ( waslist , shapes_list ) 
~~~ return vaex . utils . unlistify ( waslist , limits_outer ) 
~~ ~~ return self . _delay ( delay , finish ( limits_list ) ) 
~~ def mode ( self , expression , binby = [ ] , limits = None , shape = 256 , mode_shape = 64 , mode_limits = None , progressbar = False , selection = None ) : 
if len ( binby ) == 0 : 
~~~ len ( shape ) 
shape = tuple ( shape ) 
~~~ shape = len ( binby ) * ( shape , ) 
~~ shape = ( mode_shape , ) + shape 
subspace = self ( * ( list ( binby ) + [ expression ] ) ) 
~~~ subspace = subspace . selected ( ) 
~~ limits = self . limits ( list ( binby ) , limits ) 
mode_limits = self . limits ( [ expression ] , mode_limits ) 
limits = list ( limits ) + list ( mode_limits ) 
counts = subspace . histogram ( limits = limits , size = shape , progressbar = progressbar ) 
indices = np . argmax ( counts , axis = 0 ) 
pmin , pmax = limits [ - 1 ] 
modes = centers [ indices ] 
ok = counts . sum ( axis = 0 ) > 0 
modes [ ~ ok ] = np . nan 
return modes 
~~ ~~ def plot_widget ( self , x , y , z = None , grid = None , shape = 256 , limits = None , what = "count(*)" , figsize = None , 
f = "identity" , figure_key = None , fig = None , axes = None , xlabel = None , ylabel = None , title = None , 
show = True , selection = [ None , True ] , colormap = "afmhot" , grid_limits = None , normalize = "normalize" , 
grid_before = None , 
what_kwargs = { } , type = "default" , 
scales = None , tool_select = False , bq_cleanup = True , 
backend = "bqplot" , 
import vaex . jupyter . plot 
backend = vaex . jupyter . plot . create_backend ( backend ) 
cls = vaex . jupyter . plot . get_type ( type ) 
x = _ensure_strings_from_expressions ( x ) 
y = _ensure_strings_from_expressions ( y ) 
z = _ensure_strings_from_expressions ( z ) 
~~~ if name in kwargs : 
~~~ kwargs [ name ] = _ensure_strings_from_expressions ( kwargs [ name ] ) 
~~ ~~ plot2d = cls ( backend = backend , dataset = self , x = x , y = y , z = z , grid = grid , shape = shape , limits = limits , what = what , 
f = f , figure_key = figure_key , fig = fig , 
selection = selection , grid_before = grid_before , 
grid_limits = grid_limits , normalize = normalize , colormap = colormap , what_kwargs = what_kwargs , ** kwargs ) 
if show : 
~~~ plot2d . show ( ) 
~~ return plot2d 
~~ def healpix_count ( self , expression = None , healpix_expression = None , healpix_max_level = 12 , healpix_level = 8 , binby = None , limits = None , shape = default_shape , delay = False , progress = None , selection = None ) : 
import healpy as hp 
if healpix_expression is None : 
~~~ healpix_expression = "source_id/34359738368" 
~~ ~~ if healpix_expression is None : 
~~ reduce_level = healpix_max_level - healpix_level 
NSIDE = 2 ** healpix_level 
nmax = hp . nside2npix ( NSIDE ) 
scaling = 4 ** reduce_level 
expr = "%s/%s" % ( healpix_expression , scaling ) 
binby = [ expr ] + ( [ ] if binby is None else _ensure_list ( binby ) ) 
shape = ( nmax , ) + _expand_shape ( shape , len ( binby ) - 1 ) 
epsilon = 1. / scaling / 2 
limits = [ [ - epsilon , nmax - epsilon ] ] + ( [ ] if limits is None else limits ) 
return self . count ( expression , binby = binby , limits = limits , shape = shape , delay = delay , progress = progress , selection = selection ) 
~~ def healpix_plot ( self , healpix_expression = "source_id/34359738368" , healpix_max_level = 12 , healpix_level = 8 , what = "count(*)" , selection = None , 
grid = None , 
healpix_input = "equatorial" , healpix_output = "galactic" , f = None , 
colormap = "afmhot" , grid_limits = None , image_size = 800 , nest = True , 
figsize = None , interactive = False , title = "" , smooth = None , show = False , colorbar = True , 
rotation = ( 0 , 0 , 0 ) , ** kwargs ) : 
import pylab as plt 
~~~ reduce_level = healpix_max_level - healpix_level 
grid = self . _stat ( what = what , binby = "%s/%s" % ( healpix_expression , scaling ) , limits = [ - epsilon , nmax - epsilon ] , shape = nmax , selection = selection ) 
~~ if grid_limits : 
~~~ grid_min , grid_max = grid_limits 
~~~ grid_min = grid_max = None 
~~ f_org = f 
if smooth : 
~~~ if nest : 
~~~ grid = hp . reorder ( grid , inp = "NEST" , out = "RING" ) 
nest = False 
~~ grid = hp . smoothing ( grid , sigma = np . radians ( smooth ) ) 
~~ fgrid = f ( grid ) 
coord_map = dict ( equatorial = 'C' , galactic = 'G' , ecliptic = "E" ) 
fig = plt . gcf ( ) 
~~~ fig . set_size_inches ( * figsize ) 
~~ what_label = what 
if f_org : 
~~ f = hp . mollzoom if interactive else hp . mollview 
with warnings . catch_warnings ( ) : 
~~~ warnings . simplefilter ( "ignore" ) 
coord = coord_map [ healpix_input ] , coord_map [ healpix_output ] 
if coord_map [ healpix_input ] == coord_map [ healpix_output ] : 
~~~ coord = None 
~~ f ( fgrid , unit = what_label , rot = rotation , nest = nest , title = title , coord = coord , 
cmap = colormap , hold = True , xsize = image_size , min = grid_min , max = grid_max , cbar = colorbar , ** kwargs ) 
~~ ~~ def plot3d ( self , x , y , z , vx = None , vy = None , vz = None , vwhat = None , limits = None , grid = None , what = "count(*)" , shape = 128 , selection = [ None , True ] , f = None , 
vcount_limits = None , 
smooth_pre = None , smooth_post = None , grid_limits = None , normalize = "normalize" , colormap = "afmhot" , 
figure_key = None , fig = None , 
lighting = True , level = [ 0.1 , 0.5 , 0.9 ] , opacity = [ 0.01 , 0.05 , 0.1 ] , level_width = 0.1 , 
show = True , ** kwargs ) : 
import vaex . ext . ipyvolume 
cls = vaex . ext . ipyvolume . PlotDefault 
plot3d = cls ( df = self , x = x , y = y , z = z , vx = vx , vy = vy , vz = vz , 
grid = grid , shape = shape , limits = limits , what = what , 
selection = selection , smooth_pre = smooth_pre , smooth_post = smooth_post , 
grid_limits = grid_limits , vcount_limits = vcount_limits , normalize = normalize , colormap = colormap , ** kwargs ) 
~~~ plot3d . show ( ) 
~~ return plot3d 
~~ def col ( self ) : 
class ColumnList ( object ) : 
~~ data = ColumnList ( ) 
for name in self . get_column_names ( ) : 
~~~ expression = getattr ( self , name , None ) 
if not isinstance ( expression , Expression ) : 
~~~ expression = Expression ( self , name ) 
~~ setattr ( data , name , expression ) 
~~ return data 
~~ def byte_size ( self , selection = False , virtual = False ) : 
bytes_per_row = 0 
N = self . count ( selection = selection ) 
extra = 0 
for column in list ( self . get_column_names ( virtual = virtual ) ) : 
~~~ dtype = self . dtype ( column ) 
dtype_internal = self . dtype ( column , internal = True ) 
if isinstance ( self . columns [ column ] , ColumnString ) : 
~~~ extra += self . columns [ column ] . nbytes 
~~~ bytes_per_row += dtype_internal . itemsize 
if np . ma . isMaskedArray ( self . columns [ column ] ) : 
~~~ bytes_per_row += 1 
~~ ~~ ~~ return bytes_per_row * self . count ( selection = selection ) + extra 
~~ def dtype ( self , expression , internal = False ) : 
if expression in self . variables : 
~~~ return np . float64 ( 1 ) . dtype 
~~ elif expression in self . columns . keys ( ) : 
~~~ column = self . columns [ expression ] 
data = column [ 0 : 1 ] 
dtype = data . dtype 
~~~ data = self . evaluate ( expression , 0 , 1 , filtered = False ) 
~~ if not internal : 
~~~ if dtype != str_type : 
~~~ if dtype . kind in 'US' : 
~~~ return str_type 
~~ if dtype . kind == 'O' : 
~~~ if isinstance ( data [ 0 ] , six . string_types ) : 
~~ ~~ ~~ ~~ return dtype 
~~ def dtypes ( self ) : 
return Series ( { column_name : self . dtype ( column_name ) for column_name in self . get_column_names ( ) } ) 
~~ def is_masked ( self , column ) : 
column = _ensure_string_from_expression ( column ) 
if column in self . columns : 
~~~ return np . ma . isMaskedArray ( self . columns [ column ] ) 
~~ return False 
~~ def unit ( self , expression , default = None ) : 
~~~ unit_or_quantity = eval ( expression , expression_namespace , scopes . UnitScope ( self ) ) 
unit = unit_or_quantity . unit if hasattr ( unit_or_quantity , "unit" ) else unit_or_quantity 
return unit if isinstance ( unit , astropy . units . Unit ) else None 
~~~ return eval ( expression , expression_namespace , scopes . UnitScope ( self , 1. ) ) . unit 
~~~ return default 
~~ ~~ ~~ def ucd_find ( self , ucds , exclude = [ ] ) : 
if isinstance ( ucds , six . string_types ) : 
~~~ ucds = [ ucds ] 
~~ if len ( ucds ) == 1 : 
~~~ ucd = ucds [ 0 ] 
~~~ ucd = ucd [ 1 : ] 
columns = [ name for name in self . get_column_names ( ) if self . ucds . get ( name , "" ) . startswith ( ucd ) and name not in exclude ] 
~~~ columns = [ name for name in self . get_column_names ( ) if ucd in self . ucds . get ( name , "" ) and name not in exclude ] 
~~ return None if len ( columns ) == 0 else columns [ 0 ] 
~~~ columns = [ self . ucd_find ( [ ucd ] , exclude = exclude ) for ucd in ucds ] 
return None if None in columns else columns 
~~ ~~ def get_private_dir ( self , create = False ) : 
if self . is_local ( ) : 
~~~ server = self . server 
name = "%s_%s_%s_%s" % ( server . hostname , server . port , server . base_path . replace ( "/" , "_" ) , self . name ) 
~~ dir = os . path . join ( vaex . utils . get_private_dir ( ) , "dfs" , name ) 
if create and not os . path . exists ( dir ) : 
~~~ os . makedirs ( dir ) 
~~ return dir 
~~ def state_get ( self ) : 
virtual_names = list ( self . virtual_columns . keys ( ) ) + list ( self . variables . keys ( ) ) 
units = { key : str ( value ) for key , value in self . units . items ( ) } 
ucds = { key : value for key , value in self . ucds . items ( ) if key in virtual_names } 
descriptions = { key : value for key , value in self . descriptions . items ( ) } 
import vaex . serialize 
def check ( key , value ) : 
~~~ if not vaex . serialize . can_serialize ( value . f ) : 
~~ return True 
~~ def clean ( value ) : 
~~~ return vaex . serialize . to_dict ( value . f ) 
~~ functions = { key : clean ( value ) for key , value in self . functions . items ( ) if check ( key , value ) } 
virtual_columns = { key : value for key , value in self . virtual_columns . items ( ) } 
selections = { name : self . get_selection ( name ) for name , history in self . selection_histories . items ( ) } 
selections = { name : selection . to_dict ( ) if selection is not None else None for name , selection in selections . items ( ) } 
state = dict ( virtual_columns = virtual_columns , 
column_names = self . column_names , 
renamed_columns = self . _renamed_columns , 
variables = self . variables , 
functions = functions , 
selections = selections , 
ucds = ucds , 
units = units , 
descriptions = descriptions , 
description = self . description , 
active_range = [ self . _index_start , self . _index_end ] ) 
return state 
~~ def state_set ( self , state , use_active_range = False ) : 
self . description = state [ 'description' ] 
if use_active_range : 
~~~ self . _index_start , self . _index_end = state [ 'active_range' ] 
~~ self . _length_unfiltered = self . _index_end - self . _index_start 
if 'renamed_columns' in state : 
~~~ for old , new in state [ 'renamed_columns' ] : 
~~~ self . _rename ( old , new ) 
~~ ~~ for name , value in state [ 'functions' ] . items ( ) : 
~~~ self . add_function ( name , vaex . serialize . from_dict ( value ) ) 
~~ if 'column_names' in state : 
~~~ self . column_names = [ ] 
self . virtual_columns = collections . OrderedDict ( ) 
for name , value in state [ 'virtual_columns' ] . items ( ) : 
~~~ self [ name ] = self . _expr ( value ) 
~~ self . column_names = state [ 'column_names' ] 
~~~ self . virtual_columns = collections . OrderedDict ( ) 
~~ ~~ self . variables = state [ 'variables' ] 
units = { key : astropy . units . Unit ( value ) for key , value in state [ "units" ] . items ( ) } 
self . units . update ( units ) 
for name , selection_dict in state [ 'selections' ] . items ( ) : 
~~~ if selection_dict is None : 
~~~ selection = None 
~~~ selection = selections . selection_from_dict ( selection_dict ) 
~~ self . set_selection ( selection , name = name ) 
~~ ~~ def state_load ( self , f , use_active_range = False ) : 
state = vaex . utils . read_json_or_yaml ( f ) 
self . state_set ( state , use_active_range = use_active_range ) 
~~ def remove_virtual_meta ( self ) : 
dir = self . get_private_dir ( create = True ) 
path = os . path . join ( dir , "virtual_meta.yaml" ) 
~~~ os . remove ( path ) 
~~ if not os . listdir ( dir ) : 
~~~ os . rmdir ( dir ) 
~~ ~~ def write_virtual_meta ( self ) : 
path = os . path . join ( self . get_private_dir ( create = True ) , "virtual_meta.yaml" ) 
units = { key : str ( value ) for key , value in self . units . items ( ) if key in virtual_names } 
descriptions = { key : value for key , value in self . descriptions . items ( ) if key in virtual_names } 
meta_info = dict ( virtual_columns = self . virtual_columns , 
ucds = ucds , units = units , descriptions = descriptions ) 
vaex . utils . write_json_or_yaml ( path , meta_info ) 
~~ def update_virtual_meta ( self ) : 
~~~ path = os . path . join ( self . get_private_dir ( create = False ) , "virtual_meta.yaml" ) 
if os . path . exists ( path ) : 
~~~ meta_info = vaex . utils . read_json_or_yaml ( path ) 
if 'virtual_columns' not in meta_info : 
~~ self . virtual_columns . update ( meta_info [ "virtual_columns" ] ) 
self . variables . update ( meta_info [ "variables" ] ) 
self . ucds . update ( meta_info [ "ucds" ] ) 
self . descriptions . update ( meta_info [ "descriptions" ] ) 
units = { key : astropy . units . Unit ( value ) for key , value in meta_info [ "units" ] . items ( ) } 
~~ ~~ def write_meta ( self ) : 
path = os . path . join ( self . get_private_dir ( create = True ) , "meta.yaml" ) 
meta_info = dict ( description = self . description , 
ucds = self . ucds , units = units , descriptions = self . descriptions , 
~~ def subspaces ( self , expressions_list = None , dimensions = None , exclude = None , ** kwargs ) : 
if dimensions is not None : 
~~~ expressions_list = list ( itertools . combinations ( self . get_column_names ( ) , dimensions ) ) 
if exclude is not None : 
~~~ import six 
def excluded ( expressions ) : 
~~~ if callable ( exclude ) : 
~~~ return exclude ( expressions ) 
~~ elif isinstance ( exclude , six . string_types ) : 
~~~ return exclude in expressions 
~~ elif isinstance ( exclude , ( list , tuple ) ) : 
~~~ for e in exclude : 
~~~ if isinstance ( e , six . string_types ) : 
~~~ if e in expressions : 
~~~ return True 
~~ ~~ elif isinstance ( e , ( list , tuple ) ) : 
~~~ if set ( e ) . issubset ( expressions ) : 
~~ expressions_list = [ expr for expr in expressions_list if not excluded ( expr ) ] 
~~ import vaex . legacy 
return vaex . legacy . Subspaces ( [ self ( * expressions , ** kwargs ) for expressions in expressions_list ] ) 
~~ def set_variable ( self , name , expression_or_value , write = True ) : 
self . variables [ name ] = expression_or_value 
~~ def evaluate_variable ( self , name ) : 
if isinstance ( self . variables [ name ] , six . string_types ) : 
~~~ value = eval ( self . variables [ name ] , expression_namespace , self . variables ) 
~~~ return self . variables [ name ] 
~~ ~~ def _evaluate_selection_mask ( self , name = "default" , i1 = None , i2 = None , selection = None , cache = False ) : 
i1 = i1 or 0 
i2 = i2 or len ( self ) 
scope = scopes . _BlockScopeSelection ( self , i1 , i2 , selection , cache = cache ) 
return scope . evaluate ( name ) 
~~ def to_items ( self , column_names = None , selection = None , strings = True , virtual = False ) : 
items = [ ] 
for name in column_names or self . get_column_names ( strings = strings , virtual = virtual ) : 
~~~ items . append ( ( name , self . evaluate ( name , selection = selection ) ) ) 
~~ return items 
~~ def to_dict ( self , column_names = None , selection = None , strings = True , virtual = False ) : 
return dict ( self . to_items ( column_names = column_names , selection = selection , strings = strings , virtual = virtual ) ) 
~~ def to_copy ( self , column_names = None , selection = None , strings = True , virtual = False , selections = True ) : 
if column_names : 
~~~ column_names = _ensure_strings_from_expressions ( column_names ) 
~~ df = vaex . from_items ( * self . to_items ( column_names = column_names , selection = selection , strings = strings , virtual = False ) ) 
if virtual : 
~~~ for name , value in self . virtual_columns . items ( ) : 
~~~ df . add_virtual_column ( name , value ) 
~~ ~~ if selections : 
~~~ for key , value in self . selection_histories . items ( ) : 
~~~ if key != FILTER_SELECTION_NAME : 
~~~ df . selection_histories [ key ] = list ( value ) 
~~ ~~ for key , value in self . selection_history_indices . items ( ) : 
~~~ df . selection_history_indices [ key ] = value 
~~ ~~ ~~ df . functions . update ( self . functions ) 
df . copy_metadata ( self ) 
return df 
~~ def to_pandas_df ( self , column_names = None , selection = None , strings = True , virtual = False , index_name = None ) : 
data = self . to_dict ( column_names = column_names , selection = selection , strings = strings , virtual = virtual ) 
if index_name is not None : 
~~~ if index_name in data : 
~~~ index = data . pop ( index_name ) 
~~~ index = self . evaluate ( index_name , selection = selection ) 
~~~ index = None 
~~ df = pd . DataFrame ( data = data , index = index ) 
if index is not None : 
~~~ df . index . name = index_name 
~~ def to_arrow_table ( self , column_names = None , selection = None , strings = True , virtual = False ) : 
from vaex_arrow . convert import arrow_table_from_vaex_df 
return arrow_table_from_vaex_df ( self , column_names , selection , strings , virtual ) 
~~ def to_astropy_table ( self , column_names = None , selection = None , strings = True , virtual = False , index = None ) : 
from astropy . table import Table , Column , MaskedColumn 
meta = dict ( ) 
meta [ "name" ] = self . name 
meta [ "description" ] = self . description 
table = Table ( meta = meta ) 
for name , data in self . to_items ( column_names = column_names , selection = selection , strings = strings , virtual = virtual ) : 
~~~ data = np . array ( data ) . astype ( 'U' ) 
~~ meta = dict ( ) 
if name in self . ucds : 
~~~ meta [ "ucd" ] = self . ucds [ name ] 
~~ if np . ma . isMaskedArray ( data ) : 
~~~ cls = MaskedColumn 
~~~ cls = Column 
~~ table [ name ] = cls ( data , unit = self . unit ( name ) , description = self . descriptions . get ( name ) , meta = meta ) 
~~ return table 
~~ def validate_expression ( self , expression ) : 
vars = set ( self . get_column_names ( ) ) | set ( self . variables . keys ( ) ) 
funcs = set ( expression_namespace . keys ( ) ) 
return vaex . expresso . validate_expression ( expression , vars , funcs ) 
~~ def add_column ( self , name , f_or_array ) : 
if isinstance ( f_or_array , ( np . ndarray , Column ) ) : 
~~~ data = ar = f_or_array 
if self . _length_original is None : 
~~~ self . _length_unfiltered = _len ( data ) 
self . _length_original = _len ( data ) 
self . _index_end = self . _length_unfiltered 
~~ if _len ( ar ) != self . length_original ( ) : 
~~~ if self . filtered : 
~~~ if len ( self ) == len ( ar ) : 
~~ self . columns [ name ] = f_or_array 
if name not in self . column_names : 
~~~ self . column_names . append ( name ) 
~~ self . _save_assign_expression ( name , Expression ( self , name ) ) 
~~ def rename_column ( self , name , new_name , unique = False , store_in_state = True ) : 
new_name = vaex . utils . find_valid_name ( new_name , used = [ ] if not unique else list ( self ) ) 
data = self . columns . get ( name ) 
if data is not None : 
~~~ del self . columns [ name ] 
self . column_names [ self . column_names . index ( name ) ] = new_name 
self . columns [ new_name ] = data 
~~~ expression = self . virtual_columns [ name ] 
del self . virtual_columns [ name ] 
self . virtual_columns [ new_name ] = expression 
~~ if store_in_state : 
~~~ self . _renamed_columns . append ( ( name , new_name ) ) 
~~ for d in [ self . ucds , self . units , self . descriptions ] : 
~~~ if name in d : 
~~~ d [ new_name ] = d [ name ] 
del d [ name ] 
~~ ~~ return new_name 
~~ def add_column_healpix ( self , name = "healpix" , longitude = "ra" , latitude = "dec" , degrees = True , healpix_order = 12 , nest = True ) : 
if degrees : 
~~~ scale = "*pi/180" 
~~~ scale = "" 
~~ phi = self . evaluate ( "(%s)%s" % ( longitude , scale ) ) 
theta = self . evaluate ( "pi/2-(%s)%s" % ( latitude , scale ) ) 
hp_index = hp . ang2pix ( hp . order2nside ( healpix_order ) , theta , phi , nest = nest ) 
self . add_column ( "healpix" , hp_index ) 
~~ def propagate_uncertainties ( self , columns , depending_variables = None , cov_matrix = 'auto' , 
covariance_format = "{}_{}_covariance" , 
uncertainty_format = "{}_uncertainty" ) : 
names = _ensure_strings_from_expressions ( columns ) 
virtual_columns = self . _expr ( * columns , always_list = True ) 
if depending_variables is None : 
~~~ depending_variables = set ( ) 
for expression in virtual_columns : 
~~~ depending_variables |= expression . variables ( ) 
~~ depending_variables = list ( sorted ( list ( depending_variables ) ) ) 
~~ fs = [ self [ self . virtual_columns [ name ] ] for name in names ] 
jacobian = self . _jacobian ( fs , depending_variables ) 
m = len ( fs ) 
n = len ( depending_variables ) 
cov_matrix = self . _covariance_matrix_guess ( depending_variables , full = cov_matrix == "full" , as_expression = True ) 
cov_matrix_out = [ [ self [ '0' ] for __ in range ( m ) ] for __ in range ( m ) ] 
for i in range ( m ) : 
~~~ for j in range ( m ) : 
~~~ for k in range ( n ) : 
~~~ for l in range ( n ) : 
~~~ if jacobian [ i ] [ k ] . expression == '0' or jacobian [ j ] [ l ] . expression == '0' or cov_matrix [ k ] [ l ] . expression == '0' : 
~~~ cov_matrix_out [ i ] [ j ] = cov_matrix_out [ i ] [ j ] + jacobian [ i ] [ k ] * cov_matrix [ k ] [ l ] * jacobian [ j ] [ l ] 
~~ ~~ ~~ ~~ ~~ for i in range ( m ) : 
~~~ for j in range ( i + 1 ) : 
~~~ sigma = cov_matrix_out [ i ] [ j ] 
sigma = self . _expr ( vaex . expresso . simplify ( _ensure_string_from_expression ( sigma ) ) ) 
if i != j : 
~~~ self . add_virtual_column ( covariance_format . format ( names [ i ] , names [ j ] ) , sigma ) 
~~~ self . add_virtual_column ( uncertainty_format . format ( names [ i ] ) , np . sqrt ( sigma ) ) 
~~ ~~ ~~ ~~ def add_virtual_columns_cartesian_to_polar ( self , x = "x" , y = "y" , radius_out = "r_polar" , azimuth_out = "phi_polar" , 
x = self [ x ] 
y = self [ y ] 
if radians : 
~~~ to_degrees = "" 
~~~ to_degrees = "*180/pi" 
~~ r = np . sqrt ( x ** 2 + y ** 2 ) 
self [ radius_out ] = r 
phi = np . arctan2 ( y , x ) 
~~~ phi = phi * 180 / np . pi 
~~ self [ azimuth_out ] = phi 
~~~ self . propagate_uncertainties ( [ self [ radius_out ] , self [ azimuth_out ] ] ) 
~~ ~~ def add_virtual_columns_cartesian_velocities_to_spherical ( self , x = "x" , y = "y" , z = "z" , vx = "vx" , vy = "vy" , vz = "vz" , vr = "vr" , vlong = "vlong" , vlat = "vlat" , distance = None ) : 
~~ self . add_virtual_column ( vr , "({x}*{vx}+{y}*{vy}+{z}*{vz})/{distance}" . format ( ** locals ( ) ) ) 
self . add_virtual_column ( vlong , "-({vx}*{y}-{x}*{vy})/sqrt({x}**2+{y}**2)" . format ( ** locals ( ) ) ) 
~~ def add_virtual_columns_cartesian_velocities_to_polar ( self , x = "x" , y = "y" , vx = "vx" , radius_polar = None , vy = "vy" , vr_out = "vr_polar" , vazimuth_out = "vphi_polar" , 
propagate_uncertainties = False , ) : 
x = self . _expr ( x ) 
y = self . _expr ( y ) 
vx = self . _expr ( vx ) 
vy = self . _expr ( vy ) 
if radius_polar is None : 
~~~ radius_polar = np . sqrt ( x ** 2 + y ** 2 ) 
~~ radius_polar = self . _expr ( radius_polar ) 
self [ vr_out ] = ( x * vx + y * vy ) / radius_polar 
self [ vazimuth_out ] = ( x * vy - y * vx ) / radius_polar 
~~~ self . propagate_uncertainties ( [ self [ vr_out ] , self [ vazimuth_out ] ] ) 
~~ ~~ def add_virtual_columns_polar_velocities_to_cartesian ( self , x = 'x' , y = 'y' , azimuth = None , vr = 'vr_polar' , vazimuth = 'vphi_polar' , vx_out = 'vx' , vy_out = 'vy' , propagate_uncertainties = False ) : 
vr = self . _expr ( vr ) 
vazimuth = self . _expr ( vazimuth ) 
if azimuth is not None : 
~~~ azimuth = self . _expr ( azimuth ) 
azimuth = np . deg2rad ( azimuth ) 
~~~ azimuth = np . arctan2 ( y , x ) 
~~ azimuth = self . _expr ( azimuth ) 
self [ vx_out ] = vr * np . cos ( azimuth ) - vazimuth * np . sin ( azimuth ) 
self [ vy_out ] = vr * np . sin ( azimuth ) + vazimuth * np . cos ( azimuth ) 
~~~ self . propagate_uncertainties ( [ self [ vx_out ] , self [ vy_out ] ] ) 
~~ ~~ def add_virtual_columns_rotation ( self , x , y , xnew , ynew , angle_degrees , propagate_uncertainties = False ) : 
x = _ensure_string_from_expression ( x ) 
y = _ensure_string_from_expression ( y ) 
theta = np . radians ( angle_degrees ) 
matrix = np . array ( [ [ np . cos ( theta ) , - np . sin ( theta ) ] , [ np . sin ( theta ) , np . cos ( theta ) ] ] ) 
m = matrix_name = x + "_" + y + "_rot" 
~~~ for j in range ( 2 ) : 
~~~ self . set_variable ( matrix_name + "_%d%d" % ( i , j ) , matrix [ i , j ] . item ( ) ) 
~~~ self . propagate_uncertainties ( [ self [ xnew ] , self [ ynew ] ] ) 
~~ ~~ def add_virtual_columns_spherical_to_cartesian ( self , alpha , delta , distance , xname = "x" , yname = "y" , zname = "z" , 
center = [ 0 , 0 , 0 ] , center_name = "solar_position" , radians = False ) : 
alpha = self . _expr ( alpha ) 
delta = self . _expr ( delta ) 
distance = self . _expr ( distance ) 
~~~ alpha = alpha * self . _expr ( 'pi' ) / 180 
delta = delta * self . _expr ( 'pi' ) / 180 
~~ if center [ 0 ] : 
~~~ self [ xname ] = np . cos ( alpha ) * np . cos ( delta ) * distance + center [ 0 ] 
~~~ self [ xname ] = np . cos ( alpha ) * np . cos ( delta ) * distance 
~~ if center [ 1 ] : 
~~~ self [ yname ] = np . sin ( alpha ) * np . cos ( delta ) * distance + center [ 1 ] 
~~~ self [ yname ] = np . sin ( alpha ) * np . cos ( delta ) * distance 
~~ if center [ 2 ] : 
~~~ self [ zname ] = np . sin ( delta ) * distance + center [ 2 ] 
~~~ self [ zname ] = np . sin ( delta ) * distance 
~~~ self . propagate_uncertainties ( [ self [ xname ] , self [ yname ] , self [ zname ] ] ) 
~~ ~~ def add_virtual_columns_cartesian_to_spherical ( self , x = "x" , y = "y" , z = "z" , alpha = "l" , delta = "b" , distance = "distance" , radians = False , center = None , center_name = "solar_position" ) : 
transform = "" if radians else "*180./pi" 
if center is not None : 
~~~ self . add_variable ( center_name , center ) 
~~ if center is not None and center [ 0 ] != 0 : 
~~ if center is not None and center [ 1 ] != 0 : 
~~ if center is not None and center [ 2 ] != 0 : 
self . add_virtual_column ( delta , "(-arccos({z}/{distance})+pi/2){transform}" . format ( ** locals ( ) ) ) 
~~ def add_virtual_columns_aitoff ( self , alpha , delta , x , y , radians = True ) : 
transform = "" if radians else "*pi/180." 
aitoff_alpha = "__aitoff_alpha_%s_%s" % ( alpha , delta ) 
aitoff_alpha = re . sub ( "[^a-zA-Z_]" , "_" , aitoff_alpha ) 
self . add_virtual_column ( aitoff_alpha , "arccos(cos({delta}{transform})*cos({alpha}{transform}/2))" . format ( ** locals ( ) ) ) 
self . add_virtual_column ( x , "2*cos({delta}{transform})*sin({alpha}{transform}/2)/sinc({aitoff_alpha}/pi)/pi" . format ( ** locals ( ) ) ) 
self . add_virtual_column ( y , "sin({delta}{transform})/sinc({aitoff_alpha}/pi)/pi" . format ( ** locals ( ) ) ) 
~~ def add_virtual_column ( self , name , expression , unique = False ) : 
type = "change" if name in self . virtual_columns else "add" 
if name in self . get_column_names ( virtual = False ) : 
~~~ renamed = '__' + vaex . utils . find_valid_name ( name , used = self . get_column_names ( ) ) 
expression = self . _rename ( name , renamed , expression ) [ 0 ] . expression 
~~ name = vaex . utils . find_valid_name ( name , used = [ ] if not unique else self . get_column_names ( ) ) 
self . virtual_columns [ name ] = expression 
self . column_names . append ( name ) 
self . _save_assign_expression ( name ) 
self . signal_column_changed . emit ( self , name , "add" ) 
~~ def delete_virtual_column ( self , name ) : 
self . signal_column_changed . emit ( self , name , "delete" ) 
~~ def add_variable ( self , name , expression , overwrite = True , unique = True ) : 
if unique or overwrite or name not in self . variables : 
~~~ existing_names = self . get_column_names ( virtual = False ) + list ( self . variables . keys ( ) ) 
name = vaex . utils . find_valid_name ( name , used = [ ] if not unique else existing_names ) 
self . variables [ name ] = expression 
self . signal_variable_changed . emit ( self , name , "add" ) 
if unique : 
~~~ return name 
~~ ~~ ~~ def delete_variable ( self , name ) : 
del self . variables [ name ] 
self . signal_variable_changed . emit ( self , name , "delete" ) 
~~ def tail ( self , n = 10 ) : 
N = len ( self ) 
return self [ max ( 0 , N - n ) : min ( len ( self ) , N ) ] 
~~ def head_and_tail_print ( self , n = 5 ) : 
from IPython import display 
display . display ( display . HTML ( self . _head_and_tail_table ( n ) ) ) 
~~ def describe ( self , strings = True , virtual = True , selection = None ) : 
columns = { } 
for feature in self . get_column_names ( strings = strings , virtual = virtual ) [ : ] : 
~~~ dtype = str ( self . dtype ( feature ) ) if self . dtype ( feature ) != str else 'str' 
if self . dtype ( feature ) == str_type or self . dtype ( feature ) . kind in [ 'S' , 'U' , 'O' ] : 
~~~ count = self . count ( feature , selection = selection , delay = True ) 
self . execute ( ) 
count = count . get ( ) 
columns [ feature ] = ( ( dtype , count , N - count , '--' , '--' , '--' , '--' ) ) 
mean = self . mean ( feature , selection = selection , delay = True ) 
std = self . std ( feature , selection = selection , delay = True ) 
minmax = self . minmax ( feature , selection = selection , delay = True ) 
count , mean , std , minmax = count . get ( ) , mean . get ( ) , std . get ( ) , minmax . get ( ) 
count = int ( count ) 
columns [ feature ] = ( ( dtype , count , N - count , mean , std , minmax [ 0 ] , minmax [ 1 ] ) ) 
~~ ~~ return pd . DataFrame ( data = columns , index = [ 'dtype' , 'count' , 'missing' , 'mean' , 'std' , 'min' , 'max' ] ) 
~~ def cat ( self , i1 , i2 , format = 'html' ) : 
if format == 'html' : 
~~~ output = self . _as_html_table ( i1 , i2 ) 
display . display ( display . HTML ( output ) ) 
~~~ output = self . _as_table ( i1 , i2 , format = format ) 
print ( output ) 
~~ ~~ def set_current_row ( self , value ) : 
if ( value is not None ) and ( ( value < 0 ) or ( value >= len ( self ) ) ) : 
~~ self . _current_row = value 
self . signal_pick . emit ( self , value ) 
~~ def get_column_names ( self , virtual = True , strings = True , hidden = False , regex = None ) : 
def column_filter ( name ) : 
if regex and not re . match ( regex , name ) : 
~~ if not virtual and name in self . virtual_columns : 
~~ if not strings and ( self . dtype ( name ) == str_type or self . dtype ( name ) . type == np . string_ ) : 
~~ if not hidden and name . startswith ( '__' ) : 
~~ return [ name for name in self . column_names if column_filter ( name ) ] 
~~ def set_active_fraction ( self , value ) : 
if value != self . _active_fraction : 
~~~ self . _active_fraction = value 
self . select ( None ) 
self . set_current_row ( None ) 
self . _length_unfiltered = int ( round ( self . _length_original * self . _active_fraction ) ) 
self . _index_start = 0 
self . signal_active_fraction_changed . emit ( self , value ) 
~~ ~~ def set_active_range ( self , i1 , i2 ) : 
self . _active_fraction = ( i2 - i1 ) / float ( self . length_original ( ) ) 
self . _index_start = i1 
self . _index_end = i2 
self . _length_unfiltered = i2 - i1 
self . signal_active_fraction_changed . emit ( self , self . _active_fraction ) 
~~ def trim ( self , inplace = False ) : 
df = self if inplace else self . copy ( ) 
for name in df : 
~~~ column = df . columns . get ( name ) 
if column is not None : 
~~~ if self . _index_start == 0 and len ( column ) == self . _index_end : 
~~~ df . columns [ name ] = column [ self . _index_start : self . _index_end ] 
~~~ df . columns [ name ] = column . trim ( self . _index_start , self . _index_end ) 
~~ ~~ ~~ ~~ df . _length_original = self . length_unfiltered ( ) 
df . _length_unfiltered = df . _length_original 
df . _index_start = 0 
df . _index_end = df . _length_original 
df . _active_fraction = 1 
~~ def take ( self , indices ) : 
df = self . copy ( ) 
direct_indices_map = { } 
indices = np . array ( indices ) 
~~~ if isinstance ( column , ColumnIndexed ) : 
~~~ if id ( column . indices ) not in direct_indices_map : 
~~~ direct_indices = column . indices [ indices ] 
direct_indices_map [ id ( column . indices ) ] = direct_indices 
~~~ direct_indices = direct_indices_map [ id ( column . indices ) ] 
~~ df . columns [ name ] = ColumnIndexed ( column . df , direct_indices , column . name ) 
~~~ df . columns [ name ] = ColumnIndexed ( self , indices , name ) 
~~ ~~ ~~ df . _length_original = len ( indices ) 
df . set_selection ( None , name = FILTER_SELECTION_NAME ) 
~~ def extract ( self ) : 
trimmed = self . trim ( ) 
if trimmed . filtered : 
~~~ indices = trimmed . _filtered_range_to_unfiltered_indices ( 0 , len ( trimmed ) ) 
return trimmed . take ( indices ) 
~~~ return trimmed 
~~ ~~ def sample ( self , n = None , frac = None , replace = False , weights = None , random_state = None ) : 
self = self . extract ( ) 
if type ( random_state ) == int or random_state is None : 
~~~ random_state = np . random . RandomState ( seed = random_state ) 
~~ if n is None and frac is None : 
~~~ n = 1 
~~ elif frac is not None : 
~~~ n = int ( round ( frac * len ( self ) ) ) 
~~ weights_values = None 
if weights is not None : 
~~~ weights_values = self . evaluate ( weights ) 
weights_values = weights_values / self . sum ( weights ) 
~~ indices = random_state . choice ( len ( self ) , n , replace = replace , p = weights_values ) 
return self . take ( indices ) 
~~ def split_random ( self , frac , random_state = None ) : 
~~ indices = random_state . choice ( len ( self ) , len ( self ) , replace = False ) 
return self . take ( indices ) . split ( frac ) 
~~ def split ( self , frac ) : 
if _issequence ( frac ) : 
~~~ total = sum ( frac ) 
frac = [ k / total for k in frac ] 
frac = [ frac , 1 - frac ] 
~~ offsets = np . round ( np . cumsum ( frac ) * len ( self ) ) . astype ( np . int64 ) 
start = 0 
for offset in offsets : 
~~~ yield self [ start : offset ] 
start = offset 
~~ ~~ def sort ( self , by , ascending = True , kind = 'quicksort' ) : 
self = self . trim ( ) 
values = self . evaluate ( by , filtered = False ) 
indices = np . argsort ( values , kind = kind ) 
~~ return self . take ( indices ) 
~~ def fillna ( self , value , fill_nan = True , fill_masked = True , column_names = None , prefix = '__original_' , inplace = False ) : 
df = self . trim ( inplace = inplace ) 
column_names = column_names or list ( self ) 
~~~ new_name = df . rename_column ( name , prefix + name ) 
expr = df [ new_name ] 
df [ name ] = df . func . fillna ( expr , value , fill_nan = fill_nan , fill_masked = fill_masked ) 
~~~ df [ name ] = df . func . fillna ( df [ name ] , value , fill_nan = fill_nan , fill_masked = fill_masked ) 
~~ def materialize ( self , virtual_column , inplace = False ) : 
virtual_column = _ensure_string_from_expression ( virtual_column ) 
if virtual_column not in df . virtual_columns : 
~~ ar = df . evaluate ( virtual_column , filtered = False ) 
del df [ virtual_column ] 
df . add_column ( virtual_column , ar ) 
~~ def get_selection ( self , name = "default" ) : 
name = _normalize_selection_name ( name ) 
selection_history = self . selection_histories [ name ] 
index = self . selection_history_indices [ name ] 
if index == - 1 : 
~~~ return selection_history [ index ] 
~~ ~~ def selection_undo ( self , name = "default" , executor = None ) : 
logger . debug ( "undo" ) 
executor = executor or self . executor 
assert self . selection_can_undo ( name = name ) 
self . selection_history_indices [ name ] -= 1 
self . signal_selection_changed . emit ( self ) 
~~ def selection_redo ( self , name = "default" , executor = None ) : 
logger . debug ( "redo" ) 
assert self . selection_can_redo ( name = name ) 
next = selection_history [ index + 1 ] 
self . selection_history_indices [ name ] += 1 
~~ def selection_can_redo ( self , name = "default" ) : 
return ( self . selection_history_indices [ name ] + 1 ) < len ( self . selection_histories [ name ] ) 
~~ def select ( self , boolean_expression , mode = "replace" , name = "default" , executor = None ) : 
boolean_expression = _ensure_string_from_expression ( boolean_expression ) 
if boolean_expression is None and not self . has_selection ( name = name ) : 
~~~ def create ( current ) : 
~~~ return selections . SelectionExpression ( boolean_expression , current , mode ) if boolean_expression else None 
~~ self . _selection ( create , name ) 
~~ ~~ def select_non_missing ( self , drop_nan = True , drop_masked = True , column_names = None , mode = "replace" , name = "default" ) : 
column_names = column_names or self . get_column_names ( virtual = False ) 
def create ( current ) : 
~~~ return selections . SelectionDropNa ( drop_nan , drop_masked , column_names , current , mode ) 
~~ def dropna ( self , drop_nan = True , drop_masked = True , column_names = None ) : 
copy = self . copy ( ) 
copy . select_non_missing ( drop_nan = drop_nan , drop_masked = drop_masked , column_names = column_names , 
name = FILTER_SELECTION_NAME , mode = 'and' ) 
return copy 
~~ def select_rectangle ( self , x , y , limits , mode = "replace" , name = "default" ) : 
self . select_box ( [ x , y ] , limits , mode = mode , name = name ) 
~~ def select_box ( self , spaces , limits , mode = "replace" , name = "default" ) : 
sorted_limits = [ ( min ( l ) , max ( l ) ) for l in limits ] 
( expression , ( lmin , lmax ) ) in zip ( spaces , sorted_limits ) ] 
self . select ( "&" . join ( expressions ) , mode = mode , name = name ) 
~~ def select_circle ( self , x , y , xc , yc , r , mode = "replace" , name = "default" , inclusive = True ) : 
if inclusive : 
~~~ expr = ( self [ x ] - xc ) ** 2 + ( self [ y ] - yc ) ** 2 <= r ** 2 
~~~ expr = ( self [ x ] - xc ) ** 2 + ( self [ y ] - yc ) ** 2 < r ** 2 
~~ self . select ( boolean_expression = expr , mode = mode , name = name ) 
~~ def select_ellipse ( self , x , y , xc , yc , width , height , angle = 0 , mode = "replace" , name = "default" , radians = False , inclusive = True ) : 
~~~ alpha = np . deg2rad ( angle ) 
~~ xr = width / 2 
yr = height / 2 
r = max ( xr , yr ) 
a = xr / r 
b = yr / r 
~~~ expr = ( ( self [ x ] - xc ) * np . cos ( alpha ) + ( self [ y ] - yc ) * np . sin ( alpha ) ) ** 2 / a ** 2 + ( ( self [ x ] - xc ) * np . sin ( alpha ) - ( self [ y ] - yc ) * np . cos ( alpha ) ) ** 2 / b ** 2 <= r ** 2 
~~~ expr = ( ( self [ x ] - xc ) * np . cos ( alpha ) + ( self [ y ] - yc ) * np . sin ( alpha ) ) ** 2 / a ** 2 + ( ( self [ x ] - xc ) * np . sin ( alpha ) - ( self [ y ] - yc ) * np . cos ( alpha ) ) ** 2 / b ** 2 < r ** 2 
~~ def select_lasso ( self , expression_x , expression_y , xsequence , ysequence , mode = "replace" , name = "default" , executor = None ) : 
~~~ return selections . SelectionLasso ( expression_x , expression_y , xsequence , ysequence , current , mode ) 
~~ self . _selection ( create , name , executor = executor ) 
~~ def select_inverse ( self , name = "default" , executor = None ) : 
~~~ return selections . SelectionInvert ( current ) 
~~ def set_selection ( self , selection , name = "default" , executor = None ) : 
~~~ return selection 
~~ self . _selection ( create , name , executor = executor , execute_fully = True ) 
~~ def _selection ( self , create_selection , name , executor = None , execute_fully = False ) : 
previous_index = self . selection_history_indices [ name ] 
current = selection_history [ previous_index ] if selection_history else None 
selection = create_selection ( current ) 
selection_history . append ( selection ) 
del selection_history [ self . selection_history_indices [ name ] : - 1 ] 
if 0 : 
~~~ if self . is_local ( ) : 
~~~ result = vaex . promise . Promise . fulfilled ( None ) 
~~~ self . signal_selection_changed . emit ( self ) 
result = vaex . promise . Promise . fulfilled ( None ) 
~~ ~~ self . signal_selection_changed . emit ( self ) 
~~ def drop ( self , columns , inplace = False , check = True ) : 
columns = _ensure_list ( columns ) 
columns = _ensure_strings_from_expressions ( columns ) 
depending_columns = df . _depending_columns ( columns_exclude = columns ) 
for column in columns : 
~~~ if check and column in depending_columns : 
~~~ df . _hide_column ( column ) 
~~~ del df [ column ] 
~~ def _hide_column ( self , column ) : 
new_name = self . _find_valid_name ( '__' + column ) 
self . _rename ( column , new_name ) 
~~ def _find_valid_name ( self , initial_name ) : 
return vaex . utils . find_valid_name ( initial_name , used = self . get_column_names ( hidden = True ) ) 
~~ def _depending_columns ( self , columns = None , columns_exclude = None , check_filter = True ) : 
columns = set ( columns or self . get_column_names ( hidden = True ) ) 
if columns_exclude : 
~~~ columns -= set ( columns_exclude ) 
~~ depending_columns = set ( ) 
~~~ expression = self . _expr ( column ) 
depending_columns |= expression . variables ( ) 
~~ depending_columns -= set ( columns ) 
if check_filter : 
~~~ selection = self . get_selection ( FILTER_SELECTION_NAME ) 
depending_columns |= selection . _depending_columns ( self ) 
~~ ~~ return depending_columns 
~~ def _root_nodes ( self ) : 
root_nodes = [ ] 
leafes = [ ] 
~~~ leafes . append ( node ) 
~~~ root_nodes . remove ( node ) 
if node_repr in self . virtual_columns : 
~~~ leafes . append ( node_repr ) 
if node_repr in root_nodes : 
~~~ root_nodes . remove ( node_repr ) 
~~ ~~ for dep in deps : 
~~~ walk ( dep ) 
~~ ~~ ~~ for column in self . virtual_columns . keys ( ) : 
~~~ if column not in leafes : 
~~~ root_nodes . append ( column ) 
~~ node = self [ column ] . _graph ( ) 
node_repr , fname , fobj , deps = node 
~~ ~~ return root_nodes 
from graphviz import Digraph 
root_nodes = self . _root_nodes ( ) 
for column in root_nodes : 
~~~ self [ column ] . _graphviz ( dot = dot ) 
~~ return dot 
~~ def categorize ( self , column , labels = None , check = True ) : 
if check : 
~~~ vmin , vmax = self . minmax ( column ) 
if labels is None : 
~~~ N = int ( vmax + 1 ) 
labels = list ( map ( str , range ( N ) ) ) 
~~ if ( vmax - vmin ) >= len ( labels ) : 
~~ ~~ self . _categories [ column ] = dict ( labels = labels , N = len ( labels ) ) 
~~ def ordinal_encode ( self , column , values = None , inplace = False ) : 
df_unfiltered = df . copy ( ) 
df_unfiltered . select_nothing ( name = FILTER_SELECTION_NAME ) 
df_unfiltered . _length_unfiltered = df . _length_original 
df_unfiltered . set_active_range ( 0 , df . _length_original ) 
found_values , codes = df_unfiltered . unique ( column , return_inverse = True ) 
if values is None : 
~~~ values = found_values 
~~~ translation = np . zeros ( len ( found_values ) , dtype = np . uint64 ) 
missing_value = len ( found_values ) 
for i , found_value in enumerate ( found_values ) : 
~~~ found_value = found_value . decode ( 'ascii' ) 
~~~ translation [ i ] = missing_value 
~~~ translation [ i ] = values . index ( found_value ) 
~~ ~~ codes = translation [ codes ] 
if missing_value in translation : 
~~~ codes = np . ma . masked_array ( codes , codes == missing_value ) 
~~ ~~ original_column = df . rename_column ( column , '__original_' + column , unique = True ) 
labels = [ str ( k ) for k in values ] 
df . add_column ( column , codes ) 
df . _categories [ column ] = dict ( labels = labels , N = len ( values ) , values = values ) 
~~ def data ( self ) : 
class Datas ( object ) : 
~~ datas = Datas ( ) 
for name , array in self . columns . items ( ) : 
~~~ setattr ( datas , name , array ) 
~~ return datas 
~~ def shallow_copy ( self , virtual = True , variables = True ) : 
df = DataFrameLocal ( self . name , self . path , self . column_names ) 
df . columns . update ( self . columns ) 
df . _length_unfiltered = self . _length_unfiltered 
df . _length_original = self . _length_original 
df . _index_end = self . _index_end 
df . _index_start = self . _index_start 
df . _active_fraction = self . _active_fraction 
~~~ df . virtual_columns . update ( self . virtual_columns ) 
~~ if variables : 
~~~ df . variables . update ( self . variables ) 
~~ def length ( self , selection = False ) : 
~~~ return 0 if self . mask is None else np . sum ( self . mask ) 
~~~ return len ( self ) 
~~ ~~ def _hstack ( self , other , prefix = None ) : 
for name in other . get_column_names ( ) : 
~~~ if prefix : 
~~~ new_name = prefix + name 
~~~ new_name = name 
~~ self . add_column ( new_name , other . columns [ name ] ) 
~~ ~~ def concat ( self , other ) : 
if isinstance ( self , DataFrameConcatenated ) : 
~~~ dfs . extend ( self . dfs ) 
~~~ dfs . extend ( [ self ] ) 
~~ if isinstance ( other , DataFrameConcatenated ) : 
~~~ dfs . extend ( other . dfs ) 
~~~ dfs . extend ( [ other ] ) 
~~ return DataFrameConcatenated ( dfs ) 
~~ def evaluate ( self , expression , i1 = None , i2 = None , out = None , selection = None , filtered = True , internal = False ) : 
i2 = i2 or ( len ( self ) if ( self . filtered and filtered ) else self . length_unfiltered ( ) ) 
~~~ indices = self . _filtered_range_to_unfiltered_indices ( i1 , i2 ) 
i1 = indices [ 0 ] 
~~ if selection is not None or ( self . filtered and filtered ) : 
~~~ mask = self . evaluate_selection_mask ( selection , i1 , i2 ) 
~~ scope = scopes . _BlockScope ( self , i1 , i2 , mask = mask , ** self . variables ) 
if out is not None : 
~~~ scope . buffers [ expression ] = out 
~~ value = scope . evaluate ( expression ) 
if isinstance ( value , ColumnString ) and not internal : 
~~~ value = value . to_numpy ( ) 
~~ def compare ( self , other , report_missing = True , report_difference = False , show = 10 , orderby = None , column_names = None ) : 
if column_names is None : 
~~~ column_names = self . get_column_names ( virtual = False ) 
for other_column_name in other . get_column_names ( virtual = False ) : 
~~~ if other_column_name not in column_names : 
~~~ column_names . append ( other_column_name ) 
~~ ~~ ~~ different_values = [ ] 
missing = [ ] 
type_mismatch = [ ] 
meta_mismatch = [ ] 
assert len ( self ) == len ( other ) 
if orderby : 
~~~ index1 = np . argsort ( self . columns [ orderby ] ) 
index2 = np . argsort ( other . columns [ orderby ] ) 
~~ for column_name in column_names : 
~~~ if column_name not in self . get_column_names ( virtual = False ) : 
~~~ missing . append ( column_name ) 
if report_missing : 
~~ ~~ elif column_name not in other . get_column_names ( virtual = False ) : 
~~~ ucd1 = self . ucds . get ( column_name ) 
ucd2 = other . ucds . get ( column_name ) 
if ucd1 != ucd2 : 
meta_mismatch . append ( column_name ) 
~~ unit1 = self . units . get ( column_name ) 
unit2 = other . units . get ( column_name ) 
if unit1 != unit2 : 
~~ type1 = self . dtype ( column_name ) 
if type1 != str_type : 
~~~ type1 = type1 . type 
~~ type2 = other . dtype ( column_name ) 
if type2 != str_type : 
~~~ type2 = type2 . type 
~~ if type1 != type2 : 
type_mismatch . append ( column_name ) 
~~~ a = self . evaluate ( column_name ) 
b = other . evaluate ( column_name ) 
~~~ a = a [ index1 ] 
b = b [ index2 ] 
~~ def normalize ( ar ) : 
~~~ if ar . dtype == str_type : 
~~~ return ar 
~~ if ar . dtype . kind == "f" and hasattr ( ar , "mask" ) : 
~~~ mask = ar . mask 
ar = ar . copy ( ) 
ar [ mask ] = np . nan 
~~ if ar . dtype . kind in "SU" : 
~~~ if hasattr ( ar , "mask" ) : 
~~~ data = ar . data 
~~~ data = ar 
~~ values = [ value . strip ( ) for value in data . tolist ( ) ] 
if hasattr ( ar , "mask" ) : 
~~~ ar = np . ma . masked_array ( values , ar . mask ) 
~~~ ar = np . array ( values ) 
~~ ~~ return ar 
~~ def equal_mask ( a , b ) : 
~~~ a = normalize ( a ) 
b = normalize ( b ) 
boolean_mask = ( a == b ) 
~~~ boolean_mask |= ( np . isnan ( a ) & np . isnan ( b ) ) 
~~ return boolean_mask 
~~ boolean_mask = equal_mask ( a , b ) 
all_equal = np . all ( boolean_mask ) 
if not all_equal : 
~~~ count = np . sum ( ~ boolean_mask ) 
different_values . append ( column_name ) 
if report_difference : 
~~~ indices = np . arange ( len ( self ) ) [ ~ boolean_mask ] 
values1 = self . columns [ column_name ] [ : ] [ ~ boolean_mask ] 
values2 = other . columns [ column_name ] [ : ] [ ~ boolean_mask ] 
for i in range ( min ( len ( values1 ) , show ) ) : 
~~~ diff = values1 [ i ] - values2 [ i ] 
~~ ~~ ~~ ~~ ~~ ~~ return different_values , missing , type_mismatch , meta_mismatch 
~~ def join ( self , other , on = None , left_on = None , right_on = None , lsuffix = '' , rsuffix = '' , how = 'left' , inplace = False ) : 
ds = self if inplace else self . copy ( ) 
if how == 'left' : 
~~~ left = ds 
right = other 
~~ elif how == 'right' : 
~~~ left = other 
right = ds 
lsuffix , rsuffix = rsuffix , lsuffix 
left_on , right_on = right_on , left_on 
~~ for name in right : 
~~~ if name in left and name + rsuffix == name + lsuffix : 
assert left . length_unfiltered ( ) == left . length_original ( ) 
N = left . length_unfiltered ( ) 
N_other = len ( right ) 
left_on = left_on or on 
right_on = right_on or on 
if left_on is None and right_on is None : 
~~~ for name in right : 
~~~ right_name = name 
if name in left : 
~~~ left . rename_column ( name , name + lsuffix ) 
right_name = name + rsuffix 
~~ if name in right . virtual_columns : 
~~~ left . add_virtual_column ( right_name , right . virtual_columns [ name ] ) 
~~~ left . add_column ( right_name , right . columns [ name ] ) 
~~~ left_values = left . evaluate ( left_on , filtered = False ) 
right_values = right . evaluate ( right_on ) 
if np . ma . isMaskedArray ( left_values ) : 
~~~ mask = ~ left_values . mask 
left_values = left_values . data 
index_left = dict ( zip ( left_values [ mask ] , np . arange ( N ) [ mask ] ) ) 
~~~ index_left = dict ( zip ( left_values , np . arange ( N ) ) ) 
~~ if np . ma . isMaskedArray ( right_values ) : 
~~~ mask = ~ right_values . mask 
right_values = right_values . data 
index_other = dict ( zip ( right_values [ mask ] , np . arange ( N_other ) [ mask ] ) ) 
~~~ index_other = dict ( zip ( right_values , np . arange ( N_other ) ) ) 
~~ left_mask = np . ones ( N , dtype = np . bool ) 
left_row_to_right = np . zeros ( N , dtype = np . int64 ) - 1 
for i in range ( N_other ) : 
~~~ left_row = index_left . get ( right_values [ i ] ) 
if left_row is not None : 
left_row_to_right [ left_row ] = i 
~~ ~~ lookup = np . ma . array ( left_row_to_right , mask = left_mask ) 
for name in right : 
~~~ left . add_column ( right_name , ColumnIndexed ( right , lookup , name ) ) 
~~ ~~ ~~ return left 
~~ def export ( self , path , column_names = None , byteorder = "=" , shuffle = False , selection = False , progress = None , virtual = False , sort = None , ascending = True ) : 
if path . endswith ( '.arrow' ) : 
~~~ self . export_arrow ( path , column_names , byteorder , shuffle , selection , progress = progress , virtual = virtual , sort = sort , ascending = ascending ) 
~~ elif path . endswith ( '.hdf5' ) : 
~~~ self . export_hdf5 ( path , column_names , byteorder , shuffle , selection , progress = progress , virtual = virtual , sort = sort , ascending = ascending ) 
~~ elif path . endswith ( '.fits' ) : 
~~~ self . export_fits ( path , column_names , shuffle , selection , progress = progress , virtual = virtual , sort = sort , ascending = ascending ) 
~~ if path . endswith ( '.parquet' ) : 
~~~ self . export_parquet ( path , column_names , shuffle , selection , progress = progress , virtual = virtual , sort = sort , ascending = ascending ) 
~~ ~~ def export_arrow ( self , path , column_names = None , byteorder = "=" , shuffle = False , selection = False , progress = None , virtual = False , sort = None , ascending = True ) : 
import vaex_arrow . export 
vaex_arrow . export . export ( self , path , column_names , byteorder , shuffle , selection , progress = progress , virtual = virtual , sort = sort , ascending = ascending ) 
~~ def export_hdf5 ( self , path , column_names = None , byteorder = "=" , shuffle = False , selection = False , progress = None , virtual = False , sort = None , ascending = True ) : 
import vaex . export 
vaex . export . export_hdf5 ( self , path , column_names , byteorder , shuffle , selection , progress = progress , virtual = virtual , sort = sort , ascending = ascending ) 
~~ def groupby ( self , by = None , agg = None ) : 
from . groupby import GroupBy 
groupby = GroupBy ( self , by = by ) 
if agg is None : 
~~~ return groupby 
~~~ return groupby . agg ( agg ) 
~~ ~~ def binby ( self , by = None , agg = None ) : 
from . groupby import BinBy 
binby = BinBy ( self , by = by ) 
~~~ return binby 
~~~ return binby . agg ( agg ) 
~~ ~~ def add_column ( self , name , data ) : 
super ( DataFrameArrays , self ) . add_column ( name , data ) 
~~ def then ( self , success = None , failure = None ) : 
ret = self . create_next ( ) 
def callAndFulfill ( v ) : 
~~~ if aplus . _isFunction ( success ) : 
~~~ ret . fulfill ( success ( v ) ) 
~~~ ret . fulfill ( v ) 
~~ ~~ except Exception as e : 
~~~ Promise . last_exc_info = sys . exc_info ( ) 
e . exc_info = sys . exc_info ( ) 
ret . reject ( e ) 
~~ ~~ def callAndReject ( r ) : 
~~~ if aplus . _isFunction ( failure ) : 
~~~ ret . fulfill ( failure ( r ) ) 
~~~ ret . reject ( r ) 
~~ ~~ self . done ( callAndFulfill , callAndReject ) 
return ret 
setattr ( DataFrame , name , f ) 
~~ def plot1d ( self , x = None , what = "count(*)" , grid = None , shape = 64 , facet = None , limits = None , figsize = None , f = "identity" , n = None , normalize_axis = None , 
xlabel = None , ylabel = None , label = None , 
selection = None , show = False , tight_layout = True , hardcopy = None , 
n = _parse_n ( n ) 
if type ( shape ) == int : 
~~~ shape = ( shape , ) 
~~ binby = [ ] 
for expression in [ x ] : 
~~ ~~ limits = self . limits ( binby , limits ) 
import re 
if facet is not None : 
~~~ match = re . match ( "(.*):(.*),(.*),(.*)" , facet ) 
~~~ groups = match . groups ( ) 
facet_expression = groups [ 0 ] 
facet_limits = [ ast . literal_eval ( groups [ 1 ] ) , ast . literal_eval ( groups [ 2 ] ) ] 
facet_count = ast . literal_eval ( groups [ 3 ] ) 
limits . append ( facet_limits ) 
binby . append ( facet_expression ) 
shape = ( facet_count , ) + shape 
~~ ~~ if grid is None : 
if n is not None : 
~~~ ngrid = fgrid / fgrid . sum ( ) 
~~~ ngrid = fgrid 
~~ xmin , xmax = limits [ - 1 ] 
if facet : 
~~~ N = len ( grid [ - 1 ] ) 
~~~ N = len ( grid ) 
~~ xexpression = binby [ 0 ] 
xar = np . arange ( N + 1 ) / ( N - 0. ) * ( xmax - xmin ) + xmin 
label = str ( label or selection or x ) 
~~~ import math 
rows , columns = int ( math . ceil ( facet_count / 4. ) ) , 4 
values = np . linspace ( facet_limits [ 0 ] , facet_limits [ 1 ] , facet_count + 1 ) 
for i in range ( facet_count ) : 
~~~ ax = pylab . subplot ( rows , columns , i + 1 ) 
value = ax . plot ( xar , ngrid [ i ] , drawstyle = "steps-mid" , label = label , ** kwargs ) 
v1 , v2 = values [ i ] , values [ i + 1 ] 
pylab . xlabel ( xlabel or x ) 
pylab . ylabel ( ylabel or what ) 
if self . iscategory ( xexpression ) : 
~~~ labels = self . category_labels ( xexpression ) 
step = len ( labels ) // max_labels 
pylab . xticks ( range ( len ( labels ) ) [ : : step ] , labels [ : : step ] , size = 'small' ) 
~~~ pylab . xlabel ( xlabel or self . label ( x ) ) 
g = np . concatenate ( [ ngrid [ 0 : 1 ] , ngrid ] ) 
value = pylab . plot ( xar , g , drawstyle = "steps-pre" , label = label , ** kwargs ) 
~~ ~~ if tight_layout : 
~~~ pylab . tight_layout ( ) 
~~ if hardcopy : 
~~~ pylab . savefig ( hardcopy ) 
~~~ pylab . show ( ) 
~~ def scatter ( self , x , y , xerr = None , yerr = None , cov = None , corr = None , s_expr = None , c_expr = None , labels = None , selection = None , length_limit = 50000 , 
length_check = True , label = None , xlabel = None , ylabel = None , errorbar_kwargs = { } , ellipse_kwargs = { } , ** kwargs ) : 
label = str ( label or selection ) 
if length_check : 
~~~ count = self . count ( selection = selection ) 
if count > length_limit : 
~~ ~~ x_values = self . evaluate ( x , selection = selection ) 
y_values = self . evaluate ( y , selection = selection ) 
if s_expr : 
~~~ kwargs [ "s" ] = self . evaluate ( s_expr , selection = selection ) 
~~ if c_expr : 
~~~ kwargs [ "c" ] = self . evaluate ( c_expr , selection = selection ) 
~~ plt . xlabel ( xlabel or self . label ( x ) ) 
plt . ylabel ( ylabel or self . label ( y ) ) 
s = plt . scatter ( x_values , y_values , label = label , ** kwargs ) 
if labels : 
~~~ label_values = self . evaluate ( labels , selection = selection ) 
for i , label_value in enumerate ( label_values ) : 
~~~ plt . annotate ( label_value , ( x_values [ i ] , y_values [ i ] ) ) 
~~ ~~ xerr_values = None 
yerr_values = None 
if cov is not None or corr is not None : 
~~~ from matplotlib . patches import Ellipse 
sx = self . evaluate ( xerr , selection = selection ) 
sy = self . evaluate ( yerr , selection = selection ) 
if corr is not None : 
~~~ sxy = self . evaluate ( corr , selection = selection ) * sx * sy 
~~ elif cov is not None : 
~~~ sxy = self . evaluate ( cov , selection = selection ) 
~~ cov_matrix = np . zeros ( ( len ( sx ) , 2 , 2 ) ) 
cov_matrix [ : , 0 , 0 ] = sx ** 2 
cov_matrix [ : , 1 , 1 ] = sy ** 2 
cov_matrix [ : , 0 , 1 ] = cov_matrix [ : , 1 , 0 ] = sxy 
ax = plt . gca ( ) 
ellipse_kwargs = dict ( ellipse_kwargs ) 
ellipse_kwargs [ 'facecolor' ] = ellipse_kwargs . get ( 'facecolor' , 'none' ) 
ellipse_kwargs [ 'edgecolor' ] = ellipse_kwargs . get ( 'edgecolor' , 'black' ) 
for i in range ( len ( sx ) ) : 
~~~ eigen_values , eigen_vectors = np . linalg . eig ( cov_matrix [ i ] ) 
indices = np . argsort ( eigen_values ) [ : : - 1 ] 
eigen_values = eigen_values [ indices ] 
eigen_vectors = eigen_vectors [ : , indices ] 
v1 = eigen_vectors [ : , 0 ] 
v2 = eigen_vectors [ : , 1 ] 
varx = cov_matrix [ i , 0 , 0 ] 
vary = cov_matrix [ i , 1 , 1 ] 
angle = np . arctan2 ( v1 [ 1 ] , v1 [ 0 ] ) 
if eigen_values [ 1 ] < 0 and abs ( ( eigen_values [ 1 ] / eigen_values [ 0 ] ) ) < 1e-10 : 
~~~ eigen_values [ 1 ] = 0 
~~ if eigen_values [ 0 ] < 0 or eigen_values [ 1 ] < 0 : 
~~ width , height = np . sqrt ( np . max ( eigen_values ) ) , np . sqrt ( np . min ( eigen_values ) ) 
e = Ellipse ( xy = ( x_values [ i ] , y_values [ i ] ) , width = width , height = height , angle = np . degrees ( angle ) , ** ellipse_kwargs ) 
ax . add_artist ( e ) 
~~~ if xerr is not None : 
~~~ if _issequence ( xerr ) : 
xerr_values = [ self . evaluate ( xerr [ 0 ] , selection = selection ) , self . evaluate ( xerr [ 1 ] , selection = selection ) ] 
~~~ xerr_values = self . evaluate ( xerr , selection = selection ) 
~~ ~~ if yerr is not None : 
~~~ if _issequence ( yerr ) : 
yerr_values = [ self . evaluate ( yerr [ 0 ] , selection = selection ) , self . evaluate ( yerr [ 1 ] , selection = selection ) ] 
~~~ yerr_values = self . evaluate ( yerr , selection = selection ) 
~~ ~~ if xerr_values is not None or yerr_values is not None : 
~~~ errorbar_kwargs = dict ( errorbar_kwargs ) 
errorbar_kwargs [ 'fmt' ] = errorbar_kwargs . get ( 'fmt' , 'none' ) 
plt . errorbar ( x_values , y_values , yerr = yerr_values , xerr = xerr_values , ** errorbar_kwargs ) 
~~ ~~ return s 
~~ def plot ( self , x = None , y = None , z = None , what = "count(*)" , vwhat = None , reduce = [ "colormap" ] , f = None , 
normalize = "normalize" , normalize_axis = "what" , 
figsize = None , xlabel = None , ylabel = None , aspect = "auto" , tight_layout = True , interpolation = "nearest" , show = False , 
colorbar = True , 
colorbar_label = None , 
selection = None , selection_labels = None , title = None , 
background_color = "white" , pre_blend = False , background_alpha = 1. , 
visual = dict ( x = "x" , y = "y" , layer = "z" , fade = "selection" , row = "subspace" , column = "what" ) , 
smooth_pre = None , smooth_post = None , 
wrap = True , wrap_columns = 4 , 
return_extra = False , hardcopy = None ) : 
n = _parse_n ( normalize ) 
~~~ shape = ( shape , ) * 2 
~~ ~~ fig = pylab . gcf ( ) 
~~ import re 
what_units = None 
whats = _ensure_list ( what ) 
selections = _ensure_list ( selection ) 
selections = _ensure_strings_from_expressions ( selections ) 
limits = [ limits ] 
limits , shape = self . limits ( x , limits , shape = shape ) 
shape = shape [ 0 ] 
labels = { } 
shape = _expand_shape ( shape , 2 ) 
vshape = _expand_shape ( shape , 2 ) 
if z is not None : 
~~~ match = re . match ( "(.*):(.*),(.*),(.*)" , z ) 
z_expression = groups [ 0 ] 
z_limits = [ ast . literal_eval ( groups [ 1 ] ) , ast . literal_eval ( groups [ 2 ] ) ] 
z_shape = ast . literal_eval ( groups [ 3 ] ) 
x = [ [ z_expression ] + list ( k ) for k in x ] 
limits = np . array ( [ [ z_limits ] + list ( k ) for k in limits ] ) 
shape = ( z_shape , ) + shape 
vshape = ( z_shape , ) + vshape 
values = np . linspace ( z_limits [ 0 ] , z_limits [ 1 ] , num = z_shape + 1 ) 
~~~ z_shape = 1 
~~ if z is None : 
~~~ total_grid = np . zeros ( ( len ( x ) , len ( whats ) , len ( selections ) , 1 ) + shape , dtype = float ) 
total_vgrid = np . zeros ( ( len ( x ) , len ( whats ) , len ( selections ) , 1 ) + vshape , dtype = float ) 
~~~ total_grid = np . zeros ( ( len ( x ) , len ( whats ) , len ( selections ) ) + shape , dtype = float ) 
total_vgrid = np . zeros ( ( len ( x ) , len ( whats ) , len ( selections ) ) + vshape , dtype = float ) 
axis = dict ( plot = 0 , what = 1 , selection = 2 ) 
xlimits = limits 
grid_axes = dict ( x = - 1 , y = - 2 , z = - 3 , selection = - 4 , what = - 5 , subspace = - 6 ) 
visual_axes = dict ( x = - 1 , y = - 2 , layer = - 3 , fade = - 4 , column = - 5 , row = - 6 ) 
visual_default = dict ( x = "x" , y = "y" , layer = "z" , fade = "selection" , row = "subspace" , column = "what" ) 
def invert ( x ) : return dict ( ( v , k ) for k , v in x . items ( ) ) 
free_visual_axes = list ( visual_default . keys ( ) ) 
for visual_name , grid_name in visual . items ( ) : 
~~~ if visual_name in free_visual_axes : 
~~~ free_visual_axes . remove ( visual_name ) 
for visual_name , grid_name in visual_default . items ( ) : 
~~~ if visual_name in free_visual_axes and grid_name not in visual . values ( ) : 
visual [ visual_name ] = grid_name 
~~~ if visual_name not in free_visual_axes and grid_name not in visual . values ( ) : 
~~~ visual [ free_visual_axes . pop ( 0 ) ] = grid_name 
visual_reverse = invert ( visual ) 
visual , visual_reverse = visual_reverse , visual 
move = { } 
for grid_name , visual_name in visual . items ( ) : 
~~~ if visual_axes [ visual_name ] in visual . values ( ) : 
~~~ index = visual . values ( ) . find ( visual_name ) 
key = visual . keys ( ) [ index ] 
~~ move [ grid_axes [ grid_name ] ] = visual_axes [ visual_name ] 
~~ fs = _expand ( f , total_grid . shape [ grid_axes [ normalize_axis ] ] ) 
what_labels = [ ] 
~~~ grid_of_grids = [ ] 
for i , ( binby , limits ) in enumerate ( zip ( x , xlimits ) ) : 
~~~ grid_of_grids . append ( [ ] ) 
for j , what in enumerate ( whats ) : 
~~~ if isinstance ( what , vaex . stat . Expression ) : 
~~~ grid = what . calculate ( self , binby = binby , shape = shape , limits = limits , selection = selections , delay = True ) 
if "," in arguments : 
~~~ arguments = arguments . split ( "," ) 
~~ functions = [ "mean" , "sum" , "std" , "var" , "correlation" , "covar" , "min" , "max" , "median_approx" ] 
unit_expression = None 
if function in [ "mean" , "sum" , "std" , "min" , "max" , "median" ] : 
~~~ unit_expression = arguments 
~~ if function in [ "var" ] : 
~~ if function in [ "covar" ] : 
~~ if unit_expression : 
~~~ unit = self . unit ( unit_expression ) 
~~~ what_units = unit . to_string ( 'latex_inline' ) 
~~ ~~ if function in functions : 
~~~ grid = getattr ( self , function ) ( arguments , binby = binby , limits = limits , shape = shape , selection = selections , delay = True ) 
~~ elif function == "count" : 
~~~ grid = self . count ( arguments , binby , shape = shape , limits = limits , selection = selections , delay = True ) 
~~~ what_label = str ( whats [ j ] ) 
if what_units : 
~~ if fs [ j ] : 
~~ what_labels . append ( what_label ) 
~~ grid_of_grids [ - 1 ] . append ( grid ) 
~~ ~~ self . executor . execute ( ) 
~~~ for j , what in enumerate ( whats ) : 
~~~ grid = grid_of_grids [ i ] [ j ] . get ( ) 
total_grid [ i , j , : , : ] = grid [ : , None , ... ] 
~~ ~~ labels [ "what" ] = what_labels 
~~~ dims_left = 6 - len ( grid . shape ) 
total_grid = np . broadcast_to ( grid , ( 1 , ) * dims_left + grid . shape ) 
~~ def _selection_name ( name ) : 
~~~ if name in [ None , False ] : 
~~ elif name in [ "default" , True ] : 
~~ ~~ if selection_labels is None : 
~~~ labels [ "selection" ] = list ( [ _selection_name ( k ) for k in selections ] ) 
~~~ labels [ "selection" ] = selection_labels 
~~ axes = [ None ] * len ( move ) 
for key , value in move . items ( ) : 
~~~ axes [ value ] = key 
~~ visual_grid = np . transpose ( total_grid , axes ) 
xexpressions = [ ] 
yexpressions = [ ] 
~~~ xexpressions . append ( binby [ 0 ] ) 
yexpressions . append ( binby [ 1 ] ) 
~~ if xlabel is None : 
~~~ xlabels = [ ] 
ylabels = [ ] 
~~~ if z is not None : 
~~~ xlabels . append ( self . label ( binby [ 1 ] ) ) 
ylabels . append ( self . label ( binby [ 2 ] ) ) 
~~~ xlabels . append ( self . label ( binby [ 0 ] ) ) 
ylabels . append ( self . label ( binby [ 1 ] ) ) 
~~~ Nl = visual_grid . shape [ visual_axes [ 'row' ] ] 
xlabels = _expand ( xlabel , Nl ) 
ylabels = _expand ( ylabel , Nl ) 
~~ labels [ "x" ] = xlabels 
labels [ "y" ] = ylabels 
axes = [ ] 
background_color = np . array ( matplotlib . colors . colorConverter . to_rgb ( background_color ) ) 
import math 
facet_columns = None 
facets = visual_grid . shape [ visual_axes [ "row" ] ] * visual_grid . shape [ visual_axes [ "column" ] ] 
if visual_grid . shape [ visual_axes [ "column" ] ] == 1 and wrap : 
~~~ facet_columns = min ( wrap_columns , visual_grid . shape [ visual_axes [ "row" ] ] ) 
wrapped = True 
~~ elif visual_grid . shape [ visual_axes [ "row" ] ] == 1 and wrap : 
~~~ facet_columns = min ( wrap_columns , visual_grid . shape [ visual_axes [ "column" ] ] ) 
~~~ wrapped = False 
facet_columns = visual_grid . shape [ visual_axes [ "column" ] ] 
~~ facet_rows = int ( math . ceil ( facets / facet_columns ) ) 
grid = visual_grid * 1. 
fgrid = visual_grid * 1. 
ngrid = visual_grid * 1. 
vmins = _expand ( vmin , visual_grid . shape [ visual_axes [ visual [ normalize_axis ] ] ] , type = list ) 
vmaxs = _expand ( vmax , visual_grid . shape [ visual_axes [ visual [ normalize_axis ] ] ] , type = list ) 
visual_grid 
if smooth_pre : 
~~~ grid = vaex . grids . gf ( grid , smooth_pre ) 
~~ if 1 : 
~~~ axis = visual_axes [ visual [ normalize_axis ] ] 
for i in range ( visual_grid . shape [ axis ] ) : 
~~~ item = [ slice ( None , None , None ) , ] * len ( visual_grid . shape ) 
item [ axis ] = i 
item = tuple ( item ) 
f = _parse_f ( fs [ i ] ) 
~~~ fgrid . __setitem__ ( item , f ( grid . __getitem__ ( item ) ) ) 
~~ if vmins [ i ] is not None and vmaxs [ i ] is not None : 
~~~ nsubgrid = fgrid . __getitem__ ( item ) * 1 
nsubgrid -= vmins [ i ] 
nsubgrid /= ( vmaxs [ i ] - vmins [ i ] ) 
~~~ nsubgrid , vmin , vmax = n ( fgrid . __getitem__ ( item ) ) 
vmins [ i ] = vmin 
vmaxs [ i ] = vmax 
~~ ngrid . __setitem__ ( item , nsubgrid ) 
~~~ grid = visual_grid [ i ] 
fgrid = f ( grid ) 
finite_mask = np . isfinite ( grid ) 
finite_mask = np . any ( finite_mask , axis = 0 ) 
if vmin is not None and vmax is not None : 
~~~ ngrid = fgrid * 1 
ngrid -= vmin 
ngrid /= ( vmax - vmin ) 
ngrid = np . clip ( ngrid , 0 , 1 ) 
~~~ ngrid , vmin , vmax = n ( fgrid ) 
~~ ~~ rows , columns = int ( math . ceil ( facets / float ( facet_columns ) ) ) , facet_columns 
colorbar_location = "individual" 
if visual [ "what" ] == "row" and visual_grid . shape [ 1 ] == facet_columns : 
~~~ colorbar_location = "per_row" 
~~ if visual [ "what" ] == "column" and visual_grid . shape [ 0 ] == facet_rows : 
~~~ colorbar_location = "per_column" 
import matplotlib . gridspec as gridspec 
column_scale = 1 
row_scale = 1 
row_offset = 0 
if facets > 1 : 
~~~ if colorbar_location == "per_row" : 
~~~ column_scale = 4 
gs = gridspec . GridSpec ( rows , columns * column_scale + 1 ) 
~~ elif colorbar_location == "per_column" : 
~~~ row_offset = 1 
row_scale = 4 
gs = gridspec . GridSpec ( rows * row_scale + 1 , columns ) 
~~~ gs = gridspec . GridSpec ( rows , columns ) 
~~ ~~ facet_index = 0 
fs = _expand ( f , len ( whats ) ) 
colormaps = _expand ( colormap , len ( whats ) ) 
for i in range ( visual_grid . shape [ 0 ] ) : 
~~~ for j in range ( visual_grid . shape [ 1 ] ) : 
~~~ if colorbar and colorbar_location == "per_column" and i == 0 : 
~~~ norm = matplotlib . colors . Normalize ( vmins [ j ] , vmaxs [ j ] ) 
sm = matplotlib . cm . ScalarMappable ( norm , colormaps [ j ] ) 
~~~ ax = pylab . subplot ( gs [ 0 , j ] ) 
colorbar = fig . colorbar ( sm , cax = ax , orientation = "horizontal" ) 
~~~ colorbar = fig . colorbar ( sm ) 
~~ if "what" in labels : 
~~~ label = labels [ "what" ] [ j ] 
~~~ colorbar . ax . set_title ( label ) 
~~~ colorbar . ax . set_ylabel ( colorbar_label or label ) 
~~ ~~ ~~ if colorbar and colorbar_location == "per_row" and j == 0 : 
~~~ norm = matplotlib . colors . Normalize ( vmins [ i ] , vmaxs [ i ] ) 
sm = matplotlib . cm . ScalarMappable ( norm , colormaps [ i ] ) 
~~~ ax = pylab . subplot ( gs [ i , - 1 ] ) 
colorbar = fig . colorbar ( sm , cax = ax ) 
~~ label = labels [ "what" ] [ i ] 
colorbar . ax . set_ylabel ( colorbar_label or label ) 
~~ rgrid = ngrid [ i , j ] * 1. 
for k in range ( rgrid . shape [ 0 ] ) : 
~~~ for l in range ( rgrid . shape [ 0 ] ) : 
~~~ if smooth_post is not None : 
~~~ rgrid [ k , l ] = vaex . grids . gf ( rgrid , smooth_post ) 
~~ ~~ ~~ if visual [ "what" ] == "column" : 
~~~ what_index = j 
~~ elif visual [ "what" ] == "row" : 
~~~ what_index = i 
~~~ what_index = 0 
~~ if visual [ normalize_axis ] == "column" : 
~~~ normalize_index = j 
~~ elif visual [ normalize_axis ] == "row" : 
~~~ normalize_index = i 
~~~ normalize_index = 0 
~~ for r in reduce : 
~~~ r = _parse_reduction ( r , colormaps [ what_index ] , [ ] ) 
rgrid = r ( rgrid ) 
~~ row = facet_index // facet_columns 
column = facet_index % facet_columns 
if colorbar and colorbar_location == "individual" : 
~~~ norm = matplotlib . colors . Normalize ( vmins [ normalize_index ] , vmaxs [ normalize_index ] ) 
sm = matplotlib . cm . ScalarMappable ( norm , colormaps [ what_index ] ) 
~~~ ax = pylab . subplot ( gs [ row , column ] ) 
colorbar = fig . colorbar ( sm , ax = ax ) 
~~ label = labels [ "what" ] [ what_index ] 
~~ if facets > 1 : 
~~~ ax = pylab . subplot ( gs [ row_offset + row * row_scale : row_offset + ( row + 1 ) * row_scale , column * column_scale : ( column + 1 ) * column_scale ] ) 
~~~ ax = pylab . gca ( ) 
~~ axes . append ( ax ) 
plot_rgrid = rgrid 
plot_rgrid = plot_rgrid [ : , 0 ] 
if plot_rgrid . shape [ 0 ] > 1 : 
~~~ plot_rgrid = vaex . image . fade ( plot_rgrid [ : : - 1 ] ) 
~~~ plot_rgrid = plot_rgrid [ 0 ] 
~~ extend = None 
if visual [ "subspace" ] == "row" : 
~~~ subplot_index = i 
~~ elif visual [ "subspace" ] == "column" : 
~~~ subplot_index = j 
~~~ subplot_index = 0 
~~ extend = np . array ( xlimits [ subplot_index ] [ - 2 : ] ) . flatten ( ) 
plot_rgrid = np . transpose ( plot_rgrid , ( 1 , 0 , 2 ) ) 
im = ax . imshow ( plot_rgrid , extent = extend . tolist ( ) , origin = "lower" , aspect = aspect , interpolation = interpolation ) 
def label ( index , label , expression ) : 
~~~ if label and _issequence ( label ) : 
~~~ return label [ i ] 
~~~ return self . label ( expression ) 
~~ ~~ if visual_reverse [ "x" ] == 'x' : 
~~~ labelsx = labels [ 'x' ] 
pylab . xlabel ( labelsx [ subplot_index ] ) 
~~ if visual_reverse [ "x" ] == 'x' : 
~~~ labelsy = labels [ 'y' ] 
pylab . ylabel ( labelsy [ subplot_index ] ) 
~~ if visual [ "z" ] in [ 'row' ] : 
~~~ labelsz = labels [ 'z' ] 
ax . set_title ( labelsz [ i ] ) 
~~ if visual [ "z" ] in [ 'column' ] : 
ax . set_title ( labelsz [ j ] ) 
~~ max_labels = 10 
facet_index += 1 
~~ ~~ if title : 
~~~ fig . suptitle ( title , fontsize = "x-large" ) 
~~ if tight_layout : 
~~~ if title : 
~~~ pylab . tight_layout ( rect = [ 0 , 0.03 , 1 , 0.95 ] ) 
~~ ~~ if hardcopy : 
~~ if return_extra : 
~~~ return im , grid , fgrid , ngrid , rgrid 
~~~ return im 
~~ ~~ def register_function ( scope = None , as_property = False , name = None ) : 
prefix = '' 
if scope : 
~~~ prefix = scope + "_" 
if scope not in scopes : 
~~ ~~ def wrapper ( f , name = name ) : 
~~~ name = name or f . __name__ 
if name . startswith ( prefix ) : 
~~~ name = name [ len ( prefix ) : ] 
~~ full_name = prefix + name 
~~~ def closure ( name = name , full_name = full_name , function = f ) : 
~~~ def wrapper ( self , * args , ** kwargs ) : 
~~~ lazy_func = getattr ( self . expression . ds . func , full_name ) 
args = ( self . expression , ) + args 
return lazy_func ( * args , ** kwargs ) 
~~ return functools . wraps ( function ) ( wrapper ) 
~~ if as_property : 
~~~ setattr ( scopes [ scope ] , name , property ( closure ( ) ) ) 
~~~ setattr ( scopes [ scope ] , name , closure ( ) ) 
~~~ lazy_func = getattr ( self . ds . func , full_name ) 
args = ( self , ) + args 
~~ setattr ( vaex . expression . Expression , name , closure ( ) ) 
~~ vaex . expression . expression_namespace [ prefix + name ] = f 
~~ def fillna ( ar , value , fill_nan = True , fill_masked = True ) : 
ar = ar if not isinstance ( ar , column . Column ) else ar . to_numpy ( ) 
if ar . dtype . kind in 'O' and fill_nan : 
~~~ strings = ar . astype ( str ) 
mask = strings == 'nan' 
ar [ mask ] = value 
~~ elif ar . dtype . kind in 'f' and fill_nan : 
~~~ mask = np . isnan ( ar ) 
if np . any ( mask ) : 
~~~ ar = ar . copy ( ) 
~~ ~~ if fill_masked and np . ma . isMaskedArray ( ar ) : 
~~~ ar = ar . data . copy ( ) 
~~ def dt_dayofweek ( x ) : 
return pd . Series ( x ) . dt . dayofweek . values 
~~ def dt_dayofyear ( x ) : 
return pd . Series ( x ) . dt . dayofyear . values 
~~ def dt_is_leap_year ( x ) : 
return pd . Series ( x ) . dt . is_leap_year . values 
~~ def dt_year ( x ) : 
return pd . Series ( x ) . dt . year . values 
~~ def dt_month ( x ) : 
return pd . Series ( x ) . dt . month . values 
~~ def dt_month_name ( x ) : 
return pd . Series ( _pandas_dt_fix ( x ) ) . dt . month_name ( ) . values . astype ( str ) 
~~ def dt_day ( x ) : 
return pd . Series ( x ) . dt . day . values 
~~ def dt_day_name ( x ) : 
return pd . Series ( _pandas_dt_fix ( x ) ) . dt . day_name ( ) . values . astype ( str ) 
~~ def dt_weekofyear ( x ) : 
return pd . Series ( x ) . dt . weekofyear . values 
~~ def dt_hour ( x ) : 
return pd . Series ( x ) . dt . hour . values 
~~ def dt_minute ( x ) : 
return pd . Series ( x ) . dt . minute . values 
~~ def dt_second ( x ) : 
return pd . Series ( x ) . dt . second . values 
~~ def str_capitalize ( x ) : 
sl = _to_string_sequence ( x ) . capitalize ( ) 
return column . ColumnStringArrow ( sl . bytes , sl . indices , sl . length , sl . offset , string_sequence = sl ) 
~~ def str_cat ( x , other ) : 
sl1 = _to_string_sequence ( x ) 
sl2 = _to_string_sequence ( other ) 
sl = sl1 . concat ( sl2 ) 
return column . ColumnStringArrow . from_string_sequence ( sl ) 
~~ def str_contains ( x , pattern , regex = True ) : 
return _to_string_sequence ( x ) . search ( pattern , regex ) 
~~ def str_count ( x , pat , regex = False ) : 
return _to_string_sequence ( x ) . count ( pat , regex ) 
~~ def str_find ( x , sub , start = 0 , end = None ) : 
return _to_string_sequence ( x ) . find ( sub , start , 0 if end is None else end , end is None , True ) 
~~ def str_get ( x , i ) : 
x = _to_string_sequence ( x ) 
if i == - 1 : 
~~~ sl = x . slice_string_end ( - 1 ) 
~~~ sl = x . slice_string ( i , i + 1 ) 
~~ return column . ColumnStringArrow ( sl . bytes , sl . indices , sl . length , sl . offset , string_sequence = sl ) 
~~ def str_index ( x , sub , start = 0 , end = None ) : 
return str_find ( x , sub , start , end ) 
~~ def str_join ( x , sep ) : 
sl = _to_string_list_sequence ( x ) . join ( sep ) 
~~ def str_lower ( x ) : 
sl = _to_string_sequence ( x ) . lower ( ) 
~~ def str_lstrip ( x , to_strip = None ) : 
sl = _to_string_sequence ( x ) . lstrip ( '' if to_strip is None else to_strip ) if to_strip != '' else x 
sl = _to_string_sequence ( x ) . pad ( width , fillchar , side in [ 'left' , 'both' ] , side in [ 'right' , 'both' ] ) 
~~ def str_repeat ( x , repeats ) : 
sl = _to_string_sequence ( x ) . repeat ( repeats ) 
~~ def str_replace ( x , pat , repl , n = - 1 , flags = 0 , regex = False ) : 
sl = _to_string_sequence ( x ) . replace ( pat , repl , n , flags , regex ) 
~~ def str_rfind ( x , sub , start = 0 , end = None ) : 
return _to_string_sequence ( x ) . find ( sub , start , 0 if end is None else end , end is None , False ) 
~~ def str_rindex ( x , sub , start = 0 , end = None ) : 
return str_rfind ( x , sub , start , end ) 
sl = _to_string_sequence ( x ) . pad ( width , fillchar , True , False ) 
~~ def str_rstrip ( x , to_strip = None ) : 
sl = _to_string_sequence ( x ) . rstrip ( '' if to_strip is None else to_strip ) if to_strip != '' else x 
if stop is None : 
~~~ sll = _to_string_sequence ( x ) . slice_string_end ( start ) 
~~~ sll = _to_string_sequence ( x ) . slice_string ( start , stop ) 
~~ return sll 
~~ def str_strip ( x , to_strip = None ) : 
sl = _to_string_sequence ( x ) . strip ( '' if to_strip is None else to_strip ) if to_strip != '' else x 
~~ def str_title ( x ) : 
sl = _to_string_sequence ( x ) . title ( ) 
~~ def str_upper ( x ) : 
sl = _to_string_sequence ( x ) . upper ( ) 
~~ def format ( x , format ) : 
sl = vaex . strings . format ( x , format ) 
~~ def write_meta ( self ) : 
with h5py . File ( self . filename , "r+" ) as h5file_output : 
~~~ h5table_root = h5file_output [ self . h5table_root_name ] 
if self . description is not None : 
~~~ h5table_root . attrs [ "description" ] = self . description 
~~ h5columns = h5table_root if self . _version == 1 else h5table_root [ 'columns' ] 
for column_name in self . columns . keys ( ) : 
~~~ h5dataset = None 
if column_name in h5columns : 
~~~ h5dataset = h5columns [ column_name ] 
~~~ for group in h5columns . values ( ) : 
~~~ if 'type' in group . attrs : 
~~~ if group . attrs [ 'type' ] in [ 'csr_matrix' ] : 
~~~ for name , column in group . items ( ) : 
~~~ if name == column_name : 
~~~ h5dataset = column 
~~ ~~ ~~ ~~ ~~ ~~ if h5dataset is None : 
~~ for name , values in [ ( "ucd" , self . ucds ) , ( "unit" , self . units ) , ( "description" , self . descriptions ) ] : 
~~~ if column_name in values : 
~~~ value = ensure_string ( values [ column_name ] , cast = True ) 
h5dataset . attrs [ name ] = value 
~~~ if name in h5columns . attrs : 
~~~ del h5dataset . attrs [ name ] 
~~ ~~ ~~ ~~ ~~ ~~ def create ( cls , path , N , column_names , dtypes = None , write = True ) : 
dtypes = dtypes or [ np . float ] * len ( column_names ) 
~~ with h5py . File ( path , "w" ) as h5file_output : 
for column_name , dtype in zip ( column_names , dtypes ) : 
print ( dtype ) 
if dtype . type == np . datetime64 : 
~~~ array = h5file_output . require_dataset ( "/data/%s" % column_name , shape = shape , dtype = dtype ) 
~~ ~~ return Hdf5MemoryMapped ( path , write = write ) 
~~ def readcol ( filename , skipline = 0 , skipafter = 0 , names = False , fsep = None , twod = True , 
fixedformat = None , asdict = False , comment = '#' , verbose = True , nullval = None , 
asStruct = False , namecomment = True , removeblanks = False , header_badchars = None , 
asRecArray = False ) : 
with open ( filename , 'r' ) as f : 
~~~ f = f . readlines ( ) 
null = [ f . pop ( 0 ) for i in range ( skipline ) ] 
commentfilter = make_commentfilter ( comment ) 
if not asStruct : 
~~~ asStruct = asRecArray 
~~ if namecomment is False and ( names or asdict or asStruct ) : 
~~~ while 1 : 
~~~ line = f . pop ( 0 ) 
if line [ 0 ] != comment : 
~~~ nameline = line 
if header_badchars : 
~~~ for c in header_badchars : 
~~ ~~ nms = nameline . split ( fsep ) 
~~ elif len ( f ) == 0 : 
~~~ if names or asdict or asStruct : 
~~~ if type ( names ) == type ( 1 ) : 
~~~ nameline = f . pop ( names ) 
~~~ nameline = f . pop ( 0 ) 
~~ if nameline [ 0 ] == comment : 
~~~ nameline = nameline [ 1 : ] 
~~ if header_badchars : 
~~ ~~ nms = list ( [ name . strip ( ) for name in nameline . split ( fsep ) ] ) 
~~ ~~ null = [ f . pop ( 0 ) for i in range ( skipafter ) ] 
if fixedformat : 
~~~ myreadff = lambda x : readff ( x , fixedformat ) 
splitarr = list ( map ( myreadff , f ) ) 
splitarr = list ( filter ( commentfilter , splitarr ) ) 
~~~ fstrip = list ( map ( str . strip , f ) ) 
fseps = [ fsep for i in range ( len ( f ) ) ] 
splitarr = list ( map ( str . split , fstrip , fseps ) ) 
if removeblanks : 
~~~ for i in range ( splitarr . count ( [ '' ] ) ) : 
~~~ splitarr . remove ( [ '' ] ) 
~~ ~~ splitarr = list ( filter ( commentfilter , splitarr ) ) 
nperline = list ( map ( len , splitarr ) ) 
if hasmode : 
~~~ ncols , nrows = mode ( nperline ) 
if nrows != len ( splitarr ) : 
~~~ if verbose : 
~~~ if nperline [ i ] != ncols : 
~~~ splitarr . pop ( i ) 
~~ ~~ ~~ ~~ ~~ try : 
~~~ x = numpy . asarray ( splitarr , dtype = 'float' ) 
~~~ x = numpy . asarray ( splitarr , dtype = 'S' ) 
~~~ if hasmode : 
~~ ~~ ~~ if nullval is not None : 
~~~ x [ x == nullval ] = numpy . nan 
x = get_autotype ( x ) 
~~ if asdict or asStruct : 
~~~ mydict = OrderedDict ( zip ( nms , x . T ) ) 
for k , v in mydict . items ( ) : 
~~~ mydict [ k ] = get_autotype ( v ) 
~~ if asdict : 
~~~ return mydict 
~~ elif asRecArray : 
~~~ return Struct ( mydict ) . as_recarray ( ) 
~~ elif asStruct : 
~~~ return Struct ( mydict ) 
~~ ~~ elif names and twod : 
~~~ return nms , x 
~~ elif names : 
~~~ return nms , [ get_autotype ( x . T [ i ] ) for i in range ( x . shape [ 1 ] ) ] 
~~~ if twod : 
~~~ return x 
~~~ return [ get_autotype ( x . T [ i ] ) for i in range ( x . shape [ 1 ] ) ] 
~~ ~~ ~~ ~~ def get_autotype ( arr ) : 
~~~ narr = arr . astype ( 'float' ) 
if ( narr < sys . maxsize ) . all ( ) and ( narr % 1 ) . sum ( ) == 0 : 
~~~ return narr . astype ( 'int' ) 
~~~ return narr 
~~ ~~ except ValueError : 
~~~ return arr 
~~ ~~ def readff ( s , format ) : 
F = numpy . array ( [ 0 ] + format ) . cumsum ( ) 
bothF = zip ( F [ : - 1 ] , F [ 1 : ] ) 
strarr = [ s [ l : u ] for l , u in bothF ] 
return strarr 
~~ def as_recarray ( self ) : 
dtype = [ ( k , v . dtype ) for k , v in self . __dict__ . iteritems ( ) ] 
R = numpy . recarray ( len ( self . __dict__ [ k ] ) , dtype = dtype ) 
for key in self . __dict__ : 
~~~ R [ key ] = self . __dict__ [ key ] 
~~ return R 
~~ def store_properties ( fh , props , comment = None , timestamp = True ) : 
if comment is not None : 
~~~ write_comment ( fh , comment ) 
~~ if timestamp : 
~~ if hasattr ( props , 'keys' ) : 
~~~ for key in props : 
~~~ write_property ( fh , key , props [ key ] ) 
~~~ for key , value in props : 
~~~ write_property ( fh , key , value ) 
~~ ~~ ~~ def write_comment ( fh , comment ) : 
_require_string ( comment , 'comments' ) 
fh . write ( _escape_comment ( comment ) ) 
fh . write ( b'\\n' ) 
~~ def write_property ( fh , key , value ) : 
if key is COMMENT : 
~~~ write_comment ( fh , value ) 
~~ _require_string ( key , 'keys' ) 
_require_string ( value , 'values' ) 
fh . write ( _escape_key ( key ) ) 
fh . write ( b'=' ) 
fh . write ( _escape_value ( value ) ) 
~~ def iter_properties ( fh , comments = False ) : 
for line in _property_lines ( fh ) : 
~~~ key , value = _split_key_value ( line ) 
if key is not COMMENT : 
~~~ key = _unescape ( key ) 
~~ elif not comments : 
~~ yield key , _unescape ( value ) 
~~ ~~ def _universal_newlines ( fp ) : 
if 'U' in getattr ( fp , 'mode' , '' ) : 
~~~ for line in fp : 
~~~ yield line 
~~~ line = line . replace ( b'\\r\\n' , b'\\n' ) . replace ( b'\\r' , b'\\n' ) 
for piece in line . split ( b'\\n' ) : 
~~~ yield piece 
~~ ~~ ~~ ~~ def show_versions ( ) : 
core_deps = [ 'audioread' , 
'numpy' , 
'scipy' , 
'sklearn' , 
'joblib' , 
'decorator' , 
'six' , 
'soundfile' , 
'resampy' , 
'numba' ] 
extra_deps = [ 'numpydoc' , 
'sphinx' , 
'sphinx_rtd_theme' , 
'sphinxcontrib.versioning' , 
'sphinx-gallery' , 
'pytest' , 
'pytest-mpl' , 
'pytest-cov' , 
'matplotlib' ] 
print ( '------------------' ) 
for dep in core_deps : 
~~ print ( '' ) 
for dep in extra_deps : 
~~ pass 
~~ def rename_kw ( old_name , old_value , new_name , new_value , 
version_deprecated , version_removed ) : 
if isinstance ( old_value , Deprecated ) : 
~~~ return new_value 
~~~ stack = inspect . stack ( ) 
dep_func = stack [ 1 ] 
caller = stack [ 2 ] 
"{:}." . format ( dep_func [ 3 ] , 
old_name , new_name , 
version_deprecated , 
version_removed ) , 
category = DeprecationWarning , 
filename = caller [ 1 ] , 
lineno = caller [ 2 ] ) 
return old_value 
~~ ~~ def set_fftlib ( lib = None ) : 
global __FFTLIB 
if lib is None : 
~~~ from numpy import fft 
lib = fft 
~~ __FFTLIB = lib 
~~ def beat_track ( input_file , output_csv ) : 
y , sr = librosa . load ( input_file , sr = 22050 ) 
hop_length = 512 
tempo , beats = librosa . beat . beat_track ( y = y , sr = sr , hop_length = hop_length ) 
beat_times = librosa . frames_to_time ( beats , sr = sr , hop_length = hop_length ) 
librosa . output . times_csv ( output_csv , beat_times ) 
print ( 'done!' ) 
~~ def adjust_tuning ( input_file , output_file ) : 
y , sr = librosa . load ( input_file ) 
y_harm = librosa . effects . harmonic ( y ) 
tuning = librosa . estimate_tuning ( y = y_harm , sr = sr ) 
y_tuned = librosa . effects . pitch_shift ( y , sr , - tuning ) 
librosa . output . write_wav ( output_file , y_tuned , sr ) 
~~ def frames_to_samples ( frames , hop_length = 512 , n_fft = None ) : 
offset = 0 
if n_fft is not None : 
~~~ offset = int ( n_fft // 2 ) 
~~ return ( np . asanyarray ( frames ) * hop_length + offset ) . astype ( int ) 
~~ def samples_to_frames ( samples , hop_length = 512 , n_fft = None ) : 
~~ samples = np . asanyarray ( samples ) 
return np . floor ( ( samples - offset ) // hop_length ) . astype ( int ) 
~~ def frames_to_time ( frames , sr = 22050 , hop_length = 512 , n_fft = None ) : 
samples = frames_to_samples ( frames , 
hop_length = hop_length , 
n_fft = n_fft ) 
return samples_to_time ( samples , sr = sr ) 
~~ def time_to_frames ( times , sr = 22050 , hop_length = 512 , n_fft = None ) : 
samples = time_to_samples ( times , sr = sr ) 
return samples_to_frames ( samples , hop_length = hop_length , n_fft = n_fft ) 
~~ def note_to_midi ( note , round_midi = True ) : 
if not isinstance ( note , six . string_types ) : 
~~~ return np . array ( [ note_to_midi ( n , round_midi = round_midi ) for n in note ] ) 
~~ pitch_map = { 'C' : 0 , 'D' : 2 , 'E' : 4 , 'F' : 5 , 'G' : 7 , 'A' : 9 , 'B' : 11 } 
acc_map = { '#' : 1 , '' : 0 , 'b' : - 1 , '!' : - 1 } 
match = re . match ( r'^(?P<note>[A-Ga-g])' 
r'(?P<accidental>[#b!]*)' 
r'(?P<octave>[+-]?\\d+)?' 
r'(?P<cents>[+-]\\d+)?$' , 
note ) 
if not match : 
~~ pitch = match . group ( 'note' ) . upper ( ) 
offset = np . sum ( [ acc_map [ o ] for o in match . group ( 'accidental' ) ] ) 
octave = match . group ( 'octave' ) 
cents = match . group ( 'cents' ) 
if not octave : 
~~~ octave = 0 
~~~ octave = int ( octave ) 
~~ if not cents : 
~~~ cents = 0 
~~~ cents = int ( cents ) * 1e-2 
~~ note_value = 12 * ( octave + 1 ) + pitch_map [ pitch ] + offset + cents 
if round_midi : 
~~~ note_value = int ( np . round ( note_value ) ) 
~~ return note_value 
~~ def midi_to_note ( midi , octave = True , cents = False ) : 
if cents and not octave : 
~~ if not np . isscalar ( midi ) : 
~~~ return [ midi_to_note ( x , octave = octave , cents = cents ) for x in midi ] 
~~ note_map = [ 'C' , 'C#' , 'D' , 'D#' , 
'E' , 'F' , 'F#' , 'G' , 
'G#' , 'A' , 'A#' , 'B' ] 
note_num = int ( np . round ( midi ) ) 
note_cents = int ( 100 * np . around ( midi - note_num , 2 ) ) 
note = note_map [ note_num % 12 ] 
if octave : 
~~~ note = '{:s}{:0d}' . format ( note , int ( note_num / 12 ) - 1 ) 
~~ if cents : 
~~~ note = '{:s}{:+02d}' . format ( note , note_cents ) 
~~ return note 
~~ def hz_to_mel ( frequencies , htk = False ) : 
frequencies = np . asanyarray ( frequencies ) 
if htk : 
~~~ return 2595.0 * np . log10 ( 1.0 + frequencies / 700.0 ) 
~~ f_min = 0.0 
f_sp = 200.0 / 3 
mels = ( frequencies - f_min ) / f_sp 
if frequencies . ndim : 
~~~ log_t = ( frequencies >= min_log_hz ) 
mels [ log_t ] = min_log_mel + np . log ( frequencies [ log_t ] / min_log_hz ) / logstep 
~~ elif frequencies >= min_log_hz : 
~~~ mels = min_log_mel + np . log ( frequencies / min_log_hz ) / logstep 
~~ return mels 
~~ def mel_to_hz ( mels , htk = False ) : 
mels = np . asanyarray ( mels ) 
~~~ return 700.0 * ( 10.0 ** ( mels / 2595.0 ) - 1.0 ) 
freqs = f_min + f_sp * mels 
if mels . ndim : 
~~~ log_t = ( mels >= min_log_mel ) 
freqs [ log_t ] = min_log_hz * np . exp ( logstep * ( mels [ log_t ] - min_log_mel ) ) 
~~ elif mels >= min_log_mel : 
~~~ freqs = min_log_hz * np . exp ( logstep * ( mels - min_log_mel ) ) 
~~ return freqs 
~~ def hz_to_octs ( frequencies , A440 = 440.0 ) : 
return np . log2 ( np . asanyarray ( frequencies ) / ( float ( A440 ) / 16 ) ) 
~~ def fft_frequencies ( sr = 22050 , n_fft = 2048 ) : 
return np . linspace ( 0 , 
float ( sr ) / 2 , 
int ( 1 + n_fft // 2 ) , 
endpoint = True ) 
~~ def cqt_frequencies ( n_bins , fmin , bins_per_octave = 12 , tuning = 0.0 ) : 
correction = 2.0 ** ( float ( tuning ) / bins_per_octave ) 
frequencies = 2.0 ** ( np . arange ( 0 , n_bins , dtype = float ) / bins_per_octave ) 
return correction * fmin * frequencies 
~~ def mel_frequencies ( n_mels = 128 , fmin = 0.0 , fmax = 11025.0 , htk = False ) : 
min_mel = hz_to_mel ( fmin , htk = htk ) 
max_mel = hz_to_mel ( fmax , htk = htk ) 
mels = np . linspace ( min_mel , max_mel , n_mels ) 
return mel_to_hz ( mels , htk = htk ) 
~~ def tempo_frequencies ( n_bins , hop_length = 512 , sr = 22050 ) : 
bin_frequencies = np . zeros ( int ( n_bins ) , dtype = np . float ) 
bin_frequencies [ 0 ] = np . inf 
bin_frequencies [ 1 : ] = 60.0 * sr / ( hop_length * np . arange ( 1.0 , n_bins ) ) 
return bin_frequencies 
f_sq = frequencies ** 2.0 
const = np . array ( [ 12200 , 20.6 , 107.7 , 737.9 ] ) ** 2.0 
weights = 2.0 + 20.0 * ( np . log10 ( const [ 0 ] ) + 4 * np . log10 ( frequencies ) 
- np . log10 ( f_sq + const [ 0 ] ) 
- np . log10 ( f_sq + const [ 1 ] ) 
- 0.5 * np . log10 ( f_sq + const [ 2 ] ) 
- 0.5 * np . log10 ( f_sq + const [ 3 ] ) ) 
if min_db is not None : 
~~~ weights = np . maximum ( min_db , weights ) 
~~ def times_like ( X , sr = 22050 , hop_length = 512 , n_fft = None , axis = - 1 ) : 
samples = samples_like ( X , hop_length = hop_length , n_fft = n_fft , axis = axis ) 
~~ def samples_like ( X , hop_length = 512 , n_fft = None , axis = - 1 ) : 
if np . isscalar ( X ) : 
~~~ frames = np . arange ( X ) 
~~~ frames = np . arange ( X . shape [ axis ] ) 
~~ return frames_to_samples ( frames , hop_length = hop_length , n_fft = n_fft ) 
~~ def cqt ( y , sr = 22050 , hop_length = 512 , fmin = None , n_bins = 84 , 
bins_per_octave = 12 , tuning = 0.0 , filter_scale = 1 , 
norm = 1 , sparsity = 0.01 , window = 'hann' , 
scale = True , pad_mode = 'reflect' , res_type = None ) : 
~~~ \ 
n_octaves = int ( np . ceil ( float ( n_bins ) / bins_per_octave ) ) 
n_filters = min ( bins_per_octave , n_bins ) 
len_orig = len ( y ) 
if fmin is None : 
~~~ fmin = note_to_hz ( 'C1' ) 
~~ if tuning is None : 
~~~ tuning = estimate_tuning ( y = y , sr = sr ) 
~~ freqs = cqt_frequencies ( n_bins , fmin , 
bins_per_octave = bins_per_octave , tuning = tuning ) [ - bins_per_octave : ] 
fmin_t = np . min ( freqs ) 
fmax_t = np . max ( freqs ) 
Q = float ( filter_scale ) / ( 2.0 ** ( 1. / bins_per_octave ) - 1 ) 
filter_cutoff = fmax_t * ( 1 + 0.5 * filters . window_bandwidth ( window ) / Q ) 
nyquist = sr / 2.0 
auto_resample = False 
if not res_type : 
~~~ auto_resample = True 
if filter_cutoff < audio . BW_FASTEST * nyquist : 
~~~ res_type = 'kaiser_fast' 
~~~ res_type = 'kaiser_best' 
~~ ~~ y , sr , hop_length = __early_downsample ( y , sr , hop_length , 
res_type , 
n_octaves , 
nyquist , filter_cutoff , scale ) 
cqt_resp = [ ] 
if auto_resample and res_type != 'kaiser_fast' : 
~~~ fft_basis , n_fft , _ = __cqt_filter_fft ( sr , fmin_t , 
bins_per_octave , 
tuning , 
filter_scale , 
norm , 
sparsity , 
window = window ) 
cqt_resp . append ( __cqt_response ( y , n_fft , hop_length , fft_basis , pad_mode ) ) 
fmin_t /= 2 
fmax_t /= 2 
n_octaves -= 1 
res_type = 'kaiser_fast' 
~~ num_twos = __num_two_factors ( hop_length ) 
if num_twos < n_octaves - 1 : 
. format ( n_octaves - 1 , n_octaves ) ) 
~~ fft_basis , n_fft , _ = __cqt_filter_fft ( sr , fmin_t , 
my_y , my_sr , my_hop = y , sr , hop_length 
for i in range ( n_octaves ) : 
~~~ if i > 0 : 
~~~ if len ( my_y ) < 2 : 
n_octaves ) ) 
~~ my_y = audio . resample ( my_y , 2 , 1 , 
res_type = res_type , 
scale = True ) 
fft_basis [ : ] *= np . sqrt ( 2 ) 
my_sr /= 2.0 
my_hop //= 2 
~~ cqt_resp . append ( __cqt_response ( my_y , n_fft , my_hop , fft_basis , pad_mode ) ) 
~~ C = __trim_stack ( cqt_resp , n_bins ) 
if scale : 
~~~ lengths = filters . constant_q_lengths ( sr , fmin , 
n_bins = n_bins , 
bins_per_octave = bins_per_octave , 
tuning = tuning , 
window = window , 
filter_scale = filter_scale ) 
C /= np . sqrt ( lengths [ : , np . newaxis ] ) 
~~ return C 
~~ def hybrid_cqt ( y , sr = 22050 , hop_length = 512 , fmin = None , n_bins = 84 , 
norm = 1 , sparsity = 0.01 , window = 'hann' , scale = True , 
pad_mode = 'reflect' , res_type = None ) : 
tuning = tuning ) 
lengths = filters . constant_q_lengths ( sr , fmin , 
filter_scale = filter_scale , 
pseudo_filters = 2.0 ** np . ceil ( np . log2 ( lengths ) ) < 2 * hop_length 
n_bins_pseudo = int ( np . sum ( pseudo_filters ) ) 
n_bins_full = n_bins - n_bins_pseudo 
if n_bins_pseudo > 0 : 
~~~ fmin_pseudo = np . min ( freqs [ pseudo_filters ] ) 
cqt_resp . append ( pseudo_cqt ( y , sr , 
fmin = fmin_pseudo , 
n_bins = n_bins_pseudo , 
norm = norm , 
sparsity = sparsity , 
scale = scale , 
pad_mode = pad_mode ) ) 
~~ if n_bins_full > 0 : 
~~~ cqt_resp . append ( np . abs ( cqt ( y , sr , 
fmin = fmin , 
n_bins = n_bins_full , 
pad_mode = pad_mode , 
res_type = res_type ) ) ) 
~~ return __trim_stack ( cqt_resp , n_bins ) 
~~ def pseudo_cqt ( y , sr = 22050 , hop_length = 512 , fmin = None , n_bins = 84 , 
pad_mode = 'reflect' ) : 
~~ fft_basis , n_fft , _ = __cqt_filter_fft ( sr , fmin , n_bins , 
tuning , filter_scale , 
norm , sparsity , 
fft_basis = np . abs ( fft_basis ) 
D = np . abs ( stft ( y , n_fft = n_fft , hop_length = hop_length , pad_mode = pad_mode ) ) 
C = fft_basis . dot ( D ) 
~~~ C /= np . sqrt ( n_fft ) 
C *= np . sqrt ( lengths [ : , np . newaxis ] / n_fft ) 
~~ def icqt ( C , sr = 22050 , hop_length = 512 , fmin = None , bins_per_octave = 12 , 
tuning = 0.0 , filter_scale = 1 , norm = 1 , sparsity = 0.01 , window = 'hann' , 
scale = True , length = None , amin = util . Deprecated ( ) , res_type = 'fft' ) : 
~~ n_bins = len ( C ) 
freqs = cqt_frequencies ( n_bins , fmin , 
tuning = tuning ) [ - bins_per_octave : ] 
n_filters = min ( n_bins , bins_per_octave ) 
fft_basis , n_fft , lengths = __cqt_filter_fft ( sr , np . min ( freqs ) , 
if hop_length > min ( lengths ) : 
~~ fft_basis = fft_basis . todense ( ) * n_fft / lengths [ : , np . newaxis ] 
inv_basis = fft_basis . H 
y = None 
for octave in range ( n_octaves - 1 , - 1 , - 1 ) : 
~~~ slice_ = slice ( - ( octave + 1 ) * bins_per_octave - 1 , 
- ( octave ) * bins_per_octave - 1 ) 
C_oct = C [ slice_ ] 
inv_oct = inv_basis [ : , - C_oct . shape [ 0 ] : ] 
oct_hop = hop_length // 2 ** octave 
~~~ C_scale = np . sqrt ( lengths [ - C_oct . shape [ 0 ] : , np . newaxis ] ) / n_fft 
~~~ C_scale = lengths [ - C_oct . shape [ 0 ] : , np . newaxis ] * np . sqrt ( 2 ** octave ) / n_fft 
~~ D_oct = inv_oct . dot ( C_oct / C_scale ) 
y_oct = istft ( D_oct , window = 'ones' , hop_length = oct_hop ) 
~~~ y = y_oct 
~~~ y = audio . resample ( y , 1 , 2 , scale = True , res_type = res_type , fix = False ) 
y [ : len ( y_oct ) ] += y_oct 
~~ ~~ if length : 
~~~ y = util . fix_length ( y , length ) 
~~ return y 
~~ def __cqt_filter_fft ( sr , fmin , n_bins , bins_per_octave , tuning , 
filter_scale , norm , sparsity , hop_length = None , 
window = 'hann' ) : 
basis , lengths = filters . constant_q ( sr , 
pad_fft = True , 
n_fft = basis . shape [ 1 ] 
if ( hop_length is not None and 
n_fft < 2.0 ** ( 1 + np . ceil ( np . log2 ( hop_length ) ) ) ) : 
~~~ n_fft = int ( 2.0 ** ( 1 + np . ceil ( np . log2 ( hop_length ) ) ) ) 
~~ basis *= lengths [ : , np . newaxis ] / float ( n_fft ) 
fft = get_fftlib ( ) 
fft_basis = fft . fft ( basis , n = n_fft , axis = 1 ) [ : , : ( n_fft // 2 ) + 1 ] 
fft_basis = util . sparsify_rows ( fft_basis , quantile = sparsity ) 
return fft_basis , n_fft , lengths 
~~ def __trim_stack ( cqt_resp , n_bins ) : 
max_col = min ( x . shape [ 1 ] for x in cqt_resp ) 
cqt_resp = np . vstack ( [ x [ : , : max_col ] for x in cqt_resp ] [ : : - 1 ] ) 
return np . ascontiguousarray ( cqt_resp [ - n_bins : ] . T ) . T 
~~ def __cqt_response ( y , n_fft , hop_length , fft_basis , mode ) : 
D = stft ( y , n_fft = n_fft , hop_length = hop_length , 
window = 'ones' , 
pad_mode = mode ) 
return fft_basis . dot ( D ) 
~~ def __early_downsample_count ( nyquist , filter_cutoff , hop_length , n_octaves ) : 
downsample_count1 = max ( 0 , int ( np . ceil ( np . log2 ( audio . BW_FASTEST * nyquist / 
filter_cutoff ) ) - 1 ) - 1 ) 
num_twos = __num_two_factors ( hop_length ) 
downsample_count2 = max ( 0 , num_twos - n_octaves + 1 ) 
return min ( downsample_count1 , downsample_count2 ) 
~~ def __early_downsample ( y , sr , hop_length , res_type , n_octaves , 
nyquist , filter_cutoff , scale ) : 
downsample_count = __early_downsample_count ( nyquist , filter_cutoff , 
hop_length , n_octaves ) 
if downsample_count > 0 and res_type == 'kaiser_fast' : 
~~~ downsample_factor = 2 ** ( downsample_count ) 
hop_length //= downsample_factor 
if len ( y ) < downsample_factor : 
~~ new_sr = sr / float ( downsample_factor ) 
y = audio . resample ( y , sr , new_sr , 
if not scale : 
~~~ y *= np . sqrt ( downsample_factor ) 
~~ sr = new_sr 
~~ return y , sr , hop_length 
~~ def delta ( data , width = 9 , order = 1 , axis = - 1 , mode = 'interp' , ** kwargs ) : 
data = np . atleast_1d ( data ) 
if mode == 'interp' and width > data . shape [ axis ] : 
~~ if width < 3 or np . mod ( width , 2 ) != 1 : 
~~ if order <= 0 or not isinstance ( order , int ) : 
~~ kwargs . pop ( 'deriv' , None ) 
kwargs . setdefault ( 'polyorder' , order ) 
return scipy . signal . savgol_filter ( data , width , 
deriv = order , 
axis = axis , 
mode = mode , 
~~ def stack_memory ( data , n_steps = 2 , delay = 1 , ** kwargs ) : 
if n_steps < 1 : 
~~ if delay == 0 : 
~~ data = np . atleast_2d ( data ) 
t = data . shape [ 1 ] 
kwargs . setdefault ( 'mode' , 'constant' ) 
if kwargs [ 'mode' ] == 'constant' : 
~~~ kwargs . setdefault ( 'constant_values' , [ 0 ] ) 
~~ if delay > 0 : 
~~~ padding = ( int ( ( n_steps - 1 ) * delay ) , 0 ) 
~~~ padding = ( 0 , int ( ( n_steps - 1 ) * - delay ) ) 
~~ data = np . pad ( data , [ ( 0 , 0 ) , padding ] , ** kwargs ) 
history = data 
for i in range ( 1 , n_steps ) : 
~~~ history = np . vstack ( [ np . roll ( data , - i * delay , axis = 1 ) , history ] ) 
~~~ history = history [ : , : t ] 
~~~ history = history [ : , - t : ] 
~~ return np . ascontiguousarray ( history . T ) . T 
~~ def dtw ( X = None , Y = None , C = None , metric = 'euclidean' , step_sizes_sigma = None , 
weights_add = None , weights_mul = None , subseq = False , backtrack = True , 
global_constraints = False , band_rad = 0.25 ) : 
if step_sizes_sigma is None : 
~~~ step_sizes_sigma = np . array ( [ [ 1 , 1 ] , [ 0 , 1 ] , [ 1 , 0 ] ] ) 
~~ if weights_add is None : 
~~~ weights_add = np . zeros ( len ( step_sizes_sigma ) ) 
~~ if weights_mul is None : 
~~~ weights_mul = np . ones ( len ( step_sizes_sigma ) ) 
~~ if len ( step_sizes_sigma ) != len ( weights_add ) : 
~~ if len ( step_sizes_sigma ) != len ( weights_mul ) : 
~~ if C is None and ( X is None or Y is None ) : 
~~ if C is not None and ( X is not None or Y is not None ) : 
~~ if C is None : 
~~~ X = np . atleast_2d ( X ) 
Y = np . atleast_2d ( Y ) 
~~~ C = cdist ( X . T , Y . T , metric = metric ) 
six . reraise ( ParameterError , ParameterError ( msg ) ) 
~~ if subseq and ( X . shape [ 1 ] > Y . shape [ 1 ] ) : 
~~~ C = C . T 
~~ ~~ C = np . atleast_2d ( C ) 
if np . array_equal ( step_sizes_sigma , np . array ( [ [ 1 , 1 ] ] ) ) and ( C . shape [ 0 ] > C . shape [ 1 ] ) : 
~~ max_0 = step_sizes_sigma [ : , 0 ] . max ( ) 
max_1 = step_sizes_sigma [ : , 1 ] . max ( ) 
if global_constraints : 
~~~ fill_off_diagonal ( C , band_rad , value = np . inf ) 
~~ D = np . ones ( C . shape + np . array ( [ max_0 , max_1 ] ) ) * np . inf 
D [ max_0 , max_1 ] = C [ 0 , 0 ] 
if subseq : 
~~~ D [ max_0 , max_1 : ] = C [ 0 , : ] 
~~ D_steps = - 1 * np . ones ( D . shape , dtype = np . int ) 
D , D_steps = __dtw_calc_accu_cost ( C , D , D_steps , 
step_sizes_sigma , 
weights_mul , weights_add , 
max_0 , max_1 ) 
D = D [ max_0 : , max_1 : ] 
D_steps = D_steps [ max_0 : , max_1 : ] 
if backtrack : 
~~~ if subseq : 
~~~ wp_end_idx = np . argmin ( D [ - 1 , : ] ) + 1 
wp = __dtw_backtracking ( D_steps [ : , : wp_end_idx ] , step_sizes_sigma ) 
~~~ wp = __dtw_backtracking ( D_steps , step_sizes_sigma ) 
~~ wp = np . asarray ( wp , dtype = int ) 
if subseq and ( X . shape [ 1 ] > Y . shape [ 1 ] ) : 
~~~ wp = np . fliplr ( wp ) 
~~ return D , wp 
~~~ return D 
~~ ~~ def __dtw_calc_accu_cost ( C , D , D_steps , step_sizes_sigma , 
for cur_n in range ( max_0 , D . shape [ 0 ] ) : 
~~~ for cur_m in range ( max_1 , D . shape [ 1 ] ) : 
~~~ for cur_step_idx , cur_w_add , cur_w_mul in zip ( range ( step_sizes_sigma . shape [ 0 ] ) , 
weights_add , weights_mul ) : 
~~~ cur_D = D [ cur_n - step_sizes_sigma [ cur_step_idx , 0 ] , 
cur_m - step_sizes_sigma [ cur_step_idx , 1 ] ] 
cur_C = cur_w_mul * C [ cur_n - max_0 , cur_m - max_1 ] 
cur_C += cur_w_add 
cur_cost = cur_D + cur_C 
if cur_cost < D [ cur_n , cur_m ] : 
~~~ D [ cur_n , cur_m ] = cur_cost 
D_steps [ cur_n , cur_m ] = cur_step_idx 
~~ ~~ ~~ ~~ return D , D_steps 
wp = [ ] 
cur_idx = ( D_steps . shape [ 0 ] - 1 , D_steps . shape [ 1 ] - 1 ) 
wp . append ( ( cur_idx [ 0 ] , cur_idx [ 1 ] ) ) 
while cur_idx [ 0 ] > 0 : 
~~~ cur_step_idx = D_steps [ ( cur_idx [ 0 ] , cur_idx [ 1 ] ) ] 
cur_idx = ( cur_idx [ 0 ] - step_sizes_sigma [ cur_step_idx ] [ 0 ] , 
cur_idx [ 1 ] - step_sizes_sigma [ cur_step_idx ] [ 1 ] ) 
~~ return wp 
n_steps , n_states = log_prob . shape 
value [ 0 ] = log_prob [ 0 ] + log_p_init 
for t in range ( 1 , n_steps ) : 
~~~ trans_out = value [ t - 1 ] + log_trans . T 
for j in range ( n_states ) : 
~~~ ptr [ t , j ] = np . argmax ( trans_out [ j ] ) 
value [ t , j ] = log_prob [ t , j ] + trans_out [ j , ptr [ t ] [ j ] ] 
~~ ~~ state [ - 1 ] = np . argmax ( value [ - 1 ] ) 
for t in range ( n_steps - 2 , - 1 , - 1 ) : 
~~~ state [ t ] = ptr [ t + 1 , state [ t + 1 ] ] 
~~ ~~ def viterbi_discriminative ( prob , transition , p_state = None , p_init = None , return_logp = False ) : 
n_states , n_steps = prob . shape 
if transition . shape != ( n_states , n_states ) : 
( n_states , n_states ) ) ) 
~~ if np . any ( transition < 0 ) or not np . allclose ( transition . sum ( axis = 1 ) , 1 ) : 
~~ if np . any ( prob < 0 ) or not np . allclose ( prob . sum ( axis = 0 ) , 1 ) : 
~~ states = np . zeros ( n_steps , dtype = int ) 
values = np . zeros ( ( n_steps , n_states ) , dtype = float ) 
ptr = np . zeros ( ( n_steps , n_states ) , dtype = int ) 
epsilon = np . finfo ( prob . dtype ) . tiny 
if p_state is None : 
~~~ p_state = np . empty ( n_states ) 
p_state . fill ( 1. / n_states ) 
~~ elif p_state . shape != ( n_states , ) : 
~~ elif np . any ( p_state < 0 ) or not np . allclose ( p_state . sum ( axis = - 1 ) , 1 ) : 
'p_state={}' . format ( p_state ) ) 
~~ log_trans = np . log ( transition + epsilon ) 
log_marginal = np . log ( p_state + epsilon ) 
# 
log_prob = np . log ( prob . T + epsilon ) - log_marginal 
if p_init is None : 
~~~ p_init = np . empty ( n_states ) 
p_init . fill ( 1. / n_states ) 
~~ elif np . any ( p_init < 0 ) or not np . allclose ( p_init . sum ( ) , 1 ) : 
'p_init={}' . format ( p_init ) ) 
~~ log_p_init = np . log ( p_init + epsilon ) 
_viterbi ( log_prob , log_trans , log_p_init , states , values , ptr ) 
if return_logp : 
~~~ return states , values [ - 1 , states [ - 1 ] ] 
~~ return states 
~~ def viterbi_binary ( prob , transition , p_state = None , p_init = None , return_logp = False ) : 
prob = np . atleast_2d ( prob ) 
if transition . shape == ( 2 , 2 ) : 
~~~ transition = np . tile ( transition , ( n_states , 1 , 1 ) ) 
~~ elif transition . shape != ( n_states , 2 , 2 ) : 
~~ if np . any ( transition < 0 ) or not np . allclose ( transition . sum ( axis = - 1 ) , 1 ) : 
~~ if np . any ( prob < 0 ) or np . any ( prob > 1 ) : 
~~ if p_state is None : 
p_state . fill ( 0.5 ) 
~~~ p_state = np . atleast_1d ( p_state ) 
~~ if p_state . shape != ( n_states , ) or np . any ( p_state < 0 ) or np . any ( p_state > 1 ) : 
~~ if p_init is None : 
p_init . fill ( 0.5 ) 
~~~ p_init = np . atleast_1d ( p_init ) 
~~ if p_init . shape != ( n_states , ) or np . any ( p_init < 0 ) or np . any ( p_init > 1 ) : 
~~ states = np . empty ( ( n_states , n_steps ) , dtype = int ) 
logp = np . empty ( n_states ) 
prob_binary = np . empty ( ( 2 , n_steps ) ) 
p_state_binary = np . empty ( 2 ) 
p_init_binary = np . empty ( 2 ) 
for state in range ( n_states ) : 
~~~ prob_binary [ 0 ] = 1 - prob [ state ] 
prob_binary [ 1 ] = prob [ state ] 
p_state_binary [ 0 ] = 1 - p_state [ state ] 
p_state_binary [ 1 ] = p_state [ state ] 
p_init_binary [ 0 ] = 1 - p_init [ state ] 
p_init_binary [ 1 ] = p_init [ state ] 
states [ state , : ] , logp [ state ] = viterbi_discriminative ( prob_binary , 
transition [ state ] , 
p_state = p_state_binary , 
p_init = p_init_binary , 
return_logp = True ) 
~~ if return_logp : 
~~~ return states , logp 
~~ def transition_uniform ( n_states ) : 
if not isinstance ( n_states , int ) or n_states <= 0 : 
~~ transition = np . empty ( ( n_states , n_states ) , dtype = np . float ) 
transition . fill ( 1. / n_states ) 
return transition 
~~ def transition_loop ( n_states , prob ) : 
if not isinstance ( n_states , int ) or n_states <= 1 : 
prob = np . asarray ( prob , dtype = np . float ) 
if prob . ndim == 0 : 
~~~ prob = np . tile ( prob , n_states ) 
~~ if prob . shape != ( n_states , ) : 
~~ for i , prob_i in enumerate ( prob ) : 
~~~ transition [ i ] = ( 1. - prob_i ) / ( n_states - 1 ) 
transition [ i , i ] = prob_i 
~~ return transition 
~~ def transition_cycle ( n_states , prob ) : 
~~ transition = np . zeros ( ( n_states , n_states ) , dtype = np . float ) 
~~~ transition [ i , np . mod ( i + 1 , n_states ) ] = 1. - prob_i 
~~ def transition_local ( n_states , width , window = 'triangle' , wrap = False ) : 
~~ width = np . asarray ( width , dtype = int ) 
if width . ndim == 0 : 
~~~ width = np . tile ( width , n_states ) 
~~ if width . shape != ( n_states , ) : 
~~ if np . any ( width < 1 ) : 
for i , width_i in enumerate ( width ) : 
~~~ trans_row = pad_center ( get_window ( window , width_i , fftbins = False ) , n_states ) 
trans_row = np . roll ( trans_row , n_states // 2 + i + 1 ) 
if not wrap : 
~~~ trans_row [ min ( n_states , i + width_i // 2 + 1 ) : ] = 0 
trans_row [ : max ( 0 , i - width_i // 2 ) ] = 0 
~~ transition [ i ] = trans_row 
~~ transition /= transition . sum ( axis = 1 , keepdims = True ) 
~~ def onset_detect ( y = None , sr = 22050 , onset_envelope = None , hop_length = 512 , 
backtrack = False , energy = None , 
units = 'frames' , ** kwargs ) : 
if onset_envelope is None : 
~~~ if y is None : 
~~ onset_envelope = onset_strength ( y = y , sr = sr , hop_length = hop_length ) 
~~ onset_envelope -= onset_envelope . min ( ) 
if not onset_envelope . any ( ) : 
~~~ return np . array ( [ ] , dtype = np . int ) 
~~ onset_envelope /= onset_envelope . max ( ) 
kwargs . setdefault ( 'delta' , 0.07 ) 
onsets = util . peak_pick ( onset_envelope , ** kwargs ) 
~~~ if energy is None : 
~~~ energy = onset_envelope 
~~ onsets = onset_backtrack ( onsets , energy ) 
~~ if units == 'frames' : 
~~ elif units == 'samples' : 
~~~ onsets = core . frames_to_samples ( onsets , hop_length = hop_length ) 
~~ elif units == 'time' : 
~~~ onsets = core . frames_to_time ( onsets , hop_length = hop_length , sr = sr ) 
~~ return onsets 
~~ def onset_strength ( y = None , sr = 22050 , S = None , lag = 1 , max_size = 1 , 
ref = None , 
detrend = False , center = True , 
feature = None , aggregate = None , 
centering = None , 
if aggregate is False : 
~~ odf_all = onset_strength_multi ( y = y , 
sr = sr , 
S = S , 
lag = lag , 
max_size = max_size , 
ref = ref , 
detrend = detrend , 
center = center , 
feature = feature , 
aggregate = aggregate , 
channels = None , 
return odf_all [ 0 ] 
~~ def onset_backtrack ( events , energy ) : 
minima = np . flatnonzero ( ( energy [ 1 : - 1 ] <= energy [ : - 2 ] ) & 
( energy [ 1 : - 1 ] < energy [ 2 : ] ) ) 
minima = util . fix_frames ( 1 + minima , x_min = 0 ) 
return minima [ util . match_events ( events , minima , right = False ) ] 
~~ def onset_strength_multi ( y = None , sr = 22050 , S = None , lag = 1 , max_size = 1 , 
ref = None , detrend = False , center = True , feature = None , 
aggregate = None , channels = None , ** kwargs ) : 
if feature is None : 
~~~ feature = melspectrogram 
kwargs . setdefault ( 'fmax' , 11025.0 ) 
~~ if aggregate is None : 
~~~ aggregate = np . mean 
~~ if lag < 1 or not isinstance ( lag , int ) : 
~~ if max_size < 1 or not isinstance ( max_size , int ) : 
~~ if S is None : 
~~~ S = np . abs ( feature ( y = y , sr = sr , ** kwargs ) ) 
S = core . power_to_db ( S ) 
~~ n_fft = kwargs . get ( 'n_fft' , 2048 ) 
hop_length = kwargs . get ( 'hop_length' , 512 ) 
S = np . atleast_2d ( S ) 
if ref is None : 
~~~ if max_size == 1 : 
~~~ ref = S 
~~~ ref = scipy . ndimage . maximum_filter1d ( S , max_size , axis = 0 ) 
~~ ~~ elif ref . shape != S . shape : 
~~ onset_env = S [ : , lag : ] - ref [ : , : - lag ] 
onset_env = np . maximum ( 0.0 , onset_env ) 
pad = True 
if channels is None : 
~~~ channels = [ slice ( None ) ] 
~~~ pad = False 
~~ if aggregate : 
~~~ onset_env = util . sync ( onset_env , channels , 
pad = pad , axis = 0 ) 
~~ pad_width = lag 
if center : 
~~~ pad_width += n_fft // ( 2 * hop_length ) 
~~ onset_env = np . pad ( onset_env , ( [ 0 , 0 ] , [ int ( pad_width ) , 0 ] ) , 
mode = 'constant' ) 
if detrend : 
~~~ onset_env = scipy . signal . lfilter ( [ 1.0 , - 1.0 ] , [ 1.0 , - 0.99 ] , 
onset_env , axis = - 1 ) 
~~ if center : 
~~~ onset_env = onset_env [ : , : S . shape [ 1 ] ] 
~~ return onset_env 
~~ def annotation ( path , intervals , annotations = None , delimiter = ',' , fmt = '%0.3f' ) : 
util . valid_intervals ( intervals ) 
if annotations is not None and len ( annotations ) != len ( intervals ) : 
~~ with open ( path , 'w' ) as output_file : 
~~~ writer = csv . writer ( output_file , delimiter = delimiter ) 
if annotations is None : 
~~~ for t_int in intervals : 
~~~ writer . writerow ( [ fmt % t_int [ 0 ] , fmt % t_int [ 1 ] ] ) 
~~~ for t_int , lab in zip ( intervals , annotations ) : 
~~~ writer . writerow ( [ fmt % t_int [ 0 ] , fmt % t_int [ 1 ] , lab ] ) 
~~ ~~ ~~ ~~ def times_csv ( path , times , annotations = None , delimiter = ',' , fmt = '%0.3f' ) : 
if annotations is not None and len ( annotations ) != len ( times ) : 
~~~ for t in times : 
~~~ writer . writerow ( [ fmt % t ] ) 
~~~ for t , lab in zip ( times , annotations ) : 
~~~ writer . writerow ( [ ( fmt % t ) , lab ] ) 
~~ ~~ ~~ ~~ def write_wav ( path , y , sr , norm = False ) : 
util . valid_audio ( y , mono = False ) 
if norm and np . issubdtype ( y . dtype , np . floating ) : 
~~~ wav = util . normalize ( y , norm = np . inf , axis = None ) 
~~~ wav = y 
~~ if wav . ndim > 1 and wav . shape [ 0 ] == 2 : 
~~~ wav = wav . T 
~~ scipy . io . wavfile . write ( path , sr , wav ) 
~~ def cmap ( data , robust = True , cmap_seq = 'magma' , cmap_bool = 'gray_r' , cmap_div = 'coolwarm' ) : 
if data . dtype == 'bool' : 
~~~ return get_cmap ( cmap_bool ) 
~~ data = data [ np . isfinite ( data ) ] 
if robust : 
~~~ min_p , max_p = 2 , 98 
~~~ min_p , max_p = 0 , 100 
~~ max_val = np . percentile ( data , max_p ) 
min_val = np . percentile ( data , min_p ) 
if min_val >= 0 or max_val <= 0 : 
~~~ return get_cmap ( cmap_seq ) 
~~ return get_cmap ( cmap_div ) 
~~ def __envelope ( x , hop ) : 
return util . frame ( x , hop_length = hop , frame_length = hop ) . max ( axis = 0 ) 
~~ def waveplot ( y , sr = 22050 , max_points = 5e4 , x_axis = 'time' , offset = 0.0 , 
max_sr = 1000 , ax = None , ** kwargs ) : 
if not ( isinstance ( max_sr , int ) and max_sr > 0 ) : 
~~ target_sr = sr 
hop_length = 1 
if max_points is not None : 
~~~ if max_points <= 0 : 
~~ if max_points < y . shape [ - 1 ] : 
~~~ target_sr = min ( max_sr , ( sr * y . shape [ - 1 ] ) // max_points ) 
~~ hop_length = sr // target_sr 
if y . ndim == 1 : 
~~~ y = __envelope ( y , hop_length ) 
~~~ y = np . vstack ( [ __envelope ( _ , hop_length ) for _ in y ] ) 
~~ ~~ if y . ndim > 1 : 
~~~ y_top = y [ 0 ] 
y_bottom = - y [ 1 ] 
~~~ y_top = y 
y_bottom = - y 
~~ axes = __check_axes ( ax ) 
kwargs . setdefault ( 'color' , next ( axes . _get_lines . prop_cycler ) [ 'color' ] ) 
locs = offset + core . frames_to_time ( np . arange ( len ( y_top ) ) , 
hop_length = hop_length ) 
out = axes . fill_between ( locs , y_bottom , y_top , ** kwargs ) 
axes . set_xlim ( [ locs . min ( ) , locs . max ( ) ] ) 
if x_axis == 'time' : 
~~~ axes . xaxis . set_major_formatter ( TimeFormatter ( lag = False ) ) 
axes . xaxis . set_label_text ( 'Time' ) 
~~ elif x_axis is None or x_axis in [ 'off' , 'none' ] : 
~~~ axes . set_xticks ( [ ] ) 
~~ def specshow ( data , x_coords = None , y_coords = None , 
x_axis = None , y_axis = None , 
sr = 22050 , hop_length = 512 , 
fmin = None , fmax = None , 
bins_per_octave = 12 , 
ax = None , 
if np . issubdtype ( data . dtype , np . complexfloating ) : 
data = np . abs ( data ) 
~~ kwargs . setdefault ( 'cmap' , cmap ( data ) ) 
kwargs . setdefault ( 'rasterized' , True ) 
kwargs . setdefault ( 'edgecolors' , 'None' ) 
kwargs . setdefault ( 'shading' , 'flat' ) 
all_params = dict ( kwargs = kwargs , 
fmax = fmax , 
y_coords = __mesh_coords ( y_axis , y_coords , data . shape [ 0 ] , ** all_params ) 
x_coords = __mesh_coords ( x_axis , x_coords , data . shape [ 1 ] , ** all_params ) 
axes = __check_axes ( ax ) 
out = axes . pcolormesh ( x_coords , y_coords , data , ** kwargs ) 
__set_current_image ( ax , out ) 
axes . set_xlim ( x_coords . min ( ) , x_coords . max ( ) ) 
axes . set_ylim ( y_coords . min ( ) , y_coords . max ( ) ) 
__scale_axes ( axes , x_axis , 'x' ) 
__scale_axes ( axes , y_axis , 'y' ) 
__decorate_axis ( axes . xaxis , x_axis ) 
__decorate_axis ( axes . yaxis , y_axis ) 
return axes 
~~ def __set_current_image ( ax , img ) : 
if ax is None : 
~~~ import matplotlib . pyplot as plt 
plt . sci ( img ) 
~~ ~~ def __mesh_coords ( ax_type , coords , n , ** kwargs ) : 
if coords is not None : 
~~~ if len ( coords ) < n : 
'{}<{}' . format ( len ( coords ) , n ) ) 
~~ return coords 
~~ coord_map = { 'linear' : __coord_fft_hz , 
'hz' : __coord_fft_hz , 
'log' : __coord_fft_hz , 
'mel' : __coord_mel_hz , 
'cqt' : __coord_cqt_hz , 
'cqt_hz' : __coord_cqt_hz , 
'cqt_note' : __coord_cqt_hz , 
'chroma' : __coord_chroma , 
'time' : __coord_time , 
's' : __coord_time , 
'ms' : __coord_time , 
'lag' : __coord_time , 
'lag_s' : __coord_time , 
'lag_ms' : __coord_time , 
'tonnetz' : __coord_n , 
'off' : __coord_n , 
'tempo' : __coord_tempo , 
'frames' : __coord_n , 
None : __coord_n } 
if ax_type not in coord_map : 
~~ return coord_map [ ax_type ] ( n , ** kwargs ) 
~~ def __check_axes ( axes ) : 
if axes is None : 
axes = plt . gca ( ) 
~~ elif not isinstance ( axes , Axes ) : 
~~ return axes 
~~ def __scale_axes ( axes , ax_type , which ) : 
kwargs = dict ( ) 
if which == 'x' : 
~~~ thresh = 'linthreshx' 
base = 'basex' 
scale = 'linscalex' 
scaler = axes . set_xscale 
limit = axes . set_xlim 
~~~ thresh = 'linthreshy' 
base = 'basey' 
scale = 'linscaley' 
scaler = axes . set_yscale 
limit = axes . set_ylim 
~~ if ax_type == 'mel' : 
~~~ mode = 'symlog' 
kwargs [ thresh ] = 1000.0 
kwargs [ base ] = 2 
~~ elif ax_type == 'log' : 
kwargs [ thresh ] = core . note_to_hz ( 'C2' ) 
kwargs [ scale ] = 0.5 
~~ elif ax_type in [ 'cqt' , 'cqt_hz' , 'cqt_note' ] : 
~~~ mode = 'log' 
~~ elif ax_type == 'tempo' : 
limit ( 16 , 480 ) 
~~ scaler ( mode , ** kwargs ) 
~~ def __decorate_axis ( axis , ax_type ) : 
if ax_type == 'tonnetz' : 
~~~ axis . set_major_formatter ( TonnetzFormatter ( ) ) 
axis . set_major_locator ( FixedLocator ( 0.5 + np . arange ( 6 ) ) ) 
axis . set_label_text ( 'Tonnetz' ) 
~~ elif ax_type == 'chroma' : 
~~~ axis . set_major_formatter ( ChromaFormatter ( ) ) 
axis . set_major_locator ( FixedLocator ( 0.5 + 
np . add . outer ( 12 * np . arange ( 10 ) , 
[ 0 , 2 , 4 , 5 , 7 , 9 , 11 ] ) . ravel ( ) ) ) 
~~~ axis . set_major_formatter ( ScalarFormatter ( ) ) 
axis . set_major_locator ( LogLocator ( base = 2.0 ) ) 
axis . set_label_text ( 'BPM' ) 
~~ elif ax_type == 'time' : 
~~~ axis . set_major_formatter ( TimeFormatter ( unit = None , lag = False ) ) 
axis . set_major_locator ( MaxNLocator ( prune = None , 
steps = [ 1 , 1.5 , 5 , 6 , 10 ] ) ) 
axis . set_label_text ( 'Time' ) 
~~ elif ax_type == 's' : 
~~~ axis . set_major_formatter ( TimeFormatter ( unit = 's' , lag = False ) ) 
~~ elif ax_type == 'ms' : 
~~~ axis . set_major_formatter ( TimeFormatter ( unit = 'ms' , lag = False ) ) 
~~ elif ax_type == 'lag' : 
~~~ axis . set_major_formatter ( TimeFormatter ( unit = None , lag = True ) ) 
axis . set_label_text ( 'Lag' ) 
~~ elif ax_type == 'lag_s' : 
~~~ axis . set_major_formatter ( TimeFormatter ( unit = 's' , lag = True ) ) 
~~ elif ax_type == 'lag_ms' : 
~~~ axis . set_major_formatter ( TimeFormatter ( unit = 'ms' , lag = True ) ) 
~~ elif ax_type == 'cqt_note' : 
~~~ axis . set_major_formatter ( NoteFormatter ( ) ) 
axis . set_minor_formatter ( NoteFormatter ( major = False ) ) 
axis . set_minor_locator ( LogLocator ( base = 2.0 , 
subs = 2.0 ** ( np . arange ( 1 , 12 ) / 12.0 ) ) ) 
axis . set_label_text ( 'Note' ) 
~~ elif ax_type in [ 'cqt_hz' ] : 
~~~ axis . set_major_formatter ( LogHzFormatter ( ) ) 
axis . set_minor_formatter ( LogHzFormatter ( major = False ) ) 
axis . set_label_text ( 'Hz' ) 
~~ elif ax_type in [ 'mel' , 'log' ] : 
axis . set_major_locator ( SymmetricalLogLocator ( axis . get_transform ( ) ) ) 
~~ elif ax_type in [ 'linear' , 'hz' ] : 
~~ elif ax_type in [ 'frames' ] : 
~~~ axis . set_label_text ( 'Frames' ) 
~~ elif ax_type in [ 'off' , 'none' , None ] : 
~~~ axis . set_label_text ( '' ) 
axis . set_ticks ( [ ] ) 
~~ ~~ def __coord_fft_hz ( n , sr = 22050 , ** _kwargs ) : 
n_fft = 2 * ( n - 1 ) 
basis = core . fft_frequencies ( sr = sr , n_fft = n_fft ) 
fmax = basis [ - 1 ] 
basis -= 0.5 * ( basis [ 1 ] - basis [ 0 ] ) 
basis = np . append ( np . maximum ( 0 , basis ) , [ fmax ] ) 
return basis 
~~ def __coord_mel_hz ( n , fmin = 0 , fmax = 11025.0 , ** _kwargs ) : 
~~~ fmin = 0 
~~ if fmax is None : 
~~~ fmax = 11025.0 
~~ basis = core . mel_frequencies ( n , fmin = fmin , fmax = fmax ) 
basis [ 1 : ] -= 0.5 * np . diff ( basis ) 
~~ def __coord_cqt_hz ( n , fmin = None , bins_per_octave = 12 , ** _kwargs ) : 
~~~ fmin = core . note_to_hz ( 'C1' ) 
~~ return core . cqt_frequencies ( n + 1 , 
fmin = fmin / 2.0 ** ( 0.5 / bins_per_octave ) , 
bins_per_octave = bins_per_octave ) 
~~ def __coord_chroma ( n , bins_per_octave = 12 , ** _kwargs ) : 
return np . linspace ( 0 , ( 12.0 * n ) / bins_per_octave , num = n + 1 , endpoint = True ) 
~~ def __coord_tempo ( n , sr = 22050 , hop_length = 512 , ** _kwargs ) : 
basis = core . tempo_frequencies ( n + 2 , sr = sr , hop_length = hop_length ) [ 1 : ] 
edges = np . arange ( 1 , n + 2 ) 
return basis * ( edges + 0.5 ) / edges 
~~ def __coord_time ( n , sr = 22050 , hop_length = 512 , ** _kwargs ) : 
return core . frames_to_time ( np . arange ( n + 1 ) , sr = sr , hop_length = hop_length ) 
~~ def estimate_tuning ( y = None , sr = 22050 , S = None , n_fft = 2048 , 
resolution = 0.01 , bins_per_octave = 12 , ** kwargs ) : 
pitch , mag = piptrack ( y = y , sr = sr , S = S , n_fft = n_fft , ** kwargs ) 
pitch_mask = pitch > 0 
if pitch_mask . any ( ) : 
~~~ threshold = np . median ( mag [ pitch_mask ] ) 
~~ return pitch_tuning ( pitch [ ( mag >= threshold ) & pitch_mask ] , 
resolution = resolution , 
~~ def pitch_tuning ( frequencies , resolution = 0.01 , bins_per_octave = 12 ) : 
frequencies = np . atleast_1d ( frequencies ) 
frequencies = frequencies [ frequencies > 0 ] 
if not np . any ( frequencies ) : 
return 0.0 
~~ residual = np . mod ( bins_per_octave * 
time_frequency . hz_to_octs ( frequencies ) , 1.0 ) 
residual [ residual >= 0.5 ] -= 1.0 
bins = np . linspace ( - 0.5 , 0.5 , int ( np . ceil ( 1. / resolution ) ) + 1 ) 
counts , tuning = np . histogram ( residual , bins ) 
return tuning [ np . argmax ( counts ) ] 
~~ def piptrack ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = None , 
fmin = 150.0 , fmax = 4000.0 , threshold = 0.1 , 
win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' , 
ref = None ) : 
S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , 
win_length = win_length , window = window , 
center = center , pad_mode = pad_mode ) 
S = np . abs ( S ) 
fmin = np . maximum ( fmin , 0 ) 
fmax = np . minimum ( fmax , float ( sr ) / 2 ) 
fft_freqs = time_frequency . fft_frequencies ( sr = sr , n_fft = n_fft ) 
avg = 0.5 * ( S [ 2 : ] - S [ : - 2 ] ) 
shift = 2 * S [ 1 : - 1 ] - S [ 2 : ] - S [ : - 2 ] 
shift = avg / ( shift + ( np . abs ( shift ) < util . tiny ( shift ) ) ) 
avg = np . pad ( avg , ( [ 1 , 1 ] , [ 0 , 0 ] ) , mode = 'constant' ) 
shift = np . pad ( shift , ( [ 1 , 1 ] , [ 0 , 0 ] ) , mode = 'constant' ) 
dskew = 0.5 * avg * shift 
pitches = np . zeros_like ( S ) 
mags = np . zeros_like ( S ) 
freq_mask = ( ( fmin <= fft_freqs ) & ( fft_freqs < fmax ) ) . reshape ( ( - 1 , 1 ) ) 
~~~ ref = np . max 
~~ if six . callable ( ref ) : 
~~~ ref_value = threshold * ref ( S , axis = 0 ) 
~~~ ref_value = np . abs ( ref ) 
~~ idx = np . argwhere ( freq_mask & util . localmax ( S * ( S > ref_value ) ) ) 
pitches [ idx [ : , 0 ] , idx [ : , 1 ] ] = ( ( idx [ : , 0 ] + shift [ idx [ : , 0 ] , idx [ : , 1 ] ] ) 
* float ( sr ) / n_fft ) 
mags [ idx [ : , 0 ] , idx [ : , 1 ] ] = ( S [ idx [ : , 0 ] , idx [ : , 1 ] ] 
+ dskew [ idx [ : , 0 ] , idx [ : , 1 ] ] ) 
return pitches , mags 
~~ def hpss ( y , ** kwargs ) : 
stft = core . stft ( y ) 
stft_harm , stft_perc = decompose . hpss ( stft , ** kwargs ) 
y_harm = util . fix_length ( core . istft ( stft_harm , dtype = y . dtype ) , len ( y ) ) 
y_perc = util . fix_length ( core . istft ( stft_perc , dtype = y . dtype ) , len ( y ) ) 
return y_harm , y_perc 
~~ def harmonic ( y , ** kwargs ) : 
stft_harm = decompose . hpss ( stft , ** kwargs ) [ 0 ] 
return y_harm 
~~ def percussive ( y , ** kwargs ) : 
stft_perc = decompose . hpss ( stft , ** kwargs ) [ 1 ] 
return y_perc 
~~ def time_stretch ( y , rate ) : 
if rate <= 0 : 
~~ stft = core . stft ( y ) 
stft_stretch = core . phase_vocoder ( stft , rate ) 
y_stretch = core . istft ( stft_stretch , dtype = y . dtype ) 
return y_stretch 
~~ def pitch_shift ( y , sr , n_steps , bins_per_octave = 12 , res_type = 'kaiser_best' ) : 
if bins_per_octave < 1 or not np . issubdtype ( type ( bins_per_octave ) , np . integer ) : 
~~ rate = 2.0 ** ( - float ( n_steps ) / bins_per_octave ) 
y_shift = core . resample ( time_stretch ( y , rate ) , float ( sr ) / rate , sr , 
res_type = res_type ) 
return util . fix_length ( y_shift , len ( y ) ) 
~~ def remix ( y , intervals , align_zeros = True ) : 
y_out = [ ] 
if align_zeros : 
~~~ y_mono = core . to_mono ( y ) 
zeros = np . nonzero ( core . zero_crossings ( y_mono ) ) [ - 1 ] 
zeros = np . append ( zeros , [ len ( y_mono ) ] ) 
~~ clip = [ slice ( None ) ] * y . ndim 
for interval in intervals : 
~~~ if align_zeros : 
~~~ interval = zeros [ util . match_events ( interval , zeros ) ] 
~~ clip [ - 1 ] = slice ( interval [ 0 ] , interval [ 1 ] ) 
y_out . append ( y [ tuple ( clip ) ] ) 
~~ return np . concatenate ( y_out , axis = - 1 ) 
~~ def _signal_to_frame_nonsilent ( y , frame_length = 2048 , hop_length = 512 , top_db = 60 , 
ref = np . max ) : 
y_mono = core . to_mono ( y ) 
mse = feature . rms ( y = y_mono , 
frame_length = frame_length , 
hop_length = hop_length ) ** 2 
return ( core . power_to_db ( mse . squeeze ( ) , 
top_db = None ) > - top_db ) 
~~ def trim ( y , top_db = 60 , ref = np . max , frame_length = 2048 , hop_length = 512 ) : 
non_silent = _signal_to_frame_nonsilent ( y , 
top_db = top_db ) 
nonzero = np . flatnonzero ( non_silent ) 
if nonzero . size > 0 : 
~~~ start = int ( core . frames_to_samples ( nonzero [ 0 ] , hop_length ) ) 
end = min ( y . shape [ - 1 ] , 
int ( core . frames_to_samples ( nonzero [ - 1 ] + 1 , hop_length ) ) ) 
~~~ start , end = 0 , 0 
~~ full_index = [ slice ( None ) ] * y . ndim 
full_index [ - 1 ] = slice ( start , end ) 
return y [ tuple ( full_index ) ] , np . asarray ( [ start , end ] ) 
~~ def split ( y , top_db = 60 , ref = np . max , frame_length = 2048 , hop_length = 512 ) : 
edges = np . flatnonzero ( np . diff ( non_silent . astype ( int ) ) ) 
edges = [ edges + 1 ] 
if non_silent [ 0 ] : 
~~~ edges . insert ( 0 , [ 0 ] ) 
~~ if non_silent [ - 1 ] : 
~~~ edges . append ( [ len ( non_silent ) ] ) 
~~ edges = core . frames_to_samples ( np . concatenate ( edges ) , 
edges = np . minimum ( edges , y . shape [ - 1 ] ) 
return edges . reshape ( ( - 1 , 2 ) ) 
~~ def stft ( y , n_fft = 2048 , hop_length = None , win_length = None , window = 'hann' , 
center = True , dtype = np . complex64 , pad_mode = 'reflect' ) : 
if win_length is None : 
~~~ win_length = n_fft 
~~ if hop_length is None : 
~~~ hop_length = int ( win_length // 4 ) 
~~ fft_window = get_window ( window , win_length , fftbins = True ) 
fft_window = util . pad_center ( fft_window , n_fft ) 
fft_window = fft_window . reshape ( ( - 1 , 1 ) ) 
util . valid_audio ( y ) 
~~~ y = np . pad ( y , int ( n_fft // 2 ) , mode = pad_mode ) 
~~ y_frames = util . frame ( y , frame_length = n_fft , hop_length = hop_length ) 
stft_matrix = np . empty ( ( int ( 1 + n_fft // 2 ) , y_frames . shape [ 1 ] ) , 
dtype = dtype , 
order = 'F' ) 
n_columns = int ( util . MAX_MEM_BLOCK / ( stft_matrix . shape [ 0 ] * 
stft_matrix . itemsize ) ) 
for bl_s in range ( 0 , stft_matrix . shape [ 1 ] , n_columns ) : 
~~~ bl_t = min ( bl_s + n_columns , stft_matrix . shape [ 1 ] ) 
stft_matrix [ : , bl_s : bl_t ] = fft . rfft ( fft_window * 
y_frames [ : , bl_s : bl_t ] , 
axis = 0 ) 
~~ return stft_matrix 
~~ def istft ( stft_matrix , hop_length = None , win_length = None , window = 'hann' , 
center = True , dtype = np . float32 , length = None ) : 
n_fft = 2 * ( stft_matrix . shape [ 0 ] - 1 ) 
~~ ifft_window = get_window ( window , win_length , fftbins = True ) 
ifft_window = util . pad_center ( ifft_window , n_fft ) [ : , np . newaxis ] 
n_frames = stft_matrix . shape [ 1 ] 
expected_signal_len = n_fft + hop_length * ( n_frames - 1 ) 
y = np . zeros ( expected_signal_len , dtype = dtype ) 
n_columns = int ( util . MAX_MEM_BLOCK // ( stft_matrix . shape [ 0 ] * 
frame = 0 
for bl_s in range ( 0 , n_frames , n_columns ) : 
~~~ bl_t = min ( bl_s + n_columns , n_frames ) 
ytmp = ifft_window * fft . irfft ( stft_matrix [ : , bl_s : bl_t ] , axis = 0 ) 
__overlap_add ( y [ frame * hop_length : ] , ytmp , hop_length ) 
frame += ( bl_t - bl_s ) 
~~ ifft_window_sum = window_sumsquare ( window , 
n_frames , 
win_length = win_length , 
n_fft = n_fft , 
dtype = dtype ) 
approx_nonzero_indices = ifft_window_sum > util . tiny ( ifft_window_sum ) 
y [ approx_nonzero_indices ] /= ifft_window_sum [ approx_nonzero_indices ] 
if length is None : 
~~~ if center : 
~~~ y = y [ int ( n_fft // 2 ) : - int ( n_fft // 2 ) ] 
~~~ start = int ( n_fft // 2 ) 
~~~ start = 0 
~~ y = util . fix_length ( y [ start : ] , length ) 
~~ def ifgram ( y , sr = 22050 , n_fft = 2048 , hop_length = None , win_length = None , 
window = 'hann' , norm = False , center = True , ref_power = 1e-6 , 
clip = True , dtype = np . complex64 , pad_mode = 'reflect' ) : 
~~ fft_window = util . pad_center ( get_window ( window , win_length , 
fftbins = True ) , 
n_fft ) 
freq_angular = np . linspace ( 0 , 2 * np . pi , n_fft , endpoint = False ) 
d_window = np . sin ( - freq_angular ) * np . pi / n_fft 
stft_matrix = stft ( y , n_fft = n_fft , hop_length = hop_length , 
window = window , center = center , 
dtype = dtype , pad_mode = pad_mode ) 
diff_stft = stft ( y , n_fft = n_fft , hop_length = hop_length , 
window = d_window , center = center , 
dtype = dtype , pad_mode = pad_mode ) . conj ( ) 
mag , phase = magphase ( stft_matrix ) 
if six . callable ( ref_power ) : 
~~~ ref_power = ref_power ( mag ** 2 ) 
~~ elif ref_power < 0 : 
~~ freq_angular = freq_angular . reshape ( ( - 1 , 1 ) ) 
bin_offset = ( - phase * diff_stft ) . imag / mag 
bin_offset [ mag < ref_power ** 0.5 ] = 0 
if_gram = freq_angular [ : n_fft // 2 + 1 ] + bin_offset 
if norm : 
~~~ stft_matrix = stft_matrix * 2.0 / fft_window . sum ( ) 
~~ if clip : 
~~~ np . clip ( if_gram , 0 , np . pi , out = if_gram ) 
~~ if_gram *= float ( sr ) * 0.5 / np . pi 
return if_gram , stft_matrix 
~~ def magphase ( D , power = 1 ) : 
mag = np . abs ( D ) 
mag **= power 
phase = np . exp ( 1.j * np . angle ( D ) ) 
return mag , phase 
~~ def phase_vocoder ( D , rate , hop_length = None ) : 
n_fft = 2 * ( D . shape [ 0 ] - 1 ) 
if hop_length is None : 
~~~ hop_length = int ( n_fft // 4 ) 
~~ time_steps = np . arange ( 0 , D . shape [ 1 ] , rate , dtype = np . float ) 
d_stretch = np . zeros ( ( D . shape [ 0 ] , len ( time_steps ) ) , D . dtype , order = 'F' ) 
phi_advance = np . linspace ( 0 , np . pi * hop_length , D . shape [ 0 ] ) 
phase_acc = np . angle ( D [ : , 0 ] ) 
D = np . pad ( D , [ ( 0 , 0 ) , ( 0 , 2 ) ] , mode = 'constant' ) 
for ( t , step ) in enumerate ( time_steps ) : 
~~~ columns = D [ : , int ( step ) : int ( step + 2 ) ] 
alpha = np . mod ( step , 1.0 ) 
mag = ( ( 1.0 - alpha ) * np . abs ( columns [ : , 0 ] ) 
+ alpha * np . abs ( columns [ : , 1 ] ) ) 
d_stretch [ : , t ] = mag * np . exp ( 1.j * phase_acc ) 
dphase = ( np . angle ( columns [ : , 1 ] ) 
- np . angle ( columns [ : , 0 ] ) 
- phi_advance ) 
dphase = dphase - 2.0 * np . pi * np . round ( dphase / ( 2.0 * np . pi ) ) 
phase_acc += phi_advance + dphase 
~~ return d_stretch 
~~ def iirt ( y , sr = 22050 , win_length = 2048 , hop_length = None , center = True , 
tuning = 0.0 , pad_mode = 'reflect' , flayout = None , ** kwargs ) : 
~~~ r\ 
if flayout is None : 
FutureWarning ) 
flayout = 'ba' 
~~ elif flayout not in ( 'ba' , 'sos' ) : 
~~ util . valid_audio ( y ) 
~~~ y = np . pad ( y , int ( hop_length ) , mode = pad_mode ) 
~~ filterbank_ct , sample_rates = semitone_filterbank ( tuning = tuning , flayout = flayout , ** kwargs ) 
y_resampled = [ ] 
y_srs = np . unique ( sample_rates ) 
for cur_sr in y_srs : 
~~~ y_resampled . append ( resample ( y , sr , cur_sr ) ) 
~~ n_frames = 1 + int ( ( len ( y ) - win_length ) / float ( hop_length ) ) 
bands_power = [ ] 
for cur_sr , cur_filter in zip ( sample_rates , filterbank_ct ) : 
~~~ factor = float ( sr ) / float ( cur_sr ) 
win_length_STMSP = int ( np . round ( win_length / factor ) ) 
hop_length_STMSP = int ( np . round ( hop_length / factor ) ) 
cur_sr_idx = np . flatnonzero ( y_srs == cur_sr ) [ 0 ] 
if flayout == 'ba' : 
~~~ cur_filter_output = scipy . signal . filtfilt ( cur_filter [ 0 ] , cur_filter [ 1 ] , 
y_resampled [ cur_sr_idx ] ) 
~~ elif flayout == 'sos' : 
~~~ cur_filter_output = scipy . signal . sosfiltfilt ( cur_filter , 
~~ cur_frames = util . frame ( np . ascontiguousarray ( cur_filter_output ) , 
frame_length = win_length_STMSP , 
hop_length = hop_length_STMSP ) 
bands_power . append ( factor * np . sum ( cur_frames ** 2 , axis = 0 ) [ : n_frames ] ) 
~~ return np . asarray ( bands_power ) 
~~ def power_to_db ( S , ref = 1.0 , amin = 1e-10 , top_db = 80.0 ) : 
S = np . asarray ( S ) 
if amin <= 0 : 
~~ if np . issubdtype ( S . dtype , np . complexfloating ) : 
magnitude = np . abs ( S ) 
~~~ magnitude = S 
~~~ ref_value = ref ( magnitude ) 
~~ log_spec = 10.0 * np . log10 ( np . maximum ( amin , magnitude ) ) 
log_spec -= 10.0 * np . log10 ( np . maximum ( amin , ref_value ) ) 
if top_db is not None : 
~~~ if top_db < 0 : 
~~ log_spec = np . maximum ( log_spec , log_spec . max ( ) - top_db ) 
~~ return log_spec 
~~ def amplitude_to_db ( S , ref = 1.0 , amin = 1e-5 , top_db = 80.0 ) : 
if np . issubdtype ( S . dtype , np . complexfloating ) : 
~~ magnitude = np . abs ( S ) 
if six . callable ( ref ) : 
~~ power = np . square ( magnitude , out = magnitude ) 
return power_to_db ( power , ref = ref_value ** 2 , amin = amin ** 2 , 
~~ def perceptual_weighting ( S , frequencies , ** kwargs ) : 
offset = time_frequency . A_weighting ( frequencies ) . reshape ( ( - 1 , 1 ) ) 
return offset + power_to_db ( S , ** kwargs ) 
~~ def fmt ( y , t_min = 0.5 , n_fmt = None , kind = 'cubic' , beta = 0.5 , over_sample = 1 , axis = - 1 ) : 
n = y . shape [ axis ] 
if n < 3 : 
~~ if t_min <= 0 : 
~~ if n_fmt is None : 
~~~ if over_sample < 1 : 
~~ log_base = np . log ( n - 1 ) - np . log ( n - 2 ) 
n_fmt = int ( np . ceil ( over_sample * ( np . log ( n - 1 ) - np . log ( t_min ) ) / log_base ) ) 
~~ elif n_fmt < 3 : 
~~~ log_base = ( np . log ( n_fmt - 1 ) - np . log ( n_fmt - 2 ) ) / over_sample 
~~ if not np . all ( np . isfinite ( y ) ) : 
~~ base = np . exp ( log_base ) 
x = np . linspace ( 0 , 1 , num = n , endpoint = False ) 
f_interp = scipy . interpolate . interp1d ( x , y , kind = kind , axis = axis ) 
n_over = int ( np . ceil ( over_sample ) ) 
x_exp = np . logspace ( ( np . log ( t_min ) - np . log ( n ) ) / log_base , 
0 , 
num = n_fmt + n_over , 
endpoint = False , 
base = base ) [ : - n_over ] 
if x_exp [ 0 ] < t_min or x_exp [ - 1 ] > float ( n - 1.0 ) / n : 
~~~ x_exp = np . clip ( x_exp , float ( t_min ) / n , x [ - 1 ] ) 
~~ if len ( np . unique ( x_exp ) ) != len ( x_exp ) : 
~~ y_res = f_interp ( x_exp ) 
shape = [ 1 ] * y_res . ndim 
shape [ axis ] = - 1 
return fft . rfft ( y_res * ( ( x_exp ** beta ) . reshape ( shape ) * np . sqrt ( n ) / n_fmt ) , 
axis = axis ) 
~~ def pcen ( S , sr = 22050 , hop_length = 512 , gain = 0.98 , bias = 2 , power = 0.5 , 
time_constant = 0.400 , eps = 1e-6 , b = None , max_size = 1 , ref = None , 
axis = - 1 , max_axis = None , zi = None , return_zf = False ) : 
if power <= 0 : 
~~ if gain < 0 : 
~~ if bias < 0 : 
~~ if eps <= 0 : 
~~ if time_constant <= 0 : 
~~ if b is None : 
~~~ t_frames = time_constant * sr / float ( hop_length ) 
b = ( np . sqrt ( 1 + 4 * t_frames ** 2 ) - 1 ) / ( 2 * t_frames ** 2 ) 
~~ if not 0 <= b <= 1 : 
~~ if ref is None : 
~~ elif S . ndim == 1 : 
~~~ if max_axis is None : 
~~~ if S . ndim != 2 : 
~~ max_axis = np . mod ( 1 - axis , 2 ) 
~~ ref = scipy . ndimage . maximum_filter1d ( S , max_size , axis = max_axis ) 
~~ ~~ if zi is None : 
~~~ shape = tuple ( [ 1 ] * ref . ndim ) 
zi = np . empty ( shape ) 
zi [ : ] = scipy . signal . lfilter_zi ( [ b ] , [ 1 , b - 1 ] ) [ : ] 
~~ S_smooth , zf = scipy . signal . lfilter ( [ b ] , [ 1 , b - 1 ] , ref , zi = zi , 
smooth = np . exp ( - gain * ( np . log ( eps ) + np . log1p ( S_smooth / eps ) ) ) 
S_out = ( S * smooth + bias ) ** power - bias ** power 
if return_zf : 
~~~ return S_out , zf 
~~~ return S_out 
~~ ~~ def _spectrogram ( y = None , S = None , n_fft = 2048 , hop_length = 512 , power = 1 , 
win_length = None , window = 'hann' , center = True , pad_mode = 'reflect' ) : 
if S is not None : 
~~~ n_fft = 2 * ( S . shape [ 0 ] - 1 ) 
~~~ S = np . abs ( stft ( y , n_fft = n_fft , hop_length = hop_length , 
win_length = win_length , center = center , 
window = window , pad_mode = pad_mode ) ) ** power 
~~ return S , n_fft 
~~ def hpss_beats ( input_file , output_csv ) : 
y = librosa . effects . percussive ( y ) 
onset_env = librosa . onset . onset_strength ( y = y , 
hop_length = HOP_LENGTH , 
n_fft = N_FFT , 
aggregate = np . median ) 
tempo , beats = librosa . beat . beat_track ( onset_envelope = onset_env , 
hop_length = HOP_LENGTH ) 
beat_times = librosa . frames_to_time ( beats , 
~~ def decompose ( S , n_components = None , transformer = None , sort = False , fit = True , ** kwargs ) : 
if transformer is None : 
~~~ if fit is False : 
~~ transformer = sklearn . decomposition . NMF ( n_components = n_components , 
~~ if n_components is None : 
~~~ n_components = S . shape [ 0 ] 
~~ if fit : 
~~~ activations = transformer . fit_transform ( S . T ) . T 
~~~ activations = transformer . transform ( S . T ) . T 
~~ components = transformer . components_ . T 
if sort : 
~~~ components , idx = util . axis_sort ( components , index = True ) 
activations = activations [ idx ] 
~~ return components , activations 
~~ def hpss ( S , kernel_size = 31 , power = 2.0 , mask = False , margin = 1.0 ) : 
if np . iscomplexobj ( S ) : 
~~~ S , phase = core . magphase ( S ) 
~~~ phase = 1 
~~ if np . isscalar ( kernel_size ) : 
~~~ win_harm = kernel_size 
win_perc = kernel_size 
~~~ win_harm = kernel_size [ 0 ] 
win_perc = kernel_size [ 1 ] 
~~ if np . isscalar ( margin ) : 
~~~ margin_harm = margin 
margin_perc = margin 
~~~ margin_harm = margin [ 0 ] 
margin_perc = margin [ 1 ] 
~~ if margin_harm < 1 or margin_perc < 1 : 
~~ harm = np . empty_like ( S ) 
harm [ : ] = median_filter ( S , size = ( 1 , win_harm ) , mode = 'reflect' ) 
perc = np . empty_like ( S ) 
perc [ : ] = median_filter ( S , size = ( win_perc , 1 ) , mode = 'reflect' ) 
split_zeros = ( margin_harm == 1 and margin_perc == 1 ) 
mask_harm = util . softmask ( harm , perc * margin_harm , 
power = power , 
split_zeros = split_zeros ) 
mask_perc = util . softmask ( perc , harm * margin_perc , 
if mask : 
~~~ return mask_harm , mask_perc 
~~ return ( ( S * mask_harm ) * phase , ( S * mask_perc ) * phase ) 
~~ def nn_filter ( S , rec = None , aggregate = None , axis = - 1 , ** kwargs ) : 
if aggregate is None : 
~~ if rec is None : 
~~~ kwargs = dict ( kwargs ) 
kwargs [ 'sparse' ] = True 
rec = segment . recurrence_matrix ( S , axis = axis , ** kwargs ) 
~~ elif not scipy . sparse . issparse ( rec ) : 
~~~ rec = scipy . sparse . csr_matrix ( rec ) 
~~ if rec . shape [ 0 ] != S . shape [ axis ] or rec . shape [ 0 ] != rec . shape [ 1 ] : 
S . shape ) ) 
~~ return __nn_filter_helper ( rec . data , rec . indices , rec . indptr , 
S . swapaxes ( 0 , axis ) , aggregate ) . swapaxes ( 0 , axis ) 
~~ def __nn_filter_helper ( R_data , R_indices , R_ptr , S , aggregate ) : 
s_out = np . empty_like ( S ) 
for i in range ( len ( R_ptr ) - 1 ) : 
~~~ targets = R_indices [ R_ptr [ i ] : R_ptr [ i + 1 ] ] 
if not len ( targets ) : 
~~~ s_out [ i ] = S [ i ] 
~~ neighbors = np . take ( S , targets , axis = 0 ) 
if aggregate is np . average : 
~~~ weights = R_data [ R_ptr [ i ] : R_ptr [ i + 1 ] ] 
s_out [ i ] = aggregate ( neighbors , axis = 0 , weights = weights ) 
~~~ s_out [ i ] = aggregate ( neighbors , axis = 0 ) 
~~ ~~ return s_out 
~~ def mel ( sr , n_fft , n_mels = 128 , fmin = 0.0 , fmax = None , htk = False , 
norm = 1 , dtype = np . float32 ) : 
if fmax is None : 
~~~ fmax = float ( sr ) / 2 
~~ if norm is not None and norm != 1 and norm != np . inf : 
~~ n_mels = int ( n_mels ) 
weights = np . zeros ( ( n_mels , int ( 1 + n_fft // 2 ) ) , dtype = dtype ) 
fftfreqs = fft_frequencies ( sr = sr , n_fft = n_fft ) 
mel_f = mel_frequencies ( n_mels + 2 , fmin = fmin , fmax = fmax , htk = htk ) 
fdiff = np . diff ( mel_f ) 
ramps = np . subtract . outer ( mel_f , fftfreqs ) 
for i in range ( n_mels ) : 
~~~ lower = - ramps [ i ] / fdiff [ i ] 
upper = ramps [ i + 2 ] / fdiff [ i + 1 ] 
weights [ i ] = np . maximum ( 0 , np . minimum ( lower , upper ) ) 
~~ if norm == 1 : 
~~~ enorm = 2.0 / ( mel_f [ 2 : n_mels + 2 ] - mel_f [ : n_mels ] ) 
weights *= enorm [ : , np . newaxis ] 
~~ if not np . all ( ( mel_f [ : - 2 ] == 0 ) | ( weights . max ( axis = 1 ) > 0 ) ) : 
~~ def chroma ( sr , n_fft , n_chroma = 12 , A440 = 440.0 , ctroct = 5.0 , 
octwidth = 2 , norm = 2 , base_c = True , dtype = np . float32 ) : 
wts = np . zeros ( ( n_chroma , n_fft ) ) 
frequencies = np . linspace ( 0 , sr , n_fft , endpoint = False ) [ 1 : ] 
frqbins = n_chroma * hz_to_octs ( frequencies , A440 ) 
frqbins = np . concatenate ( ( [ frqbins [ 0 ] - 1.5 * n_chroma ] , frqbins ) ) 
binwidthbins = np . concatenate ( ( np . maximum ( frqbins [ 1 : ] - frqbins [ : - 1 ] , 
1.0 ) , [ 1 ] ) ) 
D = np . subtract . outer ( frqbins , np . arange ( 0 , n_chroma , dtype = 'd' ) ) . T 
n_chroma2 = np . round ( float ( n_chroma ) / 2 ) 
D = np . remainder ( D + n_chroma2 + 10 * n_chroma , n_chroma ) - n_chroma2 
wts = np . exp ( - 0.5 * ( 2 * D / np . tile ( binwidthbins , ( n_chroma , 1 ) ) ) ** 2 ) 
wts = util . normalize ( wts , norm = norm , axis = 0 ) 
if octwidth is not None : 
~~~ wts *= np . tile ( 
np . exp ( - 0.5 * ( ( ( frqbins / n_chroma - ctroct ) / octwidth ) ** 2 ) ) , 
( n_chroma , 1 ) ) 
~~ if base_c : 
~~~ wts = np . roll ( wts , - 3 , axis = 0 ) 
~~ return np . ascontiguousarray ( wts [ : , : int ( 1 + n_fft / 2 ) ] , dtype = dtype ) 
~~ def __float_window ( window_spec ) : 
def _wrap ( n , * args , ** kwargs ) : 
n_min , n_max = int ( np . floor ( n ) ) , int ( np . ceil ( n ) ) 
window = get_window ( window_spec , n_min ) 
if len ( window ) < n_max : 
~~~ window = np . pad ( window , [ ( 0 , n_max - len ( window ) ) ] , 
~~ window [ n_min : ] = 0.0 
return window 
~~ return _wrap 
~~ def constant_q ( sr , fmin = None , n_bins = 84 , bins_per_octave = 12 , tuning = 0.0 , 
window = 'hann' , filter_scale = 1 , pad_fft = True , norm = 1 , 
dtype = np . complex64 , ** kwargs ) : 
~~ lengths = constant_q_lengths ( sr , fmin , 
fmin = correction * fmin 
freqs = Q * sr / lengths 
filters = [ ] 
for ilen , freq in zip ( lengths , freqs ) : 
~~~ sig = np . exp ( np . arange ( - ilen // 2 , ilen // 2 , dtype = float ) * 1j * 2 * np . pi * freq / sr ) 
sig = sig * __float_window ( window ) ( len ( sig ) ) 
sig = util . normalize ( sig , norm = norm ) 
filters . append ( sig ) 
~~ max_len = max ( lengths ) 
if pad_fft : 
~~~ max_len = int ( 2.0 ** ( np . ceil ( np . log2 ( max_len ) ) ) ) 
~~~ max_len = int ( np . ceil ( max_len ) ) 
~~ filters = np . asarray ( [ util . pad_center ( filt , max_len , ** kwargs ) 
for filt in filters ] , dtype = dtype ) 
return filters , np . asarray ( lengths ) 
~~ def constant_q_lengths ( sr , fmin , n_bins = 84 , bins_per_octave = 12 , 
tuning = 0.0 , window = 'hann' , filter_scale = 1 ) : 
if fmin <= 0 : 
~~ if bins_per_octave <= 0 : 
~~ if filter_scale <= 0 : 
~~ if n_bins <= 0 or not isinstance ( n_bins , int ) : 
~~ correction = 2.0 ** ( float ( tuning ) / bins_per_octave ) 
freq = fmin * ( 2.0 ** ( np . arange ( n_bins , dtype = float ) / bins_per_octave ) ) 
if freq [ - 1 ] * ( 1 + 0.5 * window_bandwidth ( window ) / Q ) > sr / 2.0 : 
~~ lengths = Q * sr / freq 
return lengths 
~~ def cq_to_chroma ( n_input , bins_per_octave = 12 , n_chroma = 12 , 
fmin = None , window = None , base_c = True , dtype = np . float32 ) : 
n_merge = float ( bins_per_octave ) / n_chroma 
~~ if np . mod ( n_merge , 1 ) != 0 : 
~~ cq_to_ch = np . repeat ( np . eye ( n_chroma ) , n_merge , axis = 1 ) 
cq_to_ch = np . roll ( cq_to_ch , - int ( n_merge // 2 ) , axis = 1 ) 
n_octaves = np . ceil ( np . float ( n_input ) / bins_per_octave ) 
cq_to_ch = np . tile ( cq_to_ch , int ( n_octaves ) ) [ : , : n_input ] 
midi_0 = np . mod ( hz_to_midi ( fmin ) , 12 ) 
if base_c : 
~~~ roll = midi_0 
~~~ roll = midi_0 - 9 
~~ roll = int ( np . round ( roll * ( n_chroma / 12. ) ) ) 
cq_to_ch = np . roll ( cq_to_ch , roll , axis = 0 ) . astype ( dtype ) 
if window is not None : 
~~~ cq_to_ch = scipy . signal . convolve ( cq_to_ch , 
np . atleast_2d ( window ) , 
mode = 'same' ) 
~~ return cq_to_ch 
~~ def window_bandwidth ( window , n = 1000 ) : 
if hasattr ( window , '__name__' ) : 
~~~ key = window . __name__ 
~~~ key = window 
~~ if key not in WINDOW_BANDWIDTHS : 
~~~ win = get_window ( window , n ) 
WINDOW_BANDWIDTHS [ key ] = n * np . sum ( win ** 2 ) / np . sum ( np . abs ( win ) ) ** 2 
~~ return WINDOW_BANDWIDTHS [ key ] 
~~ def get_window ( window , Nx , fftbins = True ) : 
if six . callable ( window ) : 
~~~ return window ( Nx ) 
~~ elif ( isinstance ( window , ( six . string_types , tuple ) ) or 
np . isscalar ( window ) ) : 
~~~ return scipy . signal . get_window ( window , Nx , fftbins = fftbins ) 
~~ elif isinstance ( window , ( np . ndarray , list ) ) : 
~~~ if len ( window ) == Nx : 
~~~ return np . asarray ( window ) 
~~ ~~ def _multirate_fb ( center_freqs = None , sample_rates = None , Q = 25.0 , 
passband_ripple = 1 , stopband_attenuation = 50 , ftype = 'ellip' , flayout = 'ba' ) : 
if center_freqs is None : 
~~ if sample_rates is None : 
~~ if center_freqs . shape != sample_rates . shape : 
~~ nyquist = 0.5 * sample_rates 
filter_bandwidths = center_freqs / float ( Q ) 
filterbank = [ ] 
for cur_center_freq , cur_nyquist , cur_bw in zip ( center_freqs , nyquist , filter_bandwidths ) : 
~~~ passband_freqs = [ cur_center_freq - 0.5 * cur_bw , cur_center_freq + 0.5 * cur_bw ] / cur_nyquist 
stopband_freqs = [ cur_center_freq - cur_bw , cur_center_freq + cur_bw ] / cur_nyquist 
cur_filter = scipy . signal . iirdesign ( passband_freqs , stopband_freqs , 
passband_ripple , stopband_attenuation , 
analog = False , ftype = ftype , output = flayout ) 
filterbank . append ( cur_filter ) 
~~ return filterbank , sample_rates 
~~ def mr_frequencies ( tuning ) : 
center_freqs = midi_to_hz ( np . arange ( 24 + tuning , 109 + tuning ) ) 
sample_rates = np . asarray ( len ( np . arange ( 0 , 36 ) ) * [ 882 , ] + 
len ( np . arange ( 36 , 70 ) ) * [ 4410 , ] + 
len ( np . arange ( 70 , 85 ) ) * [ 22050 , ] ) 
return center_freqs , sample_rates 
~~ def semitone_filterbank ( center_freqs = None , tuning = 0.0 , sample_rates = None , flayout = 'ba' , ** kwargs ) : 
if ( center_freqs is None ) and ( sample_rates is None ) : 
~~~ center_freqs , sample_rates = mr_frequencies ( tuning ) 
~~ filterbank , fb_sample_rates = _multirate_fb ( center_freqs = center_freqs , sample_rates = sample_rates , 
flayout = flayout , ** kwargs ) 
return filterbank , fb_sample_rates 
n = len ( x ) 
n_fft = len ( win_sq ) 
for i in range ( n_frames ) : 
~~~ sample = i * hop_length 
x [ sample : min ( n , sample + n_fft ) ] += win_sq [ : max ( 0 , min ( n_fft , n - sample ) ) ] 
~~ ~~ def window_sumsquare ( window , n_frames , hop_length = 512 , win_length = None , n_fft = 2048 , 
dtype = np . float32 , norm = None ) : 
~~ n = n_fft + hop_length * ( n_frames - 1 ) 
x = np . zeros ( n , dtype = dtype ) 
win_sq = get_window ( window , win_length ) 
win_sq = util . normalize ( win_sq , norm = norm ) ** 2 
win_sq = util . pad_center ( win_sq , n_fft ) 
__window_ss_fill ( x , win_sq , n_frames , hop_length ) 
~~ def diagonal_filter ( window , n , slope = 1.0 , angle = None , zero_mean = False ) : 
if angle is None : 
~~~ angle = np . arctan ( slope ) 
~~ win = np . diag ( get_window ( window , n , fftbins = False ) ) 
if not np . isclose ( angle , np . pi / 4 ) : 
~~~ win = scipy . ndimage . rotate ( win , 45 - angle * 180 / np . pi , 
order = 5 , prefilter = False ) 
~~ np . clip ( win , 0 , None , out = win ) 
win /= win . sum ( ) 
if zero_mean : 
~~~ win -= win . mean ( ) 
~~ return win 
~~ def spectral_centroid ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = 512 , 
freq = None , win_length = None , window = 'hann' , center = True , 
win_length = win_length , window = window , center = center , 
pad_mode = pad_mode ) 
if not np . isrealobj ( S ) : 
~~ elif np . any ( S < 0 ) : 
~~ if freq is None : 
~~~ freq = fft_frequencies ( sr = sr , n_fft = n_fft ) 
~~ if freq . ndim == 1 : 
~~~ freq = freq . reshape ( ( - 1 , 1 ) ) 
~~ return np . sum ( freq * util . normalize ( S , norm = 1 , axis = 0 ) , 
axis = 0 , keepdims = True ) 
~~ def spectral_bandwidth ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = 512 , 
freq = None , centroid = None , norm = True , p = 2 ) : 
~~ if centroid is None : 
~~~ centroid = spectral_centroid ( y = y , sr = sr , S = S , 
freq = freq ) 
~~~ deviation = np . abs ( np . subtract . outer ( freq , centroid [ 0 ] ) ) 
~~~ deviation = np . abs ( freq - centroid [ 0 ] ) 
~~ if norm : 
~~~ S = util . normalize ( S , norm = 1 , axis = 0 ) 
~~ return np . sum ( S * deviation ** p , axis = 0 , keepdims = True ) ** ( 1. / p ) 
~~ def spectral_contrast ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = 512 , 
freq = None , fmin = 200.0 , n_bands = 6 , quantile = 0.02 , 
linear = False ) : 
if freq is None : 
~~ freq = np . atleast_1d ( freq ) 
if freq . ndim != 1 or len ( freq ) != S . shape [ 0 ] : 
'({:d},)' . format ( S . shape [ 0 ] ) ) 
~~ if n_bands < 1 or not isinstance ( n_bands , int ) : 
~~ if not 0.0 < quantile < 1.0 : 
~~ if fmin <= 0 : 
~~ octa = np . zeros ( n_bands + 2 ) 
octa [ 1 : ] = fmin * ( 2.0 ** np . arange ( 0 , n_bands + 1 ) ) 
if np . any ( octa [ : - 1 ] >= 0.5 * sr ) : 
~~ valley = np . zeros ( ( n_bands + 1 , S . shape [ 1 ] ) ) 
peak = np . zeros_like ( valley ) 
for k , ( f_low , f_high ) in enumerate ( zip ( octa [ : - 1 ] , octa [ 1 : ] ) ) : 
~~~ current_band = np . logical_and ( freq >= f_low , freq <= f_high ) 
idx = np . flatnonzero ( current_band ) 
if k > 0 : 
~~~ current_band [ idx [ 0 ] - 1 ] = True 
~~ if k == n_bands : 
~~~ current_band [ idx [ - 1 ] + 1 : ] = True 
~~ sub_band = S [ current_band ] 
if k < n_bands : 
~~~ sub_band = sub_band [ : - 1 ] 
~~ idx = np . rint ( quantile * np . sum ( current_band ) ) 
idx = int ( np . maximum ( idx , 1 ) ) 
sortedr = np . sort ( sub_band , axis = 0 ) 
valley [ k ] = np . mean ( sortedr [ : idx ] , axis = 0 ) 
peak [ k ] = np . mean ( sortedr [ - idx : ] , axis = 0 ) 
~~ if linear : 
~~~ return peak - valley 
~~~ return power_to_db ( peak ) - power_to_db ( valley ) 
~~ ~~ def spectral_rolloff ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = 512 , 
freq = None , roll_percent = 0.85 ) : 
if not 0.0 < roll_percent < 1.0 : 
~~ S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , 
~~ total_energy = np . cumsum ( S , axis = 0 ) 
threshold = roll_percent * total_energy [ - 1 ] 
ind = np . where ( total_energy < threshold , np . nan , 1 ) 
return np . nanmin ( ind * freq , axis = 0 , keepdims = True ) 
~~ def spectral_flatness ( y = None , S = None , n_fft = 2048 , hop_length = 512 , 
amin = 1e-10 , power = 2.0 ) : 
power = 1. , win_length = win_length , window = window , 
~~ S_thresh = np . maximum ( amin , S ** power ) 
gmean = np . exp ( np . mean ( np . log ( S_thresh ) , axis = 0 , keepdims = True ) ) 
amean = np . mean ( S_thresh , axis = 0 , keepdims = True ) 
return gmean / amean 
~~ def rms ( y = None , S = None , frame_length = 2048 , hop_length = 512 , 
center = True , pad_mode = 'reflect' ) : 
if y is not None and S is not None : 
~~ if y is not None : 
~~~ y = to_mono ( y ) 
~~~ y = np . pad ( y , int ( frame_length // 2 ) , mode = pad_mode ) 
~~ x = util . frame ( y , 
~~ elif S is not None : 
~~~ x , _ = _spectrogram ( y = y , S = S , 
n_fft = frame_length , 
~~ return np . sqrt ( np . mean ( np . abs ( x ) ** 2 , axis = 0 , keepdims = True ) ) 
~~ def poly_features ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = 512 , 
order = 1 , freq = None ) : 
~~~ coefficients = np . polyfit ( freq , S , order ) 
~~~ coefficients = np . concatenate ( [ [ np . polyfit ( freq [ : , i ] , S [ : , i ] , order ) ] 
for i in range ( S . shape [ 1 ] ) ] , axis = 0 ) . T 
~~ return coefficients 
~~ def zero_crossing_rate ( y , frame_length = 2048 , hop_length = 512 , center = True , 
~~~ y = np . pad ( y , int ( frame_length // 2 ) , mode = 'edge' ) 
~~ y_framed = util . frame ( y , frame_length , hop_length ) 
kwargs [ 'axis' ] = 0 
kwargs . setdefault ( 'pad' , False ) 
crossings = zero_crossings ( y_framed , ** kwargs ) 
return np . mean ( crossings , axis = 0 , keepdims = True ) 
~~ def chroma_stft ( y = None , sr = 22050 , S = None , norm = np . inf , n_fft = 2048 , 
hop_length = 512 , win_length = None , window = 'hann' , center = True , 
pad_mode = 'reflect' , tuning = None , ** kwargs ) : 
S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , power = 2 , 
n_chroma = kwargs . get ( 'n_chroma' , 12 ) 
if tuning is None : 
~~~ tuning = estimate_tuning ( S = S , sr = sr , bins_per_octave = n_chroma ) 
~~ if 'A440' not in kwargs : 
~~~ kwargs [ 'A440' ] = 440.0 * 2.0 ** ( float ( tuning ) / n_chroma ) 
~~ chromafb = filters . chroma ( sr , n_fft , ** kwargs ) 
raw_chroma = np . dot ( chromafb , S ) 
return util . normalize ( raw_chroma , norm = norm , axis = 0 ) 
~~ def chroma_cqt ( y = None , sr = 22050 , C = None , hop_length = 512 , fmin = None , 
norm = np . inf , threshold = 0.0 , tuning = None , n_chroma = 12 , 
n_octaves = 7 , window = None , bins_per_octave = None , cqt_mode = 'full' ) : 
cqt_func = { 'full' : cqt , 'hybrid' : hybrid_cqt } 
if bins_per_octave is None : 
~~~ bins_per_octave = n_chroma 
~~~ C = np . abs ( cqt_func [ cqt_mode ] ( y , sr = sr , 
n_bins = n_octaves * bins_per_octave , 
tuning = tuning ) ) 
~~ cq_to_chr = filters . cq_to_chroma ( C . shape [ 0 ] , 
n_chroma = n_chroma , 
chroma = cq_to_chr . dot ( C ) 
if threshold is not None : 
~~~ chroma [ chroma < threshold ] = 0.0 
~~ if norm is not None : 
~~~ chroma = util . normalize ( chroma , norm = norm , axis = 0 ) 
~~ return chroma 
~~ def chroma_cens ( y = None , sr = 22050 , C = None , hop_length = 512 , fmin = None , 
tuning = None , n_chroma = 12 , 
n_octaves = 7 , bins_per_octave = None , cqt_mode = 'full' , window = None , 
norm = 2 , win_len_smooth = 41 , smoothing_window = 'hann' ) : 
if not ( ( win_len_smooth is None ) or ( isinstance ( win_len_smooth , int ) and win_len_smooth > 0 ) ) : 
~~ chroma = chroma_cqt ( y = y , C = C , sr = sr , 
norm = None , 
n_octaves = n_octaves , 
cqt_mode = cqt_mode , 
chroma = util . normalize ( chroma , norm = 1 , axis = 0 ) 
QUANT_STEPS = [ 0.4 , 0.2 , 0.1 , 0.05 ] 
QUANT_WEIGHTS = [ 0.25 , 0.25 , 0.25 , 0.25 ] 
chroma_quant = np . zeros_like ( chroma ) 
for cur_quant_step_idx , cur_quant_step in enumerate ( QUANT_STEPS ) : 
~~~ chroma_quant += ( chroma > cur_quant_step ) * QUANT_WEIGHTS [ cur_quant_step_idx ] 
~~ if win_len_smooth : 
~~~ win = filters . get_window ( smoothing_window , win_len_smooth + 2 , fftbins = False ) 
win /= np . sum ( win ) 
win = np . atleast_2d ( win ) 
cens = scipy . signal . convolve2d ( chroma_quant , win , 
mode = 'same' , boundary = 'fill' ) 
~~~ cens = chroma_quant 
~~ return util . normalize ( cens , norm = norm , axis = 0 ) 
~~ def tonnetz ( y = None , sr = 22050 , chroma = None ) : 
if y is None and chroma is None : 
~~ if chroma is None : 
~~~ chroma = chroma_cqt ( y = y , sr = sr ) 
~~ dim_map = np . linspace ( 0 , 12 , num = chroma . shape [ 0 ] , endpoint = False ) 
scale = np . asarray ( [ 7. / 6 , 7. / 6 , 
3. / 2 , 3. / 2 , 
2. / 3 , 2. / 3 ] ) 
V = np . multiply . outer ( scale , dim_map ) 
V [ : : 2 ] -= 0.5 
phi = R [ : , np . newaxis ] * np . cos ( np . pi * V ) 
return phi . dot ( util . normalize ( chroma , norm = 1 , axis = 0 ) ) 
~~ def mfcc ( y = None , sr = 22050 , S = None , n_mfcc = 20 , dct_type = 2 , norm = 'ortho' , ** kwargs ) : 
if S is None : 
~~~ S = power_to_db ( melspectrogram ( y = y , sr = sr , ** kwargs ) ) 
~~ return scipy . fftpack . dct ( S , axis = 0 , type = dct_type , norm = norm ) [ : n_mfcc ] 
~~ def melspectrogram ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = 512 , 
power = 2.0 , ** kwargs ) : 
S , n_fft = _spectrogram ( y = y , S = S , n_fft = n_fft , hop_length = hop_length , power = power , 
mel_basis = filters . mel ( sr , n_fft , ** kwargs ) 
return np . dot ( mel_basis , S ) 
~~ def estimate_tuning ( input_file ) : 
ends = [ int_a [ 1 ] , int_b [ 1 ] ] 
if ends [ 1 ] < ends [ 0 ] : 
~~~ ends . reverse ( ) 
~~ starts = [ int_a [ 0 ] , int_b [ 0 ] ] 
if starts [ 1 ] < starts [ 0 ] : 
~~~ starts . reverse ( ) 
~~ intersection = ends [ 0 ] - starts [ 1 ] 
if intersection < 0 : 
~~~ intersection = 0. 
~~ union = ends [ 1 ] - starts [ 0 ] 
if union > 0 : 
~~~ return intersection / union 
~~ return 0.0 
best_score = - 1 
best_idx = - 1 
for idx in candidates : 
~~~ score = __jaccard ( query , intervals_to [ idx ] ) 
if score > best_score : 
~~~ best_score , best_idx = score , idx 
~~ ~~ return best_idx 
start_index = np . argsort ( intervals_to [ : , 0 ] ) 
end_index = np . argsort ( intervals_to [ : , 1 ] ) 
start_sorted = intervals_to [ start_index , 0 ] 
end_sorted = intervals_to [ end_index , 1 ] 
search_ends = np . searchsorted ( start_sorted , intervals_from [ : , 1 ] , side = 'right' ) 
search_starts = np . searchsorted ( end_sorted , intervals_from [ : , 0 ] , side = 'left' ) 
output = np . empty ( len ( intervals_from ) , dtype = numba . uint32 ) 
for i in range ( len ( intervals_from ) ) : 
~~~ query = intervals_from [ i ] 
after_query = search_ends [ i ] 
before_query = search_starts [ i ] 
candidates = set ( start_index [ : after_query ] ) & set ( end_index [ before_query : ] ) 
if len ( candidates ) > 0 : 
~~~ output [ i ] = __match_interval_overlaps ( query , intervals_to , candidates ) 
~~ elif strict : 
~~~ raise ParameterError 
~~~ dist_before = np . inf 
dist_after = np . inf 
if search_starts [ i ] > 0 : 
~~~ dist_before = query [ 0 ] - end_sorted [ search_starts [ i ] - 1 ] 
~~ if search_ends [ i ] + 1 < len ( intervals_to ) : 
~~~ dist_after = start_sorted [ search_ends [ i ] + 1 ] - query [ 1 ] 
~~ if dist_before < dist_after : 
~~~ output [ i ] = end_index [ search_starts [ i ] - 1 ] 
~~~ output [ i ] = start_index [ search_ends [ i ] + 1 ] 
~~ ~~ ~~ return output 
~~ def match_intervals ( intervals_from , intervals_to , strict = True ) : 
if len ( intervals_from ) == 0 or len ( intervals_to ) == 0 : 
~~ valid_intervals ( intervals_from ) 
valid_intervals ( intervals_to ) 
~~~ return __match_intervals ( intervals_from , intervals_to , strict = strict ) 
~~ except ParameterError : 
~~~ six . reraise ( ParameterError , 
sys . exc_info ( ) [ 2 ] ) 
~~ ~~ def match_events ( events_from , events_to , left = True , right = True ) : 
if len ( events_from ) == 0 or len ( events_to ) == 0 : 
~~ if not ( left or right ) and not np . all ( np . in1d ( events_from , events_to ) ) : 
~~ if ( not left ) and max ( events_to ) < max ( events_from ) : 
~~ if ( not right ) and min ( events_to ) > min ( events_from ) : 
~~ output = np . empty_like ( events_from , dtype = np . int ) 
return __match_events_helper ( output , events_from , events_to , left , right ) 
~~ def salience ( S , freqs , h_range , weights = None , aggregate = None , 
filter_peaks = True , fill_value = np . nan , kind = 'linear' , axis = 0 ) : 
~~~ aggregate = np . average 
~~ if weights is None : 
~~~ weights = np . ones ( ( len ( h_range ) , ) ) 
~~~ weights = np . array ( weights , dtype = float ) 
~~ S_harm = interp_harmonics ( S , freqs , h_range , kind = kind , axis = axis ) 
~~~ S_sal = aggregate ( S_harm , axis = 0 , weights = weights ) 
~~~ S_sal = aggregate ( S_harm , axis = 0 ) 
~~ if filter_peaks : 
~~~ S_peaks = scipy . signal . argrelmax ( S , axis = 0 ) 
S_out = np . empty ( S . shape ) 
S_out . fill ( fill_value ) 
S_out [ S_peaks [ 0 ] , S_peaks [ 1 ] ] = S_sal [ S_peaks [ 0 ] , S_peaks [ 1 ] ] 
S_sal = S_out 
~~ return S_sal 
~~ def interp_harmonics ( x , freqs , h_range , kind = 'linear' , fill_value = 0 , axis = 0 ) : 
out_shape = [ len ( h_range ) ] 
out_shape . extend ( x . shape ) 
x_out = np . zeros ( out_shape , dtype = x . dtype ) 
if freqs . ndim == 1 and len ( freqs ) == x . shape [ axis ] : 
~~~ harmonics_1d ( x_out , x , freqs , h_range , 
kind = kind , fill_value = fill_value , 
~~ elif freqs . ndim == 2 and freqs . shape == x . shape : 
~~~ harmonics_2d ( x_out , x , freqs , h_range , 
~~ return x_out 
~~ def harmonics_1d ( harmonic_out , x , freqs , h_range , kind = 'linear' , 
fill_value = 0 , axis = 0 ) : 
f_interp = scipy . interpolate . interp1d ( freqs , x , 
kind = kind , 
copy = False , 
bounds_error = False , 
fill_value = fill_value ) 
idx_out = [ slice ( None ) ] * harmonic_out . ndim 
interp_axis = 1 + ( axis % x . ndim ) 
for h_index , harmonic in enumerate ( h_range ) : 
~~~ idx_out [ 0 ] = h_index 
for f_index , frequency in enumerate ( freqs ) : 
~~~ idx_out [ interp_axis ] = f_index 
harmonic_out [ tuple ( idx_out ) ] = f_interp ( harmonic * frequency ) 
~~ ~~ ~~ def harmonics_2d ( harmonic_out , x , freqs , h_range , kind = 'linear' , fill_value = 0 , 
axis = 0 ) : 
idx_in = [ slice ( None ) ] * x . ndim 
idx_freq = [ slice ( None ) ] * x . ndim 
ni_axis = ( 1 + axis ) % x . ndim 
for i in range ( x . shape [ ni_axis ] ) : 
~~~ idx_in [ ni_axis ] = slice ( i , i + 1 ) 
idx_freq [ ni_axis ] = i 
idx_out [ 1 + ni_axis ] = idx_in [ ni_axis ] 
harmonics_1d ( harmonic_out [ tuple ( idx_out ) ] , x [ tuple ( idx_in ) ] , freqs [ tuple ( idx_freq ) ] , 
h_range , kind = kind , fill_value = fill_value , 
~~ ~~ def load ( path , sr = 22050 , mono = True , offset = 0.0 , duration = None , 
dtype = np . float32 , res_type = 'kaiser_best' ) : 
~~~ with sf . SoundFile ( path ) as sf_desc : 
~~~ sr_native = sf_desc . samplerate 
if offset : 
~~~ sf_desc . seek ( int ( offset * sr_native ) ) 
~~ if duration is not None : 
~~~ frame_duration = int ( duration * sr_native ) 
~~~ frame_duration = - 1 
~~ y = sf_desc . read ( frames = frame_duration , dtype = dtype , always_2d = False ) . T 
~~ ~~ except RuntimeError as exc : 
~~~ y , sr_native = __audioread_load ( path , offset , duration , dtype ) 
~~ if mono : 
~~ if sr is not None : 
~~~ y = resample ( y , sr_native , sr , res_type = res_type ) 
~~~ sr = sr_native 
~~ return y , sr 
~~ def __audioread_load ( path , offset , duration , dtype ) : 
with audioread . audio_open ( path ) as input_file : 
~~~ sr_native = input_file . samplerate 
n_channels = input_file . channels 
s_start = int ( np . round ( sr_native * offset ) ) * n_channels 
if duration is None : 
~~~ s_end = np . inf 
~~~ s_end = s_start + ( int ( np . round ( sr_native * duration ) ) 
* n_channels ) 
~~ n = 0 
for frame in input_file : 
~~~ frame = util . buf_to_float ( frame , dtype = dtype ) 
n_prev = n 
n = n + len ( frame ) 
if n < s_start : 
~~ if s_end < n_prev : 
~~ if s_end < n : 
~~~ frame = frame [ : s_end - n_prev ] 
~~ if n_prev <= s_start <= n : 
~~~ frame = frame [ ( s_start - n_prev ) : ] 
~~ y . append ( frame ) 
~~ ~~ if y : 
~~~ y = np . concatenate ( y ) 
if n_channels > 1 : 
~~~ y = y . reshape ( ( - 1 , n_channels ) ) . T 
~~~ y = np . empty ( 0 , dtype = dtype ) 
~~ return y , sr_native 
~~ def to_mono ( y ) : 
if y . ndim > 1 : 
~~~ y = np . mean ( y , axis = 0 ) 
~~ def resample ( y , orig_sr , target_sr , res_type = 'kaiser_best' , fix = True , scale = False , ** kwargs ) : 
if orig_sr == target_sr : 
~~~ return y 
~~ ratio = float ( target_sr ) / orig_sr 
n_samples = int ( np . ceil ( y . shape [ - 1 ] * ratio ) ) 
if res_type in ( 'scipy' , 'fft' ) : 
~~~ y_hat = scipy . signal . resample ( y , n_samples , axis = - 1 ) 
~~ elif res_type == 'polyphase' : 
~~~ if int ( orig_sr ) != orig_sr or int ( target_sr ) != target_sr : 
~~ orig_sr = int ( orig_sr ) 
target_sr = int ( target_sr ) 
gcd = np . gcd ( orig_sr , target_sr ) 
y_hat = scipy . signal . resample_poly ( y , target_sr // gcd , orig_sr // gcd , axis = - 1 ) 
~~~ y_hat = resampy . resample ( y , orig_sr , target_sr , filter = res_type , axis = - 1 ) 
~~ if fix : 
~~~ y_hat = util . fix_length ( y_hat , n_samples , ** kwargs ) 
~~ if scale : 
~~~ y_hat /= np . sqrt ( ratio ) 
~~ return np . ascontiguousarray ( y_hat , dtype = y . dtype ) 
~~ def get_duration ( y = None , sr = 22050 , S = None , n_fft = 2048 , hop_length = 512 , 
center = True , filename = None ) : 
if filename is not None : 
~~~ return sf . info ( filename ) . duration 
~~~ with audioread . audio_open ( filename ) as fdesc : 
~~~ return fdesc . duration 
~~ ~~ ~~ if y is None : 
~~~ if S is None : 
~~ n_frames = S . shape [ 1 ] 
n_samples = n_fft + hop_length * ( n_frames - 1 ) 
~~~ n_samples = n_samples - 2 * int ( n_fft / 2 ) 
~~~ util . valid_audio ( y , mono = False ) 
~~~ n_samples = len ( y ) 
~~~ n_samples = y . shape [ - 1 ] 
~~ ~~ return float ( n_samples ) / sr 
~~ def autocorrelate ( y , max_size = None , axis = - 1 ) : 
if max_size is None : 
~~~ max_size = y . shape [ axis ] 
~~ max_size = int ( min ( max_size , y . shape [ axis ] ) ) 
powspec = np . abs ( fft . fft ( y , n = 2 * y . shape [ axis ] + 1 , axis = axis ) ) ** 2 
autocorr = fft . ifft ( powspec , axis = axis ) 
subslice = [ slice ( None ) ] * autocorr . ndim 
subslice [ axis ] = slice ( max_size ) 
autocorr = autocorr [ tuple ( subslice ) ] 
if not np . iscomplexobj ( y ) : 
~~~ autocorr = autocorr . real 
~~ return autocorr 
~~ def lpc ( y , order ) : 
if not isinstance ( order , int ) or order < 1 : 
~~ util . valid_audio ( y , mono = True ) 
return __lpc ( y , order ) 
~~ def zero_crossings ( y , threshold = 1e-10 , ref_magnitude = None , pad = True , 
zero_pos = True , axis = - 1 ) : 
~~ if six . callable ( ref_magnitude ) : 
~~~ threshold = threshold * ref_magnitude ( np . abs ( y ) ) 
~~ elif ref_magnitude is not None : 
~~~ threshold = threshold * ref_magnitude 
~~ if threshold > 0 : 
~~~ y = y . copy ( ) 
y [ np . abs ( y ) <= threshold ] = 0 
~~ if zero_pos : 
~~~ y_sign = np . signbit ( y ) 
~~~ y_sign = np . sign ( y ) 
~~ slice_pre = [ slice ( None ) ] * y . ndim 
slice_pre [ axis ] = slice ( 1 , None ) 
slice_post = [ slice ( None ) ] * y . ndim 
slice_post [ axis ] = slice ( - 1 ) 
padding = [ ( 0 , 0 ) ] * y . ndim 
padding [ axis ] = ( 1 , 0 ) 
return np . pad ( ( y_sign [ tuple ( slice_post ) ] != y_sign [ tuple ( slice_pre ) ] ) , 
padding , 
mode = 'constant' , 
constant_values = pad ) 
~~ def clicks ( times = None , frames = None , sr = 22050 , hop_length = 512 , 
click_freq = 1000.0 , click_duration = 0.1 , click = None , length = None ) : 
if times is None : 
~~~ if frames is None : 
~~~ raise ParameterError ( \ ) 
~~ positions = frames_to_samples ( frames , hop_length = hop_length ) 
~~~ positions = time_to_samples ( times , sr = sr ) 
~~ if click is not None : 
~~~ util . valid_audio ( click , mono = True ) 
~~~ if click_duration <= 0 : 
~~ if click_freq <= 0 : 
~~ angular_freq = 2 * np . pi * click_freq / float ( sr ) 
click = np . logspace ( 0 , - 10 , 
num = int ( np . round ( sr * click_duration ) ) , 
base = 2.0 ) 
click *= np . sin ( angular_freq * np . arange ( len ( click ) ) ) 
~~ if length is None : 
~~~ length = positions . max ( ) + click . shape [ 0 ] 
~~~ if length < 1 : 
~~ positions = positions [ positions < length ] 
~~ click_signal = np . zeros ( length , dtype = np . float32 ) 
for start in positions : 
~~~ end = start + click . shape [ 0 ] 
if end >= length : 
~~~ click_signal [ start : ] += click [ : length - start ] 
~~~ click_signal [ start : end ] += click 
~~ ~~ return click_signal 
~~ def tone ( frequency , sr = 22050 , length = None , duration = None , phi = None ) : 
if frequency is None : 
~~~ if duration is None : 
~~ length = duration * sr 
~~ if phi is None : 
~~~ phi = - np . pi * 0.5 
~~ step = 1.0 / sr 
return np . cos ( 2 * np . pi * frequency * ( np . arange ( step * length , step = step ) ) + phi ) 
~~ def chirp ( fmin , fmax , sr = 22050 , length = None , duration = None , linear = False , phi = None ) : 
if fmin is None or fmax is None : 
~~ period = 1.0 / sr 
~~~ duration = period * length 
~~ method = 'linear' if linear else 'logarithmic' 
return scipy . signal . chirp ( 
np . arange ( duration , step = period ) , 
fmin , 
duration , 
fmax , 
method = method , 
~~ def tempogram ( y = None , sr = 22050 , onset_envelope = None , hop_length = 512 , 
win_length = 384 , center = True , window = 'hann' , norm = np . inf ) : 
from . . onset import onset_strength 
if win_length < 1 : 
~~ ac_window = get_window ( window , win_length , fftbins = True ) 
~~~ onset_envelope = np . ascontiguousarray ( onset_envelope ) 
~~ if onset_envelope . ndim > 1 : 
~~~ return np . asarray ( [ tempogram ( onset_envelope = oe_subband , 
norm = norm ) for oe_subband in onset_envelope ] ) 
~~ n = len ( onset_envelope ) 
~~~ onset_envelope = np . pad ( onset_envelope , int ( win_length // 2 ) , 
mode = 'linear_ramp' , end_values = [ 0 , 0 ] ) 
~~ odf_frame = util . frame ( onset_envelope , 
frame_length = win_length , 
hop_length = 1 ) 
~~~ odf_frame = odf_frame [ : , : n ] 
~~ return util . normalize ( autocorrelate ( odf_frame * ac_window [ : , np . newaxis ] , 
axis = 0 ) , 
norm = norm , axis = 0 ) 
~~ def find_files ( directory , ext = None , recurse = True , case_sensitive = False , 
limit = None , offset = 0 ) : 
if ext is None : 
~~~ ext = [ 'aac' , 'au' , 'flac' , 'm4a' , 'mp3' , 'ogg' , 'wav' ] 
~~ elif isinstance ( ext , six . string_types ) : 
~~~ ext = [ ext ] 
~~ ext = set ( ext ) 
if not case_sensitive : 
~~~ ext = set ( [ e . lower ( ) for e in ext ] ) 
ext |= set ( [ e . upper ( ) for e in ext ] ) 
~~ files = set ( ) 
if recurse : 
~~~ for walk in os . walk ( directory ) : 
~~~ files |= __get_files ( walk [ 0 ] , ext ) 
~~~ files = __get_files ( directory , ext ) 
~~ files = list ( files ) 
files . sort ( ) 
files = files [ offset : ] 
if limit is not None : 
~~~ files = files [ : limit ] 
~~ def __get_files ( dir_name , extensions ) : 
dir_name = os . path . abspath ( os . path . expanduser ( dir_name ) ) 
myfiles = set ( ) 
for sub_ext in extensions : 
~~~ globstr = os . path . join ( dir_name , '*' + os . path . extsep + sub_ext ) 
myfiles |= set ( glob . glob ( globstr ) ) 
~~ return myfiles 
~~ def stretch_demo ( input_file , output_file , speed ) : 
y_stretch = librosa . effects . time_stretch ( y , speed ) 
librosa . output . write_wav ( output_file , y_stretch , sr ) 
~~ def process_arguments ( args ) : 
parser . add_argument ( 'input_file' , 
action = 'store' , 
parser . add_argument ( 'output_file' , 
parser . add_argument ( '-s' , '--speed' , 
type = float , 
default = 2.0 , 
required = False , 
help = 'speed' ) 
return vars ( parser . parse_args ( args ) ) 
~~ def hpss_demo ( input_file , output_harmonic , output_percussive ) : 
y_harmonic , y_percussive = librosa . effects . hpss ( y ) 
librosa . output . write_wav ( output_harmonic , y_harmonic , sr ) 
librosa . output . write_wav ( output_percussive , y_percussive , sr ) 
~~ def beat_track ( y = None , sr = 22050 , onset_envelope = None , hop_length = 512 , 
start_bpm = 120.0 , tightness = 100 , trim = True , bpm = None , 
units = 'frames' ) : 
~~ onset_envelope = onset . onset_strength ( y = y , 
~~ if not onset_envelope . any ( ) : 
~~~ return ( 0 , np . array ( [ ] , dtype = int ) ) 
~~ if bpm is None : 
~~~ bpm = tempo ( onset_envelope = onset_envelope , 
start_bpm = start_bpm ) [ 0 ] 
~~ beats = __beat_tracker ( onset_envelope , 
bpm , 
float ( sr ) / hop_length , 
tightness , 
trim ) 
if units == 'frames' : 
~~~ beats = core . frames_to_samples ( beats , hop_length = hop_length ) 
~~~ beats = core . frames_to_time ( beats , hop_length = hop_length , sr = sr ) 
~~ return ( bpm , beats ) 
~~ def tempo ( y = None , sr = 22050 , onset_envelope = None , hop_length = 512 , start_bpm = 120 , 
std_bpm = 1.0 , ac_size = 8.0 , max_tempo = 320.0 , aggregate = np . mean ) : 
if start_bpm <= 0 : 
~~ win_length = np . asscalar ( core . time_to_frames ( ac_size , sr = sr , 
hop_length = hop_length ) ) 
tg = tempogram ( y = y , sr = sr , 
onset_envelope = onset_envelope , 
win_length = win_length ) 
if aggregate is not None : 
~~~ tg = aggregate ( tg , axis = 1 , keepdims = True ) 
~~ bpms = core . tempo_frequencies ( tg . shape [ 0 ] , hop_length = hop_length , sr = sr ) 
prior = np . exp ( - 0.5 * ( ( np . log2 ( bpms ) - np . log2 ( start_bpm ) ) / std_bpm ) ** 2 ) 
if max_tempo is not None : 
~~~ max_idx = np . argmax ( bpms < max_tempo ) 
prior [ : max_idx ] = 0 
~~ best_period = np . argmax ( tg * prior [ : , np . newaxis ] , axis = 0 ) 
tempi = bpms [ best_period ] 
tempi [ best_period == 0 ] = start_bpm 
return tempi 
~~ def __beat_tracker ( onset_envelope , bpm , fft_res , tightness , trim ) : 
if bpm <= 0 : 
~~ period = round ( 60.0 * fft_res / bpm ) 
localscore = __beat_local_score ( onset_envelope , period ) 
backlink , cumscore = __beat_track_dp ( localscore , period , tightness ) 
beats = [ __last_beat ( cumscore ) ] 
while backlink [ beats [ - 1 ] ] >= 0 : 
~~~ beats . append ( backlink [ beats [ - 1 ] ] ) 
~~ beats = np . array ( beats [ : : - 1 ] , dtype = int ) 
beats = __trim_beats ( localscore , beats , trim ) 
return beats 
~~ def __normalize_onsets ( onsets ) : 
norm = onsets . std ( ddof = 1 ) 
if norm > 0 : 
~~~ onsets = onsets / norm 
~~ def __beat_local_score ( onset_envelope , period ) : 
window = np . exp ( - 0.5 * ( np . arange ( - period , period + 1 ) * 32.0 / period ) ** 2 ) 
return scipy . signal . convolve ( __normalize_onsets ( onset_envelope ) , 
window , 
'same' ) 
~~ def __beat_track_dp ( localscore , period , tightness ) : 
backlink = np . zeros_like ( localscore , dtype = int ) 
cumscore = np . zeros_like ( localscore ) 
window = np . arange ( - 2 * period , - np . round ( period / 2 ) + 1 , dtype = int ) 
if tightness <= 0 : 
~~ txwt = - tightness * ( np . log ( - window / period ) ** 2 ) 
first_beat = True 
for i , score_i in enumerate ( localscore ) : 
~~~ z_pad = np . maximum ( 0 , min ( - window [ 0 ] , len ( window ) ) ) 
candidates = txwt . copy ( ) 
candidates [ z_pad : ] = candidates [ z_pad : ] + cumscore [ window [ z_pad : ] ] 
beat_location = np . argmax ( candidates ) 
cumscore [ i ] = score_i + candidates [ beat_location ] 
if first_beat and score_i < 0.01 * localscore . max ( ) : 
~~~ backlink [ i ] = - 1 
~~~ backlink [ i ] = window [ beat_location ] 
first_beat = False 
~~ window = window + 1 
~~ return backlink , cumscore 
~~ def __last_beat ( cumscore ) : 
maxes = util . localmax ( cumscore ) 
med_score = np . median ( cumscore [ np . argwhere ( maxes ) ] ) 
return np . argwhere ( ( cumscore * maxes * 2 > med_score ) ) . max ( ) 
~~ def __trim_beats ( localscore , beats , trim ) : 
smooth_boe = scipy . signal . convolve ( localscore [ beats ] , 
scipy . signal . hann ( 5 ) , 
if trim : 
~~~ threshold = 0.5 * ( ( smooth_boe ** 2 ) . mean ( ) ** 0.5 ) 
~~ valid = np . argwhere ( smooth_boe > threshold ) 
return beats [ valid . min ( ) : valid . max ( ) ] 
~~ def recurrence_matrix ( data , k = None , width = 1 , metric = 'euclidean' , 
sym = False , sparse = False , mode = 'connectivity' , 
bandwidth = None , self = False , axis = - 1 ) : 
data = np . atleast_2d ( data ) 
data = np . swapaxes ( data , axis , 0 ) 
t = data . shape [ 0 ] 
data = data . reshape ( ( t , - 1 ) ) 
if width < 1 or width > t : 
~~ if mode not in [ 'connectivity' , 'distance' , 'affinity' ] : 
"[\ 
"\ ) . format ( mode ) ) 
~~ if k is None : 
~~~ if t > 2 * width + 1 : 
~~~ k = 2 * np . ceil ( np . sqrt ( t - 2 * width + 1 ) ) 
~~~ k = 2 
~~ ~~ if bandwidth is not None : 
~~~ if bandwidth <= 0 : 
~~ ~~ k = int ( k ) 
~~~ knn = sklearn . neighbors . NearestNeighbors ( n_neighbors = min ( t - 1 , k + 2 * width ) , 
metric = metric , 
algorithm = 'auto' ) 
algorithm = 'brute' ) 
~~ knn . fit ( data ) 
if mode == 'affinity' : 
~~~ kng_mode = 'distance' 
~~~ kng_mode = mode 
~~ rec = knn . kneighbors_graph ( mode = kng_mode ) . tolil ( ) 
for diag in range ( - width + 1 , width ) : 
~~~ rec . setdiag ( 0 , diag ) 
~~ for i in range ( t ) : 
~~~ links = rec [ i ] . nonzero ( ) [ 1 ] 
idx = links [ np . argsort ( rec [ i , links ] . toarray ( ) ) ] [ 0 ] 
rec [ i , idx [ k : ] ] = 0 
~~ if self : 
~~~ if mode == 'connectivity' : 
~~~ rec . setdiag ( 1 ) 
~~ elif mode == 'affinity' : 
~~~ rec . setdiag ( - 1 ) 
~~ ~~ if sym : 
~~~ rec = rec . minimum ( rec . T ) 
~~ rec = rec . tocsr ( ) 
rec . eliminate_zeros ( ) 
if mode == 'connectivity' : 
~~~ rec = rec . astype ( np . bool ) 
~~~ if bandwidth is None : 
~~~ bandwidth = np . nanmedian ( rec . max ( axis = 1 ) . data ) 
~~ rec . data [ rec . data < 0 ] = 0.0 
rec . data [ : ] = np . exp ( rec . data / ( - 1 * bandwidth ) ) 
~~ if not sparse : 
~~~ rec = rec . toarray ( ) 
~~ return rec 
~~ def recurrence_to_lag ( rec , pad = True , axis = - 1 ) : 
axis = np . abs ( axis ) 
if rec . ndim != 2 or rec . shape [ 0 ] != rec . shape [ 1 ] : 
'{}' . format ( rec . shape ) ) 
~~ sparse = scipy . sparse . issparse ( rec ) 
roll_ax = None 
if sparse : 
~~~ roll_ax = 1 - axis 
lag_format = rec . format 
if axis == 0 : 
~~~ rec = rec . tocsc ( ) 
~~ elif axis in ( - 1 , 1 ) : 
~~~ rec = rec . tocsr ( ) 
~~ ~~ t = rec . shape [ axis ] 
~~~ if pad : 
~~~ kron = np . asarray ( [ [ 1 , 0 ] ] ) . swapaxes ( axis , 0 ) 
lag = scipy . sparse . kron ( kron . astype ( rec . dtype ) , rec , format = 'lil' ) 
~~~ lag = scipy . sparse . lil_matrix ( rec ) 
~~~ padding = [ ( 0 , 0 ) , ( 0 , 0 ) ] 
padding [ ( 1 - axis ) ] = ( 0 , t ) 
lag = np . pad ( rec , padding , mode = 'constant' ) 
~~~ lag = rec . copy ( ) 
~~ ~~ idx_slice = [ slice ( None ) ] * lag . ndim 
for i in range ( 1 , t ) : 
~~~ idx_slice [ axis ] = i 
lag [ tuple ( idx_slice ) ] = util . roll_sparse ( lag [ tuple ( idx_slice ) ] , - i , axis = roll_ax ) 
~~ if sparse : 
~~~ return lag . asformat ( lag_format ) 
~~ return np . ascontiguousarray ( lag . T ) . T 
~~ def lag_to_recurrence ( lag , axis = - 1 ) : 
if axis not in [ 0 , 1 , - 1 ] : 
~~ axis = np . abs ( axis ) 
if lag . ndim != 2 or ( lag . shape [ 0 ] != lag . shape [ 1 ] and 
lag . shape [ 1 - axis ] != 2 * lag . shape [ axis ] ) : 
~~ t = lag . shape [ axis ] 
sparse = scipy . sparse . issparse ( lag ) 
~~~ rec = scipy . sparse . lil_matrix ( lag ) 
roll_ax = 1 - axis 
~~~ rec = lag . copy ( ) 
~~ idx_slice = [ slice ( None ) ] * lag . ndim 
rec [ tuple ( idx_slice ) ] = util . roll_sparse ( lag [ tuple ( idx_slice ) ] , i , axis = roll_ax ) 
~~ sub_slice = [ slice ( None ) ] * rec . ndim 
sub_slice [ 1 - axis ] = slice ( t ) 
rec = rec [ tuple ( sub_slice ) ] 
~~~ return rec . asformat ( lag . format ) 
~~ return np . ascontiguousarray ( rec . T ) . T 
~~ def timelag_filter ( function , pad = True , index = 0 ) : 
def __my_filter ( wrapped_f , * args , ** kwargs ) : 
args = list ( args ) 
args [ index ] = recurrence_to_lag ( args [ index ] , pad = pad ) 
result = wrapped_f ( * args , ** kwargs ) 
return lag_to_recurrence ( result ) 
~~ return decorator ( __my_filter , function ) 
~~ def subsegment ( data , frames , n_segments = 4 , axis = - 1 ) : 
frames = util . fix_frames ( frames , x_min = 0 , x_max = data . shape [ axis ] , pad = True ) 
if n_segments < 1 : 
~~ boundaries = [ ] 
idx_slices = [ slice ( None ) ] * data . ndim 
for seg_start , seg_end in zip ( frames [ : - 1 ] , frames [ 1 : ] ) : 
~~~ idx_slices [ axis ] = slice ( seg_start , seg_end ) 
boundaries . extend ( seg_start + agglomerative ( data [ tuple ( idx_slices ) ] , 
min ( seg_end - seg_start , n_segments ) , 
axis = axis ) ) 
~~ return np . ascontiguousarray ( boundaries ) 
~~ def agglomerative ( data , k , clusterer = None , axis = - 1 ) : 
n = data . shape [ 0 ] 
data = data . reshape ( ( n , - 1 ) ) 
if clusterer is None : 
~~~ grid = sklearn . feature_extraction . image . grid_to_graph ( n_x = n , 
n_y = 1 , n_z = 1 ) 
clusterer = sklearn . cluster . AgglomerativeClustering ( n_clusters = k , 
connectivity = grid , 
memory = cache . memory ) 
~~ clusterer . fit ( data ) 
boundaries = [ 0 ] 
boundaries . extend ( 
list ( 1 + np . nonzero ( np . diff ( clusterer . labels_ ) ) [ 0 ] . astype ( int ) ) ) 
return np . asarray ( boundaries ) 
~~ def path_enhance ( R , n , window = 'hann' , max_ratio = 2.0 , min_ratio = None , n_filters = 7 , 
zero_mean = False , clip = True , ** kwargs ) : 
if min_ratio is None : 
~~~ min_ratio = 1. / max_ratio 
~~ elif min_ratio > max_ratio : 
~~ R_smooth = None 
for ratio in np . logspace ( np . log2 ( min_ratio ) , np . log2 ( max_ratio ) , num = n_filters , base = 2 ) : 
~~~ kernel = diagonal_filter ( window , n , slope = ratio , zero_mean = zero_mean ) 
if R_smooth is None : 
~~~ R_smooth = scipy . ndimage . convolve ( R , kernel , ** kwargs ) 
~~~ np . maximum ( R_smooth , scipy . ndimage . convolve ( R , kernel , ** kwargs ) , 
out = R_smooth ) 
~~ ~~ if clip : 
~~~ np . clip ( R_smooth , 0 , None , out = R_smooth ) 
~~ return R_smooth 
~~ def onset_detect ( input_file , output_csv ) : 
onsets = librosa . onset . onset_detect ( y = y , 
onset_times = librosa . frames_to_time ( onsets , 
librosa . output . times_csv ( output_csv , onset_times ) 
~~ def frame ( y , frame_length = 2048 , hop_length = 512 ) : 
if not isinstance ( y , np . ndarray ) : 
~~ if y . ndim != 1 : 
~~ if len ( y ) < frame_length : 
~~ if hop_length < 1 : 
~~ if not y . flags [ 'C_CONTIGUOUS' ] : 
~~ n_frames = 1 + int ( ( len ( y ) - frame_length ) / hop_length ) 
y_frames = as_strided ( y , shape = ( frame_length , n_frames ) , 
strides = ( y . itemsize , hop_length * y . itemsize ) ) 
return y_frames 
~~ def valid_audio ( y , mono = True ) : 
~~ if not np . issubdtype ( y . dtype , np . floating ) : 
~~ if mono and y . ndim != 1 : 
~~ elif y . ndim > 2 or y . ndim == 0 : 
~~ if not np . isfinite ( y ) . all ( ) : 
~~ def valid_int ( x , cast = None ) : 
if cast is None : 
~~~ cast = np . floor 
~~ if not six . callable ( cast ) : 
~~ return int ( cast ( x ) ) 
~~ def valid_intervals ( intervals ) : 
if intervals . ndim != 2 or intervals . shape [ - 1 ] != 2 : 
~~ if np . any ( intervals [ : , 0 ] > intervals [ : , 1 ] ) : 
~~ def pad_center ( data , size , axis = - 1 , ** kwargs ) : 
n = data . shape [ axis ] 
lpad = int ( ( size - n ) // 2 ) 
lengths = [ ( 0 , 0 ) ] * data . ndim 
lengths [ axis ] = ( lpad , int ( size - n - lpad ) ) 
if lpad < 0 : 
~~ return np . pad ( data , lengths , ** kwargs ) 
~~ def fix_length ( data , size , axis = - 1 , ** kwargs ) : 
if n > size : 
~~~ slices = [ slice ( None ) ] * data . ndim 
slices [ axis ] = slice ( 0 , size ) 
return data [ tuple ( slices ) ] 
~~ elif n < size : 
~~~ lengths = [ ( 0 , 0 ) ] * data . ndim 
lengths [ axis ] = ( 0 , size - n ) 
return np . pad ( data , lengths , ** kwargs ) 
~~ def fix_frames ( frames , x_min = 0 , x_max = None , pad = True ) : 
frames = np . asarray ( frames ) 
if np . any ( frames < 0 ) : 
~~ if pad and ( x_min is not None or x_max is not None ) : 
~~~ frames = np . clip ( frames , x_min , x_max ) 
~~ if pad : 
~~~ pad_data = [ ] 
if x_min is not None : 
~~~ pad_data . append ( x_min ) 
~~ if x_max is not None : 
~~~ pad_data . append ( x_max ) 
~~ frames = np . concatenate ( ( pad_data , frames ) ) 
~~ if x_min is not None : 
~~~ frames = frames [ frames >= x_min ] 
~~~ frames = frames [ frames <= x_max ] 
~~ return np . unique ( frames ) . astype ( int ) 
~~ def axis_sort ( S , axis = - 1 , index = False , value = None ) : 
~~~ value = np . argmax 
~~ if S . ndim != 2 : 
~~ bin_idx = value ( S , axis = np . mod ( 1 - axis , S . ndim ) ) 
idx = np . argsort ( bin_idx ) 
sort_slice = [ slice ( None ) ] * S . ndim 
sort_slice [ axis ] = idx 
if index : 
~~~ return S [ tuple ( sort_slice ) ] , idx 
~~~ return S [ tuple ( sort_slice ) ] 
~~ ~~ def normalize ( S , norm = np . inf , axis = 0 , threshold = None , fill = None ) : 
~~~ threshold = tiny ( S ) 
~~ elif threshold <= 0 : 
'positive' . format ( threshold ) ) 
~~ if fill not in [ None , False , True ] : 
~~ if not np . all ( np . isfinite ( S ) ) : 
~~ mag = np . abs ( S ) . astype ( np . float ) 
fill_norm = 1 
if norm == np . inf : 
~~~ length = np . max ( mag , axis = axis , keepdims = True ) 
~~ elif norm == - np . inf : 
~~~ length = np . min ( mag , axis = axis , keepdims = True ) 
~~ elif norm == 0 : 
~~~ if fill is True : 
~~ length = np . sum ( mag > 0 , axis = axis , keepdims = True , dtype = mag . dtype ) 
~~ elif np . issubdtype ( type ( norm ) , np . number ) and norm > 0 : 
~~~ length = np . sum ( mag ** norm , axis = axis , keepdims = True ) ** ( 1. / norm ) 
if axis is None : 
~~~ fill_norm = mag . size ** ( - 1. / norm ) 
~~~ fill_norm = mag . shape [ axis ] ** ( - 1. / norm ) 
~~ ~~ elif norm is None : 
~~~ return S 
~~ small_idx = length < threshold 
Snorm = np . empty_like ( S ) 
if fill is None : 
~~~ length [ small_idx ] = 1.0 
Snorm [ : ] = S / length 
~~ elif fill : 
~~~ length [ small_idx ] = np . nan 
Snorm [ np . isnan ( Snorm ) ] = fill_norm 
~~~ length [ small_idx ] = np . inf 
~~ return Snorm 
~~ def localmax ( x , axis = 0 ) : 
paddings = [ ( 0 , 0 ) ] * x . ndim 
paddings [ axis ] = ( 1 , 1 ) 
x_pad = np . pad ( x , paddings , mode = 'edge' ) 
inds1 = [ slice ( None ) ] * x . ndim 
inds1 [ axis ] = slice ( 0 , - 2 ) 
inds2 = [ slice ( None ) ] * x . ndim 
inds2 [ axis ] = slice ( 2 , x_pad . shape [ axis ] ) 
return ( x > x_pad [ tuple ( inds1 ) ] ) & ( x >= x_pad [ tuple ( inds2 ) ] ) 
~~ def peak_pick ( x , pre_max , post_max , pre_avg , post_avg , delta , wait ) : 
if pre_max < 0 : 
~~ if pre_avg < 0 : 
~~ if delta < 0 : 
~~ if wait < 0 : 
~~ if post_max <= 0 : 
~~ if post_avg <= 0 : 
~~ if x . ndim != 1 : 
~~ pre_max = valid_int ( pre_max , cast = np . ceil ) 
post_max = valid_int ( post_max , cast = np . ceil ) 
pre_avg = valid_int ( pre_avg , cast = np . ceil ) 
post_avg = valid_int ( post_avg , cast = np . ceil ) 
wait = valid_int ( wait , cast = np . ceil ) 
max_length = pre_max + post_max 
max_origin = np . ceil ( 0.5 * ( pre_max - post_max ) ) 
mov_max = scipy . ndimage . filters . maximum_filter1d ( x , int ( max_length ) , 
origin = int ( max_origin ) , 
cval = x . min ( ) ) 
avg_length = pre_avg + post_avg 
avg_origin = np . ceil ( 0.5 * ( pre_avg - post_avg ) ) 
mov_avg = scipy . ndimage . filters . uniform_filter1d ( x , int ( avg_length ) , 
mode = 'nearest' , 
origin = int ( avg_origin ) ) 
n = 0 
while n - pre_avg < 0 and n < x . shape [ 0 ] : 
~~~ start = n - pre_avg 
start = start if start > 0 else 0 
mov_avg [ n ] = np . mean ( x [ start : n + post_avg ] ) 
n += 1 
~~ n = x . shape [ 0 ] - post_avg 
n = n if n > 0 else 0 
while n < x . shape [ 0 ] : 
~~ detections = x * ( x == mov_max ) 
detections = detections * ( detections >= ( mov_avg + delta ) ) 
peaks = [ ] 
last_onset = - np . inf 
for i in np . nonzero ( detections ) [ 0 ] : 
~~~ if i > last_onset + wait : 
~~~ peaks . append ( i ) 
last_onset = i 
~~ ~~ return np . array ( peaks ) 
~~ def sparsify_rows ( x , quantile = 0.01 ) : 
if x . ndim == 1 : 
~~~ x = x . reshape ( ( 1 , - 1 ) ) 
~~ elif x . ndim > 2 : 
~~ if not 0.0 <= quantile < 1 : 
~~ x_sparse = scipy . sparse . lil_matrix ( x . shape , dtype = x . dtype ) 
mags = np . abs ( x ) 
norms = np . sum ( mags , axis = 1 , keepdims = True ) 
mag_sort = np . sort ( mags , axis = 1 ) 
cumulative_mag = np . cumsum ( mag_sort / norms , axis = 1 ) 
threshold_idx = np . argmin ( cumulative_mag < quantile , axis = 1 ) 
for i , j in enumerate ( threshold_idx ) : 
~~~ idx = np . where ( mags [ i ] >= mag_sort [ i , j ] ) 
x_sparse [ i , idx ] = x [ i , idx ] 
~~ return x_sparse . tocsr ( ) 
~~ def roll_sparse ( x , shift , axis = 0 ) : 
if not scipy . sparse . isspmatrix ( x ) : 
~~~ return np . roll ( x , shift , axis = axis ) 
~~ if axis not in [ 0 , 1 , - 1 ] : 
~~ shift = np . mod ( shift , x . shape [ axis ] ) 
if shift == 0 : 
~~~ return x . copy ( ) 
~~ fmt = x . format 
~~~ x = x . tocsc ( ) 
~~~ x = x . tocsr ( ) 
~~ x_r = scipy . sparse . lil_matrix ( x . shape , dtype = x . dtype ) 
idx_out = [ slice ( None ) ] * x_r . ndim 
idx_in [ axis ] = slice ( 0 , - shift ) 
idx_out [ axis ] = slice ( shift , None ) 
x_r [ tuple ( idx_out ) ] = x [ tuple ( idx_in ) ] 
idx_out [ axis ] = slice ( 0 , shift ) 
idx_in [ axis ] = slice ( - shift , None ) 
return x_r . asformat ( fmt ) 
~~ def buf_to_float ( x , n_bytes = 2 , dtype = np . float32 ) : 
scale = 1. / float ( 1 << ( ( 8 * n_bytes ) - 1 ) ) 
fmt = '<i{:d}' . format ( n_bytes ) 
return scale * np . frombuffer ( x , fmt ) . astype ( dtype ) 
~~ def index_to_slice ( idx , idx_min = None , idx_max = None , step = None , pad = True ) : 
idx_fixed = fix_frames ( idx , idx_min , idx_max , pad = pad ) 
return [ slice ( start , end , step ) for ( start , end ) in zip ( idx_fixed , idx_fixed [ 1 : ] ) ] 
~~ def sync ( data , idx , aggregate = None , pad = True , axis = - 1 ) : 
~~ shape = list ( data . shape ) 
if np . all ( [ isinstance ( _ , slice ) for _ in idx ] ) : 
~~~ slices = idx 
~~ elif np . all ( [ np . issubdtype ( type ( _ ) , np . integer ) for _ in idx ] ) : 
~~~ slices = index_to_slice ( np . asarray ( idx ) , 0 , shape [ axis ] , pad = pad ) 
~~ agg_shape = list ( shape ) 
agg_shape [ axis ] = len ( slices ) 
data_agg = np . empty ( agg_shape , order = 'F' if np . isfortran ( data ) else 'C' , dtype = data . dtype ) 
idx_in = [ slice ( None ) ] * data . ndim 
idx_agg = [ slice ( None ) ] * data_agg . ndim 
for ( i , segment ) in enumerate ( slices ) : 
~~~ idx_in [ axis ] = segment 
idx_agg [ axis ] = i 
data_agg [ tuple ( idx_agg ) ] = aggregate ( data [ tuple ( idx_in ) ] , axis = axis ) 
~~ return data_agg 
~~ def softmask ( X , X_ref , power = 1 , split_zeros = False ) : 
if X . shape != X_ref . shape : 
X_ref . shape ) ) 
~~ if np . any ( X < 0 ) or np . any ( X_ref < 0 ) : 
~~ if power <= 0 : 
~~ dtype = X . dtype 
if not np . issubdtype ( dtype , np . floating ) : 
~~~ dtype = np . float32 
~~ Z = np . maximum ( X , X_ref ) . astype ( dtype ) 
bad_idx = ( Z < np . finfo ( dtype ) . tiny ) 
Z [ bad_idx ] = 1 
if np . isfinite ( power ) : 
~~~ mask = ( X / Z ) ** power 
ref_mask = ( X_ref / Z ) ** power 
good_idx = ~ bad_idx 
mask [ good_idx ] /= mask [ good_idx ] + ref_mask [ good_idx ] 
if split_zeros : 
~~~ mask [ bad_idx ] = 0.5 
~~~ mask [ bad_idx ] = 0.0 
~~~ mask = X > X_ref 
~~ return mask 
~~ def tiny ( x ) : 
x = np . asarray ( x ) 
if np . issubdtype ( x . dtype , np . floating ) or np . issubdtype ( x . dtype , np . complexfloating ) : 
~~~ dtype = x . dtype 
~~ return np . finfo ( dtype ) . tiny 
~~ def fill_off_diagonal ( x , radius , value = 0 ) : 
nx , ny = x . shape 
radius = np . round ( radius * np . min ( x . shape ) ) 
offset = np . abs ( ( x . shape [ 0 ] - x . shape [ 1 ] ) ) 
if nx < ny : 
~~~ idx_u = np . triu_indices_from ( x , k = radius + offset ) 
idx_l = np . tril_indices_from ( x , k = - radius ) 
~~~ idx_u = np . triu_indices_from ( x , k = radius ) 
idx_l = np . tril_indices_from ( x , k = - radius - offset ) 
~~ x [ idx_u ] = value 
x [ idx_l ] = value 
~~ def frames2video ( frame_dir , 
video_file , 
fps = 30 , 
fourcc = 'XVID' , 
filename_tmpl = '{:06d}.jpg' , 
start = 0 , 
end = 0 , 
show_progress = True ) : 
if end == 0 : 
~~~ ext = filename_tmpl . split ( '.' ) [ - 1 ] 
end = len ( [ name for name in scandir ( frame_dir , ext ) ] ) 
~~ first_file = osp . join ( frame_dir , filename_tmpl . format ( start ) ) 
img = cv2 . imread ( first_file ) 
height , width = img . shape [ : 2 ] 
resolution = ( width , height ) 
vwriter = cv2 . VideoWriter ( video_file , VideoWriter_fourcc ( * fourcc ) , fps , 
resolution ) 
def write_frame ( file_idx ) : 
~~~ filename = osp . join ( frame_dir , filename_tmpl . format ( file_idx ) ) 
img = cv2 . imread ( filename ) 
vwriter . write ( img ) 
~~ if show_progress : 
~~~ track_progress ( write_frame , range ( start , end ) ) 
~~~ for i in range ( start , end ) : 
~~~ filename = osp . join ( frame_dir , filename_tmpl . format ( i ) ) 
~~ ~~ vwriter . release ( ) 
~~ def read ( self ) : 
if self . _cache : 
~~~ img = self . _cache . get ( self . _position ) 
if img is not None : 
~~~ ret = True 
~~~ if self . _position != self . _get_real_position ( ) : 
~~~ self . _set_real_position ( self . _position ) 
~~ ret , img = self . _vcap . read ( ) 
if ret : 
~~~ self . _cache . put ( self . _position , img ) 
~~~ ret , img = self . _vcap . read ( ) 
~~ if ret : 
~~~ self . _position += 1 
~~ def get_frame ( self , frame_id ) : 
if frame_id < 0 or frame_id >= self . _frame_cnt : 
~~~ raise IndexError ( 
\ . format ( self . _frame_cnt - 
1 ) ) 
~~ if frame_id == self . _position : 
~~~ return self . read ( ) 
~~ if self . _cache : 
~~~ img = self . _cache . get ( frame_id ) 
~~~ self . _position = frame_id + 1 
~~ ~~ self . _set_real_position ( frame_id ) 
ret , img = self . _vcap . read ( ) 
~~~ if self . _cache : 
~~ self . _position += 1 
~~ def cvt2frames ( self , 
frame_dir , 
file_start = 0 , 
max_num = 0 , 
mkdir_or_exist ( frame_dir ) 
if max_num == 0 : 
~~~ task_num = self . frame_cnt - start 
~~~ task_num = min ( self . frame_cnt - start , max_num ) 
~~ if task_num <= 0 : 
~~ if start > 0 : 
~~~ self . _set_real_position ( start ) 
~~ def write_frame ( file_idx ) : 
~~~ img = self . read ( ) 
filename = osp . join ( frame_dir , filename_tmpl . format ( file_idx ) ) 
cv2 . imwrite ( filename , img ) 
~~~ track_progress ( write_frame , range ( file_start , 
file_start + task_num ) ) 
~~~ for i in range ( task_num ) : 
if img is None : 
~~ filename = osp . join ( frame_dir , 
filename_tmpl . format ( i + file_start ) ) 
~~ ~~ ~~ def track_progress ( func , tasks , bar_width = 50 , ** kwargs ) : 
if isinstance ( tasks , tuple ) : 
~~~ assert len ( tasks ) == 2 
assert isinstance ( tasks [ 0 ] , collections_abc . Iterable ) 
assert isinstance ( tasks [ 1 ] , int ) 
task_num = tasks [ 1 ] 
tasks = tasks [ 0 ] 
~~ elif isinstance ( tasks , collections_abc . Iterable ) : 
~~~ task_num = len ( tasks ) 
~~~ raise TypeError ( 
~~ prog_bar = ProgressBar ( task_num , bar_width ) 
for task in tasks : 
~~~ results . append ( func ( task , ** kwargs ) ) 
prog_bar . update ( ) 
~~ sys . stdout . write ( '\\n' ) 
~~ def track_parallel_progress ( func , 
tasks , 
nproc , 
initializer = None , 
initargs = None , 
bar_width = 50 , 
chunksize = 1 , 
skip_first = False , 
keep_order = True ) : 
~~ pool = init_pool ( nproc , initializer , initargs ) 
start = not skip_first 
task_num -= nproc * chunksize * int ( skip_first ) 
prog_bar = ProgressBar ( task_num , bar_width , start ) 
if keep_order : 
~~~ gen = pool . imap ( func , tasks , chunksize ) 
~~~ gen = pool . imap_unordered ( func , tasks , chunksize ) 
~~ for result in gen : 
~~~ results . append ( result ) 
if skip_first : 
~~~ if len ( results ) < nproc * chunksize : 
~~ elif len ( results ) == nproc * chunksize : 
~~~ prog_bar . start ( ) 
~~ ~~ prog_bar . update ( ) 
pool . close ( ) 
pool . join ( ) 
~~ def imflip ( img , direction = 'horizontal' ) : 
assert direction in [ 'horizontal' , 'vertical' ] 
if direction == 'horizontal' : 
~~~ return np . flip ( img , axis = 1 ) 
~~~ return np . flip ( img , axis = 0 ) 
~~ ~~ def imrotate ( img , 
angle , 
center = None , 
scale = 1.0 , 
border_value = 0 , 
auto_bound = False ) : 
if center is not None and auto_bound : 
~~ h , w = img . shape [ : 2 ] 
if center is None : 
~~~ center = ( ( w - 1 ) * 0.5 , ( h - 1 ) * 0.5 ) 
~~ assert isinstance ( center , tuple ) 
matrix = cv2 . getRotationMatrix2D ( center , - angle , scale ) 
if auto_bound : 
~~~ cos = np . abs ( matrix [ 0 , 0 ] ) 
sin = np . abs ( matrix [ 0 , 1 ] ) 
new_w = h * sin + w * cos 
new_h = h * cos + w * sin 
matrix [ 0 , 2 ] += ( new_w - w ) * 0.5 
matrix [ 1 , 2 ] += ( new_h - h ) * 0.5 
w = int ( np . round ( new_w ) ) 
h = int ( np . round ( new_h ) ) 
~~ rotated = cv2 . warpAffine ( img , matrix , ( w , h ) , borderValue = border_value ) 
return rotated 
~~ def bbox_clip ( bboxes , img_shape ) : 
assert bboxes . shape [ - 1 ] % 4 == 0 
clipped_bboxes = np . empty_like ( bboxes , dtype = bboxes . dtype ) 
clipped_bboxes [ ... , 0 : : 2 ] = np . maximum ( 
np . minimum ( bboxes [ ... , 0 : : 2 ] , img_shape [ 1 ] - 1 ) , 0 ) 
clipped_bboxes [ ... , 1 : : 2 ] = np . maximum ( 
np . minimum ( bboxes [ ... , 1 : : 2 ] , img_shape [ 0 ] - 1 ) , 0 ) 
return clipped_bboxes 
~~ def bbox_scaling ( bboxes , scale , clip_shape = None ) : 
if float ( scale ) == 1.0 : 
~~~ scaled_bboxes = bboxes . copy ( ) 
~~~ w = bboxes [ ... , 2 ] - bboxes [ ... , 0 ] + 1 
h = bboxes [ ... , 3 ] - bboxes [ ... , 1 ] + 1 
dw = ( w * ( scale - 1 ) ) * 0.5 
dh = ( h * ( scale - 1 ) ) * 0.5 
scaled_bboxes = bboxes + np . stack ( ( - dw , - dh , dw , dh ) , axis = - 1 ) 
~~ if clip_shape is not None : 
~~~ return bbox_clip ( scaled_bboxes , clip_shape ) 
~~~ return scaled_bboxes 
~~ ~~ def imcrop ( img , bboxes , scale = 1.0 , pad_fill = None ) : 
chn = 1 if img . ndim == 2 else img . shape [ 2 ] 
if pad_fill is not None : 
~~~ if isinstance ( pad_fill , ( int , float ) ) : 
~~~ pad_fill = [ pad_fill for _ in range ( chn ) ] 
~~ assert len ( pad_fill ) == chn 
~~ _bboxes = bboxes [ None , ... ] if bboxes . ndim == 1 else bboxes 
scaled_bboxes = bbox_scaling ( _bboxes , scale ) . astype ( np . int32 ) 
clipped_bbox = bbox_clip ( scaled_bboxes , img . shape ) 
patches = [ ] 
for i in range ( clipped_bbox . shape [ 0 ] ) : 
~~~ x1 , y1 , x2 , y2 = tuple ( clipped_bbox [ i , : ] ) 
if pad_fill is None : 
~~~ patch = img [ y1 : y2 + 1 , x1 : x2 + 1 , ... ] 
~~~ _x1 , _y1 , _x2 , _y2 = tuple ( scaled_bboxes [ i , : ] ) 
if chn == 2 : 
~~~ patch_shape = ( _y2 - _y1 + 1 , _x2 - _x1 + 1 ) 
~~~ patch_shape = ( _y2 - _y1 + 1 , _x2 - _x1 + 1 , chn ) 
~~ patch = np . array ( 
pad_fill , dtype = img . dtype ) * np . ones ( 
patch_shape , dtype = img . dtype ) 
x_start = 0 if _x1 >= 0 else - _x1 
y_start = 0 if _y1 >= 0 else - _y1 
w = x2 - x1 + 1 
h = y2 - y1 + 1 
patch [ y_start : y_start + h , x_start : x_start + 
w , ... ] = img [ y1 : y1 + h , x1 : x1 + w , ... ] 
~~ patches . append ( patch ) 
~~ if bboxes . ndim == 1 : 
~~~ return patches [ 0 ] 
~~~ return patches 
~~ ~~ def impad ( img , shape , pad_val = 0 ) : 
if not isinstance ( pad_val , ( int , float ) ) : 
~~~ assert len ( pad_val ) == img . shape [ - 1 ] 
~~ if len ( shape ) < len ( img . shape ) : 
~~~ shape = shape + ( img . shape [ - 1 ] , ) 
~~ assert len ( shape ) == len ( img . shape ) 
for i in range ( len ( shape ) - 1 ) : 
~~~ assert shape [ i ] >= img . shape [ i ] 
~~ pad = np . empty ( shape , dtype = img . dtype ) 
pad [ ... ] = pad_val 
pad [ : img . shape [ 0 ] , : img . shape [ 1 ] , ... ] = img 
return pad 
~~ def impad_to_multiple ( img , divisor , pad_val = 0 ) : 
pad_h = int ( np . ceil ( img . shape [ 0 ] / divisor ) ) * divisor 
pad_w = int ( np . ceil ( img . shape [ 1 ] / divisor ) ) * divisor 
return impad ( img , ( pad_h , pad_w ) , pad_val ) 
~~ def _scale_size ( size , scale ) : 
w , h = size 
return int ( w * float ( scale ) + 0.5 ) , int ( h * float ( scale ) + 0.5 ) 
~~ def imresize ( img , size , return_scale = False , interpolation = 'bilinear' ) : 
h , w = img . shape [ : 2 ] 
resized_img = cv2 . resize ( 
img , size , interpolation = interp_codes [ interpolation ] ) 
if not return_scale : 
~~~ return resized_img 
~~~ w_scale = size [ 0 ] / w 
h_scale = size [ 1 ] / h 
return resized_img , w_scale , h_scale 
~~ ~~ def imresize_like ( img , dst_img , return_scale = False , interpolation = 'bilinear' ) : 
h , w = dst_img . shape [ : 2 ] 
return imresize ( img , ( w , h ) , return_scale , interpolation ) 
~~ def imrescale ( img , scale , return_scale = False , interpolation = 'bilinear' ) : 
if isinstance ( scale , ( float , int ) ) : 
~~~ if scale <= 0 : 
~~ scale_factor = scale 
~~ elif isinstance ( scale , tuple ) : 
~~~ max_long_edge = max ( scale ) 
max_short_edge = min ( scale ) 
scale_factor = min ( max_long_edge / max ( h , w ) , 
max_short_edge / min ( h , w ) ) 
type ( scale ) ) ) 
~~ new_size = _scale_size ( ( w , h ) , scale_factor ) 
rescaled_img = imresize ( img , new_size , interpolation = interpolation ) 
if return_scale : 
~~~ return rescaled_img , scale_factor 
~~~ return rescaled_img 
~~ ~~ def load ( file , file_format = None , ** kwargs ) : 
if file_format is None and is_str ( file ) : 
~~~ file_format = file . split ( '.' ) [ - 1 ] 
~~ if file_format not in file_handlers : 
~~ handler = file_handlers [ file_format ] 
if is_str ( file ) : 
~~~ obj = handler . load_from_path ( file , ** kwargs ) 
~~ elif hasattr ( file , 'read' ) : 
~~~ obj = handler . load_from_fileobj ( file , ** kwargs ) 
~~~ raise TypeError ( \ ) 
~~ return obj 
~~ def dump ( obj , file = None , file_format = None , ** kwargs ) : 
if file_format is None : 
~~~ if is_str ( file ) : 
~~ elif file is None : 
~~ ~~ if file_format not in file_handlers : 
if file is None : 
~~~ return handler . dump_to_str ( obj , ** kwargs ) 
~~ elif is_str ( file ) : 
~~~ handler . dump_to_path ( obj , file , ** kwargs ) 
~~ elif hasattr ( file , 'write' ) : 
~~~ handler . dump_to_fileobj ( obj , file , ** kwargs ) 
~~ ~~ def _register_handler ( handler , file_formats ) : 
if not isinstance ( handler , BaseFileHandler ) : 
type ( handler ) ) ) 
~~ if isinstance ( file_formats , str ) : 
~~~ file_formats = [ file_formats ] 
~~ if not is_list_of ( file_formats , str ) : 
~~ for ext in file_formats : 
~~~ file_handlers [ ext ] = handler 
~~ ~~ def get_priority ( priority ) : 
if isinstance ( priority , int ) : 
~~~ if priority < 0 or priority > 100 : 
~~ return priority 
~~ elif isinstance ( priority , Priority ) : 
~~~ return priority . value 
~~ elif isinstance ( priority , str ) : 
~~~ return Priority [ priority . upper ( ) ] . value 
~~ ~~ def quantize ( arr , min_val , max_val , levels , dtype = np . int64 ) : 
if not ( isinstance ( levels , int ) and levels > 1 ) : 
~~ if min_val >= max_val : 
min_val , max_val ) ) 
~~ arr = np . clip ( arr , min_val , max_val ) - min_val 
quantized_arr = np . minimum ( 
np . floor ( levels * arr / ( max_val - min_val ) ) . astype ( dtype ) , levels - 1 ) 
return quantized_arr 
~~ def dequantize ( arr , min_val , max_val , levels , dtype = np . float64 ) : 
~~ dequantized_arr = ( arr + 0.5 ) . astype ( dtype ) * ( 
max_val - min_val ) / levels + min_val 
return dequantized_arr 
~~ def auto_argparser ( description = None ) : 
partial_parser = ArgumentParser ( description = description ) 
cfg_file = partial_parser . parse_known_args ( ) [ 0 ] . config 
cfg = Config . from_file ( cfg_file ) 
parser = ArgumentParser ( description = description ) 
add_args ( parser , cfg ) 
return parser , cfg 
~~ def collate ( batch , samples_per_gpu = 1 ) : 
if not isinstance ( batch , collections . Sequence ) : 
~~ if isinstance ( batch [ 0 ] , DataContainer ) : 
~~~ assert len ( batch ) % samples_per_gpu == 0 
stacked = [ ] 
if batch [ 0 ] . cpu_only : 
~~~ for i in range ( 0 , len ( batch ) , samples_per_gpu ) : 
~~~ stacked . append ( 
[ sample . data for sample in batch [ i : i + samples_per_gpu ] ] ) 
~~ return DataContainer ( 
stacked , batch [ 0 ] . stack , batch [ 0 ] . padding_value , cpu_only = True ) 
~~ elif batch [ 0 ] . stack : 
~~~ assert isinstance ( batch [ i ] . data , torch . Tensor ) 
assert batch [ i ] . dim ( ) == 3 
c , h , w = batch [ i ] . size ( ) 
for sample in batch [ i : i + samples_per_gpu ] : 
~~~ assert c == sample . size ( 0 ) 
h = max ( h , sample . size ( 1 ) ) 
w = max ( w , sample . size ( 2 ) ) 
~~ padded_samples = [ 
F . pad ( 
sample . data , 
( 0 , w - sample . size ( 2 ) , 0 , h - sample . size ( 1 ) ) , 
value = sample . padding_value ) 
for sample in batch [ i : i + samples_per_gpu ] 
stacked . append ( default_collate ( padded_samples ) ) 
~~ ~~ return DataContainer ( stacked , batch [ 0 ] . stack , batch [ 0 ] . padding_value ) 
~~ elif isinstance ( batch [ 0 ] , collections . Sequence ) : 
~~~ transposed = zip ( * batch ) 
return [ collate ( samples , samples_per_gpu ) for samples in transposed ] 
~~ elif isinstance ( batch [ 0 ] , collections . Mapping ) : 
~~~ return { 
key : collate ( [ d [ key ] for d in batch ] , samples_per_gpu ) 
for key in batch [ 0 ] 
~~~ return default_collate ( batch ) 
~~ ~~ def imshow ( img , win_name = '' , wait_time = 0 ) : 
cv2 . imshow ( win_name , imread ( img ) ) 
cv2 . waitKey ( wait_time ) 
~~ def imshow_bboxes ( img , 
bboxes , 
colors = 'green' , 
top_k = - 1 , 
thickness = 1 , 
show = True , 
win_name = '' , 
wait_time = 0 , 
out_file = None ) : 
img = imread ( img ) 
if isinstance ( bboxes , np . ndarray ) : 
~~~ bboxes = [ bboxes ] 
~~ if not isinstance ( colors , list ) : 
~~~ colors = [ colors for _ in range ( len ( bboxes ) ) ] 
~~ colors = [ color_val ( c ) for c in colors ] 
assert len ( bboxes ) == len ( colors ) 
for i , _bboxes in enumerate ( bboxes ) : 
~~~ _bboxes = _bboxes . astype ( np . int32 ) 
if top_k <= 0 : 
~~~ _top_k = _bboxes . shape [ 0 ] 
~~~ _top_k = min ( top_k , _bboxes . shape [ 0 ] ) 
~~ for j in range ( _top_k ) : 
~~~ left_top = ( _bboxes [ j , 0 ] , _bboxes [ j , 1 ] ) 
right_bottom = ( _bboxes [ j , 2 ] , _bboxes [ j , 3 ] ) 
cv2 . rectangle ( 
img , left_top , right_bottom , colors [ i ] , thickness = thickness ) 
~~ ~~ if show : 
~~~ imshow ( img , win_name , wait_time ) 
~~ if out_file is not None : 
~~~ imwrite ( img , out_file ) 
~~ ~~ def imshow_det_bboxes ( img , 
labels , 
class_names = None , 
score_thr = 0 , 
bbox_color = 'green' , 
text_color = 'green' , 
font_scale = 0.5 , 
assert bboxes . ndim == 2 
assert labels . ndim == 1 
assert bboxes . shape [ 0 ] == labels . shape [ 0 ] 
assert bboxes . shape [ 1 ] == 4 or bboxes . shape [ 1 ] == 5 
if score_thr > 0 : 
~~~ assert bboxes . shape [ 1 ] == 5 
scores = bboxes [ : , - 1 ] 
inds = scores > score_thr 
bboxes = bboxes [ inds , : ] 
labels = labels [ inds ] 
~~ bbox_color = color_val ( bbox_color ) 
text_color = color_val ( text_color ) 
for bbox , label in zip ( bboxes , labels ) : 
~~~ bbox_int = bbox . astype ( np . int32 ) 
left_top = ( bbox_int [ 0 ] , bbox_int [ 1 ] ) 
right_bottom = ( bbox_int [ 2 ] , bbox_int [ 3 ] ) 
img , left_top , right_bottom , bbox_color , thickness = thickness ) 
label_text = class_names [ 
if len ( bbox ) > 4 : 
~~~ label_text += '|{:.02f}' . format ( bbox [ - 1 ] ) 
~~ cv2 . putText ( img , label_text , ( bbox_int [ 0 ] , bbox_int [ 1 ] - 2 ) , 
cv2 . FONT_HERSHEY_COMPLEX , font_scale , text_color ) 
~~ ~~ def flowread ( flow_or_path , quantize = False , concat_axis = 0 , * args , ** kwargs ) : 
if isinstance ( flow_or_path , np . ndarray ) : 
~~~ if ( flow_or_path . ndim != 3 ) or ( flow_or_path . shape [ - 1 ] != 2 ) : 
flow_or_path . shape ) ) 
~~ return flow_or_path 
~~ elif not is_str ( flow_or_path ) : 
\ . format ( 
type ( flow_or_path ) ) ) 
~~ if not quantize : 
~~~ with open ( flow_or_path , 'rb' ) as f : 
~~~ header = f . read ( 4 ) . decode ( 'utf-8' ) 
~~ except Exception : 
~~~ if header != 'PIEH' : 
~~~ raise IOError ( 
format ( flow_or_path ) ) 
~~ ~~ w = np . fromfile ( f , np . int32 , 1 ) . squeeze ( ) 
h = np . fromfile ( f , np . int32 , 1 ) . squeeze ( ) 
flow = np . fromfile ( f , np . float32 , w * h * 2 ) . reshape ( ( h , w , 2 ) ) 
~~~ assert concat_axis in [ 0 , 1 ] 
cat_flow = imread ( flow_or_path , flag = 'unchanged' ) 
if cat_flow . ndim != 2 : 
format ( flow_or_path , cat_flow . ndim ) ) 
~~ assert cat_flow . shape [ concat_axis ] % 2 == 0 
dx , dy = np . split ( cat_flow , 2 , axis = concat_axis ) 
flow = dequantize_flow ( dx , dy , * args , ** kwargs ) 
~~ return flow . astype ( np . float32 ) 
~~ def flowwrite ( flow , filename , quantize = False , concat_axis = 0 , * args , ** kwargs ) : 
if not quantize : 
~~~ with open ( filename , 'wb' ) as f : 
~~~ f . write ( 'PIEH' . encode ( 'utf-8' ) ) 
np . array ( [ flow . shape [ 1 ] , flow . shape [ 0 ] ] , dtype = np . int32 ) . tofile ( f ) 
flow = flow . astype ( np . float32 ) 
flow . tofile ( f ) 
f . flush ( ) 
dx , dy = quantize_flow ( flow , * args , ** kwargs ) 
dxdy = np . concatenate ( ( dx , dy ) , axis = concat_axis ) 
imwrite ( dxdy , filename ) 
~~ ~~ def quantize_flow ( flow , max_val = 0.02 , norm = True ) : 
h , w , _ = flow . shape 
dx = flow [ ... , 0 ] 
dy = flow [ ... , 1 ] 
dy = dy / h 
~~ flow_comps = [ 
quantize ( d , - max_val , max_val , 255 , np . uint8 ) for d in [ dx , dy ] 
return tuple ( flow_comps ) 
~~ def dequantize_flow ( dx , dy , max_val = 0.02 , denorm = True ) : 
assert dx . shape == dy . shape 
assert dx . ndim == 2 or ( dx . ndim == 3 and dx . shape [ - 1 ] == 1 ) 
dx , dy = [ dequantize ( d , - max_val , max_val , 255 ) for d in [ dx , dy ] ] 
if denorm : 
~~~ dx *= dx . shape [ 1 ] 
dy *= dx . shape [ 0 ] 
~~ flow = np . dstack ( ( dx , dy ) ) 
return flow 
~~ def load_state_dict ( module , state_dict , strict = False , logger = None ) : 
unexpected_keys = [ ] 
own_state = module . state_dict ( ) 
for name , param in state_dict . items ( ) : 
~~~ if name not in own_state : 
~~~ unexpected_keys . append ( name ) 
~~ if isinstance ( param , torch . nn . Parameter ) : 
~~~ param = param . data 
~~~ own_state [ name ] . copy_ ( param ) 
. format ( name , own_state [ name ] . size ( ) , 
param . size ( ) ) ) 
~~ ~~ missing_keys = set ( own_state . keys ( ) ) - set ( state_dict . keys ( ) ) 
err_msg = [ ] 
if unexpected_keys : 
~~ if missing_keys : 
~~ err_msg = '\\n' . join ( err_msg ) 
if err_msg : 
~~~ if strict : 
~~~ raise RuntimeError ( err_msg ) 
~~ elif logger is not None : 
~~~ logger . warn ( err_msg ) 
~~~ print ( err_msg ) 
~~ ~~ ~~ def load_checkpoint ( model , 
filename , 
map_location = None , 
strict = False , 
logger = None ) : 
if filename . startswith ( 'modelzoo://' ) : 
~~~ import torchvision 
model_urls = dict ( ) 
for _ , name , ispkg in pkgutil . walk_packages ( 
torchvision . models . __path__ ) : 
~~~ if not ispkg : 
~~~ _zoo = import_module ( 'torchvision.models.{}' . format ( name ) ) 
_urls = getattr ( _zoo , 'model_urls' ) 
model_urls . update ( _urls ) 
~~ ~~ model_name = filename [ 11 : ] 
checkpoint = model_zoo . load_url ( model_urls [ model_name ] ) 
~~ elif filename . startswith ( 'open-mmlab://' ) : 
~~~ model_name = filename [ 13 : ] 
checkpoint = model_zoo . load_url ( open_mmlab_model_urls [ model_name ] ) 
~~ elif filename . startswith ( ( 'http://' , 'https://' ) ) : 
~~~ checkpoint = model_zoo . load_url ( filename ) 
~~~ if not osp . isfile ( filename ) : 
~~ checkpoint = torch . load ( filename , map_location = map_location ) 
~~ if isinstance ( checkpoint , OrderedDict ) : 
~~~ state_dict = checkpoint 
~~ elif isinstance ( checkpoint , dict ) and 'state_dict' in checkpoint : 
~~~ state_dict = checkpoint [ 'state_dict' ] 
~~~ raise RuntimeError ( 
~~ if list ( state_dict . keys ( ) ) [ 0 ] . startswith ( 'module.' ) : 
~~~ state_dict = { k [ 7 : ] : v for k , v in checkpoint [ 'state_dict' ] . items ( ) } 
~~ if hasattr ( model , 'module' ) : 
~~~ load_state_dict ( model . module , state_dict , strict , logger ) 
~~~ load_state_dict ( model , state_dict , strict , logger ) 
~~ return checkpoint 
~~ def weights_to_cpu ( state_dict ) : 
state_dict_cpu = OrderedDict ( ) 
for key , val in state_dict . items ( ) : 
~~~ state_dict_cpu [ key ] = val . cpu ( ) 
~~ return state_dict_cpu 
~~ def save_checkpoint ( model , filename , optimizer = None , meta = None ) : 
if meta is None : 
~~~ meta = { } 
~~ elif not isinstance ( meta , dict ) : 
type ( meta ) ) ) 
~~ meta . update ( mmcv_version = mmcv . __version__ , time = time . asctime ( ) ) 
mmcv . mkdir_or_exist ( osp . dirname ( filename ) ) 
if hasattr ( model , 'module' ) : 
~~~ model = model . module 
~~ checkpoint = { 
'meta' : meta , 
'state_dict' : weights_to_cpu ( model . state_dict ( ) ) 
if optimizer is not None : 
~~~ checkpoint [ 'optimizer' ] = optimizer . state_dict ( ) 
~~ torch . save ( checkpoint , filename ) 
~~ def init_optimizer ( self , optimizer ) : 
if isinstance ( optimizer , dict ) : 
~~~ optimizer = obj_from_dict ( 
optimizer , torch . optim , dict ( params = self . model . parameters ( ) ) ) 
~~ elif not isinstance ( optimizer , torch . optim . Optimizer ) : 
~~ return optimizer 
~~ def init_logger ( self , log_dir = None , level = logging . INFO ) : 
logging . basicConfig ( 
logger = logging . getLogger ( __name__ ) 
if log_dir and self . rank == 0 : 
~~~ filename = '{}.log' . format ( self . timestamp ) 
log_file = osp . join ( log_dir , filename ) 
self . _add_file_handler ( logger , log_file , level = level ) 
~~ return logger 
~~ def current_lr ( self ) : 
if self . optimizer is None : 
~~ return [ group [ 'lr' ] for group in self . optimizer . param_groups ] 
~~ def register_hook ( self , hook , priority = 'NORMAL' ) : 
assert isinstance ( hook , Hook ) 
if hasattr ( hook , 'priority' ) : 
~~~ raise ValueError ( \ ) 
~~ priority = get_priority ( priority ) 
hook . priority = priority 
inserted = False 
for i in range ( len ( self . _hooks ) - 1 , - 1 , - 1 ) : 
~~~ if priority >= self . _hooks [ i ] . priority : 
~~~ self . _hooks . insert ( i + 1 , hook ) 
inserted = True 
~~ ~~ if not inserted : 
~~~ self . _hooks . insert ( 0 , hook ) 
~~ ~~ def run ( self , data_loaders , workflow , max_epochs , ** kwargs ) : 
assert isinstance ( data_loaders , list ) 
assert mmcv . is_list_of ( workflow , tuple ) 
assert len ( data_loaders ) == len ( workflow ) 
self . _max_epochs = max_epochs 
work_dir = self . work_dir if self . work_dir is not None else 'NONE' 
get_host_info ( ) , work_dir ) 
self . call_hook ( 'before_run' ) 
while self . epoch < max_epochs : 
~~~ for i , flow in enumerate ( workflow ) : 
~~~ mode , epochs = flow 
~~~ if not hasattr ( self , mode ) : 
\ . 
format ( mode ) ) 
~~ epoch_runner = getattr ( self , mode ) 
~~~ epoch_runner = mode 
type ( mode ) ) ) 
~~ for _ in range ( epochs ) : 
~~~ if mode == 'train' and self . epoch >= max_epochs : 
~~ epoch_runner ( data_loaders [ i ] , ** kwargs ) 
self . call_hook ( 'after_run' ) 
~~ def register_training_hooks ( self , 
lr_config , 
optimizer_config = None , 
checkpoint_config = None , 
log_config = None ) : 
if optimizer_config is None : 
~~~ optimizer_config = { } 
~~ if checkpoint_config is None : 
~~~ checkpoint_config = { } 
~~ self . register_lr_hooks ( lr_config ) 
self . register_hook ( self . build_hook ( optimizer_config , OptimizerHook ) ) 
self . register_hook ( self . build_hook ( checkpoint_config , CheckpointHook ) ) 
self . register_hook ( IterTimerHook ( ) ) 
if log_config is not None : 
~~~ self . register_logger_hooks ( log_config ) 
~~ ~~ def convert_video ( in_file , out_file , print_cmd = False , pre_options = '' , 
options = [ ] 
for k , v in kwargs . items ( ) : 
~~~ if isinstance ( v , bool ) : 
~~~ if v : 
~~~ options . append ( '-{}' . format ( k ) ) 
~~ ~~ elif k == 'log_level' : 
~~~ assert v in [ 
'quiet' , 'panic' , 'fatal' , 'error' , 'warning' , 'info' , 
'verbose' , 'debug' , 'trace' 
if print_cmd : 
~~~ print ( cmd ) 
~~ subprocess . call ( cmd , shell = True ) 
~~ def resize_video ( in_file , 
out_file , 
size = None , 
ratio = None , 
keep_ar = False , 
log_level = 'info' , 
print_cmd = False , 
if size is None and ratio is None : 
~~ elif size is not None and ratio is not None : 
~~ options = { 'log_level' : log_level } 
if size : 
~~~ if not keep_ar : 
~~~ options [ 'vf' ] = 'scale={}:{}' . format ( size [ 0 ] , size [ 1 ] ) 
~~~ options [ 'vf' ] = ( 'scale=w={}:h={}:force_original_aspect_ratio' 
'=decrease' . format ( size [ 0 ] , size [ 1 ] ) ) 
~~~ if not isinstance ( ratio , tuple ) : 
~~~ ratio = ( ratio , ratio ) 
~~ options [ 'vf' ] = \ . format ( 
ratio [ 0 ] , ratio [ 1 ] ) 
~~ convert_video ( in_file , out_file , print_cmd , ** options ) 
~~ def cut_video ( in_file , 
start = None , 
end = None , 
vcodec = None , 
acodec = None , 
options = { 'log_level' : log_level } 
if vcodec is None : 
~~~ options [ 'vcodec' ] = 'copy' 
~~ if acodec is None : 
~~~ options [ 'acodec' ] = 'copy' 
~~ if start : 
~~~ options [ 'ss' ] = start 
~~ if end : 
~~~ options [ 't' ] = end - start 
~~ def concat_video ( video_list , 
_ , tmp_filename = tempfile . mkstemp ( suffix = '.txt' , text = True ) 
with open ( tmp_filename , 'w' ) as f : 
~~~ for filename in video_list : 
~~ ~~ options = { 'log_level' : log_level } 
~~ convert_video ( 
tmp_filename , 
print_cmd , 
** options ) 
os . remove ( tmp_filename ) 
~~ def list_from_file ( filename , prefix = '' , offset = 0 , max_num = 0 ) : 
cnt = 0 
item_list = [ ] 
~~~ for _ in range ( offset ) : 
~~~ f . readline ( ) 
~~ for line in f : 
~~~ if max_num > 0 and cnt >= max_num : 
~~ item_list . append ( prefix + line . rstrip ( '\\n' ) ) 
cnt += 1 
~~ ~~ return item_list 
~~ def dict_from_file ( filename , key_type = str ) : 
mapping = { } 
~~~ items = line . rstrip ( '\\n' ) . split ( ) 
assert len ( items ) >= 2 
key = key_type ( items [ 0 ] ) 
val = items [ 1 : ] if len ( items ) > 2 else items [ 1 ] 
mapping [ key ] = val 
~~ ~~ return mapping 
~~ def conv3x3 ( in_planes , out_planes , dilation = 1 ) : 
return nn . Conv2d ( 
in_planes , 
out_planes , 
kernel_size = 3 , 
padding = dilation , 
dilation = dilation ) 
~~ def obj_from_dict ( info , parent = None , default_args = None ) : 
assert isinstance ( info , dict ) and 'type' in info 
assert isinstance ( default_args , dict ) or default_args is None 
args = info . copy ( ) 
obj_type = args . pop ( 'type' ) 
if mmcv . is_str ( obj_type ) : 
~~~ if parent is not None : 
~~~ obj_type = getattr ( parent , obj_type ) 
~~~ obj_type = sys . modules [ obj_type ] 
~~ ~~ elif not isinstance ( obj_type , type ) : 
type ( obj_type ) ) ) 
~~ if default_args is not None : 
~~~ for name , value in default_args . items ( ) : 
~~~ args . setdefault ( name , value ) 
~~ ~~ return obj_type ( ** args ) 
~~ def imread ( img_or_path , flag = 'color' ) : 
if isinstance ( img_or_path , np . ndarray ) : 
~~~ return img_or_path 
~~ elif is_str ( img_or_path ) : 
~~~ flag = imread_flags [ flag ] if is_str ( flag ) else flag 
check_file_exist ( img_or_path , 
return cv2 . imread ( img_or_path , flag ) 
~~ ~~ def imfrombytes ( content , flag = 'color' ) : 
img_np = np . frombuffer ( content , np . uint8 ) 
flag = imread_flags [ flag ] if is_str ( flag ) else flag 
img = cv2 . imdecode ( img_np , flag ) 
~~ def imwrite ( img , file_path , params = None , auto_mkdir = True ) : 
if auto_mkdir : 
~~~ dir_name = osp . abspath ( osp . dirname ( file_path ) ) 
mkdir_or_exist ( dir_name ) 
~~ return cv2 . imwrite ( file_path , img , params ) 
~~ def bgr2gray ( img , keepdim = False ) : 
out_img = cv2 . cvtColor ( img , cv2 . COLOR_BGR2GRAY ) 
if keepdim : 
~~~ out_img = out_img [ ... , None ] 
~~ return out_img 
~~ def gray2bgr ( img ) : 
img = img [ ... , None ] if img . ndim == 2 else img 
out_img = cv2 . cvtColor ( img , cv2 . COLOR_GRAY2BGR ) 
return out_img 
~~ def iter_cast ( inputs , dst_type , return_type = None ) : 
if not isinstance ( inputs , collections_abc . Iterable ) : 
~~ if not isinstance ( dst_type , type ) : 
~~ out_iterable = six . moves . map ( dst_type , inputs ) 
if return_type is None : 
~~~ return out_iterable 
~~~ return return_type ( out_iterable ) 
~~ ~~ def is_seq_of ( seq , expected_type , seq_type = None ) : 
if seq_type is None : 
~~~ exp_seq_type = collections_abc . Sequence 
~~~ assert isinstance ( seq_type , type ) 
exp_seq_type = seq_type 
~~ if not isinstance ( seq , exp_seq_type ) : 
~~ for item in seq : 
~~~ if not isinstance ( item , expected_type ) : 
~~ def slice_list ( in_list , lens ) : 
if not isinstance ( lens , list ) : 
~~ elif sum ( lens ) != len ( in_list ) : 
sum ( lens ) , len ( in_list ) ) ) 
~~ out_list = [ ] 
idx = 0 
for i in range ( len ( lens ) ) : 
~~~ out_list . append ( in_list [ idx : idx + lens [ i ] ] ) 
idx += lens [ i ] 
~~ return out_list 
~~ def check_prerequisites ( 
prerequisites , 
checker , 
msg_tmpl = \ 
def wrap ( func ) : 
~~~ @ functools . wraps ( func ) 
def wrapped_func ( * args , ** kwargs ) : 
~~~ requirements = [ prerequisites ] if isinstance ( 
prerequisites , str ) else prerequisites 
for item in requirements : 
~~~ if not checker ( item ) : 
~~~ missing . append ( item ) 
~~ ~~ if missing : 
~~ ~~ return wrapped_func 
~~ return wrap 
~~ def average ( self , n = 0 ) : 
assert n >= 0 
for key in self . val_history : 
~~~ values = np . array ( self . val_history [ key ] [ - n : ] ) 
nums = np . array ( self . n_history [ key ] [ - n : ] ) 
avg = np . sum ( values * nums ) / np . sum ( nums ) 
self . output [ key ] = avg 
~~ self . ready = True 
~~ def scatter ( input , devices , streams = None ) : 
if streams is None : 
~~~ streams = [ None ] * len ( devices ) 
~~ if isinstance ( input , list ) : 
~~~ chunk_size = ( len ( input ) - 1 ) // len ( devices ) + 1 
outputs = [ 
scatter ( input [ i ] , [ devices [ i // chunk_size ] ] , 
[ streams [ i // chunk_size ] ] ) for i in range ( len ( input ) ) 
return outputs 
~~ elif isinstance ( input , torch . Tensor ) : 
~~~ output = input . contiguous ( ) 
stream = streams [ 0 ] if output . numel ( ) > 0 else None 
with torch . cuda . device ( devices [ 0 ] ) , torch . cuda . stream ( stream ) : 
~~~ output = output . cuda ( devices [ 0 ] , non_blocking = True ) 
~~ ~~ def color_val ( color ) : 
if is_str ( color ) : 
~~~ return Color [ color ] . value 
~~ elif isinstance ( color , Color ) : 
~~~ return color . value 
~~ elif isinstance ( color , tuple ) : 
~~~ assert len ( color ) == 3 
for channel in color : 
~~~ assert channel >= 0 and channel <= 255 
~~ return color 
~~ elif isinstance ( color , int ) : 
~~~ assert color >= 0 and color <= 255 
return color , color , color 
~~ elif isinstance ( color , np . ndarray ) : 
~~~ assert color . ndim == 1 and color . size == 3 
assert np . all ( ( color >= 0 ) & ( color <= 255 ) ) 
color = color . astype ( np . uint8 ) 
return tuple ( color ) 
~~ ~~ def check_time ( timer_id ) : 
if timer_id not in _g_timers : 
~~~ _g_timers [ timer_id ] = Timer ( ) 
return 0 
~~~ return _g_timers [ timer_id ] . since_last_check ( ) 
~~ ~~ def start ( self ) : 
if not self . _is_running : 
~~~ self . _t_start = time ( ) 
self . _is_running = True 
~~ self . _t_last = time ( ) 
~~ def since_start ( self ) : 
return self . _t_last - self . _t_start 
~~ def since_last_check ( self ) : 
~~ dur = time ( ) - self . _t_last 
self . _t_last = time ( ) 
return dur 
~~ def flowshow ( flow , win_name = '' , wait_time = 0 ) : 
flow = flowread ( flow ) 
flow_img = flow2rgb ( flow ) 
imshow ( rgb2bgr ( flow_img ) , win_name , wait_time ) 
~~ def flow2rgb ( flow , color_wheel = None , unknown_thr = 1e6 ) : 
assert flow . ndim == 3 and flow . shape [ - 1 ] == 2 
if color_wheel is None : 
~~~ color_wheel = make_color_wheel ( ) 
~~ assert color_wheel . ndim == 2 and color_wheel . shape [ 1 ] == 3 
num_bins = color_wheel . shape [ 0 ] 
dx = flow [ : , : , 0 ] . copy ( ) 
dy = flow [ : , : , 1 ] . copy ( ) 
ignore_inds = ( np . isnan ( dx ) | np . isnan ( dy ) | ( np . abs ( dx ) > unknown_thr ) | 
( np . abs ( dy ) > unknown_thr ) ) 
dx [ ignore_inds ] = 0 
dy [ ignore_inds ] = 0 
rad = np . sqrt ( dx ** 2 + dy ** 2 ) 
if np . any ( rad > np . finfo ( float ) . eps ) : 
~~~ max_rad = np . max ( rad ) 
dx /= max_rad 
dy /= max_rad 
~~ [ h , w ] = dx . shape 
angle = np . arctan2 ( - dy , - dx ) / np . pi 
bin_real = ( angle + 1 ) / 2 * ( num_bins - 1 ) 
bin_left = np . floor ( bin_real ) . astype ( int ) 
bin_right = ( bin_left + 1 ) % num_bins 
w = ( bin_real - bin_left . astype ( np . float32 ) ) [ ... , None ] 
flow_img = ( 
1 - w ) * color_wheel [ bin_left , : ] + w * color_wheel [ bin_right , : ] 
small_ind = rad <= 1 
flow_img [ small_ind ] = 1 - rad [ small_ind , None ] * ( 1 - flow_img [ small_ind ] ) 
flow_img [ np . logical_not ( small_ind ) ] *= 0.75 
flow_img [ ignore_inds , : ] = 0 
return flow_img 
~~ def make_color_wheel ( bins = None ) : 
if bins is None : 
~~~ bins = [ 15 , 6 , 4 , 11 , 13 , 6 ] 
~~ assert len ( bins ) == 6 
RY , YG , GC , CB , BM , MR = tuple ( bins ) 
ry = [ 1 , np . arange ( RY ) / RY , 0 ] 
yg = [ 1 - np . arange ( YG ) / YG , 1 , 0 ] 
gc = [ 0 , 1 , np . arange ( GC ) / GC ] 
cb = [ 0 , 1 - np . arange ( CB ) / CB , 1 ] 
bm = [ np . arange ( BM ) / BM , 0 , 1 ] 
mr = [ 1 , 0 , 1 - np . arange ( MR ) / MR ] 
num_bins = RY + YG + GC + CB + BM + MR 
color_wheel = np . zeros ( ( 3 , num_bins ) , dtype = np . float32 ) 
col = 0 
for i , color in enumerate ( [ ry , yg , gc , cb , bm , mr ] ) : 
~~~ for j in range ( 3 ) : 
~~~ color_wheel [ j , col : col + bins [ i ] ] = color [ j ] 
~~ col += bins [ i ] 
~~ return color_wheel . T 
correct = pred . eq ( target . view ( 1 , - 1 ) . expand_as ( pred ) ) 
~~~ correct_k = correct [ : k ] . view ( - 1 ) . float ( ) . sum ( 0 , keepdim = True ) 
res . append ( correct_k . mul_ ( 100.0 / batch_size ) ) 
~~ ~~ def scatter ( inputs , target_gpus , dim = 0 ) : 
def scatter_map ( obj ) : 
~~~ if isinstance ( obj , torch . Tensor ) : 
~~~ return OrigScatter . apply ( target_gpus , None , dim , obj ) 
~~ if isinstance ( obj , DataContainer ) : 
~~~ if obj . cpu_only : 
~~~ return obj . data 
~~~ return Scatter . forward ( target_gpus , obj . data ) 
~~ ~~ if isinstance ( obj , tuple ) and len ( obj ) > 0 : 
~~~ return list ( zip ( * map ( scatter_map , obj ) ) ) 
~~ if isinstance ( obj , list ) and len ( obj ) > 0 : 
~~~ out = list ( map ( list , zip ( * map ( scatter_map , obj ) ) ) ) 
return out 
~~ if isinstance ( obj , dict ) and len ( obj ) > 0 : 
~~~ out = list ( map ( type ( obj ) , zip ( * map ( scatter_map , obj . items ( ) ) ) ) ) 
~~ return [ obj for targets in target_gpus ] 
~~~ return scatter_map ( inputs ) 
~~~ scatter_map = None 
~~ ~~ def scatter_kwargs ( inputs , kwargs , target_gpus , dim = 0 ) : 
inputs = scatter ( inputs , target_gpus , dim ) if inputs else [ ] 
kwargs = scatter ( kwargs , target_gpus , dim ) if kwargs else [ ] 
if len ( inputs ) < len ( kwargs ) : 
~~~ inputs . extend ( [ ( ) for _ in range ( len ( kwargs ) - len ( inputs ) ) ] ) 
~~ elif len ( kwargs ) < len ( inputs ) : 
~~~ kwargs . extend ( [ { } for _ in range ( len ( inputs ) - len ( kwargs ) ) ] ) 
~~ inputs = tuple ( inputs ) 
kwargs = tuple ( kwargs ) 
return inputs , kwargs 
~~ async def fetch ( self ) -> Response : 
if self . request_config . get ( 'DELAY' , 0 ) > 0 : 
~~~ await asyncio . sleep ( self . request_config [ 'DELAY' ] ) 
~~ timeout = self . request_config . get ( 'TIMEOUT' , 10 ) 
~~~ async with async_timeout . timeout ( timeout ) : 
~~~ resp = await self . _make_request ( ) 
~~~ resp_data = await resp . text ( encoding = self . encoding ) 
~~ except UnicodeDecodeError : 
~~~ resp_data = await resp . read ( ) 
~~ response = Response ( 
url = self . url , 
method = self . method , 
encoding = resp . get_encoding ( ) , 
html = resp_data , 
metadata = self . metadata , 
cookies = resp . cookies , 
headers = resp . headers , 
history = resp . history , 
status = resp . status , 
aws_json = resp . json , 
aws_text = resp . text , 
aws_read = resp . read ) 
aws_valid_response = self . request_config . get ( 'VALID' ) 
if aws_valid_response and iscoroutinefunction ( aws_valid_response ) : 
~~~ response = await aws_valid_response ( response ) 
~~ if response . ok : 
~~~ return response 
~~ ~~ except asyncio . TimeoutError : 
~~~ return await self . _retry ( error_msg = 'timeout' ) 
~~~ return await self . _retry ( error_msg = e ) 
~~~ await self . _close_request_session ( ) 
~~ ~~ def request ( self , * args , ** kwargs ) : 
middleware = args [ 0 ] 
@ wraps ( middleware ) 
def register_middleware ( * args , ** kwargs ) : 
~~~ self . request_middleware . append ( middleware ) 
return middleware 
~~ return register_middleware ( ) 
~~ def response ( self , * args , ** kwargs ) : 
~~~ self . response_middleware . appendleft ( middleware ) 
~~ async def json ( self , 
* , 
encoding : str = None , 
loads : JSONDecoder = DEFAULT_JSON_DECODER , 
content_type : Optional [ str ] = 'application/json' ) -> Any : 
return await self . _aws_json ( 
encoding = encoding , loads = loads , content_type = content_type ) 
~~ async def text ( self , 
encoding : Optional [ str ] = None , 
errors : str = 'strict' ) -> str : 
return await self . _aws_text ( encoding = encoding , errors = errors ) 
~~ async def _run_spider_hook ( self , hook_func ) : 
if callable ( hook_func ) : 
~~~ aws_hook_func = hook_func ( weakref . proxy ( self ) ) 
if isawaitable ( aws_hook_func ) : 
~~~ await aws_hook_func 
~~ ~~ ~~ async def process_callback_result ( self , callback_result ) : 
callback_result_name = type ( callback_result ) . __name__ 
process_func_name = self . callback_result_map . get ( 
callback_result_name , '' ) 
process_func = getattr ( self , process_func_name , None ) 
if process_func is not None : 
~~~ await process_func ( callback_result ) 
~~~ raise InvalidCallbackResult ( 
~~ ~~ async def async_start ( 
cls , 
middleware : typing . Union [ typing . Iterable , Middleware ] = None , 
loop = None , 
after_start = None , 
before_stop = None , 
loop = loop or asyncio . get_event_loop ( ) 
spider_ins = cls ( middleware = middleware , loop = loop , is_async_start = True ) 
await spider_ins . _start ( 
after_start = after_start , before_stop = before_stop ) 
~~ def start ( cls , 
close_event_loop = True , 
loop = loop or asyncio . new_event_loop ( ) 
spider_ins = cls ( middleware = middleware , loop = loop ) 
spider_ins . loop . run_until_complete ( 
spider_ins . _start ( 
spider_ins . loop . shutdown_asyncgens ( ) ) 
if close_event_loop : 
~~~ spider_ins . loop . close ( ) 
~~ ~~ async def handle_callback ( self , aws_callback : typing . Coroutine , response ) : 
callback_result = None 
~~~ callback_result = await aws_callback 
~~ except NothingMatchedError as e : 
~~ return callback_result , response 
~~ async def handle_request ( self , request : Request 
) -> typing . Tuple [ AsyncGeneratorType , Response ] : 
callback_result , response = None , None 
await self . _run_request_middleware ( request ) 
~~~ callback_result , response = await request . fetch_callback ( self . sem ) 
~~ except NotImplementedParseError as e : 
~~~ self . logger . error ( e ) 
~~ await self . _run_response_middleware ( request , response ) 
await self . _process_response ( request = request , response = response ) 
return callback_result , response 
~~ async def multiple_request ( self , urls , is_gather = False , ** kwargs ) : 
if is_gather : 
~~~ resp_results = await asyncio . gather ( 
* [ 
self . handle_request ( self . request ( url = url , ** kwargs ) ) 
for url in urls 
return_exceptions = True ) 
for index , task_result in enumerate ( resp_results ) : 
~~~ if not isinstance ( task_result , RuntimeError ) and task_result : 
~~~ _ , response = task_result 
response . index = index 
yield response 
~~~ for index , url in enumerate ( urls ) : 
~~~ _ , response = await self . handle_request ( 
self . request ( url = url , ** kwargs ) ) 
~~ ~~ ~~ def request ( self , 
url : str , 
method : str = 'GET' , 
callback = None , 
encoding : typing . Optional [ str ] = None , 
headers : dict = None , 
metadata : dict = None , 
request_config : dict = None , 
request_session = None , 
headers = headers or { } 
metadata = metadata or { } 
request_config = request_config or { } 
request_session = request_session or self . request_session 
headers . update ( self . headers . copy ( ) ) 
request_config . update ( self . request_config . copy ( ) ) 
kwargs . update ( self . kwargs . copy ( ) ) 
return Request ( 
url = url , 
callback = callback , 
encoding = encoding , 
headers = headers , 
metadata = metadata , 
request_config = request_config , 
request_session = request_session , 
~~ async def start_master ( self ) : 
for url in self . start_urls : 
~~~ request_ins = self . request ( 
url = url , callback = self . parse , metadata = self . metadata ) 
self . request_queue . put_nowait ( self . handle_request ( request_ins ) ) 
~~ workers = [ 
asyncio . ensure_future ( self . start_worker ( ) ) 
for i in range ( self . worker_numbers ) 
for worker in workers : 
~~ await self . request_queue . join ( ) 
if not self . is_async_start : 
~~~ await self . stop ( SIGINT ) 
~~~ await self . _cancel_tasks ( ) 
~~ ~~ async def stop ( self , _signal ) : 
await self . _cancel_tasks ( ) 
self . loop . stop ( ) 
~~ def _parse_match ( self , match ) : 
~~~ if self . default : 
~~~ return self . default 
~~~ raise NothingMatchedError ( 
~~~ string = match . group ( ) 
groups = match . groups ( ) 
group_dict = match . groupdict ( ) 
if group_dict : 
~~~ return group_dict 
~~ if groups : 
~~~ return groups [ 0 ] if len ( groups ) == 1 else groups 
~~ return string 
~~ ~~ def get_db ( self , db = 'test' ) : 
if db not in self . _db : 
~~~ self . _db [ db ] = self . client ( db ) [ db ] 
~~ return self . _db [ db ] 
~~ def normalize_task_v2 ( task ) : 
result = dict ( ) 
mod_arg_parser = ModuleArgsParser ( task ) 
~~~ action , arguments , result [ 'delegate_to' ] = mod_arg_parser . parse ( ) 
~~ except AnsibleParserError as e : 
~~~ task_info = "%s:%s" % ( task [ FILENAME_KEY ] , task [ LINE_NUMBER_KEY ] ) 
del task [ FILENAME_KEY ] 
del task [ LINE_NUMBER_KEY ] 
~~~ task_info = "Unknown" 
~~~ import pprint 
pp = pprint . PrettyPrinter ( indent = 2 ) 
task_pprint = pp . pformat ( task ) 
~~ except ImportError : 
~~~ task_pprint = task 
~~ raise SystemExit ( "Couldn\ % ( task_info , e . message , task_pprint ) ) 
~~ if '_uses_shell' in arguments : 
~~~ action = 'shell' 
del ( arguments [ '_uses_shell' ] ) 
~~ for ( k , v ) in list ( task . items ( ) ) : 
~~~ if k in ( 'action' , 'local_action' , 'args' , 'delegate_to' ) or k == action : 
~~~ result [ k ] = v 
~~ ~~ result [ 'action' ] = dict ( __ansible_module__ = action ) 
if '_raw_params' in arguments : 
del ( arguments [ '_raw_params' ] ) 
~~~ result [ 'action' ] [ '__ansible_arguments__' ] = list ( ) 
~~ if 'argv' in arguments and not result [ 'action' ] [ '__ansible_arguments__' ] : 
~~~ result [ 'action' ] [ '__ansible_arguments__' ] = arguments [ 'argv' ] 
del ( arguments [ 'argv' ] ) 
~~ result [ 'action' ] . update ( arguments ) 
~~ def parse_yaml_linenumbers ( data , filename ) : 
def compose_node ( parent , index ) : 
~~~ line = loader . line 
node = Composer . compose_node ( loader , parent , index ) 
node . __line__ = line + 1 
return node 
~~ def construct_mapping ( node , deep = False ) : 
~~~ if ANSIBLE_VERSION < 2 : 
~~~ mapping = Constructor . construct_mapping ( loader , node , deep = deep ) 
~~~ mapping = AnsibleConstructor . construct_mapping ( loader , node , deep = deep ) 
~~ if hasattr ( node , '__line__' ) : 
~~~ mapping [ LINE_NUMBER_KEY ] = node . __line__ 
~~~ mapping [ LINE_NUMBER_KEY ] = mapping . _line_number 
~~ mapping [ FILENAME_KEY ] = filename 
return mapping 
~~~ loader = yaml . Loader ( data ) 
~~~ import inspect 
kwargs = { } 
if 'vault_password' in inspect . getargspec ( AnsibleLoader . __init__ ) . args : 
~~~ kwargs [ 'vault_password' ] = DEFAULT_VAULT_PASSWORD 
~~ loader = AnsibleLoader ( data , ** kwargs ) 
~~ loader . compose_node = compose_node 
loader . construct_mapping = construct_mapping 
data = loader . get_single_data ( ) 
~~ except ( yaml . parser . ParserError , yaml . scanner . ScannerError ) as e : 
~~ def append_skipped_rules ( pyyaml_data , file_text , file_type ) : 
yaml = ruamel . yaml . YAML ( ) 
ruamel_data = yaml . load ( file_text ) 
if file_type in ( 'tasks' , 'handlers' ) : 
~~~ ruamel_tasks = ruamel_data 
pyyaml_tasks = pyyaml_data 
~~ elif file_type == 'playbook' : 
~~~ ruamel_tasks = [ ] 
pyyaml_tasks = [ ] 
for ruamel_play , pyyaml_play in zip ( ruamel_data , pyyaml_data ) : 
~~~ ruamel_tasks . extend ( ruamel_play . get ( 'tasks' ) ) 
pyyaml_tasks . extend ( pyyaml_play . get ( 'tasks' ) ) 
~~ ~~ except ( AttributeError , TypeError ) : 
~~~ return pyyaml_data 
~~ ~~ elif file_type == 'meta' : 
~~~ if not isinstance ( pyyaml_data , list ) : 
~~ ruamel_tasks = [ ruamel_data ] 
~~ if len ( ruamel_tasks ) != len ( pyyaml_tasks ) : 
~~ for ruamel_task , pyyaml_task in zip ( ruamel_tasks , pyyaml_tasks ) : 
~~~ skipped_rules = _get_rule_skips_from_task ( ruamel_task ) 
if skipped_rules : 
~~~ pyyaml_task [ 'skipped_rules' ] = skipped_rules 
~~ ~~ return pyyaml_data 
~~ def __should_write_changes ( self , old_value : StoreItem , new_value : StoreItem ) -> bool : 
if old_value is None or ( hasattr ( new_value , 'e_tag' ) and new_value . e_tag == '*' ) : 
~~ elif hasattr ( new_value , 'e_tag' ) and hasattr ( old_value , 'e_tag' ) : 
~~~ if new_value . e_tag is not None and old_value . e_tag is None : 
~~ if old_value . e_tag == new_value . e_tag or int ( old_value . e_tag ) <= int ( new_value . e_tag ) : 
~~ ~~ async def run_middleware ( self , context : TurnContext , callback : Callable = None ) : 
return await self . _middleware . receive_activity_with_status ( context , callback ) 
~~ def use ( self , * middleware : Middleware ) : 
for ( idx , m ) in enumerate ( middleware ) : 
~~~ if hasattr ( m , 'on_process_request' ) and callable ( m . on_process_request ) : 
~~~ self . _middleware . append ( m ) 
~~~ raise TypeError ( \ % idx ) 
~~ ~~ ~~ def track_pageview ( self , name : str , url : str , duration : int = 0 , properties : Dict [ str , object ] = None , 
measurements : Dict [ str , object ] = None ) -> None : 
self . _client . track_pageview ( name , url , duration , properties , measurements ) 
~~ def track_exception ( self , type_exception : type = None , value : Exception = None , tb : traceback = None , 
properties : Dict [ str , object ] = None , measurements : Dict [ str , object ] = None ) -> None : 
self . _client . track_exception ( type_exception , value , tb , properties , measurements ) 
~~ def track_event ( self , name : str , properties : Dict [ str , object ] = None , 
self . _client . track_event ( name , properties , measurements ) 
~~ def track_metric ( self , name : str , value : float , type : TelemetryDataPointType = None , 
count : int = None , min : float = None , max : float = None , std_dev : float = None , 
properties : Dict [ str , object ] = None ) -> NotImplemented : 
self . _client . track_metric ( name , value , type , count , min , max , std_dev , properties ) 
~~ def track_trace ( self , name : str , properties : Dict [ str , object ] = None , severity = None ) : 
self . _client . track_trace ( name , properties , severity ) 
~~ def track_request ( self , name : str , url : str , success : bool , start_time : str = None , 
duration : int = None , response_code : str = None , http_method : str = None , 
properties : Dict [ str , object ] = None , measurements : Dict [ str , object ] = None , 
request_id : str = None ) : 
self . _client . track_request ( name , url , success , start_time , duration , response_code , http_method , properties , 
measurements , request_id ) 
~~ def track_dependency ( self , name : str , data : str , type : str = None , target : str = None , duration : int = None , 
success : bool = None , result_code : str = None , properties : Dict [ str , object ] = None , 
measurements : Dict [ str , object ] = None , dependency_id : str = None ) : 
self . _client . track_dependency ( name , data , type , target , duration , success , result_code , properties , 
measurements , dependency_id ) 
~~ def create_property ( self , name : str ) -> StatePropertyAccessor : 
if not name : 
~~ return BotStatePropertyAccessor ( self , name ) 
~~ async def load ( self , turn_context : TurnContext , force : bool = False ) -> None : 
if turn_context == None : 
~~ cached_state = turn_context . turn_state . get ( self . _context_service_key ) 
storage_key = self . get_storage_key ( turn_context ) 
if ( force or not cached_state or not cached_state . state ) : 
~~~ items = await self . _storage . read ( [ storage_key ] ) 
val = items . get ( storage_key ) 
turn_context . turn_state [ self . _context_service_key ] = CachedBotState ( val ) 
~~ ~~ async def save_changes ( self , turn_context : TurnContext , force : bool = False ) -> None : 
if force or ( cached_state != None and cached_state . is_changed == True ) : 
~~~ storage_key = self . get_storage_key ( turn_context ) 
changes : Dict [ str , object ] = { storage_key : cached_state . state } 
await self . _storage . write ( changes ) 
cached_state . hash = cached_state . compute_hash ( cached_state . state ) 
~~ ~~ async def clear_state ( self , turn_context : TurnContext ) : 
~~ cache_value = CachedBotState ( ) 
cache_value . hash = '' 
turn_context . turn_state [ self . _context_service_key ] = cache_value 
~~ async def delete ( self , turn_context : TurnContext ) -> None : 
~~ turn_context . turn_state . pop ( self . _context_service_key ) 
await self . _storage . delete ( { storage_key } ) 
~~ async def set_property_value ( self , turn_context : TurnContext , property_name : str , value : object ) -> None : 
~~ if not property_name : 
cached_state . state [ property_name ] = value 
~~ async def continue_conversation ( self , reference : ConversationReference , logic ) : 
request = TurnContext . apply_conversation_reference ( Activity ( ) , reference , is_incoming = True ) 
context = self . create_context ( request ) 
return await self . run_middleware ( context , logic ) 
~~ async def create_conversation ( self , reference : ConversationReference , logic ) : 
~~~ if reference . service_url is None : 
~~ parameters = ConversationParameters ( bot = reference . bot ) 
client = self . create_connector_client ( reference . service_url ) 
resource_response = await client . conversations . create_conversation ( parameters ) 
request . conversation = ConversationAccount ( id = resource_response . id ) 
if resource_response . service_url : 
~~~ request . service_url = resource_response . service_url 
~~ context = self . create_context ( request ) 
~~~ raise e 
~~ ~~ async def process_activity ( self , req , auth_header : str , logic : Callable ) : 
activity = await self . parse_request ( req ) 
auth_header = auth_header or '' 
await self . authenticate_request ( activity , auth_header ) 
context = self . create_context ( activity ) 
~~ async def authenticate_request ( self , request : Activity , auth_header : str ) : 
await JwtTokenValidation . authenticate_request ( request , auth_header , self . _credential_provider ) 
~~ async def parse_request ( req ) : 
async def validate_activity ( activity : Activity ) : 
~~~ if not isinstance ( activity . type , str ) : 
~~ if not isinstance ( req , Activity ) : 
~~~ if hasattr ( req , 'body' ) : 
~~~ activity = Activity ( ) . deserialize ( req . body ) 
is_valid_activity = await validate_activity ( activity ) 
if is_valid_activity : 
~~~ return activity 
~~ ~~ elif 'body' in req : 
~~~ activity = Activity ( ) . deserialize ( req [ 'body' ] ) 
~~~ is_valid_activity = await validate_activity ( req ) 
~~~ return req 
~~ ~~ ~~ async def update_activity ( self , context : TurnContext , activity : Activity ) : 
~~~ client = self . create_connector_client ( activity . service_url ) 
return await client . conversations . update_activity ( 
activity . conversation . id , 
activity . conversation . activity_id , 
activity ) 
~~ ~~ async def delete_activity ( self , context : TurnContext , conversation_reference : ConversationReference ) : 
~~~ client = self . create_connector_client ( conversation_reference . service_url ) 
await client . conversations . delete_activity ( conversation_reference . conversation . id , 
conversation_reference . activity_id ) 
~~ ~~ async def delete_conversation_member ( self , context : TurnContext , member_id : str ) -> None : 
~~~ if not context . activity . service_url : 
~~ if not context . activity . conversation or not context . activity . conversation . id : 
'conversation.id' ) 
~~ service_url = context . activity . service_url 
conversation_id = context . activity . conversation . id 
client = self . create_connector_client ( service_url ) 
return await client . conversations . delete_conversation_member ( conversation_id , member_id ) 
~~ except AttributeError as attr_e : 
~~~ raise attr_e 
~~ ~~ async def get_activity_members ( self , context : TurnContext , activity_id : str ) : 
~~~ if not activity_id : 
~~~ activity_id = context . activity . id 
~~ if not context . activity . service_url : 
~~ if not activity_id : 
'context.activity.id' ) 
return await client . conversations . get_activity_members ( conversation_id , activity_id ) 
~~ ~~ async def get_conversation_members ( self , context : TurnContext ) : 
return await client . conversations . get_conversation_members ( conversation_id ) 
~~ ~~ async def get_conversations ( self , service_url : str , continuation_token : str = None ) : 
return await client . conversations . get_conversations ( continuation_token ) 
~~ def create_connector_client ( self , service_url : str ) -> ConnectorClient : 
client = ConnectorClient ( self . _credentials , base_url = service_url ) 
client . config . add_user_agent ( USER_AGENT ) 
return client 
~~ def add_dialog ( self , dialog : Dialog ) -> object : 
self . _dialogs . add ( dialog ) 
if not self . initial_dialog_id : 
~~~ self . initial_dialog_id = dialog . id 
~~ async def authenticate_request ( activity : Activity , auth_header : str , credentials : CredentialProvider ) -> ClaimsIdentity : 
if not auth_header : 
~~~ is_auth_disabled = await credentials . is_authentication_disabled ( ) 
if is_auth_disabled : 
~~ claims_identity = await JwtTokenValidation . validate_auth_header ( auth_header , credentials , activity . channel_id , activity . service_url ) 
MicrosoftAppCredentials . trust_service_url ( activity . service_url ) 
return claims_identity 
~~ def wheel_dist_name ( self ) : 
return '-' . join ( ( safer_name ( self . distribution . get_name ( ) ) , 
safer_version ( self . distribution . get_version ( ) ) ) ) 
~~ def get_archive_basename ( self ) : 
impl_tag , abi_tag , plat_tag = self . get_tag ( ) 
archive_basename = "%s-%s-%s-%s" % ( 
self . wheel_dist_name , 
impl_tag , 
abi_tag , 
plat_tag ) 
return archive_basename 
~~ def setupcfg_requirements ( self ) : 
metadata = self . distribution . get_option_dict ( 'metadata' ) 
for key , title in ( ( 'provides_extra' , 'Provides-Extra' ) , 
( 'requires_dist' , 'Requires-Dist' ) ) : 
~~~ if not key in metadata : 
~~ field = metadata [ key ] 
for line in field [ 1 ] . splitlines ( ) : 
if not line : 
~~ yield ( title , line ) 
~~ ~~ ~~ def add_requirements ( self , metadata_path ) : 
additional = list ( self . setupcfg_requirements ( ) ) 
if not additional : return 
pkg_info = read_pkg_info ( metadata_path ) 
if 'Provides-Extra' in pkg_info or 'Requires-Dist' in pkg_info : 
del pkg_info [ 'Provides-Extra' ] 
del pkg_info [ 'Requires-Dist' ] 
~~ for k , v in additional : 
~~~ pkg_info [ k ] = v 
~~ write_pkg_info ( metadata_path , pkg_info ) 
~~ def egg2dist ( self , egginfo_path , distinfo_path ) : 
def adios ( p ) : 
if os . path . exists ( p ) and not os . path . islink ( p ) and os . path . isdir ( p ) : 
~~~ shutil . rmtree ( p ) 
~~ elif os . path . exists ( p ) : 
~~~ os . unlink ( p ) 
~~ ~~ adios ( distinfo_path ) 
if not os . path . exists ( egginfo_path ) : 
~~~ import glob 
pat = os . path . join ( os . path . dirname ( egginfo_path ) , '*.egg-info' ) 
possible = glob . glob ( pat ) 
if possible : 
~~~ alt = os . path . basename ( possible [ 0 ] ) 
~~ raise ValueError ( err ) 
~~ if os . path . isfile ( egginfo_path ) : 
~~~ pkginfo_path = egginfo_path 
pkg_info = self . _pkginfo_to_metadata ( egginfo_path , egginfo_path ) 
os . mkdir ( distinfo_path ) 
~~~ pkginfo_path = os . path . join ( egginfo_path , 'PKG-INFO' ) 
pkg_info = self . _pkginfo_to_metadata ( egginfo_path , pkginfo_path ) 
shutil . copytree ( egginfo_path , distinfo_path , 
ignore = lambda x , y : set ( ( 'PKG-INFO' , 
'requires.txt' , 
'SOURCES.txt' , 
'not-zip-safe' , ) ) ) 
dependency_links_path = os . path . join ( distinfo_path , 'dependency_links.txt' ) 
with open ( dependency_links_path , 'r' ) as dependency_links_file : 
~~~ dependency_links = dependency_links_file . read ( ) . strip ( ) 
~~ if not dependency_links : 
~~~ adios ( dependency_links_path ) 
~~ ~~ write_pkg_info ( os . path . join ( distinfo_path , 'METADATA' ) , pkg_info ) 
metadata_path = os . path . join ( distinfo_path , 'METADATA' ) 
self . add_requirements ( metadata_path ) 
metadata_json_path = os . path . join ( distinfo_path , 'metadata.json' ) 
pymeta = pkginfo_to_dict ( metadata_path , 
distribution = self . distribution ) 
if 'description' in pymeta : 
~~~ description_filename = 'DESCRIPTION.rst' 
description_text = pymeta . pop ( 'description' ) 
description_path = os . path . join ( distinfo_path , 
description_filename ) 
with open ( description_path , "wb" ) as description_file : 
~~~ description_file . write ( description_text . encode ( 'utf-8' ) ) 
~~ pymeta [ 'extensions' ] [ 'python.details' ] [ 'document_names' ] [ 'description' ] = description_filename 
~~ license = self . license_file ( ) 
if license : 
~~~ license_filename = 'LICENSE.txt' 
shutil . copy ( license , os . path . join ( self . distinfo_dir , license_filename ) ) 
pymeta [ 'extensions' ] [ 'python.details' ] [ 'document_names' ] [ 'license' ] = license_filename 
~~ with open ( metadata_json_path , "w" ) as metadata_json : 
~~~ json . dump ( pymeta , metadata_json , sort_keys = True ) 
~~ adios ( egginfo_path ) 
~~ def get_conversations ( 
self , continuation_token = None , custom_headers = None , raw = False , ** operation_config ) : 
url = self . get_conversations . metadata [ 'url' ] 
query_parameters = { } 
if continuation_token is not None : 
~~~ query_parameters [ 'continuationToken' ] = self . _serialize . query ( "continuation_token" , continuation_token , 'str' ) 
~~ header_parameters = { } 
header_parameters [ 'Accept' ] = 'application/json' 
if custom_headers : 
~~~ header_parameters . update ( custom_headers ) 
~~ request = self . _client . get ( url , query_parameters , header_parameters ) 
response = self . _client . send ( request , stream = False , ** operation_config ) 
if response . status_code not in [ 200 ] : 
~~~ raise models . ErrorResponseException ( self . _deserialize , response ) 
~~ deserialized = None 
if response . status_code == 200 : 
~~~ deserialized = self . _deserialize ( 'ConversationsResult' , response ) 
~~ if raw : 
~~~ client_raw_response = ClientRawResponse ( deserialized , response ) 
return client_raw_response 
~~ return deserialized 
~~ def create_conversation ( 
self , parameters , custom_headers = None , raw = False , ** operation_config ) : 
url = self . create_conversation . metadata [ 'url' ] 
header_parameters = { } 
~~ body_content = self . _serialize . body ( parameters , 'ConversationParameters' ) 
request = self . _client . post ( url , query_parameters , header_parameters , body_content ) 
if response . status_code not in [ 200 , 201 , 202 ] : 
~~~ deserialized = self . _deserialize ( 'ConversationResourceResponse' , response ) 
~~ if response . status_code == 201 : 
~~ if response . status_code == 202 : 
~~ def get_conversation_paged_members ( 
self , conversation_id , page_size = None , continuation_token = None , custom_headers = None , raw = False , ** operation_config ) : 
url = self . get_conversation_paged_members . metadata [ 'url' ] 
path_format_arguments = { 
'conversationId' : self . _serialize . url ( "conversation_id" , conversation_id , 'str' ) 
url = self . _client . format_url ( url , ** path_format_arguments ) 
if page_size is not None : 
~~~ query_parameters [ 'pageSize' ] = self . _serialize . query ( "page_size" , page_size , 'int' ) 
~~ if continuation_token is not None : 
~~~ raise HttpOperationError ( self . _deserialize , response ) 
~~~ deserialized = self . _deserialize ( 'PagedMembersResult' , response ) 
~~ def track_pageview ( self , name : str , url , duration : int = 0 , properties : Dict [ str , object ] = None , 
~~ def track_exception ( self , type : type = None , value : Exception = None , tb : traceback = None , 
~~ def text ( text : str , speak : str = None , input_hint : Union [ InputHints , str ] = InputHints . accepting_input ) -> Activity : 
message = Activity ( type = ActivityTypes . message , text = text , input_hint = input_hint ) 
if speak : 
~~~ message . speak = speak 
~~ return message 
~~ def suggested_actions ( actions : List [ CardAction ] , text : str = None , speak : str = None , 
input_hint : Union [ InputHints , str ] = InputHints . accepting_input ) -> Activity : 
actions = SuggestedActions ( actions = actions ) 
message = Activity ( type = ActivityTypes . message , input_hint = input_hint , suggested_actions = actions ) 
if text : 
~~~ message . text = text 
~~ if speak : 
~~ def attachment ( attachment : Attachment , text : str = None , speak : str = None , 
input_hint : Union [ InputHints , str ] = None ) : 
return attachment_activity ( AttachmentLayoutTypes . list , [ attachment ] , text , speak , input_hint ) 
~~ def list ( attachments : List [ Attachment ] , text : str = None , speak : str = None , 
input_hint : Union [ InputHints , str ] = None ) -> Activity : 
return attachment_activity ( AttachmentLayoutTypes . list , attachments , text , speak , input_hint ) 
~~ def content_url ( url : str , content_type : str , name : str = None , text : str = None , speak : str = None , 
attachment = Attachment ( content_type = content_type , content_url = url ) 
if name : 
~~~ attachment . name = name 
~~ return attachment_activity ( AttachmentLayoutTypes . list , [ attachment ] , text , speak , input_hint ) 
~~ async def authenticate_token_service_url ( auth_header : str , credentials : CredentialProvider , service_url : str , channel_id : str ) -> ClaimsIdentity : 
identity = await asyncio . ensure_future ( 
ChannelValidation . authenticate_token ( auth_header , credentials , channel_id ) ) 
service_url_claim = identity . get_claim_value ( ChannelValidation . SERVICE_URL_CLAIM ) 
if service_url_claim != service_url : 
~~ return identity 
~~ async def authenticate_token ( auth_header : str , credentials : CredentialProvider , channel_id : str ) -> ClaimsIdentity : 
token_extractor = JwtTokenExtractor ( 
ChannelValidation . TO_BOT_FROM_CHANNEL_TOKEN_VALIDATION_PARAMETERS , 
Constants . TO_BOT_FROM_CHANNEL_OPEN_ID_METADATA_URL , 
Constants . ALLOWED_SIGNING_ALGORITHMS ) 
token_extractor . get_identity_from_auth_header ( auth_header , channel_id ) ) 
if not identity : 
~~ if not identity . isAuthenticated : 
~~ if identity . get_claim_value ( Constants . ISSUER_CLAIM ) != Constants . TO_BOT_FROM_CHANNEL_TOKEN_ISSUER : 
~~ aud_claim = identity . get_claim_value ( Constants . AUDIENCE_CLAIM ) 
is_valid_app_id = await asyncio . ensure_future ( credentials . is_valid_appid ( aud_claim or "" ) ) 
if not is_valid_app_id : 
~~ def create_trace ( 
turn_activity : Activity , 
name : str , 
value : object = None , 
value_type : str = None , 
label : str = None , 
) -> Activity : 
from_property = ( 
ChannelAccount ( 
id = turn_activity . recipient . id , name = turn_activity . recipient . name 
if turn_activity . recipient is not None 
else ChannelAccount ( ) 
if value_type is None and value is not None : 
~~~ value_type = type ( value ) . __name__ 
~~ reply = Activity ( 
type = ActivityTypes . trace , 
timestamp = datetime . utcnow ( ) , 
from_property = from_property , 
recipient = ChannelAccount ( 
id = turn_activity . from_property . id , name = turn_activity . from_property . name 
) , 
reply_to_id = turn_activity . id , 
service_url = turn_activity . service_url , 
channel_id = turn_activity . channel_id , 
conversation = ConversationAccount ( 
is_group = turn_activity . conversation . is_group , 
id = turn_activity . conversation . id , 
name = turn_activity . conversation . name , 
name = name , 
label = label , 
value_type = value_type , 
value = value , 
return reply 
~~ async def add ( self , dialog : Dialog ) : 
if dialog is None or not isinstance ( dialog , Dialog ) : 
~~ if dialog . id in self . _dialogs : 
~~ self . _dialogs [ dialog . id ] = dialog 
~~ async def find ( self , dialog_id : str ) -> Dialog : 
if ( not dialog_id ) : 
~~ if dialog_id in self . _dialogs : 
~~~ return self . _dialogs [ dialog_id ] 
~~ def get_storage_key ( self , context : TurnContext ) -> str : 
activity = context . activity 
channel_id = getattr ( activity , 'channel_id' , None ) 
user_id = getattr ( activity . from_property , 'id' , None ) if hasattr ( activity , 'from_property' ) else None 
storage_key = None 
if channel_id and user_id : 
~~~ storage_key = "%s/users/%s" % ( channel_id , user_id ) 
~~ return storage_key 
~~ def get_top_scoring_intent ( self ) -> TopIntent : 
if self . intents is None : 
~~ top_intent = TopIntent ( intent = "" , score = 0.0 ) 
for intent_name , intent_score in self . intents . items ( ) : 
~~~ score = intent_score . score 
if score > top_intent [ 1 ] : 
~~~ top_intent = TopIntent ( intent_name , score ) 
~~ ~~ return top_intent 
~~ def telemetry_client ( self , value : BotTelemetryClient ) -> None : 
~~~ self . _telemetry_client = NullTelemetryClient ( ) 
~~~ self . _telemetry_client = value 
~~ ~~ async def resume_dialog ( self , dc , reason : DialogReason , result : object ) : 
return await dc . EndDialog ( result ) 
~~ def from_application_endpoint ( cls , application_endpoint : str ) : 
( application_id , endpoint_key , endpoint ) = LuisApplication . _parse ( 
application_endpoint 
return cls ( application_id , endpoint_key , endpoint ) 
~~ def top_intent ( 
results : RecognizerResult , default_intent : str = "None" , min_score : float = 0.0 
) -> str : 
if results is None : 
~~ top_intent : str = None 
top_score : float = - 1.0 
if results . intents : 
~~~ for intent_name , intent_score in results . intents . items ( ) : 
if score > top_score and score >= min_score : 
~~~ top_intent = intent_name 
top_score = score 
~~ ~~ ~~ return top_intent or default_intent 
~~ async def recognize ( 
self , 
turn_context : TurnContext , 
telemetry_properties : Dict [ str , str ] = None , 
telemetry_metrics : Dict [ str , float ] = None , 
) -> RecognizerResult : 
return await self . _recognize_internal ( 
turn_context , telemetry_properties , telemetry_metrics 
~~ def on_recognizer_result ( 
recognizer_result : RecognizerResult , 
properties = self . fill_luis_event_properties ( 
recognizer_result , turn_context , telemetry_properties 
self . telemetry_client . track_event ( 
LuisTelemetryConstants . luis_result , properties , telemetry_metrics 
~~ def fill_luis_event_properties ( 
) -> Dict [ str , str ] : 
intents = recognizer_result . intents 
top_two_intents = ( 
sorted ( intents . keys ( ) , key = lambda k : intents [ k ] . score , reverse = True ) [ : 2 ] 
if intents 
else [ ] 
intent_name , intent_score = LuisRecognizer . _get_top_k_intent_score ( 
top_two_intents , intents , index = 0 
intent2_name , intent2_score = LuisRecognizer . _get_top_k_intent_score ( 
top_two_intents , intents , index = 1 
properties : Dict [ str , str ] = { 
LuisTelemetryConstants . application_id_property : self . _application . application_id , 
LuisTelemetryConstants . intent_property : intent_name , 
LuisTelemetryConstants . intent_score_property : intent_score , 
LuisTelemetryConstants . intent2_property : intent2_name , 
LuisTelemetryConstants . intent_score2_property : intent2_score , 
LuisTelemetryConstants . from_id_property : turn_context . activity . from_property . id , 
sentiment = recognizer_result . properties . get ( "sentiment" ) 
if sentiment is not None and isinstance ( sentiment , Dict ) : 
~~~ label = sentiment . get ( "label" ) 
if label is not None : 
~~~ properties [ LuisTelemetryConstants . sentiment_label_property ] = str ( label ) 
~~ score = sentiment . get ( "score" ) 
if score is not None : 
~~~ properties [ LuisTelemetryConstants . sentiment_score_property ] = str ( score ) 
~~ ~~ entities = None 
if recognizer_result . entities is not None : 
~~~ entities = json . dumps ( recognizer_result . entities ) 
~~ properties [ LuisTelemetryConstants . entities_property ] = entities 
if self . log_personal_information and turn_context . activity . text : 
~~~ properties [ 
LuisTelemetryConstants . question_property 
] = turn_context . activity . text 
~~ if telemetry_properties is not None : 
~~~ for key in telemetry_properties : 
~~~ properties [ key ] = telemetry_properties [ key ] 
~~ ~~ return properties 
~~ async def async_send_formdata ( self , request , headers = None , content = None , ** config ) : 
files = self . _prepare_send_formdata ( request , headers , content ) 
return await self . async_send ( request , headers , files = files , ** config ) 
~~ async def async_send ( self , request , headers = None , content = None , ** config ) : 
loop = asyncio . get_event_loop ( ) 
if self . config . keep_alive and self . _session is None : 
~~~ self . _session = requests . Session ( ) 
~~~ session = self . creds . signed_session ( self . _session ) 
~~~ session = self . creds . signed_session ( ) 
if self . _session is not None : 
~~~ _LOGGER . warning ( 
~~ ~~ kwargs = self . _configure_session ( session , ** config ) 
if headers : 
~~~ request . headers . update ( headers ) 
~~ if not kwargs . get ( 'files' ) : 
~~~ request . add_content ( content ) 
~~ if request . data : 
~~~ kwargs [ 'data' ] = request . data 
~~ kwargs [ 'headers' ] . update ( request . headers ) 
response = None 
~~~ future = loop . run_in_executor ( 
None , 
functools . partial ( 
session . request , 
request . method , 
request . url , 
** kwargs 
return await future 
~~ except ( oauth2 . rfc6749 . errors . InvalidGrantError , 
oauth2 . rfc6749 . errors . TokenExpiredError ) as err : 
_LOGGER . warning ( error ) 
~~~ session = self . creds . refresh_session ( ) 
kwargs = self . _configure_session ( session ) 
if request . data : 
future = loop . run_in_executor ( 
raise_with_traceback ( TokenExpiredError , msg , err ) 
~~ ~~ except ( requests . RequestException , 
oauth2 . rfc6749 . errors . OAuth2Error ) as err : 
raise_with_traceback ( ClientRequestError , msg , err ) 
~~~ self . _close_local_session_if_necessary ( response , session , kwargs [ 'stream' ] ) 
~~ ~~ def stream_download_async ( self , response , user_callback ) : 
block = self . config . connection . data_block_size 
return StreamDownloadGenerator ( response , user_callback , block ) 
~~ async def read ( self , keys : List [ str ] ) -> dict : 
~~~ if not self . __container_exists : 
~~~ self . __create_db_and_container ( ) 
~~ if len ( keys ) > 0 : 
~~~ parameters = [ 
{ 'name' : f'@id{i}' , 'value' : f'{self.__sanitize_key(key)}' } 
for i , key in enumerate ( keys ) 
parameter_sequence = ',' . join ( param . get ( 'name' ) 
for param in parameters ) 
query = { 
"query" : 
"parameters" : parameters 
options = { 'enableCrossPartitionQuery' : True } 
results = list ( 
self . client . QueryItems ( 
self . __container_link , query , options ) 
return { 
r . get ( 'realId' ) : self . __create_si ( r ) for r in results 
~~ ~~ except TypeError as e : 
~~ ~~ async def write ( self , changes : Dict [ str , StoreItem ] ) : 
~~ for ( key , change ) in changes . items ( ) : 
~~~ e_tag = change . e_tag 
doc = { 'id' : self . __sanitize_key ( key ) , 
'realId' : key , 
'document' : self . __create_dict ( change ) 
if ( e_tag == '*' or not e_tag ) : 
~~~ self . client . UpsertItem ( 
database_or_Container_link = self . __container_link , 
document = doc , 
options = { 'disableAutomaticIdGeneration' : True } 
~~ elif ( len ( e_tag ) > 0 ) : 
~~~ access_condition = { 'type' : 'IfMatch' , 'condition' : e_tag } 
self . client . ReplaceItem ( 
document_link = self . __item_link ( 
self . __sanitize_key ( key ) ) , 
new_document = doc , 
options = { 'accessCondition' : access_condition } 
~~ ~~ ~~ except Exception as e : 
~~ ~~ async def delete ( self , keys : List [ str ] ) : 
~~ for k in keys : 
~~~ self . client . DeleteItem ( 
document_link = self . __item_link ( self . __sanitize_key ( k ) ) ) 
~~ ~~ except cosmos_errors . HTTPFailure as h : 
~~~ if h . status_code != 404 : 
~~~ raise h 
~~ ~~ def __create_si ( self , result ) -> StoreItem : 
doc = result . get ( 'document' ) 
doc [ 'e_tag' ] = result . get ( '_etag' ) 
return StoreItem ( ** doc ) 
~~ def __create_dict ( self , si : StoreItem ) -> Dict : 
non_magic_attr = ( [ attr for attr in dir ( si ) 
if not attr . startswith ( '_' ) or attr . __eq__ ( 'e_tag' ) ] ) 
return ( { attr : getattr ( si , attr ) 
for attr in non_magic_attr } ) 
~~ def __sanitize_key ( self , key ) -> str : 
bad_chars = [ '\\\\' , '?' , '/' , '#' , '\\t' , '\\n' , '\\r' ] 
return '' . join ( 
map ( 
lambda x : '*' + str ( ord ( x ) ) if x in bad_chars else x , key 
~~ def __create_db_and_container ( self ) : 
db_id = self . config . database 
container_name = self . config . container 
self . db = self . __get_or_create_database ( self . client , db_id ) 
self . container = self . __get_or_create_container ( 
self . client , container_name 
~~ def __get_or_create_database ( self , doc_client , id ) -> str : 
dbs = list ( doc_client . QueryDatabases ( { 
"parameters" : [ 
{ "name" : "@id" , "value" : id } 
} ) ) 
if len ( dbs ) > 0 : 
~~~ return dbs [ 0 ] [ 'id' ] 
~~~ res = doc_client . CreateDatabase ( { 'id' : id } ) 
return res [ 'id' ] 
~~ ~~ def __get_or_create_container ( self , doc_client , container ) -> str : 
containers = list ( doc_client . QueryContainers ( 
self . __database_link , 
{ "name" : "@id" , "value" : container } 
) ) 
if len ( containers ) > 0 : 
~~~ return containers [ 0 ] [ 'id' ] 
~~~ res = doc_client . CreateContainer ( 
self . __database_link , { 'id' : container } ) 
~~ ~~ def fill_qna_event ( 
query_results : [ QueryResult ] , 
telemetry_metrics : Dict [ str , float ] = None 
) -> EventData : 
properties : Dict [ str , str ] = dict ( ) 
metrics : Dict [ str , float ] = dict ( ) 
properties [ QnATelemetryConstants . knowledge_base_id_property ] = self . _endpoint . knowledge_base_id 
text : str = turn_context . activity . text 
userName : str = turn_context . activity . from_property . name 
if self . log_personal_information : 
~~~ if text : 
~~~ properties [ QnATelemetryConstants . question_property ] = text 
~~ if userName : 
~~~ properties [ QnATelemetryConstants . username_property ] = userName 
~~ ~~ if len ( query_results ) > 0 : 
~~~ query_result = query_results [ 0 ] 
result_properties = { 
QnATelemetryConstants . matched_question_property : json . dumps ( query_result . questions ) , 
QnATelemetryConstants . question_id_property : str ( query_result . id ) , 
QnATelemetryConstants . answer_property : query_result . answer , 
QnATelemetryConstants . score_metric : query_result . score , 
QnATelemetryConstants . article_found_property : 'true' 
properties . update ( result_properties ) 
~~~ no_match_properties = { 
QnATelemetryConstants . article_found_property : 'false' 
properties . update ( no_match_properties ) 
~~ if telemetry_properties : 
~~~ properties . update ( telemetry_properties ) 
~~ if telemetry_metrics : 
~~~ metrics . update ( telemetry_metrics ) 
~~ return EventData ( properties = properties , metrics = metrics ) 
~~ async def get_answers ( 
context : TurnContext , 
options : QnAMakerOptions = None , 
telemetry_metrics : Dict [ str , int ] = None 
) -> [ QueryResult ] : 
hydrated_options = self . _hydrate_options ( options ) 
self . _validate_options ( hydrated_options ) 
result = self . _query_qna_service ( context . activity , hydrated_options ) 
await self . _emit_trace_info ( context , result , hydrated_options ) 
~~ def _hydrate_options ( self , query_options : QnAMakerOptions ) -> QnAMakerOptions : 
hydrated_options = copy ( self . _options ) 
if query_options : 
~~~ if ( 
query_options . score_threshold != hydrated_options . score_threshold 
and query_options . score_threshold 
~~~ hydrated_options . score_threshold = query_options . score_threshold 
~~ if ( query_options . top != hydrated_options . top and query_options . top != 0 ) : 
~~~ hydrated_options . top = query_options . top 
~~ if ( len ( query_options . strict_filters ) > 0 ) : 
~~~ hydrated_options . strict_filters = query_options . strict_filters 
~~ ~~ return hydrated_options 
~~ def copy_to ( self , context : 'TurnContext' ) -> None : 
for attribute in [ 'adapter' , 'activity' , '_responded' , '_services' , 
'_on_send_activities' , '_on_update_activity' , '_on_delete_activity' ] : 
~~~ setattr ( context , attribute , getattr ( self , attribute ) ) 
~~ ~~ def activity ( self , value ) : 
if not isinstance ( value , Activity ) : 
~~~ self . _activity = value 
~~ ~~ def has ( self , key : str ) -> bool : 
if key in self . _services : 
~~ def set ( self , key : str , value : object ) -> None : 
if not key or not isinstance ( key , str ) : 
~~~ raise KeyError ( \ ) 
~~ self . _services [ key ] = value 
~~ async def send_activity ( self , * activity_or_text : Union [ Activity , str ] ) -> ResourceResponse : 
reference = TurnContext . get_conversation_reference ( self . activity ) 
output = [ TurnContext . apply_conversation_reference ( 
Activity ( text = a , type = 'message' ) if isinstance ( a , str ) else a , reference ) 
for a in activity_or_text ] 
for activity in output : 
~~~ activity . input_hint = 'acceptingInput' 
~~ async def callback ( context : 'TurnContext' , output ) : 
~~~ responses = await context . adapter . send_activities ( context , output ) 
context . _responded = True 
~~ await self . _emit ( self . _on_send_activities , output , callback ( self , output ) ) 
~~ async def update_activity ( self , activity : Activity ) : 
return await self . _emit ( self . _on_update_activity , activity , self . adapter . update_activity ( self , activity ) ) 
~~ async def delete_activity ( self , id_or_reference : Union [ str , ConversationReference ] ) : 
if type ( id_or_reference ) == str : 
~~~ reference = TurnContext . get_conversation_reference ( self . activity ) 
reference . activity_id = id_or_reference 
~~~ reference = id_or_reference 
~~ return await self . _emit ( self . _on_delete_activity , reference , self . adapter . delete_activity ( self , reference ) ) 
~~ def get_conversation_reference ( activity : Activity ) -> ConversationReference : 
return ConversationReference ( activity_id = activity . id , 
user = copy ( activity . from_property ) , 
bot = copy ( activity . recipient ) , 
conversation = copy ( activity . conversation ) , 
channel_id = activity . channel_id , 
service_url = activity . service_url ) 
~~ def apply_conversation_reference ( activity : Activity , 
reference : ConversationReference , 
is_incoming : bool = False ) -> Activity : 
activity . channel_id = reference . channel_id 
activity . service_url = reference . service_url 
activity . conversation = reference . conversation 
if is_incoming : 
~~~ activity . from_property = reference . user 
activity . recipient = reference . bot 
if reference . activity_id : 
~~~ activity . id = reference . activity_id 
~~~ activity . from_property = reference . bot 
activity . recipient = reference . user 
~~~ activity . reply_to_id = reference . activity_id 
~~ ~~ return activity 
~~ def add_step ( self , step ) : 
if not step : 
~~ self . _steps . append ( step ) 
~~ def get_step_name ( self , index : int ) -> str : 
step_name = self . _steps [ index ] . __qualname__ 
if not step_name or ">" in step_name : 
~~ return step_name 
~~ def calculate_change_hash ( item : StoreItem ) -> str : 
cpy = copy ( item ) 
if cpy . e_tag is not None : 
~~~ del cpy . e_tag 
~~ return str ( cpy ) 
~~ async def begin_dialog ( self , dialog_id : str , options : object = None ) : 
~~ dialog = await self . find_dialog ( dialog_id ) 
if dialog is None : 
~~~ raise Exception ( "\ 
~~ instance = DialogInstance ( ) 
instance . id = dialog_id 
instance . state = { } 
self . _stack . append ( instance ) 
return await dialog . begin_dialog ( self , options ) 
~~ async def prompt ( self , dialog_id : str , options ) -> DialogTurnResult : 
~~ if ( not options ) : 
~~ return await self . begin_dialog ( dialog_id , options ) 
~~ async def continue_dialog ( self ) : 
if self . active_dialog != None : 
~~~ dialog = await self . find_dialog ( self . active_dialog . id ) 
if not dialog : 
~~ return await dialog . continue_dialog ( self ) 
~~~ return DialogTurnResult ( DialogTurnStatus . Empty ) 
~~ ~~ async def end_dialog ( self , result : object = None ) : 
await self . end_active_dialog ( DialogReason . EndCalled ) 
~~ return await dialog . resume_dialog ( self , DialogReason . EndCalled , result ) 
~~~ return DialogTurnResult ( DialogTurnStatus . Complete , result ) 
~~ ~~ async def cancel_all_dialogs ( self ) : 
if ( len ( self . stack ) > 0 ) : 
~~~ while ( len ( self . stack ) > 0 ) : 
~~~ await self . end_active_dialog ( DialogReason . CancelCalled ) 
~~ return DialogTurnResult ( DialogTurnStatus . Cancelled ) 
~~ ~~ async def find_dialog ( self , dialog_id : str ) -> Dialog : 
dialog = await self . dialogs . find ( dialog_id ) 
if ( dialog == None and self . parent != None ) : 
~~~ dialog = self . parent . find_dialog ( dialog_id ) 
~~ return dialog 
~~ async def replace_dialog ( self , dialog_id : str , options : object = None ) -> DialogTurnResult : 
await self . end_active_dialog ( DialogReason . ReplaceCalled ) 
return await self . begin_dialog ( dialog_id , options ) 
~~ async def reprompt_dialog ( self ) : 
~~ await dialog . reprompt_dialog ( self . context , self . active_dialog ) 
~~ ~~ def supports_suggested_actions ( channel_id : str , button_cnt : int = 100 ) -> bool : 
max_actions = { 
Channels . facebook : 10 , 
Channels . skype : 10 , 
Channels . line : 13 , 
Channels . kik : 20 , 
Channels . telegram : 100 , 
Channels . slack : 100 , 
Channels . emulator : 100 , 
Channels . direct_line : 100 , 
Channels . webchat : 100 , 
return button_cnt <= max_actions [ channel_id ] if channel_id in max_actions else False 
~~ def supports_card_actions ( channel_id : str , button_cnt : int = 100 ) -> bool : 
Channels . facebook : 3 , 
Channels . skype : 3 , 
Channels . ms_teams : 3 , 
Channels . line : 99 , 
Channels . cortana : 100 , 
~~ def get_channel_id ( turn_context : TurnContext ) -> str : 
if turn_context . activity . channel_id is None : 
~~~ return "" 
~~~ return turn_context . activity . channel_id 
~~ ~~ async def process_activity ( self , logic : Callable ) : 
~~~ msg = input ( ) 
if msg is None : 
~~~ self . _next_id += 1 
activity = Activity ( text = msg , 
channel_id = 'console' , 
from_property = ChannelAccount ( id = 'user' , name = 'User1' ) , 
recipient = ChannelAccount ( id = 'bot' , name = 'Bot' ) , 
conversation = ConversationAccount ( id = 'Convo1' ) , 
type = ActivityTypes . message , 
timestamp = datetime . datetime . now ( ) , 
id = str ( self . _next_id ) ) 
activity = TurnContext . apply_conversation_reference ( activity , self . reference , True ) 
context = TurnContext ( self , activity ) 
await self . run_middleware ( context , logic ) 
~~ ~~ ~~ async def send_activities ( self , context : TurnContext , activities : List [ Activity ] ) : 
if context is None : 
~~ if type ( activities ) != list : 
~~ if len ( activities ) == 0 : 
~~ async def next_activity ( i : int ) : 
~~~ responses = [ ] 
if i < len ( activities ) : 
~~~ responses . append ( ResourceResponse ( ) ) 
a = activities [ i ] 
if a . type == 'delay' : 
~~~ await asyncio . sleep ( a . delay ) 
await next_activity ( i + 1 ) 
~~ elif a . type == ActivityTypes . message : 
~~~ if a . attachments is not None and len ( a . attachments ) > 0 : 
~~~ print ( a . text ) 
~~ await next_activity ( i + 1 ) 
~~~ print ( f'[{a.type}]' ) 
~~~ return responses 
~~ ~~ await next_activity ( 0 ) 
~~ def is_token_from_emulator ( auth_header : str ) -> bool : 
if len ( parts ) != 2 : 
~~ auth_scheme = parts [ 0 ] 
bearer_token = parts [ 1 ] 
if auth_scheme != 'Bearer' : 
~~ token = jwt . decode ( bearer_token , verify = False ) 
if not token : 
~~ issuer = token [ 'iss' ] 
if not issuer : 
~~ issuer_list = EmulatorValidation . TO_BOT_FROM_EMULATOR_TOKEN_VALIDATION_PARAMETERS . issuer 
if issuer_list and not issuer in issuer_list : 
~~ async def authenticate_emulator_token ( auth_header : str , credentials : CredentialProvider , channel_id : str ) -> ClaimsIdentity : 
EmulatorValidation . TO_BOT_FROM_EMULATOR_TOKEN_VALIDATION_PARAMETERS , 
Constants . TO_BOT_FROM_EMULATOR_OPEN_ID_METADATA_URL , 
~~ version_claim = identity . get_claim_value ( EmulatorValidation . VERSION_CLAIM ) 
if version_claim is None : 
~~~ raise Exception ( \ ) 
~~ app_id = '' 
if not version_claim or version_claim == '1.0' : 
~~~ app_id_claim = identity . get_claim_value ( EmulatorValidation . APP_ID_CLAIM ) 
if not app_id_claim : 
~~ app_id = app_id_claim 
~~ elif version_claim == '2.0' : 
~~~ app_authz_claim = identity . get_claim_value ( Constants . AUTHORIZED_PARTY ) 
if not app_authz_claim : 
~~ app_id = app_authz_claim 
~~ is_valid_app_id = await asyncio . ensure_future ( credentials . is_valid_appid ( app_id ) ) 
~~ def adaptive_card ( card : dict ) -> Attachment : 
if not type ( card ) == dict : 
'attachment.' ) 
~~ return Attachment ( content_type = CardFactory . content_types . adaptive_card , 
content = card ) 
~~ def animation_card ( card : AnimationCard ) -> Attachment : 
if not isinstance ( card , AnimationCard ) : 
~~ return Attachment ( content_type = CardFactory . content_types . animation_card , 
~~ def audio_card ( card : AudioCard ) -> Attachment : 
if not isinstance ( card , AudioCard ) : 
~~ return Attachment ( content_type = CardFactory . content_types . audio_card , 
~~ def hero_card ( card : HeroCard ) -> Attachment : 
if not isinstance ( card , HeroCard ) : 
~~ return Attachment ( content_type = CardFactory . content_types . hero_card , 
~~ def oauth_card ( card : OAuthCard ) -> Attachment : 
if not isinstance ( card , OAuthCard ) : 
~~ return Attachment ( content_type = CardFactory . content_types . oauth_card , 
~~ def receipt_card ( card : ReceiptCard ) -> Attachment : 
if not isinstance ( card , ReceiptCard ) : 
~~ return Attachment ( content_type = CardFactory . content_types . receipt_card , 
~~ def signin_card ( card : SigninCard ) -> Attachment : 
if not isinstance ( card , SigninCard ) : 
~~ return Attachment ( content_type = CardFactory . content_types . signin_card , 
~~ def thumbnail_card ( card : ThumbnailCard ) -> Attachment : 
if not isinstance ( card , ThumbnailCard ) : 
~~ return Attachment ( content_type = CardFactory . content_types . thumbnail_card , 
~~ def video_card ( card : VideoCard ) -> Attachment : 
if not isinstance ( card , VideoCard ) : 
~~ return Attachment ( content_type = CardFactory . content_types . video_card , 
~~ def params ( self ) : 
if self . _definition and not self . _params : 
~~~ self . _params = [ ] 
for sub_instr , _ , _ in self . _definition : 
~~ return self . _params 
~~~ return self . _params 
~~ ~~ def assemble ( self ) : 
instruction = QasmQobjInstruction ( name = self . name ) 
if self . params : 
~~~ params = [ 
x . evalf ( ) if hasattr ( x , 'evalf' ) else x for x in self . params 
params = [ 
sympy . matrix2numpy ( x , dtype = complex ) if isinstance ( 
x , sympy . Matrix ) else x for x in params 
instruction . params = params 
~~ if self . num_qubits : 
~~~ instruction . qubits = list ( range ( self . num_qubits ) ) 
~~ if self . num_clbits : 
~~~ instruction . memory = list ( range ( self . num_clbits ) ) 
~~ if self . control : 
~~~ instruction . _control = self . control 
~~ return instruction 
~~ def mirror ( self ) : 
if not self . _definition : 
~~~ return self . copy ( ) 
~~ reverse_inst = self . copy ( name = self . name + '_mirror' ) 
reverse_inst . definition = [ ] 
for inst , qargs , cargs in reversed ( self . _definition ) : 
~~~ reverse_inst . _definition . append ( ( inst . mirror ( ) , qargs , cargs ) ) 
~~ return reverse_inst 
if not self . definition : 
~~ inverse_gate = self . copy ( name = self . name + '_dg' ) 
inverse_gate . _definition = [ ] 
~~~ inverse_gate . _definition . append ( ( inst . inverse ( ) , qargs , cargs ) ) 
~~ return inverse_gate 
~~ def c_if ( self , classical , val ) : 
if not isinstance ( classical , ClassicalRegister ) : 
~~ if val < 0 : 
~~ self . control = ( classical , val ) 
~~ def copy ( self , name = None ) : 
cpy = copy . copy ( self ) 
~~~ cpy . name = name 
~~ return cpy 
~~ def _qasmif ( self , string ) : 
if self . control is None : 
~~~ return string 
~~ def qasm ( self ) : 
name_param = self . name 
~~~ name_param = "%s(%s)" % ( name_param , "," . join ( 
[ str ( i ) for i in self . params ] ) ) 
~~ return self . _qasmif ( name_param ) 
~~ def _join_options ( self , passset_options ) : 
passmanager_level = { k : v for k , v in self . passmanager_options . items ( ) if v is not None } 
passset_level = { k : v for k , v in passset_options . items ( ) if v is not None } 
return { ** default , ** passmanager_level , ** passset_level } 
~~ def append ( self , passes , ignore_requires = None , ignore_preserves = None , max_iteration = None , 
** flow_controller_conditions ) : 
passset_options = { 'ignore_requires' : ignore_requires , 
'ignore_preserves' : ignore_preserves , 
'max_iteration' : max_iteration } 
options = self . _join_options ( passset_options ) 
if isinstance ( passes , BasePass ) : 
~~~ passes = [ passes ] 
~~ for pass_ in passes : 
~~~ if not isinstance ( pass_ , BasePass ) : 
~~ ~~ for name , param in flow_controller_conditions . items ( ) : 
~~~ if callable ( param ) : 
~~~ flow_controller_conditions [ name ] = partial ( param , self . fenced_property_set ) 
~~ ~~ self . working_list . append ( 
FlowController . controller_factory ( passes , options , ** flow_controller_conditions ) ) 
~~ def run ( self , circuit ) : 
name = circuit . name 
dag = circuit_to_dag ( circuit ) 
del circuit 
for passset in self . working_list : 
~~~ for pass_ in passset : 
~~~ dag = self . _do_pass ( pass_ , dag , passset . options ) 
~~ ~~ circuit = dag_to_circuit ( dag ) 
circuit . name = name 
return circuit 
~~ def _do_pass ( self , pass_ , dag , options ) : 
if not options [ "ignore_requires" ] : 
~~~ for required_pass in pass_ . requires : 
~~~ dag = self . _do_pass ( required_pass , dag , options ) 
~~ ~~ if pass_ not in self . valid_passes : 
~~~ if pass_ . is_transformation_pass : 
~~~ pass_ . property_set = self . fenced_property_set 
new_dag = pass_ . run ( dag ) 
if not isinstance ( new_dag , DAGCircuit ) : 
type ( new_dag ) ) ) 
~~ dag = new_dag 
~~ elif pass_ . is_analysis_pass : 
~~~ pass_ . property_set = self . property_set 
pass_ . run ( FencedDAGCircuit ( dag ) ) 
~~ self . _update_valid_passes ( pass_ , options [ 'ignore_preserves' ] ) 
~~ return dag 
~~ def passes ( self ) : 
for pass_ in self . working_list : 
~~~ ret . append ( pass_ . dump_passes ( ) ) 
~~ def dump_passes ( self ) : 
ret = { 'options' : self . options , 'passes' : [ ] , 'type' : type ( self ) } 
for pass_ in self . _passes : 
~~~ if isinstance ( pass_ , FlowController ) : 
~~~ ret [ 'passes' ] . append ( pass_ . dump_passes ( ) ) 
~~~ ret [ 'passes' ] . append ( pass_ ) 
~~ def remove_flow_controller ( cls , name ) : 
if name not in cls . registered_controllers : 
~~ del cls . registered_controllers [ name ] 
~~ def controller_factory ( cls , passes , options , ** partial_controller ) : 
if None in partial_controller . values ( ) : 
~~ if partial_controller : 
~~~ for registered_controller in cls . registered_controllers . keys ( ) : 
~~~ if registered_controller in partial_controller : 
~~~ return cls . registered_controllers [ registered_controller ] ( passes , options , 
** partial_controller ) 
~~~ return FlowControllerLinear ( passes , options ) 
~~ ~~ def u_base ( self , theta , phi , lam , q ) : 
return self . append ( UBase ( theta , phi , lam ) , [ q ] , [ ] ) 
~~ def to_matrix ( self ) : 
theta , phi , lam = self . params 
return numpy . array ( 
[ [ 
numpy . cos ( theta / 2 ) , 
- numpy . exp ( 1j * lam ) * numpy . sin ( theta / 2 ) 
[ 
numpy . exp ( 1j * phi ) * numpy . sin ( theta / 2 ) , 
numpy . exp ( 1j * ( phi + lam ) ) * numpy . cos ( theta / 2 ) 
] ] , 
dtype = complex ) 
~~ def single_gate_params ( gate , params = None ) : 
if gate in ( 'U' , 'u3' ) : 
~~~ return params [ 0 ] , params [ 1 ] , params [ 2 ] 
~~ elif gate == 'u2' : 
~~~ return np . pi / 2 , params [ 0 ] , params [ 1 ] 
~~ elif gate == 'u1' : 
~~~ return 0 , 0 , params [ 0 ] 
~~ elif gate == 'id' : 
~~~ return 0 , 0 , 0 
~~ def single_gate_matrix ( gate , params = None ) : 
( theta , phi , lam ) = map ( float , single_gate_params ( gate , params ) ) 
return np . array ( [ [ np . cos ( theta / 2 ) , 
- np . exp ( 1j * lam ) * np . sin ( theta / 2 ) ] , 
[ np . exp ( 1j * phi ) * np . sin ( theta / 2 ) , 
np . exp ( 1j * phi + 1j * lam ) * np . cos ( theta / 2 ) ] ] ) 
~~ def einsum_matmul_index ( gate_indices , number_of_qubits ) : 
mat_l , mat_r , tens_lin , tens_lout = _einsum_matmul_index_helper ( gate_indices , 
number_of_qubits ) 
tens_r = ascii_uppercase [ : number_of_qubits ] 
tens_lout = tens_lout , 
tens_r = tens_r ) 
~~ def einsum_vecmul_index ( gate_indices , number_of_qubits ) : 
tens_lout = tens_lout ) 
~~ def _einsum_matmul_index_helper ( gate_indices , number_of_qubits ) : 
if len ( gate_indices ) + number_of_qubits > 26 : 
~~ tens_in = ascii_lowercase [ : number_of_qubits ] 
tens_out = list ( tens_in ) 
mat_left = "" 
mat_right = "" 
for pos , idx in enumerate ( reversed ( gate_indices ) ) : 
~~~ mat_left += ascii_lowercase [ - 1 - pos ] 
mat_right += tens_in [ - 1 - idx ] 
tens_out [ - 1 - idx ] = ascii_lowercase [ - 1 - pos ] 
~~ tens_out = "" . join ( tens_out ) 
return mat_left , mat_right , tens_in , tens_out 
~~ def circuit_to_dag ( circuit ) : 
dagcircuit = DAGCircuit ( ) 
dagcircuit . name = circuit . name 
for register in circuit . qregs : 
~~~ dagcircuit . add_qreg ( register ) 
~~ for register in circuit . cregs : 
~~~ dagcircuit . add_creg ( register ) 
~~ for instruction , qargs , cargs in circuit . data : 
~~~ if instruction . control is None : 
~~~ control = None 
~~~ control = ( instruction . control [ 0 ] , instruction . control [ 1 ] ) 
~~ dagcircuit . apply_operation_back ( instruction . copy ( ) , 
qargs , cargs , control ) 
~~ return dagcircuit 
~~ def exp_fit_fun ( x , a , tau , c ) : 
return a * np . exp ( - x / tau ) + c 
~~ def osc_fit_fun ( x , a , tau , f , phi , c ) : 
return a * np . exp ( - x / tau ) * np . cos ( 2 * np . pi * f * x + phi ) + c 
~~ def plot_coherence ( xdata , ydata , std_error , fit , fit_function , xunit , exp_str , 
qubit_label ) : 
if not HAS_MATPLOTLIB : 
~~ plt . errorbar ( xdata , ydata , std_error , marker = '.' , 
markersize = 9 , c = 'b' , linestyle = '' ) 
plt . plot ( xdata , fit_function ( xdata , * fit ) , c = 'r' , linestyle = '--' , 
plt . xticks ( fontsize = 14 , rotation = 70 ) 
plt . yticks ( fontsize = 14 ) 
plt . ylabel ( 'P(1)' , fontsize = 16 ) 
plt . legend ( fontsize = 12 ) 
plt . grid ( True ) 
plt . show ( ) 
~~ def shape_rb_data ( raw_rb ) : 
rb_data = [ ] 
rb_data . append ( np . mean ( raw_rb , 0 ) ) 
rb_data . append ( np . std ( raw_rb , 0 ) ) 
return rb_data 
~~ def rb_epc ( fit , rb_pattern ) : 
for patterns in rb_pattern : 
~~~ for qubit in patterns : 
~~~ fitalpha = fit [ 'q%d' % qubit ] [ 'fit' ] [ 1 ] 
fitalphaerr = fit [ 'q%d' % qubit ] [ 'fiterr' ] [ 1 ] 
nrb = 2 ** len ( patterns ) 
fit [ 'q%d' % qubit ] [ 'fit_calcs' ] = { } 
fit [ 'q%d' % qubit ] [ 'fit_calcs' ] [ 'epc' ] = [ ( nrb - 1 ) / nrb * ( 1 - fitalpha ) , 
fitalphaerr / fitalpha ] 
fit [ 'q%d' % qubit ] [ 'fit_calcs' ] [ 'epc' ] [ 1 ] *= fit [ 'q%d' % qubit ] [ 'fit_calcs' ] [ 'epc' ] [ 0 ] 
~~ ~~ return fit 
~~ def plot_rb_data ( xdata , ydatas , yavg , yerr , fit , survival_prob , ax = None , 
show_plt = True ) : 
~~ if ax is None : 
~~~ plt . figure ( ) 
~~ for ydata in ydatas : 
~~~ ax . plot ( xdata , ydata , color = 'gray' , linestyle = 'none' , marker = 'x' ) 
~~ ax . errorbar ( xdata , yavg , yerr = yerr , color = 'r' , linestyle = '--' , linewidth = 3 ) 
ax . plot ( xdata , survival_prob ( xdata , * fit ) , color = 'blue' , linestyle = '-' , linewidth = 2 ) 
ax . tick_params ( labelsize = 14 ) 
ax . set_ylabel ( 'Z' , fontsize = 16 ) 
ax . grid ( True ) 
if show_plt : 
~~ ~~ def _split_runs_on_parameters ( runs ) : 
def _is_dagnode_parameterized ( node ) : 
~~~ return any ( isinstance ( param , Parameter ) for param in node . op . params ) 
~~ out = [ ] 
for run in runs : 
~~~ groups = groupby ( run , _is_dagnode_parameterized ) 
for group_is_parameterized , gates in groups : 
~~~ if not group_is_parameterized : 
~~~ out . append ( list ( gates ) ) 
~~ ~~ ~~ return out 
~~ def run ( self , dag ) : 
runs = dag . collect_runs ( [ "u1" , "u2" , "u3" , "id" ] ) 
runs = _split_runs_on_parameters ( runs ) 
~~~ right_name = "u1" 
for current_node in run : 
~~~ left_name = current_node . name 
if ( current_node . condition is not None 
or len ( current_node . qargs ) != 1 
or left_name not in [ "u1" , "u2" , "u3" , "id" ] ) : 
~~ if left_name == "u1" : 
~~~ left_parameters = ( 0 , 0 , current_node . op . params [ 0 ] ) 
~~ elif left_name == "u2" : 
~~~ left_parameters = ( np . pi / 2 , current_node . op . params [ 0 ] , 
current_node . op . params [ 1 ] ) 
~~ elif left_name == "u3" : 
~~~ left_parameters = tuple ( current_node . op . params ) 
left_parameters = ( 0 , 0 , 0 ) 
~~ left_parameters = tuple ( [ float ( x ) for x in left_parameters ] ) 
name_tuple = ( left_name , right_name ) 
if name_tuple == ( "u1" , "u1" ) : 
~~~ right_parameters = ( 0 , 0 , right_parameters [ 2 ] + 
left_parameters [ 2 ] ) 
~~ elif name_tuple == ( "u1" , "u2" ) : 
~~~ right_parameters = ( np . pi / 2 , right_parameters [ 1 ] + 
left_parameters [ 2 ] , right_parameters [ 2 ] ) 
~~ elif name_tuple == ( "u2" , "u1" ) : 
~~~ right_name = "u2" 
right_parameters = ( np . pi / 2 , left_parameters [ 1 ] , 
right_parameters [ 2 ] + left_parameters [ 2 ] ) 
~~ elif name_tuple == ( "u1" , "u3" ) : 
~~~ right_parameters = ( right_parameters [ 0 ] , right_parameters [ 1 ] + 
~~ elif name_tuple == ( "u3" , "u1" ) : 
~~~ right_name = "u3" 
right_parameters = ( left_parameters [ 0 ] , left_parameters [ 1 ] , 
~~ elif name_tuple == ( "u2" , "u2" ) : 
right_parameters = ( np . pi - left_parameters [ 2 ] - 
right_parameters [ 1 ] , left_parameters [ 1 ] + 
np . pi / 2 , right_parameters [ 2 ] + 
np . pi / 2 ) 
~~ elif name_tuple [ 1 ] == "nop" : 
~~~ right_name = left_name 
right_parameters = left_parameters 
right_parameters = Optimize1qGates . compose_u3 ( left_parameters [ 0 ] , 
left_parameters [ 1 ] , 
left_parameters [ 2 ] , 
right_parameters [ 0 ] , 
right_parameters [ 1 ] , 
right_parameters [ 2 ] ) 
~~ if np . mod ( right_parameters [ 0 ] , ( 2 * np . pi ) ) == 0 and right_name != "u1" : 
right_parameters = ( 0 , 0 , right_parameters [ 1 ] + 
right_parameters [ 2 ] + 
right_parameters [ 0 ] ) 
~~ if right_name == "u3" : 
~~~ if np . mod ( ( right_parameters [ 0 ] - np . pi / 2 ) , ( 2 * np . pi ) ) == 0 : 
right_parameters = ( np . pi / 2 , right_parameters [ 1 ] , 
( right_parameters [ 0 ] - np . pi / 2 ) ) 
~~ if np . mod ( ( right_parameters [ 0 ] + np . pi / 2 ) , ( 2 * np . pi ) ) == 0 : 
right_parameters = ( np . pi / 2 , right_parameters [ 1 ] + 
np . pi , right_parameters [ 2 ] - 
np . pi + ( right_parameters [ 0 ] + 
np . pi / 2 ) ) 
~~ ~~ if right_name == "u1" and np . mod ( right_parameters [ 2 ] , ( 2 * np . pi ) ) == 0 : 
~~~ right_name = "nop" 
~~ ~~ run_qarg = ( QuantumRegister ( 1 , 'q' ) , 0 ) 
new_op = Gate ( name = "" , num_qubits = 1 , params = [ ] ) 
if right_name == "u1" : 
~~~ new_op = U1Gate ( right_parameters [ 2 ] ) 
~~ if right_name == "u2" : 
~~~ new_op = U2Gate ( right_parameters [ 1 ] , right_parameters [ 2 ] ) 
~~~ new_op = U3Gate ( * right_parameters ) 
~~ if right_name != 'nop' : 
~~~ new_dag = DAGCircuit ( ) 
new_dag . add_qreg ( run_qarg [ 0 ] ) 
new_dag . apply_operation_back ( new_op , [ run_qarg ] , [ ] ) 
dag . substitute_node_with_dag ( run [ 0 ] , new_dag ) 
~~ for current_node in run [ 1 : ] : 
~~~ dag . remove_op_node ( current_node ) 
~~ if right_name == "nop" : 
~~~ dag . remove_op_node ( run [ 0 ] ) 
~~ ~~ return dag 
~~ def compose_u3 ( theta1 , phi1 , lambda1 , theta2 , phi2 , lambda2 ) : 
thetap , phip , lambdap = Optimize1qGates . yzy_to_zyz ( ( lambda1 + phi2 ) , theta1 , theta2 ) 
( theta , phi , lamb ) = ( thetap , phi1 + phip , lambda2 + lambdap ) 
return ( theta , phi , lamb ) 
quaternion_yzy = quaternion_from_euler ( [ theta1 , xi , theta2 ] , 'yzy' ) 
euler = quaternion_yzy . to_zyz ( ) 
quaternion_zyz = quaternion_from_euler ( euler , 'zyz' ) 
out_angles = ( euler [ 1 ] , euler [ 0 ] , euler [ 2 ] ) 
abs_inner = abs ( quaternion_zyz . data . dot ( quaternion_yzy . data ) ) 
if not np . allclose ( abs_inner , 1 , eps ) : 
~~ out_angles = tuple ( 0 if np . abs ( angle ) < _CHOP_THRESHOLD else angle 
for angle in out_angles ) 
return out_angles 
self . layout = self . layout or self . property_set . get ( 'layout' ) 
if self . layout is None : 
~~ layout_physical_qubits = self . layout . get_physical_bits ( ) . keys ( ) 
coupling_physical_qubits = self . coupling_map . physical_qubits 
idle_physical_qubits = [ q for q in coupling_physical_qubits 
if q not in layout_physical_qubits ] 
if idle_physical_qubits : 
~~~ if self . ancilla_name in dag . qregs : 
~~~ save_prefix = QuantumRegister . prefix 
QuantumRegister . prefix = self . ancilla_name 
qreg = QuantumRegister ( len ( idle_physical_qubits ) ) 
QuantumRegister . prefix = save_prefix 
~~~ qreg = QuantumRegister ( len ( idle_physical_qubits ) , name = self . ancilla_name ) 
~~ ~~ for idx , idle_q in enumerate ( idle_physical_qubits ) : 
~~~ self . property_set [ 'layout' ] [ idle_q ] = qreg [ idx ] 
~~ def _validate_input_state ( quantum_state ) : 
rho = np . asarray ( quantum_state ) 
if rho . ndim == 1 : 
~~~ rho = np . outer ( rho , np . conj ( rho ) ) 
~~ shape = np . shape ( rho ) 
if len ( shape ) != 2 or shape [ 0 ] != shape [ 1 ] : 
~~ num = int ( np . log2 ( rho . shape [ 0 ] ) ) 
if 2 ** num != rho . shape [ 0 ] : 
~~ return rho 
~~ def _trim ( image ) : 
background = PIL . Image . new ( image . mode , image . size , image . getpixel ( ( 0 , 0 ) ) ) 
diff = PIL . ImageChops . difference ( image , background ) 
diff = PIL . ImageChops . add ( diff , diff , 2.0 , - 100 ) 
bbox = diff . getbbox ( ) 
if bbox : 
~~~ image = image . crop ( bbox ) 
~~ return image 
~~ def _get_layered_instructions ( circuit , reverse_bits = False , justify = None ) : 
if justify : 
~~~ justify = justify . lower ( ) 
~~ justify = justify if justify in ( 'right' , 'none' ) else 'left' 
ops = [ ] 
qregs = [ ] 
cregs = [ ] 
for qreg in dag . qregs . values ( ) : 
~~~ qregs += [ ( qreg , bitno ) for bitno in range ( qreg . size ) ] 
~~ for creg in dag . cregs . values ( ) : 
~~~ cregs += [ ( creg , bitno ) for bitno in range ( creg . size ) ] 
~~ if justify == 'none' : 
~~~ for node in dag . topological_op_nodes ( ) : 
~~~ ops . append ( [ node ] ) 
~~ ~~ if justify == 'left' : 
~~~ for dag_layer in dag . layers ( ) : 
~~~ layers = [ ] 
current_layer = [ ] 
dag_nodes = dag_layer [ 'graph' ] . op_nodes ( ) 
dag_nodes . sort ( key = lambda nd : nd . _node_id ) 
for node in dag_nodes : 
~~~ multibit_gate = len ( node . qargs ) + len ( node . cargs ) > 1 
if multibit_gate : 
~~~ gate_span = _get_gate_span ( qregs , node ) 
all_indices = [ ] 
for check_node in dag_nodes : 
~~~ if check_node != node : 
~~~ all_indices += _get_gate_span ( qregs , check_node ) 
~~ ~~ if any ( i in gate_span for i in all_indices ) : 
~~~ layers . append ( [ node ] ) 
~~~ current_layer . append ( node ) 
~~ ~~ if current_layer : 
~~~ layers . append ( current_layer ) 
~~ ops += layers 
~~ ~~ if justify == 'right' : 
~~~ dag_layers = [ ] 
for dag_layer in dag . layers ( ) : 
~~~ dag_layers . append ( dag_layer ) 
~~ dag_layers . reverse ( ) 
layer_dicts = [ { } ] 
for dag_layer in dag_layers : 
~~~ dag_instructions = dag_layer [ 'graph' ] . op_nodes ( ) 
dag_instructions . sort ( key = lambda nd : nd . _node_id ) 
for instruction_node in dag_instructions : 
~~~ gate_span = _get_gate_span ( qregs , instruction_node ) 
added = False 
for i in range ( len ( layer_dicts ) ) : 
~~~ curr_dict = layer_dicts [ - 1 - i ] 
if any ( index in curr_dict for index in gate_span ) : 
~~~ added = True 
if i == 0 : 
~~~ new_dict = { } 
for index in gate_span : 
~~~ new_dict [ index ] = instruction_node 
~~ layer_dicts . append ( new_dict ) 
~~~ curr_dict = layer_dicts [ - i ] 
~~~ curr_dict [ index ] = instruction_node 
~~ ~~ break 
~~ ~~ if not added : 
~~~ for index in gate_span : 
~~~ layer_dicts [ 0 ] [ index ] = instruction_node 
~~ ~~ ~~ ~~ layer_dicts . reverse ( ) 
ops = [ list ( layer . values ( ) ) for layer in layer_dicts ] 
~~ if reverse_bits : 
~~~ qregs . reverse ( ) 
cregs . reverse ( ) 
~~ return qregs , cregs , ops 
~~ def _get_gate_span ( qregs , instruction ) : 
min_index = len ( qregs ) 
max_index = 0 
for qreg in instruction . qargs : 
~~~ index = qregs . index ( qreg ) 
if index < min_index : 
~~~ min_index = index 
~~ if index > max_index : 
~~~ max_index = index 
~~ ~~ if instruction . cargs : 
~~~ return qregs [ min_index : ] 
~~ return qregs [ min_index : max_index + 1 ] 
~~ def circuit_to_instruction ( circuit ) : 
instruction = Instruction ( name = circuit . name , 
num_qubits = sum ( [ qreg . size for qreg in circuit . qregs ] ) , 
num_clbits = sum ( [ creg . size for creg in circuit . cregs ] ) , 
params = [ ] ) 
instruction . control = None 
def find_bit_position ( bit ) : 
if isinstance ( bit [ 0 ] , QuantumRegister ) : 
~~~ ordered_regs = circuit . qregs 
~~~ ordered_regs = circuit . cregs 
~~ reg_index = ordered_regs . index ( bit [ 0 ] ) 
return sum ( [ reg . size for reg in ordered_regs [ : reg_index ] ] ) + bit [ 1 ] 
~~ definition = circuit . data . copy ( ) 
if instruction . num_qubits > 0 : 
~~~ q = QuantumRegister ( instruction . num_qubits , 'q' ) 
~~ if instruction . num_clbits > 0 : 
~~~ c = ClassicalRegister ( instruction . num_clbits , 'c' ) 
~~ definition = list ( map ( lambda x : 
( x [ 0 ] , 
list ( map ( lambda y : ( q , find_bit_position ( y ) ) , x [ 1 ] ) ) , 
list ( map ( lambda y : ( c , find_bit_position ( y ) ) , x [ 2 ] ) ) ) , definition ) ) 
instruction . definition = definition 
return instruction 
num_dag_qubits = sum ( [ qreg . size for qreg in dag . qregs . values ( ) ] ) 
if num_dag_qubits > self . coupling_map . size ( ) : 
~~ best_sub = self . _best_subset ( num_dag_qubits ) 
layout = Layout ( ) 
map_iter = 0 
~~~ for i in range ( qreg . size ) : 
~~~ layout [ ( qreg , i ) ] = int ( best_sub [ map_iter ] ) 
map_iter += 1 
~~ ~~ self . property_set [ 'layout' ] = layout 
~~ def _best_subset ( self , n_qubits ) : 
if n_qubits == 1 : 
~~~ return np . array ( [ 0 ] ) 
~~ device_qubits = self . coupling_map . size ( ) 
cmap = np . asarray ( self . coupling_map . get_edges ( ) ) 
data = np . ones_like ( cmap [ : , 0 ] ) 
sp_cmap = sp . coo_matrix ( ( data , ( cmap [ : , 0 ] , cmap [ : , 1 ] ) ) , 
shape = ( device_qubits , device_qubits ) ) . tocsr ( ) 
best = 0 
best_map = None 
for k in range ( sp_cmap . shape [ 0 ] ) : 
~~~ bfs = cs . breadth_first_order ( sp_cmap , i_start = k , directed = False , 
return_predecessors = False ) 
connection_count = 0 
sub_graph = [ ] 
for i in range ( n_qubits ) : 
~~~ node_idx = bfs [ i ] 
for j in range ( sp_cmap . indptr [ node_idx ] , 
sp_cmap . indptr [ node_idx + 1 ] ) : 
~~~ node = sp_cmap . indices [ j ] 
for counter in range ( n_qubits ) : 
~~~ if node == bfs [ counter ] : 
~~~ connection_count += 1 
sub_graph . append ( [ node_idx , node ] ) 
~~ ~~ ~~ ~~ if connection_count > best : 
~~~ best = connection_count 
best_map = bfs [ 0 : n_qubits ] 
for edge in range ( best_map . shape [ 0 ] ) : 
~~~ mapping [ best_map [ edge ] ] = edge 
~~ new_cmap = [ [ mapping [ c [ 0 ] ] , mapping [ c [ 1 ] ] ] for c in sub_graph ] 
rows = [ edge [ 0 ] for edge in new_cmap ] 
cols = [ edge [ 1 ] for edge in new_cmap ] 
data = [ 1 ] * len ( rows ) 
sp_sub_graph = sp . coo_matrix ( ( data , ( rows , cols ) ) , 
shape = ( n_qubits , n_qubits ) ) . tocsr ( ) 
perm = cs . reverse_cuthill_mckee ( sp_sub_graph ) 
best_map = best_map [ perm ] 
~~ ~~ return best_map 
~~ def is_cptp ( self , atol = None , rtol = None ) : 
if self . _data [ 1 ] is not None : 
~~ if atol is None : 
~~~ atol = self . _atol 
~~ if rtol is None : 
~~~ rtol = self . _rtol 
~~ accum = 0j 
for op in self . _data [ 0 ] : 
~~~ accum += np . dot ( np . transpose ( np . conj ( op ) ) , op ) 
~~ return is_identity_matrix ( accum , rtol = rtol , atol = atol ) 
~~ def conjugate ( self ) : 
kraus_l , kraus_r = self . _data 
kraus_l = [ k . conj ( ) for k in kraus_l ] 
if kraus_r is not None : 
~~~ kraus_r = [ k . conj ( ) for k in kraus_r ] 
~~ return Kraus ( ( kraus_l , kraus_r ) , self . input_dims ( ) , self . output_dims ( ) ) 
~~ def transpose ( self ) : 
kraus_l = [ k . T for k in kraus_l ] 
~~~ kraus_r = [ k . T for k in kraus_r ] 
~~ return Kraus ( ( kraus_l , kraus_r ) , 
input_dims = self . output_dims ( ) , 
output_dims = self . input_dims ( ) ) 
~~ def compose ( self , other , qargs = None , front = False ) : 
if qargs is not None : 
~~~ return Kraus ( 
SuperOp ( self ) . compose ( other , qargs = qargs , front = front ) ) 
~~ if not isinstance ( other , Kraus ) : 
~~~ other = Kraus ( other ) 
~~ if front and self . _input_dim != other . _output_dim : 
~~~ raise QiskitError ( 
~~ if not front and self . _output_dim != other . _input_dim : 
~~ if front : 
~~~ ka_l , ka_r = self . _data 
kb_l , kb_r = other . _data 
input_dim = other . _input_dim 
output_dim = self . _output_dim 
~~~ ka_l , ka_r = other . _data 
kb_l , kb_r = self . _data 
input_dim = self . _input_dim 
output_dim = other . _output_dim 
~~ kab_l = [ np . dot ( a , b ) for a in ka_l for b in kb_l ] 
if ka_r is None and kb_r is None : 
~~~ kab_r = None 
~~ elif ka_r is None : 
~~~ kab_r = [ np . dot ( a , b ) for a in ka_l for b in kb_r ] 
~~ elif kb_r is None : 
~~~ kab_r = [ np . dot ( a , b ) for a in ka_r for b in kb_l ] 
~~~ kab_r = [ np . dot ( a , b ) for a in ka_r for b in kb_r ] 
~~ return Kraus ( ( kab_l , kab_r ) , input_dim , output_dim ) 
~~ def power ( self , n ) : 
if n > 0 : 
~~~ return super ( ) . power ( n ) 
~~ return Kraus ( SuperOp ( self ) . power ( n ) ) 
~~ def multiply ( self , other ) : 
if not isinstance ( other , Number ) : 
~~ if isinstance ( other , complex ) or other < 0 : 
~~~ return Kraus ( Choi ( self ) . multiply ( other ) ) 
~~ val = np . sqrt ( other ) 
kraus_r = None 
kraus_l = [ val * k for k in self . _data [ 0 ] ] 
~~~ kraus_r = [ val * k for k in self . _data [ 1 ] ] 
~~ return Kraus ( ( kraus_l , kraus_r ) , self . _input_dim , self . _output_dim ) 
~~ def _evolve ( self , state , qargs = None ) : 
~~~ return SuperOp ( self ) . _evolve ( state , qargs ) 
~~ state = self . _format_state ( state ) 
if state . shape [ 0 ] != self . _input_dim : 
~~ if state . ndim == 1 and self . _data [ 1 ] is None and len ( 
self . _data [ 0 ] ) == 1 : 
~~~ return np . dot ( self . _data [ 0 ] [ 0 ] , state ) 
~~ state = self . _format_state ( state , density_matrix = True ) 
if kraus_r is None : 
~~~ kraus_r = kraus_l 
~~ return np . einsum ( 'AiB,BC,AjC->ij' , kraus_l , state , 
np . conjugate ( kraus_r ) ) 
~~ def _tensor_product ( self , other , reverse = False ) : 
if not isinstance ( other , Kraus ) : 
~~ ka_l , ka_r = self . _data 
if reverse : 
~~~ input_dims = self . input_dims ( ) + other . input_dims ( ) 
output_dims = self . output_dims ( ) + other . output_dims ( ) 
kab_l = [ np . kron ( b , a ) for a in ka_l for b in kb_l ] 
~~~ input_dims = other . input_dims ( ) + self . input_dims ( ) 
output_dims = other . output_dims ( ) + self . output_dims ( ) 
kab_l = [ np . kron ( a , b ) for a in ka_l for b in kb_l ] 
~~ if ka_r is None and kb_r is None : 
~~~ if ka_r is None : 
~~~ ka_r = ka_l 
~~ if kb_r is None : 
~~~ kb_r = kb_l 
~~ if reverse : 
~~~ kab_r = [ np . kron ( b , a ) for a in ka_r for b in kb_r ] 
~~~ kab_r = [ np . kron ( a , b ) for a in ka_r for b in kb_r ] 
~~ ~~ data = ( kab_l , kab_r ) 
return Kraus ( data , input_dims , output_dims ) 
~~ def functional_pulse ( func ) : 
@ functools . wraps ( func ) 
def to_pulse ( duration , * args , name = None , ** kwargs ) : 
if isinstance ( duration , int ) and duration > 0 : 
~~~ samples = func ( duration , * args , ** kwargs ) 
samples = np . asarray ( samples , dtype = np . complex128 ) 
return SamplePulse ( samples = samples , name = name ) 
~~ return to_pulse 
~~ def real ( self , nested_scope = None ) : 
operation = self . children [ 0 ] . operation ( ) 
lhs = self . children [ 1 ] . real ( nested_scope ) 
rhs = self . children [ 2 ] . real ( nested_scope ) 
return operation ( lhs , rhs ) 
~~ def sym ( self , nested_scope = None ) : 
lhs = self . children [ 1 ] . sym ( nested_scope ) 
rhs = self . children [ 2 ] . sym ( nested_scope ) 
~~ def barrier ( self , * qargs ) : 
qubits = [ ] 
qargs = _convert_to_bits ( qargs , [ qbit for qreg in self . qregs for qbit in qreg ] ) 
~~~ for qreg in self . qregs : 
~~~ for j in range ( qreg . size ) : 
~~~ qubits . append ( ( qreg , j ) ) 
~~ ~~ ~~ for qarg in qargs : 
~~~ if isinstance ( qarg , ( QuantumRegister , list ) ) : 
~~~ if isinstance ( qarg , QuantumRegister ) : 
~~~ qubits . extend ( [ ( qarg , j ) for j in range ( qarg . size ) ] ) 
~~~ qubits . extend ( qarg ) 
~~~ qubits . append ( qarg ) 
~~ ~~ return self . append ( Barrier ( len ( qubits ) ) , qubits , [ ] ) 
~~ def average_data ( counts , observable ) : 
if not isinstance ( observable , dict ) : 
~~~ observable = make_dict_observable ( observable ) 
~~ temp = 0 
tot = sum ( counts . values ( ) ) 
for key in counts : 
~~~ if key in observable : 
~~~ temp += counts [ key ] * observable [ key ] / tot 
~~ ~~ return temp 
~~ def _process_bit_id ( self , node ) : 
reg = None 
if node . name in self . dag . qregs : 
~~~ reg = self . dag . qregs [ node . name ] 
~~ elif node . name in self . dag . cregs : 
~~~ reg = self . dag . cregs [ node . name ] 
"line=%s" % node . line , 
"file=%s" % node . file ) 
~~ if node . type == "indexed_id" : 
~~~ return [ ( reg , node . index ) ] 
~~ elif node . type == "id" : 
~~~ if not self . bit_stack [ - 1 ] : 
~~~ return [ ( reg , j ) for j in range ( reg . size ) ] 
~~~ if node . name in self . bit_stack [ - 1 ] : 
~~~ return [ self . bit_stack [ - 1 ] [ node . name ] ] 
~~ ~~ return None 
~~ def _process_custom_unitary ( self , node ) : 
name = node . name 
if node . arguments is not None : 
~~~ args = self . _process_node ( node . arguments ) 
~~ bits = [ self . _process_bit_id ( node_element ) 
for node_element in node . bitlist . children ] 
if name in self . gates : 
~~~ gargs = self . gates [ name ] [ "args" ] 
gbits = self . gates [ name ] [ "bits" ] 
maxidx = max ( map ( len , bits ) ) 
for idx in range ( maxidx ) : 
~~~ self . arg_stack . append ( { gargs [ j ] : args [ j ] 
for j in range ( len ( gargs ) ) } ) 
element = [ idx * x for x in 
[ len ( bits [ j ] ) > 1 for j in range ( len ( bits ) ) ] ] 
self . bit_stack . append ( { gbits [ j ] : bits [ j ] [ element [ j ] ] 
for j in range ( len ( gbits ) ) } ) 
self . _create_dag_op ( name , 
[ self . arg_stack [ - 1 ] [ s ] . sym ( ) for s in gargs ] , 
[ self . bit_stack [ - 1 ] [ s ] for s in gbits ] ) 
self . arg_stack . pop ( ) 
self . bit_stack . pop ( ) 
"line=%s" % node . line , "file=%s" % node . file ) 
~~ ~~ def _process_gate ( self , node , opaque = False ) : 
self . gates [ node . name ] = { } 
de_gate = self . gates [ node . name ] 
de_gate [ "opaque" ] = opaque 
de_gate [ "n_args" ] = node . n_args ( ) 
de_gate [ "n_bits" ] = node . n_bits ( ) 
if node . n_args ( ) > 0 : 
~~~ de_gate [ "args" ] = [ element . name for element in node . arguments . children ] 
~~~ de_gate [ "args" ] = [ ] 
~~ de_gate [ "bits" ] = [ c . name for c in node . bitlist . children ] 
if opaque : 
~~~ de_gate [ "body" ] = None 
~~~ de_gate [ "body" ] = node . body 
~~ ~~ def _process_cnot ( self , node ) : 
id0 = self . _process_bit_id ( node . children [ 0 ] ) 
id1 = self . _process_bit_id ( node . children [ 1 ] ) 
if not ( len ( id0 ) == len ( id1 ) or len ( id0 ) == 1 or len ( id1 ) == 1 ) : 
~~ maxidx = max ( [ len ( id0 ) , len ( id1 ) ] ) 
~~~ if len ( id0 ) > 1 and len ( id1 ) > 1 : 
~~~ self . dag . apply_operation_back ( CXBase ( ) , [ id0 [ idx ] , id1 [ idx ] ] , [ ] , self . condition ) 
~~ elif len ( id0 ) > 1 : 
~~~ self . dag . apply_operation_back ( CXBase ( ) , [ id0 [ idx ] , id1 [ 0 ] ] , [ ] , self . condition ) 
~~~ self . dag . apply_operation_back ( CXBase ( ) , [ id0 [ 0 ] , id1 [ idx ] ] , [ ] , self . condition ) 
~~ ~~ ~~ def _process_measure ( self , node ) : 
if len ( id0 ) != len ( id1 ) : 
~~ for idx , idy in zip ( id0 , id1 ) : 
~~~ self . dag . apply_operation_back ( Measure ( ) , [ idx ] , [ idy ] , self . condition ) 
~~ ~~ def _process_if ( self , node ) : 
creg_name = node . children [ 0 ] . name 
creg = self . dag . cregs [ creg_name ] 
cval = node . children [ 1 ] . value 
self . condition = ( creg , cval ) 
self . _process_node ( node . children [ 2 ] ) 
self . condition = None 
~~ def _process_node ( self , node ) : 
if node . type == "program" : 
~~~ self . _process_children ( node ) 
~~ elif node . type == "qreg" : 
~~~ qreg = QuantumRegister ( node . index , node . name ) 
self . dag . add_qreg ( qreg ) 
~~ elif node . type == "creg" : 
~~~ creg = ClassicalRegister ( node . index , node . name ) 
self . dag . add_creg ( creg ) 
~~ elif node . type == "int" : 
~~ elif node . type == "real" : 
~~ elif node . type == "indexed_id" : 
~~ elif node . type == "id_list" : 
~~~ return [ self . _process_bit_id ( node_children ) 
for node_children in node . children ] 
~~ elif node . type == "primary_list" : 
~~~ return [ self . _process_bit_id ( m ) for m in node . children ] 
~~ elif node . type == "gate" : 
~~~ self . _process_gate ( node ) 
~~ elif node . type == "custom_unitary" : 
~~~ self . _process_custom_unitary ( node ) 
~~ elif node . type == "universal_unitary" : 
~~~ args = self . _process_node ( node . children [ 0 ] ) 
qid = self . _process_bit_id ( node . children [ 1 ] ) 
for element in qid : 
~~~ self . dag . apply_operation_back ( UBase ( * args , element ) , self . condition ) 
~~ ~~ elif node . type == "cnot" : 
~~~ self . _process_cnot ( node ) 
~~ elif node . type == "expression_list" : 
~~~ return node . children 
~~ elif node . type == "binop" : 
~~ elif node . type == "prefix" : 
~~ elif node . type == "measure" : 
~~~ self . _process_measure ( node ) 
~~ elif node . type == "format" : 
~~~ self . version = node . version ( ) 
~~ elif node . type == "barrier" : 
~~~ ids = self . _process_node ( node . children [ 0 ] ) 
for qubit in ids : 
~~~ for j , _ in enumerate ( qubit ) : 
~~~ qubits . append ( qubit [ j ] ) 
~~ ~~ self . dag . apply_operation_back ( Barrier ( len ( qubits ) ) , qubits , [ ] ) 
~~ elif node . type == "reset" : 
~~~ id0 = self . _process_bit_id ( node . children [ 0 ] ) 
for i , _ in enumerate ( id0 ) : 
~~~ self . dag . apply_operation_back ( Reset ( ) , [ id0 [ i ] ] , [ ] , self . condition ) 
~~ ~~ elif node . type == "if" : 
~~~ self . _process_if ( node ) 
~~ elif node . type == "opaque" : 
~~~ self . _process_gate ( node , opaque = True ) 
~~ elif node . type == "external" : 
node . type , "line=%s" % node . line , 
~~ def _create_dag_op ( self , name , params , qargs ) : 
if name == "u0" : 
~~~ op_class = U0Gate 
~~ elif name == "u1" : 
~~~ op_class = U1Gate 
~~ elif name == "u2" : 
~~~ op_class = U2Gate 
~~ elif name == "u3" : 
~~~ op_class = U3Gate 
~~ elif name == "x" : 
~~~ op_class = XGate 
~~ elif name == "y" : 
~~~ op_class = YGate 
~~ elif name == "z" : 
~~~ op_class = ZGate 
~~ elif name == "t" : 
~~~ op_class = TGate 
~~ elif name == "tdg" : 
~~~ op_class = TdgGate 
~~ elif name == "s" : 
~~~ op_class = SGate 
~~ elif name == "sdg" : 
~~~ op_class = SdgGate 
~~ elif name == "swap" : 
~~~ op_class = SwapGate 
~~ elif name == "rx" : 
~~~ op_class = RXGate 
~~ elif name == "ry" : 
~~~ op_class = RYGate 
~~ elif name == "rz" : 
~~~ op_class = RZGate 
~~ elif name == "rzz" : 
~~~ op_class = RZZGate 
~~ elif name == "id" : 
~~~ op_class = IdGate 
~~ elif name == "h" : 
~~~ op_class = HGate 
~~ elif name == "cx" : 
~~~ op_class = CnotGate 
~~ elif name == "cy" : 
~~~ op_class = CyGate 
~~ elif name == "cz" : 
~~~ op_class = CzGate 
~~ elif name == "ch" : 
~~~ op_class = CHGate 
~~ elif name == "crz" : 
~~~ op_class = CrzGate 
~~ elif name == "cu1" : 
~~~ op_class = Cu1Gate 
~~ elif name == "cu3" : 
~~~ op_class = Cu3Gate 
~~ elif name == "ccx" : 
~~~ op_class = ToffoliGate 
~~ elif name == "cswap" : 
~~~ op_class = FredkinGate 
~~ op = op_class ( * params ) 
self . dag . apply_operation_back ( op , qargs , [ ] , condition = self . condition ) 
~~ def qasm ( self , prec = 15 ) : 
~~ def ch_duration ( self , * channels : List [ Channel ] ) -> int : 
return self . timeslots . ch_duration ( * channels ) 
~~ def ch_start_time ( self , * channels : List [ Channel ] ) -> int : 
return self . timeslots . ch_start_time ( * channels ) 
~~ def ch_stop_time ( self , * channels : List [ Channel ] ) -> int : 
return self . timeslots . ch_stop_time ( * channels ) 
~~ def _instructions ( self , time : int = 0 ) -> Iterable [ Tuple [ int , 'Instruction' ] ] : 
for insert_time , child_sched in self . children : 
~~~ yield from child_sched . _instructions ( time + insert_time ) 
~~ ~~ def to_string ( self , indent ) : 
print ( ind , 'indexed_id' , self . name , self . index ) 
~~ def check_type ( self , value , attr , data ) : 
expected_types = self . _expected_types ( ) 
if not isinstance ( value , expected_types ) : 
~~~ raise self . _not_expected_type ( 
value , expected_types , fields = [ self ] , field_names = attr , data = data ) 
~~ def dump_additional_data ( self , valid_data , many , original_data ) : 
if many : 
~~~ for i , _ in enumerate ( valid_data ) : 
~~~ additional_keys = set ( original_data [ i ] . __dict__ ) - set ( valid_data [ i ] ) 
for key in additional_keys : 
~~~ valid_data [ i ] [ key ] = getattr ( original_data [ i ] , key ) 
~~~ additional_keys = set ( original_data . __dict__ ) - set ( valid_data ) 
~~~ valid_data [ key ] = getattr ( original_data , key ) 
~~ ~~ return valid_data 
~~ def load_additional_data ( self , valid_data , many , original_data ) : 
~~~ additional_keys = set ( original_data [ i ] ) - set ( valid_data [ i ] ) 
~~~ valid_data [ i ] [ key ] = original_data [ i ] [ key ] 
~~~ additional_keys = set ( original_data ) - set ( valid_data ) 
~~~ valid_data [ key ] = original_data [ key ] 
~~ def _create_validation_schema ( schema_cls ) : 
validation_schema = schema_cls ( ) 
for _ , field in validation_schema . fields . items ( ) : 
~~~ if isinstance ( field , ModelTypeValidator ) : 
~~~ validate_function = field . __class__ . check_type 
field . _deserialize = MethodType ( validate_function , field ) 
~~ ~~ return validation_schema 
~~ def _validate ( instance ) : 
~~~ _ = instance . schema . validate ( instance . to_dict ( ) ) 
~~ except ValidationError as ex : 
~~~ raise ModelValidationError ( 
ex . messages , ex . field_names , ex . fields , ex . data , ** ex . kwargs ) 
~~ ~~ def _validate_after_init ( init_method ) : 
@ wraps ( init_method ) 
def _decorated ( self , ** kwargs ) : 
~~~ _ = self . shallow_schema . validate ( kwargs ) 
ex . messages , ex . field_names , ex . fields , ex . data , ** ex . kwargs ) from None 
~~ init_method ( self , ** kwargs ) 
~~ return _decorated 
~~ def to_dict ( self ) : 
~~~ data , _ = self . schema . dump ( self ) 
~~ def from_dict ( cls , dict_ ) : 
~~~ data , _ = cls . schema . load ( dict_ ) 
~~ def qft ( circ , q , n ) : 
for j in range ( n ) : 
~~~ for k in range ( j ) : 
~~~ circ . cu1 ( math . pi / float ( 2 ** ( j - k ) ) , q [ j ] , q [ k ] ) 
~~ circ . h ( q [ j ] ) 
~~ ~~ def partial_trace ( state , trace_systems , dimensions = None , reverse = True ) : 
~~~ num_qubits = int ( np . log2 ( len ( state ) ) ) 
dimensions = [ 2 for _ in range ( num_qubits ) ] 
if len ( state ) != 2 ** num_qubits : 
~~~ dimensions = list ( dimensions ) 
~~ if isinstance ( trace_systems , int ) : 
~~~ trace_systems = [ trace_systems ] 
~~~ trace_systems = sorted ( trace_systems , reverse = True ) 
~~ if state . ndim == 1 : 
~~~ return __partial_trace_vec ( state , trace_systems , dimensions , reverse ) 
~~ return __partial_trace_mat ( state , trace_systems , dimensions , reverse ) 
~~ def __partial_trace_vec ( vec , trace_systems , dimensions , reverse = True ) : 
~~~ dimensions = dimensions [ : : - 1 ] 
trace_systems = len ( dimensions ) - 1 - np . array ( trace_systems ) 
~~ rho = vec . reshape ( dimensions ) 
rho = np . tensordot ( rho , rho . conj ( ) , axes = ( trace_systems , trace_systems ) ) 
d = int ( np . sqrt ( np . product ( rho . shape ) ) ) 
return rho . reshape ( d , d ) 
~~ def __partial_trace_mat ( mat , trace_systems , dimensions , reverse = True ) : 
trace_systems = sorted ( trace_systems , reverse = True ) 
for j in trace_systems : 
~~~ left_dimensions = dimensions [ j + 1 : ] 
right_dimensions = dimensions [ : j ] 
dimensions = right_dimensions + left_dimensions 
~~~ left_dimensions = dimensions [ : j ] 
right_dimensions = dimensions [ j + 1 : ] 
dimensions = left_dimensions + right_dimensions 
~~ dimension_left = int ( np . prod ( left_dimensions ) ) 
dimension_right = int ( np . prod ( right_dimensions ) ) 
mat = mat . reshape ( [ dimension_left , dimension_trace , dimension_right , 
dimension_left , dimension_trace , dimension_right ] ) 
mat = mat . trace ( axis1 = 1 , axis2 = 4 ) . reshape ( 
dimension_left * dimension_right , 
dimension_left * dimension_right ) 
~~ return mat 
~~ def vectorize ( density_matrix , method = 'col' ) : 
density_matrix = np . array ( density_matrix ) 
if method == 'col' : 
~~~ return density_matrix . flatten ( order = 'F' ) 
~~ elif method == 'row' : 
~~~ return density_matrix . flatten ( order = 'C' ) 
~~ elif method in [ 'pauli' , 'pauli_weights' ] : 
if len ( density_matrix ) != 2 ** num : 
~~ if method == 'pauli_weights' : 
~~~ pgroup = pauli_group ( num , case = 'weight' ) 
~~~ pgroup = pauli_group ( num , case = 'tensor' ) 
~~ vals = [ np . trace ( np . dot ( p . to_matrix ( ) , density_matrix ) ) 
for p in pgroup ] 
return np . array ( vals ) 
~~ def devectorize ( vectorized_mat , method = 'col' ) : 
vectorized_mat = np . array ( vectorized_mat ) 
dimension = int ( np . sqrt ( vectorized_mat . size ) ) 
if len ( vectorized_mat ) != dimension * dimension : 
~~ if method == 'col' : 
~~~ return vectorized_mat . reshape ( dimension , dimension , order = 'F' ) 
~~~ return vectorized_mat . reshape ( dimension , dimension , order = 'C' ) 
if dimension != 2 ** num_qubits : 
~~~ pgroup = pauli_group ( num_qubits , case = 'weight' ) 
~~~ pgroup = pauli_group ( num_qubits , case = 'tensor' ) 
~~ pbasis = np . array ( [ p . to_matrix ( ) for p in pgroup ] ) / 2 ** num_qubits 
return np . tensordot ( vectorized_mat , pbasis , axes = 1 ) 
~~ def choi_to_rauli ( choi , order = 1 ) : 
if order == 0 : 
~~~ order = 'weight' 
~~ elif order == 1 : 
~~~ order = 'tensor' 
~~ num_qubits = int ( np . log2 ( np . sqrt ( len ( choi ) ) ) ) 
pgp = pauli_group ( num_qubits , case = order ) 
rauli = [ ] 
for i in pgp : 
~~~ for j in pgp : 
~~~ pauliop = np . kron ( j . to_matrix ( ) . T , i . to_matrix ( ) ) 
rauli += [ np . trace ( np . dot ( choi , pauliop ) ) ] 
~~ ~~ return np . array ( rauli ) . reshape ( 4 ** num_qubits , 4 ** num_qubits ) 
~~ def chop ( array , epsilon = 1e-10 ) : 
ret = np . array ( array ) 
if np . isrealobj ( ret ) : 
~~~ ret [ abs ( ret ) < epsilon ] = 0.0 
~~~ ret . real [ abs ( ret . real ) < epsilon ] = 0.0 
ret . imag [ abs ( ret . imag ) < epsilon ] = 0.0 
~~ def outer ( vector1 , vector2 = None ) : 
if vector2 is None : 
~~~ vector2 = np . array ( vector1 ) . conj ( ) 
~~~ vector2 = np . array ( vector2 ) . conj ( ) 
~~ return np . outer ( vector1 , vector2 ) 
~~ def random_unitary_matrix ( dim , seed = None ) : 
DeprecationWarning ) 
return random . random_unitary ( dim , seed ) . data 
~~ def random_density_matrix ( length , rank = None , method = 'Hilbert-Schmidt' , seed = None ) : 
return random . random_density_matrix ( length , rank , method , seed ) 
~~ def concurrence ( state ) : 
rho = np . array ( state ) 
~~~ rho = outer ( state ) 
~~ if len ( state ) != 4 : 
~~ YY = np . fliplr ( np . diag ( [ - 1 , 1 , 1 , - 1 ] ) ) 
A = rho . dot ( YY ) . dot ( rho . conj ( ) ) . dot ( YY ) 
w = la . eigh ( A , eigvals_only = True ) 
w = np . sqrt ( np . maximum ( w , 0 ) ) 
return max ( 0.0 , w [ - 1 ] - np . sum ( w [ 0 : - 1 ] ) ) 
~~ def shannon_entropy ( pvec , base = 2 ) : 
if base == 2 : 
~~~ def logfn ( x ) : 
~~~ return - x * np . log2 ( x ) 
~~ ~~ elif base == np . e : 
~~~ return - x * np . log ( x ) 
~~~ return - x * np . log ( x ) / np . log ( base ) 
~~ ~~ h = 0. 
for x in pvec : 
~~~ if 0 < x < 1 : 
~~~ h += logfn ( x ) 
~~ ~~ return h 
~~ def entropy ( state ) : 
~~~ return 0 
~~ evals = np . maximum ( np . linalg . eigvalsh ( state ) , 0. ) 
return shannon_entropy ( evals , base = np . e ) 
~~ def mutual_information ( state , d0 , d1 = None ) : 
if d1 is None : 
~~~ d1 = int ( len ( state ) / d0 ) 
~~ mi = entropy ( partial_trace ( state , [ 0 ] , dimensions = [ d0 , d1 ] ) ) 
mi += entropy ( partial_trace ( state , [ 1 ] , dimensions = [ d0 , d1 ] ) ) 
mi -= entropy ( state ) 
return mi 
~~ def entanglement_of_formation ( state , d0 , d1 = None ) : 
state = np . array ( state ) 
~~ if state . ndim == 2 and len ( state ) == 4 and d0 == 2 and d1 == 2 : 
~~~ return __eof_qubit ( state ) 
~~ elif state . ndim == 1 : 
~~~ if d0 < d1 : 
~~~ tr = [ 1 ] 
~~~ tr = [ 0 ] 
~~ state = partial_trace ( state , tr , dimensions = [ d0 , d1 ] ) 
return entropy ( state ) 
~~ def __eof_qubit ( rho ) : 
c = concurrence ( rho ) 
c = 0.5 + 0.5 * np . sqrt ( 1 - c * c ) 
return shannon_entropy ( [ c , 1 - c ] ) 
~~ def union ( * schedules : List [ Union [ ScheduleComponent , Tuple [ int , ScheduleComponent ] ] ] , 
name : str = None ) -> Schedule : 
if name is None and schedules : 
~~~ sched = schedules [ 0 ] 
if isinstance ( sched , ( list , tuple ) ) : 
~~~ name = sched [ 1 ] . name 
~~~ name = sched . name 
~~ ~~ return Schedule ( * schedules , name = name ) 
~~ def flatten ( schedule : ScheduleComponent , name : str = None ) -> Schedule : 
if name is None : 
~~~ name = schedule . name 
~~ return Schedule ( * schedule . instructions , name = name ) 
~~ def shift ( schedule : ScheduleComponent , time : int , name : str = None ) -> Schedule : 
~~ return union ( ( time , schedule ) , name = name ) 
~~ def insert ( parent : ScheduleComponent , time : int , child : ScheduleComponent , 
return union ( parent , ( time , child ) , name = name ) 
~~ def append ( parent : ScheduleComponent , child : ScheduleComponent , 
common_channels = set ( parent . channels ) & set ( child . channels ) 
insertion_time = parent . ch_stop_time ( * common_channels ) 
return insert ( parent , insertion_time , child , name = name ) 
~~ def u3 ( self , theta , phi , lam , q ) : 
return self . append ( U3Gate ( theta , phi , lam ) , [ q ] , [ ] ) 
~~ def status ( self ) : 
return BackendStatus ( backend_name = self . name ( ) , 
backend_version = __version__ , 
operational = True , 
pending_jobs = 0 , 
status_msg = '' ) 
~~ def start ( self , iterations ) : 
self . touched = True 
self . iter = int ( iterations ) 
self . t_start = time . time ( ) 
~~ def time_remaining_est ( self , completed_iter ) : 
if completed_iter : 
~~~ t_r_est = ( time . time ( ) - self . t_start ) / completed_iter * ( self . iter - completed_iter ) 
~~~ t_r_est = 0 
~~ date_time = datetime . datetime ( 1 , 1 , 1 ) + datetime . timedelta ( seconds = t_r_est ) 
time_string = "%02d:%02d:%02d:%02d" % ( date_time . day - 1 , date_time . hour , date_time . minute , date_time . second ) 
return time_string 
q_gate_list = [ 'cx' , 'cy' , 'cz' , 'h' , 'x' , 'y' , 'z' ] 
cancellation_sets = defaultdict ( lambda : [ ] ) 
for wire in dag . wires : 
~~~ wire_name = "{0}[{1}]" . format ( str ( wire [ 0 ] . name ) , str ( wire [ 1 ] ) ) 
wire_commutation_set = self . property_set [ 'commutation_set' ] [ wire_name ] 
for com_set_idx , com_set in enumerate ( wire_commutation_set ) : 
~~~ if com_set [ 0 ] . type in [ 'in' , 'out' ] : 
~~ for node in com_set : 
~~~ num_qargs = len ( node . qargs ) 
if num_qargs == 1 and node . name in q_gate_list : 
~~~ cancellation_sets [ ( node . name , wire_name , com_set_idx ) ] . append ( node ) 
~~ if num_qargs == 1 and node . name in [ 'u1' , 'rz' , 't' , 's' ] : 
~~~ cancellation_sets [ ( 'z_rotation' , wire_name , com_set_idx ) ] . append ( node ) 
~~ elif num_qargs == 2 and node . qargs [ 0 ] == wire : 
~~~ second_op_name = "{0}[{1}]" . format ( str ( node . qargs [ 1 ] [ 0 ] . name ) , 
str ( node . qargs [ 1 ] [ 1 ] ) ) 
q2_key = ( node . name , wire_name , second_op_name , 
self . property_set [ 'commutation_set' ] [ ( node , second_op_name ) ] ) 
cancellation_sets [ q2_key ] . append ( node ) 
~~ ~~ ~~ ~~ for cancel_set_key in cancellation_sets : 
~~~ set_len = len ( cancellation_sets [ cancel_set_key ] ) 
if ( ( set_len ) > 1 and cancel_set_key [ 0 ] in q_gate_list ) : 
~~~ gates_to_cancel = cancellation_sets [ cancel_set_key ] 
for c_node in gates_to_cancel [ : ( set_len // 2 ) * 2 ] : 
~~~ dag . remove_op_node ( c_node ) 
~~ ~~ elif ( ( set_len ) > 1 and cancel_set_key [ 0 ] == 'z_rotation' ) : 
~~~ run = cancellation_sets [ cancel_set_key ] 
run_qarg = run [ 0 ] . qargs [ 0 ] 
~~~ if ( current_node . condition is not None 
or current_node . qargs [ 0 ] != run_qarg ) : 
~~ if current_node . name in [ 'u1' , 'rz' ] : 
~~~ current_angle = float ( current_node . op . params [ 0 ] ) 
~~ elif current_node . name == 't' : 
~~~ current_angle = sympy . pi / 4 
~~ elif current_node . name == 's' : 
~~~ current_angle = sympy . pi / 2 
~~ total_angle = current_angle + total_angle 
~~ new_op = U1Gate ( total_angle ) 
new_qarg = ( QuantumRegister ( 1 , 'q' ) , 0 ) 
new_dag = DAGCircuit ( ) 
new_dag . add_qreg ( new_qarg [ 0 ] ) 
new_dag . apply_operation_back ( new_op , [ new_qarg ] ) 
for current_node in run [ 1 : ] : 
~~ ~~ ~~ return dag 
~~ def _experiments_to_circuits ( qobj ) : 
if qobj . experiments : 
~~~ circuits = [ ] 
for x in qobj . experiments : 
~~~ quantum_registers = [ QuantumRegister ( i [ 1 ] , name = i [ 0 ] ) 
for i in x . header . qreg_sizes ] 
classical_registers = [ ClassicalRegister ( i [ 1 ] , name = i [ 0 ] ) 
for i in x . header . creg_sizes ] 
circuit = QuantumCircuit ( * quantum_registers , 
* classical_registers , 
name = x . header . name ) 
qreg_dict = { } 
creg_dict = { } 
for reg in quantum_registers : 
~~~ qreg_dict [ reg . name ] = reg 
~~ for reg in classical_registers : 
~~~ creg_dict [ reg . name ] = reg 
~~ for i in x . instructions : 
~~~ instr_method = getattr ( circuit , i . name ) 
~~~ for qubit in i . qubits : 
~~~ qubit_label = x . header . qubit_labels [ qubit ] 
qubits . append ( 
qreg_dict [ qubit_label [ 0 ] ] [ qubit_label [ 1 ] ] ) 
~~ clbits = [ ] 
~~~ for clbit in i . memory : 
~~~ clbit_label = x . header . clbit_labels [ clbit ] 
clbits . append ( 
creg_dict [ clbit_label [ 0 ] ] [ clbit_label [ 1 ] ] ) 
~~ params = [ ] 
~~~ params = i . params 
~~ if i . name in [ 'snapshot' ] : 
~~~ instr_method ( 
i . label , 
snapshot_type = i . snapshot_type , 
qubits = qubits , 
params = params ) 
~~ elif i . name == 'initialize' : 
~~~ instr_method ( params , qubits ) 
~~~ instr_method ( * params , * qubits , * clbits ) 
~~ ~~ circuits . append ( circuit ) 
~~ return circuits 
~~ def disassemble ( qobj ) : 
run_config = qobj . config . to_dict ( ) 
user_qobj_header = qobj . header . to_dict ( ) 
circuits = _experiments_to_circuits ( qobj ) 
return circuits , run_config , user_qobj_header 
~~ def hamming_distance ( str1 , str2 ) : 
if len ( str1 ) != len ( str2 ) : 
~~ return sum ( s1 != s2 for s1 , s2 in zip ( str1 , str2 ) ) 
~~ def plot_histogram ( data , figsize = ( 7 , 5 ) , color = None , number_to_keep = None , 
sort = 'asc' , target_string = None , 
legend = None , bar_labels = True , title = None ) : 
~~ if sort not in VALID_SORTS : 
"\ ) 
~~ elif sort in DIST_MEAS . keys ( ) and target_string is None : 
raise VisualizationError ( err_msg ) 
~~ if isinstance ( data , dict ) : 
~~~ data = [ data ] 
~~ if legend and len ( legend ) != len ( data ) : 
( len ( legend ) , len ( data ) ) ) 
~~ fig , ax = plt . subplots ( figsize = figsize ) 
labels = list ( sorted ( 
functools . reduce ( lambda x , y : x . union ( y . keys ( ) ) , data , set ( ) ) ) ) 
if number_to_keep is not None : 
~~~ labels . append ( 'rest' ) 
~~ if sort in DIST_MEAS . keys ( ) : 
~~~ dist = [ ] 
for item in labels : 
~~~ dist . append ( DIST_MEAS [ sort ] ( item , target_string ) ) 
~~ labels = [ list ( x ) for x in zip ( * sorted ( zip ( dist , labels ) , 
key = lambda pair : pair [ 0 ] ) ) ] [ 1 ] 
~~ labels_dict = OrderedDict ( ) 
if color is None : 
~~~ color = [ '#648fff' , '#dc267f' , '#785ef0' , '#ffb000' , '#fe6100' ] 
~~ elif isinstance ( color , str ) : 
~~~ color = [ color ] 
~~ all_pvalues = [ ] 
length = len ( data ) 
for item , execution in enumerate ( data ) : 
~~~ if number_to_keep is not None : 
~~~ data_temp = dict ( Counter ( execution ) . most_common ( number_to_keep ) ) 
data_temp [ "rest" ] = sum ( execution . values ( ) ) - sum ( data_temp . values ( ) ) 
execution = data_temp 
for key in labels : 
~~~ if key not in execution : 
~~~ if number_to_keep is None : 
~~~ labels_dict [ key ] = 1 
values . append ( 0 ) 
~~~ values . append ( - 1 ) 
values . append ( execution [ key ] ) 
~~ ~~ values = np . array ( values , dtype = float ) 
where_idx = np . where ( values >= 0 ) [ 0 ] 
pvalues = values [ where_idx ] / sum ( values [ where_idx ] ) 
for value in pvalues : 
~~~ all_pvalues . append ( value ) 
~~ numelem = len ( values [ where_idx ] ) 
rects = [ ] 
for idx , val in enumerate ( pvalues ) : 
~~~ label = None 
if not idx and legend : 
~~~ label = legend [ item ] 
~~ if val >= 0 : 
~~~ rects . append ( ax . bar ( idx + item * width , val , width , label = label , 
color = color [ item % len ( color ) ] , 
zorder = 2 ) ) 
~~ ~~ bar_center = ( width / 2 ) * ( length - 1 ) 
ax . set_xticks ( ind + bar_center ) 
ax . set_xticklabels ( labels_dict . keys ( ) , fontsize = 14 , rotation = 70 ) 
if bar_labels : 
~~~ for rect in rects : 
~~~ for rec in rect : 
~~~ height = rec . get_height ( ) 
if height >= 1e-3 : 
~~~ ax . text ( rec . get_x ( ) + rec . get_width ( ) / 2. , 1.05 * height , 
'%.3f' % float ( height ) , 
ha = 'center' , va = 'bottom' , zorder = 3 ) 
'0' , 
~~ ~~ ~~ ~~ ~~ ax . set_ylabel ( 'Probabilities' , fontsize = 14 ) 
ax . set_ylim ( [ 0. , min ( [ 1.2 , max ( [ 1.2 * val for val in all_pvalues ] ) ] ) ] ) 
if sort == 'desc' : 
~~~ ax . invert_xaxis ( ) 
~~ ax . yaxis . set_major_locator ( MaxNLocator ( 5 ) ) 
for tick in ax . yaxis . get_major_ticks ( ) : 
~~~ tick . label . set_fontsize ( 14 ) 
~~ ax . set_facecolor ( '#eeeeee' ) 
plt . grid ( which = 'major' , axis = 'y' , zorder = 0 , linestyle = '--' ) 
if title : 
~~~ plt . title ( title ) 
~~ if legend : 
borderaxespad = 0 , frameon = True , fontsize = 12 ) 
~~ if fig : 
~~~ plt . close ( fig ) 
~~ return fig 
~~ def quaternion_from_axis_rotation ( angle , axis ) : 
out = np . zeros ( 4 , dtype = float ) 
if axis == 'x' : 
~~~ out [ 1 ] = 1 
~~ elif axis == 'y' : 
~~~ out [ 2 ] = 1 
~~ elif axis == 'z' : 
~~~ out [ 3 ] = 1 
~~ out *= math . sin ( angle / 2.0 ) 
out [ 0 ] = math . cos ( angle / 2.0 ) 
return Quaternion ( out ) 
~~ def quaternion_from_euler ( angles , order = 'yzy' ) : 
angles = np . asarray ( angles , dtype = float ) 
quat = quaternion_from_axis_rotation ( angles [ 0 ] , order [ 0 ] ) * ( quaternion_from_axis_rotation ( angles [ 1 ] , order [ 1 ] ) 
* quaternion_from_axis_rotation ( angles [ 2 ] , order [ 2 ] ) ) 
quat . normalize ( inplace = True ) 
return quat 
~~ def normalize ( self , inplace = False ) : 
if inplace : 
~~~ nrm = self . norm ( ) 
self . data /= nrm 
~~ nrm = self . norm ( ) 
data_copy = np . array ( self . data , copy = True ) 
data_copy /= nrm 
return Quaternion ( data_copy ) 
mat = np . array ( [ 
[ 1 - 2 * y ** 2 - 2 * z ** 2 , 2 * x * y - 2 * z * w , 2 * x * z + 2 * y * w ] , 
[ 2 * x * y + 2 * z * w , 1 - 2 * x ** 2 - 2 * z ** 2 , 2 * y * z - 2 * x * w ] , 
[ 2 * x * z - 2 * y * w , 2 * y * z + 2 * x * w , 1 - 2 * x ** 2 - 2 * y ** 2 ] 
] , dtype = float ) 
return mat 
~~ def to_zyz ( self ) : 
mat = self . to_matrix ( ) 
euler = np . zeros ( 3 , dtype = float ) 
if mat [ 2 , 2 ] < 1 : 
~~~ if mat [ 2 , 2 ] > - 1 : 
~~~ euler [ 0 ] = math . atan2 ( mat [ 1 , 2 ] , mat [ 0 , 2 ] ) 
euler [ 1 ] = math . acos ( mat [ 2 , 2 ] ) 
euler [ 2 ] = math . atan2 ( mat [ 2 , 1 ] , - mat [ 2 , 0 ] ) 
~~~ euler [ 0 ] = - math . atan2 ( mat [ 1 , 0 ] , mat [ 1 , 1 ] ) 
euler [ 1 ] = np . pi 
~~~ euler [ 0 ] = math . atan2 ( mat [ 1 , 0 ] , mat [ 1 , 1 ] ) 
~~ return euler 
~~ def process_data ( data , number_to_keep ) : 
if number_to_keep != 0 : 
~~~ data_temp = dict ( Counter ( data ) . most_common ( number_to_keep ) ) 
data_temp [ 'rest' ] = sum ( data . values ( ) ) - sum ( data_temp . values ( ) ) 
data = data_temp 
~~ labels = data 
values = np . array ( [ data [ key ] for key in labels ] , dtype = float ) 
pvalues = values / sum ( values ) 
for position , label in enumerate ( labels ) : 
~~~ result [ label ] = round ( pvalues [ position ] , 5 ) 
~~ def iplot_histogram ( data , figsize = None , number_to_keep = None , 
sort = 'asc' , legend = None ) : 
div_number = str ( time . time ( ) ) 
div_number = re . sub ( '[.]' , '' , div_number ) 
if figsize is None : 
~~~ figsize = ( 7 , 5 ) 
~~ options = { 'number_to_keep' : 0 if number_to_keep is None else number_to_keep , 
'sort' : sort , 
'show_legend' : 0 , 
'width' : int ( figsize [ 0 ] ) , 
'height' : int ( figsize [ 1 ] ) } 
if legend : 
~~~ options [ 'show_legend' ] = 1 
~~ data_to_plot = [ ] 
if isinstance ( data , dict ) : 
~~ for item , execution in enumerate ( data ) : 
~~~ exec_data = process_data ( execution , options [ 'number_to_keep' ] ) 
out_dict = { 'data' : exec_data } 
~~~ out_dict [ 'name' ] = legend [ item ] 
~~ data_to_plot . append ( out_dict ) 
~~ html = html_template . substitute ( { 
'divNumber' : div_number 
javascript = javascript_template . substitute ( { 
'divNumber' : div_number , 
'executions' : data_to_plot , 
'options' : options 
display ( HTML ( html + javascript ) ) 
root_value = super ( InstructionParameter , self ) . check_type ( 
value , attr , data ) 
if is_collection ( value ) : 
~~~ _ = [ super ( InstructionParameter , self ) . check_type ( item , attr , data ) 
for item in value ] 
~~ return root_value 
~~ def check_range ( self , j ) : 
if isinstance ( j , int ) : 
~~~ if j < 0 or j >= self . size : 
~~ elif isinstance ( j , slice ) : 
~~~ if j . start < 0 or j . stop >= self . size or ( j . step is not None and 
j . step <= 0 ) : 
~~ ~~ ~~ ~~ def to_string ( self , indent ) : 
if self . root : 
~~~ print ( ind , self . type , '---' , self . root ) 
~~~ print ( ind , self . type ) 
~~ indent = indent + 3 
for children in self . children : 
~~~ if children is None : 
print ( self . children ) 
~~ if isinstance ( children , str ) : 
~~~ print ( ind , children ) 
~~ elif isinstance ( children , int ) : 
~~~ print ( ind , str ( children ) ) 
~~ elif isinstance ( children , float ) : 
~~~ children . to_string ( indent ) 
~~ ~~ ~~ def assemble ( self ) : 
instruction = super ( ) . assemble ( ) 
if self . label : 
~~~ instruction . label = self . label 
~~ def is_square_matrix ( mat ) : 
mat = np . array ( mat ) 
if mat . ndim != 2 : 
~~ shape = mat . shape 
return shape [ 0 ] == shape [ 1 ] 
~~ def is_diagonal_matrix ( mat , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : 
if atol is None : 
~~~ atol = ATOL_DEFAULT 
~~~ rtol = RTOL_DEFAULT 
~~ mat = np . array ( mat ) 
~~ return np . allclose ( mat , np . diag ( np . diagonal ( mat ) ) , rtol = rtol , atol = atol ) 
~~ def is_symmetric_matrix ( op , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : 
~~ mat = np . array ( op ) 
~~ return np . allclose ( mat , mat . T , rtol = rtol , atol = atol ) 
~~ def is_hermitian_matrix ( mat , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : 
~~ return np . allclose ( mat , np . conj ( mat . T ) , rtol = rtol , atol = atol ) 
~~ def is_positive_semidefinite_matrix ( mat , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : 
~~ if not is_hermitian_matrix ( mat , rtol = rtol , atol = atol ) : 
~~ vals = np . linalg . eigvalsh ( mat ) 
for v in vals : 
~~~ if v < - atol : 
~~ def is_identity_matrix ( mat , 
ignore_phase = False , 
rtol = RTOL_DEFAULT , 
atol = ATOL_DEFAULT ) : 
~~ if ignore_phase : 
~~~ theta = np . angle ( mat [ 0 , 0 ] ) 
mat = np . exp ( - 1j * theta ) * mat 
~~ iden = np . eye ( len ( mat ) ) 
return np . allclose ( mat , iden , rtol = rtol , atol = atol ) 
~~ def is_unitary_matrix ( mat , rtol = RTOL_DEFAULT , atol = ATOL_DEFAULT ) : 
mat = np . conj ( mat . T ) . dot ( mat ) 
return is_identity_matrix ( mat , ignore_phase = False , rtol = rtol , atol = atol ) 
swaps = dag . op_nodes ( SwapGate ) 
for swap in swaps : 
~~~ final_successor = [ ] 
for successor in dag . successors ( swap ) : 
~~~ final_successor . append ( successor . type == 'out' or ( successor . type == 'op' and 
successor . op . name == 'measure' ) ) 
~~ if all ( final_successor ) : 
~~~ swap_qargs = swap . qargs 
measure_layer = DAGCircuit ( ) 
~~~ measure_layer . add_qreg ( qreg ) 
~~~ measure_layer . add_creg ( creg ) 
~~ for successor in dag . successors ( swap ) : 
~~~ if successor . type == 'op' and successor . op . name == 'measure' : 
~~~ dag . remove_op_node ( successor ) 
old_measure_qarg = successor . qargs [ 0 ] 
new_measure_qarg = swap_qargs [ swap_qargs . index ( old_measure_qarg ) - 1 ] 
measure_layer . apply_operation_back ( Measure ( ) , [ new_measure_qarg ] , 
[ successor . cargs [ 0 ] ] ) 
~~ ~~ dag . extend_back ( measure_layer ) 
dag . remove_op_node ( swap ) 
~~ def left_sample ( continuous_pulse : Callable , duration : int , * args , ** kwargs ) -> np . ndarray : 
times = np . arange ( duration ) 
return continuous_pulse ( times , * args , ** kwargs ) 
~~ def _to_choi ( rep , data , input_dim , output_dim ) : 
if rep == 'Choi' : 
~~~ return data 
~~ if rep == 'Operator' : 
~~~ return _from_operator ( 'Choi' , data , input_dim , output_dim ) 
~~ if rep == 'SuperOp' : 
~~~ return _superop_to_choi ( data , input_dim , output_dim ) 
~~ if rep == 'Kraus' : 
~~~ return _kraus_to_choi ( data , input_dim , output_dim ) 
~~ if rep == 'Chi' : 
~~~ return _chi_to_choi ( data , input_dim , output_dim ) 
~~ if rep == 'PTM' : 
~~~ data = _ptm_to_superop ( data , input_dim , output_dim ) 
return _superop_to_choi ( data , input_dim , output_dim ) 
~~ if rep == 'Stinespring' : 
~~~ return _stinespring_to_choi ( data , input_dim , output_dim ) 
~~ def _to_superop ( rep , data , input_dim , output_dim ) : 
if rep == 'SuperOp' : 
~~~ return _from_operator ( 'SuperOp' , data , input_dim , output_dim ) 
~~ if rep == 'Choi' : 
~~~ return _choi_to_superop ( data , input_dim , output_dim ) 
~~~ return _kraus_to_superop ( data , input_dim , output_dim ) 
~~~ data = _chi_to_choi ( data , input_dim , output_dim ) 
return _choi_to_superop ( data , input_dim , output_dim ) 
~~~ return _ptm_to_superop ( data , input_dim , output_dim ) 
~~~ return _stinespring_to_superop ( data , input_dim , output_dim ) 
~~ def _to_kraus ( rep , data , input_dim , output_dim ) : 
if rep == 'Kraus' : 
~~~ return _stinespring_to_kraus ( data , input_dim , output_dim ) 
~~~ return _from_operator ( 'Kraus' , data , input_dim , output_dim ) 
~~ if rep != 'Choi' : 
~~~ data = _to_choi ( rep , data , input_dim , output_dim ) 
~~ return _choi_to_kraus ( data , input_dim , output_dim ) 
~~ def _to_chi ( rep , data , input_dim , output_dim ) : 
if rep == 'Chi' : 
~~ _check_nqubit_dim ( input_dim , output_dim ) 
if rep == 'Operator' : 
~~~ return _from_operator ( 'Chi' , data , input_dim , output_dim ) 
~~ return _choi_to_chi ( data , input_dim , output_dim ) 
~~ def _to_ptm ( rep , data , input_dim , output_dim ) : 
if rep == 'PTM' : 
~~~ return _from_operator ( 'PTM' , data , input_dim , output_dim ) 
~~ if rep != 'SuperOp' : 
~~~ data = _to_superop ( rep , data , input_dim , output_dim ) 
~~ return _superop_to_ptm ( data , input_dim , output_dim ) 
~~ def _to_stinespring ( rep , data , input_dim , output_dim ) : 
if rep == 'Stinespring' : 
~~~ return _from_operator ( 'Stinespring' , data , input_dim , output_dim ) 
~~ if rep != 'Kraus' : 
~~~ data = _to_kraus ( rep , data , input_dim , output_dim ) 
~~ return _kraus_to_stinespring ( data , input_dim , output_dim ) 
~~ def _to_operator ( rep , data , input_dim , output_dim ) : 
~~~ return _stinespring_to_operator ( data , input_dim , output_dim ) 
~~ return _kraus_to_operator ( data , input_dim , output_dim ) 
~~ def _from_operator ( rep , data , input_dim , output_dim ) : 
~~~ return np . kron ( np . conj ( data ) , data ) 
~~~ vec = np . ravel ( data , order = 'F' ) 
return np . outer ( vec , np . conj ( vec ) ) 
~~~ return ( [ data ] , None ) 
~~~ return ( data , None ) 
~~~ _check_nqubit_dim ( input_dim , output_dim ) 
data = _from_operator ( 'Choi' , data , input_dim , output_dim ) 
return _choi_to_chi ( data , input_dim , output_dim ) 
data = _from_operator ( 'SuperOp' , data , input_dim , output_dim ) 
return _superop_to_ptm ( data , input_dim , output_dim ) 
~~ def _stinespring_to_operator ( data , input_dim , output_dim ) : 
trace_dim = data [ 0 ] . shape [ 0 ] // output_dim 
if data [ 1 ] is not None or trace_dim != 1 : 
~~ return data [ 0 ] 
~~ def _superop_to_choi ( data , input_dim , output_dim ) : 
shape = ( output_dim , output_dim , input_dim , input_dim ) 
return _reshuffle ( data , shape ) 
~~ def _choi_to_superop ( data , input_dim , output_dim ) : 
shape = ( input_dim , output_dim , input_dim , output_dim ) 
~~ def _kraus_to_choi ( data , input_dim , output_dim ) : 
choi = 0 
kraus_l , kraus_r = data 
~~~ for i in kraus_l : 
~~~ vec = i . ravel ( order = 'F' ) 
choi += np . outer ( vec , vec . conj ( ) ) 
~~~ for i , j in zip ( kraus_l , kraus_r ) : 
~~~ choi += np . outer ( i . ravel ( order = 'F' ) , j . ravel ( order = 'F' ) . conj ( ) ) 
~~ ~~ return choi 
~~ def _choi_to_kraus ( data , input_dim , output_dim , atol = ATOL_DEFAULT ) : 
if is_hermitian_matrix ( data , atol = atol ) : 
~~~ w , v = la . eigh ( data ) 
if len ( w [ w < - atol ] ) == 0 : 
~~~ kraus = [ ] 
for val , vec in zip ( w , v . T ) : 
~~~ if abs ( val ) > atol : 
~~~ k = np . sqrt ( val ) * vec . reshape ( 
( output_dim , input_dim ) , order = 'F' ) 
kraus . append ( k ) 
~~ ~~ if not kraus : 
~~~ kraus . append ( np . zeros ( ( output_dim , input_dim ) , dtype = complex ) ) 
~~ return ( kraus , None ) 
~~ ~~ mat_u , svals , mat_vh = la . svd ( data ) 
kraus_l = [ ] 
kraus_r = [ ] 
for val , vec_l , vec_r in zip ( svals , mat_u . T , mat_vh . conj ( ) ) : 
~~~ kraus_l . append ( 
np . sqrt ( val ) * vec_l . reshape ( ( output_dim , input_dim ) , order = 'F' ) ) 
kraus_r . append ( 
np . sqrt ( val ) * vec_r . reshape ( ( output_dim , input_dim ) , order = 'F' ) ) 
~~ return ( kraus_l , kraus_r ) 
~~ def _stinespring_to_kraus ( data , input_dim , output_dim ) : 
kraus_pair = [ ] 
for stine in data : 
~~~ if stine is None : 
~~~ kraus_pair . append ( None ) 
~~~ trace_dim = stine . shape [ 0 ] // output_dim 
iden = np . eye ( output_dim ) 
kraus = [ ] 
for j in range ( trace_dim ) : 
~~~ vec = np . zeros ( trace_dim ) 
vec [ j ] = 1 
kraus . append ( np . kron ( iden , vec [ None , : ] ) . dot ( stine ) ) 
~~ kraus_pair . append ( kraus ) 
~~ ~~ return tuple ( kraus_pair ) 
~~ def _stinespring_to_choi ( data , input_dim , output_dim ) : 
stine_l = np . reshape ( data [ 0 ] , ( output_dim , trace_dim , input_dim ) ) 
if data [ 1 ] is None : 
~~~ stine_r = stine_l 
~~~ stine_r = np . reshape ( data [ 1 ] , ( output_dim , trace_dim , input_dim ) ) 
~~ return np . reshape ( 
np . einsum ( 'iAj,kAl->jilk' , stine_l , stine_r . conj ( ) ) , 
2 * [ input_dim * output_dim ] ) 
~~ def _kraus_to_stinespring ( data , input_dim , output_dim ) : 
stine_pair = [ None , None ] 
for i , kraus in enumerate ( data ) : 
~~~ if kraus is not None : 
~~~ num_kraus = len ( kraus ) 
stine = np . zeros ( ( output_dim * num_kraus , input_dim ) , 
for j , mat in enumerate ( kraus ) : 
~~~ vec = np . zeros ( num_kraus ) 
stine += np . kron ( mat , vec [ : , None ] ) 
~~ stine_pair [ i ] = stine 
~~ ~~ return tuple ( stine_pair ) 
~~ def _kraus_to_superop ( data , input_dim , output_dim ) : 
superop = 0 
~~~ superop += np . kron ( np . conj ( i ) , i ) 
~~~ superop += np . kron ( np . conj ( j ) , i ) 
~~ ~~ return superop 
~~ def _chi_to_choi ( data , input_dim , output_dim ) : 
num_qubits = int ( np . log2 ( input_dim ) ) 
return _transform_from_pauli ( data , num_qubits ) 
~~ def _choi_to_chi ( data , input_dim , output_dim ) : 
return _transform_to_pauli ( data , num_qubits ) 
~~ def _bipartite_tensor ( mat1 , mat2 , shape1 = None , shape2 = None ) : 
mat1 = np . array ( mat1 ) 
mat2 = np . array ( mat2 ) 
dim_a0 , dim_a1 = mat1 . shape 
dim_b0 , dim_b1 = mat2 . shape 
if shape1 is None : 
~~~ sdim_a0 = int ( np . sqrt ( dim_a0 ) ) 
sdim_a1 = int ( np . sqrt ( dim_a1 ) ) 
shape1 = ( sdim_a0 , sdim_a0 , sdim_a1 , sdim_a1 ) 
~~ if shape2 is None : 
~~~ sdim_b0 = int ( np . sqrt ( dim_b0 ) ) 
sdim_b1 = int ( np . sqrt ( dim_b1 ) ) 
shape2 = ( sdim_b0 , sdim_b0 , sdim_b1 , sdim_b1 ) 
~~ if len ( shape1 ) != 4 or shape1 [ 0 ] * shape1 [ 1 ] != dim_a0 or shape1 [ 2 ] * shape1 [ 3 ] != dim_a1 : 
~~ if len ( shape2 ) != 4 or shape2 [ 0 ] * shape2 [ 1 ] != dim_b0 or shape2 [ 2 ] * shape2 [ 3 ] != dim_b1 : 
~~ return _reravel ( mat1 , mat2 , shape1 , shape2 ) 
~~ def _reravel ( mat1 , mat2 , shape1 , shape2 ) : 
left_dims = shape1 [ : 2 ] + shape2 [ : 2 ] 
right_dims = shape1 [ 2 : ] + shape2 [ 2 : ] 
tensor_shape = left_dims + right_dims 
final_shape = ( np . product ( left_dims ) , np . product ( right_dims ) ) 
data = np . kron ( mat1 , mat2 ) 
data = np . reshape ( 
np . transpose ( np . reshape ( data , tensor_shape ) , ( 0 , 2 , 1 , 3 , 4 , 6 , 5 , 7 ) ) , 
final_shape ) 
return data 
~~ def _transform_from_pauli ( data , num_qubits ) : 
basis_mat = np . array ( 
[ [ 1 , 0 , 0 , 1 ] , [ 0 , 1 , 1j , 0 ] , [ 0 , 1 , - 1j , 0 ] , [ 1 , 0j , 0 , - 1 ] ] , 
cob = basis_mat 
for _ in range ( num_qubits - 1 ) : 
~~~ dim = int ( np . sqrt ( len ( cob ) ) ) 
cob = np . reshape ( 
np . transpose ( 
np . reshape ( 
np . kron ( basis_mat , cob ) , ( 2 , 2 , dim , dim , 4 , dim * dim ) ) , 
( 0 , 2 , 1 , 3 , 4 , 5 ) ) , ( 4 * dim * dim , 4 * dim * dim ) ) 
~~ return np . dot ( np . dot ( cob , data ) , cob . conj ( ) . T ) / 2 ** num_qubits 
~~ def _reshuffle ( mat , shape ) : 
return np . reshape ( 
np . transpose ( np . reshape ( mat , shape ) , ( 3 , 1 , 2 , 0 ) ) , 
( shape [ 3 ] * shape [ 1 ] , shape [ 0 ] * shape [ 2 ] ) ) 
~~ def _check_nqubit_dim ( input_dim , output_dim ) : 
if input_dim != output_dim : 
~~ num_qubits = int ( np . log2 ( input_dim ) ) 
if 2 ** num_qubits != input_dim : 
~~ ~~ def _hide_tick_lines_and_labels ( axis ) : 
for item in axis . get_ticklines ( ) + axis . get_ticklabels ( ) : 
~~~ item . set_visible ( False ) 
~~ ~~ def set_label_convention ( self , convention ) : 
ketex = "$\\\\left.|%s\\\\right\\\\rangle$" 
if convention == "original" : 
~~~ self . xlabel = [ '$x$' , '' ] 
self . ylabel = [ '$y$' , '' ] 
self . zlabel = [ '$\\\\left|0\\\\right>$' , '$\\\\left|1\\\\right>$' ] 
~~ elif convention == "xyz" : 
self . zlabel = [ '$z$' , '' ] 
~~~ self . xlabel = [ '$s_x$' , '' ] 
self . ylabel = [ '$s_y$' , '' ] 
self . zlabel = [ '$s_z$' , '' ] 
~~ elif convention == "01" : 
~~~ self . xlabel = [ '' , '' ] 
self . ylabel = [ '' , '' ] 
~~~ self . xlabel = [ ketex % "\\\\nearrow\\\\hspace{-1.46}\\\\swarrow" , 
ketex % "\\\\nwarrow\\\\hspace{-1.46}\\\\searrow" ] 
self . ylabel = [ ketex % "\\\\circlearrowleft" , ketex % 
"\\\\circlearrowright" ] 
self . zlabel = [ ketex % "\\\\leftrightarrow" , ketex % "\\\\updownarrow" ] 
~~~ self . xlabel = [ ketex % "D" , ketex % "A" ] 
self . ylabel = [ ketex % "L" , ketex % "R" ] 
self . zlabel = [ ketex % "H" , ketex % "V" ] 
~~~ self . ylabel = [ "$\\\\nearrow\\\\hspace{-1.46}\\\\swarrow$" , 
"$\\\\nwarrow\\\\hspace{-1.46}\\\\searrow$" ] 
self . zlabel = [ "$\\\\circlearrowleft$" , "$\\\\circlearrowright$" ] 
self . xlabel = [ "$\\\\leftrightarrow$" , "$\\\\updownarrow$" ] 
~~ ~~ def clear ( self ) : 
self . points = [ ] 
self . vectors = [ ] 
self . point_style = [ ] 
self . annotations = [ ] 
~~ def add_points ( self , points , meth = 's' ) : 
if not isinstance ( points [ 0 ] , ( list , np . ndarray ) ) : 
~~~ points = [ [ points [ 0 ] ] , [ points [ 1 ] ] , [ points [ 2 ] ] ] 
~~ points = np . array ( points ) 
if meth == 's' : 
~~~ if len ( points [ 0 ] ) == 1 : 
~~~ pnts = np . array ( [ [ points [ 0 ] [ 0 ] ] , 
[ points [ 1 ] [ 0 ] ] , [ points [ 2 ] [ 0 ] ] ] ) 
pnts = np . append ( pnts , points , axis = 1 ) 
~~~ pnts = points 
~~ self . points . append ( pnts ) 
self . point_style . append ( 's' ) 
~~ elif meth == 'l' : 
~~~ self . points . append ( points ) 
self . point_style . append ( 'l' ) 
self . point_style . append ( 'm' ) 
~~ ~~ def add_vectors ( self , vectors ) : 
if isinstance ( vectors [ 0 ] , ( list , np . ndarray ) ) : 
~~~ for vec in vectors : 
~~~ self . vectors . append ( vec ) 
~~~ self . vectors . append ( vectors ) 
~~ ~~ def add_annotation ( self , state_or_vector , text , ** kwargs ) : 
if isinstance ( state_or_vector , ( list , np . ndarray , tuple ) ) and len ( state_or_vector ) == 3 : 
~~~ vec = state_or_vector 
~~ self . annotations . append ( { 'position' : vec , 
'opts' : kwargs } ) 
~~ def render ( self , title = '' ) : 
if self . _rendered : 
~~~ self . axes . clear ( ) 
~~ self . _rendered = True 
if not self . _ext_fig : 
~~~ self . fig = plt . figure ( figsize = self . figsize ) 
~~ if not self . _ext_axes : 
~~~ self . axes = Axes3D ( self . fig , azim = self . view [ 0 ] , elev = self . view [ 1 ] ) 
~~ if self . background : 
self . axes . set_xlim3d ( - 1.3 , 1.3 ) 
self . axes . set_ylim3d ( - 1.3 , 1.3 ) 
self . axes . set_zlim3d ( - 1.3 , 1.3 ) 
~~~ self . plot_axes ( ) 
self . axes . set_axis_off ( ) 
self . axes . set_xlim3d ( - 0.7 , 0.7 ) 
self . axes . set_ylim3d ( - 0.7 , 0.7 ) 
self . axes . set_zlim3d ( - 0.7 , 0.7 ) 
~~ self . axes . grid ( False ) 
self . plot_back ( ) 
self . plot_points ( ) 
self . plot_vectors ( ) 
self . plot_front ( ) 
self . plot_axes_labels ( ) 
self . plot_annotations ( ) 
self . axes . set_title ( title , fontsize = self . font_size , y = 1.08 ) 
~~ def plot_front ( self ) : 
u_angle = np . linspace ( - np . pi , 0 , 25 ) 
v_angle = np . linspace ( 0 , np . pi , 25 ) 
x_dir = np . outer ( np . cos ( u_angle ) , np . sin ( v_angle ) ) 
y_dir = np . outer ( np . sin ( u_angle ) , np . sin ( v_angle ) ) 
z_dir = np . outer ( np . ones ( u_angle . shape [ 0 ] ) , np . cos ( v_angle ) ) 
self . axes . plot_surface ( x_dir , y_dir , z_dir , rstride = 2 , cstride = 2 , 
color = self . sphere_color , linewidth = 0 , 
alpha = self . sphere_alpha ) 
self . axes . plot_wireframe ( x_dir , y_dir , z_dir , rstride = 5 , cstride = 5 , 
color = self . frame_color , 
alpha = self . frame_alpha ) 
self . axes . plot ( 1.0 * np . cos ( u_angle ) , 1.0 * np . sin ( u_angle ) , 
zs = 0 , zdir = 'z' , lw = self . frame_width , 
color = self . frame_color ) 
zs = 0 , zdir = 'x' , lw = self . frame_width , 
~~ def plot_axes ( self ) : 
~~~ """axes""" 
span = np . linspace ( - 1.0 , 1.0 , 2 ) 
self . axes . plot ( span , 0 * span , zs = 0 , zdir = 'z' , label = 'X' , 
lw = self . frame_width , color = self . frame_color ) 
self . axes . plot ( 0 * span , span , zs = 0 , zdir = 'z' , label = 'Y' , 
self . axes . plot ( 0 * span , span , zs = 0 , zdir = 'y' , label = 'Z' , 
~~ def plot_axes_labels ( self ) : 
opts = { 'fontsize' : self . font_size , 
'color' : self . font_color , 
'horizontalalignment' : 'center' , 
'verticalalignment' : 'center' } 
self . axes . text ( 0 , - self . xlpos [ 0 ] , 0 , self . xlabel [ 0 ] , ** opts ) 
self . axes . text ( 0 , - self . xlpos [ 1 ] , 0 , self . xlabel [ 1 ] , ** opts ) 
self . axes . text ( self . ylpos [ 0 ] , 0 , 0 , self . ylabel [ 0 ] , ** opts ) 
self . axes . text ( self . ylpos [ 1 ] , 0 , 0 , self . ylabel [ 1 ] , ** opts ) 
self . axes . text ( 0 , 0 , self . zlpos [ 0 ] , self . zlabel [ 0 ] , ** opts ) 
self . axes . text ( 0 , 0 , self . zlpos [ 1 ] , self . zlabel [ 1 ] , ** opts ) 
for item in ( self . axes . w_xaxis . get_ticklines ( ) + 
self . axes . w_xaxis . get_ticklabels ( ) ) : 
~~ for item in ( self . axes . w_yaxis . get_ticklines ( ) + 
self . axes . w_yaxis . get_ticklabels ( ) ) : 
~~ for item in ( self . axes . w_zaxis . get_ticklines ( ) + 
self . axes . w_zaxis . get_ticklabels ( ) ) : 
~~ ~~ def plot_vectors ( self ) : 
for k in range ( len ( self . vectors ) ) : 
~~~ xs3d = self . vectors [ k ] [ 1 ] * np . array ( [ 0 , 1 ] ) 
ys3d = - self . vectors [ k ] [ 0 ] * np . array ( [ 0 , 1 ] ) 
zs3d = self . vectors [ k ] [ 2 ] * np . array ( [ 0 , 1 ] ) 
color = self . vector_color [ np . mod ( k , len ( self . vector_color ) ) ] 
if self . vector_style == '' : 
~~~ self . axes . plot ( xs3d , ys3d , zs3d , 
zs = 0 , zdir = 'z' , label = 'Z' , 
lw = self . vector_width , color = color ) 
~~~ arr = Arrow3D ( xs3d , ys3d , zs3d , 
mutation_scale = self . vector_mutation , 
lw = self . vector_width , 
arrowstyle = self . vector_style , 
color = color ) 
self . axes . add_artist ( arr ) 
~~ ~~ ~~ def plot_points ( self ) : 
for k in range ( len ( self . points ) ) : 
~~~ num = len ( self . points [ k ] [ 0 ] ) 
dist = [ np . sqrt ( self . points [ k ] [ 0 ] [ j ] ** 2 + 
self . points [ k ] [ 1 ] [ j ] ** 2 + 
self . points [ k ] [ 2 ] [ j ] ** 2 ) for j in range ( num ) ] 
if any ( abs ( dist - dist [ 0 ] ) / dist [ 0 ] > 1e-12 ) : 
~~~ zipped = list ( zip ( dist , range ( num ) ) ) 
dist , indperm = zip ( * zipped ) 
indperm = np . array ( indperm ) 
~~~ indperm = np . arange ( num ) 
~~ if self . point_style [ k ] == 's' : 
~~~ self . axes . scatter ( 
np . real ( self . points [ k ] [ 1 ] [ indperm ] ) , 
- np . real ( self . points [ k ] [ 0 ] [ indperm ] ) , 
np . real ( self . points [ k ] [ 2 ] [ indperm ] ) , 
s = self . point_size [ np . mod ( k , len ( self . point_size ) ) ] , 
alpha = 1 , 
edgecolor = 'none' , 
zdir = 'z' , 
color = self . point_color [ np . mod ( k , len ( self . point_color ) ) ] , 
marker = self . point_marker [ np . mod ( k , 
len ( self . point_marker ) ) ] ) 
~~ elif self . point_style [ k ] == 'm' : 
~~~ pnt_colors = np . array ( self . point_color * 
int ( np . ceil ( num / 
float ( len ( self . point_color ) ) ) ) ) 
pnt_colors = pnt_colors [ 0 : num ] 
pnt_colors = list ( pnt_colors [ indperm ] ) 
marker = self . point_marker [ np . mod ( k , len ( self . point_marker ) ) ] 
pnt_size = self . point_size [ np . mod ( k , len ( self . point_size ) ) ] 
self . axes . scatter ( np . real ( self . points [ k ] [ 1 ] [ indperm ] ) , 
s = pnt_size , alpha = 1 , edgecolor = 'none' , 
zdir = 'z' , color = pnt_colors , 
marker = marker ) 
~~ elif self . point_style [ k ] == 'l' : 
~~~ color = self . point_color [ np . mod ( k , len ( self . point_color ) ) ] 
self . axes . plot ( np . real ( self . points [ k ] [ 1 ] ) , 
- np . real ( self . points [ k ] [ 0 ] ) , 
np . real ( self . points [ k ] [ 2 ] ) , 
alpha = 0.75 , zdir = 'z' , 
~~ ~~ ~~ def plot_annotations ( self ) : 
for annotation in self . annotations : 
~~~ vec = annotation [ 'position' ] 
opts . update ( annotation [ 'opts' ] ) 
self . axes . text ( vec [ 1 ] , - vec [ 0 ] , vec [ 2 ] , 
annotation [ 'text' ] , ** opts ) 
~~ ~~ def show ( self , title = '' ) : 
self . render ( title = title ) 
if self . fig : 
~~~ plt . show ( self . fig ) 
~~ ~~ def save ( self , name = None , output = 'png' , dirc = None ) : 
self . render ( ) 
if dirc : 
~~~ if not os . path . isdir ( os . getcwd ( ) + "/" + str ( dirc ) ) : 
~~~ os . makedirs ( os . getcwd ( ) + "/" + str ( dirc ) ) 
~~ ~~ if name is None : 
~~~ if dirc : 
~~~ self . fig . savefig ( os . getcwd ( ) + "/" + str ( dirc ) + '/bloch_' + 
str ( self . savenum ) + '.' + output ) 
~~~ self . fig . savefig ( os . getcwd ( ) + '/bloch_' + str ( self . savenum ) + 
'.' + output ) 
~~~ self . fig . savefig ( name ) 
~~ self . savenum += 1 
~~~ plt . close ( self . fig ) 
~~ ~~ def two_qubit_kak ( unitary_matrix , verify_gate_sequence = False ) : 
"qiskit.quantum_info.synthesis" , DeprecationWarning ) 
return synthesis . two_qubit_kak ( unitary_matrix ) 
~~ def top ( self ) : 
ret = self . top_format % self . top_connect . center ( 
self . width , self . top_pad ) 
if self . right_fill : 
~~~ ret = ret . ljust ( self . right_fill , self . top_pad ) 
~~ if self . left_fill : 
~~~ ret = ret . rjust ( self . left_fill , self . top_pad ) 
~~ ret = ret . center ( self . layer_width , self . top_bck ) 
~~ def mid ( self ) : 
ret = self . mid_format % self . mid_content . center ( 
self . width , self . _mid_padding ) 
~~~ ret = ret . ljust ( self . right_fill , self . _mid_padding ) 
~~~ ret = ret . rjust ( self . left_fill , self . _mid_padding ) 
~~ ret = ret . center ( self . layer_width , self . mid_bck ) 
~~ def bot ( self ) : 
ret = self . bot_format % self . bot_connect . center ( 
self . width , self . bot_pad ) 
~~~ ret = ret . ljust ( self . right_fill , self . bot_pad ) 
~~~ ret = ret . rjust ( self . left_fill , self . bot_pad ) 
~~ ret = ret . center ( self . layer_width , self . bot_bck ) 
~~ def length ( self ) : 
return max ( len ( self . top ) , len ( self . mid ) , len ( self . bot ) ) 
~~ def connect ( self , wire_char , where , label = None ) : 
if 'top' in where and self . top_connector : 
~~~ self . top_connect = self . top_connector [ wire_char ] 
~~ if 'bot' in where and self . bot_connector : 
~~~ self . bot_connect = self . bot_connector [ wire_char ] 
~~ if label : 
~~~ self . top_format = self . top_format [ : - 1 ] + ( label if label else "" ) 
~~ ~~ def center_label ( self , input_length , order ) : 
location_in_the_box = '*' . center ( input_length * 2 - 1 ) . index ( '*' ) + 1 
top_limit = order * 2 + 2 
bot_limit = top_limit + 2 
if top_limit <= location_in_the_box < bot_limit : 
~~~ if location_in_the_box == top_limit : 
~~~ self . top_connect = self . label 
~~ elif location_in_the_box == top_limit + 1 : 
~~~ self . mid_content = self . label 
~~~ self . bot_connect = self . label 
~~ ~~ ~~ def fillup_layer ( layer , first_clbit ) : 
for nones in [ i for i , x in enumerate ( layer ) if x is None ] : 
~~~ layer [ nones ] = EmptyWire ( '═' ) if nones >= first_clbit else EmptyWire ( '─' ) 
~~ return layer 
~~ def fillup_layer ( layer_length , arrow_char ) : 
breakwire_layer = [ ] 
for _ in range ( layer_length ) : 
~~~ breakwire_layer . append ( BreakWire ( arrow_char ) ) 
~~ return breakwire_layer 
longest = max ( [ len ( name ) for name in names ] ) 
inputs_wires = [ ] 
for name in names : 
~~~ inputs_wires . append ( InputWire ( name . rjust ( longest ) ) ) 
~~ return inputs_wires 
~~ def dump ( self , filename , encoding = "utf8" ) : 
with open ( filename , mode = 'w' , encoding = encoding ) as text_file : 
~~~ text_file . write ( self . single_string ( ) ) 
~~ ~~ def lines ( self , line_length = None ) : 
if line_length is None : 
~~~ line_length = self . line_length 
~~ if line_length is None : 
~~~ if ( 'ipykernel' in sys . modules ) and ( 'spyder' not in sys . modules ) : 
~~~ line_length = 80 
~~~ line_length , _ = get_terminal_size ( ) 
~~ ~~ noqubits = len ( self . qregs ) 
layers = self . build_layers ( ) 
if not line_length : 
~~ layer_groups = [ [ ] ] 
rest_of_the_line = line_length 
for layerno , layer in enumerate ( layers ) : 
~~~ layers [ layerno ] = EmptyWire . fillup_layer ( layer , noqubits ) 
TextDrawing . normalize_width ( layer ) 
if line_length == - 1 : 
~~~ layer_groups [ - 1 ] . append ( layer ) 
~~ layer_length = layers [ layerno ] [ 0 ] . length 
if layer_length < rest_of_the_line : 
rest_of_the_line -= layer_length 
~~~ layer_groups [ - 1 ] . append ( BreakWire . fillup_layer ( len ( layer ) , '»' ) ) 
layer_groups . append ( [ BreakWire . fillup_layer ( len ( layer ) , '«' ) ] ) 
rest_of_the_line = line_length - layer_groups [ - 1 ] [ - 1 ] [ 0 ] . length 
layer_groups [ - 1 ] . append ( 
InputWire . fillup_layer ( self . wire_names ( with_initial_value = False ) ) ) 
rest_of_the_line -= layer_groups [ - 1 ] [ - 1 ] [ 0 ] . length 
layer_groups [ - 1 ] . append ( layer ) 
~~ ~~ lines = [ ] 
for layer_group in layer_groups : 
~~~ wires = [ i for i in zip ( * layer_group ) ] 
lines += TextDrawing . draw_wires ( wires , self . vertically_compressed ) 
~~ return lines 
~~ def wire_names ( self , with_initial_value = True ) : 
qubit_labels = self . _get_qubit_labels ( ) 
clbit_labels = self . _get_clbit_labels ( ) 
if with_initial_value : 
~~ return qubit_labels + clbit_labels 
~~ def draw_wires ( wires , vertically_compressed = True ) : 
lines = [ ] 
bot_line = None 
for wire in wires : 
~~~ top_line = '' 
for instruction in wire : 
~~~ top_line += instruction . top 
~~ if bot_line is None : 
~~~ lines . append ( top_line ) 
~~~ if vertically_compressed : 
~~~ lines . append ( TextDrawing . merge_lines ( lines . pop ( ) , top_line ) ) 
~~~ lines . append ( TextDrawing . merge_lines ( lines [ - 1 ] , top_line , icod = "bot" ) ) 
~~ ~~ mid_line = '' 
~~~ mid_line += instruction . mid 
~~ lines . append ( TextDrawing . merge_lines ( lines [ - 1 ] , mid_line , icod = "bot" ) ) 
bot_line = '' 
~~~ bot_line += instruction . bot 
~~ lines . append ( TextDrawing . merge_lines ( lines [ - 1 ] , bot_line , icod = "bot" ) ) 
~~ def params_for_label ( instruction ) : 
if not hasattr ( instruction . op , 'params' ) : 
~~ if all ( [ isinstance ( param , ndarray ) for param in instruction . op . params ] ) : 
for param in instruction . op . params : 
~~~ if isinstance ( param , ( sympy . Number , float ) ) : 
~~~ ret . append ( '%.5g' % param ) 
~~~ ret . append ( '%s' % param ) 
~~ def label_for_box ( instruction ) : 
label = instruction . name . capitalize ( ) 
params = TextDrawing . params_for_label ( instruction ) 
if params : 
~~~ label += "(%s)" % ',' . join ( params ) 
~~ return label 
~~ def merge_lines ( top , bot , icod = "top" ) : 
ret = "" 
for topc , botc in zip ( top , bot ) : 
~~~ if topc == botc : 
~~~ ret += topc 
~~~ ret += "│" 
~~~ ret += botc 
~~~ ret += '│' 
~~~ ret += '║' 
~~ elif topc in '┬│' and botc == "═" : 
~~~ ret += '╪' 
~~ elif topc in '┬│' and botc == "─" : 
~~~ ret += '┼' 
~~ elif topc in "║╥" and botc in "═" : 
~~~ ret += "╬" 
~~ elif topc in "║╥" and botc in "─" : 
~~~ ret += "╫" 
~~~ ret += "║" 
~~ elif topc == '└' and botc == "┌" : 
~~~ ret += "├" 
~~ elif topc == '┘' and botc == "┐" : 
~~~ ret += "┤" 
~~ elif botc in "┐┌" and icod == 'top' : 
~~~ ret += "┬" 
~~ elif topc in "┘└" and botc in "─" and icod == 'top' : 
~~~ ret += "┴" 
~~ def normalize_width ( layer ) : 
instructions = [ instruction for instruction in filter ( lambda x : x is not None , layer ) ] 
longest = max ( [ instruction . length for instruction in instructions ] ) 
for instruction in instructions : 
~~~ instruction . layer_width = longest 
~~ ~~ def _instruction_to_gate ( self , instruction , layer ) : 
current_cons = [ ] 
connection_label = None 
def add_connected_gate ( instruction , gates , layer , current_cons ) : 
~~~ for i , gate in enumerate ( gates ) : 
~~~ layer . set_qubit ( instruction . qargs [ i ] , gate ) 
actual_index = self . qregs . index ( instruction . qargs [ i ] ) 
current_cons . append ( ( actual_index , gate ) ) 
~~ ~~ if instruction . name == 'measure' : 
~~~ gate = MeasureFrom ( ) 
layer . set_qubit ( instruction . qargs [ 0 ] , gate ) 
layer . set_clbit ( instruction . cargs [ 0 ] , MeasureTo ( ) ) 
~~ elif instruction . name in [ 'barrier' , 'snapshot' , 'save' , 'load' , 
'noise' ] : 
~~~ if not self . plotbarriers : 
~~~ return layer , current_cons , connection_label 
~~ for qubit in instruction . qargs : 
~~~ layer . set_qubit ( qubit , Barrier ( ) ) 
~~ ~~ elif instruction . name == 'swap' : 
~~~ gates = [ Ex ( ) for _ in range ( len ( instruction . qargs ) ) ] 
add_connected_gate ( instruction , gates , layer , current_cons ) 
~~ elif instruction . name == 'cswap' : 
~~~ gates = [ Bullet ( ) , Ex ( ) , Ex ( ) ] 
~~ elif instruction . name == 'reset' : 
~~~ layer . set_qubit ( instruction . qargs [ 0 ] , Reset ( ) ) 
~~ elif instruction . condition is not None : 
~~~ cllabel = TextDrawing . label_for_conditional ( instruction ) 
qulabel = TextDrawing . label_for_box ( instruction ) 
layer . set_cl_multibox ( instruction . condition [ 0 ] , cllabel , top_connect = '┴' ) 
layer . set_qubit ( instruction . qargs [ 0 ] , BoxOnQuWire ( qulabel , bot_connect = '┬' ) ) 
~~ elif instruction . name in [ 'cx' , 'CX' , 'ccx' ] : 
~~~ gates = [ Bullet ( ) for _ in range ( len ( instruction . qargs ) - 1 ) ] 
gates . append ( BoxOnQuWire ( 'X' ) ) 
~~ elif instruction . name == 'cy' : 
~~~ gates = [ Bullet ( ) , BoxOnQuWire ( 'Y' ) ] 
~~ elif instruction . name == 'cz' : 
~~~ gates = [ Bullet ( ) , Bullet ( ) ] 
~~ elif instruction . name == 'ch' : 
~~~ gates = [ Bullet ( ) , BoxOnQuWire ( 'H' ) ] 
~~ elif instruction . name == 'cu1' : 
~~~ connection_label = TextDrawing . params_for_label ( instruction ) [ 0 ] 
gates = [ Bullet ( ) , Bullet ( ) ] 
~~ elif instruction . name == 'rzz' : 
~~~ connection_label = "zz(%s)" % TextDrawing . params_for_label ( instruction ) [ 0 ] 
~~ elif instruction . name == 'cu3' : 
~~~ params = TextDrawing . params_for_label ( instruction ) 
gates = [ Bullet ( ) , BoxOnQuWire ( "U3(%s)" % ',' . join ( params ) ) ] 
~~ elif instruction . name == 'crz' : 
~~~ label = "Rz(%s)" % TextDrawing . params_for_label ( instruction ) [ 0 ] 
gates = [ Bullet ( ) , BoxOnQuWire ( label ) ] 
~~ elif len ( instruction . qargs ) == 1 and not instruction . cargs : 
~~~ layer . set_qubit ( instruction . qargs [ 0 ] , 
BoxOnQuWire ( TextDrawing . label_for_box ( instruction ) ) ) 
~~ elif len ( instruction . qargs ) >= 2 and not instruction . cargs : 
~~~ label = instruction . name 
~~ layer . set_qu_multibox ( instruction . qargs , label ) 
~~~ raise VisualizationError ( 
~~ current_cons . sort ( key = lambda tup : tup [ 0 ] ) 
current_cons = [ g for q , g in current_cons ] 
return layer , current_cons , connection_label 
~~ def build_layers ( self ) : 
wire_names = self . wire_names ( with_initial_value = True ) 
if not wire_names : 
~~ layers = [ InputWire . fillup_layer ( wire_names ) ] 
for instruction_layer in self . instructions : 
~~~ layer = Layer ( self . qregs , self . cregs ) 
for instruction in instruction_layer : 
~~~ layer , current_connections , connection_label = self . _instruction_to_gate ( instruction , layer ) 
layer . connections . append ( ( connection_label , current_connections ) ) 
layer . connect_with ( "│" ) 
~~ layers . append ( layer . full_layer ) 
~~ return layers 
~~ def set_qubit ( self , qubit , element ) : 
self . qubit_layer [ self . qregs . index ( qubit ) ] = element 
~~ def set_clbit ( self , clbit , element ) : 
self . clbit_layer [ self . cregs . index ( clbit ) ] = element 
~~ def set_cl_multibox ( self , creg , label , top_connect = '┴' ) : 
clbit = [ bit for bit in self . cregs if bit [ 0 ] == creg ] 
self . _set_multibox ( "cl" , clbit , label , top_connect = top_connect ) 
~~ def connect_with ( self , wire_char ) : 
if len ( [ qbit for qbit in self . qubit_layer if qbit is not None ] ) == 1 : 
~~ for label , affected_bits in self . connections : 
~~~ if not affected_bits : 
~~ affected_bits [ 0 ] . connect ( wire_char , [ 'bot' ] ) 
for affected_bit in affected_bits [ 1 : - 1 ] : 
~~~ affected_bit . connect ( wire_char , [ 'bot' , 'top' ] ) 
~~ affected_bits [ - 1 ] . connect ( wire_char , [ 'top' ] , label ) 
if label : 
~~~ for affected_bit in affected_bits : 
~~~ affected_bit . right_fill = len ( label ) + len ( affected_bit . mid ) 
~~ ~~ ~~ ~~ def latex ( self , prec = 15 , nested_scope = None ) : 
if not nested_scope : 
~~~ return "\\textrm{" + self . name + "}" 
~~~ if self . name not in nested_scope [ - 1 ] : 
"file=%s" % self . file ) 
~~~ return nested_scope [ - 1 ] [ self . name ] . latex ( prec , 
nested_scope [ 0 : - 1 ] ) 
~~ ~~ ~~ def sym ( self , nested_scope = None ) : 
if not nested_scope or self . name not in nested_scope [ - 1 ] : 
self . name , self . line , self . file ) ) 
~~~ return nested_scope [ - 1 ] [ self . name ] . sym ( nested_scope [ 0 : - 1 ] ) 
~~ ~~ def real ( self , nested_scope = None ) : 
~~~ return nested_scope [ - 1 ] [ self . name ] . real ( nested_scope [ 0 : - 1 ] ) 
~~ ~~ def compile ( circuits , backend , 
config = None , basis_gates = None , coupling_map = None , initial_layout = None , 
shots = 1024 , max_credits = 10 , seed = None , qobj_id = None , seed_mapper = None , 
pass_manager = None , memory = False ) : 
new_circuits = transpile ( circuits , 
basis_gates = basis_gates , 
coupling_map = coupling_map , 
initial_layout = initial_layout , 
seed_transpiler = seed_mapper , 
backend = backend , 
pass_manager = pass_manager ) 
qobj = assemble ( new_circuits , 
qobj_header = None , 
shots = shots , 
max_credits = max_credits , 
seed_simulator = seed , 
memory = memory , 
qobj_id = qobj_id , 
return qobj 
~~ def _filter_deprecation_warnings ( ) : 
deprecation_filter = ( 'always' , None , DeprecationWarning , 
re . compile ( r'^qiskit\\.*' , re . UNICODE ) , 0 ) 
~~~ warnings . _add_filter ( * deprecation_filter , append = False ) 
~~ except AttributeError : 
~~ warnings . simplefilter ( 'ignore' , category = ChangedInMarshmallow3Warning ) 
~~ def local_hardware_info ( ) : 
results = { 
'os' : platform . system ( ) , 
'memory' : psutil . virtual_memory ( ) . total / ( 1024 ** 3 ) , 
'cpus' : psutil . cpu_count ( logical = False ) or 1 
~~ def _has_connection ( hostname , port ) : 
~~~ host = socket . gethostbyname ( hostname ) 
socket . create_connection ( ( host , port ) , 2 ) 
~~ ~~ def compose ( self , other , qargs = None , front = False ) : 
~~~ return PTM ( 
~~ if not isinstance ( other , PTM ) : 
~~~ other = PTM ( other ) 
~~~ input_dim = other . _input_dim 
return PTM ( np . dot ( self . _data , other . data ) , input_dim , output_dim ) 
~~ input_dim = self . _input_dim 
return PTM ( np . dot ( other . data , self . _data ) , input_dim , output_dim ) 
~~ return PTM ( SuperOp ( self ) . power ( n ) ) 
~~ def _html_checker ( job_var , interval , status , header , 
_interval_set = False ) : 
job_status = job_var . status ( ) 
job_status_name = job_status . name 
job_status_msg = job_status . value 
status . value = header % ( job_status_msg ) 
while job_status_name not in [ 'DONE' , 'CANCELLED' ] : 
~~~ time . sleep ( interval ) 
if job_status_name == 'ERROR' : 
~~~ if job_status_name == 'QUEUED' : 
if not _interval_set : 
~~~ interval = max ( job_var . queue_position ( ) , 2 ) 
~~~ if not _interval_set : 
~~~ interval = 2 
~~ ~~ status . value = header % ( job_status_msg ) 
~~ def constant ( times : np . ndarray , amp : complex ) -> np . ndarray : 
return np . full ( len ( times ) , amp , dtype = np . complex_ ) 
~~ def square ( times : np . ndarray , amp : complex , period : float , phase : float = 0 ) -> np . ndarray : 
x = times / period + phase / np . pi 
return amp * ( 2 * ( 2 * np . floor ( x ) - np . floor ( 2 * x ) ) + 1 ) . astype ( np . complex_ ) 
~~ def triangle ( times : np . ndarray , amp : complex , period : float , phase : float = 0 ) -> np . ndarray : 
return amp * ( - 2 * np . abs ( sawtooth ( times , 1 , period , ( phase - np . pi / 2 ) / 2 ) ) + 1 ) . astype ( np . complex_ ) 
~~ def cos ( times : np . ndarray , amp : complex , freq : float , phase : float = 0 ) -> np . ndarray : 
return amp * np . cos ( 2 * np . pi * freq * times + phase ) . astype ( np . complex_ ) 
~~ def _fix_gaussian_width ( gaussian_samples , amp : float , center : float , sigma : float , 
zeroed_width : Union [ None , float ] = None , rescale_amp : bool = False , 
ret_scale_factor : bool = False ) -> np . ndarray : 
if zeroed_width is None : 
~~~ zeroed_width = 2 * ( center + 1 ) 
~~ zero_offset = gaussian ( np . array ( [ - zeroed_width / 2 ] ) , amp , center , sigma ) 
gaussian_samples -= zero_offset 
amp_scale_factor = 1. 
if rescale_amp : 
~~~ amp_scale_factor = amp / ( amp - zero_offset ) 
gaussian_samples *= amp_scale_factor 
~~ if ret_scale_factor : 
~~~ return gaussian_samples , amp_scale_factor 
~~ return gaussian_samples 
~~ def gaussian ( times : np . ndarray , amp : complex , center : float , sigma : float , 
ret_x : bool = False ) -> Union [ np . ndarray , Tuple [ np . ndarray , np . ndarray ] ] : 
times = np . asarray ( times , dtype = np . complex_ ) 
x = ( times - center ) / sigma 
gauss = amp * np . exp ( - x ** 2 / 2 ) . astype ( np . complex_ ) 
if zeroed_width is not None : 
~~~ gauss = _fix_gaussian_width ( gauss , amp = amp , center = center , sigma = sigma , 
zeroed_width = zeroed_width , rescale_amp = rescale_amp ) 
~~ if ret_x : 
~~~ return gauss , x 
~~ return gauss 
~~ def gaussian_deriv ( times : np . ndarray , amp : complex , center : float , sigma : float , 
ret_gaussian : bool = False ) -> np . ndarray : 
gauss , x = gaussian ( times , amp = amp , center = center , sigma = sigma , ret_x = True ) 
gauss_deriv = - x / sigma * gauss 
if ret_gaussian : 
~~~ return gauss_deriv , gauss 
~~ return gauss_deriv 
~~ def gaussian_square ( times : np . ndarray , amp : complex , center : float , width : float , 
sigma : float , zeroed_width : Union [ None , float ] = None ) -> np . ndarray : 
square_start = center - width / 2 
square_stop = center + width / 2 
if zeroed_width : 
~~~ zeroed_width = min ( width , zeroed_width ) 
gauss_zeroed_width = zeroed_width - width 
~~~ gauss_zeroed_width = None 
~~ funclist = [ functools . partial ( gaussian , amp = amp , center = square_start , sigma = sigma , 
zeroed_width = gauss_zeroed_width , rescale_amp = True ) , 
functools . partial ( gaussian , amp = amp , center = square_stop , sigma = sigma , 
functools . partial ( constant , amp = amp ) ] 
condlist = [ times <= square_start , times >= square_stop ] 
return np . piecewise ( times . astype ( np . complex_ ) , condlist , funclist ) 
~~ def drag ( times : np . ndarray , amp : complex , center : float , sigma : float , beta : float , 
zeroed_width : Union [ None , float ] = None , rescale_amp : bool = False ) -> np . ndarray : 
gauss_deriv , gauss = gaussian_deriv ( times , amp = amp , center = center , sigma = sigma , 
ret_gaussian = True ) 
~~~ gauss , scale_factor = _fix_gaussian_width ( gauss , amp = amp , center = center , sigma = sigma , 
zeroed_width = zeroed_width , 
rescale_amp = rescale_amp , 
ret_scale_factor = True ) 
gauss_deriv *= scale_factor 
~~ return gauss + 1j * beta * gauss_deriv 
~~ def default_pass_manager ( basis_gates , coupling_map , initial_layout , seed_transpiler ) : 
pass_manager = PassManager ( ) 
pass_manager . property_set [ 'layout' ] = initial_layout 
pass_manager . append ( Unroller ( basis_gates ) ) 
pass_manager . append ( TrivialLayout ( coupling_map ) , 
condition = lambda property_set : not property_set [ 'layout' ] ) 
pass_manager . append ( CheckMap ( coupling_map ) ) 
pass_manager . append ( DenseLayout ( coupling_map ) , 
condition = lambda property_set : not property_set [ 'is_swap_mapped' ] ) 
pass_manager . append ( FullAncillaAllocation ( coupling_map ) ) 
pass_manager . append ( EnlargeWithAncilla ( ) ) 
pass_manager . append ( Unroll3qOrMore ( ) ) 
pass_manager . append ( LegacySwap ( coupling_map , trials = 20 , seed = seed_transpiler ) ) 
pass_manager . append ( Decompose ( SwapGate ) ) 
pass_manager . append ( CXDirection ( coupling_map ) ) 
pass_manager . append ( Unroller ( [ 'u1' , 'u2' , 'u3' , 'id' , 'cx' ] ) ) 
simplification_passes = [ Optimize1qGates ( ) , CXCancellation ( ) , RemoveResetInZeroState ( ) ] 
pass_manager . append ( simplification_passes + [ Depth ( ) , FixedPoint ( 'depth' ) ] , 
do_while = lambda property_set : not property_set [ 'depth_fixed_point' ] ) 
return pass_manager 
~~ def default_pass_manager_simulator ( basis_gates ) : 
pass_manager . append ( [ RemoveResetInZeroState ( ) , Depth ( ) , FixedPoint ( 'depth' ) ] , 
~~ def has_register ( self , register ) : 
has_reg = False 
if ( isinstance ( register , QuantumRegister ) and 
register in self . qregs ) : 
~~~ has_reg = True 
~~ elif ( isinstance ( register , ClassicalRegister ) and 
register in self . cregs ) : 
~~ return has_reg 
reverse_circ = self . copy ( name = self . name + '_mirror' ) 
reverse_circ . data = [ ] 
for inst , qargs , cargs in reversed ( self . data ) : 
~~~ reverse_circ . data . append ( ( inst . mirror ( ) , qargs , cargs ) ) 
~~ return reverse_circ 
inverse_circ = self . copy ( name = self . name + '_dg' ) 
inverse_circ . data = [ ] 
~~~ inverse_circ . data . append ( ( inst . inverse ( ) , qargs , cargs ) ) 
~~ return inverse_circ 
~~ def combine ( self , rhs ) : 
self . _check_compatible_regs ( rhs ) 
combined_qregs = deepcopy ( self . qregs ) 
combined_cregs = deepcopy ( self . cregs ) 
for element in rhs . qregs : 
~~~ if element not in self . qregs : 
~~~ combined_qregs . append ( element ) 
~~ ~~ for element in rhs . cregs : 
~~~ if element not in self . cregs : 
~~~ combined_cregs . append ( element ) 
~~ ~~ circuit = QuantumCircuit ( * combined_qregs , * combined_cregs ) 
for instruction_context in itertools . chain ( self . data , rhs . data ) : 
~~~ circuit . append ( * instruction_context ) 
~~ return circuit 
~~ def extend ( self , rhs ) : 
~~~ self . qregs . append ( element ) 
~~~ self . cregs . append ( element ) 
~~ ~~ for instruction_context in rhs . data : 
~~~ self . append ( * instruction_context ) 
~~ def append ( self , instruction , qargs = None , cargs = None ) : 
qargs = qargs or [ ] 
cargs = cargs or [ ] 
if not isinstance ( instruction , Instruction ) and hasattr ( instruction , 'to_instruction' ) : 
~~~ instruction = instruction . to_instruction ( ) 
~~ if not isinstance ( instruction , Instruction ) : 
~~ self . _check_dups ( qargs ) 
self . _check_qargs ( qargs ) 
self . _check_cargs ( cargs ) 
if instruction . num_qubits != len ( qargs ) or instruction . num_clbits != len ( cargs ) : 
( instruction . name , 
instruction . num_qubits , instruction . num_clbits , 
len ( qargs ) , len ( cargs ) ) ) 
~~ instruction_context = instruction , qargs , cargs 
self . data . append ( instruction_context ) 
for param_index , param in enumerate ( instruction . params ) : 
~~~ if isinstance ( param , Parameter ) : 
~~~ current_symbols = self . parameters 
if param in current_symbols : 
~~~ self . _parameter_table [ param ] . append ( ( instruction , param_index ) ) 
~~~ self . _parameter_table [ param ] = [ ( instruction , param_index ) ] 
~~ ~~ ~~ return instruction 
~~ def _attach ( self , instruction , qargs , cargs ) : 
self . append ( instruction , qargs , cargs ) 
~~ def add_register ( self , * regs ) : 
if not regs : 
~~ if any ( [ isinstance ( reg , int ) for reg in regs ] ) : 
~~~ if len ( regs ) == 1 and isinstance ( regs [ 0 ] , int ) : 
~~~ regs = ( QuantumRegister ( regs [ 0 ] , 'q' ) , ) 
~~ elif len ( regs ) == 2 and all ( [ isinstance ( reg , int ) for reg in regs ] ) : 
~~~ regs = ( QuantumRegister ( regs [ 0 ] , 'q' ) , ClassicalRegister ( regs [ 1 ] , 'c' ) ) 
~~ ~~ for register in regs : 
~~~ if register in self . qregs or register in self . cregs : 
% register . name ) 
~~ if isinstance ( register , QuantumRegister ) : 
~~~ self . qregs . append ( register ) 
~~ elif isinstance ( register , ClassicalRegister ) : 
~~~ self . cregs . append ( register ) 
~~ ~~ ~~ def _check_dups ( self , qubits ) : 
squbits = set ( qubits ) 
if len ( squbits ) != len ( qubits ) : 
~~ ~~ def _check_qargs ( self , qargs ) : 
if not all ( isinstance ( i , tuple ) and 
isinstance ( i [ 0 ] , QuantumRegister ) and 
isinstance ( i [ 1 ] , int ) for i in qargs ) : 
~~ if not all ( self . has_register ( i [ 0 ] ) for i in qargs ) : 
~~ for qubit in qargs : 
~~~ qubit [ 0 ] . check_range ( qubit [ 1 ] ) 
~~ ~~ def _check_cargs ( self , cargs ) : 
isinstance ( i [ 0 ] , ClassicalRegister ) and 
isinstance ( i [ 1 ] , int ) for i in cargs ) : 
~~ if not all ( self . has_register ( i [ 0 ] ) for i in cargs ) : 
~~ for clbit in cargs : 
~~~ clbit [ 0 ] . check_range ( clbit [ 1 ] ) 
~~ ~~ def decompose ( self ) : 
from qiskit . transpiler . passes . decompose import Decompose 
from qiskit . converters . circuit_to_dag import circuit_to_dag 
from qiskit . converters . dag_to_circuit import dag_to_circuit 
pass_ = Decompose ( ) 
decomposed_dag = pass_ . run ( circuit_to_dag ( self ) ) 
return dag_to_circuit ( decomposed_dag ) 
~~ def _check_compatible_regs ( self , rhs ) : 
list1 = self . qregs + self . cregs 
list2 = rhs . qregs + rhs . cregs 
for element1 in list1 : 
~~~ for element2 in list2 : 
~~~ if element2 . name == element1 . name : 
~~~ if element1 != element2 : 
~~ ~~ ~~ ~~ ~~ def qasm ( self ) : 
string_temp = self . header + "\\n" 
string_temp += self . extension_lib + "\\n" 
for register in self . qregs : 
~~~ string_temp += register . qasm ( ) + "\\n" 
~~ for register in self . cregs : 
~~ for instruction , qargs , cargs in self . data : 
~~~ if instruction . name == 'measure' : 
~~~ qubit = qargs [ 0 ] 
clbit = cargs [ 0 ] 
qubit [ 0 ] . name , qubit [ 1 ] , 
clbit [ 0 ] . name , clbit [ 1 ] ) 
"," . join ( [ "%s[%d]" % ( j [ 0 ] . name , j [ 1 ] ) 
for j in qargs + cargs ] ) ) 
~~ ~~ return string_temp 
~~ def draw ( self , scale = 0.7 , filename = None , style = None , output = 'text' , 
interactive = False , line_length = None , plot_barriers = True , 
reverse_bits = False , justify = None ) : 
from qiskit . tools import visualization 
return visualization . circuit_drawer ( self , scale = scale , 
filename = filename , style = style , 
output = output , 
interactive = interactive , 
line_length = line_length , 
plot_barriers = plot_barriers , 
reverse_bits = reverse_bits , 
justify = justify ) 
~~ def size ( self ) : 
gate_ops = 0 
for instr , _ , _ in self . data : 
~~~ if instr . name not in [ 'barrier' , 'snapshot' ] : 
~~~ gate_ops += 1 
~~ ~~ return gate_ops 
~~ def depth ( self ) : 
reg_offset = 0 
reg_map = { } 
for reg in self . qregs + self . cregs : 
~~~ reg_map [ reg . name ] = reg_offset 
reg_offset += reg . size 
~~ op_stack = [ 0 ] * reg_offset 
for instr , qargs , cargs in self . data : 
~~~ levels = [ ] 
reg_ints = [ ] 
for ind , reg in enumerate ( qargs + cargs ) : 
~~~ reg_ints . append ( reg_map [ reg [ 0 ] . name ] + reg [ 1 ] ) 
levels . append ( op_stack [ reg_ints [ ind ] ] + 1 ) 
~~ if instr . control : 
~~~ cint = reg_map [ instr . control [ 0 ] . name ] 
for off in range ( instr . control [ 0 ] . size ) : 
~~~ if cint + off not in reg_ints : 
~~~ reg_ints . append ( cint + off ) 
levels . append ( op_stack [ cint + off ] + 1 ) 
~~ ~~ ~~ max_level = max ( levels ) 
for ind in reg_ints : 
~~~ op_stack [ ind ] = max_level 
~~ ~~ ~~ return max ( op_stack ) 
~~ def width ( self ) : 
return sum ( reg . size for reg in self . qregs + self . cregs ) 
~~ def count_ops ( self ) : 
count_ops = { } 
~~~ if instr . name in count_ops . keys ( ) : 
~~~ count_ops [ instr . name ] += 1 
~~~ count_ops [ instr . name ] = 1 
~~ ~~ return count_ops 
~~ def num_connected_components ( self , unitary_only = False ) : 
if unitary_only : 
~~~ regs = self . qregs 
~~~ regs = self . qregs + self . cregs 
~~ for reg in regs : 
~~ sub_graphs = [ [ bit ] for bit in range ( reg_offset ) ] 
num_sub_graphs = len ( sub_graphs ) 
~~~ if unitary_only : 
~~~ args = qargs 
num_qargs = len ( args ) 
~~~ args = qargs + cargs 
num_qargs = len ( args ) + ( 1 if instr . control else 0 ) 
~~ if num_qargs >= 2 and instr . name not in [ 'barrier' , 'snapshot' ] : 
~~~ graphs_touched = [ ] 
num_touched = 0 
if instr . control and not unitary_only : 
~~~ creg = instr . control [ 0 ] 
creg_int = reg_map [ creg . name ] 
for coff in range ( creg . size ) : 
~~~ temp_int = creg_int + coff 
for k in range ( num_sub_graphs ) : 
~~~ if temp_int in sub_graphs [ k ] : 
~~~ graphs_touched . append ( k ) 
num_touched += 1 
~~ ~~ ~~ ~~ for item in args : 
~~~ reg_int = reg_map [ item [ 0 ] . name ] + item [ 1 ] 
~~~ if reg_int in sub_graphs [ k ] : 
~~~ if k not in graphs_touched : 
~~ ~~ ~~ ~~ if num_touched > 1 : 
~~~ connections = [ ] 
for idx in graphs_touched : 
~~~ connections . extend ( sub_graphs [ idx ] ) 
~~ _sub_graphs = [ ] 
for idx in range ( num_sub_graphs ) : 
~~~ if idx not in graphs_touched : 
~~~ _sub_graphs . append ( sub_graphs [ idx ] ) 
~~ ~~ _sub_graphs . append ( connections ) 
sub_graphs = _sub_graphs 
num_sub_graphs -= ( num_touched - 1 ) 
~~ ~~ if num_sub_graphs == 1 : 
~~ ~~ return num_sub_graphs 
~~ def bind_parameters ( self , value_dict ) : 
new_circuit = self . copy ( ) 
if value_dict . keys ( ) > self . parameters : 
[ str ( p ) for p in value_dict . keys ( ) - self . parameters ] ) ) 
~~ for parameter , value in value_dict . items ( ) : 
~~~ new_circuit . _bind_parameter ( parameter , value ) 
~~ for parameter in value_dict : 
~~~ del new_circuit . _parameter_table [ parameter ] 
~~ return new_circuit 
~~ def _bind_parameter ( self , parameter , value ) : 
for ( instr , param_index ) in self . _parameter_table [ parameter ] : 
~~~ instr . params [ param_index ] = value 
~~ ~~ def pulse_drawer ( samples , duration , dt = None , interp_method = 'None' , 
filename = None , interactive = False , 
dpi = 150 , nop = 1000 , size = ( 6 , 5 ) ) : 
~~~ from matplotlib import pyplot as plt 
~~ if dt : 
~~~ _dt = dt 
~~~ _dt = 1 
~~ re_y = np . real ( samples ) 
im_y = np . imag ( samples ) 
image = plt . figure ( figsize = size ) 
ax0 = image . add_subplot ( 111 ) 
if interp_method == 'CubicSpline' : 
~~~ time = np . arange ( 0 , duration + 1 ) * _dt + 0.5 * _dt 
cs_ry = CubicSpline ( time [ : - 1 ] , re_y ) 
cs_iy = CubicSpline ( time [ : - 1 ] , im_y ) 
_time = np . linspace ( 0 , duration * _dt , nop ) 
_re_y = cs_ry ( _time ) 
_im_y = cs_iy ( _time ) 
~~ elif interp_method == 'None' : 
~~~ time = np . arange ( 0 , duration + 1 ) * _dt 
_time = np . r_ [ time [ 0 ] , np . repeat ( time [ 1 : - 1 ] , 2 ) , time [ - 1 ] ] 
_re_y = np . repeat ( re_y , 2 ) 
_im_y = np . repeat ( im_y , 2 ) 
~~~ raise QiskitError ( \ % interp_method ) 
~~ ax0 . fill_between ( x = _time , y1 = _re_y , y2 = np . zeros_like ( _time ) , 
facecolor = 'red' , alpha = 0.3 , 
edgecolor = 'red' , linewidth = 1.5 , 
ax0 . fill_between ( x = _time , y1 = _im_y , y2 = np . zeros_like ( _time ) , 
facecolor = 'blue' , alpha = 0.3 , 
edgecolor = 'blue' , linewidth = 1.5 , 
ax0 . set_xlim ( 0 , duration * _dt ) 
ax0 . grid ( b = True , linestyle = '-' ) 
ncol = 2 , frameon = False , fontsize = 14 ) 
if filename : 
~~~ image . savefig ( filename , dpi = dpi , bbox_inches = 'tight' ) 
~~ plt . close ( image ) 
if image and interactive : 
~~~ plt . show ( image ) 
~~ def _search_forward_n_swaps ( layout , gates , coupling_map , 
depth = SEARCH_DEPTH , width = SEARCH_WIDTH ) : 
gates_mapped , gates_remaining = _map_free_gates ( layout , gates , coupling_map ) 
base_step = { 'layout' : layout , 
'swaps_added' : 0 , 
'gates_mapped' : gates_mapped , 
'gates_remaining' : gates_remaining } 
if not gates_remaining or depth == 0 : 
~~~ return base_step 
~~ possible_swaps = coupling_map . get_edges ( ) 
def _score_swap ( swap ) : 
trial_layout = layout . copy ( ) 
trial_layout . swap ( * swap ) 
return _calc_layout_distance ( gates , coupling_map , trial_layout ) 
~~ ranked_swaps = sorted ( possible_swaps , key = _score_swap ) 
best_swap , best_step = None , None 
for swap in ranked_swaps [ : width ] : 
~~~ trial_layout = layout . copy ( ) 
next_step = _search_forward_n_swaps ( trial_layout , gates_remaining , 
coupling_map , depth - 1 , width ) 
if best_swap is None or _score_step ( next_step ) > _score_step ( best_step ) : 
~~~ best_swap , best_step = swap , next_step 
~~ ~~ best_swap_gate = _swap_ops_from_edge ( best_swap , layout ) 
'layout' : best_step [ 'layout' ] , 
'swaps_added' : 1 + best_step [ 'swaps_added' ] , 
'gates_remaining' : best_step [ 'gates_remaining' ] , 
'gates_mapped' : gates_mapped + best_swap_gate + best_step [ 'gates_mapped' ] , 
~~ def _map_free_gates ( layout , gates , coupling_map ) : 
blocked_qubits = set ( ) 
mapped_gates = [ ] 
remaining_gates = [ ] 
for gate in gates : 
~~~ if not gate [ 'partition' ] : 
~~~ qubits = [ n for n in gate [ 'graph' ] . nodes ( ) if n . type == 'op' ] [ 0 ] . qargs 
if not qubits : 
~~ if blocked_qubits . intersection ( qubits ) : 
~~~ blocked_qubits . update ( qubits ) 
remaining_gates . append ( gate ) 
~~~ mapped_gate = _transform_gate_for_layout ( gate , layout ) 
mapped_gates . append ( mapped_gate ) 
~~ qubits = gate [ 'partition' ] [ 0 ] 
if blocked_qubits . intersection ( qubits ) : 
~~ elif len ( qubits ) == 1 : 
~~ elif coupling_map . distance ( * [ layout [ q ] for q in qubits ] ) == 1 : 
~~ ~~ return mapped_gates , remaining_gates 
~~ def _calc_layout_distance ( gates , coupling_map , layout , max_gates = None ) : 
if max_gates is None : 
~~~ max_gates = 50 + 10 * len ( coupling_map . physical_qubits ) 
~~ return sum ( coupling_map . distance ( * [ layout [ q ] for q in gate [ 'partition' ] [ 0 ] ] ) 
for gate in gates [ : max_gates ] 
if gate [ 'partition' ] and len ( gate [ 'partition' ] [ 0 ] ) == 2 ) 
~~ def _score_step ( step ) : 
return len ( [ g for g in step [ 'gates_mapped' ] 
if len ( g . qargs ) == 2 ] ) - 3 * step [ 'swaps_added' ] 
~~ def _copy_circuit_metadata ( source_dag , coupling_map ) : 
target_dag = DAGCircuit ( ) 
target_dag . name = source_dag . name 
for creg in source_dag . cregs . values ( ) : 
~~~ target_dag . add_creg ( creg ) 
~~ device_qreg = QuantumRegister ( len ( coupling_map . physical_qubits ) , 'q' ) 
target_dag . add_qreg ( device_qreg ) 
return target_dag 
~~ def _transform_gate_for_layout ( gate , layout ) : 
mapped_op_node = deepcopy ( [ n for n in gate [ 'graph' ] . nodes ( ) if n . type == 'op' ] [ 0 ] ) 
device_qreg = QuantumRegister ( len ( layout . get_physical_bits ( ) ) , 'q' ) 
mapped_qargs = [ ( device_qreg , layout [ a ] ) for a in mapped_op_node . qargs ] 
mapped_op_node . qargs = mapped_op_node . op . qargs = mapped_qargs 
mapped_op_node . pop ( 'name' ) 
return mapped_op_node 
~~ def _swap_ops_from_edge ( edge , layout ) : 
qreg_edge = [ ( device_qreg , i ) for i in edge ] 
return [ 
DAGNode ( { 'op' : SwapGate ( ) , 'qargs' : qreg_edge , 'cargs' : [ ] , 'type' : 'op' } ) 
coupling_map = self . _coupling_map 
ordered_virtual_gates = list ( dag . serial_layers ( ) ) 
if self . initial_layout is None : 
~~~ if self . property_set [ "layout" ] : 
~~~ self . initial_layout = self . property_set [ "layout" ] 
~~~ self . initial_layout = Layout . generate_trivial_layout ( * dag . qregs . values ( ) ) 
~~ ~~ if len ( dag . qubits ( ) ) != len ( self . initial_layout ) : 
~~ if len ( self . _coupling_map . physical_qubits ) != len ( self . initial_layout ) : 
~~~ raise TranspilerError ( 
~~ mapped_gates = [ ] 
layout = self . initial_layout . copy ( ) 
gates_remaining = ordered_virtual_gates . copy ( ) 
while gates_remaining : 
~~~ best_step = _search_forward_n_swaps ( layout , gates_remaining , 
coupling_map ) 
layout = best_step [ 'layout' ] 
gates_mapped = best_step [ 'gates_mapped' ] 
gates_remaining = best_step [ 'gates_remaining' ] 
mapped_gates . extend ( gates_mapped ) 
~~ mapped_dag = _copy_circuit_metadata ( dag , coupling_map ) 
for node in mapped_gates : 
~~~ mapped_dag . apply_operation_back ( op = node . op , qargs = node . qargs , cargs = node . cargs ) 
~~ return mapped_dag 
~~ def add_physical_qubit ( self , physical_qubit ) : 
if not isinstance ( physical_qubit , int ) : 
~~ if physical_qubit in self . physical_qubits : 
~~~ raise CouplingError ( 
~~ self . graph . add_node ( physical_qubit ) 
self . _qubit_list = None 
~~ def add_edge ( self , src , dst ) : 
if src not in self . physical_qubits : 
~~~ self . add_physical_qubit ( src ) 
~~ if dst not in self . physical_qubits : 
~~~ self . add_physical_qubit ( dst ) 
~~ self . graph . add_edge ( src , dst ) 
self . _dist_matrix = None 
~~ def subgraph ( self , nodelist ) : 
subcoupling = CouplingMap ( ) 
subcoupling . graph = self . graph . subgraph ( nodelist ) 
for node in nodelist : 
~~~ if node not in subcoupling . physical_qubits : 
~~~ subcoupling . add_physical_qubit ( node ) 
~~ ~~ return subcoupling 
~~ def physical_qubits ( self ) : 
if self . _qubit_list is None : 
~~~ self . _qubit_list = sorted ( [ pqubit for pqubit in self . graph . nodes ] ) 
~~ return self . _qubit_list 
~~ def is_connected ( self ) : 
~~~ return nx . is_weakly_connected ( self . graph ) 
~~ except nx . exception . NetworkXException : 
~~ ~~ def _compute_distance_matrix ( self ) : 
if not self . is_connected ( ) : 
~~ lengths = nx . all_pairs_shortest_path_length ( self . graph . to_undirected ( as_view = True ) ) 
lengths = dict ( lengths ) 
size = len ( lengths ) 
cmap = np . zeros ( ( size , size ) ) 
for idx in range ( size ) : 
~~~ cmap [ idx , np . fromiter ( lengths [ idx ] . keys ( ) , dtype = int ) ] = np . fromiter ( 
lengths [ idx ] . values ( ) , dtype = int ) 
~~ self . _dist_matrix = cmap 
~~ def distance ( self , physical_qubit1 , physical_qubit2 ) : 
if physical_qubit1 not in self . physical_qubits : 
~~ if physical_qubit2 not in self . physical_qubits : 
~~ if self . _dist_matrix is None : 
~~~ self . _compute_distance_matrix ( ) 
~~ return self . _dist_matrix [ physical_qubit1 , physical_qubit2 ] 
~~ def transpile ( circuits , backend = None , basis_gates = None , coupling_map = None , 
initial_layout = None , seed_mapper = None , pass_manager = None ) : 
return compiler . transpile ( circuits = circuits , backend = backend , 
basis_gates = basis_gates , coupling_map = coupling_map , 
initial_layout = initial_layout , seed_transpiler = seed_mapper , 
~~ def transpile_dag ( dag , basis_gates = None , coupling_map = None , 
if basis_gates is None : 
~~~ basis_gates = [ 'u1' , 'u2' , 'u3' , 'cx' , 'id' ] 
~~ if pass_manager is None : 
~~~ if coupling_map : 
~~~ pass_manager = default_pass_manager ( basis_gates , 
CouplingMap ( coupling_map ) , 
initial_layout , 
seed_transpiler = seed_mapper ) 
~~~ pass_manager = default_pass_manager_simulator ( basis_gates ) 
~~ ~~ name = dag . name 
circuit = dag_to_circuit ( dag ) 
circuit = pass_manager . run ( circuit ) 
dag . name = name 
return dag 
~~ def cu1 ( self , theta , ctl , tgt ) : 
return self . append ( Cu1Gate ( theta ) , [ ctl , tgt ] , [ ] ) 
~~ def add ( self , gate , qargs , cargs ) : 
if not isinstance ( gate , Instruction ) : 
~~ self . instructions . append ( gate ) 
self . qargs . append ( qargs ) 
self . cargs . append ( cargs ) 
for index , instruction in enumerate ( self . instructions ) : 
~~~ self . instructions [ index ] = instruction . inverse ( ) 
~~ def q_if ( self , * qregs ) : 
for gate in self . instructions : 
~~~ gate . q_if ( * qregs ) 
~~~ gate . c_if ( classical , val ) 
~~ def subscribe ( self , event , callback ) : 
if not callable ( callback ) : 
~~ if event not in self . _subscribers : 
~~~ self . _subscribers [ event ] = [ ] 
~~ new_subscription = self . _Subscription ( event , callback ) 
if new_subscription in self . _subscribers [ event ] : 
~~ self . _subscribers [ event ] . append ( new_subscription ) 
~~ def dispatch ( self , event , * args , ** kwargs ) : 
if event not in self . _subscribers : 
~~ for subscriber in self . _subscribers [ event ] : 
~~~ subscriber . callback ( * args , ** kwargs ) 
~~ ~~ def unsubscribe ( self , event , callback ) : 
~~~ self . _subscribers [ event ] . remove ( self . _Subscription ( event , callback ) ) 
~~ def publish ( self , event , * args , ** kwargs ) : 
return self . _broker . dispatch ( event , * args , ** kwargs ) 
~~ def initialize ( self , params , qubits ) : 
if isinstance ( qubits , QuantumRegister ) : 
~~~ qubits = qubits [ : ] 
~~~ qubits = _convert_to_bits ( [ qubits ] , [ qbit for qreg in self . qregs for qbit in qreg ] ) [ 0 ] 
~~ return self . append ( Initialize ( params ) , qubits ) 
~~ def _define ( self ) : 
disentangling_circuit = self . gates_to_uncompute ( ) 
initialize_instr = disentangling_circuit . to_instruction ( ) . inverse ( ) 
q = QuantumRegister ( self . num_qubits , 'q' ) 
initialize_circuit = QuantumCircuit ( q , name = 'init_def' ) 
for qubit in q : 
~~~ initialize_circuit . append ( Reset ( ) , [ qubit ] ) 
~~ initialize_circuit . append ( initialize_instr , q [ : ] ) 
self . definition = initialize_circuit . data 
~~ def gates_to_uncompute ( self ) : 
q = QuantumRegister ( self . num_qubits ) 
circuit = QuantumCircuit ( q , name = 'disentangler' ) 
remaining_param = self . params 
for i in range ( self . num_qubits ) : 
~~~ ( remaining_param , 
thetas , 
phis ) = Initialize . _rotations_to_disentangle ( remaining_param ) 
rz_mult = self . _multiplex ( RZGate , phis ) 
ry_mult = self . _multiplex ( RYGate , thetas ) 
circuit . append ( rz_mult . to_instruction ( ) , q [ i : self . num_qubits ] ) 
circuit . append ( ry_mult . to_instruction ( ) , q [ i : self . num_qubits ] ) 
~~ def _rotations_to_disentangle ( local_param ) : 
remaining_vector = [ ] 
thetas = [ ] 
phis = [ ] 
param_len = len ( local_param ) 
for i in range ( param_len // 2 ) : 
~~~ ( remains , 
add_theta , 
add_phi ) = Initialize . _bloch_angles ( local_param [ 2 * i : 2 * ( i + 1 ) ] ) 
remaining_vector . append ( remains ) 
thetas . append ( - add_theta ) 
phis . append ( - add_phi ) 
~~ return remaining_vector , thetas , phis 
~~ def _bloch_angles ( pair_of_complex ) : 
[ a_complex , b_complex ] = pair_of_complex 
a_complex = complex ( a_complex ) 
b_complex = complex ( b_complex ) 
mag_a = np . absolute ( a_complex ) 
final_r = float ( np . sqrt ( mag_a ** 2 + np . absolute ( b_complex ) ** 2 ) ) 
if final_r < _EPS : 
~~~ theta = 0 
phi = 0 
final_r = 0 
final_t = 0 
~~~ theta = float ( 2 * np . arccos ( mag_a / final_r ) ) 
a_arg = np . angle ( a_complex ) 
b_arg = np . angle ( b_complex ) 
final_t = a_arg + b_arg 
phi = b_arg - a_arg 
~~ return final_r * np . exp ( 1.J * final_t / 2 ) , theta , phi 
~~ def _multiplex ( self , target_gate , list_of_angles ) : 
list_len = len ( list_of_angles ) 
local_num_qubits = int ( math . log2 ( list_len ) ) + 1 
q = QuantumRegister ( local_num_qubits ) 
circuit = QuantumCircuit ( q , name = "multiplex" + local_num_qubits . __str__ ( ) ) 
lsb = q [ 0 ] 
msb = q [ local_num_qubits - 1 ] 
if local_num_qubits == 1 : 
~~~ circuit . append ( target_gate ( list_of_angles [ 0 ] ) , [ q [ 0 ] ] ) 
~~ angle_weight = scipy . kron ( [ [ 0.5 , 0.5 ] , [ 0.5 , - 0.5 ] ] , 
np . identity ( 2 ** ( local_num_qubits - 2 ) ) ) 
list_of_angles = angle_weight . dot ( np . array ( list_of_angles ) ) . tolist ( ) 
multiplex_1 = self . _multiplex ( target_gate , list_of_angles [ 0 : ( list_len // 2 ) ] ) 
circuit . append ( multiplex_1 . to_instruction ( ) , q [ 0 : - 1 ] ) 
circuit . append ( CnotGate ( ) , [ msb , lsb ] ) 
multiplex_2 = self . _multiplex ( target_gate , list_of_angles [ ( list_len // 2 ) : ] ) 
if list_len > 1 : 
~~~ circuit . append ( multiplex_2 . to_instruction ( ) . mirror ( ) , q [ 0 : - 1 ] ) 
~~~ circuit . append ( multiplex_2 . to_instruction ( ) , q [ 0 : - 1 ] ) 
~~ circuit . append ( CnotGate ( ) , [ msb , lsb ] ) 
~~ def from_dict ( self , input_dict ) : 
if len ( input_dict ) >= 1 : 
~~~ key = list ( input_dict . keys ( ) ) [ 0 ] 
value = input_dict [ key ] 
len ( key ) == 2 and 
isinstance ( key [ 0 ] , str ) and 
isinstance ( key [ 1 ] , int ) and 
isinstance ( value , tuple ) and 
len ( value ) == 2 and 
isinstance ( key [ 1 ] , int ) ) : 
qreg_names = { qubit [ 0 ] for qubit in input_dict . keys ( ) } 
qregs = { } 
for qreg_name in qreg_names : 
~~~ qregs [ qreg_name ] = QuantumRegister ( 
max ( [ qubit [ 1 ] for qubit in input_dict . keys ( ) if qubit [ 0 ] == qreg_name ] ) + 1 , 
qreg_name ) 
~~ new_input_dict = { } 
for key , value in input_dict . items ( ) : 
~~~ new_input_dict [ value [ 1 ] ] = ( qregs [ key [ 0 ] ] , key [ 1 ] ) 
~~ input_dict = new_input_dict 
~~ ~~ for key , value in input_dict . items ( ) : 
~~~ virtual , physical = Layout . order_based_on_type ( key , value ) 
self . _p2v [ physical ] = virtual 
if virtual is None : 
~~ self . _v2p [ virtual ] = physical 
~~ ~~ def order_based_on_type ( value1 , value2 ) : 
if isinstance ( value1 , int ) and Layout . is_virtual ( value2 ) : 
~~~ physical = value1 
virtual = value2 
~~ elif isinstance ( value2 , int ) and Layout . is_virtual ( value1 ) : 
~~~ physical = value2 
virtual = value1 
~~ return virtual , physical 
~~ def is_virtual ( value ) : 
return value is None or isinstance ( value , tuple ) and len ( value ) == 2 and isinstance ( 
value [ 0 ] , Register ) and isinstance ( value [ 1 ] , int ) 
~~ def copy ( self ) : 
layout_copy = type ( self ) ( ) 
layout_copy . _p2v = self . _p2v . copy ( ) 
layout_copy . _v2p = self . _v2p . copy ( ) 
return layout_copy 
~~ def add ( self , virtual_bit , physical_bit = None ) : 
if physical_bit is None : 
~~~ physical_candidate = len ( self ) 
while physical_candidate in self . _p2v : 
~~~ physical_candidate += 1 
~~ physical_bit = physical_candidate 
~~ self [ virtual_bit ] = physical_bit 
~~ def swap ( self , left , right ) : 
if type ( left ) is not type ( right ) : 
~~ temp = self [ left ] 
self [ left ] = self [ right ] 
self [ right ] = temp 
~~ def combine_into_edge_map ( self , another_layout ) : 
edge_map = dict ( ) 
for virtual , physical in self . get_virtual_bits ( ) . items ( ) : 
~~~ if physical not in another_layout . _p2v : 
~~ edge_map [ virtual ] = another_layout [ physical ] 
~~ return edge_map 
~~ def generate_trivial_layout ( * regs ) : 
for reg in regs : 
~~~ layout . add_register ( reg ) 
~~ return layout 
~~ def from_intlist ( int_list , * qregs ) : 
if not all ( ( isinstance ( i , int ) for i in int_list ) ) : 
~~ if len ( int_list ) != len ( set ( int_list ) ) : 
~~ n_qubits = sum ( reg . size for reg in qregs ) 
if len ( int_list ) < n_qubits : 
raise LayoutError ( err_msg ) 
~~ out = Layout ( ) 
main_idx = 0 
for qreg in qregs : 
~~~ for idx in range ( qreg . size ) : 
~~~ out [ ( qreg , idx ) ] = int_list [ main_idx ] 
main_idx += 1 
~~ ~~ if main_idx != len ( int_list ) : 
~~~ for int_item in int_list [ main_idx : ] : 
~~~ out [ int_item ] = None 
~~ ~~ return out 
~~ def from_tuplelist ( tuple_list ) : 
out = Layout ( ) 
for physical , virtual in enumerate ( tuple_list ) : 
~~~ if virtual is None : 
~~ elif Layout . is_virtual ( virtual ) : 
~~~ if virtual in out . _v2p : 
~~ out [ virtual ] = physical 
~~ def ccx ( self , ctl1 , ctl2 , tgt ) : 
return self . append ( ToffoliGate ( ) , [ ctl1 , ctl2 , tgt ] , [ ] ) 
definition = [ ] 
q = QuantumRegister ( 3 , "q" ) 
rule = [ 
( HGate ( ) , [ q [ 2 ] ] , [ ] ) , 
( CnotGate ( ) , [ q [ 1 ] , q [ 2 ] ] , [ ] ) , 
( TdgGate ( ) , [ q [ 2 ] ] , [ ] ) , 
( CnotGate ( ) , [ q [ 0 ] , q [ 2 ] ] , [ ] ) , 
( TGate ( ) , [ q [ 2 ] ] , [ ] ) , 
( TGate ( ) , [ q [ 1 ] ] , [ ] ) , 
( CnotGate ( ) , [ q [ 0 ] , q [ 1 ] ] , [ ] ) , 
( TGate ( ) , [ q [ 0 ] ] , [ ] ) , 
( TdgGate ( ) , [ q [ 1 ] ] , [ ] ) , 
( CnotGate ( ) , [ q [ 0 ] , q [ 1 ] ] , [ ] ) 
for inst in rule : 
~~~ definition . append ( inst ) 
~~ self . definition = definition 
self . property_set [ 'commutation_set' ] = defaultdict ( list ) 
good_names = [ "cx" , "u1" , "u2" , "u3" , "id" ] 
block_list = [ ] 
nodes = list ( dag . topological_nodes ( ) ) 
nodes_seen = dict ( zip ( nodes , [ False ] * len ( nodes ) ) ) 
for nd in dag . topological_op_nodes ( ) : 
~~~ group = [ ] 
if nd . name == "cx" and nd . condition is None and not nodes_seen [ nd ] : 
~~~ these_qubits = sorted ( nd . qargs ) 
pred = list ( dag . predecessors ( nd ) ) 
explore = True 
while explore : 
~~~ pred_next = [ ] 
if len ( pred ) == 1 and not nodes_seen [ pred [ 0 ] ] : 
~~~ pnd = pred [ 0 ] 
if pnd . name in good_names : 
~~~ if ( pnd . name == "cx" and sorted ( pnd . qargs ) == these_qubits ) or pnd . name != "cx" : 
~~~ group . append ( pnd ) 
nodes_seen [ pnd ] = True 
pred_next . extend ( dag . predecessors ( pnd ) ) 
~~ ~~ ~~ elif len ( pred ) == 2 : 
~~~ if pred [ 0 ] in dag . predecessors ( pred [ 1 ] ) : 
~~ elif pred [ 1 ] in dag . predecessors ( pred [ 0 ] ) : 
~~~ if pred [ 0 ] . name == "cx" and sorted ( pred [ 0 ] . qargs ) == these_qubits : 
~~~ sorted_pred = [ pred [ 1 ] ] 
~~ elif pred [ 1 ] . name == "cx" and sorted ( pred [ 1 ] . qargs ) == these_qubits : 
~~~ sorted_pred = [ pred [ 0 ] ] 
~~~ sorted_pred = pred 
~~ ~~ if len ( sorted_pred ) == 2 and sorted_pred [ 0 ] . name == "cx" and sorted_pred [ 1 ] . name == "cx" : 
~~ for pnd in sorted_pred : 
~~~ if pnd . name not in good_names : 
~~ if pnd . name != "cx" : 
~~~ if not nodes_seen [ pnd ] : 
~~~ pred_qubits = sorted ( pnd . qargs ) 
if pred_qubits == these_qubits : 
~~~ these_qubits = list ( set ( these_qubits ) - 
set ( pred_qubits ) ) 
~~ ~~ ~~ ~~ pred = list ( set ( pred_next ) ) 
if not pred : 
~~~ explore = False 
~~ ~~ group . reverse ( ) 
group . append ( nd ) 
nodes_seen [ nd ] = True 
these_qubits = sorted ( nd . qargs ) 
succ = list ( dag . successors ( nd ) ) 
~~~ succ_next = [ ] 
if len ( succ ) == 1 and not nodes_seen [ succ [ 0 ] ] : 
~~~ snd = succ [ 0 ] 
if snd . name in good_names : 
~~~ if ( snd . name == "cx" and sorted ( snd . qargs ) == these_qubits ) or snd . name != "cx" : 
~~~ group . append ( snd ) 
nodes_seen [ snd ] = True 
succ_next . extend ( dag . successors ( snd ) ) 
~~ ~~ ~~ elif len ( succ ) == 2 : 
~~~ if succ [ 0 ] in dag . successors ( succ [ 1 ] ) : 
~~ elif succ [ 1 ] in dag . successors ( succ [ 0 ] ) : 
~~~ if succ [ 0 ] . name == "cx" and sorted ( succ [ 0 ] . qargs ) == these_qubits : 
~~~ sorted_succ = [ succ [ 1 ] ] 
~~ elif succ [ 1 ] . name == "cx" and sorted ( succ [ 1 ] . qargs ) == these_qubits : 
~~~ sorted_succ = [ succ [ 0 ] ] 
~~~ sorted_succ = succ 
~~ ~~ if len ( sorted_succ ) == 2 and sorted_succ [ 0 ] . name == "cx" and sorted_succ [ 1 ] . name == "cx" : 
~~ for snd in sorted_succ : 
~~~ if snd . name not in good_names : 
~~ if snd . name != "cx" : 
~~~ if not nodes_seen [ snd ] : 
~~~ succ_qubits = sorted ( snd . qargs ) 
if succ_qubits == these_qubits : 
set ( succ_qubits ) ) 
~~ ~~ ~~ ~~ succ = list ( set ( succ_next ) ) 
if not succ : 
~~ ~~ block_list . append ( tuple ( group ) ) 
~~ ~~ self . property_set [ 'block_list' ] = block_list 
~~ def u2 ( self , phi , lam , q ) : 
return self . append ( U2Gate ( phi , lam ) , [ q ] , [ ] ) 
isqrt2 = 1 / numpy . sqrt ( 2 ) 
phi , lam = self . params 
phi , lam = float ( phi ) , float ( lam ) 
return numpy . array ( [ [ isqrt2 , - numpy . exp ( 1j * lam ) * isqrt2 ] , 
numpy . exp ( 1j * phi ) * isqrt2 , 
numpy . exp ( 1j * ( phi + lam ) ) * isqrt2 
~~ def insert ( self , start_time : int , schedule : ScheduleComponent ) -> 'ScheduleComponent' : 
return ops . insert ( self , start_time , schedule ) 
~~ def _check_if_fenced ( self , name ) : 
if name in object . __getattribute__ ( self , '_attributes_to_fence' ) : 
( type ( object . __getattribute__ ( self , '_wrapped' ) ) , name ) ) 
~~ ~~ def is_cptp ( self , atol = None , rtol = None ) : 
~~ if self . _data [ 1 ] is not None : 
~~ check = np . dot ( np . transpose ( np . conj ( self . _data [ 0 ] ) ) , self . _data [ 0 ] ) 
return is_identity_matrix ( check , rtol = self . _rtol , atol = self . _atol ) 
stine_l = np . conjugate ( self . _data [ 0 ] ) 
stine_r = None 
~~~ stine_r = np . conjugate ( self . _data [ 1 ] ) 
~~ return Stinespring ( ( stine_l , stine_r ) , self . input_dims ( ) , 
self . output_dims ( ) ) 
din , dout = self . dim 
dtr = self . _data [ 0 ] . shape [ 0 ] // dout 
stine = [ None , None ] 
for i , mat in enumerate ( self . _data ) : 
~~~ if mat is not None : 
~~~ stine [ i ] = np . reshape ( 
np . transpose ( np . reshape ( mat , ( dout , dtr , din ) ) , ( 2 , 1 , 0 ) ) , 
( din * dtr , dout ) ) 
~~ ~~ return Stinespring ( 
tuple ( stine ) , 
~~~ return Stinespring ( 
~~ return Stinespring ( Kraus ( self ) . compose ( other , front = front ) ) 
~~ return Stinespring ( SuperOp ( self ) . power ( n ) ) 
~~ if isinstance ( other , complex ) or other < 1 : 
~~~ return Stinespring ( Choi ( self ) . multiply ( other ) ) 
~~ num = np . sqrt ( other ) 
stine_l , stine_r = self . _data 
stine_l = num * self . _data [ 0 ] 
~~~ stine_r = num * self . _data [ 1 ] 
~~ if state . ndim == 1 and self . _data [ 1 ] is None and self . _data [ 0 ] . shape [ 0 ] // self . _output_dim == 1 : 
~~~ return np . dot ( self . _data [ 0 ] , state ) 
if stine_r is None : 
~~ din , dout = self . dim 
dtr = stine_l . shape [ 0 ] // dout 
shape = ( dout , dtr , din ) 
return np . einsum ( 'iAB,BC,jAC->ij' , np . reshape ( stine_l , shape ) , state , 
np . reshape ( np . conjugate ( stine_r ) , shape ) ) 
if not isinstance ( other , Stinespring ) : 
~~~ other = Stinespring ( other ) 
~~ sa_l , sa_r = self . _data 
sb_l , sb_r = other . _data 
din_a , dout_a = self . dim 
din_b , dout_b = other . dim 
dtr_a = sa_l . shape [ 0 ] // dout_a 
dtr_b = sb_l . shape [ 0 ] // dout_b 
~~~ shape_in = ( dout_b , dtr_b , dout_a , dtr_a , din_b * din_a ) 
shape_out = ( dout_b * dtr_b * dout_a * dtr_a , din_b * din_a ) 
~~~ shape_in = ( dout_a , dtr_a , dout_b , dtr_b , din_a * din_b ) 
shape_out = ( dout_a * dtr_a * dout_b * dtr_b , din_a * din_b ) 
sab_l = np . kron ( sb_l , sa_l ) 
sab_l = np . kron ( sa_l , sb_l ) 
~~ sab_l = np . reshape ( 
np . transpose ( np . reshape ( sab_l , shape_in ) , ( 0 , 2 , 1 , 3 , 4 ) ) , 
shape_out ) 
if sa_r is None and sb_r is None : 
~~~ sab_r = None 
~~~ if sa_r is None : 
~~~ sa_r = sa_l 
~~ elif sb_r is None : 
~~~ sb_r = sb_l 
~~~ sab_r = np . kron ( sb_r , sa_r ) 
~~~ sab_r = np . kron ( sa_r , sb_r ) 
~~ sab_r = np . reshape ( 
np . transpose ( np . reshape ( sab_r , shape_in ) , ( 0 , 2 , 1 , 3 , 4 ) ) , 
~~ return Stinespring ( ( sab_l , sab_r ) , input_dims , output_dims ) 
~~ if len ( self . coupling_map . physical_qubits ) != len ( self . initial_layout ) : 
~~ current_layout = self . initial_layout . copy ( ) 
for layer in dag . serial_layers ( ) : 
~~~ subdag = layer [ 'graph' ] 
for gate in subdag . twoQ_gates ( ) : 
~~~ physical_q0 = current_layout [ gate . qargs [ 0 ] ] 
physical_q1 = current_layout [ gate . qargs [ 1 ] ] 
if self . coupling_map . distance ( physical_q0 , physical_q1 ) != 1 : 
~~~ swap_layer = DAGCircuit ( ) 
path = self . coupling_map . shortest_undirected_path ( physical_q0 , physical_q1 ) 
for swap in range ( len ( path ) - 2 ) : 
~~~ connected_wire_1 = path [ swap ] 
connected_wire_2 = path [ swap + 1 ] 
qubit_1 = current_layout [ connected_wire_1 ] 
qubit_2 = current_layout [ connected_wire_2 ] 
for qreg in current_layout . get_registers ( ) : 
~~~ if qreg not in swap_layer . qregs . values ( ) : 
~~~ swap_layer . add_qreg ( qreg ) 
~~ ~~ swap_layer . apply_operation_back ( SwapGate ( ) , 
qargs = [ qubit_1 , qubit_2 ] , 
cargs = [ ] ) 
~~ edge_map = current_layout . combine_into_edge_map ( self . initial_layout ) 
new_dag . compose_back ( swap_layer , edge_map ) 
~~~ current_layout . swap ( path [ swap ] , path [ swap + 1 ] ) 
~~ ~~ ~~ edge_map = current_layout . combine_into_edge_map ( self . initial_layout ) 
new_dag . extend_back ( subdag , edge_map ) 
~~ return new_dag 
~~ def _layer_permutation ( layer_partition , initial_layout , layout , qubit_subset , 
coupling , trials , qregs , rng ) : 
pformat ( layer_partition ) ) 
pformat ( layout . get_virtual_bits ( ) ) ) 
pformat ( qubit_subset ) ) 
for gate_args in layer_partition : 
~~~ if len ( gate_args ) > 2 : 
~~ elif len ( gate_args ) == 2 : 
~~~ gates . append ( tuple ( gate_args ) ) 
dist = sum ( [ coupling . distance ( layout [ g [ 0 ] ] , layout [ g [ 1 ] ] ) 
for g in gates ] ) 
if dist == len ( gates ) : 
circ = DAGCircuit ( ) 
for register in layout . get_virtual_bits ( ) . keys ( ) : 
~~~ if register [ 0 ] not in circ . qregs . values ( ) : 
~~~ circ . add_qreg ( register [ 0 ] ) 
~~ ~~ return True , circ , 0 , layout , ( not bool ( gates ) ) 
~~ num_qubits = len ( layout ) 
cdist2 = coupling . _dist_matrix ** 2 
scale = np . zeros ( ( num_qubits , num_qubits ) ) 
int_qubit_subset = regtuple_to_numeric ( qubit_subset , qregs ) 
int_gates = gates_to_idx ( gates , qregs ) 
int_layout = nlayout_from_layout ( layout , qregs , coupling . size ( ) ) 
~~~ if register [ 0 ] not in trial_circuit . qregs . values ( ) : 
~~~ trial_circuit . add_qreg ( register [ 0 ] ) 
~~~ if register [ 0 ] not in slice_circuit . qregs . values ( ) : 
~~~ slice_circuit . add_qreg ( register [ 0 ] ) 
~~ ~~ edges = np . asarray ( coupling . get_edges ( ) , dtype = np . int32 ) . ravel ( ) 
cdist = coupling . _dist_matrix 
for trial in range ( trials ) : 
dist , optim_edges , trial_layout , depth_step = swap_trial ( num_qubits , int_layout , 
int_qubit_subset , 
int_gates , cdist2 , 
cdist , edges , scale , 
rng ) 
if dist == len ( gates ) and depth_step < best_depth : 
depth_step ) 
best_edges = optim_edges 
best_layout = trial_layout 
best_depth = min ( best_depth , depth_step ) 
~~ if best_depth == 1 : 
~~ ~~ if best_layout is None : 
return False , None , None , None , False 
~~ edgs = best_edges . edges ( ) 
for idx in range ( best_edges . size // 2 ) : 
~~~ slice_circuit . apply_operation_back ( 
SwapGate ( ) , [ initial_layout [ edgs [ 2 * idx ] ] , initial_layout [ edgs [ 2 * idx + 1 ] ] ] , [ ] ) 
~~ trial_circuit . extend_back ( slice_circuit ) 
best_circuit = trial_circuit 
best_lay = best_layout . to_layout ( qregs ) 
return True , best_circuit , best_depth , best_lay , False 
~~ def regtuple_to_numeric ( items , qregs ) : 
sizes = [ qr . size for qr in qregs . values ( ) ] 
reg_idx = np . cumsum ( [ 0 ] + sizes ) 
regint = { } 
for ind , qreg in enumerate ( qregs . values ( ) ) : 
~~~ regint [ qreg ] = ind 
~~ out = np . zeros ( len ( items ) , dtype = np . int32 ) 
for idx , val in enumerate ( items ) : 
~~~ out [ idx ] = reg_idx [ regint [ val [ 0 ] ] ] + val [ 1 ] 
~~ def gates_to_idx ( gates , qregs ) : 
~~ out = np . zeros ( 2 * len ( gates ) , dtype = np . int32 ) 
for idx , gate in enumerate ( gates ) : 
~~~ out [ 2 * idx ] = reg_idx [ regint [ gate [ 0 ] [ 0 ] ] ] + gate [ 0 ] [ 1 ] 
out [ 2 * idx + 1 ] = reg_idx [ regint [ gate [ 1 ] [ 0 ] ] ] + gate [ 1 ] [ 1 ] 
~~ self . input_layout = self . initial_layout . copy ( ) 
self . qregs = dag . qregs 
if self . seed is None : 
~~~ self . seed = np . random . randint ( 0 , np . iinfo ( np . int32 ) . max ) 
~~ self . rng = np . random . RandomState ( self . seed ) 
new_dag = self . _mapper ( dag , self . coupling_map , trials = self . trials ) 
return new_dag 
~~ def _layer_permutation ( self , layer_partition , layout , qubit_subset , 
coupling , trials ) : 
return _layer_permutation ( layer_partition , self . initial_layout , 
layout , qubit_subset , 
coupling , trials , 
self . qregs , self . rng ) 
~~ def _layer_update ( self , i , first_layer , best_layout , best_depth , 
best_circuit , layer_list ) : 
layout = best_layout 
dagcircuit_output = DAGCircuit ( ) 
~~~ if register [ 0 ] not in dagcircuit_output . qregs . values ( ) : 
~~~ dagcircuit_output . add_qreg ( register [ 0 ] ) 
~~ ~~ if first_layer : 
for j in range ( i + 1 ) : 
~~~ edge_map = layout . combine_into_edge_map ( self . initial_layout ) 
for bit in dagcircuit_output . clbits ( ) : 
~~~ edge_map [ bit ] = bit 
~~ dagcircuit_output . compose_back ( layer_list [ j ] [ "graph" ] , edge_map ) 
~~~ if best_depth > 0 : 
dagcircuit_output . extend_back ( best_circuit ) 
~~ edge_map = layout . combine_into_edge_map ( self . initial_layout ) 
~~ dagcircuit_output . compose_back ( layer_list [ i ] [ "graph" ] , edge_map ) 
~~ return dagcircuit_output 
~~ def _mapper ( self , circuit_graph , coupling_graph , 
trials = 20 ) : 
layerlist = list ( circuit_graph . layers ( ) ) 
logger . debug ( "schedule:" ) 
for i , v in enumerate ( layerlist ) : 
~~ if self . initial_layout is not None : 
~~~ qubit_subset = self . initial_layout . get_virtual_bits ( ) . keys ( ) 
~~~ self . initial_layout = Layout ( ) 
physical_qubit = 0 
for qreg in circuit_graph . qregs . values ( ) : 
~~~ for index in range ( qreg . size ) : 
~~~ self . initial_layout [ ( qreg , index ) ] = physical_qubit 
physical_qubit += 1 
~~ ~~ qubit_subset = self . initial_layout . get_virtual_bits ( ) . keys ( ) 
coupling_graph = coupling_graph . subgraph ( 
self . initial_layout . get_physical_bits ( ) . keys ( ) ) 
if coupling_graph . size ( ) < len ( self . initial_layout ) : 
~~ layout = self . initial_layout . copy ( ) 
dagcircuit_output . name = circuit_graph . name 
~~~ dagcircuit_output . add_qreg ( qreg ) 
~~ for creg in circuit_graph . cregs . values ( ) : 
~~~ dagcircuit_output . add_creg ( creg ) 
~~ identity_wire_map = { } 
for qubit in circuit_graph . qubits ( ) : 
~~~ identity_wire_map [ qubit ] = qubit 
~~ for bit in circuit_graph . clbits ( ) : 
~~~ identity_wire_map [ bit ] = bit 
for i , layer in enumerate ( layerlist ) : 
~~~ success_flag , best_circuit , best_depth , best_layout , trivial_flag = self . _layer_permutation ( layer [ "partition" ] , layout , 
qubit_subset , coupling_graph , 
trials ) 
success_flag , str ( best_depth ) , trivial_flag ) 
if not success_flag : 
serial_layerlist = list ( layer [ "graph" ] . serial_layers ( ) ) 
for j , serial_layer in enumerate ( serial_layerlist ) : 
~~~ success_flag , best_circuit , best_depth , best_layout , trivial_flag = self . _layer_permutation ( 
serial_layer [ "partition" ] , 
coupling_graph , 
"trivial_flag=%s" , 
~~ if trivial_flag and first_layer : 
~~ if first_layer : 
~~~ self . initial_layout = layout 
~~ layout = best_layout 
dagcircuit_output . extend_back ( 
self . _layer_update ( j , 
first_layer , 
best_layout , 
best_depth , 
best_circuit , 
serial_layerlist ) , 
identity_wire_map ) 
if first_layer : 
~~~ first_layer = False 
~~~ layout = best_layout 
~~ dagcircuit_output . extend_back ( 
self . _layer_update ( i , 
layerlist ) , 
last_edgemap = layout . combine_into_edge_map ( self . initial_layout ) 
layout = self . initial_layout 
dagcircuit_output . compose_back ( layer [ "graph" ] , edge_map ) 
~~ ~~ return dagcircuit_output 
~~ def pauli_group ( number_of_qubits , case = 'weight' ) : 
if number_of_qubits < 5 : 
~~~ temp_set = [ ] 
if case == 'weight' : 
~~~ tmp = pauli_group ( number_of_qubits , case = 'tensor' ) 
return sorted ( tmp , key = lambda x : - np . count_nonzero ( 
np . array ( x . to_label ( ) , 'c' ) == b'I' ) ) 
~~ elif case == 'tensor' : 
~~~ for k in range ( 4 ** number_of_qubits ) : 
~~~ z = np . zeros ( number_of_qubits , dtype = np . bool ) 
x = np . zeros ( number_of_qubits , dtype = np . bool ) 
for j in range ( number_of_qubits ) : 
~~~ element = ( k // ( 4 ** j ) ) % 4 
if element == 1 : 
~~~ x [ j ] = True 
~~ elif element == 2 : 
~~~ z [ j ] = True 
x [ j ] = True 
~~ elif element == 3 : 
~~ ~~ temp_set . append ( Pauli ( z , x ) ) 
~~ return temp_set 
~~ def from_label ( cls , label ) : 
z = np . zeros ( len ( label ) , dtype = np . bool ) 
x = np . zeros ( len ( label ) , dtype = np . bool ) 
for i , char in enumerate ( label ) : 
~~~ if char == 'X' : 
~~~ x [ - i - 1 ] = True 
~~ elif char == 'Z' : 
~~~ z [ - i - 1 ] = True 
~~ elif char == 'Y' : 
x [ - i - 1 ] = True 
~~ elif char != 'I' : 
"\ . format ( char ) ) 
~~ ~~ return cls ( z = z , x = x ) 
~~ def _init_from_bool ( self , z , x ) : 
if z is None : 
~~ if x is None : 
~~ if len ( z ) != len ( x ) : 
~~ z = _make_np_bool ( z ) 
x = _make_np_bool ( x ) 
self . _z = z 
self . _x = x 
~~ def sgn_prod ( p1 , p2 ) : 
phase = Pauli . _prod_phase ( p1 , p2 ) 
new_pauli = p1 * p2 
return new_pauli , phase 
~~ def to_spmatrix ( self ) : 
mat = sparse . coo_matrix ( 1 ) 
for z , x in zip ( self . _z , self . _x ) : 
~~~ mat = sparse . bmat ( [ [ mat , None ] , [ None , mat ] ] , format = 'coo' ) 
~~~ mat = sparse . bmat ( [ [ mat , None ] , [ None , - mat ] ] , format = 'coo' ) 
~~~ mat = sparse . bmat ( [ [ None , mat ] , [ mat , None ] ] , format = 'coo' ) 
~~~ mat = mat * 1j 
mat = sparse . bmat ( [ [ None , - mat ] , [ mat , None ] ] , format = 'coo' ) 
~~ ~~ return mat . tocsr ( ) 
~~ def to_operator ( self ) : 
from qiskit . quantum_info . operators . operator import Operator 
return Operator ( self . to_matrix ( ) ) 
~~ def to_instruction ( self ) : 
from qiskit . circuit import QuantumCircuit , QuantumRegister 
from qiskit . extensions . standard import IdGate , XGate , YGate , ZGate 
gates = { 'I' : IdGate ( ) , 'X' : XGate ( ) , 'Y' : YGate ( ) , 'Z' : ZGate ( ) } 
label = self . to_label ( ) 
n_qubits = self . numberofqubits 
qreg = QuantumRegister ( n_qubits ) 
circuit = QuantumCircuit ( qreg , name = 'Pauli:{}' . format ( label ) ) 
for i , pauli in enumerate ( reversed ( label ) ) : 
~~~ circuit . append ( gates [ pauli ] , [ qreg [ i ] ] ) 
~~ return circuit . to_instruction ( ) 
~~ def update_z ( self , z , indices = None ) : 
z = _make_np_bool ( z ) 
if indices is None : 
~~~ if len ( self . _z ) != len ( z ) : 
~~ self . _z = z 
~~~ if not isinstance ( indices , list ) and not isinstance ( indices , np . ndarray ) : 
~~~ indices = [ indices ] 
~~ for p , idx in enumerate ( indices ) : 
~~~ self . _z [ idx ] = z [ p ] 
~~ ~~ return self 
~~ def update_x ( self , x , indices = None ) : 
~~~ if len ( self . _x ) != len ( x ) : 
~~ self . _x = x 
~~~ self . _x [ idx ] = x [ p ] 
~~ def insert_paulis ( self , indices = None , paulis = None , pauli_labels = None ) : 
if pauli_labels is not None : 
~~~ if paulis is not None : 
~~ if isinstance ( pauli_labels , str ) : 
~~~ pauli_labels = list ( pauli_labels ) 
~~ paulis = Pauli . from_label ( pauli_labels [ : : - 1 ] ) 
~~~ self . _z = np . concatenate ( ( self . _z , paulis . z ) ) 
self . _x = np . concatenate ( ( self . _x , paulis . x ) ) 
~~~ if not isinstance ( indices , list ) : 
~~ self . _z = np . insert ( self . _z , indices , paulis . z ) 
self . _x = np . insert ( self . _x , indices , paulis . x ) 
~~ def append_paulis ( self , paulis = None , pauli_labels = None ) : 
return self . insert_paulis ( None , paulis = paulis , pauli_labels = pauli_labels ) 
~~ def delete_qubits ( self , indices ) : 
if not isinstance ( indices , list ) : 
~~ self . _z = np . delete ( self . _z , indices ) 
self . _x = np . delete ( self . _x , indices ) 
~~ def random ( cls , num_qubits , seed = None ) : 
if seed is not None : 
~~~ np . random . seed ( seed ) 
~~ z = np . random . randint ( 2 , size = num_qubits ) . astype ( np . bool ) 
x = np . random . randint ( 2 , size = num_qubits ) . astype ( np . bool ) 
return cls ( z , x ) 
~~ def pauli_single ( cls , num_qubits , index , pauli_label ) : 
tmp = Pauli . from_label ( pauli_label ) 
z = np . zeros ( num_qubits , dtype = np . bool ) 
x = np . zeros ( num_qubits , dtype = np . bool ) 
z [ index ] = tmp . z [ 0 ] 
x [ index ] = tmp . x [ 0 ] 
~~ def _add_unitary_single ( self , gate , qubit ) : 
indexes = einsum_vecmul_index ( [ qubit ] , self . _number_of_qubits ) 
gate_tensor = np . array ( gate , dtype = complex ) 
self . _statevector = np . einsum ( indexes , gate_tensor , 
self . _statevector , 
dtype = complex , 
casting = 'no' ) 
~~ def _add_unitary_two ( self , gate , qubit0 , qubit1 ) : 
indexes = einsum_vecmul_index ( [ qubit0 , qubit1 ] , self . _number_of_qubits ) 
gate_tensor = np . reshape ( np . array ( gate , dtype = complex ) , 4 * [ 2 ] ) 
~~ def _get_measure_outcome ( self , qubit ) : 
axis = list ( range ( self . _number_of_qubits ) ) 
axis . remove ( self . _number_of_qubits - 1 - qubit ) 
probabilities = np . sum ( np . abs ( self . _statevector ) ** 2 , axis = tuple ( axis ) ) 
random_number = self . _local_random . rand ( ) 
if random_number < probabilities [ 0 ] : 
~~~ return '0' , probabilities [ 0 ] 
~~ return '1' , probabilities [ 1 ] 
~~ def _add_sample_measure ( self , measure_params , num_samples ) : 
measured_qubits = list ( { qubit for qubit , cmembit in measure_params } ) 
num_measured = len ( measured_qubits ) 
for qubit in reversed ( measured_qubits ) : 
~~~ axis . remove ( self . _number_of_qubits - 1 - qubit ) 
~~ probabilities = np . reshape ( np . sum ( np . abs ( self . _statevector ) ** 2 , 
axis = tuple ( axis ) ) , 
2 ** num_measured ) 
samples = self . _local_random . choice ( range ( 2 ** num_measured ) , 
num_samples , p = probabilities ) 
memory = [ ] 
for sample in samples : 
~~~ classical_memory = self . _classical_memory 
for count , ( qubit , cmembit ) in enumerate ( sorted ( measure_params ) ) : 
~~~ qubit_outcome = int ( ( sample & ( 1 << count ) ) >> count ) 
membit = 1 << cmembit 
classical_memory = ( classical_memory & ( ~ membit ) ) | ( qubit_outcome << cmembit ) 
~~ value = bin ( classical_memory ) [ 2 : ] 
memory . append ( hex ( int ( value , 2 ) ) ) 
~~ return memory 
~~ def _add_qasm_measure ( self , qubit , cmembit , cregbit = None ) : 
outcome , probability = self . _get_measure_outcome ( qubit ) 
self . _classical_memory = ( self . _classical_memory & ( ~ membit ) ) | ( int ( outcome ) << cmembit ) 
if cregbit is not None : 
~~~ regbit = 1 << cregbit 
self . _classical_register = ( self . _classical_register & ( ~ regbit ) ) | ( int ( outcome ) << cregbit ) 
~~ if outcome == '0' : 
~~~ update_diag = [ [ 1 / np . sqrt ( probability ) , 0 ] , [ 0 , 0 ] ] 
~~~ update_diag = [ [ 0 , 0 ] , [ 0 , 1 / np . sqrt ( probability ) ] ] 
~~ self . _add_unitary_single ( update_diag , qubit ) 
~~ def _add_qasm_reset ( self , qubit ) : 
if outcome == '0' : 
~~~ update = [ [ 1 / np . sqrt ( probability ) , 0 ] , [ 0 , 0 ] ] 
self . _add_unitary_single ( update , qubit ) 
~~~ update = [ [ 0 , 1 / np . sqrt ( probability ) ] , [ 0 , 0 ] ] 
~~ ~~ def _validate_initial_statevector ( self ) : 
if self . _initial_statevector is None : 
~~ length = len ( self . _initial_statevector ) 
required_dim = 2 ** self . _number_of_qubits 
if length != required_dim : 
~~ ~~ def _set_options ( self , qobj_config = None , backend_options = None ) : 
self . _initial_statevector = self . DEFAULT_OPTIONS [ "initial_statevector" ] 
self . _chop_threshold = self . DEFAULT_OPTIONS [ "chop_threshold" ] 
if backend_options is None : 
~~~ backend_options = { } 
~~ if 'initial_statevector' in backend_options : 
~~~ self . _initial_statevector = np . array ( backend_options [ 'initial_statevector' ] , 
~~ elif hasattr ( qobj_config , 'initial_statevector' ) : 
~~~ self . _initial_statevector = np . array ( qobj_config . initial_statevector , 
~~ if self . _initial_statevector is not None : 
~~~ norm = np . linalg . norm ( self . _initial_statevector ) 
if round ( norm , 12 ) != 1 : 
~~ ~~ if 'chop_threshold' in backend_options : 
~~~ self . _chop_threshold = backend_options [ 'chop_threshold' ] 
~~ elif hasattr ( qobj_config , 'chop_threshold' ) : 
~~~ self . _chop_threshold = qobj_config . chop_threshold 
~~ ~~ def _initialize_statevector ( self ) : 
~~~ self . _statevector = np . zeros ( 2 ** self . _number_of_qubits , 
self . _statevector [ 0 ] = 1 
~~~ self . _statevector = self . _initial_statevector . copy ( ) 
~~ self . _statevector = np . reshape ( self . _statevector , 
self . _number_of_qubits * [ 2 ] ) 
~~ def _get_statevector ( self ) : 
vec = np . reshape ( self . _statevector , 2 ** self . _number_of_qubits ) 
vec = np . stack ( [ vec . real , vec . imag ] , axis = 1 ) 
vec [ abs ( vec ) < self . _chop_threshold ] = 0.0 
return vec 
~~ def _validate_measure_sampling ( self , experiment ) : 
if self . _shots <= 1 : 
~~~ self . _sample_measure = False 
~~ if hasattr ( experiment . config , 'allows_measure_sampling' ) : 
~~~ self . _sample_measure = experiment . config . allows_measure_sampling 
~~~ measure_flag = False 
for instruction in experiment . instructions : 
~~~ if instruction . name == "reset" : 
~~ if measure_flag : 
~~~ if instruction . name not in [ "measure" , "barrier" , "id" , "u0" ] : 
~~ ~~ elif instruction . name == "measure" : 
~~~ measure_flag = True 
~~ ~~ self . _sample_measure = True 
~~ ~~ def run ( self , qobj , backend_options = None ) : 
self . _set_options ( qobj_config = qobj . config , 
backend_options = backend_options ) 
job_id = str ( uuid . uuid4 ( ) ) 
job = BasicAerJob ( self , job_id , self . _run_job , qobj ) 
job . submit ( ) 
return job 
~~ def _run_job ( self , job_id , qobj ) : 
self . _validate ( qobj ) 
result_list = [ ] 
self . _shots = qobj . config . shots 
self . _memory = getattr ( qobj . config , 'memory' , False ) 
self . _qobj_config = qobj . config 
start = time . time ( ) 
for experiment in qobj . experiments : 
~~~ result_list . append ( self . run_experiment ( experiment ) ) 
~~ end = time . time ( ) 
result = { 'backend_name' : self . name ( ) , 
'backend_version' : self . _configuration . backend_version , 
'qobj_id' : qobj . qobj_id , 
'job_id' : job_id , 
'results' : result_list , 
'status' : 'COMPLETED' , 
'success' : True , 
'time_taken' : ( end - start ) , 
'header' : qobj . header . as_dict ( ) } 
return Result . from_dict ( result ) 
~~ def run_experiment ( self , experiment ) : 
self . _number_of_qubits = experiment . config . n_qubits 
self . _number_of_cmembits = experiment . config . memory_slots 
self . _statevector = 0 
self . _classical_memory = 0 
self . _classical_register = 0 
self . _sample_measure = False 
self . _validate_initial_statevector ( ) 
if hasattr ( experiment . config , 'seed' ) : 
~~~ seed = experiment . config . seed 
~~ elif hasattr ( self . _qobj_config , 'seed' ) : 
~~~ seed = self . _qobj_config . seed 
~~~ seed = np . random . randint ( 2147483647 , dtype = 'int32' ) 
~~ self . _local_random . seed ( seed = seed ) 
self . _validate_measure_sampling ( experiment ) 
if self . _sample_measure : 
~~~ shots = 1 
measure_sample_ops = [ ] 
~~~ shots = self . _shots 
~~ for _ in range ( shots ) : 
~~~ self . _initialize_statevector ( ) 
for operation in experiment . instructions : 
~~~ conditional = getattr ( operation , 'conditional' , None ) 
if isinstance ( conditional , int ) : 
~~~ conditional_bit_set = ( self . _classical_register >> conditional ) & 1 
if not conditional_bit_set : 
~~ ~~ elif conditional is not None : 
~~~ mask = int ( operation . conditional . mask , 16 ) 
if mask > 0 : 
~~~ value = self . _classical_memory & mask 
while ( mask & 0x1 ) == 0 : 
~~~ mask >>= 1 
value >>= 1 
~~ if value != int ( operation . conditional . val , 16 ) : 
~~ ~~ ~~ if operation . name in ( 'U' , 'u1' , 'u2' , 'u3' ) : 
~~~ params = getattr ( operation , 'params' , None ) 
qubit = operation . qubits [ 0 ] 
gate = single_gate_matrix ( operation . name , params ) 
self . _add_unitary_single ( gate , qubit ) 
~~ elif operation . name in ( 'id' , 'u0' ) : 
~~ elif operation . name in ( 'CX' , 'cx' ) : 
~~~ qubit0 = operation . qubits [ 0 ] 
qubit1 = operation . qubits [ 1 ] 
gate = cx_gate_matrix ( ) 
self . _add_unitary_two ( gate , qubit0 , qubit1 ) 
~~ elif operation . name == 'reset' : 
~~~ qubit = operation . qubits [ 0 ] 
self . _add_qasm_reset ( qubit ) 
~~ elif operation . name == 'barrier' : 
~~ elif operation . name == 'measure' : 
cmembit = operation . memory [ 0 ] 
cregbit = operation . register [ 0 ] if hasattr ( operation , 'register' ) else None 
~~~ measure_sample_ops . append ( ( qubit , cmembit ) ) 
~~~ self . _add_qasm_measure ( qubit , cmembit , cregbit ) 
~~ ~~ elif operation . name == 'bfunc' : 
~~~ mask = int ( operation . mask , 16 ) 
relation = operation . relation 
val = int ( operation . val , 16 ) 
cregbit = operation . register 
cmembit = operation . memory if hasattr ( operation , 'memory' ) else None 
compared = ( self . _classical_register & mask ) - val 
if relation == '==' : 
~~~ outcome = ( compared == 0 ) 
~~ elif relation == '!=' : 
~~~ outcome = ( compared != 0 ) 
~~ elif relation == '<' : 
~~~ outcome = ( compared < 0 ) 
~~ elif relation == '<=' : 
~~~ outcome = ( compared <= 0 ) 
~~ elif relation == '>' : 
~~~ outcome = ( compared > 0 ) 
~~ elif relation == '>=' : 
~~~ outcome = ( compared >= 0 ) 
~~ regbit = 1 << cregbit 
if cmembit is not None : 
~~~ membit = 1 << cmembit 
~~~ backend = self . name ( ) 
err_msg = \ 
raise BasicAerError ( err_msg . format ( backend , operation . name ) ) 
~~ ~~ if self . _number_of_cmembits > 0 : 
~~~ if self . _sample_measure : 
~~~ memory = self . _add_sample_measure ( measure_sample_ops , self . _shots ) 
~~~ outcome = bin ( self . _classical_memory ) [ 2 : ] 
memory . append ( hex ( int ( outcome , 2 ) ) ) 
~~ ~~ ~~ data = { 'counts' : dict ( Counter ( memory ) ) } 
if self . _memory : 
~~~ data [ 'memory' ] = memory 
~~ if self . SHOW_FINAL_STATE : 
~~~ data [ 'statevector' ] = self . _get_statevector ( ) 
if not data [ 'counts' ] : 
~~~ data . pop ( 'counts' ) 
~~ if 'memory' in data and not data [ 'memory' ] : 
~~~ data . pop ( 'memory' ) 
~~ ~~ end = time . time ( ) 
return { 'name' : experiment . header . name , 
'seed' : seed , 
'shots' : self . _shots , 
'data' : data , 
'status' : 'DONE' , 
'header' : experiment . header . as_dict ( ) } 
~~ def _validate ( self , qobj ) : 
n_qubits = qobj . config . n_qubits 
max_qubits = self . configuration ( ) . n_qubits 
if n_qubits > max_qubits : 
\ . format ( self . name ( ) ) ) 
~~ for experiment in qobj . experiments : 
~~~ name = experiment . header . name 
if experiment . config . memory_slots == 0 : 
~~~ logger . warning ( \ 
~~ elif 'measure' not in [ op . name for op in experiment . instructions ] : 
~~ ~~ ~~ def _add_unitary_single ( self , gate , qubit ) : 
indexes = einsum_matmul_index ( [ qubit ] , self . _number_of_qubits ) 
self . _unitary = np . einsum ( indexes , gate_tensor , self . _unitary , 
dtype = complex , casting = 'no' ) 
indexes = einsum_matmul_index ( [ qubit0 , qubit1 ] , self . _number_of_qubits ) 
~~ def _validate_initial_unitary ( self ) : 
if self . _initial_unitary is None : 
~~ shape = np . shape ( self . _initial_unitary ) 
required_shape = ( 2 ** self . _number_of_qubits , 
2 ** self . _number_of_qubits ) 
if shape != required_shape : 
self . _initial_unitary = self . DEFAULT_OPTIONS [ "initial_unitary" ] 
~~ if 'initial_unitary' in backend_options : 
~~~ self . _initial_unitary = np . array ( backend_options [ 'initial_unitary' ] , 
~~ elif hasattr ( qobj_config , 'initial_unitary' ) : 
~~~ self . _initial_unitary = np . array ( qobj_config . initial_unitary , 
~~ if self . _initial_unitary is not None : 
~~~ shape = np . shape ( self . _initial_unitary ) 
~~ iden = np . eye ( len ( self . _initial_unitary ) ) 
u_dagger_u = np . dot ( self . _initial_unitary . T . conj ( ) , 
self . _initial_unitary ) 
norm = np . linalg . norm ( u_dagger_u - iden ) 
if round ( norm , 10 ) != 0 : 
~~ ~~ def _initialize_unitary ( self ) : 
self . _validate_initial_unitary ( ) 
~~~ self . _unitary = np . eye ( 2 ** self . _number_of_qubits , 
~~~ self . _unitary = self . _initial_unitary . copy ( ) 
~~ self . _unitary = np . reshape ( self . _unitary , 
self . _number_of_qubits * [ 2 , 2 ] ) 
~~ def _get_unitary ( self ) : 
unitary = np . reshape ( self . _unitary , 2 * [ 2 ** self . _number_of_qubits ] ) 
unitary = np . stack ( ( unitary . real , unitary . imag ) , axis = - 1 ) 
unitary [ abs ( unitary ) < self . _chop_threshold ] = 0.0 
return unitary 
self . _number_of_qubits = experiment . header . n_qubits 
self . _initialize_unitary ( ) 
~~~ if operation . name in ( 'U' , 'u1' , 'u2' , 'u3' ) : 
~~ ~~ data = { 'unitary' : self . _get_unitary ( ) } 
end = time . time ( ) 
'shots' : 1 , 
~~ if hasattr ( qobj . config , 'shots' ) and qobj . config . shots != 1 : 
~~~ logger . info ( \ , 
self . name ( ) ) 
qobj . config . shots = 1 
if getattr ( experiment . config , 'shots' , 1 ) != 1 : 
~~~ logger . info ( \ 
\ , 
self . name ( ) , name ) 
experiment . config . shots = 1 
~~ for operation in experiment . instructions : 
~~~ if operation . name in [ 'measure' , 'reset' ] : 
~~~ raise BasicAerError ( \ + 
\ , self . name ( ) , 
operation . name , name ) 
~~ ~~ ~~ ~~ def _is_bit ( obj ) : 
if isinstance ( obj , tuple ) and len ( obj ) == 2 : 
~~~ if isinstance ( obj [ 0 ] , Register ) and isinstance ( obj [ 1 ] , int ) and obj [ 1 ] < len ( obj [ 0 ] ) : 
~~ ~~ return False 
~~ def _convert_to_bits ( a_list , bits ) : 
new_list = [ ] 
for item in a_list : 
~~~ if isinstance ( item , ( int , slice ) ) : 
~~~ new_list . append ( bits [ item ] ) 
~~ except IndexError : 
~~ ~~ elif isinstance ( item , list ) : 
~~~ new_list . append ( _convert_to_bits ( item , bits ) ) 
~~ elif isinstance ( item , range ) : 
~~~ new_list . append ( _convert_to_bits ( [ index for index in item ] , bits ) ) 
~~~ new_list . append ( item ) 
~~ ~~ return new_list 
~~ def _to_bits ( nqbits , ncbits = 0 , func = None ) : 
if func is None : 
~~~ return functools . partial ( _to_bits , nqbits , ncbits ) 
~~ @ functools . wraps ( func ) 
def wrapper ( self , * args ) : 
~~~ qbits = self . qubits ( ) 
cbits = self . clbits ( ) 
nparams = len ( args ) - nqbits - ncbits 
params = args [ : nparams ] 
qb_args = args [ nparams : nparams + nqbits ] 
cl_args = args [ nparams + nqbits : ] 
args = list ( params ) + _convert_to_bits ( qb_args , qbits ) + _convert_to_bits ( cl_args , cbits ) 
return func ( self , * args ) 
~~ def _op_expand ( n_bits , func = None , broadcastable = None ) : 
~~~ return functools . partial ( _op_expand , n_bits , broadcastable = broadcastable ) 
~~~ params = args [ 0 : - n_bits ] if len ( args ) > n_bits else tuple ( ) 
rargs = args [ - n_bits : ] 
if broadcastable is None : 
~~~ blist = [ True ] * len ( rargs ) 
~~~ blist = broadcastable 
~~ if not all ( [ _is_bit ( arg ) for arg in rargs ] ) : 
~~~ rarg_size = [ 1 ] * n_bits 
for iarg , arg in enumerate ( rargs ) : 
~~~ if isinstance ( arg , Register ) : 
~~~ rarg_size [ iarg ] = len ( arg ) 
~~ elif isinstance ( arg , list ) and all ( [ _is_bit ( bit ) for bit in arg ] ) : 
~~ elif _is_bit ( arg ) : 
~~~ rarg_size [ iarg ] = 1 
~~ ~~ broadcast_size = max ( rarg_size ) 
expanded_rargs = [ ] 
for arg , broadcast in zip ( rargs , blist ) : 
~~~ arg = [ ( arg , i ) for i in range ( len ( arg ) ) ] 
~~ elif isinstance ( arg , tuple ) : 
~~~ arg = [ arg ] 
~~ if isinstance ( arg , list ) and len ( arg ) == 1 and broadcast : 
~~~ arg = arg * broadcast_size 
~~ if len ( arg ) != broadcast_size : 
~~ expanded_rargs . append ( arg ) 
~~ rargs = expanded_rargs 
if all ( [ isinstance ( arg , list ) for arg in rargs ] ) : 
~~~ if all ( rargs ) : 
~~~ instructions = InstructionSet ( ) 
for irargs in zip ( * rargs ) : 
~~~ instructions . add ( func ( self , * params , * irargs ) , 
[ i for i in irargs if isinstance ( i [ 0 ] , QuantumRegister ) ] , 
[ i for i in irargs if isinstance ( i [ 0 ] , ClassicalRegister ) ] ) 
~~ return instructions 
~~ ~~ ~~ return func ( self , * params , * rargs ) 
lam = self . params [ 0 ] 
lam = float ( lam ) 
return numpy . array ( [ [ 1 , 0 ] , [ 0 , numpy . exp ( 1j * lam ) ] ] , dtype = complex ) 
~~ self . property_set [ 'layout' ] = Layout . generate_trivial_layout ( * dag . qregs . values ( ) ) 
~~ def has_overlap ( self , interval : 'Interval' ) -> bool : 
if self . begin < interval . end and interval . begin < self . end : 
~~ def shift ( self , time : int ) -> 'Interval' : 
return Interval ( self . _begin + time , self . _end + time ) 
~~ def shift ( self , time : int ) -> 'Timeslot' : 
return Timeslot ( self . interval . shift ( time ) , self . channel ) 
intervals = list ( itertools . chain ( * ( self . _table [ chan ] for chan in channels 
if chan in self . _table ) ) ) 
if intervals : 
~~~ return min ( ( interval . begin for interval in intervals ) ) 
~~~ return max ( ( interval . end for interval in intervals ) ) 
~~ def is_mergeable_with ( self , timeslots : 'TimeslotCollection' ) -> bool : 
for slot in timeslots . timeslots : 
~~~ for interval in self . _table [ slot . channel ] : 
~~~ if slot . interval . has_overlap ( interval ) : 
~~ ~~ ~~ return True 
~~ def merged ( self , timeslots : 'TimeslotCollection' ) -> 'TimeslotCollection' : 
slots = [ Timeslot ( slot . interval , slot . channel ) for slot in self . timeslots ] 
slots . extend ( [ Timeslot ( slot . interval , slot . channel ) for slot in timeslots . timeslots ] ) 
return TimeslotCollection ( * slots ) 
~~ def shift ( self , time : int ) -> 'TimeslotCollection' : 
slots = [ Timeslot ( slot . interval . shift ( time ) , slot . channel ) for slot in self . timeslots ] 
op = self . children [ 0 ] . name 
expr = self . children [ 1 ] 
dispatch = { 
'sin' : sympy . sin , 
'cos' : sympy . cos , 
'tan' : sympy . tan , 
'asin' : sympy . asin , 
'acos' : sympy . acos , 
'atan' : sympy . atan , 
'exp' : sympy . exp , 
'ln' : sympy . log , 
'sqrt' : sympy . sqrt 
if op in dispatch : 
~~~ arg = expr . real ( nested_scope ) 
return dispatch [ op ] ( arg ) 
~~ ~~ def report ( self , branch , commit , infourl = None ) : 
issue_number = self . _get_report_issue_number ( ) 
if issue_number : 
~~~ self . _report_as_comment ( issue_number , branch , commit , infourl ) 
~~~ self . _report_as_issue ( branch , commit , infourl ) 
~~ ~~ def process_data ( rho ) : 
num = int ( np . log2 ( len ( rho ) ) ) 
labels = list ( map ( lambda x : x . to_label ( ) , pauli_group ( num ) ) ) 
values = list ( map ( lambda x : np . real ( np . trace ( np . dot ( x . to_matrix ( ) , rho ) ) ) , 
pauli_group ( num ) ) ) 
~~~ result [ label ] = values [ position ] 
~~ def iplot_state_paulivec ( rho , figsize = None , slider = False , show_legend = False ) : 
rho = _validate_input_state ( rho ) 
~~ options = { 'width' : figsize [ 0 ] , 'height' : figsize [ 1 ] , 
'slider' : int ( slider ) , 'show_legend' : int ( show_legend ) } 
data_to_plot = [ ] 
rho_data = process_data ( rho ) 
data_to_plot . append ( dict ( 
data = rho_data 
html = html_template . substitute ( { 
~~ def iplot_state ( quantum_state , method = 'city' , figsize = None ) : 
rho = _validate_input_state ( quantum_state ) 
if method == "city" : 
~~~ iplot_state_city ( rho , figsize = figsize ) 
~~ elif method == "paulivec" : 
~~~ iplot_state_paulivec ( rho , figsize = figsize ) 
~~ elif method == "qsphere" : 
~~~ iplot_state_qsphere ( rho , figsize = figsize ) 
~~ elif method == "bloch" : 
~~~ iplot_bloch_multivector ( rho , figsize = figsize ) 
~~ elif method == "hinton" : 
~~~ iplot_state_hinton ( rho , figsize = figsize ) 
~~ ~~ def rzz ( self , theta , qubit1 , qubit2 ) : 
return self . append ( RZZGate ( theta ) , [ qubit1 , qubit2 ] , [ ] ) 
~~ def cswap ( self , ctl , tgt1 , tgt2 ) : 
return self . append ( FredkinGate ( ) , [ ctl , tgt1 , tgt2 ] , [ ] ) 
( CnotGate ( ) , [ q [ 2 ] , q [ 1 ] ] , [ ] ) , 
( ToffoliGate ( ) , [ q [ 0 ] , q [ 1 ] , q [ 2 ] ] , [ ] ) , 
( CnotGate ( ) , [ q [ 2 ] , q [ 1 ] ] , [ ] ) 
~~ def _initialize_backend_prop ( self ) : 
backend_prop = self . backend_prop 
for ginfo in backend_prop . gates : 
~~~ if ginfo . gate == 'cx' : 
~~~ for item in ginfo . parameters : 
~~~ if item . name == 'gate_error' : 
~~~ g_reliab = 1.0 - item . value 
~~~ g_reliab = 1.0 
~~ ~~ swap_reliab = - math . log ( pow ( g_reliab , 3 ) ) 
self . swap_graph . add_edge ( ginfo . qubits [ 0 ] , ginfo . qubits [ 1 ] , weight = swap_reliab ) 
self . swap_graph . add_edge ( ginfo . qubits [ 1 ] , ginfo . qubits [ 0 ] , weight = swap_reliab ) 
self . cx_errors [ ( ginfo . qubits [ 0 ] , ginfo . qubits [ 1 ] ) ] = g_reliab 
self . gate_list . append ( ( ginfo . qubits [ 0 ] , ginfo . qubits [ 1 ] ) ) 
~~ ~~ idx = 0 
for q in backend_prop . qubits : 
~~~ for nduv in q : 
~~~ if nduv . name == 'readout_error' : 
~~~ self . readout_errors [ idx ] = 1.0 - nduv . value 
self . available_hw_qubits . append ( idx ) 
~~ ~~ idx += 1 
~~ for edge in self . cx_errors : 
~~~ self . gate_cost [ edge ] = self . cx_errors [ edge ] * self . readout_errors [ edge [ 0 ] ] * self . readout_errors [ edge [ 1 ] ] 
~~ self . swap_paths , swap_costs_temp = nx . algorithms . shortest_paths . dense . floyd_warshall_predecessor_and_distance ( self . swap_graph , weight = 'weight' ) 
for i in swap_costs_temp : 
~~~ self . swap_costs [ i ] = { } 
for j in swap_costs_temp [ i ] : 
~~~ if ( i , j ) in self . cx_errors : 
~~~ self . swap_costs [ i ] [ j ] = self . cx_errors [ ( i , j ) ] 
~~ elif ( j , i ) in self . cx_errors : 
~~~ self . swap_costs [ i ] [ j ] = self . cx_errors [ ( j , i ) ] 
~~~ best_reliab = 0.0 
for n in self . swap_graph . neighbors ( j ) : 
~~~ if ( n , j ) in self . cx_errors : 
~~~ reliab = math . exp ( - swap_costs_temp [ i ] [ n ] ) * self . cx_errors [ ( n , j ) ] 
~~~ reliab = math . exp ( - swap_costs_temp [ i ] [ n ] ) * self . cx_errors [ ( j , n ) ] 
~~ if reliab > best_reliab : 
~~~ best_reliab = reliab 
~~ ~~ self . swap_costs [ i ] [ j ] = best_reliab 
~~ ~~ ~~ ~~ def _create_program_graph ( self , dag ) : 
for q in dag . qubits ( ) : 
~~~ self . qarg_to_id [ q [ 0 ] . name + str ( q [ 1 ] ) ] = idx 
idx += 1 
~~ for gate in dag . twoQ_gates ( ) : 
~~~ qid1 = self . _qarg_to_id ( gate . qargs [ 0 ] ) 
qid2 = self . _qarg_to_id ( gate . qargs [ 1 ] ) 
min_q = min ( qid1 , qid2 ) 
max_q = max ( qid1 , qid2 ) 
edge_weight = 1 
if self . prog_graph . has_edge ( min_q , max_q ) : 
~~~ edge_weight = self . prog_graph [ min_q ] [ max_q ] [ 'weight' ] + 1 
~~ self . prog_graph . add_edge ( min_q , max_q , weight = edge_weight ) 
~~ return idx 
~~ def _select_next_edge ( self ) : 
for edge in self . pending_program_edges : 
~~~ q1_mapped = edge [ 0 ] in self . prog2hw 
q2_mapped = edge [ 1 ] in self . prog2hw 
assert not ( q1_mapped and q2_mapped ) 
if q1_mapped or q2_mapped : 
~~~ return edge 
~~ ~~ return self . pending_program_edges [ 0 ] 
~~ def _select_best_remaining_cx ( self ) : 
candidates = [ ] 
for gate in self . gate_list : 
~~~ chk1 = gate [ 0 ] in self . available_hw_qubits 
chk2 = gate [ 1 ] in self . available_hw_qubits 
if chk1 and chk2 : 
~~~ candidates . append ( gate ) 
~~ ~~ best_reliab = 0 
best_item = None 
for item in candidates : 
~~~ if self . gate_cost [ item ] > best_reliab : 
~~~ best_reliab = self . gate_cost [ item ] 
best_item = item 
~~ ~~ return best_item 
~~ def _select_best_remaining_qubit ( self , prog_qubit ) : 
reliab_store = { } 
for hw_qubit in self . available_hw_qubits : 
~~~ reliab = 1 
for n in self . prog_graph . neighbors ( prog_qubit ) : 
~~~ if n in self . prog2hw : 
~~~ reliab *= self . swap_costs [ self . prog2hw [ n ] ] [ hw_qubit ] 
~~ ~~ reliab *= self . readout_errors [ hw_qubit ] 
reliab_store [ hw_qubit ] = reliab 
~~ max_reliab = 0 
best_hw_qubit = None 
for hw_qubit in reliab_store : 
~~~ if reliab_store [ hw_qubit ] > max_reliab : 
~~~ max_reliab = reliab_store [ hw_qubit ] 
best_hw_qubit = hw_qubit 
~~ ~~ return best_hw_qubit 
self . _initialize_backend_prop ( ) 
num_qubits = self . _create_program_graph ( dag ) 
if num_qubits > len ( self . swap_graph ) : 
~~ for end1 , end2 , _ in sorted ( self . prog_graph . edges ( data = True ) , 
key = lambda x : x [ 2 ] [ 'weight' ] , reverse = True ) : 
~~~ self . pending_program_edges . append ( ( end1 , end2 ) ) 
~~ while self . pending_program_edges : 
~~~ edge = self . _select_next_edge ( ) 
q1_mapped = edge [ 0 ] in self . prog2hw 
if ( not q1_mapped ) and ( not q2_mapped ) : 
~~~ best_hw_edge = self . _select_best_remaining_cx ( ) 
self . prog2hw [ edge [ 0 ] ] = best_hw_edge [ 0 ] 
self . prog2hw [ edge [ 1 ] ] = best_hw_edge [ 1 ] 
self . available_hw_qubits . remove ( best_hw_edge [ 0 ] ) 
self . available_hw_qubits . remove ( best_hw_edge [ 1 ] ) 
~~ elif not q1_mapped : 
~~~ best_hw_qubit = self . _select_best_remaining_qubit ( edge [ 0 ] ) 
self . prog2hw [ edge [ 0 ] ] = best_hw_qubit 
self . available_hw_qubits . remove ( best_hw_qubit ) 
~~~ best_hw_qubit = self . _select_best_remaining_qubit ( edge [ 1 ] ) 
self . prog2hw [ edge [ 1 ] ] = best_hw_qubit 
~~ new_edges = [ x for x in self . pending_program_edges 
if not ( x [ 0 ] in self . prog2hw and x [ 1 ] in self . prog2hw ) ] 
self . pending_program_edges = new_edges 
~~ for qid in self . qarg_to_id . values ( ) : 
~~~ if qid not in self . prog2hw : 
~~~ self . prog2hw [ qid ] = self . available_hw_qubits [ 0 ] 
self . available_hw_qubits . remove ( self . prog2hw [ qid ] ) 
~~ ~~ layout = Layout ( ) 
~~~ pid = self . _qarg_to_id ( q ) 
hwid = self . prog2hw [ pid ] 
layout [ ( q [ 0 ] , q [ 1 ] ) ] = hwid 
~~ self . property_set [ 'layout' ] = layout 
~~ def instruction_list ( self ) : 
instruction_list = [ ] 
for instruction in self . data : 
~~~ if isinstance ( instruction , CompositeGate ) : 
~~~ instruction_list . extend ( instruction . instruction_list ( ) ) 
~~~ instruction_list . append ( instruction ) 
~~ ~~ return instruction_list 
self . data = [ gate . inverse ( ) for gate in reversed ( self . data ) ] 
self . inverse_flag = not self . inverse_flag 
self . data = [ gate . q_if ( qregs ) for gate in self . data ] 
self . data = [ gate . c_if ( classical , val ) for gate in self . data ] 
~~ def is_unitary ( self , atol = None , rtol = None ) : 
~~ return is_unitary_matrix ( self . _data , rtol = rtol , atol = atol ) 
return Operator ( 
np . conj ( self . data ) , self . input_dims ( ) , self . output_dims ( ) ) 
np . transpose ( self . data ) , self . input_dims ( ) , self . output_dims ( ) ) 
if not isinstance ( n , int ) : 
~~ if self . input_dims ( ) != self . output_dims ( ) : 
~~ return Operator ( 
np . linalg . matrix_power ( self . data , n ) , self . input_dims ( ) , 
~~ def add ( self , other ) : 
if not isinstance ( other , Operator ) : 
~~~ other = Operator ( other ) 
~~ if self . dim != other . dim : 
~~ return Operator ( self . data + other . data , self . input_dims ( ) , 
~~ return Operator ( other * self . data , self . input_dims ( ) , 
~~ def _shape ( self ) : 
return tuple ( reversed ( self . output_dims ( ) ) ) + tuple ( 
reversed ( self . input_dims ( ) ) ) 
state = self . _format_state ( state ) 
if qargs is None : 
~~~ if state . shape [ 0 ] != self . _input_dim : 
~~~ return np . dot ( self . data , state ) 
~~ return np . dot ( 
np . dot ( self . data , state ) , np . transpose ( np . conj ( self . data ) ) ) 
~~ return self . _evolve_subsystem ( state , qargs ) 
~~ def _evolve_subsystem ( self , state , qargs ) : 
mat = np . reshape ( self . data , self . _shape ) 
state_size = len ( state ) 
state_dims = self . _automatic_dims ( None , state_size ) 
if self . input_dims ( ) != len ( qargs ) * ( 2 , ) : 
~~~ tensor = np . reshape ( state , state_dims ) 
indices = [ len ( state_dims ) - 1 - qubit for qubit in qargs ] 
tensor = self . _einsum_matmul ( tensor , mat , indices ) 
return np . reshape ( tensor , state_size ) 
~~ tensor = np . reshape ( state , 2 * state_dims ) 
right_shift = len ( state_dims ) 
tensor = self . _einsum_matmul ( 
tensor , np . conj ( mat ) , indices , shift = right_shift ) 
return np . reshape ( tensor , [ state_size , state_size ] ) 
~~ def _format_state ( self , state ) : 
shape = state . shape 
ndim = state . ndim 
if ndim > 2 : 
~~ if ndim == 2 : 
~~~ if shape [ 1 ] != 1 and shape [ 1 ] != shape [ 0 ] : 
~~ if shape [ 1 ] == 1 : 
~~~ state = np . reshape ( state , shape [ 0 ] ) 
~~ ~~ return state 
~~ def _instruction_to_operator ( cls , instruction ) : 
if isinstance ( instruction , QuantumCircuit ) : 
~~ op = Operator ( np . eye ( 2 ** instruction . num_qubits ) ) 
op . _append_instruction ( instruction ) 
return op 
~~ def _append_instruction ( self , obj , qargs = None ) : 
if isinstance ( obj , Instruction ) : 
~~~ mat = None 
if hasattr ( obj , 'to_matrix' ) : 
~~~ mat = obj . to_matrix ( ) 
~~ except QiskitError : 
~~ ~~ if mat is not None : 
~~~ op = self . compose ( mat , qargs = qargs ) 
self . _data = op . data 
~~~ if obj . definition is None : 
~~ for instr , qregs , cregs in obj . definition : 
~~~ if cregs : 
instr . name ) ) 
~~ new_qargs = [ tup [ 1 ] for tup in qregs ] 
self . _append_instruction ( instr , qargs = new_qargs ) 
~~ ~~ def run ( self , dag ) : 
if dag . width ( ) > self . coupling_map . size ( ) : 
~~ layerlist = list ( dag . layers ( ) ) 
if self . initial_layout is None and self . property_set [ "layout" ] : 
~~~ virtual_qubits = self . initial_layout . get_virtual_bits ( ) 
self . initial_layout = { ( v [ 0 ] . name , v [ 1 ] ) : ( 'q' , self . initial_layout [ v ] ) for v in 
virtual_qubits } 
device_register = QuantumRegister ( self . coupling_map . size ( ) , 'q' ) 
initial_layout = { ( dag . qregs [ k [ 0 ] ] , k [ 1 ] ) : ( device_register , v [ 1 ] ) 
for k , v in self . initial_layout . items ( ) } 
circ_qubits = dag . qubits ( ) 
coup_qubits = [ ( QuantumRegister ( self . coupling_map . size ( ) , 'q' ) , wire ) for wire in 
self . coupling_map . physical_qubits ] 
qubit_subset = [ ] 
for k , v in initial_layout . items ( ) : 
~~~ qubit_subset . append ( v ) 
if k not in circ_qubits : 
"DAGCircuit" % ( k [ 0 ] . name , k [ 1 ] ) ) 
~~ if v not in coup_qubits : 
"CouplingGraph" % ( v [ 0 ] . name , v [ 1 ] ) ) 
~~~ qubit_subset = [ ( QuantumRegister ( self . coupling_map . size ( ) , 'q' ) , wire ) for wire in 
qubit_subset = qubit_subset [ 0 : dag . width ( ) ] 
initial_layout = { a : b for a , b in zip ( dag . qubits ( ) , qubit_subset ) } 
~~ layout = initial_layout . copy ( ) 
dagcircuit_output . name = dag . name 
dagcircuit_output . add_qreg ( QuantumRegister ( self . coupling_map . size ( ) , "q" ) ) 
for creg in dag . cregs . values ( ) : 
q = QuantumRegister ( self . coupling_map . size ( ) , 'q' ) 
for j in range ( self . coupling_map . size ( ) ) : 
~~~ identity_wire_map [ ( q , j ) ] = ( q , j ) 
~~~ for j in range ( creg . size ) : 
~~~ identity_wire_map [ ( creg , j ) ] = ( creg , j ) 
~~~ success_flag , best_circ , best_d , best_layout , trivial_flag = self . layer_permutation ( layer [ "partition" ] , layout , qubit_subset ) 
~~~ serial_layerlist = list ( layer [ "graph" ] . serial_layers ( ) ) 
~~~ success_flag , best_circ , best_d , best_layout , trivial_flag = self . layer_permutation ( serial_layer [ "partition" ] , layout , qubit_subset ) 
dagcircuit_output . compose_back ( 
self . swap_mapper_layer_update ( j , 
best_d , 
best_circ , 
~~~ initial_layout = layout 
first_layer = False 
self . swap_mapper_layer_update ( i , 
~~ ~~ ~~ if first_layer : 
~~~ layout = initial_layout 
~~~ dagcircuit_output . compose_back ( layer [ "graph" ] , layout ) 
~~ def layer_permutation ( self , layer_partition , layout , qubit_subset ) : 
~~ rng = np . random . RandomState ( self . seed ) 
rev_layout = { b : a for a , b in layout . items ( ) } 
gates = [ ] 
for layer in layer_partition : 
~~~ if len ( layer ) > 2 : 
~~ elif len ( layer ) == 2 : 
~~~ gates . append ( tuple ( layer ) ) 
~~ ~~ dist = sum ( [ self . coupling_map . distance ( layout [ g [ 0 ] ] [ 1 ] , layout [ g [ 1 ] ] [ 1 ] ) for g in gates ] ) 
~~~ circ = DAGCircuit ( ) 
circ . add_qreg ( QuantumRegister ( self . coupling_map . size ( ) , "q" ) ) 
return True , circ , 0 , layout , bool ( gates ) 
~~ n = self . coupling_map . size ( ) 
QR = QuantumRegister ( self . coupling_map . size ( ) , "q" ) 
for _ in range ( self . trials ) : 
rev_trial_layout = rev_layout . copy ( ) 
trial_circ = DAGCircuit ( ) 
trial_circ . add_qreg ( QR ) 
xi = { } 
for i in self . coupling_map . physical_qubits : 
~~~ xi [ ( QR , i ) ] = { } 
~~ for i in self . coupling_map . physical_qubits : 
~~~ i = ( QR , i ) 
for j in self . coupling_map . physical_qubits : 
~~~ j = ( QR , j ) 
scale = 1 + rng . normal ( 0 , 1 / n ) 
xi [ i ] [ j ] = scale * self . coupling_map . distance ( i [ 1 ] , j [ 1 ] ) ** 2 
xi [ j ] [ i ] = xi [ i ] [ j ] 
~~ ~~ d = 1 
circ . add_qreg ( QR ) 
identity_wire_map = { ( QR , j ) : ( QR , j ) for j in range ( n ) } 
while d < 2 * n + 1 : 
~~~ qubit_set = set ( qubit_subset ) 
while qubit_set : 
~~~ min_cost = sum ( [ xi [ trial_layout [ g [ 0 ] ] ] [ trial_layout [ g [ 1 ] ] ] 
progress_made = False 
for e in self . coupling_map . get_edges ( ) : 
~~~ e = [ ( QR , edge ) for edge in e ] 
if e [ 0 ] in qubit_set and e [ 1 ] in qubit_set : 
~~~ new_layout = trial_layout . copy ( ) 
new_layout [ rev_trial_layout [ e [ 0 ] ] ] = e [ 1 ] 
new_layout [ rev_trial_layout [ e [ 1 ] ] ] = e [ 0 ] 
rev_new_layout = rev_trial_layout . copy ( ) 
rev_new_layout [ e [ 0 ] ] = rev_trial_layout [ e [ 1 ] ] 
rev_new_layout [ e [ 1 ] ] = rev_trial_layout [ e [ 0 ] ] 
new_cost = sum ( [ xi [ new_layout [ g [ 0 ] ] ] [ new_layout [ g [ 1 ] ] ] 
if new_cost < min_cost : 
~~~ progress_made = True 
min_cost = new_cost 
opt_layout = new_layout 
rev_opt_layout = rev_new_layout 
opt_edge = e 
~~ ~~ ~~ if progress_made : 
~~~ qubit_set . remove ( opt_edge [ 0 ] ) 
qubit_set . remove ( opt_edge [ 1 ] ) 
trial_layout = opt_layout 
rev_trial_layout = rev_opt_layout 
circ . apply_operation_back ( 
SwapGate ( ) , 
[ ( opt_edge [ 0 ] [ 0 ] , opt_edge [ 0 ] [ 1 ] ) , ( opt_edge [ 1 ] [ 0 ] , opt_edge [ 1 ] [ 1 ] ) ] , 
[ ] ) 
~~ ~~ dist = sum ( [ self . coupling_map . distance ( trial_layout [ g [ 0 ] ] [ 1 ] , 
trial_layout [ g [ 1 ] ] [ 1 ] ) for g in gates ] ) 
~~~ trial_circ . compose_back ( circ , identity_wire_map ) 
~~ d += 1 
~~ dist = sum ( [ self . coupling_map . distance ( trial_layout [ g [ 0 ] ] [ 1 ] , 
~~~ if d < best_d : 
~~~ best_circ = trial_circ 
~~ best_d = min ( best_d , d ) 
~~ ~~ if best_circ is None : 
~~~ return False , None , None , None , False 
~~ return True , best_circ , best_d , best_layout , False 
~~ def swap_mapper_layer_update ( self , i , first_layer , best_layout , best_d , 
best_circ , layer_list ) : 
QR = QuantumRegister ( self . coupling_map . size ( ) , 'q' ) 
dagcircuit_output . add_qreg ( QR ) 
identity_wire_map = { ( QR , j ) : ( QR , j ) for j in range ( self . coupling_map . size ( ) ) } 
~~~ dagcircuit_output . compose_back ( layer_list [ j ] [ "graph" ] , layout ) 
~~~ if best_d > 0 : 
~~~ dagcircuit_output . compose_back ( best_circ , identity_wire_map ) 
~~ dagcircuit_output . compose_back ( layer_list [ i ] [ "graph" ] , layout ) 
expr = self . children [ 1 ] . real ( nested_scope ) 
return operation ( expr ) 
expr = self . children [ 1 ] . sym ( nested_scope ) 
~~ def _separate_bitstring ( bitstring , creg_sizes ) : 
substrings = [ ] 
running_index = 0 
for _ , size in reversed ( creg_sizes ) : 
~~~ substrings . append ( bitstring [ running_index : running_index + size ] ) 
running_index += size 
~~ def format_counts_memory ( shot_memory , header = None ) : 
if shot_memory . startswith ( '0x' ) : 
~~~ shot_memory = _hex_to_bin ( shot_memory ) 
~~ if header : 
~~~ creg_sizes = header . get ( 'creg_sizes' , None ) 
memory_slots = header . get ( 'memory_slots' , None ) 
if memory_slots : 
~~~ shot_memory = _pad_zeros ( shot_memory , memory_slots ) 
~~ if creg_sizes and memory_slots : 
~~~ shot_memory = _separate_bitstring ( shot_memory , creg_sizes ) 
~~ ~~ return shot_memory 
~~ def _list_to_complex_array ( complex_list ) : 
arr = np . asarray ( complex_list , dtype = np . complex_ ) 
if not arr . shape [ - 1 ] == 2 : 
~~ return arr [ ... , 0 ] + 1j * arr [ ... , 1 ] 
~~ def format_level_0_memory ( memory ) : 
formatted_memory = _list_to_complex_array ( memory ) 
if not 2 <= len ( formatted_memory . shape ) <= 3 : 
~~ return formatted_memory 
~~ def format_level_1_memory ( memory ) : 
if not 1 <= len ( formatted_memory . shape ) <= 2 : 
~~ def format_level_2_memory ( memory , header = None ) : 
memory_list = [ ] 
for shot_memory in memory : 
~~~ memory_list . append ( format_counts_memory ( shot_memory , header ) ) 
~~ return memory_list 
~~ def format_counts ( counts , header = None ) : 
counts_dict = { } 
for key , val in counts . items ( ) : 
~~~ key = format_counts_memory ( key , header ) 
counts_dict [ key ] = val 
~~ return counts_dict 
~~ def format_statevector ( vec , decimals = None ) : 
num_basis = len ( vec ) 
vec_complex = np . zeros ( num_basis , dtype = complex ) 
for i in range ( num_basis ) : 
~~~ vec_complex [ i ] = vec [ i ] [ 0 ] + 1j * vec [ i ] [ 1 ] 
~~ if decimals : 
~~~ vec_complex = np . around ( vec_complex , decimals = decimals ) 
~~ return vec_complex 
~~ def format_unitary ( mat , decimals = None ) : 
num_basis = len ( mat ) 
mat_complex = np . zeros ( ( num_basis , num_basis ) , dtype = complex ) 
for i , vec in enumerate ( mat ) : 
~~~ mat_complex [ i ] = format_statevector ( vec , decimals ) 
~~ return mat_complex 
~~ def requires_submit ( func ) : 
def _wrapper ( self , * args , ** kwargs ) : 
~~~ if self . _future is None : 
~~ return func ( self , * args , ** kwargs ) 
~~ return _wrapper 
~~ def submit ( self ) : 
if self . _future is not None : 
~~ validate_qobj_against_schema ( self . _qobj ) 
self . _future = self . _executor . submit ( self . _fn , self . _job_id , self . _qobj ) 
if self . _future . running ( ) : 
~~~ _status = JobStatus . RUNNING 
~~ elif self . _future . cancelled ( ) : 
~~~ _status = JobStatus . CANCELLED 
~~ elif self . _future . done ( ) : 
~~~ _status = JobStatus . DONE if self . _future . exception ( ) is None else JobStatus . ERROR 
~~~ _status = JobStatus . INITIALIZING 
~~ return _status 
~~ def includes ( self , lo_freq : float ) -> bool : 
if self . _lb <= lo_freq <= self . _ub : 
~~ def iplot_bloch_multivector ( rho , figsize = None ) : 
~~~ options = { } 
~~~ options = { 'width' : figsize [ 0 ] , 'height' : figsize [ 1 ] } 
~~ num = int ( np . log2 ( len ( rho ) ) ) 
bloch_data = [ ] 
for i in range ( num ) : 
~~~ pauli_singles = [ Pauli . pauli_single ( num , i , 'X' ) , Pauli . pauli_single ( num , i , 'Y' ) , 
Pauli . pauli_single ( num , i , 'Z' ) ] 
bloch_state = list ( map ( lambda x : np . real ( np . trace ( np . dot ( x . to_matrix ( ) , rho ) ) ) , 
pauli_singles ) ) 
bloch_data . append ( bloch_state ) 
~~ div_number = str ( time . time ( ) ) 
'data' : bloch_data , 
num_processes = CPU_COUNT ) : 
if len ( values ) == 1 : 
~~~ return [ task ( values [ 0 ] , * task_args , ** task_kwargs ) ] 
~~ Publisher ( ) . publish ( "terra.parallel.start" , len ( values ) ) 
nfinished = [ 0 ] 
def _callback ( _ ) : 
~~~ nfinished [ 0 ] += 1 
Publisher ( ) . publish ( "terra.parallel.done" , nfinished [ 0 ] ) 
~~ if platform . system ( ) != 'Windows' and num_processes > 1 and os . getenv ( 'QISKIT_IN_PARALLEL' ) == 'FALSE' : 
~~~ os . environ [ 'QISKIT_IN_PARALLEL' ] = 'TRUE' 
~~~ pool = Pool ( processes = num_processes ) 
async_res = [ pool . apply_async ( task , ( value , ) + task_args , task_kwargs , 
_callback ) for value in values ] 
while not all ( [ item . ready ( ) for item in async_res ] ) : 
~~~ for item in async_res : 
~~~ item . wait ( timeout = 0.1 ) 
~~ ~~ pool . terminate ( ) 
~~~ pool . terminate ( ) 
Publisher ( ) . publish ( "terra.parallel.finish" ) 
~~ Publisher ( ) . publish ( "terra.parallel.finish" ) 
os . environ [ 'QISKIT_IN_PARALLEL' ] = 'FALSE' 
return [ ar . get ( ) for ar in async_res ] 
for _ , value in enumerate ( values ) : 
~~~ result = task ( value , * task_args , ** task_kwargs ) 
results . append ( result ) 
_callback ( 0 ) 
~~ def get_qubit_los ( self , user_lo_config ) : 
~~~ _q_los = self . default_qubit_los . copy ( ) 
~~ for channel , lo_freq in user_lo_config . qubit_lo_dict ( ) . items ( ) : 
~~~ _q_los [ channel . index ] = lo_freq 
~~ if _q_los == self . default_qubit_los : 
~~ return _q_los 
~~ def get_meas_los ( self , user_lo_config ) : 
~~~ _m_los = self . default_meas_los . copy ( ) 
~~ for channel , lo_freq in user_lo_config . meas_lo_dict ( ) . items ( ) : 
~~~ _m_los [ channel . index ] = lo_freq 
~~ if _m_los == self . default_meas_los : 
~~ return _m_los 
for node in dag . op_nodes ( ) : 
~~~ basic_insts = [ 'measure' , 'reset' , 'barrier' , 'snapshot' ] 
if node . name in basic_insts : 
~~ rule = node . op . definition 
if not rule : 
( str ( self . basis ) , node . op . name ) ) 
~~ decomposition = DAGCircuit ( ) 
decomposition . add_qreg ( rule [ 0 ] [ 1 ] [ 0 ] [ 0 ] ) 
~~~ decomposition . apply_operation_back ( * inst ) 
dag . substitute_node_with_dag ( node , unrolled_dag ) 
~~ def iplot_state_qsphere ( rho , figsize = None ) : 
~~ qspheres_data = [ ] 
weig , stateall = linalg . eigh ( rho ) 
for _ in range ( 2 ** num ) : 
~~~ probmix = weig . max ( ) 
prob_location = weig . argmax ( ) 
if probmix > 0.001 : 
~~~ state = stateall [ : , prob_location ] 
loc = np . absolute ( state ) . argmax ( ) 
for j in range ( 2 ** num ) : 
~~~ test = np . absolute ( np . absolute ( state [ j ] ) - 
np . absolute ( state [ loc ] ) ) 
if test < 0.001 : 
~~~ loc = j 
~~ ~~ angles = ( np . angle ( state [ loc ] ) + 2 * np . pi ) % ( 2 * np . pi ) 
angleset = np . exp ( - 1j * angles ) 
state = angleset * state 
state . flatten ( ) 
spherepoints = [ ] 
for i in range ( 2 ** num ) : 
~~~ element = bin ( i ) [ 2 : ] . zfill ( num ) 
weight = element . count ( "1" ) 
number_of_divisions = n_choose_k ( num , weight ) 
weight_order = bit_string_index ( element ) 
angle = weight_order * 2 * np . pi / number_of_divisions 
zvalue = - 2 * weight / num + 1 
xvalue = np . sqrt ( 1 - zvalue ** 2 ) * np . cos ( angle ) 
yvalue = np . sqrt ( 1 - zvalue ** 2 ) * np . sin ( angle ) 
prob = np . real ( np . dot ( state [ i ] , state [ i ] . conj ( ) ) ) 
angles = ( np . angle ( state [ i ] ) + 2 * np . pi ) % ( 2 * np . pi ) 
qpoint = { 
'x' : xvalue , 
'y' : yvalue , 
'z' : zvalue , 
'prob' : prob , 
'phase' : angles 
spherepoints . append ( qpoint ) 
~~ sphere = { 
'points' : spherepoints , 
'eigenvalue' : probmix 
qspheres_data . append ( sphere ) 
weig [ prob_location ] = 0 
~~ ~~ div_number = str ( time . time ( ) ) 
'data' : qspheres_data , 
~~ def n_choose_k ( n , k ) : 
~~ return reduce ( lambda x , y : x * y [ 0 ] / y [ 1 ] , 
zip ( range ( n - k + 1 , n + 1 ) , 
range ( 1 , k + 1 ) ) , 1 ) 
~~ def bit_string_index ( text ) : 
n = len ( text ) 
k = text . count ( "1" ) 
if text . count ( "0" ) != n - k : 
~~ ones = [ pos for pos , char in enumerate ( text ) if char == "1" ] 
return lex_index ( n , k , ones ) 
~~ def lex_index ( n , k , lst ) : 
if len ( lst ) != k : 
~~ comb = list ( map ( lambda x : n - 1 - x , lst ) ) 
dualm = sum ( [ n_choose_k ( comb [ k - 1 - i ] , i + 1 ) for i in range ( k ) ] ) 
return int ( dualm ) 
~~ def plot_state_hinton ( rho , title = '' , figsize = None ) : 
~~ rho = _validate_input_state ( rho ) 
~~~ figsize = ( 8 , 5 ) 
fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = figsize ) 
max_weight = 2 ** np . ceil ( np . log ( np . abs ( rho ) . max ( ) ) / np . log ( 2 ) ) 
datareal = np . real ( rho ) 
dataimag = np . imag ( rho ) 
column_names = [ bin ( i ) [ 2 : ] . zfill ( num ) for i in range ( 2 ** num ) ] 
row_names = [ bin ( i ) [ 2 : ] . zfill ( num ) for i in range ( 2 ** num ) ] 
ly = len ( datareal [ : , 0 ] ) 
ax1 . patch . set_facecolor ( 'gray' ) 
ax1 . set_aspect ( 'equal' , 'box' ) 
ax1 . xaxis . set_major_locator ( plt . NullLocator ( ) ) 
ax1 . yaxis . set_major_locator ( plt . NullLocator ( ) ) 
for ( x , y ) , w in np . ndenumerate ( datareal ) : 
~~~ color = 'white' if w > 0 else 'black' 
size = np . sqrt ( np . abs ( w ) / max_weight ) 
rect = plt . Rectangle ( [ x - size / 2 , y - size / 2 ] , size , size , 
facecolor = color , edgecolor = color ) 
ax1 . add_patch ( rect ) 
~~ ax1 . set_xticks ( np . arange ( 0 , lx + 0.5 , 1 ) ) 
ax1 . set_yticks ( np . arange ( 0 , ly + 0.5 , 1 ) ) 
ax1 . set_yticklabels ( row_names , fontsize = 14 ) 
ax1 . set_xticklabels ( column_names , fontsize = 14 , rotation = 90 ) 
ax1 . autoscale_view ( ) 
ax1 . invert_yaxis ( ) 
ax1 . set_title ( 'Real[rho]' , fontsize = 14 ) 
ax2 . patch . set_facecolor ( 'gray' ) 
ax2 . set_aspect ( 'equal' , 'box' ) 
ax2 . xaxis . set_major_locator ( plt . NullLocator ( ) ) 
ax2 . yaxis . set_major_locator ( plt . NullLocator ( ) ) 
for ( x , y ) , w in np . ndenumerate ( dataimag ) : 
ax2 . add_patch ( rect ) 
~~ if np . any ( dataimag != 0 ) : 
~~~ ax2 . set_xticks ( np . arange ( 0 , lx + 0.5 , 1 ) ) 
ax2 . set_yticks ( np . arange ( 0 , ly + 0.5 , 1 ) ) 
ax2 . set_yticklabels ( row_names , fontsize = 14 ) 
ax2 . set_xticklabels ( column_names , fontsize = 14 , rotation = 90 ) 
~~ ax2 . autoscale_view ( ) 
ax2 . invert_yaxis ( ) 
ax2 . set_title ( 'Imag[rho]' , fontsize = 14 ) 
~~~ fig . suptitle ( title , fontsize = 16 ) 
~~ plt . tight_layout ( ) 
plt . close ( fig ) 
return fig 
~~ def plot_bloch_vector ( bloch , title = "" , ax = None , figsize = None ) : 
~~ if figsize is None : 
~~~ figsize = ( 5 , 5 ) 
~~ B = Bloch ( axes = ax ) 
B . add_vectors ( bloch ) 
B . render ( title = title ) 
~~~ fig = B . fig 
fig . set_size_inches ( figsize [ 0 ] , figsize [ 1 ] ) 
~~ def plot_bloch_multivector ( rho , title = '' , figsize = None ) : 
width , height = plt . figaspect ( 1 / num ) 
fig = plt . figure ( figsize = ( width , height ) ) 
~~~ ax = fig . add_subplot ( 1 , num , i + 1 , projection = '3d' ) 
pauli_singles = [ 
Pauli . pauli_single ( num , i , 'X' ) , 
Pauli . pauli_single ( num , i , 'Y' ) , 
Pauli . pauli_single ( num , i , 'Z' ) 
bloch_state = list ( 
map ( lambda x : np . real ( np . trace ( np . dot ( x . to_matrix ( ) , rho ) ) ) , 
figsize = figsize ) 
~~ fig . suptitle ( title , fontsize = 16 ) 
~~ def plot_state_city ( rho , title = "" , figsize = None , color = None , 
alpha = 1 ) : 
ypos = np . arange ( 0 , ly , 1 ) 
xpos , ypos = np . meshgrid ( xpos + 0.25 , ypos + 0.25 ) 
xpos = xpos . flatten ( ) 
ypos = ypos . flatten ( ) 
zpos = np . zeros ( lx * ly ) 
dy = dx . copy ( ) 
dzr = datareal . flatten ( ) 
dzi = dataimag . flatten ( ) 
~~~ color = [ "#648fff" , "#648fff" ] 
~~~ if len ( color ) != 2 : 
~~~ raise ValueError ( "\ ) 
~~ if color [ 0 ] is None : 
~~~ color [ 0 ] = "#648fff" 
~~ if color [ 1 ] is None : 
~~~ color [ 1 ] = "#648fff" 
~~ ~~ if figsize is None : 
~~~ figsize = ( 15 , 5 ) 
~~ fig = plt . figure ( figsize = figsize ) 
ax1 = fig . add_subplot ( 1 , 2 , 1 , projection = '3d' ) 
x = [ 0 , max ( xpos ) + 0.5 , max ( xpos ) + 0.5 , 0 ] 
y = [ 0 , 0 , max ( ypos ) + 0.5 , max ( ypos ) + 0.5 ] 
z = [ 0 , 0 , 0 , 0 ] 
verts = [ list ( zip ( x , y , z ) ) ] 
fc1 = generate_facecolors ( xpos , ypos , zpos , dx , dy , dzr , color [ 0 ] ) 
for idx , cur_zpos in enumerate ( zpos ) : 
~~~ if dzr [ idx ] > 0 : 
~~~ zorder = 2 
~~~ zorder = 0 
~~ b1 = ax1 . bar3d ( xpos [ idx ] , ypos [ idx ] , cur_zpos , 
dx [ idx ] , dy [ idx ] , dzr [ idx ] , 
alpha = alpha , zorder = zorder ) 
b1 . set_facecolors ( fc1 [ 6 * idx : 6 * idx + 6 ] ) 
~~ pc1 = Poly3DCollection ( verts , alpha = 0.15 , facecolor = 'k' , 
linewidths = 1 , zorder = 1 ) 
if min ( dzr ) < 0 < max ( dzr ) : 
~~~ ax1 . add_collection3d ( pc1 ) 
~~ ax2 = fig . add_subplot ( 1 , 2 , 2 , projection = '3d' ) 
fc2 = generate_facecolors ( xpos , ypos , zpos , dx , dy , dzi , color [ 1 ] ) 
~~~ if dzi [ idx ] > 0 : 
~~ b2 = ax2 . bar3d ( xpos [ idx ] , ypos [ idx ] , cur_zpos , 
dx [ idx ] , dy [ idx ] , dzi [ idx ] , 
b2 . set_facecolors ( fc2 [ 6 * idx : 6 * idx + 6 ] ) 
~~ pc2 = Poly3DCollection ( verts , alpha = 0.2 , facecolor = 'k' , 
if min ( dzi ) < 0 < max ( dzi ) : 
~~~ ax2 . add_collection3d ( pc2 ) 
~~ ax1 . set_xticks ( np . arange ( 0.5 , lx + 0.5 , 1 ) ) 
ax1 . set_yticks ( np . arange ( 0.5 , ly + 0.5 , 1 ) ) 
max_dzr = max ( dzr ) 
min_dzr = min ( dzr ) 
if max_dzr != min_dzr : 
~~~ ax1 . axes . set_zlim3d ( np . min ( dzr ) , np . max ( dzr ) + 1e-9 ) 
~~~ if min_dzr == 0 : 
~~~ ax1 . axes . set_zlim3d ( auto = True ) 
~~ ~~ ax1 . zaxis . set_major_locator ( MaxNLocator ( 5 ) ) 
ax1 . w_xaxis . set_ticklabels ( row_names , fontsize = 14 , rotation = 45 ) 
ax1 . w_yaxis . set_ticklabels ( column_names , fontsize = 14 , rotation = - 22.5 ) 
ax1 . set_zlabel ( "Real[rho]" , fontsize = 14 ) 
for tick in ax1 . zaxis . get_major_ticks ( ) : 
~~ ax2 . set_xticks ( np . arange ( 0.5 , lx + 0.5 , 1 ) ) 
ax2 . set_yticks ( np . arange ( 0.5 , ly + 0.5 , 1 ) ) 
min_dzi = np . min ( dzi ) 
max_dzi = np . max ( dzi ) 
if min_dzi != max_dzi : 
~~~ eps = 0 
ax2 . zaxis . set_major_locator ( MaxNLocator ( 5 ) ) 
ax2 . axes . set_zlim3d ( np . min ( dzi ) , np . max ( dzi ) + eps ) 
~~~ if min_dzi == 0 : 
~~~ ax2 . set_zticks ( [ 0 ] ) 
eps = 1e-9 
~~~ ax2 . axes . set_zlim3d ( auto = True ) 
~~ ~~ ax2 . w_xaxis . set_ticklabels ( row_names , fontsize = 14 , rotation = 45 ) 
ax2 . w_yaxis . set_ticklabels ( column_names , fontsize = 14 , rotation = - 22.5 ) 
ax2 . set_zlabel ( "Imag[rho]" , fontsize = 14 ) 
for tick in ax2 . zaxis . get_major_ticks ( ) : 
~~ plt . suptitle ( title , fontsize = 16 ) 
plt . tight_layout ( ) 
~~ def plot_state_paulivec ( rho , title = "" , figsize = None , color = None ) : 
numelem = len ( values ) 
~~~ color = "#648fff" 
fig , ax = plt . subplots ( figsize = figsize ) 
ax . grid ( zorder = 0 , linewidth = 1 , linestyle = '--' ) 
ax . bar ( ind , values , width , color = color , zorder = 2 ) 
ax . axhline ( linewidth = 1 , color = 'k' ) 
ax . set_xticks ( ind ) 
ax . set_yticks ( [ - 1 , - 0.5 , 0 , 0.5 , 1 ] ) 
ax . set_xticklabels ( labels , fontsize = 14 , rotation = 70 ) 
ax . set_xlabel ( 'Pauli' , fontsize = 14 ) 
ax . set_ylim ( [ - 1 , 1 ] ) 
ax . set_facecolor ( '#eeeeee' ) 
for tick in ax . xaxis . get_major_ticks ( ) + ax . yaxis . get_major_ticks ( ) : 
~~ ax . set_title ( title , fontsize = 16 ) 
~~ def bit_string_index ( s ) : 
n = len ( s ) 
k = s . count ( "1" ) 
if s . count ( "0" ) != n - k : 
~~ ones = [ pos for pos , char in enumerate ( s ) if char == "1" ] 
~~ def phase_to_color_wheel ( complex_number ) : 
angles = np . angle ( complex_number ) 
angle_round = int ( ( ( angles + 2 * np . pi ) % ( 2 * np . pi ) ) / np . pi * 6 ) 
color_map = { 
return color_map [ angle_round ] 
~~ def plot_state_qsphere ( rho , figsize = None ) : 
~~~ figsize = ( 7 , 7 ) 
we , stateall = linalg . eigh ( rho ) 
~~~ probmix = we . max ( ) 
prob_location = we . argmax ( ) 
fig = plt . figure ( figsize = figsize ) 
ax = fig . add_subplot ( 111 , projection = '3d' ) 
ax . axes . set_xlim3d ( - 1.0 , 1.0 ) 
ax . axes . set_ylim3d ( - 1.0 , 1.0 ) 
ax . axes . set_zlim3d ( - 1.0 , 1.0 ) 
ax . set_aspect ( "equal" ) 
ax . axes . grid ( False ) 
u = np . linspace ( 0 , 2 * np . pi , 25 ) 
v = np . linspace ( 0 , np . pi , 25 ) 
x = np . outer ( np . cos ( u ) , np . sin ( v ) ) 
y = np . outer ( np . sin ( u ) , np . sin ( v ) ) 
z = np . outer ( np . ones ( np . size ( u ) ) , np . cos ( v ) ) 
ax . plot_surface ( x , y , z , rstride = 1 , cstride = 1 , color = 'k' , 
alpha = 0.05 , linewidth = 0 ) 
ax . w_xaxis . set_pane_color ( ( 1.0 , 1.0 , 1.0 , 0.0 ) ) 
ax . w_yaxis . set_pane_color ( ( 1.0 , 1.0 , 1.0 , 0.0 ) ) 
ax . w_zaxis . set_pane_color ( ( 1.0 , 1.0 , 1.0 , 0.0 ) ) 
ax . w_xaxis . line . set_color ( ( 1.0 , 1.0 , 1.0 , 0.0 ) ) 
ax . w_yaxis . line . set_color ( ( 1.0 , 1.0 , 1.0 , 0.0 ) ) 
ax . w_zaxis . line . set_color ( ( 1.0 , 1.0 , 1.0 , 0.0 ) ) 
ax . set_xticks ( [ ] ) 
ax . set_yticks ( [ ] ) 
ax . set_zticks ( [ ] ) 
d = num 
zvalue = - 2 * weight / d + 1 
number_of_divisions = n_choose_k ( d , weight ) 
ax . plot ( [ xvalue ] , [ yvalue ] , [ zvalue ] , 
markerfacecolor = ( .5 , .5 , .5 ) , 
markeredgecolor = ( .5 , .5 , .5 ) , 
marker = 'o' , markersize = 10 , alpha = 1 ) 
colorstate = phase_to_color_wheel ( state [ i ] ) 
a = Arrow3D ( [ 0 , xvalue ] , [ 0 , yvalue ] , [ 0 , zvalue ] , 
mutation_scale = 20 , alpha = prob , arrowstyle = "-" , 
color = colorstate , lw = 10 ) 
ax . add_artist ( a ) 
~~ for weight in range ( d + 1 ) : 
~~~ theta = np . linspace ( - 2 * np . pi , 2 * np . pi , 100 ) 
z = - 2 * weight / d + 1 
r = np . sqrt ( 1 - z ** 2 ) 
x = r * np . cos ( theta ) 
y = r * np . sin ( theta ) 
ax . plot ( x , y , z , color = ( .5 , .5 , .5 ) ) 
~~ ax . plot ( [ 0 ] , [ 0 ] , [ 0 ] , markerfacecolor = ( .5 , .5 , .5 ) , 
markeredgecolor = ( .5 , .5 , .5 ) , marker = 'o' , markersize = 10 , 
alpha = 1 ) 
we [ prob_location ] = 0 
~~ ~~ plt . tight_layout ( ) 
~~ def plot_state ( quantum_state , method = 'city' , figsize = None ) : 
fig = None 
if method == 'city' : 
~~~ fig = plot_state_city ( rho , figsize = figsize ) 
~~~ fig = plot_state_paulivec ( rho , figsize = figsize ) 
~~~ fig = plot_state_qsphere ( rho , figsize = figsize ) 
~~~ plot_bloch_multivector ( rho , figsize = figsize ) 
~~~ fig = plot_state_hinton ( rho , figsize = figsize ) 
~~ def generate_facecolors ( x , y , z , dx , dy , dz , color ) : 
cuboid = np . array ( [ 
( 
( 0 , 0 , 0 ) , 
( 0 , 1 , 0 ) , 
( 1 , 1 , 0 ) , 
( 1 , 0 , 0 ) , 
( 0 , 0 , 1 ) , 
( 1 , 0 , 1 ) , 
( 1 , 1 , 1 ) , 
( 0 , 1 , 1 ) , 
] ) 
polys = np . empty ( x . shape + cuboid . shape ) 
for i , p , dp in [ ( 0 , x , dx ) , ( 1 , y , dy ) , ( 2 , z , dz ) ] : 
~~~ p = p [ ... , np . newaxis , np . newaxis ] 
dp = dp [ ... , np . newaxis , np . newaxis ] 
polys [ ... , i ] = p + dp * cuboid [ ... , i ] 
~~ polys = polys . reshape ( ( - 1 , ) + polys . shape [ 2 : ] ) 
facecolors = [ ] 
if len ( color ) == len ( x ) : 
~~~ for c in color : 
~~~ facecolors . extend ( [ c ] * 6 ) 
~~~ facecolors = list ( mcolors . to_rgba_array ( color ) ) 
if len ( facecolors ) < len ( x ) : 
~~~ facecolors *= ( 6 * len ( x ) ) 
~~ ~~ normals = _generate_normals ( polys ) 
return _shade_colors ( facecolors , normals ) 
~~ def _generate_normals ( polygons ) : 
if isinstance ( polygons , np . ndarray ) : 
~~~ n = polygons . shape [ - 2 ] 
i1 , i2 , i3 = 0 , n // 3 , 2 * n // 3 
v1 = polygons [ ... , i1 , : ] - polygons [ ... , i2 , : ] 
v2 = polygons [ ... , i2 , : ] - polygons [ ... , i3 , : ] 
~~~ v1 = np . empty ( ( len ( polygons ) , 3 ) ) 
v2 = np . empty ( ( len ( polygons ) , 3 ) ) 
for poly_i , ps in enumerate ( polygons ) : 
~~~ n = len ( ps ) 
v1 [ poly_i , : ] = ps [ i1 , : ] - ps [ i2 , : ] 
v2 [ poly_i , : ] = ps [ i2 , : ] - ps [ i3 , : ] 
~~ ~~ return np . cross ( v1 , v2 ) 
~~ def _shade_colors ( color , normals , lightsource = None ) : 
if lightsource is None : 
~~~ lightsource = LightSource ( azdeg = 225 , altdeg = 19.4712 ) 
~~ shade = np . array ( [ np . dot ( n / proj3d . mod ( n ) , lightsource . direction ) 
if proj3d . mod ( n ) else np . nan 
for n in normals ] ) 
mask = ~ np . isnan ( shade ) 
if mask . any ( ) : 
~~~ norm = Normalize ( min ( shade [ mask ] ) , max ( shade [ mask ] ) ) 
shade [ ~ mask ] = min ( shade [ mask ] ) 
color = mcolors . to_rgba_array ( color ) 
alpha = color [ : , 3 ] 
colors = ( 0.5 + norm ( shade ) [ : , np . newaxis ] * 0.5 ) * color 
colors [ : , 3 ] = alpha 
~~~ colors = np . asanyarray ( color ) . copy ( ) 
~~ return colors 
~~ def get_unique_backends ( ) : 
backends = IBMQ . backends ( ) 
unique_hardware_backends = [ ] 
unique_names = [ ] 
for back in backends : 
~~~ if back . name ( ) not in unique_names and not back . configuration ( ) . simulator : 
~~~ unique_hardware_backends . append ( back ) 
unique_names . append ( back . name ( ) ) 
~~ ~~ if not unique_hardware_backends : 
~~ return unique_hardware_backends 
~~ def backend_monitor ( backend ) : 
if not isinstance ( backend , IBMQBackend ) : 
~~ config = backend . configuration ( ) . to_dict ( ) 
status = backend . status ( ) . to_dict ( ) 
config_dict = { ** status , ** config } 
if not config [ 'simulator' ] : 
~~~ props = backend . properties ( ) . to_dict ( ) 
~~ print ( backend . name ( ) ) 
print ( '=' * len ( backend . name ( ) ) ) 
print ( 'Configuration' ) 
print ( '-' * 13 ) 
upper_list = [ 'n_qubits' , 'operational' , 
'status_msg' , 'pending_jobs' , 
'basis_gates' , 'local' , 'simulator' ] 
lower_list = list ( set ( config_dict . keys ( ) ) . difference ( upper_list ) ) 
lower_list . remove ( 'gates' ) 
for item in upper_list + lower_list : 
~~~ print ( offset + item + ':' , config_dict [ item ] ) 
~~ if config [ 'simulator' ] : 
~~ print ( ) 
print ( qubit_header ) 
print ( '-' * len ( qubit_header ) ) 
for qub in range ( len ( props [ 'qubits' ] ) ) : 
~~~ name = 'Q%s' % qub 
qubit_data = props [ 'qubits' ] [ qub ] 
gate_data = props [ 'gates' ] [ 3 * qub : 3 * qub + 3 ] 
t1_info = qubit_data [ 0 ] 
t2_info = qubit_data [ 1 ] 
freq_info = qubit_data [ 2 ] 
readout_info = qubit_data [ 3 ] 
U1 = str ( round ( gate_data [ 0 ] [ 'parameters' ] [ 0 ] [ 'value' ] , 5 ) ) 
U2 = str ( round ( gate_data [ 1 ] [ 'parameters' ] [ 0 ] [ 'value' ] , 5 ) ) 
U3 = str ( round ( gate_data [ 2 ] [ 'parameters' ] [ 0 ] [ 'value' ] , 5 ) ) 
readout_error = str ( round ( readout_info [ 'value' ] , 5 ) ) 
qstr = sep . join ( [ name , freq , T1 , T2 , U1 , U2 , U3 , readout_error ] ) 
print ( offset + qstr ) 
multi_qubit_gates = props [ 'gates' ] [ 3 * config [ 'n_qubits' ] : ] 
print ( multi_header ) 
print ( '-' * len ( multi_header ) ) 
for gate in multi_qubit_gates : 
~~~ name = gate [ 'name' ] 
ttype = gate [ 'gate' ] 
error = str ( round ( gate [ 'parameters' ] [ 0 ] [ 'value' ] , 5 ) ) 
mstr = sep . join ( [ name , ttype , error ] ) 
print ( offset + mstr ) 
~~ ~~ def backend_overview ( ) : 
unique_hardware_backends = get_unique_backends ( ) 
_backends = [ ] 
for idx , back in enumerate ( unique_hardware_backends ) : 
~~~ if back . status ( ) . operational : 
~~~ _backends = [ back ] + _backends 
~~~ _backends = _backends + [ back ] 
~~ ~~ stati = [ back . status ( ) for back in _backends ] 
idx = list ( range ( len ( _backends ) ) ) 
pending = [ s . pending_jobs for s in stati ] 
_ , least_idx = zip ( * sorted ( zip ( pending , idx ) ) ) 
for ind in least_idx : 
~~~ if stati [ ind ] . operational : 
~~~ least_pending_idx = ind 
~~ ~~ num_rows = math . ceil ( len ( _backends ) / 3 ) 
num_backends = len ( _backends ) 
for _ in range ( num_rows ) : 
~~~ max_len = 0 
str_list = [ '' ] * 8 
for idx in range ( 3 ) : 
config = _backends [ count ] . configuration ( ) . to_dict ( ) 
props = _backends [ count ] . properties ( ) . to_dict ( ) 
n_qubits = config [ 'n_qubits' ] 
str_list [ 0 ] += _backends [ count ] . name ( ) 
str_list [ 1 ] += '-' * len ( _backends [ count ] . name ( ) ) 
for q in props [ 'qubits' ] ] ) / n_qubits , 1 ) 
count += 1 
if count == num_backends : 
~~ max_len = max ( [ len ( s ) for s in str_list ] ) 
~~ print ( "\\n" . join ( str_list ) ) 
print ( '\\n' * 2 ) 
~~ ~~ def op ( self ) : 
if 'type' not in self . data_dict or self . data_dict [ 'type' ] != 'op' : 
~~ return self . data_dict . get ( 'op' ) 
~~ def wire ( self ) : 
if self . data_dict [ 'type' ] not in [ 'in' , 'out' ] : 
~~ return self . data_dict . get ( 'wire' ) 
~~ def semantic_eq ( node1 , node2 ) : 
if 'barrier' == node1 . name == node2 . name : 
~~~ return set ( node1 . qargs ) == set ( node2 . qargs ) 
~~ return node1 . data_dict == node2 . data_dict 
~~ def constant ( duration : int , amp : complex , name : str = None ) -> SamplePulse : 
return _sampled_constant_pulse ( duration , amp , name = name ) 
~~ def zero ( duration : int , name : str = None ) -> SamplePulse : 
return _sampled_zero_pulse ( duration , name = name ) 
~~ def square ( duration : int , amp : complex , period : float = None , 
phase : float = 0 , name : str = None ) -> SamplePulse : 
if period is None : 
~~~ period = duration 
~~ return _sampled_square_pulse ( duration , amp , period , phase = phase , name = name ) 
~~ def sawtooth ( duration : int , amp : complex , period : float = None , 
~~ return _sampled_sawtooth_pulse ( duration , amp , period , phase = phase , name = name ) 
~~ def triangle ( duration : int , amp : complex , period : float = None , 
~~ return _sampled_triangle_pulse ( duration , amp , period , phase = phase , name = name ) 
~~ def cos ( duration : int , amp : complex , freq : float = None , 
~~~ freq = 1 / duration 
~~ return _sampled_cos_pulse ( duration , amp , freq , phase = phase , name = name ) 
~~ def sin ( duration : int , amp : complex , freq : float = None , 
~~ return _sampled_sin_pulse ( duration , amp , freq , phase = phase , name = name ) 
~~ def gaussian ( duration : int , amp : complex , sigma : float , name : str = None ) -> SamplePulse : 
center = duration / 2 
zeroed_width = duration + 2 
return _sampled_gaussian_pulse ( duration , amp , center , sigma , 
zeroed_width = zeroed_width , rescale_amp = True , name = name ) 
~~ def gaussian_deriv ( duration : int , amp : complex , sigma : float , name : str = None ) -> SamplePulse : 
return _sampled_gaussian_deriv_pulse ( duration , amp , center , sigma , name = name ) 
~~ def gaussian_square ( duration : int , amp : complex , sigma : float , 
risefall : int , name : str = None ) -> SamplePulse : 
width = duration - 2 * risefall 
return _sampled_gaussian_square_pulse ( duration , amp , center , width , sigma , 
zeroed_width = zeroed_width , name = name ) 
~~ def drag ( duration : int , amp : complex , sigma : float , beta : float , name : str = None ) -> SamplePulse : 
return _sampled_drag_pulse ( duration , amp , center , sigma , beta , 
diagonal_1q_gates = ( RZGate , ZGate , TGate , SGate , TdgGate , SdgGate , U1Gate ) 
diagonal_2q_gates = ( CzGate , CrzGate , Cu1Gate , RZZGate ) 
nodes_to_remove = set ( ) 
for measure in dag . op_nodes ( Measure ) : 
~~~ predecessor = dag . quantum_predecessors ( measure ) [ 0 ] 
if predecessor . type == 'op' and isinstance ( predecessor . op , diagonal_1q_gates ) : 
~~~ nodes_to_remove . add ( predecessor ) 
~~ if predecessor . type == 'op' and isinstance ( predecessor . op , diagonal_2q_gates ) : 
~~~ successors = dag . quantum_successors ( predecessor ) 
if all ( [ s . type == 'op' and isinstance ( s . op , Measure ) for s in successors ] ) : 
~~ ~~ ~~ for node_to_remove in nodes_to_remove : 
~~~ dag . remove_op_node ( node_to_remove ) 
~~ def plot_gate_map ( backend , figsize = None , 
plot_directed = False , 
label_qubits = True , 
qubit_size = 24 , 
line_width = 4 , 
font_size = 12 , 
qubit_color = None , 
line_color = None , 
font_color = 'w' ) : 
~~ if backend . configuration ( ) . simulator : 
~~ mpl_data = { } 
mpl_data [ 'ibmq_20_tokyo' ] = [ [ 0 , 0 ] , [ 0 , 1 ] , [ 0 , 2 ] , [ 0 , 3 ] , [ 0 , 4 ] , 
[ 1 , 0 ] , [ 1 , 1 ] , [ 1 , 2 ] , [ 1 , 3 ] , [ 1 , 4 ] , 
[ 2 , 0 ] , [ 2 , 1 ] , [ 2 , 2 ] , [ 2 , 3 ] , [ 2 , 4 ] , 
[ 3 , 0 ] , [ 3 , 1 ] , [ 3 , 2 ] , [ 3 , 3 ] , [ 3 , 4 ] ] 
mpl_data [ 'ibmq_poughkeepsie' ] = mpl_data [ 'ibmq_20_tokyo' ] 
mpl_data [ 'ibmq_16_melbourne' ] = [ [ 0 , 0 ] , [ 0 , 1 ] , [ 0 , 2 ] , [ 0 , 3 ] , [ 0 , 4 ] , 
[ 0 , 5 ] , [ 0 , 6 ] , [ 1 , 7 ] , [ 1 , 6 ] , [ 1 , 5 ] , 
[ 1 , 4 ] , [ 1 , 3 ] , [ 1 , 2 ] , [ 1 , 1 ] ] 
mpl_data [ 'ibmq_16_rueschlikon' ] = [ [ 1 , 0 ] , [ 0 , 0 ] , [ 0 , 1 ] , [ 0 , 2 ] , [ 0 , 3 ] , 
[ 0 , 4 ] , [ 0 , 5 ] , [ 0 , 6 ] , [ 0 , 7 ] , [ 1 , 7 ] , 
[ 1 , 6 ] , [ 1 , 5 ] , [ 1 , 4 ] , [ 1 , 3 ] , [ 1 , 2 ] , [ 1 , 1 ] ] 
mpl_data [ 'ibmq_5_tenerife' ] = [ [ 1 , 0 ] , [ 0 , 1 ] , [ 1 , 1 ] , [ 1 , 2 ] , [ 2 , 1 ] ] 
mpl_data [ 'ibmq_5_yorktown' ] = mpl_data [ 'ibmq_5_tenerife' ] 
config = backend . configuration ( ) 
name = config . backend_name 
cmap = config . coupling_map 
dep_names = { 'ibmqx5' : 'ibmq_16_rueschlikon' , 
'ibmqx4' : 'ibmq_5_tenerife' , 
'ibmqx2' : 'ibmq_5_yorktown' } 
if name in dep_names . keys ( ) : 
~~~ name = dep_names [ name ] 
~~ if name in mpl_data . keys ( ) : 
~~~ grid_data = mpl_data [ name ] 
ax . axis ( 'off' ) 
~~ x_max = max ( [ d [ 1 ] for d in grid_data ] ) 
y_max = max ( [ d [ 0 ] for d in grid_data ] ) 
max_dim = max ( x_max , y_max ) 
~~~ if x_max / max_dim > 0.33 and y_max / max_dim > 0.33 : 
~~~ figsize = ( 9 , 3 ) 
fig . tight_layout ( ) 
if qubit_color is None : 
~~~ qubit_color = [ '#648fff' ] * config . n_qubits 
~~ if line_color is None : 
~~~ line_color = [ '#648fff' ] * len ( cmap ) 
~~ for ind , edge in enumerate ( cmap ) : 
~~~ is_symmetric = False 
if edge [ : : - 1 ] in cmap : 
~~~ is_symmetric = True 
~~ y_start = grid_data [ edge [ 0 ] ] [ 0 ] 
x_start = grid_data [ edge [ 0 ] ] [ 1 ] 
y_end = grid_data [ edge [ 1 ] ] [ 0 ] 
x_end = grid_data [ edge [ 1 ] ] [ 1 ] 
if is_symmetric : 
~~~ if y_start == y_end : 
~~~ x_end = ( x_end - x_start ) / 2 + x_start 
~~ elif x_start == x_end : 
~~~ y_end = ( y_end - y_start ) / 2 + y_start 
y_end = ( y_end - y_start ) / 2 + y_start 
~~ ~~ ax . add_artist ( plt . Line2D ( [ x_start , x_end ] , [ - y_start , - y_end ] , 
color = line_color [ ind ] , linewidth = line_width , 
zorder = 0 ) ) 
if plot_directed : 
~~~ x_arrow = x_start + dx * 0.95 
y_arrow = - y_start - dy * 0.95 
dx_arrow = dx * 0.01 
dy_arrow = - dy * 0.01 
head_width = 0.15 
~~~ x_arrow = x_start + dx * 0.5 
y_arrow = - y_start - dy * 0.5 
dx_arrow = dx * 0.2 
dy_arrow = - dy * 0.2 
head_width = 0.2 
~~ ax . add_patch ( mpatches . FancyArrow ( x_arrow , 
y_arrow , 
dx_arrow , 
dy_arrow , 
head_width = head_width , 
length_includes_head = True , 
edgecolor = None , 
linewidth = 0 , 
facecolor = line_color [ ind ] , 
zorder = 1 ) ) 
~~ ~~ for var , idx in enumerate ( grid_data ) : 
~~~ _idx = [ idx [ 1 ] , - idx [ 0 ] ] 
width = _GraphDist ( qubit_size , ax , True ) 
height = _GraphDist ( qubit_size , ax , False ) 
ax . add_artist ( mpatches . Ellipse ( 
_idx , width , height , color = qubit_color [ var ] , zorder = 1 ) ) 
if label_qubits : 
~~~ ax . text ( * _idx , s = str ( var ) , 
horizontalalignment = 'center' , 
verticalalignment = 'center' , 
color = font_color , size = font_size , weight = 'bold' ) 
~~ ~~ ax . set_xlim ( [ - 1 , x_max + 1 ] ) 
ax . set_ylim ( [ - ( y_max + 1 ) , 1 ] ) 
~~ def dist_real ( self ) : 
( 0 , 0 ) ) 
( 1 , 1 ) ) 
value = x1 - x0 if self . x else y1 - y0 
~~ def dist_abs ( self ) : 
bounds = self . ax . get_xlim ( ) if self . x else self . ax . get_ylim ( ) 
return bounds [ 0 ] - bounds [ 1 ] 
~~ def to_string ( self , indent ) : 
print ( ind , 'qreg' ) 
self . children [ 0 ] . to_string ( indent + 3 ) 
~~ def _verify_backends ( self ) : 
ret = OrderedDict ( ) 
for backend_cls in SIMULATORS : 
~~~ backend_instance = self . _get_backend_instance ( backend_cls ) 
backend_name = backend_instance . name ( ) 
ret [ backend_name ] = backend_instance 
~~ except QiskitError as err : 
backend_cls , str ( err ) ) 
~~ def _get_backend_instance ( self , backend_cls ) : 
~~~ backend_instance = backend_cls ( provider = self ) 
~~ except Exception as err : 
( backend_cls , err ) ) 
~~ return backend_instance 
~~ def qubits ( self ) : 
return [ ( v , i ) for k , v in self . qregs . items ( ) for i in range ( v . size ) ] 
~~ def clbits ( self ) : 
return [ ( v , i ) for k , v in self . cregs . items ( ) for i in range ( v . size ) ] 
~~ def rename_register ( self , regname , newname ) : 
if regname == newname : 
~~ if newname in self . qregs or newname in self . cregs : 
~~ if regname not in self . qregs and regname not in self . cregs : 
~~ if regname in self . qregs : 
~~~ reg = self . qregs [ regname ] 
reg . name = newname 
self . qregs [ newname ] = reg 
self . qregs . pop ( regname , None ) 
~~ if regname in self . cregs : 
~~~ reg = self . cregs [ regname ] 
~~ for node in self . _multi_graph . nodes ( ) : 
~~~ if node . type == "in" or node . type == "out" : 
~~~ if node . name and regname in node . name : 
~~~ node . name = newname 
~~ ~~ elif node . type == "op" : 
~~~ qa = [ ] 
for a in node . qargs : 
~~~ if a [ 0 ] == regname : 
~~~ a = ( newname , a [ 1 ] ) 
~~ qa . append ( a ) 
~~ node . qargs = qa 
ca = [ ] 
for a in node . cargs : 
~~ ca . append ( a ) 
~~ node . cargs = ca 
if node . condition is not None : 
~~~ if node . condition [ 0 ] == regname : 
~~~ node . condition = ( newname , node . condition [ 1 ] ) 
~~ ~~ ~~ ~~ for _ , _ , edge_data in self . _multi_graph . edges ( data = True ) : 
~~~ if regname in edge_data [ 'name' ] : 
~~~ edge_data [ 'name' ] = re . sub ( regname , newname , edge_data [ 'name' ] ) 
~~ ~~ ~~ def remove_all_ops_named ( self , opname ) : 
for n in self . named_nodes ( opname ) : 
~~~ self . remove_op_node ( n ) 
~~ ~~ def add_qreg ( self , qreg ) : 
if not isinstance ( qreg , QuantumRegister ) : 
~~ if qreg . name in self . qregs : 
~~ self . qregs [ qreg . name ] = qreg 
for j in range ( qreg . size ) : 
~~~ self . _add_wire ( ( qreg , j ) ) 
~~ ~~ def add_creg ( self , creg ) : 
if not isinstance ( creg , ClassicalRegister ) : 
~~ if creg . name in self . cregs : 
~~ self . cregs [ creg . name ] = creg 
for j in range ( creg . size ) : 
~~~ self . _add_wire ( ( creg , j ) ) 
~~ ~~ def _add_wire ( self , wire ) : 
if wire not in self . wires : 
~~~ self . wires . append ( wire ) 
self . _max_node_id += 1 
input_map_wire = self . input_map [ wire ] = self . _max_node_id 
output_map_wire = self . _max_node_id 
wire_name = "%s[%s]" % ( wire [ 0 ] . name , wire [ 1 ] ) 
inp_node = DAGNode ( data_dict = { 'type' : 'in' , 'name' : wire_name , 'wire' : wire } , 
nid = input_map_wire ) 
outp_node = DAGNode ( data_dict = { 'type' : 'out' , 'name' : wire_name , 'wire' : wire } , 
nid = output_map_wire ) 
self . _id_to_node [ input_map_wire ] = inp_node 
self . _id_to_node [ output_map_wire ] = outp_node 
self . input_map [ wire ] = inp_node 
self . output_map [ wire ] = outp_node 
self . _multi_graph . add_node ( inp_node ) 
self . _multi_graph . add_node ( outp_node ) 
self . _multi_graph . add_edge ( inp_node , 
outp_node ) 
self . _multi_graph . adj [ inp_node ] [ outp_node ] [ 0 ] [ "name" ] = "%s[%s]" % ( wire [ 0 ] . name , wire [ 1 ] ) 
self . _multi_graph . adj [ inp_node ] [ outp_node ] [ 0 ] [ "wire" ] = wire 
~~ ~~ def _check_condition ( self , name , condition ) : 
if condition is not None and condition [ 0 ] . name not in self . cregs : 
~~ ~~ def _check_bits ( self , args , amap ) : 
for wire in args : 
~~~ if wire not in amap : 
( wire [ 0 ] . name , wire [ 1 ] ) ) 
~~ ~~ ~~ def _bits_in_condition ( self , cond ) : 
all_bits = [ ] 
if cond is not None : 
~~~ all_bits . extend ( [ ( cond [ 0 ] , j ) for j in range ( self . cregs [ cond [ 0 ] . name ] . size ) ] ) 
~~ return all_bits 
~~ def _add_op_node ( self , op , qargs , cargs , condition = None ) : 
node_properties = { 
"type" : "op" , 
"op" : op , 
"name" : op . name , 
"qargs" : qargs , 
"cargs" : cargs , 
"condition" : condition 
new_node = DAGNode ( data_dict = node_properties , nid = self . _max_node_id ) 
self . _multi_graph . add_node ( new_node ) 
self . _id_to_node [ self . _max_node_id ] = new_node 
~~ def apply_operation_back ( self , op , qargs = None , cargs = None , condition = None ) : 
all_cbits = self . _bits_in_condition ( condition ) 
all_cbits . extend ( cargs ) 
self . _check_condition ( op . name , condition ) 
self . _check_bits ( qargs , self . output_map ) 
self . _check_bits ( all_cbits , self . output_map ) 
self . _add_op_node ( op , qargs , cargs , condition ) 
al = [ qargs , all_cbits ] 
for q in itertools . chain ( * al ) : 
~~~ ie = list ( self . _multi_graph . predecessors ( self . output_map [ q ] ) ) 
if len ( ie ) != 1 : 
~~ self . _multi_graph . add_edge ( ie [ 0 ] , self . _id_to_node [ self . _max_node_id ] , 
name = "%s[%s]" % ( q [ 0 ] . name , q [ 1 ] ) , wire = q ) 
self . _multi_graph . remove_edge ( ie [ 0 ] , self . output_map [ q ] ) 
self . _multi_graph . add_edge ( self . _id_to_node [ self . _max_node_id ] , self . output_map [ q ] , 
~~ return self . _id_to_node [ self . _max_node_id ] 
~~ def _check_edgemap_registers ( self , edge_map , keyregs , valregs , valreg = True ) : 
add_regs = set ( ) 
reg_frag_chk = { } 
for v in keyregs . values ( ) : 
~~~ reg_frag_chk [ v ] = { j : False for j in range ( len ( v ) ) } 
~~ for k in edge_map . keys ( ) : 
~~~ if k [ 0 ] . name in keyregs : 
~~~ reg_frag_chk [ k [ 0 ] ] [ k [ 1 ] ] = True 
~~ ~~ for k , v in reg_frag_chk . items ( ) : 
~~~ s = set ( v . values ( ) ) 
if len ( s ) == 2 : 
~~ elif s == set ( [ False ] ) : 
~~~ if k in self . qregs . values ( ) or k in self . cregs . values ( ) : 
~~~ add_regs . add ( k ) 
~~~ if valreg : 
~~~ if not edge_map [ ( k , 0 ) ] [ 0 ] . name in valregs : 
~~~ size = max ( map ( lambda x : x [ 1 ] , 
filter ( lambda x : x [ 0 ] == edge_map [ ( k , 0 ) ] [ 0 ] , 
edge_map . values ( ) ) ) ) 
qreg = QuantumRegister ( size + 1 , edge_map [ ( k , 0 ) ] [ 0 ] . name ) 
add_regs . add ( qreg ) 
~~ ~~ ~~ ~~ return add_regs 
~~ def _check_wiremap_validity ( self , wire_map , keymap , valmap ) : 
for k , v in wire_map . items ( ) : 
~~~ kname = "%s[%d]" % ( k [ 0 ] . name , k [ 1 ] ) 
vname = "%s[%d]" % ( v [ 0 ] . name , v [ 1 ] ) 
if k not in keymap : 
~~ if v not in valmap : 
~~ if type ( k ) is not type ( v ) : 
( kname , vname ) ) 
~~ ~~ ~~ def _map_condition ( self , wire_map , condition ) : 
if condition is None : 
~~~ new_condition = None 
~~~ bit0 = ( condition [ 0 ] , 0 ) 
new_condition = ( wire_map . get ( bit0 , bit0 ) [ 0 ] , condition [ 1 ] ) 
~~ return new_condition 
~~ def extend_back ( self , dag , edge_map = None ) : 
edge_map = edge_map or { } 
~~~ if qreg . name not in self . qregs : 
~~~ self . add_qreg ( QuantumRegister ( qreg . size , qreg . name ) ) 
~~ edge_map . update ( [ ( qbit , qbit ) for qbit in qreg if qbit not in edge_map ] ) 
~~~ if creg . name not in self . cregs : 
~~~ self . add_creg ( ClassicalRegister ( creg . size , creg . name ) ) 
~~ edge_map . update ( [ ( cbit , cbit ) for cbit in creg if cbit not in edge_map ] ) 
~~ self . compose_back ( dag , edge_map ) 
~~ def compose_back ( self , input_circuit , edge_map = None ) : 
if len ( set ( edge_map . values ( ) ) ) != len ( edge_map ) : 
~~ add_qregs = self . _check_edgemap_registers ( edge_map , 
input_circuit . qregs , 
self . qregs ) 
for qreg in add_qregs : 
~~~ self . add_qreg ( qreg ) 
~~ add_cregs = self . _check_edgemap_registers ( edge_map , 
input_circuit . cregs , 
self . cregs ) 
for creg in add_cregs : 
~~~ self . add_creg ( creg ) 
~~ self . _check_wiremap_validity ( edge_map , input_circuit . input_map , 
self . output_map ) 
for nd in input_circuit . topological_nodes ( ) : 
~~~ if nd . type == "in" : 
~~~ m_wire = edge_map . get ( nd . wire , nd . wire ) 
if m_wire not in self . output_map : 
~~ if nd . wire not in input_circuit . wires : 
% ( nd . wire [ 0 ] . name , nd . wire [ 1 ] ) ) 
~~ ~~ elif nd . type == "out" : 
~~ elif nd . type == "op" : 
~~~ condition = self . _map_condition ( edge_map , nd . condition ) 
self . _check_condition ( nd . name , condition ) 
m_qargs = list ( map ( lambda x : edge_map . get ( x , x ) , nd . qargs ) ) 
m_cargs = list ( map ( lambda x : edge_map . get ( x , x ) , nd . cargs ) ) 
self . apply_operation_back ( nd . op , m_qargs , m_cargs , condition ) 
~~ ~~ ~~ def depth ( self ) : 
if not nx . is_directed_acyclic_graph ( self . _multi_graph ) : 
~~ depth = nx . dag_longest_path_length ( self . _multi_graph ) - 1 
return depth if depth != - 1 else 0 
~~ def _check_wires_list ( self , wires , node ) : 
if len ( set ( wires ) ) != len ( wires ) : 
~~ wire_tot = len ( node . qargs ) + len ( node . cargs ) 
~~~ wire_tot += node . condition [ 0 ] . size 
~~ if len ( wires ) != wire_tot : 
% ( wire_tot , len ( wires ) ) ) 
~~ ~~ def _make_pred_succ_maps ( self , node ) : 
pred_map = { e [ 2 ] [ 'wire' ] : e [ 0 ] for e in 
self . _multi_graph . in_edges ( nbunch = node , data = True ) } 
succ_map = { e [ 2 ] [ 'wire' ] : e [ 1 ] for e in 
self . _multi_graph . out_edges ( nbunch = node , data = True ) } 
return pred_map , succ_map 
~~ def _full_pred_succ_maps ( self , pred_map , succ_map , input_circuit , 
wire_map ) : 
full_pred_map = { } 
full_succ_map = { } 
for w in input_circuit . input_map : 
~~~ if w in wire_map : 
~~~ full_pred_map [ wire_map [ w ] ] = pred_map [ wire_map [ w ] ] 
full_succ_map [ wire_map [ w ] ] = succ_map [ wire_map [ w ] ] 
~~~ full_succ_map [ w ] = self . output_map [ w ] 
full_pred_map [ w ] = self . _multi_graph . predecessors ( 
self . output_map [ w ] ) [ 0 ] 
if len ( list ( self . _multi_graph . predecessors ( self . output_map [ w ] ) ) ) != 1 : 
~~ ~~ ~~ return full_pred_map , full_succ_map 
~~ def topological_nodes ( self ) : 
return nx . lexicographical_topological_sort ( self . _multi_graph , 
key = lambda x : str ( x . qargs ) ) 
~~ def substitute_node_with_dag ( self , node , input_dag , wires = None ) : 
if isinstance ( node , int ) : 
DeprecationWarning , 2 ) 
node = self . _id_to_node [ node ] 
~~ condition = node . condition 
if condition : 
~~~ input_dag . add_creg ( condition [ 0 ] ) 
to_replay = [ ] 
for sorted_node in input_dag . topological_nodes ( ) : 
~~~ if sorted_node . type == "op" : 
~~~ sorted_node . op . control = condition 
to_replay . append ( sorted_node ) 
~~ ~~ for input_node in input_dag . op_nodes ( ) : 
~~~ input_dag . remove_op_node ( input_node ) 
~~ for replay_node in to_replay : 
~~~ input_dag . apply_operation_back ( replay_node . op , replay_node . qargs , 
replay_node . cargs , condition = condition ) 
~~ ~~ if wires is None : 
~~~ qwires = [ w for w in input_dag . wires if isinstance ( w [ 0 ] , QuantumRegister ) ] 
cwires = [ w for w in input_dag . wires if isinstance ( w [ 0 ] , ClassicalRegister ) ] 
wires = qwires + cwires 
~~ self . _check_wires_list ( wires , node ) 
proxy_map = { w : QuantumRegister ( 1 , 'proxy' ) for w in wires } 
add_qregs = self . _check_edgemap_registers ( proxy_map , 
input_dag . qregs , 
{ } , False ) 
~~ add_cregs = self . _check_edgemap_registers ( proxy_map , 
input_dag . cregs , 
~~ if node . type != "op" : 
% node . type ) 
~~ condition_bit_list = self . _bits_in_condition ( node . condition ) 
wire_map = { k : v for k , v in zip ( wires , 
[ i for s in [ node . qargs , 
node . cargs , 
condition_bit_list ] 
for i in s ] ) } 
self . _check_wiremap_validity ( wire_map , wires , self . input_map ) 
pred_map , succ_map = self . _make_pred_succ_maps ( node ) 
full_pred_map , full_succ_map = self . _full_pred_succ_maps ( pred_map , succ_map , 
input_dag , wire_map ) 
self . _multi_graph . remove_node ( node ) 
for sorted_node in input_dag . topological_op_nodes ( ) : 
~~~ condition = self . _map_condition ( wire_map , sorted_node . condition ) 
m_qargs = list ( map ( lambda x : wire_map . get ( x , x ) , 
sorted_node . qargs ) ) 
m_cargs = list ( map ( lambda x : wire_map . get ( x , x ) , 
sorted_node . cargs ) ) 
self . _add_op_node ( sorted_node . op , m_qargs , m_cargs , condition ) 
all_cbits . extend ( m_cargs ) 
al = [ m_qargs , all_cbits ] 
~~~ self . _multi_graph . add_edge ( full_pred_map [ q ] , 
self . _id_to_node [ self . _max_node_id ] , 
name = "%s[%s]" % ( q [ 0 ] . name , q [ 1 ] ) , 
wire = q ) 
full_pred_map [ q ] = self . _id_to_node [ self . _max_node_id ] 
~~ ~~ for w in full_pred_map : 
~~~ self . _multi_graph . add_edge ( full_pred_map [ w ] , 
full_succ_map [ w ] , 
name = "%s[%s]" % ( w [ 0 ] . name , w [ 1 ] ) , 
wire = w ) 
o_pred = list ( self . _multi_graph . predecessors ( self . output_map [ w ] ) ) 
if len ( o_pred ) > 1 : 
~~~ if len ( o_pred ) != 2 : 
~~ p = [ x for x in o_pred if x != full_pred_map [ w ] ] 
if len ( p ) != 1 : 
~~ self . _multi_graph . remove_edge ( p [ 0 ] , self . output_map [ w ] ) 
~~ ~~ ~~ def edges ( self , nodes = None ) : 
for source_node , dest_node , edge_data in self . _multi_graph . edges ( nodes , data = True ) : 
~~~ yield source_node , dest_node , edge_data 
~~ ~~ def get_op_nodes ( self , op = None , data = False ) : 
if data : 
~~ nodes = [ ] 
for node in self . _multi_graph . nodes ( ) : 
~~~ if node . type == "op" : 
~~~ if op is None or isinstance ( node . op , op ) : 
~~~ nodes . append ( ( node . _node_id , node . data_dict ) ) 
~~ ~~ ~~ if not data : 
~~~ nodes = [ n [ 0 ] for n in nodes ] 
~~ return nodes 
~~ def op_nodes ( self , op = None ) : 
nodes = [ ] 
~~~ nodes . append ( node ) 
~~ ~~ ~~ return nodes 
~~ def get_gate_nodes ( self , data = False ) : 
for node in self . op_nodes ( ) : 
~~~ if isinstance ( node . op , Gate ) : 
~~~ nodes . append ( ( node . _node_id , node ) ) 
~~ ~~ if not data : 
~~ def gate_nodes ( self ) : 
~~ ~~ return nodes 
~~ def get_named_nodes ( self , * names ) : 
named_nodes = [ ] 
~~~ if node . type == 'op' and node . op . name in names : 
~~~ named_nodes . append ( node . _node_id ) 
~~ ~~ return named_nodes 
~~ def named_nodes ( self , * names ) : 
~~~ named_nodes . append ( node ) 
~~ def get_2q_nodes ( self ) : 
two_q_nodes = [ ] 
~~~ if node . type == 'op' and len ( node . qargs ) == 2 : 
~~~ two_q_nodes . append ( node . data_dict ) 
~~ ~~ return two_q_nodes 
~~ def twoQ_gates ( self ) : 
two_q_gates = [ ] 
for node in self . gate_nodes ( ) : 
~~~ if len ( node . qargs ) == 2 : 
~~~ two_q_gates . append ( node ) 
~~ ~~ return two_q_gates 
~~ def get_3q_or_more_nodes ( self ) : 
three_q_nodes = [ ] 
~~~ if node . type == 'op' and len ( node . qargs ) >= 3 : 
~~~ three_q_nodes . append ( ( node . _node_id , node . data_dict ) ) 
~~ ~~ return three_q_nodes 
~~ def threeQ_or_more_gates ( self ) : 
three_q_gates = [ ] 
~~~ if len ( node . qargs ) >= 3 : 
~~~ three_q_gates . append ( node ) 
~~ ~~ return three_q_gates 
~~ def predecessors ( self , node ) : 
~~ return self . _multi_graph . predecessors ( node ) 
~~ def quantum_predecessors ( self , node ) : 
predecessors = [ ] 
for predecessor in self . predecessors ( node ) : 
~~~ if isinstance ( self . _multi_graph . get_edge_data ( predecessor , node , key = 0 ) [ 'wire' ] [ 0 ] , 
QuantumRegister ) : 
~~~ predecessors . append ( predecessor ) 
~~ ~~ return predecessors 
~~ def ancestors ( self , node ) : 
~~ return nx . ancestors ( self . _multi_graph , node ) 
~~ def quantum_successors ( self , node ) : 
~~ successors = [ ] 
for successor in self . successors ( node ) : 
~~~ if isinstance ( self . _multi_graph . get_edge_data ( 
node , successor , key = 0 ) [ 'wire' ] [ 0 ] , 
~~~ successors . append ( successor ) 
~~ ~~ return successors 
~~ def remove_op_node ( self , node ) : 
~~ if node . type != 'op' : 
~~~ raise DAGCircuitError ( \ 
~~ pred_map , succ_map = self . _make_pred_succ_maps ( node ) 
for w in pred_map . keys ( ) : 
~~~ self . _multi_graph . add_edge ( pred_map [ w ] , succ_map [ w ] , 
name = "%s[%s]" % ( w [ 0 ] . name , w [ 1 ] ) , wire = w ) 
~~ ~~ def remove_ancestors_of ( self , node ) : 
~~ anc = nx . ancestors ( self . _multi_graph , node ) 
for anc_node in anc : 
~~~ if anc_node . type == "op" : 
~~~ self . remove_op_node ( anc_node ) 
~~ ~~ ~~ def remove_descendants_of ( self , node ) : 
~~ desc = nx . descendants ( self . _multi_graph , node ) 
for desc_node in desc : 
~~~ if desc_node . type == "op" : 
~~~ self . remove_op_node ( desc_node ) 
~~ ~~ ~~ def remove_nonancestors_of ( self , node ) : 
comp = list ( set ( self . _multi_graph . nodes ( ) ) - set ( anc ) ) 
for n in comp : 
~~~ if n . type == "op" : 
~~ ~~ ~~ def remove_nondescendants_of ( self , node ) : 
~~ dec = nx . descendants ( self . _multi_graph , node ) 
comp = list ( set ( self . _multi_graph . nodes ( ) ) - set ( dec ) ) 
~~ ~~ ~~ def layers ( self ) : 
graph_layers = self . multigraph_layers ( ) 
~~ except StopIteration : 
~~ def add_nodes_from ( layer , nodes ) : 
layer . _multi_graph . add_nodes_from ( nodes ) 
~~ for graph_layer in graph_layers : 
~~~ op_nodes = [ node for node in graph_layer if node . type == "op" ] 
if not op_nodes : 
~~ new_layer = DAGCircuit ( ) 
new_layer . name = self . name 
for creg in self . cregs . values ( ) : 
~~~ new_layer . add_creg ( creg ) 
~~ for qreg in self . qregs . values ( ) : 
~~~ new_layer . add_qreg ( qreg ) 
~~ add_nodes_from ( new_layer , self . input_map . values ( ) ) 
add_nodes_from ( new_layer , self . output_map . values ( ) ) 
add_nodes_from ( new_layer , op_nodes ) 
support_list = [ 
op_node . qargs 
for op_node in op_nodes 
if op_node . name not in { "barrier" , "snapshot" , "save" , "load" , "noise" } 
wires = { self . input_map [ wire ] : self . output_map [ wire ] 
for wire in self . wires } 
for op_node in op_nodes : 
~~~ args = self . _bits_in_condition ( op_node . condition ) + op_node . cargs + op_node . qargs 
arg_ids = ( self . input_map [ ( arg [ 0 ] , arg [ 1 ] ) ] for arg in args ) 
for arg_id in arg_ids : 
~~~ wires [ arg_id ] , wires [ op_node ] = op_node , wires [ arg_id ] 
~~ ~~ new_layer . _multi_graph . add_edges_from ( wires . items ( ) ) 
yield { "graph" : new_layer , "partition" : support_list } 
~~ ~~ def serial_layers ( self ) : 
for next_node in self . topological_op_nodes ( ) : 
~~~ new_layer = DAGCircuit ( ) 
for qreg in self . qregs . values ( ) : 
~~ for creg in self . cregs . values ( ) : 
~~ support_list = [ ] 
op = copy . copy ( next_node . op ) 
qa = copy . copy ( next_node . qargs ) 
ca = copy . copy ( next_node . cargs ) 
co = copy . copy ( next_node . condition ) 
_ = self . _bits_in_condition ( co ) 
new_layer . apply_operation_back ( op , qa , ca , co ) 
if next_node . name not in [ "barrier" , 
"snapshot" , "save" , "load" , "noise" ] : 
~~~ support_list . append ( list ( qa ) ) 
~~ l_dict = { "graph" : new_layer , "partition" : support_list } 
yield l_dict 
~~ ~~ def multigraph_layers ( self ) : 
cur_layer = [ node for node in self . input_map . values ( ) ] 
yield cur_layer 
next_layer = [ ] 
while cur_layer : 
~~~ for node in cur_layer : 
~~~ for successor in self . _multi_graph . successors ( node ) : 
~~~ multiplicity = self . _multi_graph . number_of_edges ( node , successor ) 
if successor in predecessor_count : 
~~~ predecessor_count [ successor ] -= multiplicity 
~~~ predecessor_count [ successor ] = self . _multi_graph . in_degree ( successor ) - multiplicity 
~~ if predecessor_count [ successor ] == 0 : 
~~~ next_layer . append ( successor ) 
del predecessor_count [ successor ] 
~~ ~~ ~~ yield next_layer 
cur_layer = next_layer 
~~ ~~ def collect_runs ( self , namelist ) : 
group_list = [ ] 
topo_ops = list ( self . topological_op_nodes ( ) ) 
nodes_seen = dict ( zip ( topo_ops , [ False ] * len ( topo_ops ) ) ) 
for node in topo_ops : 
~~~ if node . name in namelist and node . condition is None and not nodes_seen [ node ] : 
~~~ group = [ node ] 
nodes_seen [ node ] = True 
s = list ( self . _multi_graph . successors ( node ) ) 
while len ( s ) == 1 and s [ 0 ] . type == "op" and s [ 0 ] . name in namelist : 
~~~ group . append ( s [ 0 ] ) 
nodes_seen [ s [ 0 ] ] = True 
s = list ( self . _multi_graph . successors ( s [ 0 ] ) ) 
~~ if len ( group ) >= 1 : 
~~~ group_list . append ( tuple ( group ) ) 
~~ ~~ ~~ return set ( group_list ) 
~~ def nodes_on_wire ( self , wire , only_ops = False ) : 
current_node = self . input_map . get ( wire , None ) 
if not current_node : 
% str ( wire ) ) 
~~ more_nodes = True 
while more_nodes : 
~~~ more_nodes = False 
if current_node . type == 'op' or not only_ops : 
~~~ yield current_node 
~~ for node , edges in self . _multi_graph . adj [ current_node ] . items ( ) : 
~~~ if any ( wire == edge [ 'wire' ] for edge in edges . values ( ) ) : 
~~~ current_node = node 
more_nodes = True 
~~ ~~ ~~ ~~ def count_ops ( self ) : 
op_dict = { } 
for node in self . topological_op_nodes ( ) : 
~~~ name = node . name 
if name not in op_dict : 
~~~ op_dict [ name ] = 1 
~~~ op_dict [ name ] += 1 
~~ ~~ return op_dict 
~~ def properties ( self ) : 
summary = { "size" : self . size ( ) , 
"depth" : self . depth ( ) , 
"width" : self . width ( ) , 
"bits" : self . num_cbits ( ) , 
"factors" : self . num_tensor_factors ( ) , 
"operations" : self . count_ops ( ) } 
return summary 
~~ def tomography_basis ( basis , prep_fun = None , meas_fun = None ) : 
ret = TomographyBasis ( basis ) 
ret . prep_fun = prep_fun 
ret . meas_fun = meas_fun 
~~ def __pauli_prep_gates ( circuit , qreg , op ) : 
bas , proj = op 
if bas not in [ 'X' , 'Y' , 'Z' ] : 
~~~ raise QiskitError ( "There\ 
"preparation" ) 
~~ if bas == "X" : 
~~~ if proj == 1 : 
~~ ~~ elif bas == "Y" : 
~~ ~~ elif bas == "Z" and proj == 1 : 
~~~ circuit . u3 ( np . pi , 0. , np . pi , qreg ) 
~~ ~~ def __pauli_meas_gates ( circuit , qreg , op ) : 
if op not in [ 'X' , 'Y' , 'Z' ] : 
"measurement" ) 
~~ if op == "X" : 
~~ elif op == "Y" : 
~~~ circuit . u2 ( 0. , 0.5 * np . pi , qreg ) 
~~ ~~ def __sic_prep_gates ( circuit , qreg , op ) : 
if bas != 'S' : 
~~ theta = - 2 * np . arctan ( np . sqrt ( 2 ) ) 
if proj == 1 : 
~~~ circuit . u3 ( theta , np . pi , 0.0 , qreg ) 
~~ elif proj == 2 : 
~~~ circuit . u3 ( theta , np . pi / 3 , 0.0 , qreg ) 
~~ elif proj == 3 : 
~~~ circuit . u3 ( theta , - np . pi / 3 , 0.0 , qreg ) 
~~ ~~ def tomography_set ( meas_qubits , 
meas_basis = 'Pauli' , 
prep_qubits = None , 
prep_basis = None ) : 
if not isinstance ( meas_qubits , list ) : 
~~ num_of_qubits = len ( meas_qubits ) 
if prep_qubits is None : 
~~~ prep_qubits = meas_qubits 
~~ if not isinstance ( prep_qubits , list ) : 
~~ if len ( prep_qubits ) != len ( meas_qubits ) : 
~~ if isinstance ( meas_basis , str ) : 
~~~ if meas_basis . lower ( ) == 'pauli' : 
~~~ meas_basis = PAULI_BASIS 
~~ ~~ if isinstance ( prep_basis , str ) : 
~~~ if prep_basis . lower ( ) == 'pauli' : 
~~~ prep_basis = PAULI_BASIS 
~~ elif prep_basis . lower ( ) == 'sic' : 
~~~ prep_basis = SIC_BASIS 
~~ ~~ circuits = [ ] 
circuit_labels = [ ] 
if prep_basis is None : 
~~~ for meas_product in product ( meas_basis . keys ( ) , repeat = num_of_qubits ) : 
~~~ meas = dict ( zip ( meas_qubits , meas_product ) ) 
circuits . append ( { 'meas' : meas } ) 
label = '_meas_' 
for qubit , op in meas . items ( ) : 
~~~ label += '%s(%d)' % ( op [ 0 ] , qubit ) 
~~ circuit_labels . append ( label ) 
~~ return { 'qubits' : meas_qubits , 
'circuits' : circuits , 
'circuit_labels' : circuit_labels , 
'meas_basis' : meas_basis } 
~~ num_of_s = len ( list ( prep_basis . values ( ) ) [ 0 ] ) 
plst_single = [ ( b , s ) 
for b in prep_basis . keys ( ) 
for s in range ( num_of_s ) ] 
for plst_product in product ( plst_single , repeat = num_of_qubits ) : 
~~~ for meas_product in product ( meas_basis . keys ( ) , 
repeat = num_of_qubits ) : 
~~~ prep = dict ( zip ( prep_qubits , plst_product ) ) 
meas = dict ( zip ( meas_qubits , meas_product ) ) 
circuits . append ( { 'prep' : prep , 'meas' : meas } ) 
label = '_prep_' 
for qubit , op in prep . items ( ) : 
~~~ label += '%s%d(%d)' % ( op [ 0 ] , op [ 1 ] , qubit ) 
~~ label += '_meas_' 
~~ ~~ return { 'qubits' : meas_qubits , 
'prep_basis' : prep_basis , 
~~ def process_tomography_set ( meas_qubits , meas_basis = 'Pauli' , 
prep_qubits = None , prep_basis = 'SIC' ) : 
return tomography_set ( meas_qubits , meas_basis = meas_basis , 
prep_qubits = prep_qubits , prep_basis = prep_basis ) 
~~ def create_tomography_circuits ( circuit , qreg , creg , tomoset ) : 
if not isinstance ( circuit , QuantumCircuit ) : 
~~ dics = tomoset [ 'circuits' ] 
labels = tomography_circuit_names ( tomoset , circuit . name ) 
tomography_circuits = [ ] 
for label , conf in zip ( labels , dics ) : 
~~~ tmp = circuit 
if 'prep' in conf : 
~~~ prep = QuantumCircuit ( qreg , creg , name = 'tmp_prep' ) 
for qubit , op in conf [ 'prep' ] . items ( ) : 
~~~ tomoset [ 'prep_basis' ] . prep_gate ( prep , qreg [ qubit ] , op ) 
prep . barrier ( qreg [ qubit ] ) 
~~ tmp = prep + tmp 
~~ meas = QuantumCircuit ( qreg , creg , name = 'tmp_meas' ) 
for qubit , op in conf [ 'meas' ] . items ( ) : 
~~~ meas . barrier ( qreg [ qubit ] ) 
tomoset [ 'meas_basis' ] . meas_gate ( meas , qreg [ qubit ] , op ) 
meas . measure ( qreg [ qubit ] , creg [ qubit ] ) 
~~ tmp = tmp + meas 
tmp . name = label 
tomography_circuits . append ( tmp ) 
~~ logger . info ( \ , circuit . name ) 
return tomography_circuits 
~~ def tomography_data ( results , name , tomoset ) : 
labels = tomography_circuit_names ( tomoset , name ) 
circuits = tomoset [ 'circuits' ] 
data = [ ] 
prep = None 
for j , _ in enumerate ( labels ) : 
~~~ counts = marginal_counts ( results . get_counts ( labels [ j ] ) , 
tomoset [ 'qubits' ] ) 
shots = sum ( counts . values ( ) ) 
meas = circuits [ j ] [ 'meas' ] 
prep = circuits [ j ] . get ( 'prep' , None ) 
meas_qubits = sorted ( meas . keys ( ) ) 
if prep : 
~~~ prep_qubits = sorted ( prep . keys ( ) ) 
~~ circuit = { } 
for c in counts . keys ( ) : 
~~~ circuit [ c ] = { } 
circuit [ c ] [ 'meas' ] = [ ( meas [ meas_qubits [ k ] ] , int ( c [ - 1 - k ] ) ) 
for k in range ( len ( meas_qubits ) ) ] 
~~~ circuit [ c ] [ 'prep' ] = [ prep [ prep_qubits [ k ] ] 
for k in range ( len ( prep_qubits ) ) ] 
~~ ~~ data . append ( { 'counts' : counts , 'shots' : shots , 'circuit' : circuit } ) 
~~ ret = { 'data' : data , 'meas_basis' : tomoset [ 'meas_basis' ] } 
~~~ ret [ 'prep_basis' ] = tomoset [ 'prep_basis' ] 
~~ def marginal_counts ( counts , meas_qubits ) : 
num_of_qubits = len ( list ( counts . keys ( ) ) [ 0 ] ) 
qs = sorted ( meas_qubits , reverse = True ) 
meas_keys = count_keys ( len ( qs ) ) 
rgx = [ 
reduce ( lambda x , y : ( key [ qs . index ( y ) ] if y in qs else '\\\\d' ) + x , 
range ( num_of_qubits ) , '' ) for key in meas_keys 
meas_counts = [ ] 
for m in rgx : 
~~~ c = 0 
~~~ if match ( m , key ) : 
~~~ c += val 
~~ ~~ meas_counts . append ( c ) 
~~ return dict ( zip ( meas_keys , meas_counts ) ) 
~~ def fit_tomography_data ( tomo_data , method = 'wizard' , options = None ) : 
if isinstance ( method , str ) and method . lower ( ) in [ 'wizard' , 'leastsq' ] : 
~~~ trace = __get_option ( 'trace' , options ) 
beta = __get_option ( 'beta' , options ) 
rho = __leastsq_fit ( tomo_data , trace = trace , beta = beta ) 
if method == 'wizard' : 
~~~ epsilon = __get_option ( 'epsilon' , options ) 
rho = __wizard ( rho , epsilon = epsilon ) 
~~~ raise Exception ( \ % method ) 
~~ ~~ def __leastsq_fit ( tomo_data , weights = None , trace = None , beta = None ) : 
if trace is None : 
~~ data = tomo_data [ 'data' ] 
keys = data [ 0 ] [ 'circuit' ] . keys ( ) 
counts = [ ] 
shots = [ ] 
for dat in data : 
~~~ for key in keys : 
~~~ counts . append ( dat [ 'counts' ] [ key ] ) 
shots . append ( dat [ 'shots' ] ) 
projectors = dat [ 'circuit' ] [ key ] 
op = __projector ( projectors [ 'meas' ] , tomo_data [ 'meas_basis' ] ) 
if 'prep' in projectors : 
~~~ op_prep = __projector ( projectors [ 'prep' ] , 
tomo_data [ 'prep_basis' ] ) 
op = np . kron ( op_prep . conj ( ) , op ) 
~~ ops . append ( op ) 
~~ ~~ counts = np . array ( counts ) 
shots = np . array ( shots ) 
freqs = counts / shots 
if weights is None : 
~~~ if beta is None : 
~~~ beta = 0.50922 
~~ K = len ( keys ) 
freqs_hedged = ( counts + beta ) / ( shots + K * beta ) 
weights = np . sqrt ( shots / ( freqs_hedged * ( 1 - freqs_hedged ) ) ) 
~~ return __tomo_linear_inv ( freqs , ops , weights , trace = trace ) 
~~ def __projector ( op_list , basis ) : 
ret = 1 
for op in op_list : 
~~~ label , eigenstate = op 
ret = np . kron ( basis [ label ] [ eigenstate ] , ret ) 
~~ def __tomo_linear_inv ( freqs , ops , weights = None , trace = None ) : 
~~~ W = np . array ( weights ) 
if W . ndim == 1 : 
~~~ W = np . diag ( W ) 
~~ ~~ S = np . array ( [ vectorize ( m ) . conj ( ) 
for m in ops ] ) . reshape ( len ( ops ) , ops [ 0 ] . size ) 
ret = devectorize ( np . dot ( inv , np . dot ( Sdg , v ) ) ) 
if trace is not None : 
~~~ ret = trace * ret / np . trace ( ret ) 
~~ def __wizard ( rho , epsilon = None ) : 
if epsilon is None : 
~~ dim = len ( rho ) 
rho_wizard = np . zeros ( [ dim , dim ] ) 
for j in range ( dim ) : 
~~~ if v [ j ] < epsilon : 
~~~ tmp = v [ j ] 
v [ j ] = 0. 
x = 0. 
for k in range ( j + 1 , dim ) : 
~~~ x += tmp / ( dim - ( j + 1 ) ) 
v [ k ] = v [ k ] + tmp / ( dim - ( j + 1 ) ) 
~~ ~~ ~~ for j in range ( dim ) : 
~~~ rho_wizard = rho_wizard + v [ j ] * outer ( w [ : , j ] ) 
~~ return rho_wizard 
~~ def build_wigner_circuits ( circuit , phis , thetas , qubits , 
qreg , creg ) : 
~~ tomography_circuits = [ ] 
points = len ( phis [ 0 ] ) 
for point in range ( points ) : 
~~~ label = '_wigner_phase_point' 
label += str ( point ) 
tmp_circ = QuantumCircuit ( qreg , creg , name = label ) 
for qubit , _ in enumerate ( qubits ) : 
~~~ tmp_circ . u3 ( thetas [ qubit ] [ point ] , 0 , 
phis [ qubit ] [ point ] , qreg [ qubits [ qubit ] ] ) 
tmp_circ . measure ( qreg [ qubits [ qubit ] ] , creg [ qubits [ qubit ] ] ) 
~~ tmp_circ = circuit + tmp_circ 
tmp_circ . name = circuit . name + label 
tomography_circuits . append ( tmp_circ ) 
~~ def wigner_data ( q_result , meas_qubits , labels , shots = None ) : 
num = len ( meas_qubits ) 
dim = 2 ** num 
p = [ 0.5 + 0.5 * np . sqrt ( 3 ) , 0.5 - 0.5 * np . sqrt ( 3 ) ] 
parity = 1 
~~~ parity = np . kron ( parity , p ) 
~~ w = [ 0 ] * len ( labels ) 
wpt = 0 
counts = [ marginal_counts ( q_result . get_counts ( circ ) , meas_qubits ) 
for circ in labels ] 
for entry in counts : 
~~~ x = [ 0 ] * dim 
for i in range ( dim ) : 
~~~ if bin ( i ) [ 2 : ] . zfill ( num ) in entry : 
~~~ x [ i ] = float ( entry [ bin ( i ) [ 2 : ] . zfill ( num ) ] ) 
~~ ~~ if shots is None : 
~~~ shots = np . sum ( x ) 
~~ for i in range ( dim ) : 
~~~ w [ wpt ] = w [ wpt ] + ( x [ i ] / shots ) * parity [ i ] 
~~ wpt += 1 
~~ return w 
~~ def prep_gate ( self , circuit , qreg , op ) : 
if self . prep_fun is None : 
~~~ self . prep_fun ( circuit , qreg , op ) 
~~ ~~ def meas_gate ( self , circuit , qreg , op ) : 
if self . meas_fun is None : 
~~~ self . meas_fun ( circuit , qreg , op ) 
~~ ~~ def _text_checker ( job , interval , _interval_set = False , quiet = False , output = sys . stdout ) : 
status = job . status ( ) 
msg = status . value 
prev_msg = msg 
msg_len = len ( msg ) 
if not quiet : 
~~ while status . name not in [ 'DONE' , 'CANCELLED' , 'ERROR' ] : 
if status . name == 'QUEUED' : 
~~~ interval = max ( job . queue_position ( ) , 2 ) 
~~ ~~ if len ( msg ) < msg_len : 
~~ elif len ( msg ) > msg_len : 
~~~ msg_len = len ( msg ) 
~~ if msg != prev_msg and not quiet : 
~~ ~~ if not quiet : 
~~~ print ( '' , file = output ) 
~~ ~~ def job_monitor ( job , interval = None , monitor_async = False , quiet = False , output = sys . stdout ) : 
if interval is None : 
~~~ _interval_set = False 
interval = 2 
~~~ _interval_set = True 
~~ if _NOTEBOOK_ENV : 
~~~ if monitor_async : 
style = "font-size:16px;" 
style = style ) 
status = widgets . HTML ( value = header % job . status ( ) . value ) 
display ( status ) 
thread = threading . Thread ( target = _html_checker , args = ( job , interval , 
status , header ) ) 
thread . start ( ) 
~~~ _text_checker ( job , interval , _interval_set , 
quiet = quiet , output = output ) 
~~ _text_checker ( job , interval , _interval_set , quiet = quiet , output = output ) 
~~ ~~ def euler_angles_1q ( unitary_matrix ) : 
if unitary_matrix . shape != ( 2 , 2 ) : 
~~ phase = la . det ( unitary_matrix ) ** ( - 1.0 / 2.0 ) 
if abs ( U [ 0 , 0 ] ) > _CUTOFF_PRECISION : 
~~~ theta = 2 * math . acos ( abs ( U [ 0 , 0 ] ) ) 
~~~ theta = 2 * math . asin ( abs ( U [ 1 , 0 ] ) ) 
~~ phase11 = 0.0 
phase10 = 0.0 
if abs ( math . cos ( theta / 2.0 ) ) > _CUTOFF_PRECISION : 
~~~ phase11 = U [ 1 , 1 ] / math . cos ( theta / 2.0 ) 
~~ if abs ( math . sin ( theta / 2.0 ) ) > _CUTOFF_PRECISION : 
~~~ phase10 = U [ 1 , 0 ] / math . sin ( theta / 2.0 ) 
~~ phiplambda = 2 * math . atan2 ( np . imag ( phase11 ) , np . real ( phase11 ) ) 
phimlambda = 2 * math . atan2 ( np . imag ( phase10 ) , np . real ( phase10 ) ) 
phi = 0.0 
if abs ( U [ 0 , 0 ] ) > _CUTOFF_PRECISION and abs ( U [ 1 , 0 ] ) > _CUTOFF_PRECISION : 
~~~ phi = ( phiplambda + phimlambda ) / 2.0 
lamb = ( phiplambda - phimlambda ) / 2.0 
~~~ if abs ( U [ 0 , 0 ] ) < _CUTOFF_PRECISION : 
~~~ lamb = - phimlambda 
~~~ lamb = phiplambda 
~~ ~~ Rzphi = np . array ( [ [ np . exp ( - 1j * phi / 2.0 ) , 0 ] , 
[ 0 , np . exp ( 1j * phi / 2.0 ) ] ] , dtype = complex ) 
Rytheta = np . array ( [ [ np . cos ( theta / 2.0 ) , - np . sin ( theta / 2.0 ) ] , 
[ np . sin ( theta / 2.0 ) , np . cos ( theta / 2.0 ) ] ] , dtype = complex ) 
Rzlambda = np . array ( [ [ np . exp ( - 1j * lamb / 2.0 ) , 0 ] , 
[ 0 , np . exp ( 1j * lamb / 2.0 ) ] ] , dtype = complex ) 
V = np . dot ( Rzphi , np . dot ( Rytheta , Rzlambda ) ) 
if la . norm ( V - U ) > _CUTOFF_PRECISION : 
~~ return theta , phi , lamb 
~~ def simplify_U ( theta , phi , lam ) : 
gate = U3Gate ( theta , phi , lam ) 
if abs ( gate . params [ 0 ] % ( 2.0 * math . pi ) ) < _CUTOFF_PRECISION : 
~~~ gate = U1Gate ( gate . params [ 0 ] + gate . params [ 1 ] + gate . params [ 2 ] ) 
~~ if isinstance ( gate , U3Gate ) : 
~~~ if abs ( ( gate . params [ 0 ] - math . pi / 2 ) % ( 2.0 * math . pi ) ) < _CUTOFF_PRECISION : 
~~~ gate = U2Gate ( gate . params [ 1 ] , 
gate . params [ 2 ] + ( gate . params [ 0 ] - math . pi / 2 ) ) 
~~ if abs ( ( gate . params [ 0 ] + math . pi / 2 ) % ( 2.0 * math . pi ) ) < _CUTOFF_PRECISION : 
~~~ gate = U2Gate ( gate . params [ 1 ] + math . pi , 
gate . params [ 2 ] - math . pi + ( gate . params [ 0 ] + math . pi / 2 ) ) 
~~ ~~ if isinstance ( gate , U1Gate ) and abs ( gate . params [ 0 ] % ( 4.0 * math . pi ) ) < _CUTOFF_PRECISION : 
~~~ gate = IdGate ( ) 
~~ return gate 
~~ def two_qubit_kak ( unitary ) : 
if hasattr ( unitary , 'to_operator' ) : 
~~~ unitary = unitary . to_operator ( ) . data 
~~ if hasattr ( unitary , 'to_matrix' ) : 
~~~ unitary = unitary . to_matrix ( ) 
~~ unitary_matrix = np . array ( unitary , dtype = complex ) 
if unitary_matrix . shape != ( 4 , 4 ) : 
~~ if not is_unitary_matrix ( unitary_matrix ) : 
~~ phase = la . det ( unitary_matrix ) ** ( - 1.0 / 4.0 ) 
U = phase * unitary_matrix 
B = ( 1.0 / math . sqrt ( 2 ) ) * np . array ( [ [ 1 , 1j , 0 , 0 ] , 
[ 0 , 0 , 1j , 1 ] , 
[ 0 , 0 , 1j , - 1 ] , 
[ 1 , - 1j , 0 , 0 ] ] , dtype = complex ) 
Bdag = B . conj ( ) . T 
Uprime = Bdag . dot ( U . dot ( B ) ) 
M2 = Uprime . T . dot ( Uprime ) 
D , P = la . eig ( M2 ) 
D = np . diag ( D ) 
if abs ( la . det ( P ) + 1 ) < 1e-5 : 
~~~ swap = np . array ( [ [ 1 , 0 , 0 , 0 ] , 
[ 0 , 0 , 1 , 0 ] , 
[ 0 , 1 , 0 , 0 ] , 
[ 0 , 0 , 0 , 1 ] ] , dtype = complex ) 
P = P . dot ( swap ) 
D = swap . dot ( D . dot ( swap ) ) 
if abs ( la . det ( Q ) + 1 ) < 1e-5 : 
~~~ Q [ 0 , 0 ] = - Q [ 0 , 0 ] 
~~ Pprime = la . solve ( Q , P . T ) 
Kprime = Uprime . dot ( P . dot ( Pprime ) ) 
K1 = B . dot ( Kprime . dot ( P . dot ( Bdag ) ) ) 
A = B . dot ( Q . dot ( Bdag ) ) 
K2 = B . dot ( P . T . dot ( Bdag ) ) 
KAK = K1 . dot ( A . dot ( K2 ) ) 
if la . norm ( KAK - U ) > 1e-6 : 
~~ xx = np . array ( [ [ 0 , 0 , 0 , 1 ] , 
[ 1 , 0 , 0 , 0 ] ] , dtype = complex ) 
yy = np . array ( [ [ 0 , 0 , 0 , - 1 ] , 
[ - 1 , 0 , 0 , 0 ] ] , dtype = complex ) 
zz = np . array ( [ [ 1 , 0 , 0 , 0 ] , 
[ 0 , - 1 , 0 , 0 ] , 
[ 0 , 0 , - 1 , 0 ] , 
A_real_tr = A . real . trace ( ) 
alpha = math . atan2 ( A . dot ( xx ) . imag . trace ( ) , A_real_tr ) 
beta = math . atan2 ( A . dot ( yy ) . imag . trace ( ) , A_real_tr ) 
gamma = math . atan2 ( A . dot ( zz ) . imag . trace ( ) , A_real_tr ) 
L = K1 [ 0 : 2 , 0 : 2 ] 
if la . norm ( L ) < 1e-9 : 
~~~ L = K1 [ 0 : 2 , 2 : 4 ] 
~~~ L = K1 [ 2 : 4 , 2 : 4 ] 
~~ ~~ Q = L . dot ( L . conj ( ) . T ) 
U2 = L / math . sqrt ( Q [ 0 , 0 ] . real ) 
R = K1 . dot ( np . kron ( np . identity ( 2 ) , U2 . conj ( ) . T ) ) 
U1 = np . zeros ( ( 2 , 2 ) , dtype = complex ) 
U1 [ 0 , 0 ] = R [ 0 , 0 ] 
U1 [ 0 , 1 ] = R [ 0 , 2 ] 
U1 [ 1 , 0 ] = R [ 2 , 0 ] 
U1 [ 1 , 1 ] = R [ 2 , 2 ] 
L = K2 [ 0 : 2 , 0 : 2 ] 
~~~ L = K2 [ 0 : 2 , 2 : 4 ] 
~~~ L = K2 [ 2 : 4 , 2 : 4 ] 
~~ ~~ Q = np . dot ( L , np . transpose ( L . conjugate ( ) ) ) 
V2 = L / np . sqrt ( Q [ 0 , 0 ] ) 
R = np . dot ( K2 , np . kron ( np . identity ( 2 ) , np . transpose ( V2 . conjugate ( ) ) ) ) 
V1 = np . zeros_like ( U1 ) 
V1 [ 0 , 0 ] = R [ 0 , 0 ] 
V1 [ 0 , 1 ] = R [ 0 , 2 ] 
V1 [ 1 , 0 ] = R [ 2 , 0 ] 
V1 [ 1 , 1 ] = R [ 2 , 2 ] 
if la . norm ( np . kron ( U1 , U2 ) - K1 ) > 1e-4 : 
~~ if la . norm ( np . kron ( V1 , V2 ) - K2 ) > 1e-4 : 
~~ test = la . expm ( 1j * ( alpha * xx + beta * yy + gamma * zz ) ) 
if la . norm ( A - test ) > 1e-4 : 
~~ V2 = np . array ( [ [ np . exp ( 1j * np . pi / 4 ) , 0 ] , 
[ 0 , np . exp ( - 1j * np . pi / 4 ) ] ] , dtype = complex ) . dot ( V2 ) 
U1 = U1 . dot ( np . array ( [ [ np . exp ( - 1j * np . pi / 4 ) , 0 ] , 
[ 0 , np . exp ( 1j * np . pi / 4 ) ] ] , dtype = complex ) ) 
U1 = U1 . dot ( np . array ( [ [ np . exp ( 1j * np . pi / 4 ) , 0 ] , 
U1 = phase . conjugate ( ) * U1 
g1 = np . kron ( V1 , V2 ) 
g2 = np . array ( [ [ 1 , 0 , 0 , 0 ] , 
[ 0 , 0 , 0 , 1 ] , 
[ 0 , 1 , 0 , 0 ] ] , dtype = complex ) 
theta = 2 * gamma - np . pi / 2 
Ztheta = np . array ( [ [ np . exp ( 1j * theta / 2 ) , 0 ] , 
[ 0 , np . exp ( - 1j * theta / 2 ) ] ] , dtype = complex ) 
kappa = np . pi / 2 - 2 * alpha 
Ykappa = np . array ( [ [ math . cos ( kappa / 2 ) , math . sin ( kappa / 2 ) ] , 
[ - math . sin ( kappa / 2 ) , math . cos ( kappa / 2 ) ] ] , dtype = complex ) 
g3 = np . kron ( Ztheta , Ykappa ) 
g4 = np . array ( [ [ 1 , 0 , 0 , 0 ] , 
[ 0 , 0 , 1 , 0 ] ] , dtype = complex ) 
zeta = 2 * beta - np . pi / 2 
Yzeta = np . array ( [ [ math . cos ( zeta / 2 ) , math . sin ( zeta / 2 ) ] , 
[ - math . sin ( zeta / 2 ) , math . cos ( zeta / 2 ) ] ] , dtype = complex ) 
g5 = np . kron ( np . identity ( 2 ) , Yzeta ) 
g6 = g2 
g7 = np . kron ( U1 , U2 ) 
V = g2 . dot ( g1 ) 
V = g3 . dot ( V ) 
V = g4 . dot ( V ) 
V = g5 . dot ( V ) 
V = g6 . dot ( V ) 
V = g7 . dot ( V ) 
if la . norm ( V - U * phase . conjugate ( ) ) > 1e-6 : 
~~ v1_param = euler_angles_1q ( V1 ) 
v2_param = euler_angles_1q ( V2 ) 
u1_param = euler_angles_1q ( U1 ) 
u2_param = euler_angles_1q ( U2 ) 
v1_gate = U3Gate ( v1_param [ 0 ] , v1_param [ 1 ] , v1_param [ 2 ] ) 
v2_gate = U3Gate ( v2_param [ 0 ] , v2_param [ 1 ] , v2_param [ 2 ] ) 
u1_gate = U3Gate ( u1_param [ 0 ] , u1_param [ 1 ] , u1_param [ 2 ] ) 
u2_gate = U3Gate ( u2_param [ 0 ] , u2_param [ 1 ] , u2_param [ 2 ] ) 
q = QuantumRegister ( 2 ) 
return_circuit = QuantumCircuit ( q ) 
return_circuit . append ( v1_gate , [ q [ 1 ] ] ) 
return_circuit . append ( v2_gate , [ q [ 0 ] ] ) 
return_circuit . append ( CnotGate ( ) , [ q [ 0 ] , q [ 1 ] ] ) 
gate = U3Gate ( 0.0 , 0.0 , - 2.0 * gamma + np . pi / 2.0 ) 
return_circuit . append ( gate , [ q [ 1 ] ] ) 
gate = U3Gate ( - np . pi / 2.0 + 2.0 * alpha , 0.0 , 0.0 ) 
return_circuit . append ( gate , [ q [ 0 ] ] ) 
return_circuit . append ( CnotGate ( ) , [ q [ 1 ] , q [ 0 ] ] ) 
gate = U3Gate ( - 2.0 * beta + np . pi / 2.0 , 0.0 , 0.0 ) 
return_circuit . append ( u1_gate , [ q [ 1 ] ] ) 
return_circuit . append ( u2_gate , [ q [ 0 ] ] ) 
return return_circuit 
self . layout = self . layout or self . property_set [ 'layout' ] 
~~ layout_virtual_qubits = self . layout . get_virtual_bits ( ) . keys ( ) 
new_qregs = set ( virtual_qubit [ 0 ] for virtual_qubit in layout_virtual_qubits 
if virtual_qubit not in dag . wires ) 
for qreg in new_qregs : 
~~~ dag . add_qreg ( qreg ) 
q = QuantumRegister ( 2 , "q" ) 
( HGate ( ) , [ q [ 1 ] ] , [ ] ) , 
( SdgGate ( ) , [ q [ 1 ] ] , [ ] ) , 
( SGate ( ) , [ q [ 1 ] ] , [ ] ) , 
( XGate ( ) , [ q [ 1 ] ] , [ ] ) , 
( SGate ( ) , [ q [ 0 ] ] , [ ] ) 
~~ def config_tab ( backend ) : 
config = backend . configuration ( ) . to_dict ( ) 
upper_str = "<table>" 
footer = "</table>" 
upper_str += "<tr><th>Property</th><th>Value</th></tr>" 
for key in upper_list : 
key , config_dict [ key ] ) 
~~ upper_str += footer 
upper_table = widgets . HTML ( 
value = upper_str , layout = widgets . Layout ( width = '100%' , grid_area = 'left' ) ) 
image_widget = widgets . Output ( 
layout = widgets . Layout ( display = 'flex-inline' , grid_area = 'right' , 
width = 'auto' , max_height = '300px' , 
align_items = 'center' ) ) 
~~~ with image_widget : 
~~~ gate_map = plot_gate_map ( backend ) 
display ( gate_map ) 
~~ plt . close ( gate_map ) 
~~ lower_str = "<table>" 
lower_str += "<tr><th></th><th></th></tr>" 
for key in lower_list : 
~~~ if key != 'name' : 
~~~ lower_str += "<tr><td>%s</td><td>%s</td></tr>" % ( 
~~ ~~ lower_str += footer 
lower_table = widgets . HTML ( value = lower_str , 
layout = widgets . Layout ( 
width = 'auto' , 
grid_area = 'bottom' ) ) 
grid = widgets . GridBox ( children = [ upper_table , image_widget , lower_table ] , 
grid_template_areas = \ , 
return grid 
~~ def qubits_tab ( backend ) : 
props = backend . properties ( ) . to_dict ( ) 
header_html = header_html . format ( key = 'last_update_date' , 
value = props [ 'last_update_date' ] ) 
update_date_widget = widgets . HTML ( value = header_html ) 
qubit_html = "<table>" 
qubit_html += "<tr><th></th><th>Frequency</th><th>T1</th><th>T2</th>" 
qubit_footer = "</table>" 
readout_error = round ( readout_info [ 'value' ] , 5 ) 
qubit_html += "<td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td></tr>" 
qubit_html = qubit_html % ( name , freq , T1 , T2 , U1 , U2 , U3 , readout_error ) 
~~ qubit_html += qubit_footer 
qubit_widget = widgets . HTML ( value = qubit_html ) 
out = widgets . VBox ( [ update_date_widget , 
qubit_widget ] ) 
~~ def gates_tab ( backend ) : 
update_date_widget = widgets . HTML ( value = header_html , 
layout = widgets . Layout ( grid_area = 'top' ) ) 
gate_html = "<table>" 
gate_footer = "</table>" 
left_num = math . ceil ( len ( multi_qubit_gates ) / 3 ) 
mid_num = math . ceil ( ( len ( multi_qubit_gates ) - left_num ) / 2 ) 
left_table = gate_html 
for qub in range ( left_num ) : 
~~~ gate = multi_qubit_gates [ qub ] 
name = gate [ 'name' ] 
error = round ( gate [ 'parameters' ] [ 0 ] [ 'value' ] , 5 ) 
left_table += "</td><td>%s</td><td>%s</td></tr>" 
left_table = left_table % ( name , ttype , error ) 
~~ left_table += gate_footer 
middle_table = gate_html 
for qub in range ( left_num , left_num + mid_num ) : 
middle_table += "</td><td>%s</td><td>%s</td></tr>" 
middle_table = middle_table % ( name , ttype , error ) 
~~ middle_table += gate_footer 
right_table = gate_html 
for qub in range ( left_num + mid_num , len ( multi_qubit_gates ) ) : 
right_table += "</td><td>%s</td><td>%s</td></tr>" 
right_table = right_table % ( name , ttype , error ) 
~~ right_table += gate_footer 
left_table_widget = widgets . HTML ( value = left_table , 
layout = widgets . Layout ( grid_area = 'left' ) ) 
middle_table_widget = widgets . HTML ( value = middle_table , 
layout = widgets . Layout ( grid_area = 'middle' ) ) 
right_table_widget = widgets . HTML ( value = right_table , 
layout = widgets . Layout ( grid_area = 'right' ) ) 
grid = widgets . GridBox ( children = [ update_date_widget , 
left_table_widget , 
middle_table_widget , 
right_table_widget ] , 
~~ def detailed_map ( backend ) : 
single_gate_errors = [ q [ 'parameters' ] [ 0 ] [ 'value' ] 
for q in props [ 'gates' ] [ 2 : 3 * config [ 'n_qubits' ] : 3 ] ] 
single_norm = matplotlib . colors . Normalize ( 
vmin = min ( single_gate_errors ) , vmax = max ( single_gate_errors ) ) 
q_colors = [ cm . viridis ( single_norm ( err ) ) for err in single_gate_errors ] 
cmap = config [ 'coupling_map' ] 
cx_errors = [ ] 
for line in cmap : 
~~~ for item in props [ 'gates' ] [ 3 * config [ 'n_qubits' ] : ] : 
~~~ if item [ 'qubits' ] == line : 
~~~ cx_errors . append ( item [ 'parameters' ] [ 0 ] [ 'value' ] ) 
~~ ~~ cx_norm = matplotlib . colors . Normalize ( 
vmin = min ( cx_errors ) , vmax = max ( cx_errors ) ) 
line_colors = [ cm . viridis ( cx_norm ( err ) ) for err in cx_errors ] 
single_widget = widgets . Output ( layout = widgets . Layout ( display = 'flex-inline' , grid_area = 'left' , 
cmap_widget = widgets . Output ( layout = widgets . Layout ( display = 'flex-inline' , grid_area = 'top' , 
width = 'auto' , height = 'auto' , 
cx_widget = widgets . Output ( layout = widgets . Layout ( display = 'flex-inline' , grid_area = 'right' , 
tick_locator = mpl . ticker . MaxNLocator ( nbins = 5 ) 
with cmap_widget : 
~~~ noise_map = plot_gate_map ( backend , qubit_color = q_colors , 
line_color = line_colors , 
qubit_size = 28 , 
plot_directed = True ) 
width , height = noise_map . get_size_inches ( ) 
noise_map . set_size_inches ( 1.25 * width , 1.25 * height ) 
display ( noise_map ) 
plt . close ( noise_map ) 
~~ with single_widget : 
~~~ cbl_fig = plt . figure ( figsize = ( 3 , 1 ) ) 
ax1 = cbl_fig . add_axes ( [ 0.05 , 0.80 , 0.9 , 0.15 ] ) 
single_cb = mpl . colorbar . ColorbarBase ( ax1 , cmap = cm . viridis , 
norm = single_norm , 
orientation = 'horizontal' ) 
single_cb . locator = tick_locator 
single_cb . update_ticks ( ) 
display ( cbl_fig ) 
plt . close ( cbl_fig ) 
~~ with cx_widget : 
~~~ cx_fig = plt . figure ( figsize = ( 3 , 1 ) ) 
ax2 = cx_fig . add_axes ( [ 0.05 , 0.80 , 0.9 , 0.15 ] ) 
cx_cb = mpl . colorbar . ColorbarBase ( ax2 , cmap = cm . viridis , 
norm = cx_norm , 
cx_cb . locator = tick_locator 
cx_cb . update_ticks ( ) 
display ( cx_fig ) 
plt . close ( cx_fig ) 
~~ out_box = widgets . GridBox ( [ single_widget , cmap_widget , cx_widget ] , 
return out_box 
~~ def job_history ( backend ) : 
year = widgets . Output ( layout = widgets . Layout ( display = 'flex-inline' , 
align_items = 'center' , 
min_height = '400px' ) ) 
month = widgets . Output ( layout = widgets . Layout ( display = 'flex-inline' , 
week = widgets . Output ( layout = widgets . Layout ( display = 'flex-inline' , 
tabs = widgets . Tab ( layout = widgets . Layout ( max_height = '620px' ) ) 
tabs . children = [ year , month , week ] 
tabs . set_title ( 0 , 'Year' ) 
tabs . set_title ( 1 , 'Month' ) 
tabs . set_title ( 2 , 'Week' ) 
tabs . selected_index = 1 
_build_job_history ( tabs , backend ) 
return tabs 
~~ def plot_job_history ( jobs , interval = 'year' ) : 
def get_date ( job ) : 
return datetime . datetime . strptime ( job . creation_date ( ) , 
'%Y-%m-%dT%H:%M:%S.%fZ' ) 
~~ current_time = datetime . datetime . now ( ) 
if interval == 'year' : 
~~~ bins = [ ( current_time - datetime . timedelta ( days = k * 365 / 12 ) ) 
for k in range ( 12 ) ] 
~~ elif interval == 'month' : 
~~~ bins = [ ( current_time - datetime . timedelta ( days = k ) ) for k in range ( 30 ) ] 
~~ elif interval == 'week' : 
~~~ bins = [ ( current_time - datetime . timedelta ( days = k ) ) for k in range ( 7 ) ] 
~~ binned_jobs = [ 0 ] * len ( bins ) 
~~~ for job in jobs : 
~~~ for ind , dat in enumerate ( bins ) : 
~~~ date = get_date ( job ) 
if date . month == dat . month : 
~~~ binned_jobs [ ind ] += 1 
if date . day == dat . day and date . month == dat . month : 
~~ ~~ ~~ nz_bins = [ ] 
nz_idx = [ ] 
for ind , val in enumerate ( binned_jobs ) : 
~~~ if val != 0 : 
~~~ nz_idx . append ( ind ) 
nz_bins . append ( val ) 
~~ ~~ total_jobs = sum ( binned_jobs ) 
colors = [ '#003f5c' , '#ffa600' , '#374c80' , '#ff764a' , 
'#7a5195' , '#ef5675' , '#bc5090' ] 
~~~ labels = [ '{}-{}' . format ( str ( bins [ b ] . year ) [ 2 : ] , bins [ b ] . month ) for b in nz_idx ] 
~~~ labels = [ '{}-{}' . format ( bins [ b ] . month , bins [ b ] . day ) for b in nz_idx ] 
ax . pie ( nz_bins [ : : - 1 ] , labels = labels , colors = colors , textprops = { 'fontsize' : 14 } , 
rotatelabels = True , counterclock = False ) 
ax . add_artist ( Circle ( ( 0 , 0 ) , 0.7 , color = 'white' , zorder = 1 ) ) 
ax . text ( 0 , 0 , total_jobs , horizontalalignment = 'center' , 
verticalalignment = 'center' , fontsize = 26 ) 
resets = dag . op_nodes ( Reset ) 
for reset in resets : 
~~~ predecessor = next ( dag . predecessors ( reset ) ) 
if predecessor . type == 'in' : 
~~~ dag . remove_op_node ( reset ) 
~~ def draw ( self , ** kwargs ) : 
from qiskit . tools . visualization import pulse_drawer 
return pulse_drawer ( self . _samples , self . duration , ** kwargs ) 
~~ def cu3 ( self , theta , phi , lam , ctl , tgt ) : 
return self . append ( Cu3Gate ( theta , phi , lam ) , [ ctl , tgt ] , [ ] ) 
( U1Gate ( ( self . params [ 2 ] - self . params [ 1 ] ) / 2 ) , [ q [ 1 ] ] , [ ] ) , 
( U3Gate ( - self . params [ 0 ] / 2 , 0 , - ( self . params [ 1 ] + self . params [ 2 ] ) / 2 ) , [ q [ 1 ] ] , [ ] ) , 
( U3Gate ( self . params [ 0 ] / 2 , self . params [ 1 ] , 0 ) , [ q [ 1 ] ] , [ ] ) 
~~ def build_bell_circuit ( ) : 
c = ClassicalRegister ( 2 ) 
qc = QuantumCircuit ( q , c ) 
qc . h ( q [ 0 ] ) 
qc . cx ( q [ 0 ] , q [ 1 ] ) 
qc . measure ( q , c ) 
return qc 
~~ def transpile ( circuits , 
backend = None , 
basis_gates = None , coupling_map = None , backend_properties = None , 
initial_layout = None , seed_transpiler = None , 
optimization_level = None , 
pass_manager = None , 
if seed_mapper : 
seed_transpiler = seed_mapper 
~~ if isinstance ( circuits , Schedule ) or ( isinstance ( circuits , list ) and all ( isinstance ( c , Schedule ) for c in circuits ) ) : 
~~~ return circuits 
~~ circuits = circuits if isinstance ( circuits , list ) else [ circuits ] 
transpile_configs = _parse_transpile_args ( circuits , backend , basis_gates , coupling_map , 
backend_properties , initial_layout , 
seed_transpiler , optimization_level , 
pass_manager ) 
circuits = parallel_map ( _transpile_circuit , list ( zip ( circuits , transpile_configs ) ) ) 
if len ( circuits ) == 1 : 
~~~ return circuits [ 0 ] 
~~ def _transpile_circuit ( circuit_config_tuple ) : 
circuit , transpile_config = circuit_config_tuple 
if transpile_config . pass_manager : 
~~~ pass_manager = transpile_config . pass_manager 
~~ elif transpile_config . coupling_map : 
~~~ pass_manager = default_pass_manager ( transpile_config . basis_gates , 
transpile_config . coupling_map , 
transpile_config . initial_layout , 
transpile_config . seed_transpiler ) 
~~~ pass_manager = default_pass_manager_simulator ( transpile_config . basis_gates ) 
~~ return pass_manager . run ( circuit ) 
~~ def _parse_transpile_args ( circuits , backend , 
basis_gates , coupling_map , backend_properties , 
initial_layout , seed_transpiler , optimization_level , 
pass_manager ) : 
num_circuits = len ( circuits ) 
basis_gates = _parse_basis_gates ( basis_gates , backend , circuits ) 
coupling_map = _parse_coupling_map ( coupling_map , backend , num_circuits ) 
backend_properties = _parse_backend_properties ( backend_properties , backend , num_circuits ) 
initial_layout = _parse_initial_layout ( initial_layout , circuits ) 
seed_transpiler = _parse_seed_transpiler ( seed_transpiler , num_circuits ) 
optimization_level = _parse_optimization_level ( optimization_level , num_circuits ) 
pass_manager = _parse_pass_manager ( pass_manager , num_circuits ) 
transpile_configs = [ ] 
for args in zip ( basis_gates , coupling_map , backend_properties , initial_layout , 
seed_transpiler , optimization_level , pass_manager ) : 
~~~ transpile_config = TranspileConfig ( basis_gates = args [ 0 ] , 
coupling_map = args [ 1 ] , 
backend_properties = args [ 2 ] , 
initial_layout = args [ 3 ] , 
seed_transpiler = args [ 4 ] , 
optimization_level = args [ 5 ] , 
pass_manager = args [ 6 ] ) 
transpile_configs . append ( transpile_config ) 
~~ return transpile_configs 
~~ def execute ( experiments , backend , 
backend_properties = None , initial_layout = None , 
seed_transpiler = None , optimization_level = None , pass_manager = None , 
memory = False , max_credits = 10 , seed_simulator = None , 
schedule_los = None , meas_level = 2 , meas_return = 'avg' , 
memory_slots = None , memory_slot_size = 100 , rep_time = None , parameter_binds = None , 
config = None , circuits = None , 
** run_config ) : 
if circuits is not None : 
~~~ experiments = circuits 
~~ experiments = transpile ( experiments , 
backend_properties = backend_properties , 
seed_transpiler = seed_transpiler , 
optimization_level = optimization_level , 
pass_manager = pass_manager , 
qobj = assemble ( experiments , 
qobj_header = qobj_header , 
seed_simulator = seed_simulator , 
default_qubit_los = default_qubit_los , 
default_meas_los = default_meas_los , 
schedule_los = schedule_los , 
meas_level = meas_level , 
meas_return = meas_return , 
memory_slots = memory_slots , 
memory_slot_size = memory_slot_size , 
rep_time = rep_time , 
parameter_binds = parameter_binds , 
run_config = run_config 
return backend . run ( qobj , ** run_config ) 
~~ def drive ( self ) -> DriveChannel : 
if self . _drives : 
~~~ return self . _drives [ 0 ] 
~~ ~~ def control ( self ) -> ControlChannel : 
if self . _controls : 
~~~ return self . _controls [ 0 ] 
~~ ~~ def measure ( self ) -> MeasureChannel : 
if self . _measures : 
~~~ return self . _measures [ 0 ] 
~~ ~~ def acquire ( self ) -> AcquireChannel : 
if self . _acquires : 
~~~ return self . _acquires [ 0 ] 
~~ ~~ def input_state ( circ , q , n ) : 
~~~ circ . h ( q [ j ] ) 
circ . u1 ( math . pi / float ( 2 ** ( j ) ) , q [ j ] ) . inverse ( ) 
~~ ~~ def assemble_circuits ( circuits , qobj_id = None , qobj_header = None , run_config = None ) : 
qobj_config = QasmQobjConfig ( ) 
if run_config : 
~~~ qobj_config = QasmQobjConfig ( ** run_config . to_dict ( ) ) 
~~ experiments = [ ] 
max_n_qubits = 0 
max_memory_slots = 0 
for circuit in circuits : 
~~~ n_qubits = 0 
memory_slots = 0 
qubit_labels = [ ] 
clbit_labels = [ ] 
qreg_sizes = [ ] 
creg_sizes = [ ] 
for qreg in circuit . qregs : 
~~~ qreg_sizes . append ( [ qreg . name , qreg . size ] ) 
~~~ qubit_labels . append ( [ qreg . name , j ] ) 
~~ n_qubits += qreg . size 
~~ for creg in circuit . cregs : 
~~~ creg_sizes . append ( [ creg . name , creg . size ] ) 
~~~ clbit_labels . append ( [ creg . name , j ] ) 
~~ memory_slots += creg . size 
~~ experimentheader = QobjExperimentHeader ( qubit_labels = qubit_labels , 
n_qubits = n_qubits , 
qreg_sizes = qreg_sizes , 
clbit_labels = clbit_labels , 
creg_sizes = creg_sizes , 
name = circuit . name ) 
experimentconfig = QasmQobjExperimentConfig ( n_qubits = n_qubits , memory_slots = memory_slots ) 
is_conditional_experiment = any ( op . control for ( op , qargs , cargs ) in circuit . data ) 
max_conditional_idx = 0 
instructions = [ ] 
for op_context in circuit . data : 
~~~ instruction = op_context [ 0 ] . assemble ( ) 
qargs = op_context [ 1 ] 
cargs = op_context [ 2 ] 
if qargs : 
~~~ qubit_indices = [ qubit_labels . index ( [ qubit [ 0 ] . name , qubit [ 1 ] ] ) 
for qubit in qargs ] 
instruction . qubits = qubit_indices 
~~ if cargs : 
~~~ clbit_indices = [ clbit_labels . index ( [ clbit [ 0 ] . name , clbit [ 1 ] ] ) 
for clbit in cargs ] 
instruction . memory = clbit_indices 
if instruction . name == "measure" and is_conditional_experiment : 
~~~ instruction . register = clbit_indices 
~~ ~~ if hasattr ( instruction , '_control' ) : 
~~~ ctrl_reg , ctrl_val = instruction . _control 
mask = 0 
val = 0 
for clbit in clbit_labels : 
~~~ if clbit [ 0 ] == ctrl_reg . name : 
~~~ mask |= ( 1 << clbit_labels . index ( clbit ) ) 
val |= ( ( ( ctrl_val >> clbit [ 1 ] ) & 1 ) << clbit_labels . index ( clbit ) ) 
~~ ~~ conditional_reg_idx = memory_slots + max_conditional_idx 
conversion_bfunc = QasmQobjInstruction ( name = 'bfunc' , 
mask = "0x%X" % mask , 
relation = '==' , 
val = "0x%X" % val , 
register = conditional_reg_idx ) 
instructions . append ( conversion_bfunc ) 
instruction . conditional = conditional_reg_idx 
max_conditional_idx += 1 
del instruction . _control 
~~ instructions . append ( instruction ) 
~~ experiments . append ( QasmQobjExperiment ( instructions = instructions , header = experimentheader , 
config = experimentconfig ) ) 
if n_qubits > max_n_qubits : 
~~~ max_n_qubits = n_qubits 
~~ if memory_slots > max_memory_slots : 
~~~ max_memory_slots = memory_slots 
~~ ~~ qobj_config . memory_slots = max_memory_slots 
qobj_config . n_qubits = max_n_qubits 
return QasmQobj ( qobj_id = qobj_id , 
config = qobj_config , 
experiments = experiments , 
header = qobj_header ) 
~~ def assemble_schedules ( schedules , qobj_id = None , qobj_header = None , run_config = None ) : 
~~ instruction_converter = PulseQobjConverter 
instruction_converter = instruction_converter ( PulseQobjInstruction , ** run_config . to_dict ( ) ) 
lo_converter = LoConfigConverter ( PulseQobjExperimentConfig , run_config . qubit_lo_freq , 
run_config . meas_lo_freq , ** run_config . to_dict ( ) ) 
qobj_schedules = [ ] 
user_pulselib = set ( ) 
for idx , schedule in enumerate ( schedules ) : 
~~~ qobj_instructions = [ ] 
for shift , instruction in schedule . instructions : 
~~~ qobj_instructions . append ( instruction_converter ( shift , instruction ) ) 
if isinstance ( instruction , PulseInstruction ) : 
~~~ user_pulselib . add ( instruction . command ) 
~~ ~~ qobj_experiment_header = QobjExperimentHeader ( 
name = schedule . name or 'Experiment-%d' % idx 
qobj_schedules . append ( { 
'header' : qobj_experiment_header , 
'instructions' : qobj_instructions 
~~ run_config . pulse_library = [ QobjPulseLibrary ( name = pulse . name , samples = pulse . samples ) 
for pulse in user_pulselib ] 
experiments = [ ] 
if len ( run_config . schedule_los ) == 1 : 
~~~ lo_dict = run_config . schedule_los . pop ( ) 
q_los = lo_converter . get_qubit_los ( lo_dict ) 
if q_los : 
~~~ run_config . qubit_lo_freq = q_los 
~~ m_los = lo_converter . get_meas_los ( lo_dict ) 
if m_los : 
~~~ run_config . meas_lo_freq = m_los 
~~ ~~ if run_config . schedule_los : 
~~~ if len ( qobj_schedules ) == 1 : 
~~~ for lo_dict in run_config . schedule_los : 
~~~ experiments . append ( PulseQobjExperiment ( 
instructions = qobj_schedules [ 0 ] [ 'instructions' ] , 
experimentheader = qobj_schedules [ 0 ] [ 'header' ] , 
experimentconfig = lo_converter ( lo_dict ) 
~~ ~~ elif len ( qobj_schedules ) == len ( run_config . schedule_los ) : 
~~~ for lo_dict , schedule in zip ( run_config . schedule_los , qobj_schedules ) : 
instructions = schedule [ 'instructions' ] , 
experimentheader = schedule [ 'header' ] , 
~~~ for schedule in qobj_schedules : 
~~ ~~ qobj_config = PulseQobjConfig ( ** run_config . to_dict ( ) ) 
return PulseQobj ( qobj_id = qobj_id , 
~~ def assemble ( experiments , 
shots = 1024 , memory = False , max_credits = None , seed_simulator = None , 
if config : 
'run_config.' , DeprecationWarning ) 
run_config = run_config or config 
~~ if seed : 
seed_simulator = seed_simulator or seed 
~~ experiments = experiments if isinstance ( experiments , list ) else [ experiments ] 
qobj_id , qobj_header , run_config = _parse_run_args ( backend , qobj_id , qobj_header , 
shots , memory , max_credits , seed_simulator , 
default_qubit_los , default_meas_los , 
schedule_los , meas_level , meas_return , 
memory_slots , memory_slot_size , rep_time , 
parameter_binds , ** run_config ) 
if all ( isinstance ( exp , QuantumCircuit ) for exp in experiments ) : 
~~~ bound_experiments , run_config = _expand_parameters ( circuits = experiments , 
run_config = run_config ) 
return assemble_circuits ( circuits = bound_experiments , qobj_id = qobj_id , 
qobj_header = qobj_header , run_config = run_config ) 
~~ elif all ( isinstance ( exp , Schedule ) for exp in experiments ) : 
~~~ return assemble_schedules ( schedules = experiments , qobj_id = qobj_id , 
~~ ~~ def _parse_run_args ( backend , qobj_id , qobj_header , 
parameter_binds , ** run_config ) : 
backend_config = None 
backend_default = None 
if backend : 
~~~ backend_config = backend . configuration ( ) 
~~~ backend_default = backend . defaults ( ) 
~~ except ( ModelValidationError , AttributeError ) : 
~~~ from collections import namedtuple 
backend_config_defaults = getattr ( backend_config , 'defaults' , { } ) 
BackendDefault = namedtuple ( 'BackendDefault' , ( 'qubit_freq_est' , 'meas_freq_est' ) ) 
backend_default = BackendDefault ( 
qubit_freq_est = backend_config_defaults . get ( 'qubit_freq_est' ) , 
meas_freq_est = backend_config_defaults . get ( 'meas_freq_est' ) 
~~ ~~ memory_slots = memory_slots or getattr ( backend_config , 'memory_slots' , None ) 
rep_time = rep_time or getattr ( backend_config , 'rep_times' , None ) 
if isinstance ( rep_time , list ) : 
~~~ rep_time = rep_time [ - 1 ] 
~~ parameter_binds = parameter_binds or [ ] 
schedule_los = schedule_los or [ ] 
if isinstance ( schedule_los , ( LoConfig , dict ) ) : 
~~~ schedule_los = [ schedule_los ] 
~~ schedule_los = [ lo_config if isinstance ( lo_config , LoConfig ) else LoConfig ( lo_config ) 
for lo_config in schedule_los ] 
qubit_lo_freq = default_qubit_los or getattr ( backend_default , 'qubit_freq_est' , [ ] ) 
meas_lo_freq = default_meas_los or getattr ( backend_default , 'meas_freq_est' , [ ] ) 
qobj_id = qobj_id or str ( uuid . uuid4 ( ) ) 
qobj_header = qobj_header or { } 
if isinstance ( qobj_header , QobjHeader ) : 
~~~ qobj_header = qobj_header . to_dict ( ) 
~~ backend_name = getattr ( backend_config , 'backend_name' , None ) 
backend_version = getattr ( backend_config , 'backend_version' , None ) 
qobj_header = { ** dict ( backend_name = backend_name , backend_version = backend_version ) , 
** qobj_header } 
qobj_header = QobjHeader ( ** { k : v for k , v in qobj_header . items ( ) if v is not None } ) 
run_config_dict = dict ( shots = shots , 
qubit_lo_freq = qubit_lo_freq , 
meas_lo_freq = meas_lo_freq , 
** run_config ) 
run_config = RunConfig ( ** { k : v for k , v in run_config_dict . items ( ) if v is not None } ) 
return qobj_id , qobj_header , run_config 
~~ def _expand_parameters ( circuits , run_config ) : 
parameter_binds = run_config . parameter_binds 
if parameter_binds or any ( circuit . parameters for circuit in circuits ) : 
~~~ all_bind_parameters = [ bind . keys ( ) 
for bind in parameter_binds ] 
all_circuit_parameters = [ circuit . parameters for circuit in circuits ] 
unique_parameters = set ( param 
for param_list in all_bind_parameters + all_circuit_parameters 
for param in param_list ) 
if not all_bind_parameters or not all_circuit_parameters or any ( unique_parameters != bind_params for bind_params in all_bind_parameters ) or any ( unique_parameters != parameters for parameters in all_circuit_parameters ) : 
~~ circuits = [ circuit . bind_parameters ( binds ) 
for circuit in circuits 
for binds in parameter_binds ] 
run_config = copy . deepcopy ( run_config ) 
run_config . parameter_binds = [ ] 
~~ return circuits , run_config 
~~ def unset_qiskit_logger ( ) : 
qiskit_logger = logging . getLogger ( 'qiskit' ) 
for handler in qiskit_logger . handlers : 
~~~ qiskit_logger . removeHandler ( handler ) 
~~ ~~ def iplot_state_hinton ( rho , figsize = None ) : 
real = [ ] 
imag = [ ] 
for xvalue in rho : 
~~~ row_real = [ ] 
col_imag = [ ] 
for value_real in xvalue . real : 
~~~ row_real . append ( float ( value_real ) ) 
~~ real . append ( row_real ) 
for value_imag in xvalue . imag : 
~~~ col_imag . append ( float ( value_imag ) ) 
~~ imag . append ( col_imag ) 
'executions' : [ { 'data' : real } , { 'data' : imag } ] , 
~~ def process_fidelity ( channel1 , channel2 , require_cptp = True ) : 
is_cptp1 = None 
is_cptp2 = None 
if isinstance ( channel1 , ( list , np . ndarray ) ) : 
~~~ channel1 = Operator ( channel1 ) 
if require_cptp : 
~~~ is_cptp1 = channel1 . is_unitary ( ) 
~~ ~~ if isinstance ( channel2 , ( list , np . ndarray ) ) : 
~~~ channel2 = Operator ( channel2 ) 
~~~ is_cptp2 = channel2 . is_unitary ( ) 
~~ ~~ s1 = SuperOp ( channel1 ) 
s2 = SuperOp ( channel2 ) 
~~~ if is_cptp1 is None : 
~~~ is_cptp1 = s1 . is_cptp ( ) 
~~ if not is_cptp1 : 
~~ if is_cptp2 is None : 
~~~ is_cptp2 = s2 . is_cptp ( ) 
~~ if not is_cptp2 : 
~~ ~~ input_dim1 , output_dim1 = s1 . dim 
input_dim2 , output_dim2 = s2 . dim 
if input_dim1 != output_dim1 or input_dim2 != output_dim2 : 
~~ if input_dim1 != input_dim2 : 
~~ fidelity = np . trace ( s1 . compose ( s2 . adjoint ( ) ) . data ) / ( input_dim1 ** 2 ) 
return fidelity 
~~ def input ( self , data ) : 
self . data = data 
self . lexer . input ( data ) 
~~ def pop ( self ) : 
self . lexer = self . stack . pop ( ) 
self . filename = self . lexer . qasm_file 
self . lineno = self . lexer . qasm_line 
~~ def push ( self , filename ) : 
self . lexer . qasm_file = self . filename 
self . lexer . qasm_line = self . lineno 
self . stack . append ( self . lexer ) 
self . __mklexer__ ( filename ) 
~~ def t_INCLUDE ( self , t ) : 
~~~ 'include' 
next_token = self . lexer . token ( ) 
lineno = next_token . lineno 
if isinstance ( next_token . value , str ) : 
~~~ incfile = next_token . value . strip ( \ ) 
~~ if incfile in CORE_LIBS : 
~~~ incfile = os . path . join ( CORE_LIBS_PATH , incfile ) 
~~ next_token = self . lexer . token ( ) 
if next_token is None or next_token . value != ';' : 
~~~ raise QasmError ( \ , str ( lineno ) ) 
~~ if not os . path . exists ( incfile ) : 
~~~ raise QasmError ( 
( incfile , str ( next_token . lineno ) , self . filename ) ) 
~~ self . push ( incfile ) 
return self . lexer . token ( ) 
~~ def t_ID ( self , t ) : 
~~~ r'[a-z][a-zA-Z0-9_]*' 
t . type = self . reserved . get ( t . value , 'ID' ) 
if t . type == 'ID' : 
~~~ t . value = node . Id ( t . value , self . lineno , self . filename ) 
~~ return t 
~~ def t_newline ( self , t ) : 
~~~ r'\\n+' 
self . lineno += len ( t . value ) 
t . lexer . lineno = self . lineno 
~~ def create_from ( cls , backend ) : 
backend_config = backend . configuration ( ) 
~~ except ModelValidationError : 
qubit_freq_est = backend_config . defaults [ 'qubit_freq_est' ] , 
meas_freq_est = backend_config . defaults [ 'meas_freq_est' ] 
~~ n_qubits = backend_config . n_qubits 
n_registers = backend_config . n_registers 
n_uchannels = backend_config . n_uchannels 
if n_uchannels > 0 and n_uchannels != n_qubits : 
~~ qubit_lo_freqs = backend_default . qubit_freq_est 
qubit_lo_ranges = backend_config . qubit_lo_range 
meas_lo_freqs = backend_default . meas_freq_est 
meas_lo_ranges = backend_config . meas_lo_range 
drives = [ 
DriveChannel ( i , qubit_lo_freqs [ i ] , tuple ( qubit_lo_ranges [ i ] ) ) 
for i in range ( n_qubits ) 
measures = [ 
MeasureChannel ( i , meas_lo_freqs [ i ] , tuple ( meas_lo_ranges [ i ] ) ) 
acquires = [ AcquireChannel ( i ) for i in range ( n_qubits ) ] 
controls = [ ControlChannel ( i ) for i in range ( n_uchannels ) ] 
~~~ qubit = Qubit ( i , 
drive_channels = [ drives [ i ] ] , 
control_channels = None if n_uchannels == 0 else controls [ i ] , 
measure_channels = [ measures [ i ] ] , 
acquire_channels = [ acquires [ i ] ] ) 
qubits . append ( qubit ) 
~~ registers = [ RegisterSlot ( i ) for i in range ( n_registers ) ] 
mem_slots = [ MemorySlot ( i ) for i in range ( len ( qubits ) ) ] 
return DeviceSpecification ( qubits , registers , mem_slots ) 
~~~ new_dag . add_qreg ( qreg ) 
~~~ new_dag . add_creg ( creg ) 
~~ global_index_map = { } 
~~~ if not isinstance ( wire [ 0 ] , QuantumRegister ) : 
~~ global_qregs = list ( dag . qregs . values ( ) ) 
global_index_map [ wire ] = global_qregs . index ( wire [ 0 ] ) + wire [ 1 ] 
~~ blocks = self . property_set [ 'block_list' ] 
nodes_seen = set ( ) 
for node in dag . topological_op_nodes ( ) : 
~~~ if node in nodes_seen or node . type == 'in' or node . type == 'out' : 
~~ if blocks and node in blocks [ 0 ] : 
~~~ block = blocks [ 0 ] 
block_qargs = set ( ) 
for nd in block : 
~~~ block_qargs |= set ( nd . qargs ) 
~~ block_width = len ( block_qargs ) 
q = QuantumRegister ( block_width ) 
subcirc = QuantumCircuit ( q ) 
block_index_map = self . _block_qargs_to_indices ( block_qargs , 
global_index_map ) 
~~~ nodes_seen . add ( nd ) 
subcirc . append ( nd . op , [ q [ block_index_map [ i ] ] for i in nd . qargs ] ) 
new_dag . apply_operation_back ( 
unitary , sorted ( block_qargs , key = lambda x : block_index_map [ x ] ) ) 
del blocks [ 0 ] 
~~~ for block in blocks [ 1 : ] : 
~~~ if node in block : 
~~~ nodes_seen . add ( node ) 
new_dag . apply_operation_back ( node . op , node . qargs , node . cargs ) 
~~ ~~ ~~ return new_dag 
~~ def _block_qargs_to_indices ( self , block_qargs , global_index_map ) : 
block_indices = [ global_index_map [ q ] for q in block_qargs ] 
ordered_block_indices = sorted ( block_indices ) 
block_positions = { q : ordered_block_indices . index ( global_index_map [ q ] ) 
for q in block_qargs } 
return block_positions 
~~ def get_bound_method ( self , instruction ) : 
~~~ return self . _bound_instructions [ type ( instruction ) ] 
~~ ~~ def convert_acquire ( self , shift , instruction ) : 
meas_level = self . _run_config . get ( 'meas_level' , 2 ) 
command_dict = { 
'name' : 'acquire' , 
't0' : shift + instruction . start_time , 
'duration' : instruction . duration , 
'qubits' : [ q . index for q in instruction . acquires ] , 
'memory_slot' : [ m . index for m in instruction . mem_slots ] 
if meas_level == 2 : 
~~~ if instruction . command . discriminator : 
~~~ command_dict . update ( { 
'discriminators' : [ 
QobjMeasurementOption ( 
name = instruction . command . discriminator . name , 
params = instruction . command . discriminator . params ) 
~~ command_dict . update ( { 
'register_slot' : [ regs . index for regs in instruction . reg_slots ] 
~~ if meas_level >= 1 : 
~~~ if instruction . command . kernel : 
'kernels' : [ 
name = instruction . command . kernel . name , 
params = instruction . command . kernel . params ) 
~~ ~~ return self . _qobj_model ( ** command_dict ) 
~~ def convert_frame_change ( self , shift , instruction ) : 
'name' : 'fc' , 
'ch' : instruction . channels [ 0 ] . name , 
'phase' : instruction . command . phase 
return self . _qobj_model ( ** command_dict ) 
~~ def convert_persistent_value ( self , shift , instruction ) : 
'name' : 'pv' , 
'val' : instruction . command . value 
~~ def convert_drive ( self , shift , instruction ) : 
'name' : instruction . command . name , 
'ch' : instruction . channels [ 0 ] . name 
~~ def convert_snapshot ( self , shift , instruction ) : 
'name' : 'snapshot' , 
'label' : instruction . name , 
'type' : instruction . type 
~~ def _update_annotations ( discretized_pulse : Callable ) -> Callable : 
undecorated_annotations = list ( discretized_pulse . __annotations__ . items ( ) ) 
decorated_annotations = undecorated_annotations [ 1 : ] 
decorated_annotations . insert ( 0 , ( 'duration' , int ) ) 
discretized_pulse . __annotations__ = dict ( decorated_annotations ) 
return discretized_pulse 
~~ def _update_docstring ( discretized_pulse : Callable , sampler_inst : Callable ) -> Callable : 
wrapped_docstring = pydoc . render_doc ( discretized_pulse , '%s' ) 
header , body = wrapped_docstring . split ( '\\n' , 1 ) 
wrapped_docstring = header + body 
sampler_name = sampler_inst . __name__ , 
continuous_doc = wrapped_docstring ) 
discretized_pulse . __doc__ = updated_ds 
~~ def sampler ( sample_function : Callable ) -> Callable : 
def generate_sampler ( continuous_pulse : Callable ) -> Callable : 
@ functools . wraps ( continuous_pulse ) 
def call_sampler ( duration : int , * args , ** kwargs ) -> commands . SamplePulse : 
sampled_pulse = sample_function ( continuous_pulse , duration , * args , ** kwargs ) 
return np . asarray ( sampled_pulse , dtype = np . complex_ ) 
~~ call_sampler = _update_annotations ( call_sampler ) 
call_sampler = _update_docstring ( call_sampler , sample_function ) 
call_sampler . __dict__ . pop ( '__wrapped__' ) 
return commands . functional_pulse ( call_sampler ) 
~~ return generate_sampler 
~~ def filter_backends ( backends , filters = None , ** kwargs ) : 
def _match_all ( obj , criteria ) : 
return all ( getattr ( obj , key_ , None ) == value_ for 
key_ , value_ in criteria . items ( ) ) 
~~ configuration_filters = { } 
status_filters = { } 
for key , value in kwargs . items ( ) : 
~~~ if all ( key in backend . configuration ( ) for backend in backends ) : 
~~~ configuration_filters [ key ] = value 
~~~ status_filters [ key ] = value 
~~ ~~ if configuration_filters : 
~~~ backends = [ b for b in backends if 
_match_all ( b . configuration ( ) , configuration_filters ) ] 
~~ if status_filters : 
_match_all ( b . status ( ) , status_filters ) ] 
~~ backends = list ( filter ( filters , backends ) ) 
return backends 
~~ def resolve_backend_name ( name , backends , deprecated , aliased ) : 
available = [ backend . name ( ) for backend in backends ] 
resolved_name = deprecated . get ( name , aliased . get ( name , name ) ) 
if isinstance ( resolved_name , list ) : 
~~~ resolved_name = next ( ( b for b in resolved_name if b in available ) , "" ) 
~~ if resolved_name not in available : 
~~ if name in deprecated : 
~~ return resolved_name 
~~ def dag_to_circuit ( dag ) : 
qregs = collections . OrderedDict ( ) 
~~~ qreg_tmp = QuantumRegister ( qreg . size , name = qreg . name ) 
qregs [ qreg . name ] = qreg_tmp 
~~ cregs = collections . OrderedDict ( ) 
~~~ creg_tmp = ClassicalRegister ( creg . size , name = creg . name ) 
cregs [ creg . name ] = creg_tmp 
~~ name = dag . name or None 
circuit = QuantumCircuit ( * qregs . values ( ) , * cregs . values ( ) , name = name ) 
~~~ qubits = [ ] 
for qubit in node . qargs : 
~~~ qubits . append ( qregs [ qubit [ 0 ] . name ] [ qubit [ 1 ] ] ) 
for clbit in node . cargs : 
~~~ clbits . append ( cregs [ clbit [ 0 ] . name ] [ clbit [ 1 ] ] ) 
~~ if node . condition is None : 
~~~ control = ( node . condition [ 0 ] , node . condition [ 1 ] ) 
~~ inst = node . op . copy ( ) 
inst . control = control 
circuit . append ( inst , qubits , clbits ) 
~~ def make_dict_observable ( matrix_observable ) : 
dict_observable = { } 
observable = np . array ( matrix_observable ) 
observable_size = len ( observable ) 
observable_bits = int ( np . ceil ( np . log2 ( observable_size ) ) ) 
binary_formater = '0{}b' . format ( observable_bits ) 
if observable . ndim == 2 : 
~~~ observable = observable . diagonal ( ) 
~~ for state_no in range ( observable_size ) : 
~~~ state_str = format ( state_no , binary_formater ) 
dict_observable [ state_str ] = observable [ state_no ] 
~~ return dict_observable 
~~ def update_symtab ( self , obj ) : 
if obj . name in self . current_symtab : 
~~~ prev = self . current_symtab [ obj . name ] 
+ obj . name + "\ , str ( obj . line ) 
~~ self . current_symtab [ obj . name ] = obj 
~~ def verify_declared_bit ( self , obj ) : 
if obj . name not in self . current_symtab : 
+ "\ , 
str ( obj . line ) , 'file' , obj . file ) 
~~ sym = self . current_symtab [ obj . name ] 
if not ( sym . type == 'id' and sym . is_bit ) : 
~~~ raise QasmError ( "Bit" , obj . name , 
~~ ~~ def verify_exp_list ( self , obj ) : 
if obj . children is not None : 
~~~ for children in obj . children : 
~~~ if isinstance ( children , node . Id ) : 
~~~ if children . name in self . external_functions : 
~~ if children . name not in self . current_symtab : 
+ "\ 
"file" , children . file ) 
~~~ if hasattr ( children , "children" ) : 
~~~ self . verify_exp_list ( children ) 
~~ ~~ ~~ ~~ ~~ def verify_as_gate ( self , obj , bitlist , arglist = None ) : 
if obj . name not in self . global_symtab : 
+ "\ , str ( obj . line ) , 'file' , obj . file ) 
~~ g_sym = self . global_symtab [ obj . name ] 
if not ( g_sym . type == 'gate' or g_sym . type == 'opaque' ) : 
~~~ raise QasmError ( "\ + obj . name + "\ 
~~ if g_sym . n_bits ( ) != bitlist . size ( ) : 
+ "\ , str ( bitlist . size ( ) ) , 
str ( g_sym . n_bits ( ) ) , "qubits" , "line" , 
~~ if arglist : 
~~~ if g_sym . n_args ( ) != arglist . size ( ) : 
+ "\ , str ( arglist . size ( ) ) , 
str ( g_sym . n_args ( ) ) , "qubits" , "line" , 
~~~ if g_sym . n_args ( ) > 0 : 
~~ ~~ ~~ def verify_reg ( self , obj , object_type ) : 
'file' , obj . file ) 
if g_sym . type != object_type : 
+ object_type + "\ 
+ g_sym . type + "\ , "line" , str ( obj . line ) , 
"file" , obj . file ) 
~~ if obj . type == 'indexed_id' : 
~~~ bound = g_sym . index 
ndx = obj . index 
if ndx < 0 or ndx >= bound : 
+ "\ , str ( ndx ) , 
~~ ~~ ~~ def verify_reg_list ( self , obj , object_type ) : 
for children in obj . children : 
~~~ self . verify_reg ( children , object_type ) 
~~ ~~ def id_tuple_list ( self , id_node ) : 
if id_node . type != "id" : 
~~ bit_list = [ ] 
~~~ g_sym = self . current_symtab [ id_node . name ] 
~~~ g_sym = self . global_symtab [ id_node . name ] 
~~ if g_sym . type == "qreg" or g_sym . type == "creg" : 
~~~ for idx in range ( g_sym . index ) : 
~~~ bit_list . append ( ( id_node . name , idx ) ) 
~~~ bit_list . append ( ( id_node . name , - 1 ) ) 
~~ return bit_list 
~~ def verify_distinct ( self , list_of_nodes ) : 
bit_list = [ ] 
line_number = - 1 
filename = "" 
for node_ in list_of_nodes : 
~~~ if node_ . type == "id" : 
~~~ bit_list . extend ( self . id_tuple_list ( node_ ) ) 
line_number = node_ . line 
filename = node_ . file 
~~ elif node_ . type == "indexed_id" : 
~~~ bit_list . append ( ( node_ . name , node_ . index ) ) 
~~ elif node_ . type == "primary_list" : 
~~~ for child in node_ . children : 
~~~ if child . type == "id" : 
~~~ bit_list . extend ( self . id_tuple_list ( child ) ) 
~~~ bit_list . append ( ( child . name , child . index ) ) 
~~ line_number = child . line 
filename = child . file 
~~ ~~ elif node_ . type == "id_list" : 
line_number = child . line 
~~ ~~ if len ( bit_list ) != len ( set ( bit_list ) ) : 
% ( line_number , filename ) ) 
~~ ~~ def p_statement ( self , program ) : 
if len ( program ) > 2 : 
~~~ if program [ 2 ] != ';' : 
+ "received" , str ( program [ 2 ] . value ) ) 
~~ ~~ program [ 0 ] = program [ 1 ] 
~~ def p_indexed_id ( self , program ) : 
if len ( program ) == 4 : 
str ( program [ 3 ] . value ) ) 
~~ if program [ 4 ] != ']' : 
str ( program [ 4 ] . value ) ) 
~~ program [ 0 ] = node . IndexedId ( [ program [ 1 ] , node . Int ( program [ 3 ] ) ] ) 
~~ def p_gate_id_list_0 ( self , program ) : 
program [ 0 ] = node . IdList ( [ program [ 1 ] ] ) 
self . update_symtab ( program [ 1 ] ) 
~~ def p_gate_id_list_1 ( self , program ) : 
program [ 0 ] = program [ 1 ] 
program [ 0 ] . add_child ( program [ 3 ] ) 
self . update_symtab ( program [ 3 ] ) 
~~ def p_bit_list_0 ( self , program ) : 
program [ 1 ] . is_bit = True 
~~ def p_bit_list_1 ( self , program ) : 
program [ 3 ] . is_bit = True 
~~ def p_decl ( self , program ) : 
~~ def p_qreg_decl ( self , program ) : 
program [ 0 ] = node . Qreg ( [ program [ 2 ] ] ) 
if program [ 2 ] . name in self . external_functions : 
~~ if program [ 2 ] . index == 0 : 
~~ self . update_symtab ( program [ 0 ] ) 
~~ def p_creg_decl ( self , program ) : 
program [ 0 ] = node . Creg ( [ program [ 2 ] ] ) 
~~ def p_gate_decl_1 ( self , program ) : 
program [ 0 ] = node . Gate ( [ program [ 2 ] , program [ 6 ] , program [ 7 ] ] ) 
~~ self . pop_scope ( ) 
self . update_symtab ( program [ 0 ] ) 
~~ def p_gate_body_0 ( self , program ) : 
if program [ 2 ] != '}' : 
+ str ( program [ 2 ] . value ) + "\ ) 
~~ program [ 0 ] = node . GateBody ( None ) 
~~ def p_unitary_op_0 ( self , program ) : 
program [ 0 ] = node . UniversalUnitary ( [ program [ 3 ] , program [ 5 ] ] ) 
self . verify_reg ( program [ 5 ] , 'qreg' ) 
self . verify_exp_list ( program [ 3 ] ) 
~~ def p_unitary_op_1 ( self , program ) : 
program [ 0 ] = node . Cnot ( [ program [ 2 ] , program [ 4 ] ] ) 
self . verify_reg ( program [ 2 ] , 'qreg' ) 
self . verify_reg ( program [ 4 ] , 'qreg' ) 
self . verify_distinct ( [ program [ 2 ] , program [ 4 ] ] ) 
~~ def p_unitary_op_2 ( self , program ) : 
program [ 0 ] = node . CustomUnitary ( [ program [ 1 ] , program [ 2 ] ] ) 
self . verify_as_gate ( program [ 1 ] , program [ 2 ] ) 
self . verify_reg_list ( program [ 2 ] , 'qreg' ) 
self . verify_distinct ( [ program [ 2 ] ] ) 
~~ def p_unitary_op_3 ( self , program ) : 
program [ 0 ] = node . CustomUnitary ( [ program [ 1 ] , program [ 4 ] ] ) 
self . verify_as_gate ( program [ 1 ] , program [ 4 ] ) 
self . verify_reg_list ( program [ 4 ] , 'qreg' ) 
self . verify_distinct ( [ program [ 4 ] ] ) 
~~ def p_unitary_op_4 ( self , program ) : 
program [ 0 ] = node . CustomUnitary ( [ program [ 1 ] , program [ 3 ] , program [ 5 ] ] ) 
self . verify_as_gate ( program [ 1 ] , program [ 5 ] , arglist = program [ 3 ] ) 
self . verify_reg_list ( program [ 5 ] , 'qreg' ) 
self . verify_distinct ( [ program [ 5 ] ] ) 
~~ def p_gate_op_0 ( self , program ) : 
self . verify_declared_bit ( program [ 5 ] ) 
~~ def p_gate_op_1 ( self , program ) : 
self . verify_declared_bit ( program [ 2 ] ) 
self . verify_declared_bit ( program [ 4 ] ) 
~~ def p_gate_op_2 ( self , program ) : 
self . verify_bit_list ( program [ 2 ] ) 
~~ def p_gate_op_3 ( self , program ) : 
self . verify_bit_list ( program [ 4 ] ) 
~~ def p_gate_op_4 ( self , program ) : 
self . verify_bit_list ( program [ 5 ] ) 
~~ def p_gate_op_5 ( self , program ) : 
program [ 0 ] = node . Barrier ( [ program [ 2 ] ] ) 
~~ def p_opaque_0 ( self , program ) : 
program [ 0 ] = node . Opaque ( [ program [ 2 ] , program [ 4 ] ] ) 
~~ def p_opaque_1 ( self , program ) : 
program [ 0 ] = node . Opaque ( [ program [ 2 ] , program [ 6 ] ] ) 
self . pop_scope ( ) 
~~ def p_measure ( self , program ) : 
program [ 0 ] = node . Measure ( [ program [ 2 ] , program [ 4 ] ] ) 
self . verify_reg ( program [ 4 ] , 'creg' ) 
~~ def p_barrier ( self , program ) : 
~~ def p_reset ( self , program ) : 
program [ 0 ] = node . Reset ( [ program [ 2 ] ] ) 
~~ def p_if ( self , program ) : 
if len ( program ) == 3 : 
~~ if len ( program ) == 5 : 
~~ if len ( program ) == 6 : 
~~ if len ( program ) == 7 : 
~~ if program [ 7 ] . type == 'if' : 
~~ if program [ 7 ] . type == 'barrier' : 
~~ program [ 0 ] = node . If ( [ program [ 3 ] , node . Int ( program [ 5 ] ) , program [ 7 ] ] ) 
~~ def p_unary_6 ( self , program ) : 
if program [ 1 ] . name not in self . external_functions : 
str ( program [ 1 ] . name ) ) 
~~ program [ 0 ] = node . External ( [ program [ 1 ] , program [ 3 ] ] ) 
~~ def p_expression_1 ( self , program ) : 
program [ 0 ] = node . Prefix ( [ node . UnaryOperator ( program [ 1 ] ) , program [ 2 ] ] ) 
~~ def p_expression_0 ( self , program ) : 
program [ 0 ] = node . BinaryOp ( [ node . BinaryOperator ( program [ 2 ] ) , 
program [ 1 ] , program [ 3 ] ] ) 
~~ def find_column ( self , input_ , token ) : 
if token is None : 
~~ last_cr = input_ . rfind ( '\\n' , 0 , token . lexpos ) 
if last_cr < 0 : 
~~~ last_cr = 0 
~~ column = ( token . lexpos - last_cr ) + 1 
return column 
~~ def get_tokens ( self ) : 
~~~ token = self . lexer . token ( ) 
~~ yield token 
~~ ~~ except QasmError as e : 
~~ ~~ def parse_debug ( self , val ) : 
if val is True : 
~~~ self . parse_deb = True 
~~ elif val is False : 
~~~ self . parse_deb = False 
+ "\ ) 
~~ ~~ def parse ( self , data ) : 
self . parser . parse ( data , lexer = self . lexer , debug = self . parse_deb ) 
if self . qasm is None : 
~~ return self . qasm 
~~ def run ( self , data ) : 
ast = self . parser . parse ( data , debug = True ) 
self . parser . parse ( data , debug = True ) 
ast . to_string ( 0 ) 
if self . _filename : 
~~~ with open ( self . _filename ) as ifile : 
~~~ self . _data = ifile . read ( ) 
~~ ~~ with QasmParser ( self . _filename ) as qasm_p : 
~~~ return qasm_p . get_tokens ( ) 
~~ ~~ def parse ( self ) : 
~~~ qasm_p . parse_debug ( False ) 
return qasm_p . parse ( self . _data ) 
~~ ~~ def crz ( self , theta , ctl , tgt ) : 
return self . append ( CrzGate ( theta ) , [ ctl , tgt ] , [ ] ) 
~~ def basis_state ( str_state , num ) : 
n = int ( str_state , 2 ) 
if num >= len ( str_state ) : 
~~~ state = np . zeros ( 1 << num , dtype = complex ) 
state [ n ] = 1 
~~ ~~ def projector ( state , flatten = False ) : 
density_matrix = np . outer ( state . conjugate ( ) , state ) 
~~ return density_matrix 
~~ def purity ( state ) : 
~~~ return 1.0 
~~ return np . real ( np . trace ( rho . dot ( rho ) ) ) 
if self . arguments is not None : 
~~~ string += "(" + self . arguments . qasm ( prec ) + ")" 
string += "{\\n" + self . body . qasm ( prec ) + "}" 
return string 
self . property_set [ 'commutation_set' ] [ wire_name ] = [ ] 
~~ for node in dag . topological_op_nodes ( ) : 
~~~ for ( _ , _ , edge_data ) in dag . edges ( node ) : 
~~~ edge_name = edge_data [ 'name' ] 
self . property_set [ 'commutation_set' ] [ ( node , edge_name ) ] = - 1 
~~ ~~ for wire in dag . wires : 
for current_gate in dag . nodes_on_wire ( wire ) : 
~~~ current_comm_set = self . property_set [ 'commutation_set' ] [ wire_name ] 
if not current_comm_set : 
~~~ current_comm_set . append ( [ current_gate ] ) 
~~ if current_gate not in current_comm_set [ - 1 ] : 
~~~ prev_gate = current_comm_set [ - 1 ] [ - 1 ] 
if _commute ( current_gate , prev_gate ) : 
~~~ current_comm_set [ - 1 ] . append ( current_gate ) 
~~ ~~ temp_len = len ( current_comm_set ) 
self . property_set [ 'commutation_set' ] [ ( current_gate , wire_name ) ] = temp_len - 1 
~~ ~~ ~~ def backend_widget ( backend ) : 
name = widgets . HTML ( value = "<h4>{name}</h4>" . format ( name = backend . name ( ) ) , 
layout = widgets . Layout ( ) ) 
qubit_count = widgets . HTML ( value = "<h5><b>{qubits}</b></h5>" . format ( qubits = n_qubits ) , 
layout = widgets . Layout ( justify_content = 'center' ) ) 
cmap = widgets . Output ( layout = widgets . Layout ( min_width = '250px' , max_width = '250px' , 
max_height = '250px' , 
min_height = '250px' , 
justify_content = 'center' , 
with cmap : 
~~~ _cmap_fig = plot_gate_map ( backend , 
label_qubits = False ) 
if _cmap_fig is not None : 
~~~ display ( _cmap_fig ) 
plt . close ( _cmap_fig ) 
~~ ~~ pending = generate_jobs_pending_widget ( ) 
is_oper = widgets . HTML ( value = "<h5></h5>" , 
least_busy = widgets . HTML ( value = "<h5></h5>" , 
t1_units = props [ 'qubits' ] [ 0 ] [ 0 ] [ 'unit' ] 
avg_t1 = round ( sum ( [ q [ 0 ] [ 'value' ] for q in props [ 'qubits' ] ] ) / n_qubits , 1 ) 
t2_units = props [ 'qubits' ] [ 0 ] [ 1 ] [ 'unit' ] 
avg_t2 = round ( sum ( [ q [ 1 ] [ 'value' ] for q in props [ 'qubits' ] ] ) / n_qubits , 1 ) 
out = widgets . VBox ( [ name , cmap , qubit_count , pending , 
least_busy , is_oper , t1_widget , t2_widget ] , 
layout = widgets . Layout ( display = 'inline-flex' , 
flex_flow = 'column' , 
out . _is_alive = True 
~~ def update_backend_info ( self , interval = 60 ) : 
my_thread = threading . currentThread ( ) 
current_interval = 0 
started = False 
all_dead = False 
stati = [ None ] * len ( self . _backends ) 
while getattr ( my_thread , "do_run" , True ) and not all_dead : 
~~~ if current_interval == interval or started is False : 
~~~ for ind , back in enumerate ( self . _backends ) : 
~~~ _value = self . children [ ind ] . children [ 2 ] . value 
_head = _value . split ( '<b>' ) [ 0 ] 
~~~ _status = back . status ( ) 
stati [ ind ] = _status 
~~~ self . children [ ind ] . children [ 2 ] . value = _value . replace ( 
self . children [ ind ] . _is_alive = False 
~~~ self . children [ ind ] . _is_alive = True 
self . children [ ind ] . children [ 2 ] . value = _value . replace ( 
_head , "<h5>" ) 
~~ ~~ idx = list ( range ( len ( self . _backends ) ) ) 
~~ ~~ for var in idx : 
~~~ if var == least_pending_idx : 
~~ self . children [ var ] . children [ 3 ] . children [ 1 ] . value = pending [ var ] 
self . children [ var ] . children [ 3 ] . children [ 1 ] . max = max ( 
self . children [ var ] . children [ 3 ] . children [ 1 ] . max , pending [ var ] + 10 ) 
if stati [ var ] . operational : 
~~ ~~ started = True 
~~ time . sleep ( 1 ) 
all_dead = not any ( [ wid . _is_alive for wid in self . children ] ) 
current_interval += 1 
~~ ~~ def generate_jobs_pending_widget ( ) : 
pbar = widgets . IntProgress ( 
value = 0 , 
min = 0 , 
max = 50 , 
description = '' , 
orientation = 'horizontal' , layout = widgets . Layout ( max_width = '180px' ) ) 
pbar . style . bar_color = '#71cddd' 
pbar_current = widgets . Label ( 
value = str ( pbar . value ) , layout = widgets . Layout ( min_width = 'auto' ) ) 
pbar_max = widgets . Label ( 
value = str ( pbar . max ) , layout = widgets . Layout ( min_width = 'auto' ) ) 
def _on_max_change ( change ) : 
~~~ pbar_max . value = str ( change [ 'new' ] ) 
~~ def _on_val_change ( change ) : 
~~~ pbar_current . value = str ( change [ 'new' ] ) 
~~ pbar . observe ( _on_max_change , names = 'max' ) 
pbar . observe ( _on_val_change , names = 'value' ) 
jobs_widget = widgets . HBox ( [ pbar_current , pbar , pbar_max ] , 
layout = widgets . Layout ( max_width = '250px' , 
min_width = '250px' , 
justify_content = 'center' ) ) 
return jobs_widget 
~~~ self . layout = Layout . generate_trivial_layout ( * dag . qregs . values ( ) ) 
~~ for layer in dag . serial_layers ( ) : 
for cnot_node in subdag . named_nodes ( 'cx' , 'CX' ) : 
~~~ control = cnot_node . qargs [ 0 ] 
target = cnot_node . qargs [ 1 ] 
physical_q0 = self . layout [ control ] 
physical_q1 = self . layout [ target ] 
~~ if ( physical_q0 , physical_q1 ) not in self . coupling_map . get_edges ( ) : 
~~~ if control [ 0 ] not in subdag . qregs . values ( ) : 
~~~ subdag . add_qreg ( control [ 0 ] ) 
~~ if target [ 0 ] not in subdag . qregs . values ( ) : 
~~~ subdag . add_qreg ( target [ 0 ] ) 
~~ subdag . apply_operation_back ( HGate ( ) , [ target ] , [ ] ) 
subdag . apply_operation_back ( HGate ( ) , [ control ] , [ ] ) 
subdag . apply_operation_front ( HGate ( ) , [ target ] , [ ] ) 
subdag . apply_operation_front ( HGate ( ) , [ control ] , [ ] ) 
cnot_node . qargs [ 0 ] , cnot_node . qargs [ 1 ] = target , control 
~~ ~~ new_dag . extend_back ( subdag ) 
cx_runs = dag . collect_runs ( [ "cx" ] ) 
for cx_run in cx_runs : 
~~~ partition = [ ] 
chunk = [ ] 
for i in range ( len ( cx_run ) - 1 ) : 
~~~ chunk . append ( cx_run [ i ] ) 
qargs0 = cx_run [ i ] . qargs 
qargs1 = cx_run [ i + 1 ] . qargs 
if qargs0 != qargs1 : 
~~~ partition . append ( chunk ) 
~~ ~~ chunk . append ( cx_run [ - 1 ] ) 
partition . append ( chunk ) 
for chunk in partition : 
~~~ if len ( chunk ) % 2 == 0 : 
~~~ for n in chunk : 
~~~ dag . remove_op_node ( n ) 
~~~ for n in chunk [ 1 : ] : 
~~ ~~ ~~ ~~ return dag 
~~ def get_backend ( self , name = None , ** kwargs ) : 
backends = self . backends ( name , ** kwargs ) 
if len ( backends ) > 1 : 
~~ elif not backends : 
~~ return backends [ 0 ] 
~~ def _bipartite_shape ( self ) : 
return ( self . _input_dim , self . _output_dim , self . _input_dim , 
self . _output_dim ) 
return Choi ( np . conj ( self . _data ) , self . input_dims ( ) , self . output_dims ( ) ) 
d_in , d_out = self . dim 
data = np . reshape ( self . _data , ( d_in , d_out , d_in , d_out ) ) 
data = np . transpose ( data , ( 1 , 0 , 3 , 2 ) ) 
data = np . reshape ( data , ( d_in * d_out , d_in * d_out ) ) 
return Choi ( 
data , input_dims = self . output_dims ( ) , output_dims = self . input_dims ( ) ) 
~~~ return Choi ( 
~~ if not isinstance ( other , Choi ) : 
~~~ other = Choi ( other ) 
~~~ first = np . reshape ( other . _data , other . _bipartite_shape ) 
second = np . reshape ( self . _data , self . _bipartite_shape ) 
input_dims = other . input_dims ( ) 
output_dims = self . output_dims ( ) 
~~~ first = np . reshape ( self . _data , self . _bipartite_shape ) 
second = np . reshape ( other . _data , other . _bipartite_shape ) 
input_dims = self . input_dims ( ) 
output_dims = other . output_dims ( ) 
~~ data = np . reshape ( 
np . einsum ( 'iAjB,AkBl->ikjl' , first , second ) , 
( input_dim * output_dim , input_dim * output_dim ) ) 
return Choi ( data , input_dims , output_dims ) 
~~ return Choi ( SuperOp ( self ) . power ( n ) ) 
~~ return np . einsum ( 'AB,AiBj->ij' , state , 
np . reshape ( self . _data , self . _bipartite_shape ) ) 
if not isinstance ( other , Choi ) : 
data = _bipartite_tensor ( 
other . data , 
self . _data , 
shape1 = other . _bipartite_shape , 
shape2 = self . _bipartite_shape ) 
shape1 = self . _bipartite_shape , 
shape2 = other . _bipartite_shape ) 
~~ return Choi ( data , input_dims , output_dims ) 
~~ def _get_register_specs ( bit_labels ) : 
it = itertools . groupby ( bit_labels , operator . itemgetter ( 0 ) ) 
for register_name , sub_it in it : 
~~~ yield register_name , max ( ind [ 1 ] for ind in sub_it ) + 1 
~~ ~~ def _truncate_float ( matchobj , format_str = '0.2g' ) : 
if matchobj . group ( 0 ) : 
~~~ return format ( float ( matchobj . group ( 0 ) ) , format_str ) 
~~ return '' 
~~ def latex ( self , aliases = None ) : 
self . _initialize_latex_array ( aliases ) 
self . _build_latex_array ( aliases ) 
beamer_line = "\\\\usepackage[size=custom,height=%d,width=%d,scale=%.1f]{beamerposter}\\n" 
output = io . StringIO ( ) 
output . write ( header_1 ) 
output . write ( beamer_line % self . _get_beamer_page ( ) ) 
output . write ( header_2 ) 
output . write ( qcircuit_line % 
( self . column_separation , self . row_separation ) ) 
for i in range ( self . img_width ) : 
for j in range ( self . img_depth + 1 ) : 
~~~ cell_str = self . _latex [ i ] [ j ] 
if 'barrier' in cell_str : 
~~~ output . write ( cell_str ) 
~~~ cell_str = re . sub ( r'[-+]?\\d*\\.\\d{2,}|\\d{2,}' , 
_truncate_float , 
cell_str ) 
output . write ( cell_str ) 
~~ if j != self . img_depth : 
~~~ output . write ( r'\\\\' + '\\n' ) 
output . write ( '\\\\end{equation*}\\n\\n' ) 
output . write ( '\\\\end{document}' ) 
contents = output . getvalue ( ) 
output . close ( ) 
return contents 
~~ def _get_image_depth ( self ) : 
max_column_widths = [ ] 
for layer in self . ops : 
~~~ current_max = 0 
for op in layer : 
~~~ arg_str_len = 0 
for arg in op . op . params : 
~~~ arg_str = re . sub ( r'[-+]?\\d*\\.\\d{2,}|\\d{2,}' , 
_truncate_float , str ( arg ) ) 
arg_str_len += len ( arg_str ) 
~~ current_max = max ( arg_str_len , current_max ) 
~~ max_column_widths . append ( current_max ) 
~~ columns = 2 
columns += len ( self . ops ) 
sum_column_widths = sum ( 1 + v / 3 for v in max_column_widths ) 
return columns , math . ceil ( sum_column_widths ) + 4 
~~ def _get_beamer_page ( self ) : 
PIL_limit = 40000 
beamer_limit = 550 
aspect_ratio = self . sum_row_heights / self . sum_column_widths 
margin_factor = 1.5 
height = min ( self . sum_row_heights * margin_factor , beamer_limit ) 
width = min ( self . sum_column_widths * margin_factor , beamer_limit ) 
if height * width > PIL_limit : 
~~~ height = min ( np . sqrt ( PIL_limit * aspect_ratio ) , beamer_limit ) 
width = min ( np . sqrt ( PIL_limit / aspect_ratio ) , beamer_limit ) 
~~ height = max ( height , 10 ) 
width = max ( width , 10 ) 
return ( height , width , self . scale ) 
~~ def _build_latex_array ( self , aliases = None ) : 
columns = 1 
if aliases : 
~~~ qregdata = { } 
for q in aliases . values ( ) : 
~~~ if q [ 0 ] not in qregdata : 
~~~ qregdata [ q [ 0 ] ] = q [ 1 ] + 1 
~~ elif qregdata [ q [ 0 ] ] < q [ 1 ] + 1 : 
~~~ qregdata = self . qregs 
~~ for column , layer in enumerate ( self . ops , 1 ) : 
~~~ for op in layer : 
~~~ if op . condition : 
~~~ mask = self . _get_mask ( op . condition [ 0 ] ) 
cl_reg = self . clbit_list [ self . _ffs ( mask ) ] 
if_reg = cl_reg [ 0 ] 
pos_2 = self . img_regs [ cl_reg ] 
if_value = format ( op . condition [ 1 ] , 
'b' ) . zfill ( self . cregs [ if_reg ] ) [ : : - 1 ] 
~~ if op . name not in [ 'measure' , 'barrier' , 'snapshot' , 'load' , 
'save' , 'noise' ] : 
~~~ nm = op . name 
qarglist = op . qargs 
if aliases is not None : 
~~~ qarglist = map ( lambda x : aliases [ x ] , qarglist ) 
~~ if len ( qarglist ) == 1 : 
~~~ pos_1 = self . img_regs [ ( qarglist [ 0 ] [ 0 ] , 
qarglist [ 0 ] [ 1 ] ) ] 
if op . condition : 
if nm == "x" : 
~~~ self . _latex [ pos_1 ] [ column ] = "\\\\gate{X}" 
~~ elif nm == "y" : 
~~~ self . _latex [ pos_1 ] [ column ] = "\\\\gate{Y}" 
~~ elif nm == "z" : 
~~~ self . _latex [ pos_1 ] [ column ] = "\\\\gate{Z}" 
~~ elif nm == "h" : 
~~~ self . _latex [ pos_1 ] [ column ] = "\\\\gate{H}" 
~~ elif nm == "s" : 
~~~ self . _latex [ pos_1 ] [ column ] = "\\\\gate{S}" 
~~ elif nm == "sdg" : 
~~~ self . _latex [ pos_1 ] [ column ] = "\\\\gate{S^\\\\dag}" 
~~ elif nm == "t" : 
~~~ self . _latex [ pos_1 ] [ column ] = "\\\\gate{T}" 
~~ elif nm == "tdg" : 
~~~ self . _latex [ pos_1 ] [ column ] = "\\\\gate{T^\\\\dag}" 
~~ elif nm == "u0" : 
~~~ self . _latex [ pos_1 ] [ column ] = "\\\\gate{U_0(%s)}" % ( 
op . op . params [ 0 ] ) 
~~ elif nm == "u1" : 
~~~ self . _latex [ pos_1 ] [ column ] = "\\\\gate{U_1(%s)}" % ( 
~~ elif nm == "u2" : 
~~~ self . _latex [ pos_1 ] [ column ] = "\\\\gate{U_2\\\\left(%s,%s\\\\right)}" % ( 
op . op . params [ 0 ] , op . op . params [ 1 ] ) 
~~ elif nm == "u3" : 
~~~ self . _latex [ pos_1 ] [ column ] = ( "\\\\gate{U_3(%s,%s,%s)}" % ( 
op . op . params [ 0 ] , 
op . op . params [ 1 ] , 
op . op . params [ 2 ] ) ) 
~~ elif nm == "rx" : 
~~~ self . _latex [ pos_1 ] [ column ] = "\\\\gate{R_x(%s)}" % ( 
~~ elif nm == "ry" : 
~~~ self . _latex [ pos_1 ] [ column ] = "\\\\gate{R_y(%s)}" % ( 
~~ elif nm == "rz" : 
~~~ self . _latex [ pos_1 ] [ column ] = "\\\\gate{R_z(%s)}" % ( 
~~~ self . _latex [ pos_1 ] [ columns ] = "\\\\gate{%s}" % nm 
~~ gap = pos_2 - pos_1 
for i in range ( self . cregs [ if_reg ] ) : 
~~~ if if_value [ i ] == '1' : 
gap = 1 
~~~ if nm == "x" : 
~~ elif nm == "reset" : 
~~~ self . _latex [ pos_1 ] [ column ] = ( 
"\\\\push{\\\\rule{.6em}{0em}\\\\ket{0}\\\\" 
~~ ~~ ~~ elif len ( qarglist ) == 2 : 
~~~ pos_1 = self . img_regs [ ( qarglist [ 0 ] [ 0 ] , qarglist [ 0 ] [ 1 ] ) ] 
pos_2 = self . img_regs [ ( qarglist [ 1 ] [ 0 ] , qarglist [ 1 ] [ 1 ] ) ] 
~~~ pos_3 = self . img_regs [ ( if_reg , 0 ) ] 
temp = [ pos_1 , pos_2 , pos_3 ] 
temp . sort ( key = int ) 
bottom = temp [ 1 ] 
gap = pos_3 - bottom 
~~ ~~ if nm == "cx" : 
~~~ self . _latex [ pos_1 ] [ column ] = "\\\\ctrl{" + str ( pos_2 - pos_1 ) + "}" 
self . _latex [ pos_2 ] [ column ] = "\\\\targ" 
~~ elif nm == "cz" : 
self . _latex [ pos_2 ] [ column ] = "\\\\control\\\\qw" 
~~ elif nm == "cy" : 
self . _latex [ pos_2 ] [ column ] = "\\\\gate{Y}" 
~~ elif nm == "ch" : 
self . _latex [ pos_2 ] [ column ] = "\\\\gate{H}" 
~~ elif nm == "swap" : 
~~~ self . _latex [ pos_1 ] [ column ] = "\\\\qswap" 
~~ elif nm == "crz" : 
self . _latex [ pos_2 ] [ column ] = "\\\\gate{R_z(%s)}" % ( op . op . params [ 0 ] ) 
~~ elif nm == "cu1" : 
~~~ self . _latex [ pos_1 ] [ column - 1 ] = "\\\\ctrl{" + str ( 
pos_2 - pos_1 ) + "}" 
self . _latex [ pos_2 ] [ column - 1 ] = "\\\\control\\\\qw" 
self . _latex [ min ( pos_1 , pos_2 ) ] [ column ] = "\\\\dstick{%s}\\\\qw" % ( op . op . params [ 0 ] ) 
self . _latex [ max ( pos_1 , pos_2 ) ] [ column ] = "\\\\qw" 
~~ elif nm == "cu3" : 
self . _latex [ pos_2 ] [ column ] = "\\\\gate{U_3(%s,%s,%s)}" % ( op . op . params [ 0 ] , 
op . op . params [ 2 ] ) 
~~~ temp = [ pos_1 , pos_2 ] 
if nm == "cx" : 
~~~ self . _latex [ pos_1 ] [ column ] = "\\\\ctrl{" + str ( 
self . _latex [ pos_2 ] [ column ] = ( "\\\\gate{U_3(%s,%s,%s)}" % ( 
~~~ start_pos = min ( [ pos_1 , pos_2 ] ) 
stop_pos = max ( [ pos_1 , pos_2 ] ) 
if stop_pos - start_pos >= 2 : 
~~~ delta = stop_pos - start_pos 
self . _latex [ start_pos ] [ columns ] = ( 
"\\\\multigate{%s}{%s}" % ( delta , nm ) ) 
for i_pos in range ( start_pos + 1 , stop_pos + 1 ) : 
~~~ self . _latex [ i_pos ] [ columns ] = "\\\\ghost{%s}" % nm 
~~~ self . _latex [ start_pos ] [ columns ] = ( 
"\\\\multigate{1}{%s}" % nm ) 
self . _latex [ stop_pos ] [ columns ] = "\\\\ghost{%s}" % nm 
~~ ~~ ~~ ~~ elif len ( qarglist ) == 3 : 
pos_3 = self . img_regs [ ( qarglist [ 2 ] [ 0 ] , qarglist [ 2 ] [ 1 ] ) ] 
~~~ pos_4 = self . img_regs [ ( if_reg , 0 ) ] 
temp = [ pos_1 , pos_2 , pos_3 , pos_4 ] 
bottom = temp [ 2 ] 
prev_column = [ x [ column - 1 ] for x in self . _latex ] 
for item , prev_entry in enumerate ( prev_column ) : 
~~~ if 'barrier' in prev_entry : 
~~~ span = re . search ( 'barrier{(.*)}' , prev_entry ) 
if span and any ( i in temp for i in range ( 
item , int ( span . group ( 1 ) ) ) ) : 
~~~ self . _latex [ item ] [ column - 1 ] = prev_entry . replace ( 
'\\\\barrier{' , 
'\\\\barrier[-0.65em]{' ) 
~~ ~~ ~~ gap = pos_4 - bottom 
~~ ~~ if nm == "ccx" : 
self . _latex [ pos_2 ] [ column ] = "\\\\ctrl{" + str ( 
pos_3 - pos_2 ) + "}" 
self . _latex [ pos_3 ] [ column ] = "\\\\targ" 
~~ if nm == "cswap" : 
self . _latex [ pos_2 ] [ column ] = "\\\\qswap" 
~~~ temp = [ pos_1 , pos_2 , pos_3 ] 
~~ ~~ ~~ if nm == "ccx" : 
~~ elif nm == "cswap" : 
~~~ start_pos = min ( [ pos_1 , pos_2 , pos_3 ] ) 
stop_pos = max ( [ pos_1 , pos_2 , pos_3 ] ) 
if stop_pos - start_pos >= 3 : 
~~~ self . _latex [ pos_1 ] [ columns ] = ( 
"\\\\multigate{2}{%s}" % nm ) 
self . _latex [ pos_2 ] [ columns ] = "\\\\ghost{%s}" % nm 
self . _latex [ pos_3 ] [ columns ] = "\\\\ghost{%s}" % nm 
~~ ~~ ~~ ~~ elif len ( qarglist ) > 3 : 
~~~ nbits = len ( qarglist ) 
pos_array = [ self . img_regs [ ( qarglist [ 0 ] [ 0 ] , 
qarglist [ 0 ] [ 1 ] ) ] ] 
for i in range ( 1 , nbits ) : 
~~~ pos_array . append ( self . img_regs [ ( qarglist [ i ] [ 0 ] , 
qarglist [ i ] [ 1 ] ) ] ) 
~~ pos_start = min ( pos_array ) 
pos_stop = max ( pos_array ) 
delta = pos_stop - pos_start 
self . _latex [ pos_start ] [ columns ] = ( 
"\\\\multigate{%s}{%s}" % ( nbits - 1 , nm ) ) 
for pos in range ( pos_start + 1 , pos_stop + 1 ) : 
~~~ self . _latex [ pos ] [ columns ] = "\\\\ghost{%s}" % nm 
~~ ~~ ~~ elif op . name == "measure" : 
~~~ if ( len ( op . cargs ) != 1 
or len ( op . qargs ) != 1 
or op . op . params ) : 
~~ if op . condition : 
~~~ raise exceptions . VisualizationError ( 
~~ qname , qindex = op . qargs [ 0 ] 
cname , cindex = op . cargs [ 0 ] 
~~~ newq = aliases [ ( qname , qindex ) ] 
qname = newq [ 0 ] 
qindex = newq [ 1 ] 
~~ pos_1 = self . img_regs [ ( qname , qindex ) ] 
pos_2 = self . img_regs [ ( cname , cindex ) ] 
~~~ self . _latex [ pos_1 ] [ column ] = "\\\\meter" 
if span and ( 
item + int ( span . group ( 1 ) ) ) - pos_1 >= 0 : 
'\\\\barrier[-1.15em]{' ) 
~~ ~~ elif op . name in [ 'barrier' , 'snapshot' , 'load' , 'save' , 
~~~ if self . plot_barriers : 
~~~ qarglist = op . qargs 
indexes = [ self . _get_qubit_index ( x ) for x in qarglist ] 
start_bit = self . qubit_list [ min ( indexes ) ] 
~~ start = self . img_regs [ start_bit ] 
span = len ( op . qargs ) - 1 
span ) + "}" 
~~ ~~ ~~ ~~ def _get_qubit_index ( self , qubit ) : 
for i , bit in enumerate ( self . qubit_list ) : 
~~~ if qubit == bit : 
~~~ qindex = i 
~~ return qindex 
~~ def _load_schema ( file_path , name = None ) : 
~~~ name = os . path . splitext ( os . path . basename ( file_path ) ) [ 0 ] 
~~ if name not in _SCHEMAS : 
~~~ with open ( file_path , 'r' ) as schema_file : 
~~~ _SCHEMAS [ name ] = json . load ( schema_file ) 
~~ ~~ return _SCHEMAS [ name ] 
~~ def _get_validator ( name , schema = None , check_schema = True , 
validator_class = None , ** validator_kwargs ) : 
if schema is None : 
~~~ schema = _SCHEMAS [ name ] 
~~ ~~ if name not in _VALIDATORS : 
~~~ if validator_class is None : 
~~~ validator_class = jsonschema . validators . validator_for ( schema ) 
~~ _VALIDATORS [ name ] = validator_class ( schema , ** validator_kwargs ) 
~~ validator = _VALIDATORS [ name ] 
if check_schema : 
~~~ validator . check_schema ( schema ) 
~~ return validator 
~~ def _load_schemas_and_validators ( ) : 
schema_base_path = os . path . join ( os . path . dirname ( __file__ ) , '../..' ) 
for name , path in _DEFAULT_SCHEMA_PATHS . items ( ) : 
~~~ _load_schema ( os . path . join ( schema_base_path , path ) , name ) 
_get_validator ( name ) 
~~ ~~ def validate_json_against_schema ( json_dict , schema , 
err_msg = None ) : 
~~~ if isinstance ( schema , str ) : 
~~~ schema_name = schema 
schema = _SCHEMAS [ schema_name ] 
validator = _get_validator ( schema_name ) 
validator . validate ( json_dict ) 
~~~ jsonschema . validate ( json_dict , schema ) 
~~ ~~ except jsonschema . ValidationError as err : 
~~~ if err_msg is None : 
~~ newerr = SchemaValidationError ( err_msg ) 
newerr . __cause__ = _SummaryValidationError ( err ) 
logger . debug ( '%s' , _format_causes ( err ) ) 
raise newerr 
~~ ~~ def _format_causes ( err , level = 0 ) : 
def _print ( string , offset = 0 ) : 
~~~ lines . append ( _pad ( string , offset = offset ) ) 
~~ def _pad ( string , offset = 0 ) : 
padded_lines = [ padding + line for line in string . split ( '\\n' ) ] 
return '\\n' . join ( padded_lines ) 
~~ def _format_path ( path ) : 
~~~ def _format ( item ) : 
~~~ if isinstance ( item , str ) : 
~~~ return '.{}' . format ( item ) 
~~ return '[{}]' . format ( item ) 
~~ return '' . join ( [ '<root>' ] + list ( map ( _format , path ) ) ) 
err . validator , _format_path ( err . absolute_path ) ) ) 
if not err . context : 
~~~ _print ( str ( err . message ) , offset = 1 ) 
~~~ for suberr in err . context : 
~~~ lines . append ( _format_causes ( suberr , level + 1 ) ) 
~~ ~~ return '\\n' . join ( lines ) 
return "," . join ( [ self . children [ j ] . qasm ( prec ) 
for j in range ( self . size ( ) ) ] ) 
~~ def majority ( p , a , b , c ) : 
p . cx ( c , b ) 
p . cx ( c , a ) 
p . ccx ( a , b , c ) 
~~ def unmajority ( p , a , b , c ) : 
p . cx ( a , b ) 
~~ def _collect_potential_merges ( dag , barriers ) : 
if len ( barriers ) < 2 : 
~~ node_to_barrier_qubits = { } 
current_barrier = barriers [ 0 ] 
end_of_barrier = current_barrier 
current_barrier_nodes = [ current_barrier ] 
current_qubits = set ( current_barrier . qargs ) 
current_ancestors = dag . ancestors ( current_barrier ) 
current_descendants = dag . descendants ( current_barrier ) 
barrier_to_add = Barrier ( len ( current_qubits ) ) 
for next_barrier in barriers [ 1 : ] : 
~~~ next_ancestors = { nd for nd in dag . ancestors ( next_barrier ) 
if nd not in current_barrier_nodes } 
next_descendants = { nd for nd in dag . descendants ( next_barrier ) 
next_qubits = set ( next_barrier . qargs ) 
if ( 
not current_qubits . isdisjoint ( next_qubits ) 
and current_ancestors . isdisjoint ( next_descendants ) 
and current_descendants . isdisjoint ( next_ancestors ) 
~~~ current_ancestors = current_ancestors | next_ancestors 
current_descendants = current_descendants | next_descendants 
current_qubits = current_qubits | next_qubits 
~~~ if barrier_to_add : 
~~~ node_to_barrier_qubits [ end_of_barrier ] = current_qubits 
~~ current_qubits = set ( next_barrier . qargs ) 
current_ancestors = dag . ancestors ( next_barrier ) 
current_descendants = dag . descendants ( next_barrier ) 
current_barrier_nodes = [ ] 
~~ end_of_barrier = next_barrier 
current_barrier_nodes . append ( end_of_barrier ) 
~~ if barrier_to_add : 
~~ return node_to_barrier_qubits 
~~ def circuit_drawer ( circuit , 
scale = 0.7 , 
filename = None , 
style = None , 
output = 'text' , 
interactive = False , 
line_length = None , 
plot_barriers = True , 
reverse_bits = False , 
justify = None ) : 
image = None 
if output == 'text' : 
~~~ return _text_circuit_drawer ( circuit , filename = filename , 
plotbarriers = plot_barriers , 
~~ elif output == 'latex' : 
~~~ image = _latex_circuit_drawer ( circuit , scale = scale , 
~~ elif output == 'latex_source' : 
~~~ return _generate_latex_source ( circuit , 
filename = filename , scale = scale , 
style = style , 
~~ elif output == 'mpl' : 
~~~ image = _matplotlib_circuit_drawer ( circuit , scale = scale , 
~~ if image and interactive : 
~~~ image . show ( ) 
~~ def _text_circuit_drawer ( circuit , filename = None , line_length = None , reverse_bits = False , 
plotbarriers = True , justify = None , vertically_compressed = True ) : 
qregs , cregs , ops = utils . _get_layered_instructions ( circuit , 
text_drawing = _text . TextDrawing ( qregs , cregs , ops ) 
text_drawing . plotbarriers = plotbarriers 
text_drawing . line_length = line_length 
text_drawing . vertically_compressed = vertically_compressed 
~~~ text_drawing . dump ( filename ) 
~~ return text_drawing 
~~ def _latex_circuit_drawer ( circuit , 
tmpfilename = 'circuit' 
with tempfile . TemporaryDirectory ( ) as tmpdirname : 
~~~ tmppath = os . path . join ( tmpdirname , tmpfilename + '.tex' ) 
_generate_latex_source ( circuit , filename = tmppath , 
scale = scale , style = style , 
reverse_bits = reverse_bits , justify = justify ) 
~~~ subprocess . run ( [ "pdflatex" , "-halt-on-error" , 
"-output-directory={}" . format ( tmpdirname ) , 
"{}" . format ( tmpfilename + '.tex' ) ] , 
stdout = subprocess . PIPE , stderr = subprocess . DEVNULL , 
check = True ) 
~~ except OSError as ex : 
~~~ if ex . errno == errno . ENOENT : 
~~ raise 
~~ except subprocess . CalledProcessError as ex : 
~~~ with open ( 'latex_error.log' , 'wb' ) as error_file : 
~~~ error_file . write ( ex . stdout ) 
~~~ base = os . path . join ( tmpdirname , tmpfilename ) 
subprocess . run ( [ "pdftocairo" , "-singlefile" , "-png" , "-q" , 
base + '.pdf' , base ] ) 
image = Image . open ( base + '.png' ) 
image = utils . _trim ( image ) 
os . remove ( base + '.png' ) 
~~~ image . save ( filename , 'PNG' ) 
~~ ~~ except OSError as ex : 
~~ ~~ return image 
~~ ~~ def _generate_latex_source ( circuit , filename = None , 
scale = 0.7 , style = None , reverse_bits = False , 
plot_barriers = True , justify = None ) : 
qcimg = _latex . QCircuitImage ( qregs , cregs , ops , scale , style = style , 
reverse_bits = reverse_bits ) 
latex = qcimg . latex ( ) 
~~~ with open ( filename , 'w' ) as latex_file : 
~~~ latex_file . write ( latex ) 
~~ ~~ return latex 
~~ def _matplotlib_circuit_drawer ( circuit , 
qcd = _matplotlib . MatplotlibDrawer ( qregs , cregs , ops , scale = scale , style = style , 
return qcd . draw ( filename ) 
~~ def random_state ( dim , seed = None ) : 
if seed is None : 
~~~ seed = np . random . randint ( 0 , np . iinfo ( np . int32 ) . max ) 
~~ rng = np . random . RandomState ( seed ) 
x = rng . rand ( dim ) 
x += x == 0 
x = - np . log ( x ) 
sumx = sum ( x ) 
phases = rng . rand ( dim ) * 2.0 * np . pi 
return np . sqrt ( x / sumx ) * np . exp ( 1j * phases ) 
~~ def random_unitary ( dim , seed = None ) : 
if dim == 0 or not math . log2 ( dim ) . is_integer ( ) : 
~~ matrix = np . zeros ( [ dim , dim ] , dtype = complex ) 
~~~ if j == 0 : 
~~~ a = random_state ( dim , seed ) 
~~~ a = random_state ( dim ) 
~~ matrix [ : , j ] = np . copy ( a ) 
i = j - 1 
while i >= 0 : 
~~~ dc = np . vdot ( matrix [ : , i ] , a ) 
matrix [ : , j ] = matrix [ : , j ] - dc * matrix [ : , i ] 
i = i - 1 
~~ matrix [ : , j ] = matrix [ : , j ] * ( 1.0 / np . sqrt ( np . vdot ( matrix [ : , j ] , matrix [ : , j ] ) ) ) 
~~ return Operator ( matrix ) 
if method == 'Hilbert-Schmidt' : 
~~~ return __random_density_hs ( length , rank , seed ) 
~~ elif method == 'Bures' : 
~~~ return __random_density_bures ( length , rank , seed ) 
~~ ~~ def __ginibre_matrix ( nrow , ncol = None , seed = None ) : 
if ncol is None : 
~~~ ncol = nrow 
~~ if seed is not None : 
~~ G = np . random . normal ( size = ( nrow , ncol ) ) + np . random . normal ( size = ( nrow , ncol ) ) * 1j 
return G 
~~ def __random_density_hs ( N , rank = None , seed = None ) : 
G = __ginibre_matrix ( N , rank , seed ) 
G = G . dot ( G . conj ( ) . T ) 
return G / np . trace ( G ) 
~~ def __random_density_bures ( N , rank = None , seed = None ) : 
P = np . eye ( N ) + random_unitary ( N ) . data 
G = P . dot ( __ginibre_matrix ( N , rank , seed ) ) 
string = "" 
~~ def calls ( self ) : 
lst = [ ] 
~~~ if children . type == "custom_unitary" : 
~~~ lst . append ( children . name ) 
~~ ~~ return lst 
if self . value == pi : 
~~~ return "pi" 
~~ return ccode ( self . value , precision = prec ) 
q = QuantumRegister ( 1 , "q" ) 
( U1Gate ( - pi / 2 ) , [ q [ 0 ] ] , [ ] ) 
~~~ return Chi ( 
~~ return Chi ( Choi ( self ) . compose ( other , front = front ) ) 
~~ return Chi ( SuperOp ( self ) . power ( n ) ) 
if not isinstance ( other , Chi ) : 
~~~ other = Chi ( other ) 
~~ return Chi ( self . _data + other . data , self . _input_dims , 
self . _output_dims ) 
~~ return Chi ( other * self . _data , self . _input_dims , self . _output_dims ) 
data = np . kron ( other . data , self . _data ) 
data = np . kron ( self . _data , other . data ) 
~~ return Chi ( data , input_dims , output_dims ) 
return SuperOp ( 
np . conj ( self . _data ) , self . input_dims ( ) , self . output_dims ( ) ) 
np . transpose ( self . _data ) , 
if not isinstance ( other , SuperOp ) : 
~~~ other = SuperOp ( other ) 
~~ if front and self . input_dims ( qargs = qargs ) != other . output_dims ( ) : 
~~ if not front and self . output_dims ( qargs = qargs ) != other . input_dims ( ) : 
~~ if qargs is None : 
~~~ if front : 
~~~ return SuperOp ( 
np . dot ( self . _data , other . data ) , 
input_dims = other . input_dims ( ) , 
output_dims = self . output_dims ( ) ) 
~~ return SuperOp ( 
np . dot ( other . data , self . _data ) , 
input_dims = self . input_dims ( ) , 
output_dims = other . output_dims ( ) ) 
~~ return self . _compose_subsystem ( other , qargs , front ) 
if not isinstance ( n , ( int , np . integer ) ) : 
~~ if self . _input_dim != self . _output_dim : 
np . linalg . matrix_power ( self . _data , n ) , self . input_dims ( ) , 
~~ return SuperOp ( self . _data + other . data , self . input_dims ( ) , 
~~ return SuperOp ( other * self . _data , self . input_dims ( ) , 
state = self . _format_state ( state , density_matrix = True ) 
~~ shape_in = self . _input_dim * self . _input_dim 
shape_out = ( self . _output_dim , self . _output_dim ) 
np . dot ( self . _data , np . reshape ( state , shape_in , order = 'F' ) ) , 
shape_out , 
~~ def _compose_subsystem ( self , other , qargs , front = False ) : 
input_dims = list ( self . input_dims ( ) ) 
output_dims = list ( self . output_dims ( ) ) 
if front : 
~~~ num_indices = len ( self . input_dims ( ) ) 
shift = 2 * len ( self . output_dims ( ) ) 
right_mul = True 
for pos , qubit in enumerate ( qargs ) : 
~~~ input_dims [ qubit ] = other . _input_dims [ pos ] 
~~~ num_indices = len ( self . output_dims ( ) ) 
shift = 0 
right_mul = False 
~~~ output_dims [ qubit ] = other . _output_dims [ pos ] 
~~ ~~ tensor = np . reshape ( self . data , self . _shape ) 
mat = np . reshape ( other . data , other . _shape ) 
indices = [ 2 * num_indices - 1 - qubit for qubit in qargs 
] + [ num_indices - 1 - qubit for qubit in qargs ] 
final_shape = [ np . product ( output_dims ) ** 2 , np . product ( input_dims ) ** 2 ] 
self . _einsum_matmul ( tensor , mat , indices , shift , right_mul ) , 
return SuperOp ( data , input_dims , output_dims ) 
num_inidices = len ( state_dims ) 
indices = [ num_inidices - 1 - qubit for qubit in qargs 
] + [ 2 * num_inidices - 1 - qubit for qubit in qargs ] 
~~ def _instruction_to_superop ( cls , instruction ) : 
~~ op = SuperOp ( np . eye ( 4 ** instruction . num_qubits ) ) 
~~~ chan = None 
if obj . name == 'reset' : 
~~~ chan = SuperOp ( 
np . array ( [ [ 1 , 0 , 0 , 1 ] , [ 0 , 0 , 0 , 0 ] , [ 0 , 0 , 0 , 0 ] , 
[ 0 , 0 , 0 , 0 ] ] ) ) 
~~ if obj . name == 'kraus' : 
~~~ kraus = obj . params 
dim = len ( kraus [ 0 ] ) 
chan = SuperOp ( _to_superop ( 'Kraus' , ( kraus , None ) , dim , dim ) ) 
~~ elif hasattr ( obj , 'to_matrix' ) : 
~~~ kraus = [ obj . to_matrix ( ) ] 
chan = SuperOp ( 
_to_superop ( 'Kraus' , ( kraus , None ) , dim , dim ) ) 
~~ ~~ if chan is not None : 
~~~ op = self . compose ( chan , qargs = qargs ) 
obj . name ) ) 
. format ( instr . name ) ) 
final_op_types = [ 'measure' , 'barrier' ] 
final_ops = [ ] 
for candidate_node in dag . named_nodes ( * final_op_types ) : 
~~~ is_final_op = True 
for _ , child_successors in dag . bfs_successors ( candidate_node ) : 
~~~ if any ( suc . type == 'op' and suc . name not in final_op_types 
for suc in child_successors ) : 
~~~ is_final_op = False 
~~ ~~ if is_final_op : 
~~~ final_ops . append ( candidate_node ) 
~~ ~~ if not final_ops : 
~~~ return dag 
~~ barrier_layer = DAGCircuit ( ) 
~~~ barrier_layer . add_qreg ( qreg ) 
~~~ barrier_layer . add_creg ( creg ) 
~~ final_qubits = set ( final_op . qargs [ 0 ] for final_op in final_ops ) 
barrier_layer . apply_operation_back ( 
Barrier ( len ( final_qubits ) ) , list ( final_qubits ) , [ ] ) 
ordered_final_nodes = [ node for node in dag . topological_op_nodes ( ) 
if node in set ( final_ops ) ] 
for final_node in ordered_final_nodes : 
~~~ barrier_layer . apply_operation_back ( final_node . op , 
final_node . qargs , 
final_node . cargs ) 
~~ for final_op in final_ops : 
~~~ dag . remove_op_node ( final_op ) 
~~ dag . extend_back ( barrier_layer ) 
adjacent_pass = MergeAdjacentBarriers ( ) 
return adjacent_pass . run ( dag ) 
~~ def circuits_to_qobj ( circuits , qobj_header = None , 
qobj_id = None , backend_name = None , 
config = None , shots = None , max_credits = None , 
basis_gates = None , 
coupling_map = None , seed = None , memory = None ) : 
qobj_header = qobj_header or QobjHeader ( ) 
if backend_name : 
~~~ qobj_header . backend_name = backend_name 
~~ if basis_gates : 
~~ if coupling_map : 
~~ qobj = assemble ( experiments = circuits , 
config = config ) 
for node in dag . threeQ_or_more_gates ( ) : 
~~~ rule = node . op . definition 
node . op . name ) 
dag . substitute_node_with_dag ( node , decomposition ) 
for node in dag . op_nodes ( self . gate ) : 
~~~ if not node . op . definition : 
decomposition = DAGCircuit ( ) 
if rule [ 0 ] [ 2 ] : 
~~~ decomposition . add_creg ( rule [ 0 ] [ 2 ] [ 0 ] [ 0 ] ) 
~~ for inst in rule : 
~~ dag . substitute_node_with_dag ( node , decomposition ) 
~~ def unitary ( self , obj , qubits , label = None ) : 
~~ return self . append ( UnitaryGate ( obj , label = label ) , qubits , [ ] ) 
if self . num_qubits == 1 : 
~~~ q = QuantumRegister ( 1 , "q" ) 
angles = euler_angles_1q ( self . to_matrix ( ) ) 
self . definition = [ ( U3Gate ( * angles ) , [ q [ 0 ] ] , [ ] ) ] 
~~ if self . num_qubits == 2 : 
~~~ self . definition = two_qubit_kak ( self . to_matrix ( ) ) 
~~ ~~ def check_type ( self , value , attr , data ) : 
if self . many and not is_collection ( value ) : 
value , Iterable , fields = [ self ] , field_names = attr , data = data ) 
~~ _check_type = super ( ) . check_type 
errors = [ ] 
values = value if self . many else [ value ] 
for idx , v in enumerate ( values ) : 
~~~ _check_type ( v , idx , values ) 
~~ except ValidationError as err : 
~~~ errors . append ( err . messages ) 
~~ ~~ if errors : 
~~~ errors = errors if self . many else errors [ 0 ] 
raise ValidationError ( errors ) 
super ( ) . check_type ( value , attr , data ) 
for idx , v in enumerate ( value ) : 
~~~ self . container . check_type ( v , idx , value ) 
~~~ raise ValidationError ( errors ) 
~~ def dag_drawer ( dag , scale = 0.7 , filename = None , style = 'color' ) : 
~~~ import nxpd 
~~ G = dag . to_networkx ( ) 
G . graph [ 'dpi' ] = 100 * scale 
if style == 'plain' : 
~~ elif style == 'color' : 
~~~ for node in G . nodes : 
~~~ n = G . nodes [ node ] 
n [ 'label' ] = node . name 
if node . type == 'op' : 
~~~ n [ 'color' ] = 'blue' 
n [ 'style' ] = 'filled' 
n [ 'fillcolor' ] = 'lightblue' 
~~ if node . type == 'in' : 
~~~ n [ 'color' ] = 'black' 
n [ 'fillcolor' ] = 'green' 
~~ if node . type == 'out' : 
n [ 'fillcolor' ] = 'red' 
~~ ~~ for e in G . edges ( data = True ) : 
~~~ e [ 2 ] [ 'label' ] = e [ 2 ] [ 'name' ] 
~~ if filename : 
~~~ show = False 
~~ elif ( 'ipykernel' in sys . modules ) and ( 'spyder' not in sys . modules ) : 
~~~ show = 'ipynb' 
~~~ show = True 
~~ return nxpd . draw ( G , filename = filename , show = show ) 
~~ def _atol ( self , atol ) : 
max_tol = self . __class__ . MAX_TOL 
if atol < 0 : 
~~ if atol > max_tol : 
~~ self . __class__ . ATOL = atol 
~~ def _rtol ( self , rtol ) : 
if rtol < 0 : 
~~ if rtol > max_tol : 
~~ self . __class__ . RTOL = rtol 
~~ def _reshape ( self , input_dims = None , output_dims = None ) : 
if input_dims is not None : 
~~~ if np . product ( input_dims ) != self . _input_dim : 
~~ self . _input_dims = tuple ( input_dims ) 
~~ if output_dims is not None : 
~~~ if np . product ( output_dims ) != self . _output_dim : 
~~ self . _output_dims = tuple ( output_dims ) 
~~ def input_dims ( self , qargs = None ) : 
~~~ return self . _input_dims 
~~ return tuple ( self . _input_dims [ i ] for i in qargs ) 
~~ def output_dims ( self , qargs = None ) : 
~~~ return self . _output_dims 
~~ return tuple ( self . _output_dims [ i ] for i in qargs ) 
return self . __class__ ( self . data , self . input_dims ( ) , self . output_dims ( ) ) 
if not isinstance ( n , ( int , np . integer ) ) or n < 1 : 
~~ ret = self . copy ( ) 
for _ in range ( 1 , n ) : 
~~~ ret = ret . compose ( self ) 
~~ def _automatic_dims ( cls , dims , size ) : 
if dims is None : 
~~~ dims = size 
~~ elif np . product ( dims ) != size : 
~~ if isinstance ( dims , ( int , np . integer ) ) : 
~~~ num_qubits = int ( np . log2 ( dims ) ) 
if 2 ** num_qubits == size : 
~~~ return num_qubits * ( 2 , ) 
~~ return ( dims , ) 
~~ return tuple ( dims ) 
~~ def _einsum_matmul ( cls , tensor , mat , indices , shift = 0 , right_mul = False ) : 
rank = tensor . ndim 
rank_mat = mat . ndim 
if rank_mat % 2 != 0 : 
~~ indices_tensor = list ( range ( rank ) ) 
for j , index in enumerate ( indices ) : 
~~~ indices_tensor [ index + shift ] = rank + j 
~~ mat_contract = list ( reversed ( range ( rank , rank + len ( indices ) ) ) ) 
mat_free = [ index + shift for index in reversed ( indices ) ] 
if right_mul : 
~~~ indices_mat = mat_contract + mat_free 
~~~ indices_mat = mat_free + mat_contract 
~~ return np . einsum ( tensor , indices_tensor , mat , indices_mat ) 
~~ def _deserialize ( self , value , attr , data ) : 
~~~ return super ( ) . _deserialize ( value , attr , data ) 
~~~ if 'deserialization_schema_selector' in ex . messages [ 0 ] : 
~~ ~~ def _serialize ( self , value , key , obj ) : 
~~~ return super ( ) . _serialize ( value , key , obj ) 
~~ except TypeError as ex : 
~~~ if 'serialization_schema_selector' in str ( ex ) : 
for field in self . choices : 
~~~ return field . check_type ( value , attr , data ) 
~~ except ValidationError : 
~~ ~~ ~~ raise self . _not_expected_type ( 
value , [ field . __class__ for field in self . choices ] , 
fields = [ self ] , field_names = attr , data = data ) 
~~ def state_fidelity ( state1 , state2 ) : 
s1 = np . array ( state1 ) 
s2 = np . array ( state2 ) 
if s1 . ndim == 1 and s2 . ndim == 1 : 
~~~ return np . abs ( s2 . conj ( ) . dot ( s1 ) ) ** 2 
~~ elif s1 . ndim == 1 : 
~~~ return np . abs ( s1 . conj ( ) . dot ( s2 ) . dot ( s1 ) ) 
~~ elif s2 . ndim == 1 : 
~~~ return np . abs ( s2 . conj ( ) . dot ( s1 ) . dot ( s2 ) ) 
~~ s1sq = _funm_svd ( s1 , np . sqrt ) 
s2sq = _funm_svd ( s2 , np . sqrt ) 
return np . linalg . norm ( s1sq . dot ( s2sq ) , ord = 'nuc' ) ** 2 
~~ def _funm_svd ( a , func ) : 
U , s , Vh = la . svd ( a , lapack_driver = 'gesvd' ) 
S = np . diag ( func ( s ) ) 
return U . dot ( S ) . dot ( Vh ) 
~~~ self . layout = self . property_set [ "layout" ] 
~~ ~~ self . property_set [ 'is_swap_mapped' ] = True 
for gate in dag . twoQ_gates ( ) : 
~~~ physical_q0 = self . layout [ gate . qargs [ 0 ] ] 
physical_q1 = self . layout [ gate . qargs [ 1 ] ] 
~~~ self . property_set [ 'is_swap_mapped' ] = False 
~~ ~~ ~~ def snapshot ( self , 
label , 
snapshot_type = 'statevector' , 
qubits = None , 
params = None ) : 
if not isinstance ( label , str ) : 
~~~ warnings . warn ( 
label = str ( label ) 
~~ if isinstance ( qubits , QuantumRegister ) : 
~~ if not qubits : 
~~~ tuples = [ ] 
if isinstance ( self , QuantumCircuit ) : 
~~~ for register in self . qregs : 
~~~ tuples . append ( register ) 
~~ ~~ if not tuples : 
~~ qubits = [ ] 
for tuple_element in tuples : 
~~~ if isinstance ( tuple_element , QuantumRegister ) : 
~~~ for j in range ( tuple_element . size ) : 
~~~ qubits . append ( ( tuple_element , j ) ) 
~~~ qubits . append ( tuple_element ) 
~~ ~~ ~~ return self . append ( 
Snapshot ( 
snapshot_type = snapshot_type , 
num_qubits = len ( qubits ) , 
params = params ) , qubits ) 
~~ def assemble ( self ) : 
instruction . label = self . _label 
instruction . snapshot_type = self . _snapshot_type 
return Snapshot ( self . num_qubits , self . num_clbits , self . params [ 0 ] , 
self . params [ 1 ] ) 
~~ def label ( self , name ) : 
if isinstance ( name , str ) : 
~~~ self . _label = name 
choi = _to_choi ( self . rep , self . _data , * self . dim ) 
return self . _is_cp_helper ( choi , atol , rtol ) and self . _is_tp_helper ( 
choi , atol , rtol ) 
~~ def is_tp ( self , atol = None , rtol = None ) : 
return self . _is_tp_helper ( choi , atol , rtol ) 
~~ def is_cp ( self , atol = None , rtol = None ) : 
return self . _is_cp_helper ( choi , atol , rtol ) 
~~~ op = self . to_operator ( ) 
return op . is_unitary ( atol = atol , rtol = rtol ) 
~~ ~~ def to_operator ( self ) : 
mat = _to_operator ( self . rep , self . _data , * self . dim ) 
return Operator ( mat , self . input_dims ( ) , self . output_dims ( ) ) 
from qiskit . circuit . instruction import Instruction 
n_qubits = int ( np . log2 ( self . _input_dim ) ) 
if self . _input_dim != self . _output_dim or 2 ** n_qubits != self . _input_dim : 
~~ if not self . is_cptp ( ) : 
~~ kraus , _ = _to_kraus ( self . rep , self . _data , * self . dim ) 
if len ( kraus ) == 1 : 
~~~ return Operator ( kraus [ 0 ] ) . to_instruction ( ) 
~~ return Instruction ( 'kraus' , n_qubits , 0 , kraus ) 
~~ def _is_cp_helper ( self , choi , atol , rtol ) : 
~~ return is_positive_semidefinite_matrix ( choi , rtol = rtol , atol = atol ) 
~~ def _is_tp_helper ( self , choi , atol , rtol ) : 
~~ d_in , d_out = self . dim 
mat = np . trace ( 
np . reshape ( choi , ( d_in , d_out , d_in , d_out ) ) , axis1 = 1 , axis2 = 3 ) 
return is_identity_matrix ( mat , rtol = rtol , atol = atol ) 
~~ def _format_state ( self , state , density_matrix = False ) : 
~~ ~~ if density_matrix and ndim == 1 : 
~~~ state = np . outer ( state , np . transpose ( np . conj ( state ) ) ) 
~~ return state 
~~ def _init_transformer ( cls , data ) : 
if isinstance ( data , QuantumChannel ) : 
~~ if hasattr ( data , 'to_quantumchannel' ) : 
~~~ return data . to_channel ( ) 
~~ if hasattr ( data , 'to_channel' ) : 
~~ return Operator ( data ) 
~~ ~~ self . property_set [ 'is_direction_mapped' ] = True 
edges = self . coupling_map . get_edges ( ) 
if isinstance ( gate . op , ( CXBase , CnotGate ) ) and ( 
physical_q0 , physical_q1 ) not in edges : 
~~~ self . property_set [ 'is_direction_mapped' ] = False 
~~ ~~ ~~ def sort_enum_for_model ( cls , name = None , symbol_name = _symbol_name ) : 
enum , _ = _sort_enum_for_model ( cls , name , symbol_name ) 
return enum 
~~ def sort_argument_for_model ( cls , has_default = True ) : 
enum , default = _sort_enum_for_model ( cls ) 
if not has_default : 
~~~ default = None 
~~ return Argument ( List ( enum ) , default_value = default ) 
~~ def search_dates ( text , languages = None , settings = None , add_detected_language = False ) : 
result = _search_with_detection . search_dates ( 
text = text , languages = languages , settings = settings 
language , dates = result . get ( 'Language' ) , result . get ( 'Dates' ) 
if dates : 
~~~ if add_detected_language : 
~~~ dates = [ date + ( language , ) for date in dates ] 
~~ return dates 
~~ ~~ def patch_strptime ( ) : 
_strptime = imp . load_module ( 
'strptime_patched' , * imp . find_module ( '_strptime' ) 
_calendar = imp . load_module ( 
'calendar_patched' , * imp . find_module ( '_strptime' ) 
_strptime . _getlang = lambda : ( 'en_US' , 'UTF-8' ) 
_strptime . calendar = _calendar 
_strptime . calendar . day_abbr = [ 
'mon' , 'tue' , 'wed' , 'thu' , 'fri' , 'sat' , 'sun' 
_strptime . calendar . day_name = [ 
'monday' , 'tuesday' , 'wednesday' , 'thursday' , 
'friday' , 'saturday' , 'sunday' 
_strptime . calendar . month_abbr = [ 
'' , 'jan' , 'feb' , 'mar' , 'apr' , 'may' , 'jun' , 
'jul' , 'aug' , 'sep' , 'oct' , 'nov' , 'dec' 
_strptime . calendar . month_name = [ 
'' , 'january' , 'february' , 'march' , 'april' , 
'may' , 'june' , 'july' , 'august' , 'september' , 
'october' , 'november' , 'december' 
return _strptime . _strptime_time 
~~ def get_locale_map ( self , languages = None , locales = None , region = None , 
use_given_order = False , allow_conflicting_locales = False ) : 
return OrderedDict ( self . _load_data ( 
languages = languages , locales = locales , region = region , use_given_order = use_given_order , 
allow_conflicting_locales = allow_conflicting_locales ) ) 
~~ def get_locales ( self , languages = None , locales = None , region = None , 
for _ , locale in self . _load_data ( 
languages = languages , locales = locales , region = region , 
use_given_order = use_given_order , 
allow_conflicting_locales = allow_conflicting_locales ) : 
~~~ yield locale 
~~ ~~ def are_tokens_valid ( self , tokens ) : 
match_relative_regex = self . _get_match_relative_regex_cache ( ) 
for token in tokens : 
~~~ if any ( [ match_relative_regex . match ( token ) , 
token in self , token . isdigit ( ) ] ) : 
~~ ~~ def split ( self , string , keep_formatting = False ) : 
if not string : 
~~ split_relative_regex = self . _get_split_relative_regex_cache ( ) 
tokens = split_relative_regex . split ( string ) 
for i , token in enumerate ( tokens ) : 
~~~ if match_relative_regex . match ( token ) : 
~~~ tokens [ i ] = [ token ] 
~~ tokens [ i ] = self . _split_by_known_words ( token , keep_formatting ) 
~~ return list ( filter ( bool , chain ( * tokens ) ) ) 
~~ def search_dates ( self , text , languages = None , settings = None ) : 
language_shortname = self . detect_language ( text = text , languages = languages ) 
if not language_shortname : 
~~~ return { 'Language' : None , 'Dates' : None } 
~~ return { 'Language' : language_shortname , 'Dates' : self . search . search_parse ( language_shortname , text , 
settings = settings ) } 
~~ def parse ( date_string , date_formats = None , languages = None , locales = None , region = None , settings = None ) : 
parser = _default_parser 
if any ( [ languages , locales , region , not settings . _default ] ) : 
~~~ parser = DateDataParser ( languages = languages , locales = locales , 
region = region , settings = settings ) 
~~ data = parser . get_date_data ( date_string , date_formats ) 
~~~ return data [ 'date_obj' ] 
~~ ~~ def _parse_time ( self , date_string , settings ) : 
date_string = PATTERN . sub ( '' , date_string ) 
date_string = re . sub ( r'\\b(?:ago|in)\\b' , '' , date_string ) 
~~~ return time_parser ( date_string ) 
~~ ~~ def is_applicable ( self , date_string , strip_timezone = False , settings = None ) : 
if strip_timezone : 
~~~ date_string , _ = pop_tz_offset_from_string ( date_string , as_offset = False ) 
~~ date_string = self . _translate_numerals ( date_string ) 
if settings . NORMALIZE : 
~~~ date_string = normalize_unicode ( date_string ) 
~~ date_string = self . _simplify ( date_string , settings = settings ) 
dictionary = self . _get_dictionary ( settings ) 
date_tokens = dictionary . split ( date_string ) 
return dictionary . are_tokens_valid ( date_tokens ) 
~~ def translate ( self , date_string , keep_formatting = False , settings = None ) : 
date_string = self . _translate_numerals ( date_string ) 
date_string_tokens = dictionary . split ( date_string , keep_formatting ) 
relative_translations = self . _get_relative_translations ( settings = settings ) 
for i , word in enumerate ( date_string_tokens ) : 
for pattern , replacement in relative_translations . items ( ) : 
~~~ if pattern . match ( word ) : 
~~~ date_string_tokens [ i ] = pattern . sub ( replacement , word ) 
~~~ if word in dictionary : 
~~~ date_string_tokens [ i ] = dictionary [ word ] or '' 
~~ ~~ ~~ if "in" in date_string_tokens : 
~~~ date_string_tokens = self . _clear_future_words ( date_string_tokens ) 
~~ return self . _join ( list ( filter ( bool , date_string_tokens ) ) , 
~~ def parse_with_formats ( date_string , date_formats , settings ) : 
period = 'day' 
for date_format in date_formats : 
~~~ date_obj = datetime . strptime ( date_string , date_format ) 
~~~ if '%d' not in date_format : 
~~~ period = 'month' 
date_obj = date_obj . replace ( 
day = get_last_day_of_month ( date_obj . year , date_obj . month ) ) 
~~ if not ( '%y' in date_format or '%Y' in date_format ) : 
~~~ today = datetime . today ( ) 
date_obj = date_obj . replace ( year = today . year ) 
~~ date_obj = apply_timezone_from_settings ( date_obj , settings ) 
return { 'date_obj' : date_obj , 'period' : period } 
~~~ return { 'date_obj' : None , 'period' : period } 
~~ ~~ def get_date_data ( self , date_string , date_formats = None ) : 
if not ( isinstance ( date_string , six . text_type ) or isinstance ( date_string , six . string_types ) ) : 
~~ if isinstance ( date_string , bytes ) : 
~~~ date_string = date_string . decode ( 'utf-8' ) 
~~ res = parse_with_formats ( date_string , date_formats or [ ] , self . _settings ) 
if res [ 'date_obj' ] : 
~~ date_string = sanitize_date ( date_string ) 
for locale in self . _get_applicable_locales ( date_string ) : 
~~~ parsed_date = _DateLocaleParser . parse ( 
locale , date_string , date_formats , settings = self . _settings ) 
if parsed_date : 
~~~ parsed_date [ 'locale' ] = locale . shortname 
if self . try_previous_locales : 
~~~ self . previous_locales . insert ( 0 , locale ) 
~~ return parsed_date 
~~~ return { 'date_obj' : None , 'period' : 'day' , 'locale' : None } 
~~ ~~ def get_load_plan ( self ) : 
if self . rps_schedule and self . instances_schedule : 
~~~ raise StepperConfigurationError ( 
~~ elif self . rps_schedule : 
~~~ info . status . publish ( 'loadscheme' , self . rps_schedule ) 
return lp . create ( self . rps_schedule ) 
~~ elif self . instances_schedule : 
~~~ info . status . publish ( 'loadscheme' , self . instances_schedule ) 
return ip . create ( self . instances_schedule ) 
~~~ self . instances_schedule = [ ] 
info . status . publish ( 'loadscheme' , self . instances_schedule ) 
~~ ~~ def get_ammo_generator ( self ) : 
af_readers = { 
'phantom' : missile . AmmoFileReader , 
'slowlog' : missile . SlowLogReader , 
'line' : missile . LineReader , 
'uri' : missile . UriReader , 
'uripost' : missile . UriPostReader , 
'access' : missile . AccessLogReader , 
'caseline' : missile . CaseLineReader , 
if self . uris and self . ammo_file : 
~~ elif self . uris : 
~~~ ammo_gen = missile . UriStyleGenerator ( 
self . uris , self . headers , http_ver = self . http_ver ) 
~~ elif self . ammo_file : 
~~~ if self . ammo_type in af_readers : 
~~~ if self . ammo_type == 'phantom' : 
~~~ opener = resource . get_opener ( self . ammo_file ) 
with opener ( self . use_cache ) as ammo : 
~~~ if not ammo . next ( ) [ 0 ] . isdigit ( ) : 
~~~ self . ammo_type = 'uri' 
self . log . info ( 
~~~ self . log . info ( 
~~ ~~ except StopIteration : 
~~~ self . log . exception ( 
"Couldn\ ) 
raise AmmoFileError ( 
~~~ raise NotImplementedError ( 
\ % self . ammo_type ) 
~~ ammo_gen = af_readers [ self . ammo_type ] ( 
self . ammo_file , headers = self . headers , http_ver = self . http_ver , use_cache = self . use_cache ) 
return ammo_gen 
~~ def _exc_to_net ( param1 , success ) : 
if len ( param1 ) <= 3 : 
~~~ if success : 
~~~ return 314 
if exc in KNOWN_EXC . keys ( ) : 
~~~ return KNOWN_EXC [ exc ] 
~~~ logger . warning ( 
param1 ) 
return 41 
~~ ~~ def _exc_to_http ( param1 ) : 
~~~ int ( param1 ) 
~~~ logger . error ( 
~~~ return int ( param1 ) 
~~ ~~ def read_config ( self ) : 
self . threads = self . cfg [ "threads" ] or str ( int ( multiprocessing . cpu_count ( ) / 2 ) + 1 ) 
self . phantom_modules_path = self . cfg [ "phantom_modules_path" ] 
self . answ_log_level = self . cfg [ "writelog" ] 
if self . answ_log_level . lower ( ) in [ '0' , 'false' ] : 
~~~ self . answ_log_level = 'none' 
~~ elif self . answ_log_level . lower ( ) in [ '1' , 'true' ] : 
~~~ self . answ_log_level = 'all' 
~~ self . timeout = parse_duration ( self . cfg [ "timeout" ] ) 
if self . timeout > 120000 : 
"You\ 
~~ self . answ_log = self . core . mkstemp ( ".log" , "answ_" ) 
self . core . add_artifact_file ( self . answ_log ) 
self . core . add_artifact_file ( self . phout_file ) 
self . core . add_artifact_file ( self . stat_log ) 
self . phantom_log = self . core . mkstemp ( ".log" , "phantom_" ) 
self . core . add_artifact_file ( self . phantom_log ) 
main_stream = StreamConfig ( 
self . core , 
len ( self . streams ) , self . phout_file , self . answ_log , 
self . answ_log_level , self . timeout , self . cfg , True ) 
self . streams . append ( main_stream ) 
for section in self . multi ( ) : 
~~~ self . streams . append ( 
StreamConfig ( 
self . answ_log_level , self . timeout , section ) ) 
~~ for stream in self . streams : 
~~~ stream . read_config ( ) 
~~ if any ( stream . ssl for stream in self . streams ) : 
~~ ~~ def compose_config ( self ) : 
streams_config = '' 
stat_benchmarks = '' 
for stream in self . streams : 
~~~ streams_config += stream . compose_config ( ) 
if not stream . is_main : 
~~ ~~ kwargs = { } 
kwargs [ 'threads' ] = self . threads 
kwargs [ 'phantom_log' ] = self . phantom_log 
kwargs [ 'stat_log' ] = self . stat_log 
kwargs [ 'benchmarks_block' ] = streams_config 
kwargs [ 'stat_benchmarks' ] = stat_benchmarks 
kwargs [ 'additional_libs' ] = self . additional_libs 
kwargs [ 'phantom_modules_path' ] = self . phantom_modules_path 
filename = self . core . mkstemp ( ".conf" , "phantom_" ) 
self . core . add_artifact_file ( filename ) 
template_str = resource_string ( __name__ , "config/phantom.conf.tpl" ) 
tpl = string . Template ( template_str ) 
config = tpl . substitute ( kwargs ) 
with open ( filename , 'w' ) as conffile : 
~~~ conffile . write ( config ) 
~~ return filename 
~~ def get_info ( self ) : 
result = copy . copy ( self . streams [ 0 ] ) 
result . stat_log = self . stat_log 
result . steps = [ ] 
result . ammo_file = '' 
result . rps_schedule = None 
result . ammo_count = 0 
result . duration = 0 
result . instances = 0 
result . loadscheme = [ ] 
result . loop_count = 0 
~~~ sec_no = 0 
for item in stream . stepper_wrapper . steps : 
~~~ for x in range ( 0 , item [ 1 ] ) : 
~~~ if len ( result . steps ) > sec_no : 
~~~ result . steps [ sec_no ] [ 0 ] += item [ 0 ] 
~~~ result . steps . append ( [ item [ 0 ] , 1 ] ) 
~~ sec_no += 1 
~~ ~~ if result . rps_schedule : 
~~~ result . rps_schedule = [ ] 
~~~ result . rps_schedule = stream . stepper_wrapper . loadscheme 
~~ if result . loadscheme : 
~~~ result . loadscheme = '' 
~~ if result . loop_count : 
~~~ result . loop_count = u'0' 
~~~ result . loop_count = stream . stepper_wrapper . loop_count 
result . ammo_count += stream . stepper_wrapper . ammo_count 
result . duration = max ( 
result . duration , stream . stepper_wrapper . duration ) 
result . instances += stream . instances 
~~ if not result . ammo_count : 
~~ def read_config ( self ) : 
self . ssl = self . get_option ( "ssl" ) 
self . tank_type = self . get_option ( "tank_type" ) 
self . method_prefix = self . get_option ( "method_prefix" ) 
self . method_options = self . get_option ( "method_options" ) 
self . source_log_prefix = self . get_option ( "source_log_prefix" ) 
self . phantom_http_line = self . get_option ( "phantom_http_line" ) 
self . phantom_http_field_num = self . get_option ( "phantom_http_field_num" ) 
self . phantom_http_field = self . get_option ( "phantom_http_field" ) 
self . phantom_http_entity = self . get_option ( "phantom_http_entity" ) 
self . address = self . get_option ( 'address' ) 
do_test_connect = self . get_option ( "connection_test" ) 
explicit_port = self . get_option ( 'port' , '' ) 
self . ipv6 , self . resolved_ip , self . port , self . address = self . address_wizard . resolve ( 
self . address , do_test_connect , explicit_port ) 
logger . info ( 
self . client_cipher_suites = self . get_option ( "client_cipher_suites" , "" ) 
self . client_certificate = self . get_option ( "client_certificate" , "" ) 
self . client_key = self . get_option ( "client_key" , "" ) 
self . stepper_wrapper . read_config ( ) 
~~ def compose_config ( self ) : 
self . stepper_wrapper . prepare_stepper ( ) 
self . stpd = self . stepper_wrapper . stpd 
if self . stepper_wrapper . instances : 
~~~ self . instances = self . stepper_wrapper . instances 
~~ if not self . stpd : 
~~ kwargs = { } 
kwargs [ 'sequence_no' ] = self . sequence_no 
if self . ssl : 
~~~ _auth_section = '' 
_ciphers = '' 
if self . client_certificate or self . client_key : 
~~~ _auth_section = \ % ( self . client_key , self . client_certificate ) 
~~ if self . client_cipher_suites : 
~~~ _ciphers = \ % self . client_cipher_suites 
~~ kwargs [ 'ssl_transport' ] = ssl_template % ( _auth_section , _ciphers ) 
~~~ kwargs [ 'ssl_transport' ] = "" 
~~ kwargs [ 'method_stream' ] = self . method_prefix + "_ipv6_t" if self . ipv6 else self . method_prefix + "_ipv4_t" 
kwargs [ 'phout' ] = self . phout_file 
kwargs [ 'answ_log' ] = self . answ_log 
kwargs [ 'answ_log_level' ] = self . answ_log_level 
kwargs [ 'stpd' ] = self . stpd 
kwargs [ 'source_log_prefix' ] = self . source_log_prefix 
kwargs [ 'method_options' ] = self . method_options 
if self . tank_type : 
~~~ kwargs [ 
'proto' ] = "proto=http_proto%s" % self . sequence_no if self . tank_type == 'http' else "proto=none_proto" 
kwargs [ 'comment_proto' ] = "" 
~~~ kwargs [ 'proto' ] = "" 
kwargs [ 'comment_proto' ] = "#" 
~~ if self . gatling : 
~~~ kwargs [ 'bind' ] = '' 
~~ kwargs [ 'ip' ] = self . resolved_ip 
kwargs [ 'port' ] = self . port 
kwargs [ 'timeout' ] = self . timeout 
kwargs [ 'instances' ] = self . instances 
tune = '' 
if self . phantom_http_entity : 
~~ if self . phantom_http_field : 
~~ if self . phantom_http_field_num : 
~~ if self . phantom_http_line : 
~~ if tune : 
~~~ kwargs [ 'reply_limits' ] = '' 
~~ if self . is_main : 
~~~ fname = 'phantom_benchmark_main.tpl' 
~~~ fname = 'phantom_benchmark_additional.tpl' 
~~ template_str = resource_string ( 
__name__ , "config/" + fname ) 
return config 
~~ def patch_config ( self , config ) : 
if config . get ( "monitoring" ) : 
~~~ if config [ "monitoring" ] . get ( "expvar" ) : 
~~~ self . expvar = config [ "monitoring" ] [ "expvar" ] . get ( "enabled" ) 
if config [ "monitoring" ] [ "expvar" ] . get ( "port" ) : 
~~~ self . expvar_port = config [ "monitoring" ] [ "expvar" ] . get ( "port" ) 
~~~ self . expvar_port = self . DEFAULT_EXPVAR_PORT 
~~~ config [ "monitoring" ] = { 
"expvar" : { 
"enabled" : True , 
self . expvar = True 
self . expvar_port = self . DEFAULT_EXPVAR_PORT 
~~ for pool in config [ 'pools' ] : 
~~~ if pool . get ( 'ammo' , { } ) . get ( 'file' , '' ) : 
~~~ self . ammofile = pool [ 'ammo' ] [ 'file' ] 
pool [ 'ammo' ] [ 'file' ] = resource_manager . resource_filename ( 
self . ammofile 
~~ if not pool . get ( 'result' ) or 'phout' not in pool . get ( 'result' , { } ) . get ( 'type' , '' ) : 
pool [ 'result' ] = dict ( 
destination = self . DEFAULT_REPORT_FILE , 
type = 'phout' , 
~~ ~~ return config 
~~ def validate_duration ( self , field , duration ) : 
DURATION_RE = r'^(\\d+d)?(\\d+h)?(\\d+m)?(\\d+s?)?$' 
if not re . match ( DURATION_RE , duration ) : 
~~ ~~ def _validator_load_scheme ( self , field , value ) : 
if self . document [ 'load_type' ] in 'stpd_file' : 
~~ PRIMARY_RE = r'(step|line|const)\\((.+?)\\)' 
N_OF_ARGS = { 
'step' : 4 , 
'line' : 3 , 
'const' : 2 , 
matches = re . findall ( PRIMARY_RE , value ) 
if len ( matches ) == 0 : 
~~~ for match in matches : 
~~~ curve , params_str = match 
params = [ v . strip ( ) for v in params_str . split ( ',' ) ] 
if not len ( params ) == N_OF_ARGS [ curve ] : 
N_OF_ARGS [ curve ] , 
len ( params ) ) ) 
~~ for param in params [ : - 1 ] : 
~~~ if not self . is_number ( param ) : 
~~ ~~ self . validate_duration ( field , params [ - 1 ] ) 
~~ ~~ ~~ def __parse_enabled_plugins ( self ) : 
plugin_name , 
plugin [ 'package' ] , 
plugin ) for plugin_name , 
plugin in self . raw_config_dict . items ( ) if ( 
plugin_name not in self . BASE_SCHEMA . keys ( ) ) and isinstance ( 
plugin , 
dict ) and plugin . get ( 'enabled' ) ] 
~~ def plugins ( self ) : 
if not self . _plugins : 
~~~ self . _plugins = [ 
( plugin_name , 
plugin_cfg [ 'package' ] , 
plugin_cfg ) for plugin_name , plugin_cfg in self . validated . items ( ) if ( 
plugin_name not in self . base_schema . keys ( ) ) and plugin_cfg [ 'enabled' ] ] 
~~ return self . _plugins 
~~ def log_stdout_stderr ( log , stdout , stderr , comment = "" ) : 
readable = select . select ( [ stdout ] , [ ] , [ ] , 0 ) [ 0 ] 
if stderr : 
~~~ exceptional = select . select ( [ stderr ] , [ ] , [ ] , 0 ) [ 0 ] 
~~~ exceptional = [ ] 
for handle in readable : 
~~~ line = handle . read ( ) 
readable . remove ( handle ) 
if line : 
~~ ~~ for handle in exceptional : 
exceptional . remove ( handle ) 
~~ ~~ ~~ def expand_time ( str_time , default_unit = 's' , multiplier = 1 ) : 
parser = re . compile ( r'(\\d+)([a-zA-Z]*)' ) 
parts = parser . findall ( str_time ) 
result = 0.0 
for value , unit in parts : 
~~~ value = int ( value ) 
unit = unit . lower ( ) 
if unit == '' : 
~~~ unit = default_unit 
~~ if unit == 'ms' : 
~~~ result += value * 0.001 
~~ elif unit == 's' : 
~~~ result += value 
~~ elif unit == 'm' : 
~~~ result += value * 60 
~~ elif unit == 'h' : 
~~~ result += value * 60 * 60 
~~ elif unit == 'd' : 
~~~ result += value * 60 * 60 * 24 
~~ elif unit == 'w' : 
~~~ result += value * 60 * 60 * 24 * 7 
~~ ~~ return int ( result * multiplier ) 
~~ def pid_exists ( pid ) : 
if pid < 0 : 
~~~ os . kill ( pid , 0 ) 
~~ except OSError as exc : 
return exc . errno == errno . EPERM 
~~~ p = psutil . Process ( pid ) 
return p . status != psutil . STATUS_ZOMBIE 
~~ ~~ def splitstring ( string ) : 
patt = re . compile ( r\ ) 
if patt . search ( string ) : 
~~~ quoted_item = patt . search ( string ) . group ( ) 
newstring = patt . sub ( '' , string ) 
return newstring . split ( ) + [ quoted_item ] 
~~~ return string . split ( ) 
~~ ~~ def read_with_lock ( self , pos , _len = None ) : 
self . wait_lock ( ) 
~~~ self . _opened_file . seek ( pos ) 
result = self . _opened_file . read ( _len ) if _len is not None else self . _opened_file . readline ( ) 
stop_pos = self . _opened_file . tell ( ) 
~~~ self . unlock ( ) 
~~ if not result and self . stop . is_set ( ) : 
~~~ result = None 
~~ return result , stop_pos 
~~ def get_option ( self , option , param2 = None ) : 
result = self . cfg [ option ] 
self . log . debug ( 
self . ammo_file = self . get_option ( self . OPTION_AMMOFILE ) 
self . ammo_type = self . get_option ( 'ammo_type' ) 
if self . ammo_file : 
~~~ self . ammo_file = os . path . expanduser ( self . ammo_file ) 
~~ self . loop_limit = self . get_option ( self . OPTION_LOOP ) 
self . ammo_limit = self . get_option ( "ammo_limit" ) 
self . load_profile = LoadProfile ( ** self . get_option ( 'load_profile' ) ) 
self . instances = int ( 
self . get_option ( self . OPTION_INSTANCES_LIMIT , '1000' ) ) 
self . uris = self . get_option ( "uris" , [ ] ) 
while '' in self . uris : 
~~~ self . uris . remove ( '' ) 
~~ self . headers = self . get_option ( "headers" ) 
self . http_ver = self . get_option ( "header_http" ) 
self . autocases = self . get_option ( "autocases" ) 
self . enum_ammo = self . get_option ( "enum_ammo" ) 
self . use_caching = self . get_option ( "use_caching" ) 
self . file_cache = self . get_option ( 'file_cache' ) 
cache_dir = self . get_option ( "cache_dir" ) or self . core . artifacts_base_dir 
self . cache_dir = os . path . expanduser ( cache_dir ) 
self . force_stepping = self . get_option ( "force_stepping" ) 
if self . get_option ( self . OPTION_LOAD ) [ self . OPTION_LOAD_TYPE ] == 'stpd_file' : 
~~~ self . stpd = self . get_option ( self . OPTION_LOAD ) [ self . OPTION_SCHEDULE ] 
~~ self . chosen_cases = self . get_option ( "chosen_cases" ) . split ( ) 
if self . chosen_cases : 
~~ ~~ def prepare_stepper ( self ) : 
def publish_info ( stepper_info ) : 
~~~ info . status . publish ( 'loadscheme' , stepper_info . loadscheme ) 
info . status . publish ( 'loop_count' , stepper_info . loop_count ) 
info . status . publish ( 'steps' , stepper_info . steps ) 
info . status . publish ( 'duration' , stepper_info . duration ) 
info . status . ammo_count = stepper_info . ammo_count 
info . status . publish ( 'instances' , stepper_info . instances ) 
self . core . publish ( 'stepper' , 'loadscheme' , stepper_info . loadscheme ) 
self . core . publish ( 'stepper' , 'loop_count' , stepper_info . loop_count ) 
self . core . publish ( 'stepper' , 'steps' , stepper_info . steps ) 
self . core . publish ( 'stepper' , 'duration' , stepper_info . duration ) 
self . core . publish ( 'stepper' , 'ammo_count' , stepper_info . ammo_count ) 
self . core . publish ( 'stepper' , 'instances' , stepper_info . instances ) 
return stepper_info 
~~~ self . stpd = self . __get_stpd_filename ( ) 
if self . use_caching and not self . force_stepping and os . path . exists ( 
self . stpd ) and os . path . exists ( self . __si_filename ( ) ) : 
stepper_info = self . __read_cached_options ( ) 
if self . instances and self . load_profile . is_rps ( ) : 
self . instances ) 
stepper_info = stepper_info . _replace ( 
instances = self . instances ) 
~~ publish_info ( stepper_info ) 
self . force_stepping and os . path . exists ( self . __si_filename ( ) ) ) : 
~~~ os . remove ( self . __si_filename ( ) ) 
~~ self . __make_stpd_file ( ) 
stepper_info = info . status . get_info ( ) 
self . __write_cached_options ( stepper_info ) 
stepper_info = publish_info ( self . __read_cached_options ( ) ) 
~~ self . ammo_count = stepper_info . ammo_count 
self . duration = stepper_info . duration 
self . loop_count = stepper_info . loop_count 
self . loadscheme = stepper_info . loadscheme 
self . steps = stepper_info . steps 
if stepper_info . instances : 
~~~ self . instances = stepper_info . instances 
~~ ~~ def __get_stpd_filename ( self ) : 
if self . use_caching : 
~~~ sep = "|" 
hasher = hashlib . md5 ( ) 
hashed_str += sep + str ( self . ammo_limit ) + sep + ';' . join ( 
self . load_profile . schedule ) + sep + str ( self . autocases ) 
hashed_str += sep + ";" . join ( self . uris ) + sep + ";" . join ( 
self . headers ) + sep + self . http_ver + sep + ";" . join ( 
self . chosen_cases ) 
hashed_str += sep + str ( self . enum_ammo ) + sep + str ( self . ammo_type ) 
if self . load_profile . is_instances ( ) : 
~~~ hashed_str += sep + str ( self . instances ) 
~~ if self . ammo_file : 
hashed_str += sep + opener . hash 
~~~ if not self . uris : 
~~ hashed_str += sep + ';' . join ( self . uris ) + sep + ';' . join ( self . headers ) 
hasher . update ( hashed_str . encode ( 'utf8' ) ) 
if not os . path . exists ( self . cache_dir ) : 
~~~ os . makedirs ( self . cache_dir ) 
~~ stpd = self . cache_dir + '/' + os . path . basename ( self . ammo_file ) + "_" + hasher . hexdigest ( ) + ".stpd" 
~~~ stpd = os . path . realpath ( "ammo.stpd" ) 
return stpd 
~~ def __read_cached_options ( self ) : 
with open ( self . __si_filename ( ) , 'r' ) as si_file : 
~~~ si = info . StepperInfo ( ** json . load ( si_file ) ) 
~~ return si 
~~ def __write_cached_options ( self , si ) : 
with open ( self . __si_filename ( ) , 'w' ) as si_file : 
~~~ json . dump ( si . _asdict ( ) , si_file , indent = 4 ) 
~~ ~~ def __make_stpd_file ( self ) : 
stepper = Stepper ( 
rps_schedule = self . load_profile . schedule if self . load_profile . is_rps ( ) else None , 
http_ver = self . http_ver , 
ammo_file = self . ammo_file , 
instances_schedule = self . load_profile . schedule if self . load_profile . is_instances ( ) else None , 
instances = self . instances , 
loop_limit = self . loop_limit , 
ammo_limit = self . ammo_limit , 
uris = self . uris , 
headers = [ header . strip ( '[]' ) for header in self . headers ] , 
autocases = self . autocases , 
enum_ammo = self . enum_ammo , 
ammo_type = self . ammo_type , 
chosen_cases = self . chosen_cases , 
use_cache = self . use_caching ) 
with open ( self . stpd , 'w' , self . file_cache ) as os : 
~~~ stepper . write ( os ) 
~~ ~~ def create ( rps_schedule ) : 
if len ( rps_schedule ) > 1 : 
~~~ lp = Composite ( 
[ StepFactory . produce ( step_config ) for step_config in rps_schedule ] ) 
~~~ lp = StepFactory . produce ( rps_schedule [ 0 ] ) 
~~ info . status . publish ( 'duration' , lp . get_duration ( ) / 1000 ) 
info . status . publish ( 'steps' , lp . get_rps_list ( ) ) 
info . status . lp_len = len ( lp ) 
return lp 
~~ def ts ( self , n ) : 
~~~ root1 , root2 = solve_quadratic ( self . slope / 2.0 , self . minrps , - n ) 
~~ except ZeroDivisionError : 
~~~ root2 = float ( n ) / self . minrps 
~~ return int ( root2 * 1000 ) 
~~ def rps_at ( self , t ) : 
if 0 <= t <= self . duration : 
~~~ return self . minrps + float ( self . maxrps - self . minrps ) * t / self . duration 
~~ ~~ def get_float_rps_list ( self ) : 
int_rps = range ( int ( self . minrps ) , int ( self . maxrps ) + 1 ) 
step_duration = float ( self . duration ) / len ( int_rps ) 
rps_list = [ ( rps , int ( step_duration ) ) for rps in int_rps ] 
return rps_list 
~~ def get_rps_list ( self ) : 
seconds = range ( 0 , int ( self . duration ) + 1 ) 
rps_groups = groupby ( [ proper_round ( self . rps_at ( t ) ) for t in seconds ] , 
lambda x : x ) 
rps_list = [ ( rps , len ( list ( rpl ) ) ) for rps , rpl in rps_groups ] 
~~ def execute ( self , cmd ) : 
retcode = execute ( 
cmd , shell = True , poll_period = 0.1 , catch_out = self . catch_out ) [ 0 ] 
if retcode : 
~~ return retcode 
~~ def decode_monitoring ( self , data ) : 
points = list ( ) 
for second_data in data : 
~~~ for host , host_data in second_data [ "data" ] . iteritems ( ) : 
~~~ points . append ( 
self . __make_points ( 
"monitoring" , 
{ "host" : host , "comment" : host_data . get ( "comment" ) } , 
second_data [ "timestamp" ] , 
metric : value 
for metric , value in host_data [ "metrics" ] . iteritems ( ) 
~~ ~~ return points 
~~ def __make_points_for_label ( self , ts , data , label , prefix , gun_stats ) : 
label_points = list ( ) 
label_points . extend ( 
prefix + "overall_quantiles" , 
{ "label" : label } , 
ts , 
self . __make_quantile_fields ( data ) 
prefix + "overall_meta" , 
self . __make_overall_meta_fields ( data , gun_stats ) 
prefix + "net_codes" , 
self . __make_netcodes_fields ( data ) 
prefix + "proto_codes" , 
self . __make_protocodes_fields ( data ) 
if self . histograms : 
~~~ for bin_ , count in zip ( data [ "interval_real" ] [ "hist" ] [ "bins" ] , 
data [ "interval_real" ] [ "hist" ] [ "data" ] ) : 
~~~ label_points . append ( 
prefix + "histograms" , 
{ "bin" : bin_ , "count" : count } 
~~ ~~ return label_points 
~~ def __make_points ( self , measurement , additional_tags , ts , fields ) : 
tags = self . tags . copy ( ) 
tags . update ( additional_tags ) 
"measurement" : measurement , 
"tags" : tags , 
"time" : int ( ts ) , 
"fields" : fields , 
~~ def publish ( self , key , value ) : 
self . core . publish ( self . __class__ . __name__ , key , value ) 
~~ def count_matched_codes ( codes_regex , codes_dict ) : 
total = 0 
for code , count in codes_dict . items ( ) : 
~~~ if codes_regex . match ( str ( code ) ) : 
~~~ total += count 
~~ ~~ return total 
~~ def stop ( self ) : 
self . quit . set ( ) 
while sorted ( [ 
self . pool [ i ] . is_alive ( ) 
for i in xrange ( len ( self . pool ) ) ] ) [ - 1 ] : 
~~~ time . sleep ( 1 ) 
~~~ while not self . task_queue . empty ( ) : 
~~~ self . task_queue . get ( timeout = 0.1 ) 
~~ self . task_queue . close ( ) 
self . feeder . join ( ) 
~~ except Exception as ex : 
~~~ logger . info ( ex ) 
~~ ~~ def _feed ( self ) : 
self . plan = StpdReader ( self . stpd_filename ) 
if self . cached_stpd : 
~~~ self . plan = list ( self . plan ) 
~~ for task in self . plan : 
~~~ if self . quit . is_set ( ) : 
~~ while True : 
~~~ self . task_queue . put ( task , timeout = 1 ) 
~~ except Full : 
~~~ if self . quit . is_set ( ) or self . workers_finished : 
~~ ~~ ~~ ~~ workers_count = self . instances 
retry_delay = 1 
for _ in range ( 5 ) : 
~~~ [ 
self . task_queue . put ( None , timeout = 1 ) 
for _ in xrange ( 0 , workers_count ) 
~~~ logger . debug ( 
"Couldn\ 
time . sleep ( retry_delay ) 
retry_delay *= 2 
map ( lambda x : x . join ( ) , self . pool ) 
self . workers_finished = True 
~~ except ( KeyboardInterrupt , SystemExit ) : 
~~~ self . task_queue . close ( ) 
self . results . close ( ) 
~~ ~~ def _worker ( self ) : 
~~~ self . gun . setup ( ) 
~~~ logger . exception ( "Couldn\ ) 
~~ while not self . quit . is_set ( ) : 
~~~ task = self . task_queue . get ( timeout = 1 ) 
if not task : 
~~ timestamp , missile , marker = task 
planned_time = self . start_time + ( timestamp / 1000.0 ) 
delay = planned_time - time . time ( ) 
if delay > 0 : 
~~~ time . sleep ( delay ) 
~~~ with self . instance_counter . get_lock ( ) : 
~~~ self . instance_counter . value += 1 
~~ self . gun . shoot ( missile , marker ) 
~~~ self . instance_counter . value -= 1 
~~ ~~ ~~ except ( KeyboardInterrupt , SystemExit ) : 
~~ except Empty : 
~~ ~~ except Full : 
~~~ logger . warning ( "Couldn\ ) 
~~~ self . gun . teardown ( ) 
~~ def _green_worker ( self ) : 
while not self . quit . is_set ( ) : 
~~~ task = self . green_queue . get ( timeout = 1 ) 
timestamp , missile , marker = task 
~~ self . _free_threads_count += 1 
~~ ~~ except ( KeyboardInterrupt , SystemExit ) : 
~~ ~~ ~~ def init_logging ( self , log_filename = "tank.log" ) : 
logger = logging . getLogger ( '' ) 
self . log_filename = log_filename 
self . core . add_artifact_file ( self . log_filename ) 
file_handler = logging . FileHandler ( self . log_filename ) 
file_handler . setLevel ( logging . DEBUG ) 
file_handler . setFormatter ( 
logging . Formatter ( 
logger . addHandler ( file_handler ) 
console_handler = logging . StreamHandler ( sys . stdout ) 
stderr_hdl = logging . StreamHandler ( sys . stderr ) 
fmt_regular = logging . Formatter ( 
console_handler . setLevel ( logging . INFO ) 
console_handler . setFormatter ( fmt_regular ) 
stderr_hdl . setFormatter ( fmt_regular ) 
f_err = SingleLevelFilter ( logging . ERROR , True ) 
f_warn = SingleLevelFilter ( logging . WARNING , True ) 
f_crit = SingleLevelFilter ( logging . CRITICAL , True ) 
console_handler . addFilter ( f_err ) 
console_handler . addFilter ( f_warn ) 
console_handler . addFilter ( f_crit ) 
logger . addHandler ( console_handler ) 
f_info = SingleLevelFilter ( logging . INFO , True ) 
f_debug = SingleLevelFilter ( logging . DEBUG , True ) 
stderr_hdl . addFilter ( f_info ) 
stderr_hdl . addFilter ( f_debug ) 
logger . addHandler ( stderr_hdl ) 
~~ def __add_user_options ( self ) : 
if self . options . get ( 'user_options' , None ) : 
~~~ self . core . apply_shorthand_options ( self . options [ 'user_options' ] ) 
~~ ~~ def configure ( self , options ) : 
self . options = options 
if self . options . get ( 'lock_dir' , None ) : 
~~~ self . core . set_option ( self . core . SECTION , "lock_dir" , self . options [ 'lock_dir' ] ) 
~~ if self . options . get ( 'ignore_lock' , None ) : 
~~~ self . core . set_option ( self . core . SECTION , 'ignore_lock' , self . options [ 'ignore_lock' ] ) 
~~~ self . core . get_lock ( ) 
~~ except Exception as exc : 
~~~ if self . options . get ( 'lock_fail' , None ) : 
~~ self . log . info ( 
"Couldn\ , 
str ( exc ) ) 
time . sleep ( 5 ) 
~~ ~~ configs = self . get_default_configs ( ) 
if self . options . get ( 'config' , None ) : 
~~~ configs . append ( self . options [ 'config' ] ) 
~~ self . core . load_configs ( configs ) 
self . __add_user_options ( ) 
self . core . load_plugins ( ) 
if self . options . get ( 'ignore_lock' , None ) : 
~~~ self . core . set_option ( self . core . SECTION , self . IGNORE_LOCKS , "1" ) 
~~ ~~ def get_default_configs ( self ) : 
configs = [ resource_filename ( __name__ , 'config/00-base.ini' ) ] 
~~~ conf_files = sorted ( os . listdir ( self . baseconfigs_location ) ) 
for filename in conf_files : 
~~~ if fnmatch . fnmatch ( filename , '*.ini' ) : 
~~~ configs += [ 
os . path . realpath ( 
self . baseconfigs_location + os . sep + filename ) 
~~ ~~ ~~ except OSError : 
~~~ self . log . warn ( 
~~ configs += [ os . path . expanduser ( '~/.yandex-tank' ) ] 
return configs 
~~ def __graceful_shutdown ( self ) : 
retcode = 1 
retcode = self . core . plugins_end_test ( retcode ) 
retcode = self . core . plugins_post_process ( retcode ) 
return retcode 
~~ def _collect_data ( self , end = False ) : 
data = get_nowait_from_queue ( self . results ) 
stats = get_nowait_from_queue ( self . stats_results ) 
for item in data : 
~~~ ts = item [ 'ts' ] 
if ts in self . stat_cache : 
~~~ data_item = item 
stat_item = self . stat_cache . pop ( ts ) 
self . __notify_listeners ( data_item , stat_item ) 
~~~ self . data_cache [ ts ] = item 
~~ ~~ for item in stats : 
if ts in self . data_cache : 
~~~ data_item = self . data_cache . pop ( ts ) 
stat_item = item 
~~~ self . stat_cache [ ts ] = item 
~~ ~~ if end and len ( self . data_cache ) > 0 : 
for ts , data_item in sorted ( self . data_cache . items ( ) , key = lambda i : i [ 0 ] ) : 
~~~ logger . info ( ts ) 
self . __notify_listeners ( data_item , StatsReader . stats_item ( ts , 0 , 0 ) ) 
~~ ~~ ~~ def __notify_listeners ( self , data , stats ) : 
for listener in self . listeners : 
~~~ listener . on_aggregated_data ( data , stats ) 
~~ ~~ def get_marker ( marker_type , enum_ammo = False ) : 
~~~ limit = int ( marker_type ) 
if limit : 
~~~ marker = __UriMarker ( limit ) 
~~~ def marker ( m ) : 
~~~ return '' 
~~ ~~ ~~ except ValueError : 
~~~ if marker_type in __markers : 
~~~ marker = __markers [ marker_type ] 
~~~ raise NotImplementedError ( \ % marker_type ) 
~~ ~~ if enum_ammo : 
~~~ marker = __Enumerator ( marker ) 
~~ return marker 
~~ def get_uploader ( data_session , column_mapping , overall_only = False ) : 
for col_name , name in column_mapping . items ( ) } 
def upload_df ( df ) : 
~~~ for col_name , metric in overall . items ( ) : 
~~~ df [ 'value' ] = df [ col_name ] 
metric . put ( df ) 
~~ ~~ return upload_df 
~~ def cfg_folder_loader ( path ) : 
CFG_WILDCARD = '*.yaml' 
return [ load_cfg ( filename ) for filename in sorted ( glob . glob ( os . path . join ( path , CFG_WILDCARD ) ) ) ] 
~~ def parse_options ( options ) : 
if options is None : 
~~~ return [ 
convert_single_option ( key . strip ( ) , value . strip ( ) ) 
for key , value 
in [ option . split ( '=' , 1 ) for option in options ] 
~~ ~~ def get_default_configs ( ) : 
baseconfigs_location = '/etc/yandex-tank' 
~~~ conf_files = sorted ( os . listdir ( baseconfigs_location ) ) 
baseconfigs_location + os . sep + filename ) 
~~~ logger . info ( 
~~ def clean_markup ( self , orig_str ) : 
for val in [ 
self . YELLOW , self . RED , self . RESET , self . CYAN , self . BG_MAGENTA , 
self . WHITE , self . BG_GREEN , self . GREEN , self . BG_BROWN , 
self . RED_DARK , self . MAGENTA , self . BG_CYAN 
] : 
~~~ orig_str = orig_str . replace ( val , '' ) 
~~ return orig_str 
~~ def parse_duration ( duration ) : 
_re_token = re . compile ( "([0-9.]+)([dhms]?)" ) 
def parse_token ( time , multiplier ) : 
~~~ multipliers = { 
'd' : 86400 , 
'h' : 3600 , 
'm' : 60 , 
's' : 1 , 
if multiplier : 
~~~ if multiplier in multipliers : 
~~~ return int ( float ( time ) * multipliers [ multiplier ] * 1000 ) 
~~~ return int ( float ( time ) * 1000 ) 
~~ ~~ return sum ( parse_token ( * token ) for token in _re_token . findall ( duration ) ) 
~~ def solve_quadratic ( a , b , c ) : 
discRoot = math . sqrt ( ( b * b ) - 4 * a * c ) 
root1 = ( - b - discRoot ) / ( 2 * a ) 
root2 = ( - b + discRoot ) / ( 2 * a ) 
return ( root1 , root2 ) 
~~ def proper_round ( n ) : 
return int ( n ) + ( n / abs ( n ) ) * int ( abs ( n - int ( n ) ) >= 0.5 ) if n != 0 else 0 
~~ def start ( self ) : 
args = self . python . split ( ) + [ 
os . path . join ( 
self . workdir , 
self . AGENT_FILENAME ) , 
'--telegraf' , 
self . path [ 'TELEGRAF_LOCAL_PATH' ] , 
'--host' , 
self . host ] 
if self . kill_old : 
~~~ args . append ( self . kill_old ) 
~~ self . session = self . popen ( args ) 
self . reader_thread = threading . Thread ( target = self . read_buffer ) 
self . reader_thread . setDaemon ( True ) 
return self . session 
~~ def uninstall ( self ) : 
if self . session : 
self . session . terminate ( ) 
self . session . wait ( ) 
self . session = None 
~~ log_filename = "agent_{host}.log" . format ( host = "localhost" ) 
data_filename = "agent_{host}.rawdata" . format ( host = "localhost" ) 
copyfile ( self . workdir + "/_agent.log" , log_filename ) 
copyfile ( self . workdir + "/monitoring.rawdata" , data_filename ) 
rmtree ( self . workdir ) 
return log_filename , data_filename 
~~ def install ( self ) : 
self . username , 
self . host ) 
cmd = self . python + \ 
~~~ out , errors , err_code = self . ssh . execute ( cmd ) 
self . host , 
exc_info = True ) 
return None , None , None 
~~ if errors : 
~~ if err_code : 
( self . username , self . host , err_code , out . strip ( ) ) ) 
~~ remote_dir = out . strip ( ) 
if remote_dir : 
~~~ self . path [ 'AGENT_REMOTE_FOLDER' ] = remote_dir 
self . agent_remote_folder = remote_dir 
~~ logger . debug ( 
agent_config = self . config . create_collector_config ( 
self . path [ 'AGENT_REMOTE_FOLDER' ] ) 
startup_config = self . config . create_startup_config ( ) 
customs_script = self . config . create_custom_exec_script ( ) 
remote_cmd = \ + self . path [ 
'TELEGRAF_REMOTE_PATH' ] + \ 
remote_telegraf_exists = "False" 
~~~ out , err , err_code = self . ssh . execute ( cmd ) 
~~~ if err : 
~~ if out . strip ( ) : 
~~~ remote_telegraf_exists = out . strip ( ) 
~~~ if remote_telegraf_exists in "True" : 
if os . path . isfile ( self . path [ 'TELEGRAF_LOCAL_PATH' ] ) : 
~~~ self . ssh . send_file ( 
self . path [ 'TELEGRAF_REMOTE_PATH' ] ) 
~~ elif os . path . isfile ( "/usr/bin/telegraf" ) : 
'/usr/bin/telegraf' , self . path [ 'TELEGRAF_REMOTE_PATH' ] ) 
~~ ~~ self . ssh . send_file ( 
self . path [ 'AGENT_LOCAL_FOLDER' ] , 
self . path [ 'AGENT_REMOTE_FOLDER' ] , 
self . AGENT_FILENAME ) ) 
self . ssh . send_file ( 
agent_config , 
'agent.cfg' ) ) 
startup_config , 
'agent_startup.cfg' ) ) 
customs_script , 
'agent_customs.sh' ) ) 
~~ return agent_config , startup_config , customs_script 
python = self . python , 
agent_path = os . path . join ( 
telegraf_path = self . path [ 'TELEGRAF_REMOTE_PATH' ] , 
host = self . host , 
kill_old = self . kill_old ) 
self . session = self . ssh . async_session ( command ) 
log_filename = "agent_{host}.log" . format ( host = self . host ) 
data_filename = "agent_{host}.rawdata" . format ( host = self . host ) 
~~~ if self . session : 
~~~ self . session . send ( "stop\\n" ) 
self . session . close ( ) 
~~ ~~ except BaseException : 
log_filename , 
~~~ self . ssh . get_file ( 
"_agent.log" ) , 
log_filename ) 
self . ssh . get_file ( 
"monitoring.rawdata" ) , 
data_filename ) 
self . ssh . rm_r ( self . path [ 'AGENT_REMOTE_FOLDER' ] ) 
~~ ~~ self . _kill_agent ( ) 
~~ def parse_sections ( cfg_ini ) : 
return [ Section ( section . lower ( ) , 
guess_plugin ( section . lower ( ) ) , 
without_defaults ( cfg_ini , section ) ) 
for section in cfg_ini . sections ( ) 
if not re . match ( CORE_SECTION_PATTERN , section . lower ( ) ) and section . lower ( ) not in DEPRECATED_SECTIONS ] 
~~ def combine_sections ( sections ) : 
PLUGINS_TO_COMBINE = { 
'Phantom' : ( 'phantom' , 'multi' , True ) , 
'Bfg' : ( 'bfg' , 'gun_config' , False ) 
plugins = { } 
ready_sections = [ ] 
for section in sections : 
~~~ if section . plugin in PLUGINS_TO_COMBINE . keys ( ) : 
~~~ plugins [ section . plugin ] . append ( section ) 
~~~ plugins [ section . plugin ] = [ section ] 
~~~ ready_sections . append ( section ) 
~~ ~~ for plugin_name , _sections in plugins . items ( ) : 
~~~ if isinstance ( _sections , list ) : 
~~~ parent_name , child_name , is_list = PLUGINS_TO_COMBINE [ plugin_name ] 
ready_sections . append ( Section . from_multiple ( _sections , parent_name , child_name , is_list ) ) 
~~ ~~ return ready_sections 
~~ def converted ( self ) : 
if self . _converted is None : 
~~~ self . _converted = self . converter ( self . name , self . value ) 
~~ return self . _converted 
~~ def as_tuple ( self ) : 
if self . _as_tuple is None : 
~~~ self . _as_tuple = self . converted . items ( ) [ 0 ] 
~~ return self . _as_tuple 
~~ def converter ( self ) : 
if self . _converter is None : 
~~~ self . _converter = self . SPECIAL_CONVERTERS [ self . plugin ] [ self . name ] 
~~~ self . _converter = self . _get_scheme_converter ( ) 
~~ except UnknownOption : 
~~~ self . _converter = self . CONVERTERS_FOR_UNKNOWN . get ( self . plugin , self . dummy_converter ) 
~~ ~~ ~~ return self . _converter 
~~ def from_multiple ( cls , sections , parent_name = None , child_name = None , is_list = True ) : 
if len ( sections ) == 1 : 
~~~ return sections [ 0 ] 
~~ if parent_name : 
~~~ master_section = filter ( lambda section : section . name == parent_name , sections ) [ 0 ] 
rest = filter ( lambda section : section . name != parent_name , sections ) 
~~~ master_section = sections [ 0 ] 
parent_name = master_section . name 
rest = sections [ 1 : ] 
~~ child = { 'multi' : [ section . get_cfg_dict ( with_meta = False ) for section in rest ] } if is_list else { child_name : cls . _select_one ( master_section , rest ) . get_cfg_dict ( with_meta = False ) } 
master_section . merged_options . update ( child ) 
return master_section 
prepared_content = content . strip ( ) . replace ( '\\n' , new_line_replacement ) . replace ( '\\t' , tab_replacement ) 
return u'{}\\n{}' . format ( prepared_content , '=' * len ( prepared_content ) ) 
~~ def __discover_jmeter_udp_port ( self ) : 
r = re . compile ( self . DISCOVER_PORT_PATTERN ) 
with open ( self . process_stderr . name , 'r' ) as f : 
~~~ cnt = 0 
while self . process . pid and cnt < 10 : 
~~~ line = f . readline ( ) 
m = r . match ( line ) 
if m is None : 
~~~ cnt += 1 
time . sleep ( 1 ) 
~~~ port = int ( m . group ( 'port' ) ) 
return port 
~~ ~~ ~~ def __add_jmeter_components ( self , jmx , jtl , variables ) : 
with open ( jmx , 'r' ) as src_jmx : 
~~~ source_lines = src_jmx . readlines ( ) 
~~~ closing = source_lines . pop ( - 1 ) 
if "WorkBenchGui" in source_lines [ - 5 ] : 
last_string_count = 6 
~~~ last_string_count = 2 
~~ while last_string_count > 0 : 
~~~ closing = source_lines . pop ( - 1 ) + closing 
last_string_count -= 1 
~~ udv_tpl = resource_string ( __name__ , 'config/jmeter_var_template.xml' ) 
udv_set = [ ] 
for var_name , var_value in variables . iteritems ( ) : 
~~~ udv_set . append ( udv_tpl % ( var_name , var_name , var_value ) ) 
~~ udv = "\\n" . join ( udv_set ) 
if self . jmeter_ver >= 2.13 : 
~~~ save_connect = '<connectTime>true</connectTime>' 
~~~ save_connect = '' 
~~ if self . ext_log in [ 'errors' , 'all' ] : 
~~~ level_map = { 'errors' : 'true' , 'all' : 'false' } 
tpl_resource = 'jmeter_writer_ext.xml' 
tpl_args = { 
'jtl' : self . jtl_file , 
'udv' : udv , 
'ext_log' : self . ext_log_file , 
'ext_level' : level_map [ self . ext_log ] , 
'save_connect' : save_connect 
~~~ tpl_resource = 'jmeter_writer.xml' 
~~ tpl = resource_string ( __name__ , 'config/' + tpl_resource ) 
~~~ new_jmx = self . core . mkstemp ( 
'.jmx' , 'modified_' , os . path . dirname ( os . path . realpath ( jmx ) ) ) 
~~~ logger . debug ( "Can\ , exc ) 
new_jmx = self . core . mkstemp ( '.jmx' , 'modified_' ) 
with open ( new_jmx , "wb" ) as fh : 
~~~ fh . write ( '' . join ( source_lines ) ) 
fh . write ( tpl % tpl_args ) 
fh . write ( closing ) 
~~ return new_jmx 
~~ def __terminate ( self ) : 
if self . __stderr_file : 
~~~ self . __stderr_file . close ( ) 
~~ if not self . __process : 
~~ waitfor = time . time ( ) + _PROCESS_KILL_TIMEOUT 
while time . time ( ) < waitfor : 
~~~ self . __process . terminate ( ) 
~~ except EnvironmentError as e : 
~~~ if e . errno != errno . ESRCH : 
~~ time . sleep ( 0.1 ) 
~~~ self . __process . kill ( ) 
~~ ~~ def _read_data ( self , lines ) : 
for line in lines : 
~~~ timestamp , rps , instances = line . split ( "\\t" ) 
if self . __last_ts < curr_ts : 
~~~ self . __last_ts = curr_ts 
results . append ( self . stats_item ( self . __last_ts , float ( rps ) , float ( instances ) ) ) 
~~ def __create_criterion ( self , criterion_str ) : 
parsed = criterion_str . split ( "(" ) 
type_str = parsed [ 0 ] . strip ( ) . lower ( ) 
parsed [ 1 ] = parsed [ 1 ] . split ( ")" ) [ 0 ] . strip ( ) 
for criterion_class in self . custom_criterions : 
~~~ if criterion_class . get_type_string ( ) == type_str : 
~~~ return criterion_class ( self , parsed [ 1 ] ) 
~~ ~~ raise ValueError ( 
~~ def getconfig ( self , filename , target_hint ) : 
~~~ tree = self . parse_xml ( filename ) 
~~ except IOError as exc : 
raise RuntimeError ( "Can\ % filename ) 
~~ hosts = tree . findall ( 'Host' ) 
config = [ ] 
for host in hosts : 
~~~ host_config = self . get_host_config ( host , target_hint ) 
config . append ( host_config ) 
~~ return config 
~~ def create_startup_config ( self ) : 
cfg_path = "agent_startup_{}.cfg" . format ( self . host ) 
if os . path . isfile ( cfg_path ) : 
handle , cfg_path = tempfile . mkstemp ( '.cfg' , 'agent_' ) 
os . close ( handle ) 
~~~ config = ConfigParser . RawConfigParser ( ) 
config . add_section ( 'startup' ) 
config . set ( 'startup' , "cmd%s" % idx , cmd ) 
for idx , cmd in enumerate ( self . startups ) 
config . add_section ( 'shutdown' ) 
config . set ( 'shutdown' , "cmd%s" % idx , cmd ) 
for idx , cmd in enumerate ( self . shutdowns ) 
config . add_section ( 'source' ) 
config . set ( 'source' , "file%s" % idx , path ) 
for idx , path in enumerate ( self . sources ) 
with open ( cfg_path , 'w' ) as fds : 
~~~ config . write ( fds ) 
~~ ~~ except Exception as exc : 
exc , 
~~ return cfg_path 
~~ def create_custom_exec_script ( self ) : 
cfg_path = "agent_customs_{}.cfg" . format ( self . host ) 
handle , cfg_path = tempfile . mkstemp ( '.sh' , 'agent_customs_' ) 
~~ cmds = "" 
for idx , cmd in enumerate ( self . custom ) : 
~~~ fds . write ( customs_script ) 
~~ def create_collector_config ( self , workdir ) : 
cfg_path = "agent_collector_{}.cfg" . format ( self . host ) 
handle , cfg_path = tempfile . mkstemp ( '.cfg' , 'agent_collector_' ) 
~~ self . monitoring_data_output = "{remote_folder}/monitoring.rawdata" . format ( 
remote_folder = workdir ) 
defaults_old_enabled = [ 'CPU' , 'Memory' , 'Disk' , 'Net' , 'System' ] 
config . add_section ( "global_tags" ) 
config . add_section ( "agent" ) 
config . set ( 
"agent" , 
"interval" , 
"\ . format ( interval = self . interval ) ) 
config . set ( "agent" , "round_interval" , "true" ) 
config . set ( "agent" , "flush_interval" , "\ ) 
config . set ( "agent" , "collection_jitter" , "\ ) 
config . set ( "agent" , "flush_jitter" , "\ ) 
for section in self . host_config . keys ( ) : 
~~~ if not self . old_style_configs : 
~~~ config . add_section ( 
"{section_name}" . format ( 
section_name = self . host_config [ section ] [ 'name' ] ) ) 
for key , value in iteritems ( self . host_config [ section ] ) : 
~~~ config . set ( 
section_name = self . host_config [ section ] [ 
'name' ] ) , 
"{key}" . format ( key = key ) , 
"{value}" . format ( value = value ) ) 
~~~ if section in defaults_old_enabled : 
~~~ if key in [ 
'fielddrop' , 'fieldpass' , 'percpu' , 
'devices' , 'interfaces' 
~~ ~~ ~~ ~~ ~~ config . add_section ( "[outputs.file]" ) 
"[outputs.file]" , 
"files" , 
"[\ . format ( config = self . monitoring_data_output ) ) 
config . set ( "[outputs.file]" , "data_format" , "\ ) 
~~ inputs = "" 
~~~ inputs += "[[inputs.exec]]\\n" 
workdir = workdir , idx = idx ) 
if cmd [ 'diff' ] : 
~~~ decoder . diff_metrics [ 'custom' ] . append ( 
decoder . find_common_names ( cmd . get ( 'label' ) ) ) 
~~ ~~ with open ( cfg_path , 'a' ) as fds : 
~~~ fds . write ( inputs ) 
~~ telegraf_raw = "" 
for element in self . telegrafraw : 
~~~ telegraf_raw += element 
~~ with open ( cfg_path , 'a' ) as fds : 
~~~ fds . write ( telegraf_raw ) 
~~ def __check_disk ( self ) : 
cmd += self . core . artifacts_base_dir 
res = execute ( cmd , True , 0.1 , True ) 
if not len ( res [ 1 ] ) : 
~~ disk_free = res [ 1 ] 
if int ( disk_free . strip ( ) ) < self . disk_limit : 
% ( 
self . disk_limit , self . core . artifacts_base_dir , 
int ( disk_free . strip ( ) ) ) ) 
~~ ~~ def __check_mem ( self ) : 
mem_free = psutil . virtual_memory ( ) . available / 2 ** 20 
if mem_free < self . mem_limit : 
~~ ~~ def get_terminal_size ( ) : 
default_size = ( 30 , 120 ) 
env = os . environ 
def ioctl_gwinsz ( file_d ) : 
~~~ sizes = struct . unpack ( 
'hh' , fcntl . ioctl ( file_d , termios . TIOCGWINSZ , '1234' ) ) 
~~~ sizes = default_size 
~~ return sizes 
~~ sizes = ioctl_gwinsz ( 0 ) or ioctl_gwinsz ( 1 ) or ioctl_gwinsz ( 2 ) 
if not sizes : 
~~~ file_d = os . open ( os . ctermid ( ) , os . O_RDONLY ) 
sizes = ioctl_gwinsz ( file_d ) 
os . close ( file_d . fileno ( ) ) 
~~ ~~ if not sizes : 
~~~ sizes = ( env [ 'LINES' ] , env [ 'COLUMNS' ] ) 
~~ ~~ return int ( sizes [ 1 ] ) , int ( sizes [ 0 ] ) 
~~ def __get_right_line ( self , widget_output ) : 
right_line = '' 
if widget_output : 
~~~ right_line = widget_output . pop ( 0 ) 
if len ( right_line ) > self . right_panel_width : 
~~~ right_line_plain = self . markup . clean_markup ( right_line ) 
if len ( right_line_plain ) > self . right_panel_width : 
~~~ right_line = right_line [ : self . right_panel_width ] + self . markup . RESET 
~~ ~~ ~~ return right_line 
~~ def __truncate ( self , line_arr , max_width ) : 
def is_space ( chunk ) : 
~~ def is_empty ( chunks , markups ) : 
~~~ result = [ ] 
for chunk in chunks : 
~~~ if chunk in markups : 
~~~ result . append ( True ) 
~~ elif is_space ( chunk ) : 
~~~ result . append ( False ) 
~~ ~~ return all ( result ) 
~~ left = max_width 
result = '' 
markups = self . markup . get_markup_vars ( ) 
for num , chunk in enumerate ( line_arr ) : 
~~~ result += chunk 
~~~ if left > 0 : 
~~~ if len ( chunk ) <= left : 
left -= len ( chunk ) 
~~~ leftover = ( chunk [ left : ] , ) + line_arr [ num + 1 : ] 
was_cut = not is_empty ( leftover , markups ) 
if was_cut : 
~~~ result += chunk [ : left - 1 ] + self . markup . RESET + u'\\u2026' 
~~~ result += chunk [ : left ] 
~~ left = 0 
~~ ~~ ~~ ~~ return result 
~~ def __render_left_panel ( self ) : 
left_block = self . left_panel 
left_block . render ( ) 
blank_space = self . left_panel_width - left_block . width 
if not left_block . lines : 
~~~ while self . left_panel . lines : 
~~~ src_line = self . left_panel . lines . pop ( 0 ) 
line = pre_space + self . __truncate ( src_line , self . left_panel_width ) 
line += post_space + self . markup . RESET 
lines . append ( line ) 
~~ ~~ return lines 
~~ def render_screen ( self ) : 
self . term_width , self . term_height = get_terminal_size ( ) 
self . right_panel_width = int ( 
( self . term_width - len ( self . RIGHT_PANEL_SEPARATOR ) ) 
* ( float ( self . info_panel_percent ) / 100 ) ) - 1 
if self . right_panel_width > 0 : 
~~~ self . left_panel_width = self . term_width - self . right_panel_width - len ( self . RIGHT_PANEL_SEPARATOR ) - 2 
~~~ self . right_panel_width = 0 
self . left_panel_width = self . term_width - 1 
~~ self . log . debug ( 
self . right_panel_width ) 
widget_output = [ ] 
if self . right_panel_width : 
~~~ widget_output = [ ] 
for index , widget in sorted ( 
self . info_widgets . iteritems ( ) , 
key = lambda item : ( item [ 1 ] . get_index ( ) , item [ 0 ] ) ) : 
widget_out = widget . render ( self ) . strip ( ) 
if widget_out : 
~~~ widget_output += widget_out . split ( "\\n" ) 
widget_output += [ "" ] 
~~ ~~ ~~ left_lines = self . __render_left_panel ( ) 
output = [ ] 
for line_no in range ( 1 , self . term_height ) : 
if line_no > 1 and left_lines : 
~~~ left_line = left_lines . pop ( 0 ) 
left_line_plain = self . markup . clean_markup ( left_line ) 
left_line += ( 
line += left_line 
~~ if self . right_panel_width : 
~~~ line += self . markup . RESET 
line += self . markup . WHITE 
line += self . RIGHT_PANEL_SEPARATOR 
line += self . markup . RESET 
right_line = self . __get_right_line ( widget_output ) 
line += right_line 
~~ output . append ( line ) 
~~ return self . markup . new_line . join ( output ) + self . markup . new_line 
~~ def add_info_widget ( self , widget ) : 
index = widget . get_index ( ) 
while index in self . info_widgets . keys ( ) : 
~~~ index += 1 
~~ self . info_widgets [ widget . get_index ( ) ] = widget 
~~ def fill_rectangle ( self , prepared ) : 
result = [ ] 
width = max ( [ self . clean_len ( line ) for line in prepared ] ) 
for line in prepared : 
result . append ( line + ( self . screen . markup . RESET , spacer ) ) 
~~ return ( width , result ) 
~~ def clean_len ( self , line ) : 
if isinstance ( line , basestring ) : 
~~~ return len ( self . screen . markup . clean_markup ( line ) ) 
~~ elif isinstance ( line , tuple ) or isinstance ( line , list ) : 
~~~ markups = self . screen . markup . get_markup_vars ( ) 
length = 0 
for i in line : 
~~~ if i not in markups : 
~~~ length += len ( i ) 
~~ ~~ return length 
~~ ~~ def create ( instances_schedule ) : 
lpb = LoadPlanBuilder ( ) . add_all_steps ( instances_schedule ) 
lp = lpb . create ( ) 
info . status . publish ( 'duration' , 0 ) 
info . status . publish ( 'steps' , [ ] ) 
info . status . publish ( 'instances' , lpb . instances ) 
~~ def get_level_str ( self ) : 
if self . is_relative : 
~~~ level_str = str ( self . level ) + "%" 
~~~ level_str = self . level 
~~ return level_str 
~~ def calc_measurement_error ( self , tangents ) : 
if len ( tangents ) < 2 : 
~~ avg_tan = float ( sum ( tangents ) / len ( tangents ) ) 
numerator = float ( ) 
for i in tangents : 
~~~ numerator += ( i - avg_tan ) * ( i - avg_tan ) 
~~ return math . sqrt ( numerator / len ( tangents ) / ( len ( tangents ) - 1 ) ) 
if not self . screen : 
~~~ self . screen . add_info_widget ( widget ) 
~~ ~~ def clean_markup ( self , orig_str ) : 
for val in self . get_markup_vars ( ) : 
~~ def __make_writer_request ( 
params = None , 
json = None , 
http_method = "POST" , 
trace = False ) : 
request = requests . Request ( 
http_method , 
self . writer_url , 
params = params , 
json = json , 
'User-Agent' : self . user_agent } ) 
ids = id_gen ( str ( uuid . uuid4 ( ) ) ) 
network_timeouts = self . network_timeouts ( ) 
maintenance_timeouts = self . maintenance_timeouts ( ) 
~~~ response = self . __send_single_request ( request , ids . next ( ) , trace = trace ) 
~~ except ( Timeout , ConnectionError , ProtocolError ) : 
~~~ logger . warn ( traceback . format_exc ( ) ) 
~~~ timeout = next ( network_timeouts ) 
logger . warn ( 
timeout ) 
time . sleep ( timeout ) 
~~~ raise self . NetworkError ( ) 
~~ ~~ except self . UnderMaintenance as e : 
~~~ timeout = next ( maintenance_timeouts ) 
~~ ~~ ~~ ~~ def new_job ( 
task , 
person , 
tank , 
target_host , 
target_port , 
loadscheme = None , 
detailed_time = None , 
notify_list = None , 
if not notify_list : 
~~~ notify_list = [ ] 
~~ data = { 
'task' : task , 
'person' : person , 
'tank' : tank , 
'host' : target_host , 
'port' : target_port , 
'loadscheme' : loadscheme , 
'detailed_time' : detailed_time , 
'notify' : notify_list 
api_timeouts = self . api_timeouts ( ) 
~~~ response = self . __post ( 
"api/job/create.json" , data , trace = trace ) [ 0 ] 
return response [ 'job' ] , response [ 'upload_token' ] 
~~ except ( self . NotAvailable , self . StoppedFromOnline ) as e : 
~~~ timeout = next ( api_timeouts ) 
raise self . JobNotCreated ( e . message ) 
~~ ~~ except requests . HTTPError as e : 
logger . warn ( repr ( e ) , ) 
raise self . JobNotCreated ( ) 
~~ ~~ ~~ def plugins ( self ) : 
if self . _plugins is None : 
~~~ self . load_plugins ( ) 
~~~ self . _plugins = { } 
~~ ~~ return self . _plugins 
~~ def load_plugins ( self ) : 
for ( plugin_name , plugin_path , plugin_cfg ) in self . config . plugins : 
if plugin_path == "yandextank.plugins.Overload" : 
~~~ plugin = il . import_module ( plugin_path ) 
~~~ instance = getattr ( plugin , 'Plugin' ) ( self , cfg = plugin_cfg , name = plugin_name ) 
~~~ self . register_plugin ( self . PLUGIN_PREFIX + plugin_name , instance ) 
~~ def plugins_configure ( self ) : 
self . publish ( "core" , "stage" , "configure" ) 
self . taskset_affinity = self . get_option ( self . SECTION , 'affinity' ) 
if self . taskset_affinity : 
~~~ self . __setup_taskset ( self . taskset_affinity , pid = os . getpid ( ) ) 
~~ for plugin in self . plugins . values ( ) : 
~~~ if not self . interrupted . is_set ( ) : 
plugin . configure ( ) 
~~ ~~ ~~ def wait_for_finish ( self ) : 
if not self . interrupted . is_set ( ) : 
self . publish ( "core" , "stage" , "shoot" ) 
if not self . plugins : 
~~~ raise RuntimeError ( "It\ ) 
~~ ~~ while not self . interrupted . is_set ( ) : 
~~~ begin_time = time . time ( ) 
aggr_retcode = self . job . aggregator . is_test_finished ( ) 
if aggr_retcode >= 0 : 
~~~ return aggr_retcode 
retcode = plugin . is_test_finished ( ) 
if retcode >= 0 : 
~~~ return retcode 
~~ ~~ end_time = time . time ( ) 
diff = end_time - begin_time 
if diff < 0.5 : 
~~~ time . sleep ( 0.5 - diff ) 
~~ ~~ return 1 
~~ def plugins_post_process ( self , retcode ) : 
self . publish ( "core" , "stage" , "post_process" ) 
for plugin in self . plugins . values ( ) : 
retcode = plugin . post_process ( retcode ) 
if not retcode : 
~~~ retcode = 1 
~~ ~~ ~~ return retcode 
~~ def __setup_taskset ( self , affinity , pid = None , args = None ) : 
self . taskset_path = self . get_option ( self . SECTION , 'taskset_path' ) 
if args : 
~~~ return [ self . taskset_path , '-c' , affinity ] + args 
~~ if pid : 
retcode , stdout , stderr = execute ( args , shell = True , poll_period = 0.1 , catch_out = True ) 
if retcode == 0 : 
raise KeyError ( stderr ) 
~~ ~~ ~~ def get_plugin_of_type ( self , plugin_class ) : 
matches = [ plugin for plugin in self . plugins . values ( ) if isinstance ( plugin , plugin_class ) ] 
if matches : 
~~~ if len ( matches ) > 1 : 
plugin_class ) 
~~ return matches [ - 1 ] 
~~ ~~ def get_plugins_of_type ( self , plugin_class ) : 
~~~ return matches 
~~ ~~ def __collect_file ( self , filename , keep_original = False ) : 
dest = self . artifacts_dir + '/' + os . path . basename ( filename ) 
if not filename or not os . path . exists ( filename ) : 
~~ if os . path . exists ( dest ) : 
~~ if keep_original : 
~~~ shutil . copy ( filename , self . artifacts_dir ) 
~~~ shutil . move ( filename , self . artifacts_dir ) 
~~ os . chmod ( dest , 0o644 ) 
~~ def add_artifact_file ( self , filename , keep_original = False ) : 
filename ) 
self . artifact_files [ filename ] = keep_original 
~~ ~~ def mkstemp ( self , suffix , prefix , directory = None ) : 
if not directory : 
~~~ directory = self . artifacts_dir 
~~ fd , fname = tempfile . mkstemp ( suffix , prefix , directory ) 
os . close ( fd ) 
return fname 
~~ def close ( self ) : 
~~~ plugin . close ( ) 
logger . debug ( 
~~ ~~ ~~ def load_files ( self , configs ) : 
config_filenames = [ resource . resource_filename ( config ) for config in configs ] 
~~~ self . config . read ( config_filenames ) 
~~~ logger . error ( "Can\ , ex ) 
raise ex 
~~ ~~ def flush ( self , filename = None ) : 
~~~ filename = self . file 
~~~ with open ( filename , 'w' ) as handle : 
~~~ self . config . write ( handle ) 
~~ ~~ ~~ def get_options ( self , section , prefix = '' ) : 
~~~ for option in self . config . options ( section ) : 
~~~ if not prefix or option . find ( prefix ) == 0 : 
~~~ res += [ ( 
option [ len ( prefix ) : ] , self . config . get ( section , option ) ) ] 
~~ ~~ ~~ except ConfigParser . NoSectionError as ex : 
~~ def find_sections ( self , prefix ) : 
for section in self . config . sections ( ) : 
~~~ if section . startswith ( prefix ) : 
~~~ res . append ( section ) 
~~ ~~ return res 
~~ def _decode_stat_data ( self , chunk ) : 
for date_str , statistics in chunk . iteritems ( ) : 
~~~ date_obj = datetime . datetime . strptime ( 
chunk_date = int ( time . mktime ( date_obj . timetuple ( ) ) ) 
instances = 0 
for benchmark_name , benchmark in statistics . iteritems ( ) : 
~~~ if not benchmark_name . startswith ( "benchmark_io" ) : 
~~ for method , meth_obj in benchmark . iteritems ( ) : 
~~~ if "mmtasks" in meth_obj : 
~~~ instances += meth_obj [ "mmtasks" ] [ 2 ] 
~~ ~~ ~~ offset = chunk_date - 1 - self . start_time 
reqps = 0 
if 0 <= offset < len ( self . phantom_info . steps ) : 
~~~ reqps = self . phantom_info . steps [ offset ] [ 0 ] 
~~ yield self . stats_item ( chunk_date - 1 , instances , reqps ) 
~~ ~~ def phantom ( self ) : 
if not self . _phantom : 
~~~ self . _phantom = PhantomConfig ( self . core , self . cfg , self . stat_log ) 
self . _phantom . read_config ( ) 
~~ return self . _phantom 
if not self . cached_info : 
~~~ if not self . phantom : 
~~ self . cached_info = self . phantom . get_info ( ) 
~~ return self . cached_info 
~~ def prepare ( self ) : 
agent_configs = [ ] 
if self . config : 
~~~ agent_configs = self . config_manager . getconfig ( 
self . config , self . default_target ) 
~~ for config in agent_configs : 
~~~ if config [ 'host' ] in [ 'localhost' , '127.0.0.1' , '::1' ] : 
~~~ client = self . clients [ 'localhost' ] ( 
config , self . old_style_configs , kill_old = self . kill_old ) 
~~~ client = self . clients [ 'ssh' ] ( 
config , self . old_style_configs , timeout = 5 , kill_old = self . kill_old ) 
agent_config , startup_config , customs_script = client . install ( ) 
if agent_config : 
~~~ self . agents . append ( client ) 
self . artifact_files . append ( agent_config ) 
~~ if startup_config : 
~~~ self . artifact_files . append ( startup_config ) 
~~ if customs_script : 
~~~ self . artifact_files . append ( customs_script ) 
~~ ~~ ~~ def start ( self ) : 
[ agent . start ( ) for agent in self . agents ] 
[ agent . reader_thread . start ( ) for agent in self . agents ] 
~~ def poll ( self ) : 
start_time = time . time ( ) 
for agent in self . agents : 
~~~ for collect in agent . reader : 
~~~ if not collect : 
~~ for chunk in collect : 
~~~ ts , prepared_results = chunk 
if self . load_start_time and int ( 
ts ) >= self . load_start_time : 
~~~ ready_to_send = { 
"timestamp" : int ( ts ) , 
"data" : { 
self . hash_hostname ( agent . host ) : { 
"comment" : agent . config . comment , 
"metrics" : prepared_results 
self . __collected_data . append ( ready_to_send ) 
~~ ~~ ~~ ~~ logger . debug ( 
( time . time ( ) - start_time ) * 1000 ) 
collected_data_length = len ( self . __collected_data ) 
if not self . first_data_received and self . __collected_data : 
~~~ self . first_data_received = True 
~~~ self . send_collected_data ( ) 
~~ return collected_data_length 
~~~ log_filename , data_filename = agent . uninstall ( ) 
self . artifact_files . append ( log_filename ) 
self . artifact_files . append ( data_filename ) 
~~ for agent in self . agents : 
agent . reader_thread . join ( 10 ) 
~~ ~~ ~~ def send_collected_data ( self ) : 
data = self . __collected_data 
self . __collected_data = [ ] 
~~~ listener . monitoring_data ( copy . deepcopy ( data ) ) 
~~ ~~ def __detect_configuration ( self ) : 
~~~ is_telegraf = self . core . get_option ( 'telegraf' , "config" ) 
~~~ is_telegraf = None 
~~~ is_monitoring = self . core . get_option ( 'monitoring' , "config" ) 
~~~ is_monitoring = None 
~~ if is_telegraf and is_monitoring : 
~~ if is_telegraf and not is_monitoring : 
~~~ return 'telegraf' 
~~ if not is_telegraf and is_monitoring : 
~~~ return 'monitoring' 
~~ if not is_telegraf and not is_monitoring : 
~~~ is_telegraf_dt = self . core . get_option ( 'telegraf' ) 
~~ except NoOptionError : 
~~~ is_telegraf_dt = None 
~~~ is_monitoring_dt = self . core . get_option ( 'monitoring' ) 
~~~ is_monitoring_dt = None 
~~ if is_telegraf_dt and is_monitoring_dt : 
~~ if is_telegraf_dt and not is_monitoring_dt : 
~~ if not is_telegraf_dt and is_monitoring_dt : 
~~~ self . core . set_option ( 
"telegraf" , "default_target" , is_monitoring_dt ) 
~~ if not is_telegraf_dt and not is_monitoring_dt : 
~~ ~~ ~~ def __handle_data_items ( self , host , data ) : 
for metric , value in data . iteritems ( ) : 
~~~ if value == '' : 
~~~ self . sign [ host ] [ metric ] = - 1 
self . data [ host ] [ metric ] = value 
~~~ if not self . data [ host ] . get ( metric , None ) : 
~~~ self . sign [ host ] [ metric ] = 1 
~~ elif float ( value ) > float ( self . data [ host ] [ metric ] ) : 
~~ elif float ( value ) < float ( self . data [ host ] [ metric ] ) : 
~~~ self . sign [ host ] [ metric ] = 0 
~~ self . data [ host ] [ metric ] = "%.2f" % float ( value ) 
~~ ~~ ~~ def _decode_agents_data ( self , block ) : 
collect = [ ] 
if block : 
~~~ for chunk in block . split ( '\\n' ) : 
~~~ if chunk : 
~~~ prepared_results = { } 
jsn = json . loads ( chunk ) 
for ts , values in jsn . iteritems ( ) : 
~~~ for key , value in values . iteritems ( ) : 
~~~ key_group , key_name = key . split ( '_' ) [ 0 ] . split ( '-' ) [ 0 ] , '_' . join ( key . split ( '_' ) [ 1 : ] ) 
~~~ key_group , key_name = key . split ( '_' ) [ 0 ] , '_' . join ( key . split ( '_' ) [ 1 : ] ) 
~~ if key_group in decoder . diff_metrics . keys ( ) : 
~~~ if key_name in decoder . diff_metrics [ key_group ] : 
~~~ decoded_key = decoder . find_common_names ( 
key ) 
if self . prev_check : 
~~~ value = jsn [ ts ] [ key ] - self . prev_check [ key ] 
value = 0 
~~ prepared_results [ decoded_key ] = value 
prepared_results [ decoded_key ] = value 
~~ ~~ self . prev_check = jsn [ ts ] 
collect . append ( ( ts , prepared_results ) ) 
return [ ] 
chunk , 
~~ ~~ if collect : 
~~~ return collect 
~~ ~~ ~~ async def subscribe ( self , channels ) : 
ws_channels = [ ] 
nats_channels = [ ] 
for c in channels : 
~~~ if c . startswith ( ( 'Q.' , 'T.' , 'A.' , 'AM.' , ) ) : 
~~~ nats_channels . append ( c ) 
~~~ ws_channels . append ( c ) 
~~ ~~ if len ( ws_channels ) > 0 : 
~~~ await self . _ensure_ws ( ) 
await self . _ws . send ( json . dumps ( { 
'action' : 'listen' , 
'data' : { 
'streams' : ws_channels , 
~~ if len ( nats_channels ) > 0 : 
~~~ await self . _ensure_nats ( ) 
await self . polygon . subscribe ( nats_channels ) 
~~ ~~ def run ( self , initial_channels = [ ] ) : 
~~~ loop . run_until_complete ( self . subscribe ( initial_channels ) ) 
loop . run_forever ( ) 
~~~ loop . run_until_complete ( self . close ( ) ) 
~~ ~~ async def close ( self ) : 
if self . _ws is not None : 
~~~ await self . _ws . close ( ) 
~~ if self . polygon is not None : 
~~~ await self . polygon . close ( ) 
~~ ~~ def df ( self ) : 
if not hasattr ( self , '_df' ) : 
~~~ dfs = [ ] 
for symbol , bars in self . items ( ) : 
~~~ df = bars . df . copy ( ) 
df . columns = pd . MultiIndex . from_product ( 
[ [ symbol , ] , df . columns ] ) 
dfs . append ( df ) 
~~ if len ( dfs ) == 0 : 
~~~ self . _df = pd . DataFrame ( ) 
~~~ self . _df = pd . concat ( dfs , axis = 1 ) 
~~ ~~ return self . _df 
~~ def _one_request ( self , method , url , opts , retry ) : 
retry_codes = self . _retry_codes 
resp = self . _session . request ( method , url , ** opts ) 
~~~ resp . raise_for_status ( ) 
~~ except HTTPError as http_error : 
~~~ if resp . status_code in retry_codes and retry > 0 : 
~~~ raise RetryException ( ) 
~~ if 'code' in resp . text : 
~~~ error = resp . json ( ) 
if 'code' in error : 
~~~ raise APIError ( error , http_error ) 
~~ ~~ if resp . text != '' : 
~~~ return resp . json ( ) 
~~ def list_orders ( self , status = None , limit = None , after = None , until = None , 
direction = None , params = None ) : 
if params is None : 
~~~ params = dict ( ) 
~~ if limit is not None : 
~~~ params [ 'limit' ] = limit 
~~ if after is not None : 
~~~ params [ 'after' ] = after 
~~ if until is not None : 
~~~ params [ 'until' ] = until 
~~ if direction is not None : 
~~~ params [ 'direction' ] = direction 
~~ if status is not None : 
~~~ params [ 'status' ] = status 
~~ resp = self . get ( '/orders' , params ) 
return [ Order ( o ) for o in resp ] 
~~ def submit_order ( self , symbol , qty , side , type , time_in_force , 
limit_price = None , stop_price = None , client_order_id = None ) : 
params = { 
'symbol' : symbol , 
'qty' : qty , 
'side' : side , 
'type' : type , 
'time_in_force' : time_in_force , 
if limit_price is not None : 
~~~ params [ 'limit_price' ] = limit_price 
~~ if stop_price is not None : 
~~~ params [ 'stop_price' ] = stop_price 
~~ if client_order_id is not None : 
~~~ params [ 'client_order_id' ] = client_order_id 
~~ resp = self . post ( '/orders' , params ) 
return Order ( resp ) 
~~ def get_order ( self , order_id ) : 
resp = self . get ( '/orders/{}' . format ( order_id ) ) 
~~ def get_position ( self , symbol ) : 
resp = self . get ( '/positions/{}' . format ( symbol ) ) 
return Position ( resp ) 
~~ def list_assets ( self , status = None , asset_class = None ) : 
'status' : status , 
'assert_class' : asset_class , 
resp = self . get ( '/assets' , params ) 
return [ Asset ( o ) for o in resp ] 
~~ def get_asset ( self , symbol ) : 
resp = self . get ( '/assets/{}' . format ( symbol ) ) 
return Asset ( resp ) 
~~ def get_barset ( self , 
symbols , 
timeframe , 
limit = None , 
after = None , 
until = None ) : 
if not isinstance ( symbols , str ) : 
~~~ symbols = ',' . join ( symbols ) 
~~ params = { 
'symbols' : symbols , 
~~ if start is not None : 
~~~ params [ 'start' ] = start 
~~ if end is not None : 
~~~ params [ 'end' ] = end 
~~ resp = self . data_get ( '/bars/{}' . format ( timeframe ) , params ) 
return BarSet ( resp ) 
~~ def lambda_solid ( name = None , inputs = None , output = None , description = None ) : 
output = output or OutputDefinition ( ) 
if callable ( name ) : 
~~~ check . invariant ( inputs is None ) 
check . invariant ( description is None ) 
return _LambdaSolid ( output = output ) ( name ) 
~~ return _LambdaSolid ( name = name , inputs = inputs , output = output , description = description ) 
~~ def solid ( name = None , inputs = None , outputs = None , config_field = None , description = None ) : 
check . invariant ( outputs is None ) 
check . invariant ( config_field is None ) 
return _Solid ( ) ( name ) 
~~ return _Solid ( 
inputs = inputs , 
outputs = outputs , 
config_field = config_field , 
description = description , 
~~ def from_dict ( result_dict ) : 
check . dict_param ( result_dict , 'result_dict' , key_type = str ) 
for name , value in result_dict . items ( ) : 
~~~ results . append ( Result ( value , name ) ) 
~~ return MultipleResults ( * results ) 
~~ def create_joining_subplan ( 
pipeline_def , solid , join_step_key , parallel_steps , parallel_step_output 
check . inst_param ( pipeline_def , 'pipeline_def' , PipelineDefinition ) 
check . inst_param ( solid , 'solid' , Solid ) 
check . str_param ( join_step_key , 'join_step_key' ) 
check . list_param ( parallel_steps , 'parallel_steps' , of_type = ExecutionStep ) 
check . str_param ( parallel_step_output , 'parallel_step_output' ) 
for parallel_step in parallel_steps : 
~~~ check . invariant ( parallel_step . has_step_output ( parallel_step_output ) ) 
~~ join_step = create_join_step ( 
output_name = join_step . step_outputs [ 0 ] . name 
return ExecutionValueSubplan ( 
parallel_steps + [ join_step ] , StepOutputHandle . from_step ( join_step , output_name ) 
~~ def gunzipper ( gzip_file ) : 
path_prefix = os . path . dirname ( gzip_file ) 
output_folder = os . path . join ( path_prefix , 'raw/2019/01/01' ) 
outfile = os . path . join ( output_folder , 'data.json' ) 
if not safe_isfile ( outfile ) : 
~~~ mkdir_p ( output_folder ) 
with gzip . open ( gzip_file , 'rb' ) as f_in , open ( outfile , 'wb' ) as f_out : 
~~~ shutil . copyfileobj ( f_in , f_out ) 
~~ ~~ return [ path_prefix ] 
~~ def _check_key_value_types ( obj , key_type , value_type , key_check = isinstance , value_check = isinstance ) : 
if not isinstance ( obj , dict ) : 
~~~ raise_with_traceback ( _type_mismatch_error ( obj , dict ) ) 
~~ if key_type is str : 
~~~ key_type = string_types 
~~ if value_type is str : 
~~~ value_type = string_types 
~~ for key , value in obj . items ( ) : 
~~~ if key_type and not key_check ( key , key_type ) : 
~~~ raise_with_traceback ( 
CheckError ( 
key_type = repr ( key_type ) , obj_repr = repr ( key ) 
~~ if value_type and not value_check ( value , value_type ) : 
vtype = repr ( value_type ) , obj_type = type ( value ) , key = key , value = value 
~~ ~~ return obj 
~~ def dict_param ( obj , param_name , key_type = None , value_type = None ) : 
~~~ raise_with_traceback ( _param_type_mismatch_exception ( obj , dict , param_name ) ) 
~~ if not ( key_type or value_type ) : 
~~~ return obj 
~~ return _check_key_value_types ( obj , key_type , value_type ) 
~~ def opt_dict_param ( obj , param_name , key_type = None , value_type = None , value_class = None ) : 
if obj is not None and not isinstance ( obj , dict ) : 
~~ if not obj : 
~~~ return { } 
~~ if value_class : 
~~~ return _check_key_value_types ( obj , key_type , value_type = value_class , value_check = issubclass ) 
~~ def construct_event_logger ( event_record_callback ) : 
check . callable_param ( event_record_callback , 'event_record_callback' ) 
return construct_single_handler_logger ( 
'event-logger' , 
DEBUG , 
StructuredLoggerHandler ( 
lambda logger_message : event_record_callback ( construct_event_record ( logger_message ) ) 
~~ def construct_json_event_logger ( json_path ) : 
check . str_param ( json_path , 'json_path' ) 
"json-event-record-logger" , 
JsonEventLoggerHandler ( 
json_path , 
lambda record : construct_event_record ( 
StructuredLoggerMessage ( 
name = record . name , 
message = record . msg , 
level = record . levelno , 
meta = record . dagster_meta , 
record = record , 
~~ def from_file ( cls , path = None ) : 
path = path or cls . CONFIG_PATH 
if not os . path . exists ( path ) : 
raise ConfigFileError ( error ) 
~~ config = read_config ( path ) 
return cls ( config ) 
~~ def get_repository_config ( self , repository ) : 
servers = self . _read_index_servers ( ) 
repo_config = self . _find_repo_config ( servers , repository ) 
return repo_config 
~~ def replace_parameters ( context , nb , parameters ) : 
nb = copy . deepcopy ( nb ) 
param_content = DagsterTranslator . codify ( parameters ) 
newcell = nbformat . v4 . new_code_cell ( source = param_content ) 
newcell . metadata [ 'tags' ] = [ 'injected-parameters' ] 
param_cell_index = _find_first_tagged_cell_index ( nb , 'parameters' ) 
injected_cell_index = _find_first_tagged_cell_index ( nb , 'injected-parameters' ) 
if injected_cell_index >= 0 : 
~~~ before = nb . cells [ : injected_cell_index ] 
after = nb . cells [ injected_cell_index + 1 : ] 
check . int_value_param ( param_cell_index , - 1 , 'param_cell_index' ) 
~~ elif param_cell_index >= 0 : 
~~~ before = nb . cells [ : param_cell_index ] 
after = nb . cells [ param_cell_index + 1 : ] 
~~~ context . log . debug ( 
before = nb . cells [ : 1 ] 
after = nb . cells [ 1 : ] 
~~ nb . cells = before + [ newcell ] + after 
nb . metadata . papermill [ 'parameters' ] = parameters 
return nb 
~~ def nonce_solid ( name , n_inputs , n_outputs ) : 
@ solid ( 
inputs = [ 
InputDefinition ( name = 'input_{}' . format ( i ) ) for i in range ( n_inputs ) 
OutputDefinition ( name = 'output_{}' . format ( i ) ) 
for i in range ( n_outputs ) 
def solid_fn ( context , ** _kwargs ) : 
~~~ for i in range ( 200 ) : 
~~~ time . sleep ( 0.02 ) 
if i % 1000 == 420 : 
~~~ context . log . error ( 
i = i , name = name 
~~ elif i % 100 == 0 : 
~~~ context . log . warning ( 
~~ elif i % 10 == 0 : 
~~~ context . log . info ( 
~~ ~~ return MultipleResults . from_dict ( 
{ 'output_{}' . format ( i ) : 'foo' for i in range ( n_outputs ) } 
~~ return solid_fn 
~~ def format_config_for_graphql ( config ) : 
def _format_config_subdict ( config , current_indent = 0 ) : 
~~~ check . dict_param ( config , 'config' , key_type = str ) 
printer = IndentingStringIoPrinter ( indent_level = 2 , current_indent = current_indent ) 
printer . line ( '{' ) 
n_elements = len ( config ) 
for i , key in enumerate ( sorted ( config , key = lambda x : x [ 0 ] ) ) : 
~~~ value = config [ key ] 
with printer . with_indent ( ) : 
~~~ formatted_value = ( 
_format_config_item ( value , current_indent = printer . current_indent ) 
. rstrip ( '\\n' ) 
printer . line ( 
key = key , 
formatted_value = formatted_value , 
comma = ',' if i != n_elements - 1 else '' , 
~~ ~~ printer . line ( '}' ) 
return printer . read ( ) 
~~ def _format_config_sublist ( config , current_indent = 0 ) : 
~~~ printer = IndentingStringIoPrinter ( indent_level = 2 , current_indent = current_indent ) 
printer . line ( '[' ) 
for i , value in enumerate ( config ) : 
~~~ with printer . with_indent ( ) : 
'{formatted_value}{comma}' . format ( 
formatted_value = formatted_value , comma = ',' if i != n_elements - 1 else '' 
~~ ~~ printer . line ( ']' ) 
~~ def _format_config_item ( config , current_indent = 0 ) : 
if isinstance ( config , dict ) : 
~~~ return _format_config_subdict ( config , printer . current_indent ) 
~~ elif isinstance ( config , list ) : 
~~~ return _format_config_sublist ( config , printer . current_indent ) 
~~ elif isinstance ( config , bool ) : 
~~~ return repr ( config ) . lower ( ) 
~~~ return repr ( config ) . replace ( '\\'' , \ ) 
~~ ~~ check . dict_param ( config , 'config' , key_type = str ) 
if not isinstance ( config , dict ) : 
~~ return _format_config_subdict ( config ) 
~~ def get_pipeline ( self , name ) : 
check . str_param ( name , 'name' ) 
if name in self . _pipeline_cache : 
~~~ return self . _pipeline_cache [ name ] 
~~~ pipeline = self . pipeline_dict [ name ] ( ) 
~~~ raise DagsterInvariantViolationError ( 
\ . format ( pipeline_name = pipeline_name ) 
for pipeline_name in self . pipeline_dict . keys ( ) 
~~ check . invariant ( 
pipeline . name == name , 
name = name , pipeline = pipeline 
self . _pipeline_cache [ name ] = check . inst ( 
pipeline , 
PipelineDefinition , 
'PipelineDefinition' 
) . format ( key = name ) , 
return pipeline 
~~ def get_all_pipelines ( self ) : 
pipelines = list ( map ( self . get_pipeline , self . pipeline_dict . keys ( ) ) ) 
self . _construct_solid_defs ( pipelines ) 
return pipelines 
~~ def define_spark_config ( ) : 
master_url = Field ( 
String , 
is_optional = False , 
deploy_mode = Field ( 
SparkDeployMode , 
is_optional = True , 
application_jar = Field ( 
Path , 
application_arguments = Field ( 
spark_home = Field ( 
return Field ( 
Dict ( 
fields = { 
'master_url' : master_url , 
'deploy_mode' : deploy_mode , 
'application_jar' : application_jar , 
'spark_conf' : spark_config ( ) , 
'spark_home' : spark_home , 
'application_arguments' : application_arguments , 
'spark_outputs' : spark_outputs , 
~~ def get_next_event ( process , queue ) : 
~~~ return queue . get ( block = True , timeout = TICK ) 
~~ except multiprocessing . queues . Empty : 
~~~ if not process . is_alive ( ) : 
~~~ return queue . get ( block = False ) 
~~~ return PROCESS_DEAD_AND_QUEUE_EMPTY 
~~ ~~ ~~ ~~ check . failed ( 'unreachable' ) 
~~ def execute_pipeline_through_queue ( 
repository_info , 
pipeline_name , 
solid_subset , 
environment_dict , 
run_id , 
message_queue , 
reexecution_config , 
step_keys_to_execute , 
message_queue . put ( ProcessStartedSentinel ( os . getpid ( ) ) ) 
run_config = RunConfig ( 
event_callback = message_queue . put , 
executor_config = InProcessExecutorConfig ( raise_on_error = False ) , 
reexecution_config = reexecution_config , 
step_keys_to_execute = step_keys_to_execute , 
repository_container = RepositoryContainer ( repository_info ) 
if repository_container . repo_error : 
~~~ message_queue . put ( 
MultiprocessingError ( 
serializable_error_info_from_exc_info ( repository_container . repo_error ) 
~~~ result = execute_pipeline ( 
repository_container . repository . get_pipeline ( pipeline_name ) . build_sub_pipeline ( 
solid_subset 
run_config = run_config , 
~~~ error_info = serializable_error_info_from_exc_info ( sys . exc_info ( ) ) 
message_queue . put ( MultiprocessingError ( error_info ) ) 
~~~ message_queue . put ( MultiprocessingDone ( ) ) 
message_queue . close ( ) 
~~ ~~ def join ( self ) : 
~~~ with self . _processes_lock : 
~~~ if not self . _processes and self . _processing_semaphore . locked ( ) : 
~~ ~~ gevent . sleep ( 0.1 ) 
~~ ~~ def Field ( 
dagster_type , 
default_value = FIELD_NO_DEFAULT_PROVIDED , 
is_optional = INFER_OPTIONAL_COMPOSITE_FIELD , 
is_secret = False , 
description = None , 
config_type = resolve_to_config_type ( dagster_type ) 
if not config_type : 
~~~ raise DagsterInvalidDefinitionError ( 
) . format ( value_repr = repr ( dagster_type ) ) 
~~ return FieldImpl ( 
config_type = resolve_to_config_type ( dagster_type ) , 
default_value = default_value , 
is_optional = is_optional , 
is_secret = is_secret , 
~~ def define_snowflake_config ( ) : 
account = Field ( 
database = Field ( 
schema = Field ( 
role = Field ( 
warehouse = Field ( 
autocommit = Field ( 
Bool , 
client_prefetch_threads = Field ( 
Int , 
client_session_keep_alive = Field ( 
login_timeout = Field ( 
description = \ , 
network_timeout = Field ( 
ocsp_response_cache_filename = Field ( 
validate_default_parameters = Field ( 
paramstyle = Field ( 
timezone = Field ( 
'account' : account , 
'user' : user , 
'password' : password , 
'database' : database , 
'schema' : schema , 
'role' : role , 
'warehouse' : warehouse , 
'autocommit' : autocommit , 
'client_prefetch_threads' : client_prefetch_threads , 
'client_session_keep_alive' : client_session_keep_alive , 
'login_timeout' : login_timeout , 
'network_timeout' : network_timeout , 
'ocsp_response_cache_filename' : ocsp_response_cache_filename , 
'validate_default_parameters' : validate_default_parameters , 
'paramstyle' : paramstyle , 
'timezone' : timezone , 
~~ def build ( self , pipeline_def , artifacts_persisted ) : 
deps = { step . key : set ( ) for step in self . steps } 
for step in self . steps : 
~~~ for step_input in step . step_inputs : 
~~~ deps [ step . key ] . add ( step_input . prev_output_handle . step_key ) 
~~ ~~ step_dict = { step . key : step for step in self . steps } 
return ExecutionPlan ( pipeline_def , step_dict , deps , artifacts_persisted ) 
~~ def build ( pipeline_def , environment_config ) : 
check . inst_param ( environment_config , 'environment_config' , EnvironmentConfig ) 
plan_builder = _PlanBuilder ( ) 
for solid in solids_in_topological_order ( pipeline_def ) : 
~~~ step_inputs = [ ] 
for input_def in solid . definition . input_defs : 
~~~ prev_step_output_handle = get_input_source_step_handle ( 
pipeline_def , environment_config , plan_builder , solid , input_def 
if not prev_step_output_handle : 
~~ subplan = create_subplan_for_input ( 
pipeline_def , environment_config , solid , prev_step_output_handle , input_def 
plan_builder . add_steps ( subplan . steps ) 
step_inputs . append ( 
StepInput ( 
input_def . name , input_def . runtime_type , subplan . terminal_step_output_handle 
~~ solid_transform_step = create_transform_step ( 
pipeline_def , environment_config , solid , step_inputs 
plan_builder . add_step ( solid_transform_step ) 
for output_def in solid . definition . output_defs : 
~~~ subplan = create_subplan_for_output ( 
pipeline_def , environment_config , solid , solid_transform_step , output_def 
output_handle = solid . output_handle ( output_def . name ) 
plan_builder . set_output_handle ( output_handle , subplan . terminal_step_output_handle ) 
~~ ~~ return plan_builder . build ( 
pipeline_def = pipeline_def , 
artifacts_persisted = environment_config . storage . construct_run_storage ( ) . is_persistent , 
~~ def _build_sub_pipeline ( pipeline_def , solid_names ) : 
check . list_param ( solid_names , 'solid_names' , of_type = str ) 
solid_name_set = set ( solid_names ) 
solids = list ( map ( pipeline_def . solid_named , solid_names ) ) 
deps = { _dep_key_of ( solid ) : { } for solid in solids } 
def _out_handle_of_inp ( input_handle ) : 
~~~ if pipeline_def . dependency_structure . has_dep ( input_handle ) : 
~~~ output_handle = pipeline_def . dependency_structure . get_dep ( input_handle ) 
if output_handle . solid . name in solid_name_set : 
~~~ return output_handle 
~~ for solid in solids : 
~~~ for input_handle in solid . input_handles ( ) : 
~~~ output_handle = _out_handle_of_inp ( input_handle ) 
if output_handle : 
~~~ deps [ _dep_key_of ( solid ) ] [ input_handle . input_def . name ] = DependencyDefinition ( 
solid = output_handle . solid . name , output = output_handle . output_def . name 
~~ ~~ ~~ return PipelineDefinition ( 
name = pipeline_def . name , 
solids = list ( { solid . definition for solid in solids } ) , 
context_definitions = pipeline_def . context_definitions , 
dependencies = deps , 
~~ def solid_named ( self , name ) : 
if name not in self . _solid_dict : 
pipeline_name = self . name , name = name 
~~ return self . _solid_dict [ name ] 
~~ def construct_publish_comands ( additional_steps = None , nightly = False ) : 
publish_commands = ( 
+ ( additional_steps if additional_steps else [ ] ) 
+ [ 
return publish_commands 
~~ def publish ( nightly ) : 
~~~ RCParser . from_file ( ) 
~~ except ConfigFileError : 
~~~ raise ConfigFileError ( PYPIRC_EXCEPTION_MESSAGE ) 
~~ assert '\\nwheel' in subprocess . check_output ( [ 'pip' , 'list' ] ) . decode ( 'utf-8' ) , ( 
assert which_ ( 'twine' ) , ( 
assert which_ ( 'yarn' ) , ( 
'https://yarnpkg.com/lang/en/docs/install/' 
check_versions ( nightly = nightly ) 
if not nightly : 
check_git_status ( ) 
if nightly : 
~~~ new_version = increment_nightly_versions ( ) 
set_git_tag ( '{nightly}' . format ( nightly = new_version [ '__nightly__' ] ) ) 
git_push ( ) 
git_push ( tags = True ) 
~~ publish_all ( nightly ) 
~~ def release ( version ) : 
check_new_version ( version ) 
set_new_version ( version ) 
commit_new_version ( version ) 
set_git_tag ( version ) 
~~ def passthrough_context_definition ( context_params ) : 
check . inst_param ( context_params , 'context' , ExecutionContext ) 
context_definition = PipelineContextDefinition ( context_fn = lambda * _args : context_params ) 
return { DEFAULT_CONTEXT_NAME : context_definition } 
~~ def input_selector_schema ( config_cls ) : 
config_type = resolve_config_cls_arg ( config_cls ) 
check . param_invariant ( config_type . is_selector , 'config_cls' ) 
def _wrap ( func ) : 
~~~ def _selector ( context , config_value ) : 
~~~ selector_key , selector_value = single_item ( config_value ) 
return func ( context , selector_key , selector_value ) 
~~ return _create_input_schema ( config_type , _selector ) 
~~ def output_selector_schema ( config_cls ) : 
~~~ def _selector ( context , config_value , runtime_value ) : 
return func ( context , selector_key , selector_value , runtime_value ) 
~~ return _create_output_schema ( config_type , _selector ) 
~~ def block ( self , text , prefix = '' ) : 
wrapper = TextWrapper ( 
width = self . line_length - len ( self . current_indent_str ) , 
initial_indent = prefix , 
subsequent_indent = prefix , 
break_long_words = False , 
break_on_hyphens = False , 
for line in wrapper . wrap ( text ) : 
~~~ self . line ( line ) 
~~ ~~ def _define_shared_fields ( ) : 
clustering_fields = Field ( 
List ( String ) , 
create_disposition = Field ( 
BQCreateDisposition , 
destination_encryption_configuration = Field ( 
schema_update_options = Field ( 
List ( BQSchemaUpdateOption ) , 
time_partitioning = Field ( 
'expiration_ms' : Field ( 
'field' : Field ( 
'require_partition_filter' : Field ( 
write_disposition = Field ( 
BQWriteDisposition , 
'clustering_fields' : clustering_fields , 
'create_disposition' : create_disposition , 
'destination_encryption_configuration' : destination_encryption_configuration , 
'schema_update_options' : schema_update_options , 
'time_partitioning' : time_partitioning , 
'write_disposition' : write_disposition , 
~~ def define_bigquery_query_config ( ) : 
sf = _define_shared_fields ( ) 
allow_large_results = Field ( 
default_dataset = Field ( 
Dataset , 
destination = Field ( 
Table , 
dry_run = Field ( 
flatten_results = Field ( 
maximum_billing_tier = Field ( 
maximum_bytes_billed = Field ( 
priority = Field ( 
BQPriority , 
query_parameters = Field ( 
use_legacy_sql = Field ( 
use_query_cache = Field ( 
'query_job_config' : Field ( 
'allow_large_results' : allow_large_results , 
'clustering_fields' : sf [ 'clustering_fields' ] , 
'create_disposition' : sf [ 'create_disposition' ] , 
'default_dataset' : default_dataset , 
'destination' : destination , 
'destination_encryption_configuration' : sf [ 
'destination_encryption_configuration' 
'dry_run' : dry_run , 
'flatten_results' : flatten_results , 
'maximum_billing_tier' : maximum_billing_tier , 
'maximum_bytes_billed' : maximum_bytes_billed , 
'priority' : priority , 
'query_parameters' : query_parameters , 
'schema_update_options' : sf [ 'schema_update_options' ] , 
'time_partitioning' : sf [ 'time_partitioning' ] , 
'use_legacy_sql' : use_legacy_sql , 
'use_query_cache' : use_query_cache , 
'write_disposition' : sf [ 'write_disposition' ] , 
~~ def sql_solid ( name , select_statement , materialization_strategy , table_name = None , inputs = None ) : 
inputs = check . opt_list_param ( inputs , 'inputs' , InputDefinition ) 
'table' : SqlTableName , 
if materialization_strategy not in materialization_strategy_output_types : 
~~~ raise Exception ( 
materialization_strategy = materialization_strategy , 
materialization_strategies = str ( list ( materialization_strategy_output_types . keys ( ) ) ) , 
~~ if materialization_strategy == 'table' : 
~~~ if table_name is None : 
~~ ~~ output_description = ( 
if materialization_strategy == 'table' 
select_statement = select_statement 
sql_statement = ( 
) . format ( table_name = table_name , select_statement = select_statement ) 
def transform_fn ( context , _inputs ) : 
context . log . info ( 
context . resources . db_info . engine . execute ( text ( sql_statement ) ) 
yield Result ( value = table_name , output_name = 'result' ) 
~~ return SolidDefinition ( 
OutputDefinition ( 
materialization_strategy_output_types [ materialization_strategy ] , 
description = output_description , 
transform_fn = transform_fn , 
metadata = { 'kind' : 'sql' , 'sql' : sql_statement } , 
~~ def download_from_s3 ( context ) : 
target_file = context . solid_config [ 'target_file' ] 
return context . resources . download_manager . download_file_contents ( context , target_file ) 
~~ def upload_to_s3 ( context , file_obj ) : 
bucket = context . solid_config [ 'bucket' ] 
key = context . solid_config [ 'key' ] 
context . resources . s3 . put_object ( 
Bucket = bucket , Body = file_obj . read ( ) , Key = key , ** ( context . solid_config . get ( 'kwargs' ) or { } ) 
yield Result ( bucket , 'bucket' ) 
yield Result ( key , 'key' ) 
~~ def user_code_error_boundary ( error_cls , msg , ** kwargs ) : 
check . str_param ( msg , 'msg' ) 
check . subclass_param ( error_cls , 'error_cls' , DagsterUserCodeExecutionError ) 
~~~ yield 
~~~ if isinstance ( e , DagsterError ) : 
~~~ raise_from ( 
error_cls ( msg , user_exception = e , original_exc_info = sys . exc_info ( ) , ** kwargs ) , e 
~~ ~~ ~~ def mkdir_p ( newdir , mode = 0o777 ) : 
~~~ os . makedirs ( newdir , mode ) 
~~ except OSError as err : 
~~~ if err . errno != errno . EEXIST or not os . path . isdir ( newdir ) : 
~~ ~~ ~~ def user_code_context_manager ( user_fn , error_cls , msg ) : 
check . callable_param ( user_fn , 'user_fn' ) 
with user_code_error_boundary ( error_cls , msg ) : 
~~~ thing_or_gen = user_fn ( ) 
gen = _ensure_gen ( thing_or_gen ) 
~~~ thing = next ( gen ) 
~~ yield thing 
stopped = False 
~~~ next ( gen ) 
~~~ stopped = True 
~~ ~~ def construct_run_storage ( run_config , environment_config ) : 
check . inst_param ( run_config , 'run_config' , RunConfig ) 
if run_config . storage_mode : 
~~~ if run_config . storage_mode == RunStorageMode . FILESYSTEM : 
~~~ return FileSystemRunStorage ( ) 
~~ elif run_config . storage_mode == RunStorageMode . IN_MEMORY : 
~~~ return InMemoryRunStorage ( ) 
~~ elif run_config . storage_mode == RunStorageMode . S3 : 
~~ ~~ elif environment_config . storage . storage_mode == 'filesystem' : 
~~ elif environment_config . storage . storage_mode == 'in_memory' : 
~~ elif environment_config . storage . storage_mode == 's3' : 
~~ elif environment_config . storage . storage_mode is None : 
~~ ~~ def _create_context_free_log ( run_config , pipeline_def ) : 
loggers = [ define_colored_console_logger ( 'dagster' ) ] 
if run_config . event_callback : 
~~~ loggers += [ construct_event_logger ( run_config . event_callback ) ] 
~~ elif run_config . loggers : 
~~~ loggers += run_config . loggers 
~~ return DagsterLog ( run_config . run_id , get_logging_tags ( None , run_config , pipeline_def ) , loggers ) 
~~ def execute_pipeline_iterator ( pipeline , environment_dict = None , run_config = None ) : 
check . inst_param ( pipeline , 'pipeline' , PipelineDefinition ) 
environment_dict = check . opt_dict_param ( environment_dict , 'environment_dict' ) 
run_config = check_run_config_param ( run_config ) 
environment_config = create_environment_config ( pipeline , environment_dict ) 
intermediates_manager = construct_intermediates_manager ( 
run_config , environment_config , pipeline 
with _pipeline_execution_context_manager ( 
pipeline , environment_config , run_config , intermediates_manager 
) as pipeline_context : 
~~~ return _execute_pipeline_iterator ( pipeline_context ) 
~~ ~~ def execute_pipeline ( pipeline , environment_dict = None , run_config = None ) : 
~~~ event_list = list ( _execute_pipeline_iterator ( pipeline_context ) ) 
~~ return PipelineExecutionResult ( 
run_config . run_id , 
event_list , 
lambda : _pipeline_execution_context_manager ( 
~~ def result_for_solid ( self , name ) : 
if not self . pipeline . has_solid ( name ) : 
name = name , pipeline = self . pipeline . display_name 
~~ if name not in self . solid_result_dict : 
name = name 
~~ return self . solid_result_dict [ name ] 
~~ def success ( self ) : 
any_success = False 
for step_event in itertools . chain ( 
self . input_expectations , self . output_expectations , self . transforms 
~~~ if step_event . event_type == DagsterEventType . STEP_FAILURE : 
~~ if step_event . event_type == DagsterEventType . STEP_SUCCESS : 
~~~ any_success = True 
~~ ~~ return any_success 
~~ def skipped ( self ) : 
return all ( 
step_event . event_type == DagsterEventType . STEP_SKIPPED 
~~ def transformed_values ( self ) : 
if self . success and self . transforms : 
~~~ with self . reconstruct_context ( ) as context : 
~~~ values = { 
result . step_output_data . output_name : self . _get_value ( 
context , result . step_output_data 
for result in self . transforms 
if result . is_successful_output 
~~ ~~ def transformed_value ( self , output_name = DEFAULT_OUTPUT ) : 
check . str_param ( output_name , 'output_name' ) 
if not self . solid . definition . has_output ( output_name ) : 
output_name = output_name , solid = self . solid . name 
~~ if self . success : 
~~~ for result in self . transforms : 
result . is_successful_output 
and result . step_output_data . output_name == output_name 
~~~ value = self . _get_value ( context , result . step_output_data ) 
~~ ~~ raise DagsterInvariantViolationError ( 
) . format ( output_name = output_name , self = self ) 
~~ ~~ def failure_data ( self ) : 
for result in itertools . chain ( 
~~~ if result . event_type == DagsterEventType . STEP_FAILURE : 
~~~ return result . step_failure_data 
~~ ~~ ~~ def NamedDict ( name , fields , description = None , type_attributes = DEFAULT_TYPE_ATTRIBUTES ) : 
check_user_facing_fields_dict ( fields , \ . format ( name ) ) 
class _NamedDict ( _ConfigComposite ) : 
~~~ def __init__ ( self ) : 
~~~ super ( _NamedDict , self ) . __init__ ( 
key = name , 
fields = fields , 
type_attributes = type_attributes , 
~~ ~~ return _NamedDict 
~~ def Dict ( fields ) : 
check_user_facing_fields_dict ( fields , 'Dict' ) 
class _Dict ( _ConfigComposite ) : 
~~~ key = 'Dict.' + str ( DictCounter . get_next_count ( ) ) 
super ( _Dict , self ) . __init__ ( 
name = None , 
type_attributes = ConfigTypeAttributes ( is_builtin = True ) , 
~~ ~~ return _Dict 
~~ def PermissiveDict ( fields = None ) : 
if fields : 
~~~ check_user_facing_fields_dict ( fields , 'PermissiveDict' ) 
~~ class _PermissiveDict ( _ConfigComposite ) : 
~~~ key = 'PermissiveDict.' + str ( DictCounter . get_next_count ( ) ) 
super ( _PermissiveDict , self ) . __init__ ( 
fields = fields or dict ( ) , 
~~ @ property 
def is_permissive_composite ( self ) : 
~~ ~~ return _PermissiveDict 
~~ def Selector ( fields ) : 
check_user_facing_fields_dict ( fields , 'Selector' ) 
class _Selector ( _ConfigSelector ) : 
~~~ key = 'Selector.' + str ( DictCounter . get_next_count ( ) ) 
super ( _Selector , self ) . __init__ ( 
~~ ~~ return _Selector 
~~ def NamedSelector ( name , fields , description = None , type_attributes = DEFAULT_TYPE_ATTRIBUTES ) : 
class _NamedSelector ( _ConfigSelector ) : 
~~~ super ( _NamedSelector , self ) . __init__ ( 
~~ ~~ return _NamedSelector 
~~ def _is_valid_dataset ( config_value ) : 
return re . match ( 
r'^' + RE_PROJECT + r'\\.' + RE_DS_TABLE + r'$|^' + RE_DS_TABLE + r'$' , 
config_value , 
~~ def _is_valid_table ( config_value ) : 
r'^' 
+ r'$' , 
~~ def _execute_core_transform ( transform_context , inputs ) : 
check . inst_param ( transform_context , 'transform_context' , SystemTransformExecutionContext ) 
check . dict_param ( inputs , 'inputs' , key_type = str ) 
step = transform_context . step 
solid = step . solid 
transform_context . log . debug ( 
all_results = [ ] 
for step_output in _yield_transform_results ( transform_context , inputs ) : 
~~~ yield step_output 
if isinstance ( step_output , StepOutputValue ) : 
~~~ all_results . append ( step_output ) 
~~ ~~ if len ( all_results ) != len ( solid . definition . output_defs ) : 
~~~ emitted_result_names = { r . output_name for r in all_results } 
solid_output_names = { output_def . name for output_def in solid . definition . output_defs } 
omitted_outputs = solid_output_names . difference ( emitted_result_names ) 
transform_context . log . info ( 
solid = solid . name , outputs = repr ( omitted_outputs ) 
~~ ~~ def dagster_type ( 
input_schema = None , 
output_schema = None , 
serialization_strategy = None , 
storage_plugins = None , 
def _with_args ( bare_cls ) : 
~~~ check . type_param ( bare_cls , 'bare_cls' ) 
new_name = name if name else bare_cls . __name__ 
return _decorate_as_dagster_type ( 
bare_cls = bare_cls , 
key = new_name , 
name = new_name , 
input_schema = input_schema , 
output_schema = output_schema , 
serialization_strategy = serialization_strategy , 
storage_plugins = storage_plugins , 
~~ if callable ( name ) : 
~~~ klass = name 
new_name = klass . __name__ 
bare_cls = klass , key = new_name , name = new_name , description = None 
~~ return _with_args 
~~ def as_dagster_type ( 
existing_type , 
check . type_param ( existing_type , 'existing_type' ) 
check . opt_str_param ( name , 'name' ) 
check . opt_str_param ( description , 'description' ) 
check . opt_inst_param ( input_schema , 'input_schema' , InputSchema ) 
check . opt_inst_param ( output_schema , 'output_schema' , OutputSchema ) 
check . opt_inst_param ( serialization_strategy , 'serialization_strategy' , SerializationStrategy ) 
storage_plugins = check . opt_dict_param ( storage_plugins , 'storage_plugins' ) 
if serialization_strategy is None : 
~~~ serialization_strategy = PickleSerializationStrategy ( ) 
~~ name = existing_type . __name__ if name is None else name 
~~ def resource ( config_field = None , description = None ) : 
if callable ( config_field ) : 
~~~ return ResourceDefinition ( resource_fn = config_field ) 
~~ def _wrap ( resource_fn ) : 
~~~ return ResourceDefinition ( resource_fn , config_field , description ) 
~~ def run_spark_subprocess ( cmd , logger ) : 
def reader ( pipe , pipe_name , p , msg_queue ) : 
~~~ with pipe : 
~~~ while p . poll ( ) is None : 
~~~ for line in pipe . readlines ( ) : 
~~~ match = re . match ( log4j_regex , line ) 
~~~ line = match . groups ( ) [ 2 ] 
~~ msg_queue . put ( ( pipe_name , line ) ) 
~~ ~~ ~~ ~~ finally : 
~~~ msg_queue . put ( None ) 
~~ ~~ p = subprocess . Popen ( 
stdout = subprocess . PIPE , 
stderr = subprocess . PIPE , 
bufsize = 0 , 
universal_newlines = True , 
shell = True , 
q = queue . Queue ( ) 
Thread ( target = reader , args = [ p . stdout , 'stdout' , p , q ] ) . start ( ) 
Thread ( target = reader , args = [ p . stderr , 'stderr' , p , q ] ) . start ( ) 
~~~ for pipe_name , line in iter ( q . get , None ) : 
~~~ if pipe_name == 'stdout' : 
~~~ logger . info ( line ) 
~~ elif pipe_name == 'stderr' : 
~~~ logger . error ( line ) 
~~ ~~ ~~ p . wait ( ) 
return p . returncode 
~~ def parse_spark_config ( spark_conf ) : 
spark_conf_list = flatten_dict ( spark_conf ) 
return list ( 
itertools . chain . from_iterable ( [ ( '--conf' , '{}={}' . format ( * c ) ) for c in spark_conf_list ] ) 
~~ def SystemNamedDict ( name , fields , description = None ) : 
return NamedDict ( name , fields , description , ConfigTypeAttributes ( is_system_config = True ) ) 
~~ def EventV2_create ( 
summary , 
source , 
severity , 
event_action = 'trigger' , 
dedup_key = None , 
timestamp = None , 
component = None , 
group = None , 
event_class = None , 
custom_details = None , 
data = { 
'routing_key' : self . routing_key , 
'event_action' : event_action , 
'payload' : { 'summary' : summary , 'source' : source , 'severity' : severity } , 
if dedup_key is not None : 
~~~ data [ 'dedup_key' ] = dedup_key 
~~ if timestamp is not None : 
~~~ data [ 'payload' ] [ 'timestamp' ] = timestamp 
~~ if component is not None : 
~~~ data [ 'payload' ] [ 'component' ] = component 
~~ if group is not None : 
~~~ data [ 'payload' ] [ 'group' ] = group 
~~ if event_class is not None : 
~~~ data [ 'payload' ] [ 'class' ] = event_class 
~~ if custom_details is not None : 
~~~ data [ 'payload' ] [ 'custom_details' ] = custom_details 
~~ return pypd . EventV2 . create ( data = data ) 
~~ def coalesce_execution_steps ( execution_plan ) : 
solid_order = _coalesce_solid_order ( execution_plan ) 
steps = defaultdict ( list ) 
for solid_name , solid_steps in itertools . groupby ( 
execution_plan . topological_steps ( ) , lambda x : x . solid_name 
~~~ steps [ solid_name ] += list ( solid_steps ) 
~~ return OrderedDict ( [ ( solid_name , steps [ solid_name ] ) for solid_name in solid_order ] ) 
~~ def get_connection_params ( self ) : 
valid_settings = { 
'NAME' : 'name' , 
'HOST' : 'host' , 
'PORT' : 'port' , 
'USER' : 'username' , 
'PASSWORD' : 'password' , 
'AUTH_SOURCE' : 'authSource' , 
'AUTH_MECHANISM' : 'authMechanism' , 
'ENFORCE_SCHEMA' : 'enforce_schema' , 
'REPLICASET' : 'replicaset' , 
'SSL' : 'ssl' , 
'SSL_CERTFILE' : 'ssl_certfile' , 
'SSL_CA_CERTS' : 'ssl_ca_certs' , 
'READ_PREFERENCE' : 'read_preference' 
connection_params = { 
'name' : 'djongo_test' , 
'enforce_schema' : True 
for setting_name , kwarg in valid_settings . items ( ) : 
~~~ setting = self . settings_dict [ setting_name ] 
~~ if setting or setting is False : 
~~~ connection_params [ kwarg ] = setting 
~~ ~~ return connection_params 
~~ def get_new_connection ( self , connection_params ) : 
name = connection_params . pop ( 'name' ) 
es = connection_params . pop ( 'enforce_schema' ) 
connection_params [ 'document_class' ] = OrderedDict 
if self . client_connection is not None : 
~~~ self . client_connection . close ( ) 
~~ self . client_connection = Database . connect ( ** connection_params ) 
database = self . client_connection [ name ] 
self . djongo_connection = DjongoClient ( database , es ) 
return self . client_connection [ name ] 
~~ def create_cursor ( self , name = None ) : 
return Cursor ( self . client_connection , self . connection , self . djongo_connection ) 
~~ def _close ( self ) : 
if self . connection : 
~~~ with self . wrap_database_errors : 
~~~ self . connection . client . close ( ) 
~~ ~~ ~~ def make_mdl ( model , model_dict ) : 
for field_name in model_dict : 
~~~ field = model . _meta . get_field ( field_name ) 
model_dict [ field_name ] = field . to_python ( model_dict [ field_name ] ) 
~~ return model ( ** model_dict ) 
~~ def to_python ( self , value ) : 
~~ assert isinstance ( value , list ) 
for mdl_dict in value : 
~~~ if isinstance ( mdl_dict , self . model_container ) : 
~~~ ret . append ( mdl_dict ) 
~~ mdl = make_mdl ( self . model_container , mdl_dict ) 
ret . append ( mdl ) 
~~ def formfield ( self , ** kwargs ) : 
'form_class' : ArrayFormField , 
'model_container' : self . model_container , 
'model_form_class' : self . model_form_class , 
'name' : self . attname , 
'mdl_form_kw_l' : self . model_form_kwargs_l 
defaults . update ( kwargs ) 
return super ( ) . formfield ( ** defaults ) 
if value is None or isinstance ( value , self . model_container ) : 
~~ assert isinstance ( value , dict ) 
instance = make_mdl ( self . model_container , value ) 
return instance 
~~ def _apply_rel_filters ( self , queryset ) : 
queryset . _add_hints ( instance = self . instance ) 
if self . _db : 
~~~ queryset = queryset . using ( self . _db ) 
~~ queryset = queryset . filter ( ** self . core_filters ) 
return queryset 
~~ def validate_value ( self , value ) : 
if 'readOnly' in self . metadata and self . metadata [ 'readOnly' ] : 
~~~ validate ( value , self . metadata ) 
~~ ~~ def as_property_description ( self ) : 
description = deepcopy ( self . metadata ) 
if 'links' not in description : 
~~~ description [ 'links' ] = [ ] 
~~ description [ 'links' ] . append ( 
'rel' : 'property' , 
'href' : self . href_prefix + self . href , 
return description 
~~ def set_value ( self , value ) : 
self . validate_value ( value ) 
self . value . set ( value ) 
~~ def get_thing ( self , idx ) : 
~~~ idx = int ( idx ) 
~~ if idx < 0 or idx >= len ( self . things ) : 
~~ return self . things [ idx ] 
~~ def initialize ( self , things , hosts ) : 
self . things = things 
self . hosts = hosts 
~~ def set_default_headers ( self , * args , ** kwargs ) : 
self . set_header ( 'Access-Control-Allow-Origin' , '*' ) 
self . set_header ( 'Access-Control-Allow-Headers' , 
self . set_header ( 'Access-Control-Allow-Methods' , 
~~ def get ( self ) : 
self . set_header ( 'Content-Type' , 'application/json' ) 
ws_href = '{}://{}' . format ( 
'wss' if self . request . protocol == 'https' else 'ws' , 
self . request . headers . get ( 'Host' , '' ) 
descriptions = [ ] 
for thing in self . things . get_things ( ) : 
~~~ description = thing . as_thing_description ( ) 
description [ 'links' ] . append ( { 
'rel' : 'alternate' , 
'href' : '{}{}' . format ( ws_href , thing . get_href ( ) ) , 
descriptions . append ( description ) 
~~ self . write ( json . dumps ( descriptions ) ) 
host = self . request . headers . get ( 'Host' , None ) 
if host is not None and host in self . hosts : 
~~ raise tornado . web . HTTPError ( 403 ) 
~~ def get ( self , thing_id = '0' ) : 
self . thing = self . get_thing ( thing_id ) 
if self . thing is None : 
~~~ self . set_status ( 404 ) 
self . finish ( ) 
~~ if self . request . headers . get ( 'Upgrade' , '' ) . lower ( ) == 'websocket' : 
~~~ yield tornado . websocket . WebSocketHandler . get ( self ) 
~~ self . set_header ( 'Content-Type' , 'application/json' ) 
description = self . thing . as_thing_description ( ) 
'href' : '{}{}' . format ( ws_href , self . thing . get_href ( ) ) , 
self . write ( json . dumps ( description ) ) 
~~ def on_message ( self , message ) : 
~~~ message = json . loads ( message ) 
~~~ self . write_message ( json . dumps ( { 
'messageType' : 'error' , 
~~ except tornado . websocket . WebSocketClosedError : 
~~ if 'messageType' not in message or 'data' not in message : 
~~ msg_type = message [ 'messageType' ] 
if msg_type == 'setProperty' : 
~~~ for property_name , property_value in message [ 'data' ] . items ( ) : 
~~~ self . thing . set_property ( property_name , property_value ) 
~~ except PropertyError as e : 
'message' : str ( e ) , 
~~ ~~ ~~ elif msg_type == 'requestAction' : 
~~~ for action_name , action_params in message [ 'data' ] . items ( ) : 
~~~ input_ = None 
if 'input' in action_params : 
~~~ input_ = action_params [ 'input' ] 
~~ action = self . thing . perform_action ( action_name , input_ ) 
if action : 
~~~ tornado . ioloop . IOLoop . current ( ) . spawn_callback ( 
perform_action , 
action , 
'request' : message , 
~~ ~~ ~~ elif msg_type == 'addEventSubscription' : 
~~~ for event_name in message [ 'data' ] . keys ( ) : 
~~~ self . thing . add_event_subscriber ( event_name , self ) 
~~ ~~ ~~ def get ( self , thing_id = '0' , property_name = None ) : 
thing = self . get_thing ( thing_id ) 
if thing is None : 
~~ if thing . has_property ( property_name ) : 
~~~ self . set_header ( 'Content-Type' , 'application/json' ) 
self . write ( json . dumps ( { 
property_name : thing . get_property ( property_name ) , 
~~ ~~ def put ( self , thing_id = '0' , property_name = None ) : 
~~~ args = json . loads ( self . request . body . decode ( ) ) 
~~~ self . set_status ( 400 ) 
~~ if property_name not in args : 
~~~ thing . set_property ( property_name , args [ property_name ] ) 
~~ except PropertyError : 
~~ ~~ def post ( self , thing_id = '0' ) : 
~~~ message = json . loads ( self . request . body . decode ( ) ) 
~~ response = { } 
for action_name , action_params in message . items ( ) : 
~~ action = thing . perform_action ( action_name , input_ ) 
~~~ response . update ( action . as_action_description ( ) ) 
tornado . ioloop . IOLoop . current ( ) . spawn_callback ( 
~~ ~~ self . set_status ( 201 ) 
self . write ( json . dumps ( response ) ) 
~~ def get ( self , thing_id = '0' , action_name = None , action_id = None ) : 
~~ action = thing . get_action ( action_name , action_id ) 
if action is None : 
self . write ( json . dumps ( action . as_action_description ( ) ) ) 
~~ def put ( self , thing_id = '0' , action_name = None , action_id = None ) : 
~~ self . set_status ( 200 ) 
~~ def delete ( self , thing_id = '0' , action_name = None , action_id = None ) : 
~~ if thing . remove_action ( action_name , action_id ) : 
~~~ self . set_status ( 204 ) 
~~ ~~ def get ( self , thing_id = '0' ) : 
self . write ( json . dumps ( thing . get_event_descriptions ( ) ) ) 
self . service_info = ServiceInfo ( 
'_webthing._tcp.local.' , 
'{}._webthing._tcp.local.' . format ( self . name ) , 
address = socket . inet_aton ( get_ip ( ) ) , 
port = self . port , 
properties = { 
'path' : '/' , 
server = '{}.local.' . format ( socket . gethostname ( ) ) ) 
self . zeroconf = Zeroconf ( ) 
self . zeroconf . register_service ( self . service_info ) 
self . server . listen ( self . port ) 
tornado . ioloop . IOLoop . current ( ) . start ( ) 
self . zeroconf . unregister_service ( self . service_info ) 
self . zeroconf . close ( ) 
self . server . stop ( ) 
~~ def as_action_description ( self ) : 
description = { 
self . name : { 
'timeRequested' : self . time_requested , 
'status' : self . status , 
if self . input is not None : 
~~~ description [ self . name ] [ 'input' ] = self . input 
~~ if self . time_completed is not None : 
~~~ description [ self . name ] [ 'timeCompleted' ] = self . time_completed 
~~ return description 
self . status = 'pending' 
self . thing . action_notify ( self ) 
self . perform_action ( ) 
~~ def finish ( self ) : 
self . status = 'completed' 
self . time_completed = timestamp ( ) 
~~ def as_event_description ( self ) : 
'timestamp' : self . time , 
if self . data is not None : 
~~~ description [ self . name ] [ 'data' ] = self . data 
~~ def get_ip ( ) : 
s = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) 
~~~ s . connect ( ( '10.255.255.255' , 1 ) ) 
ip = s . getsockname ( ) [ 0 ] 
~~ except ( socket . error , IndexError ) : 
~~~ ip = '127.0.0.1' 
~~~ s . close ( ) 
~~ return ip 
~~ def get_addresses ( ) : 
addresses = set ( ) 
for iface in ifaddr . get_adapters ( ) : 
~~~ for addr in iface . ips : 
~~~ if addr . is_IPv4 : 
~~~ ip = addr . ip 
if not ip . startswith ( '169.254.' ) : 
~~~ addresses . add ( ip ) 
~~ ~~ elif addr . is_IPv6 : 
~~~ ip = addr . ip [ 0 ] . split ( '%' ) [ 0 ] . lower ( ) 
if not ip . startswith ( 'fe80:' ) : 
~~~ addresses . add ( '[{}]' . format ( ip ) ) 
~~ ~~ ~~ ~~ return sorted ( list ( addresses ) ) 
~~ def set ( self , value ) : 
if self . value_forwarder is not None : 
~~~ self . value_forwarder ( value ) 
~~ self . notify_of_external_update ( value ) 
~~ def notify_of_external_update ( self , value ) : 
if value is not None and value != self . last_value : 
~~~ self . last_value = value 
self . emit ( 'update' , value ) 
~~ ~~ def as_thing_description ( self ) : 
thing = { 
'name' : self . name , 
'href' : self . href_prefix if self . href_prefix else '/' , 
'@context' : self . context , 
'@type' : self . type , 
'properties' : self . get_property_descriptions ( ) , 
'actions' : { } , 
'events' : { } , 
'links' : [ 
'rel' : 'properties' , 
'href' : '{}/properties' . format ( self . href_prefix ) , 
'rel' : 'actions' , 
'href' : '{}/actions' . format ( self . href_prefix ) , 
'rel' : 'events' , 
'href' : '{}/events' . format ( self . href_prefix ) , 
for name , action in self . available_actions . items ( ) : 
~~~ thing [ 'actions' ] [ name ] = action [ 'metadata' ] 
thing [ 'actions' ] [ name ] [ 'links' ] = [ 
'rel' : 'action' , 
'href' : '{}/actions/{}' . format ( self . href_prefix , name ) , 
~~ for name , event in self . available_events . items ( ) : 
~~~ thing [ 'events' ] [ name ] = event [ 'metadata' ] 
thing [ 'events' ] [ name ] [ 'links' ] = [ 
'rel' : 'event' , 
'href' : '{}/events/{}' . format ( self . href_prefix , name ) , 
~~ if self . ui_href is not None : 
~~~ thing [ 'links' ] . append ( { 
'mediaType' : 'text/html' , 
'href' : self . ui_href , 
~~ if self . description : 
~~~ thing [ 'description' ] = self . description 
~~ return thing 
~~ def set_href_prefix ( self , prefix ) : 
self . href_prefix = prefix 
for property_ in self . properties . values ( ) : 
~~~ property_ . set_href_prefix ( prefix ) 
~~ for action_name in self . actions . keys ( ) : 
~~~ for action in self . actions [ action_name ] : 
~~~ action . set_href_prefix ( prefix ) 
~~ ~~ ~~ def get_property_descriptions ( self ) : 
return { k : v . as_property_description ( ) 
for k , v in self . properties . items ( ) } 
~~ def get_action_descriptions ( self , action_name = None ) : 
if action_name is None : 
~~~ for name in self . actions : 
~~~ for action in self . actions [ name ] : 
~~~ descriptions . append ( action . as_action_description ( ) ) 
~~ ~~ ~~ elif action_name in self . actions : 
~~ ~~ return descriptions 
~~ def get_event_descriptions ( self , event_name = None ) : 
if event_name is None : 
~~~ return [ e . as_event_description ( ) for e in self . events ] 
~~~ return [ e . as_event_description ( ) 
for e in self . events if e . get_name ( ) == event_name ] 
~~ ~~ def add_property ( self , property_ ) : 
property_ . set_href_prefix ( self . href_prefix ) 
self . properties [ property_ . name ] = property_ 
~~ def remove_property ( self , property_ ) : 
if property_ . name in self . properties : 
~~~ del self . properties [ property_ . name ] 
~~ ~~ def get_property ( self , property_name ) : 
prop = self . find_property ( property_name ) 
if prop : 
~~~ return prop . get_value ( ) 
~~ def get_properties ( self ) : 
return { prop . get_name ( ) : prop . get_value ( ) 
for prop in self . properties . values ( ) } 
~~ def set_property ( self , property_name , value ) : 
if not prop : 
~~ prop . set_value ( value ) 
~~ def get_action ( self , action_name , action_id ) : 
if action_name not in self . actions : 
~~ for action in self . actions [ action_name ] : 
~~~ if action . id == action_id : 
~~~ return action 
~~ def add_event ( self , event ) : 
self . events . append ( event ) 
self . event_notify ( event ) 
~~ def add_available_event ( self , name , metadata ) : 
if metadata is None : 
~~~ metadata = { } 
~~ self . available_events [ name ] = { 
'metadata' : metadata , 
'subscribers' : set ( ) , 
~~ def perform_action ( self , action_name , input_ = None ) : 
if action_name not in self . available_actions : 
~~ action_type = self . available_actions [ action_name ] 
if 'input' in action_type [ 'metadata' ] : 
~~~ validate ( input_ , action_type [ 'metadata' ] [ 'input' ] ) 
~~ ~~ action = action_type [ 'class' ] ( self , input_ = input_ ) 
action . set_href_prefix ( self . href_prefix ) 
self . action_notify ( action ) 
self . actions [ action_name ] . append ( action ) 
return action 
~~ def remove_action ( self , action_name , action_id ) : 
action = self . get_action ( action_name , action_id ) 
~~ action . cancel ( ) 
self . actions [ action_name ] . remove ( action ) 
~~ def add_available_action ( self , name , metadata , cls ) : 
~~ self . available_actions [ name ] = { 
'class' : cls , 
self . actions [ name ] = [ ] 
~~ def remove_subscriber ( self , ws ) : 
if ws in self . subscribers : 
~~~ self . subscribers . remove ( ws ) 
~~ for name in self . available_events : 
~~~ self . remove_event_subscriber ( name , ws ) 
~~ ~~ def add_event_subscriber ( self , name , ws ) : 
if name in self . available_events : 
~~~ self . available_events [ name ] [ 'subscribers' ] . add ( ws ) 
~~ ~~ def remove_event_subscriber ( self , name , ws ) : 
if name in self . available_events and ws in self . available_events [ name ] [ 'subscribers' ] : 
~~~ self . available_events [ name ] [ 'subscribers' ] . remove ( ws ) 
~~ ~~ def property_notify ( self , property_ ) : 
message = json . dumps ( { 
'messageType' : 'propertyStatus' , 
property_ . name : property_ . get_value ( ) , 
for subscriber in list ( self . subscribers ) : 
~~~ subscriber . write_message ( message ) 
~~ ~~ ~~ def action_notify ( self , action ) : 
'messageType' : 'actionStatus' , 
'data' : action . as_action_description ( ) , 
~~ ~~ ~~ def event_notify ( self , event ) : 
if event . name not in self . available_events : 
~~ message = json . dumps ( { 
'messageType' : 'event' , 
'data' : event . as_event_description ( ) , 
for subscriber in self . available_events [ event . name ] [ 'subscribers' ] : 
~~ ~~ ~~ def fetch_items ( self , category , ** kwargs ) : 
from_date = kwargs [ 'from_date' ] 
self . url , self . channel , str ( from_date ) ) 
fetching = True 
page = 0 
nposts = 0 
since = int ( from_date . timestamp ( ) * 1000 ) 
while fetching : 
~~~ raw_posts = self . client . posts ( self . channel , page = page ) 
posts_before = nposts 
for post in self . _parse_posts ( raw_posts ) : 
~~~ if post [ 'update_at' ] < since : 
~~~ fetching = False 
~~ user_id = post [ 'user_id' ] 
user = self . _get_or_fetch_user ( user_id ) 
post [ 'user_data' ] = user 
yield post 
nposts += 1 
~~ if fetching : 
~~~ if posts_before == nposts : 
~~~ page += 1 
~~ def _init_client ( self , from_archive = False ) : 
return MattermostClient ( self . url , self . api_token , 
max_items = self . max_items , 
sleep_for_rate = self . sleep_for_rate , 
min_rate_to_sleep = self . min_rate_to_sleep , 
sleep_time = self . sleep_time , 
archive = self . archive , from_archive = from_archive ) 
~~ def _parse_posts ( self , raw_posts ) : 
parsed_posts = self . parse_json ( raw_posts ) 
for post_id in parsed_posts [ 'order' ] : 
~~~ yield parsed_posts [ 'posts' ] [ post_id ] 
~~ ~~ def posts ( self , channel , page = None ) : 
entrypoint = self . RCHANNELS + '/' + channel + '/' + self . RPOSTS 
self . PPER_PAGE : self . max_items 
if page is not None : 
~~~ params [ self . PPAGE ] = page 
~~ response = self . _fetch ( entrypoint , params ) 
~~ def user ( self , user ) : 
entrypoint = self . RUSERS + '/' + user 
response = self . _fetch ( entrypoint , None ) 
~~ def _fetch ( self , entry_point , params ) : 
url = self . API_URL % { 'base_url' : self . base_url , 'entrypoint' : entry_point } 
entry_point , str ( params ) ) 
r = self . fetch ( url , payload = params ) 
return r . text 
~~ def _pre_init ( self ) : 
if not self . parsed_args . mboxes_path : 
~~~ base_path = os . path . expanduser ( '~/.perceval/mailinglists/' ) 
dirpath = os . path . join ( base_path , self . parsed_args . url ) 
~~~ dirpath = self . parsed_args . mboxes_path 
~~ setattr ( self . parsed_args , 'dirpath' , dirpath ) 
~~ def fetch ( self , from_date = DEFAULT_DATETIME ) : 
self . url , str ( from_date ) ) 
from_date = datetime_to_utc ( from_date ) 
r = requests . get ( self . url , verify = self . verify ) 
r . raise_for_status ( ) 
links = self . _parse_archive_links ( r . text ) 
fetched = [ ] 
if not os . path . exists ( self . dirpath ) : 
~~~ os . makedirs ( self . dirpath ) 
~~ for l in links : 
~~~ filename = os . path . basename ( l ) 
mbox_dt = self . _parse_date_from_filepath ( filename ) 
if ( ( from_date . year == mbox_dt . year and 
from_date . month == mbox_dt . month ) or 
from_date < mbox_dt ) : 
~~~ filepath = os . path . join ( self . dirpath , filename ) 
success = self . _download_archive ( l , filepath ) 
if success : 
~~~ fetched . append ( ( l , filepath ) ) 
return fetched 
~~ def mboxes ( self ) : 
archives = [ ] 
for mbox in super ( ) . mboxes : 
~~~ dt = self . _parse_date_from_filepath ( mbox . filepath ) 
archives . append ( ( dt , mbox ) ) 
~~ archives . sort ( key = lambda x : x [ 0 ] ) 
return [ a [ 1 ] for a in archives ] 
~~ def fetch ( self , category = CATEGORY_ENTRY ) : 
items = super ( ) . fetch ( category , ** kwargs ) 
return items 
~~ def fetch_items ( self , category , ** kwargs ) : 
raw_entries = self . client . get_entries ( ) 
entries = self . parse_feed ( raw_entries ) [ 'entries' ] 
for item in entries : 
~~~ yield item 
nentries += 1 
return RSSClient ( self . url , self . archive , from_archive ) 
~~ def setup_cmd_parser ( cls ) : 
parser = BackendCommandArgumentParser ( cls . BACKEND . CATEGORIES , 
archive = True ) 
parser . parser . add_argument ( 'url' , 
return parser 
~~ def fetch ( self , category = CATEGORY_BUG , from_date = DEFAULT_DATETIME ) : 
if not from_date : 
~~~ from_date = DEFAULT_DATETIME 
~~ kwargs = { 'from_date' : from_date } 
nbugs = 0 
for bug in self . __fetch_and_parse_bugs ( from_date ) : 
~~~ nbugs += 1 
yield bug 
return BugzillaRESTClient ( self . url , user = self . user , password = self . password , api_token = self . api_token , 
~~ def login ( self , user , password ) : 
self . PBUGZILLA_LOGIN : user , 
self . PBUGZILLA_PASSWORD : password 
~~~ r = self . call ( self . RLOGIN , params ) 
~~ except requests . exceptions . HTTPError as e : 
raise BackendError ( cause = cause ) 
~~ data = json . loads ( r ) 
self . api_token = data [ 'token' ] 
~~ def bugs ( self , from_date = DEFAULT_DATETIME , offset = None , max_bugs = MAX_BUGS ) : 
date = datetime_to_utc ( from_date ) 
date = date . strftime ( "%Y-%m-%dT%H:%M:%SZ" ) 
self . PLAST_CHANGE_TIME : date , 
self . PLIMIT : max_bugs , 
self . PORDER : self . VCHANGE_DATE_ORDER , 
self . PINCLUDE_FIELDS : self . VINCLUDE_ALL 
~~~ params [ self . POFFSET ] = offset 
~~ response = self . call ( self . RBUG , params ) 
~~ def comments ( self , * bug_ids ) : 
resource = urijoin ( self . RBUG , bug_ids [ 0 ] , self . RCOMMENT ) 
self . PIDS : bug_ids 
response = self . call ( resource , params ) 
~~ def history ( self , * bug_ids ) : 
resource = urijoin ( self . RBUG , bug_ids [ 0 ] , self . RHISTORY ) 
~~ def attachments ( self , * bug_ids ) : 
resource = urijoin ( self . RBUG , bug_ids [ 0 ] , self . RATTACHMENT ) 
self . PIDS : bug_ids , 
self . PEXCLUDE_FIELDS : self . VEXCLUDE_ATTCH_DATA 
~~ def call ( self , resource , params ) : 
url = self . URL % { 'base' : self . base_url , 'resource' : resource } 
if self . api_token : 
~~~ params [ self . PBUGZILLA_TOKEN ] = self . api_token 
resource , str ( params ) ) 
result = r . json ( ) 
if result . get ( 'error' , False ) : 
~~~ raise BugzillaRESTError ( error = result [ 'message' ] , 
code = result [ 'code' ] ) 
~~ return r . text 
~~ def sanitize_for_archive ( url , headers , payload ) : 
if BugzillaRESTClient . PBUGZILLA_LOGIN in payload : 
~~~ payload . pop ( BugzillaRESTClient . PBUGZILLA_LOGIN ) 
~~ if BugzillaRESTClient . PBUGZILLA_PASSWORD in payload : 
~~~ payload . pop ( BugzillaRESTClient . PBUGZILLA_PASSWORD ) 
~~ if BugzillaRESTClient . PBUGZILLA_TOKEN in payload : 
~~~ payload . pop ( BugzillaRESTClient . PBUGZILLA_TOKEN ) 
~~ return url , headers , payload 
if category == CATEGORY_ISSUE : 
~~~ items = self . __fetch_issues ( from_date ) 
~~~ items = self . __fetch_merge_requests ( from_date ) 
~~ def __fetch_issues ( self , from_date ) : 
issues_groups = self . client . issues ( from_date = from_date ) 
for raw_issues in issues_groups : 
~~~ issues = json . loads ( raw_issues ) 
for issue in issues : 
~~~ issue_id = issue [ 'iid' ] 
if self . blacklist_ids and issue_id in self . blacklist_ids : 
~~ self . __init_issue_extra_fields ( issue ) 
issue [ 'notes_data' ] = self . __get_issue_notes ( issue_id ) 
issue [ 'award_emoji_data' ] = self . __get_award_emoji ( GitLabClient . ISSUES , issue_id ) 
yield issue 
~~ ~~ ~~ def __get_issue_notes ( self , issue_id ) : 
notes = [ ] 
group_notes = self . client . notes ( GitLabClient . ISSUES , issue_id ) 
for raw_notes in group_notes : 
~~~ for note in json . loads ( raw_notes ) : 
~~~ note_id = note [ 'id' ] 
note [ 'award_emoji_data' ] = self . __get_note_award_emoji ( GitLabClient . ISSUES , issue_id , note_id ) 
notes . append ( note ) 
~~ ~~ return notes 
~~ def __fetch_merge_requests ( self , from_date ) : 
merges_groups = self . client . merges ( from_date = from_date ) 
for raw_merges in merges_groups : 
~~~ merges = json . loads ( raw_merges ) 
for merge in merges : 
~~~ merge_id = merge [ 'iid' ] 
if self . blacklist_ids and merge_id in self . blacklist_ids : 
~~ merge_full_raw = self . client . merge ( merge_id ) 
merge_full = json . loads ( merge_full_raw ) 
self . __init_merge_extra_fields ( merge_full ) 
merge_full [ 'notes_data' ] = self . __get_merge_notes ( merge_id ) 
merge_full [ 'award_emoji_data' ] = self . __get_award_emoji ( GitLabClient . MERGES , merge_id ) 
merge_full [ 'versions_data' ] = self . __get_merge_versions ( merge_id ) 
yield merge_full 
~~ ~~ ~~ def __get_merge_notes ( self , merge_id ) : 
group_notes = self . client . notes ( GitLabClient . MERGES , merge_id ) 
note [ 'award_emoji_data' ] = self . __get_note_award_emoji ( GitLabClient . MERGES , merge_id , note_id ) 
~~ def __get_merge_versions ( self , merge_id ) : 
versions = [ ] 
group_versions = self . client . merge_versions ( merge_id ) 
for raw_versions in group_versions : 
~~~ for version in json . loads ( raw_versions ) : 
~~~ version_id = version [ 'id' ] 
version_full_raw = self . client . merge_version ( merge_id , version_id ) 
version_full = json . loads ( version_full_raw ) 
version_full . pop ( 'diffs' , None ) 
versions . append ( version_full ) 
~~ ~~ return versions 
~~ def __get_award_emoji ( self , item_type , item_id ) : 
emojis = [ ] 
group_emojis = self . client . emojis ( item_type , item_id ) 
for raw_emojis in group_emojis : 
~~~ for emoji in json . loads ( raw_emojis ) : 
~~~ emojis . append ( emoji ) 
~~ ~~ return emojis 
~~ def __get_note_award_emoji ( self , item_type , item_id , note_id ) : 
group_emojis = self . client . note_emojis ( item_type , item_id , note_id ) 
~~~ for raw_emojis in group_emojis : 
~~ ~~ ~~ except requests . exceptions . HTTPError as error : 
~~~ if error . response . status_code == 404 : 
urijoin ( item_type , str ( item_id ) , GitLabClient . NOTES , 
str ( note_id ) , GitLabClient . EMOJI ) ) 
return emojis 
~~ def issues ( self , from_date = None ) : 
payload = { 
'state' : 'all' , 
'order_by' : 'updated_at' , 
'sort' : 'asc' , 
'per_page' : PER_PAGE 
if from_date : 
~~~ payload [ 'updated_after' ] = from_date . isoformat ( ) 
~~ return self . fetch_items ( GitLabClient . ISSUES , payload ) 
~~ def merges ( self , from_date = None ) : 
'view' : 'simple' , 
~~ return self . fetch_items ( GitLabClient . MERGES , payload ) 
~~ def merge ( self , merge_id ) : 
path = urijoin ( self . base_url , 
GitLabClient . PROJECTS , self . owner + '%2F' + self . repository , 
GitLabClient . MERGES , merge_id ) 
response = self . fetch ( path ) 
return response . text 
~~ def merge_versions ( self , merge_id ) : 
path = urijoin ( GitLabClient . MERGES , str ( merge_id ) , GitLabClient . VERSIONS ) 
return self . fetch_items ( path , payload ) 
~~ def merge_version ( self , merge_id , version_id ) : 
GitLabClient . MERGES , merge_id , GitLabClient . VERSIONS , version_id ) 
~~ def notes ( self , item_type , item_id ) : 
path = urijoin ( item_type , str ( item_id ) , GitLabClient . NOTES ) 
~~ def emojis ( self , item_type , item_id ) : 
path = urijoin ( item_type , str ( item_id ) , GitLabClient . EMOJI ) 
~~ def note_emojis ( self , item_type , item_id , note_id ) : 
path = urijoin ( item_type , str ( item_id ) , GitLabClient . NOTES , 
str ( note_id ) , GitLabClient . EMOJI ) 
~~ def calculate_time_to_reset ( self ) : 
time_to_reset = self . rate_limit_reset_ts - ( datetime_utcnow ( ) . replace ( microsecond = 0 ) . timestamp ( ) + 1 ) 
if time_to_reset < 0 : 
~~~ time_to_reset = 0 
~~ return time_to_reset 
~~ def fetch ( self , url , payload = None , headers = None , method = HttpClient . GET , stream = False ) : 
if not self . from_archive : 
~~~ self . sleep_for_rate_limit ( ) 
~~ response = super ( ) . fetch ( url , payload , headers , method , stream ) 
~~~ self . update_rate_limit ( response ) 
~~ def fetch_items ( self , path , payload ) : 
url_next = urijoin ( self . base_url , GitLabClient . PROJECTS , self . owner + '%2F' + self . repository , path ) 
response = self . fetch ( url_next , payload = payload ) 
items = response . text 
page += 1 
if 'last' in response . links : 
~~~ last_url = response . links [ 'last' ] [ 'url' ] 
last_page = last_url . split ( '&page=' ) [ 1 ] . split ( '&' ) [ 0 ] 
last_page = int ( last_page ) 
~~ while items : 
~~~ yield items 
items = None 
if 'next' in response . links : 
~~ ~~ ~~ def sanitize_for_archive ( url , headers , payload ) : 
if headers and 'PRIVATE-TOKEN' in headers : 
~~~ headers . pop ( 'PRIVATE-TOKEN' , None ) 
~~ def _init_rate_limit ( self ) : 
url = urijoin ( self . base_url , 'projects' , self . owner + '%2F' + self . repository ) 
~~~ response = super ( ) . fetch ( url ) 
self . update_rate_limit ( response ) 
~~ except requests . exceptions . HTTPError as error : 
~~~ if error . response . status_code == 401 : 
~~~ raise error 
~~ ~~ ~~ def setup_cmd_parser ( cls ) : 
from_date = True , 
token_auth = True , 
group . add_argument ( '--enterprise-url' , dest = 'base_url' , 
group . add_argument ( '--sleep-for-rate' , dest = 'sleep_for_rate' , 
action = 'store_true' , 
group . add_argument ( '--min-rate-to-sleep' , dest = 'min_rate_to_sleep' , 
default = MIN_RATE_LIMIT , type = int , 
group . add_argument ( '--blacklist-ids' , dest = 'blacklist_ids' , 
nargs = '*' , type = int , 
group . add_argument ( '--max-retries' , dest = 'max_retries' , 
default = MAX_RETRIES , type = int , 
group . add_argument ( '--sleep-time' , dest = 'sleep_time' , 
default = DEFAULT_SLEEP_TIME , type = int , 
parser . parser . add_argument ( 'owner' , 
parser . parser . add_argument ( 'repository' , 
~~ def fetch ( self , category = CATEGORY_MESSAGE , from_date = DEFAULT_DATETIME ) : 
~~ from_date = datetime_to_utc ( from_date ) 
latest = datetime_utcnow ( ) . timestamp ( ) 
kwargs = { 'from_date' : from_date , 'latest' : latest } 
latest = kwargs [ 'latest' ] 
self . channel , str ( from_date ) ) 
raw_info = self . client . channel_info ( self . channel ) 
channel_info = self . parse_channel_info ( raw_info ) 
channel_info [ 'num_members' ] = self . client . conversation_members ( self . channel ) 
oldest = datetime_to_utc ( from_date ) . timestamp ( ) 
if oldest == 0.0 : 
~~~ oldest = 0 
~~ if oldest > 0.0 : 
~~~ oldest -= .00001 
~~ fetching = True 
nmsgs = 0 
~~~ raw_history = self . client . history ( self . channel , 
oldest = oldest , latest = latest ) 
messages , fetching = self . parse_history ( raw_history ) 
for message in messages : 
~~~ user_id = None 
if 'user' in message : 
~~~ user_id = message [ 'user' ] 
~~ elif 'comment' in message : 
~~~ user_id = message [ 'comment' ] [ 'user' ] 
~~ if user_id : 
~~~ message [ 'user_data' ] = self . __get_or_fetch_user ( user_id ) 
~~ message [ 'channel_info' ] = channel_info 
yield message 
nmsgs += 1 
if fetching : 
~~~ latest = float ( message [ 'ts' ] ) 
~~ def metadata_id ( item ) : 
if 'user' in item : 
~~~ nick = item [ 'user' ] 
~~ elif 'comment' in item : 
~~~ nick = item [ 'comment' ] [ 'user' ] 
~~~ nick = item [ 'bot_id' ] 
~~ return item [ 'ts' ] + nick 
return SlackClient ( self . api_token , self . max_items , self . archive , from_archive ) 
~~ def conversation_members ( self , conversation ) : 
members = 0 
resource = self . RCONVERSATION_INFO 
self . PCHANNEL : conversation , 
raw_response = self . _fetch ( resource , params ) 
response = json . loads ( raw_response ) 
members += len ( response [ "members" ] ) 
while 'next_cursor' in response [ 'response_metadata' ] and response [ 'response_metadata' ] [ 'next_cursor' ] : 
~~~ params [ 'cursor' ] = response [ 'response_metadata' ] [ 'next_cursor' ] 
~~ return members 
~~ def channel_info ( self , channel ) : 
resource = self . RCHANNEL_INFO 
self . PCHANNEL : channel , 
response = self . _fetch ( resource , params ) 
~~ def history ( self , channel , oldest = None , latest = None ) : 
resource = self . RCHANNEL_HISTORY 
self . PCOUNT : self . max_items 
if oldest is not None : 
~~~ params [ self . POLDEST ] = oldest 
~~ if latest is not None : 
~~~ params [ self . PLATEST ] = latest 
~~ response = self . _fetch ( resource , params ) 
~~ def user ( self , user_id ) : 
resource = self . RUSER_INFO 
self . PUSER : user_id 
if SlackClient . PTOKEN in payload : 
~~~ payload . pop ( SlackClient . PTOKEN ) 
~~ def _fetch ( self , resource , params ) : 
url = self . URL % { 'resource' : resource } 
params [ self . PTOKEN ] = self . api_token 
if not result [ 'ok' ] : 
~~~ raise SlackClientError ( error = result [ 'error' ] ) 
action = parser . parser . _option_string_actions [ '--api-token' ] 
action . required = True 
group . add_argument ( '--max-items' , dest = 'max_items' , 
type = int , default = MAX_ITEMS , 
parser . parser . add_argument ( 'channel' , 
buglist = [ bug for bug in self . __fetch_buglist ( from_date ) ] 
tbugs = len ( buglist ) 
for i in range ( 0 , tbugs , self . max_bugs ) : 
~~~ chunk = buglist [ i : i + self . max_bugs ] 
bugs_ids = [ b [ 'bug_id' ] for b in chunk ] 
bugs = self . __fetch_and_parse_bugs_details ( bugs_ids ) 
for bug in bugs : 
~~~ bug_id = bug [ 'bug_id' ] [ 0 ] [ '__text__' ] 
bug [ 'activity' ] = self . __fetch_and_parse_bug_activity ( bug_id ) 
nbugs += 1 
nbugs , tbugs ) 
~~ def metadata_updated_on ( item ) : 
ts = item [ 'delta_ts' ] [ 0 ] [ '__text__' ] 
ts = str_to_datetime ( ts ) 
ts = ts . replace ( tzinfo = dateutil . tz . tzutc ( ) ) 
return ts . timestamp ( ) 
~~ def parse_buglist ( raw_csv ) : 
reader = csv . DictReader ( raw_csv . split ( '\\n' ) , 
delimiter = ',' , quotechar = \ ) 
for row in reader : 
~~~ yield row 
~~ ~~ def parse_bugs_details ( raw_xml ) : 
bugs = xml_to_dict ( raw_xml ) 
if 'bug' not in bugs : 
raise ParseError ( cause = cause ) 
~~ for bug in bugs [ 'bug' ] : 
~~~ yield bug 
~~ ~~ def parse_bug_activity ( raw_html ) : 
def is_activity_empty ( bs ) : 
tag = bs . find ( text = re . compile ( EMPTY_ACTIVITY ) ) 
return tag is not None 
~~ def find_activity_table ( bs ) : 
~~~ tables = bs . find_all ( 'table' ) 
for tb in tables : 
~~~ nheaders = len ( tb . tr . find_all ( 'th' , recursive = False ) ) 
if nheaders == 5 : 
~~~ return tb 
~~ def remove_tags ( bs ) : 
~~~ HTML_TAGS_TO_REMOVE = [ 'a' , 'i' , 'span' ] 
for tag in bs . find_all ( HTML_TAGS_TO_REMOVE ) : 
~~~ tag . replaceWith ( tag . text ) 
~~ ~~ def format_text ( bs ) : 
return s 
~~ bs = bs4 . BeautifulSoup ( raw_html , 'html.parser' ) 
if is_activity_empty ( bs ) : 
~~~ fields = [ ] 
~~~ activity_tb = find_activity_table ( bs ) 
remove_tags ( activity_tb ) 
fields = activity_tb . find_all ( 'td' ) 
~~ while fields : 
~~~ who = fields . pop ( 0 ) 
when = fields . pop ( 0 ) 
n = int ( who . get ( 'rowspan' ) ) 
for _ in range ( n ) : 
~~~ what = fields . pop ( 0 ) 
removed = fields . pop ( 0 ) 
added = fields . pop ( 0 ) 
event = { 'Who' : format_text ( who ) , 
'When' : format_text ( when ) , 
'What' : format_text ( what ) , 
'Removed' : format_text ( removed ) , 
'Added' : format_text ( added ) } 
yield event 
~~ ~~ ~~ def _init_client ( self , from_archive = False ) : 
return BugzillaClient ( self . url , user = self . user , password = self . password , 
max_bugs_csv = self . max_bugs_csv , 
url = self . URL % { 'base' : self . base_url , 'cgi' : self . CGI_LOGIN } 
self . PBUGZILLA_PASSWORD : password , 
headers = { 'Referer' : self . base_url } 
req = self . fetch ( url , payload = payload , headers = headers , method = HttpClient . POST ) 
if req . text . find ( "index.cgi?logout=1" ) < 0 : 
user , self . base_url ) 
~~ def logout ( self ) : 
self . PLOGOUT : '1' 
self . call ( self . CGI_LOGIN , params ) 
self . _close_http_session ( ) 
self . base_url ) 
~~ def metadata ( self ) : 
self . PCTYPE : self . CTYPE_XML 
response = self . call ( self . CGI_BUG , params ) 
~~ def buglist ( self , from_date = DEFAULT_DATETIME ) : 
if not self . version : 
~~~ self . version = self . __fetch_version ( ) 
~~ if self . version in self . OLD_STYLE_VERSIONS : 
~~~ order = 'Last+Changed' 
~~~ order = 'changeddate' 
self . PCHFIELD_FROM : date , 
self . PCTYPE : self . CTYPE_CSV , 
self . PLIMIT : self . max_bugs_csv , 
self . PORDER : order 
response = self . call ( self . CGI_BUGLIST , params ) 
~~ def bugs ( self , * bug_ids ) : 
self . PBUG_ID : bug_ids , 
self . PCTYPE : self . CTYPE_XML , 
self . PEXCLUDE_FIELD : 'attachmentdata' 
~~ def bug_activity ( self , bug_id ) : 
self . PBUG_ID : bug_id 
response = self . call ( self . CGI_BUG_ACTIVITY , params ) 
~~ def call ( self , cgi , params ) : 
url = self . URL % { 'base' : self . base_url , 'cgi' : cgi } 
cgi , str ( params ) ) 
req = self . fetch ( url , payload = params ) 
return req . text 
if BugzillaClient . PBUGZILLA_LOGIN in payload : 
~~~ payload . pop ( BugzillaClient . PBUGZILLA_LOGIN ) 
~~ if BugzillaClient . PBUGZILLA_PASSWORD in payload : 
~~~ payload . pop ( BugzillaClient . PBUGZILLA_PASSWORD ) 
~~ if BugzillaClient . PLOGIN in payload : 
~~~ payload . pop ( BugzillaClient . PLOGIN ) 
~~ def fetch ( self , category = CATEGORY_EVENT , from_date = DEFAULT_DATETIME , to_date = None , 
filter_classified = False ) : 
kwargs = { "from_date" : from_date , "to_date" : to_date } 
items = super ( ) . fetch ( category , 
filter_classified = filter_classified , 
to_date = kwargs [ 'to_date' ] 
self . group , str ( from_date ) , 
str ( to_date ) if to_date else '--' ) 
to_date_ts = datetime_to_utc ( to_date ) . timestamp ( ) if to_date else None 
nevents = 0 
stop_fetching = False 
ev_pages = self . client . events ( self . group , from_date = from_date ) 
for evp in ev_pages : 
~~~ events = [ event for event in self . parse_json ( evp ) ] 
for event in events : 
~~~ event_id = event [ 'id' ] 
event [ 'comments' ] = self . __fetch_and_parse_comments ( event_id ) 
event [ 'rsvps' ] = self . __fetch_and_parse_rsvps ( event_id ) 
event_ts = self . metadata_updated_on ( event ) 
if to_date_ts and event_ts >= to_date_ts : 
~~~ stop_fetching = True 
~~ yield event 
nevents += 1 
~~ if stop_fetching : 
~~ def events ( self , group , from_date = DEFAULT_DATETIME ) : 
date = date . strftime ( "since:%Y-%m-%dT%H:%M:%S.000Z" ) 
resource = urijoin ( group , self . REVENTS ) 
fixed_params = '?' + self . PFIELDS + '=' + ',' . join ( self . VEVENT_FIELDS ) 
fixed_params += '&' + self . PSTATUS + '=' + ',' . join ( self . VSTATUS ) 
resource += fixed_params 
self . PORDER : self . VUPDATED , 
self . PSCROLL : date , 
self . PPAGE : self . max_items 
~~~ for page in self . _fetch ( resource , params ) : 
~~~ yield page 
~~ ~~ except requests . exceptions . HTTPError as error : 
~~~ if error . response . status_code == 410 : 
raise RepositoryError ( cause = msg ) 
~~ ~~ ~~ def comments ( self , group , event_id ) : 
resource = urijoin ( group , self . REVENTS , event_id , self . RCOMMENTS ) 
for page in self . _fetch ( resource , params ) : 
~~ ~~ def rsvps ( self , group , event_id ) : 
resource = urijoin ( group , self . REVENTS , event_id , self . RRSVPS ) 
fixed_params = '?' + self . PFIELDS + '=' + ',' . join ( self . VRSVP_FIELDS ) 
fixed_params += '&' + self . PRESPONSE + '=' + ',' . join ( self . VRESPONSE ) 
~~ ~~ def sanitize_for_archive ( url , headers , payload ) : 
if MeetupClient . PKEY in payload : 
~~~ payload . pop ( MeetupClient . PKEY ) 
~~ if MeetupClient . PSIGN in payload : 
~~~ payload . pop ( MeetupClient . PSIGN ) 
url = urijoin ( self . base_url , resource ) 
params [ self . PKEY ] = self . api_key 
params [ self . PSIGN ] = 'true' , 
do_fetch = True 
while do_fetch : 
~~ r = self . fetch ( url , payload = params ) 
~~~ self . update_rate_limit ( r ) 
~~ yield r . text 
if r . links and 'next' in r . links : 
~~~ url = r . links [ 'next' ] [ 'url' ] 
self . PKEY : self . api_key , 
self . PSIGN : 'true' 
~~~ do_fetch = False 
from_date = datetime_to_utc ( kwargs [ 'from_date' ] ) . timestamp ( ) 
questions_groups = self . client . get_api_questions ( AskbotClient . API_QUESTIONS ) 
for questions in questions_groups : 
~~~ for question in questions [ 'questions' ] : 
~~~ updated_at = int ( question [ 'last_activity_at' ] ) 
if updated_at > from_date : 
~~~ html_question = self . __fetch_question ( question ) 
if not html_question : 
comments = self . __fetch_comments ( question ) 
question_obj = self . __build_question ( html_question , question , comments ) 
question . update ( question_obj ) 
yield question 
~~ ~~ ~~ ~~ def _init_client ( self , from_archive = False ) : 
return AskbotClient ( self . url , self . archive , from_archive ) 
~~ def __fetch_question ( self , question ) : 
html_question_items = [ ] 
npages = 1 
next_request = True 
while next_request : 
~~~ html_question = self . client . get_html_question ( question [ 'id' ] , npages ) 
html_question_items . append ( html_question ) 
tpages = self . ab_parser . parse_number_of_html_pages ( html_question ) 
if npages == tpages : 
~~~ next_request = False 
~~ npages = npages + 1 
~~ except requests . exceptions . TooManyRedirects as e : 
next_request = False 
~~ ~~ return html_question_items 
~~ def __fetch_comments ( self , question ) : 
comments = { } 
comments [ question [ 'id' ] ] = json . loads ( self . client . get_comments ( question [ 'id' ] ) ) 
for object_id in question [ 'answer_ids' ] : 
~~~ comments [ object_id ] = json . loads ( self . client . get_comments ( object_id ) ) 
~~ return comments 
~~ def __build_question ( html_question , question , comments ) : 
question_object = { } 
question_container = AskbotParser . parse_question_container ( html_question [ 0 ] ) 
question_object . update ( question_container ) 
if comments [ int ( question [ 'id' ] ) ] : 
~~~ question_object [ 'comments' ] = comments [ int ( question [ 'id' ] ) ] 
~~ answers = [ ] 
for page in html_question : 
~~~ answers . extend ( AskbotParser . parse_answers ( page ) ) 
~~ if len ( answers ) != 0 : 
~~~ question_object [ 'answers' ] = answers 
for answer in question_object [ 'answers' ] : 
~~~ if comments [ int ( answer [ 'id' ] ) ] : 
~~~ answer [ 'comments' ] = comments [ int ( answer [ 'id' ] ) ] 
~~ ~~ ~~ return question_object 
~~ def get_api_questions ( self , path ) : 
path = urijoin ( self . base_url , path ) 
~~~ params = { 
'page' : npages , 
'sort' : self . ORDER_API 
response = self . fetch ( path , payload = params ) 
whole_page = response . text 
raw_questions = json . loads ( whole_page ) 
tpages = raw_questions [ 'pages' ] 
self . base_url , npages , tpages ) 
yield raw_questions 
~~ ~~ ~~ def get_html_question ( self , question_id , page = 1 ) : 
path = urijoin ( self . base_url , self . HTML_QUESTION , question_id ) 
'page' : page , 
'sort' : self . ORDER_HTML 
~~ def get_comments ( self , post_id ) : 
path = urijoin ( self . base_url , self . COMMENTS if self . _use_new_urls else self . COMMENTS_OLD ) 
'post_id' : post_id , 
'post_type' : 'answer' , 
'avatar_size' : 0 
headers = { 'X-Requested-With' : 'XMLHttpRequest' } 
~~~ response = self . fetch ( path , payload = params , headers = headers ) 
raw = response . text 
~~ except requests . exceptions . HTTPError as ex : 
~~~ if ex . response . status_code == 404 : 
self . _use_new_urls = False 
path = urijoin ( self . base_url , self . COMMENTS_OLD ) 
response = self . fetch ( path , payload = params , headers = headers ) 
~~ elif ex . response . status_code == 500 : 
raw = '[]' 
~~~ raise ex 
~~ ~~ return raw 
~~ def parse_question_container ( html_question ) : 
container_info = { } 
bs_question = bs4 . BeautifulSoup ( html_question , "html.parser" ) 
question = AskbotParser . _find_question_container ( bs_question ) 
container = question . select ( "div.post-update-info" ) 
created = container [ 0 ] 
container_info [ 'author' ] = AskbotParser . parse_user_info ( created ) 
~~~ container [ 1 ] 
~~~ updated = container [ 1 ] 
if AskbotParser . parse_user_info ( updated ) : 
~~~ container_info [ 'updated_by' ] = AskbotParser . parse_user_info ( updated ) 
~~ ~~ return container_info 
~~ def parse_answers ( html_question ) : 
def parse_answer_container ( update_info ) : 
created = update_info [ 0 ] 
answered_at = created . abbr . attrs [ "title" ] 
container_info [ 'added_at' ] = str ( str_to_datetime ( answered_at ) . timestamp ( ) ) 
container_info [ 'answered_by' ] = AskbotParser . parse_user_info ( created ) 
~~~ update_info [ 1 ] 
~~~ updated = update_info [ 1 ] 
updated_at = updated . abbr . attrs [ "title" ] 
container_info [ 'updated_at' ] = str ( str_to_datetime ( updated_at ) . timestamp ( ) ) 
~~ answer_list = [ ] 
bs_answers = bs_question . select ( "div.answer" ) 
for bs_answer in bs_answers : 
~~~ answer_id = bs_answer . attrs [ "data-post-id" ] 
votes_element = bs_answer . select ( "div.vote-number" ) [ 0 ] . text 
accepted_answer = bs_answer . select ( "div.answer-img-accept" ) [ 0 ] . get ( 'title' ) . endswith ( "correct" ) 
body = bs_answer . select ( "div.post-body" ) 
update_info = body [ 0 ] . select ( "div.post-update-info" ) 
answer_container = parse_answer_container ( update_info ) 
body [ 0 ] . div . extract ( ) . select ( "div.post-update-info-container" ) 
body = body [ 0 ] . get_text ( strip = True ) 
answer = { 'id' : answer_id , 
'score' : votes_element , 
'summary' : body , 
'accepted' : accepted_answer 
answer . update ( answer_container ) 
answer_list . append ( answer ) 
~~ return answer_list 
~~ def parse_number_of_html_pages ( html_question ) : 
~~~ bs_question . select ( 'div.paginator' ) [ 0 ] 
~~~ return 1 
~~~ return int ( bs_question . select ( 'div.paginator' ) [ 0 ] . attrs [ 'data-num-pages' ] ) 
~~ ~~ def parse_user_info ( update_info ) : 
user_info = { } 
if update_info . select ( "div.user-info" ) : 
~~~ elements = update_info . select ( "div.user-info" ) [ 0 ] . find_all ( "a" ) 
href = elements [ 0 ] . attrs [ "href" ] 
user_info [ 'id' ] = re . search ( r'\\d+' , href ) . group ( 0 ) 
user_info [ 'username' ] = elements [ 0 ] . text 
user_info [ 'reputation' ] = update_info . select ( 'span.reputation-score' ) [ 0 ] . text 
user_info [ 'badges' ] = update_info . select ( "span.badges" ) [ 0 ] . attrs [ "title" ] 
~~~ elements [ 1 ] 
~~~ user_info [ 'website' ] = elements [ 1 ] . attrs [ "href" ] 
~~ if update_info . select ( "img.flag" ) : 
~~~ flag = update_info . select ( "img.flag" ) [ 0 ] . attrs [ "alt" ] 
~~ ~~ return user_info 
if self . client . version [ 0 ] == 2 and self . client . version [ 1 ] == 8 : 
~~~ fetcher = self . _fetch_gerrit28 ( from_date ) 
~~~ fetcher = self . _fetch_gerrit ( from_date ) 
~~ for review in fetcher : 
~~~ yield review 
~~ ~~ def parse_reviews ( raw_data ) : 
items_raw = "[" + raw_data . replace ( "\\n" , "," ) + "]" 
items_raw = items_raw . replace ( ",]" , "]" ) 
items = json . loads ( items_raw ) 
reviews = [ ] 
for item in items : 
~~~ if 'project' in item . keys ( ) : 
~~~ reviews . append ( item ) 
~~ ~~ return reviews 
~~ def _fetch_gerrit28 ( self , from_date = DEFAULT_DATETIME ) : 
from_ut = datetime_to_utc ( from_date ) 
from_ut = from_ut . timestamp ( ) 
filter_open = "status:open" 
filter_closed = "status:closed" 
last_item_open = self . client . next_retrieve_group_item ( ) 
last_item_closed = self . client . next_retrieve_group_item ( ) 
reviews_open = self . _get_reviews ( last_item_open , filter_open ) 
reviews_closed = self . _get_reviews ( last_item_closed , filter_closed ) 
last_nreviews_open = len ( reviews_open ) 
last_nreviews_closed = len ( reviews_closed ) 
while reviews_open or reviews_closed : 
~~~ if reviews_open and reviews_closed : 
~~~ if reviews_open [ 0 ] [ 'lastUpdated' ] >= reviews_closed [ 0 ] [ 'lastUpdated' ] : 
~~~ review_open = reviews_open . pop ( 0 ) 
review = review_open 
~~~ review_closed = reviews_closed . pop ( 0 ) 
review = review_closed 
~~ ~~ elif reviews_closed : 
~~ updated = review [ 'lastUpdated' ] 
if updated <= from_ut : 
~~ if not reviews_open and last_nreviews_open >= self . max_reviews : 
~~~ last_item_open = self . client . next_retrieve_group_item ( last_item_open , review_open ) 
~~ if not reviews_closed and last_nreviews_closed >= self . max_reviews : 
~~~ last_item_closed = self . client . next_retrieve_group_item ( last_item_closed , review_closed ) 
~~ ~~ ~~ def version ( self ) : 
if self . _version : 
~~~ return self . _version 
raw_data = self . __execute ( cmd ) 
raw_data = str ( raw_data , "UTF-8" ) 
m = re . match ( GerritClient . VERSION_REGEX , raw_data ) 
if not m : 
~~~ mayor = int ( m . group ( 1 ) ) 
minor = int ( m . group ( 2 ) ) 
~~ self . _version = [ mayor , minor ] 
return self . _version 
~~ def reviews ( self , last_item , filter_ = None ) : 
cmd = self . _get_gerrit_cmd ( last_item , filter_ ) 
return raw_data 
~~ def next_retrieve_group_item ( self , last_item = None , entry = None ) : 
next_item = None 
gerrit_version = self . version 
if gerrit_version [ 0 ] == 2 and gerrit_version [ 1 ] > 9 : 
~~~ if last_item is None : 
~~~ next_item = 0 
~~~ next_item = last_item 
~~ ~~ elif gerrit_version [ 0 ] == 2 and gerrit_version [ 1 ] == 9 : 
~~~ if entry is not None : 
~~~ next_item = entry [ 'sortKey' ] 
~~ ~~ return next_item 
~~ def __execute ( self , cmd ) : 
if self . from_archive : 
~~~ response = self . __execute_from_archive ( cmd ) 
~~~ response = self . __execute_from_remote ( cmd ) 
~~ def __execute_from_archive ( self , cmd ) : 
cmd = self . sanitize_for_archive ( cmd ) 
response = self . archive . retrieve ( cmd , None , None ) 
if isinstance ( response , RuntimeError ) : 
~~~ raise response 
~~ def __execute_from_remote ( self , cmd ) : 
retries = 0 
while retries < self . MAX_RETRIES : 
~~~ result = subprocess . check_output ( cmd , shell = True ) 
time . sleep ( self . RETRY_WAIT * retries ) 
retries += 1 
~~ ~~ if result is None : 
~~ if self . archive : 
~~~ cmd = self . sanitize_for_archive ( cmd ) 
self . archive . store ( cmd , None , None , result ) 
~~ if isinstance ( result , RuntimeError ) : 
~~~ raise result 
group . add_argument ( '--user' , dest = 'user' , 
group . add_argument ( '--max-reviews' , dest = 'max_reviews' , 
type = int , default = MAX_REVIEWS , 
group . add_argument ( '--blacklist-reviews' , dest = 'blacklist_reviews' , 
nargs = '*' , 
group . add_argument ( '--disable-host-key-check' , dest = 'disable_host_key_check' , action = 'store_true' , 
help = "Don\ ) 
group . add_argument ( '--ssh-port' , dest = 'port' , 
default = PORT , type = int , 
parser . parser . add_argument ( 'hostname' , 
self . distribution , str ( from_date ) ) 
nissues = 0 
for issue in self . _fetch_issues ( from_date ) : 
~~~ yield issue 
nissues += 1 
return LaunchpadClient ( self . distribution , self . package , self . items_per_page , 
self . sleep_time , self . archive , from_archive ) 
~~ def _fetch_issues ( self , from_date ) : 
issues_groups = self . client . issues ( start = from_date ) 
~~~ issues = json . loads ( raw_issues ) [ 'entries' ] 
~~~ issue = self . __init_extra_issue_fields ( issue ) 
issue_id = self . __extract_issue_id ( issue [ 'bug_link' ] ) 
for field in TARGET_ISSUE_FIELDS : 
~~~ if not issue [ field ] : 
~~ if field == 'bug_link' : 
~~~ issue [ 'bug_data' ] = self . __fetch_issue_data ( issue_id ) 
issue [ 'activity_data' ] = [ activity for activity in self . __fetch_issue_activities ( issue_id ) ] 
issue [ 'messages_data' ] = [ message for message in self . __fetch_issue_messages ( issue_id ) ] 
issue [ 'attachments_data' ] = [ attachment for attachment in self . __fetch_issue_attachments ( issue_id ) ] 
~~ elif field == 'assignee_link' : 
~~~ issue [ 'assignee_data' ] = self . __fetch_user_data ( '{ASSIGNEE}' , issue [ field ] ) 
~~ elif field == 'owner_link' : 
~~~ issue [ 'owner_data' ] = self . __fetch_user_data ( '{OWNER}' , issue [ field ] ) 
~~ ~~ yield issue 
~~ ~~ ~~ def __fetch_issue_data ( self , issue_id ) : 
raw_issue = self . client . issue ( issue_id ) 
issue = json . loads ( raw_issue ) 
return issue 
~~ def __fetch_issue_attachments ( self , issue_id ) : 
for attachments_raw in self . client . issue_collection ( issue_id , "attachments" ) : 
~~~ attachments = json . loads ( attachments_raw ) 
for attachment in attachments [ 'entries' ] : 
~~~ yield attachment 
~~ ~~ ~~ def __fetch_issue_messages ( self , issue_id ) : 
for messages_raw in self . client . issue_collection ( issue_id , "messages" ) : 
~~~ messages = json . loads ( messages_raw ) 
for msg in messages [ 'entries' ] : 
~~~ msg [ 'owner_data' ] = self . __fetch_user_data ( '{OWNER}' , msg [ 'owner_link' ] ) 
yield msg 
~~ ~~ ~~ def __fetch_issue_activities ( self , issue_id ) : 
for activities_raw in self . client . issue_collection ( issue_id , "activity" ) : 
~~~ activities = json . loads ( activities_raw ) 
for act in activities [ 'entries' ] : 
~~~ act [ 'person_data' ] = self . __fetch_user_data ( '{PERSON}' , act [ 'person_link' ] ) 
yield act 
~~ ~~ ~~ def __fetch_user_data ( self , tag_type , user_link ) : 
user_name = self . client . user_name ( user_link ) 
user = { } 
if not user_name : 
~~~ return user 
~~ user_raw = self . client . user ( user_name ) 
user = json . loads ( user_raw ) 
return user 
~~ def issues ( self , start = None ) : 
payload = self . __build_payload ( size = self . items_per_page , operation = True , startdate = start ) 
path = self . __get_url_project ( ) 
return self . __fetch_items ( path = path , payload = payload ) 
~~ def user ( self , user_name ) : 
user = None 
if user_name in self . _users : 
~~~ return self . _users [ user_name ] 
~~ url_user = self . __get_url ( "~" + user_name ) 
~~~ raw_user = self . __send_request ( url_user ) 
user = raw_user 
~~~ if e . response . status_code in [ 404 , 410 ] : 
user = '{}' 
~~ ~~ self . _users [ user_name ] = user 
~~ def issue ( self , issue_id ) : 
path = urijoin ( "bugs" , str ( issue_id ) ) 
url_issue = self . __get_url ( path ) 
raw_text = self . __send_request ( url_issue ) 
return raw_text 
~~ def issue_collection ( self , issue_id , collection_name ) : 
path = urijoin ( "bugs" , str ( issue_id ) , collection_name ) 
url_collection = self . __get_url ( path ) 
payload = { 'ws.size' : self . items_per_page , 'ws.start' : 0 , 'order_by' : 'date_last_updated' } 
raw_items = self . __fetch_items ( path = url_collection , payload = payload ) 
return raw_items 
~~ def __get_url_project ( self ) : 
if self . package : 
~~~ url = self . __get_url_distribution_package ( ) 
~~~ url = self . __get_url_distribution ( ) 
~~ return url 
~~ def __send_request ( self , url , params = None ) : 
~~ def __build_payload ( self , size , operation = False , startdate = None ) : 
'ws.size' : size , 
'order_by' : 'date_last_updated' , 
'omit_duplicates' : 'false' , 
'status' : [ "New" , "Incomplete" , "Opinion" , "Invalid" , "Won\ , 
if operation : 
~~~ payload [ 'ws.op' ] = 'searchTasks' 
~~ if startdate : 
~~~ startdate = startdate . isoformat ( ) 
payload [ 'modified_since' ] = startdate 
~~ return payload 
~~ def __fetch_items ( self , path , payload ) : 
url_next = path 
fetch_data = True 
while fetch_data : 
~~~ raw_content = self . __send_request ( url_next , payload ) 
content = json . loads ( raw_content ) 
~~~ if e . response . status_code in [ 410 ] : 
raw_content = \ 
~~ ~~ if 'next_collection_link' in content : 
~~~ url_next = content [ 'next_collection_link' ] 
payload = None 
~~~ fetch_data = False 
~~ yield raw_content 
~~ ~~ def fetch_items ( self , category , ** kwargs ) : 
self . uri , str ( from_date ) ) 
mailing_list = GroupsioClient ( self . group_name , self . dirpath , 
self . api_token , self . verify ) 
mailing_list . fetch ( ) 
messages = self . _fetch_and_parse_messages ( mailing_list , from_date ) 
~~~ yield message 
~~ def fetch ( self ) : 
~~ group_id = self . __find_group_id ( ) 
url = urijoin ( GROUPSIO_API_URL , self . DOWNLOAD_ARCHIVES ) 
payload = { 'group_id' : group_id } 
filepath = os . path . join ( self . dirpath , MBOX_FILE ) 
success = self . _download_archive ( url , payload , filepath ) 
return success 
~~ def subscriptions ( self , per_page = PER_PAGE ) : 
url = urijoin ( GROUPSIO_API_URL , self . GET_SUBSCRIPTIONS ) 
keep_fetching = True 
"limit" : per_page 
while keep_fetching : 
~~~ r = self . __fetch ( url , payload ) 
response_raw = r . json ( ) 
subscriptions = response_raw [ 'data' ] 
yield subscriptions 
total_subscriptions = response_raw [ 'total_count' ] 
payload [ 'page_token' ] = response_raw [ 'next_page_token' ] 
keep_fetching = response_raw [ 'has_more' ] 
~~ ~~ def __find_group_id ( self ) : 
group_subscriptions = self . subscriptions ( self . auth ) 
for subscriptions in group_subscriptions : 
~~~ for sub in subscriptions : 
~~~ if sub [ 'group_name' ] == self . group_name : 
~~~ return sub [ 'group_id' ] 
raise BackendError ( cause = msg ) 
~~ def __fetch ( self , url , payload ) : 
r = requests . get ( url , params = payload , auth = self . auth , verify = self . verify ) 
~~~ r . raise_for_status ( ) 
~~ return r 
dirpath = os . path . join ( base_path , GROUPSIO_URL , 'g' , self . parsed_args . group_name ) 
token_auth = True ) 
group . add_argument ( '--mboxes-path' , dest = 'mboxes_path' , 
group . add_argument ( '--no-verify' , dest = 'verify' , 
action = 'store_false' , 
parser . parser . add_argument ( 'group_name' , 
~~ def uuid ( * args ) : 
def check_value ( v ) : 
~~~ if not isinstance ( v , str ) : 
~~ elif not v : 
~~~ return v 
~~ ~~ s = ':' . join ( map ( check_value , args ) ) 
sha1 = hashlib . sha1 ( s . encode ( 'utf-8' , errors = 'surrogateescape' ) ) 
uuid_sha1 = sha1 . hexdigest ( ) 
return uuid_sha1 
~~ def fetch ( backend_class , backend_args , category , filter_classified = False , 
manager = None ) : 
init_args = find_signature_parameters ( backend_class . __init__ , 
backend_args ) 
archive = manager . create_archive ( ) if manager else None 
init_args [ 'archive' ] = archive 
backend = backend_class ( ** init_args ) 
if category : 
~~~ backend_args [ 'category' ] = category 
~~ if filter_classified : 
~~~ backend_args [ 'filter_classified' ] = filter_classified 
~~ fetch_args = find_signature_parameters ( backend . fetch , 
items = backend . fetch ( ** fetch_args ) 
~~~ for item in items : 
~~~ if manager : 
~~~ archive_path = archive . archive_path 
manager . remove_archive ( archive_path ) 
~~ raise e 
~~ ~~ def fetch_from_archive ( backend_class , backend_args , manager , 
category , archived_after ) : 
filepaths = manager . search ( backend . origin , 
backend . __class__ . __name__ , 
category , 
archived_after ) 
for filepath in filepaths : 
~~~ backend . archive = Archive ( filepath ) 
items = backend . fetch_from_archive ( ) 
~~ ~~ except ArchiveError as e : 
~~ ~~ ~~ def find_backends ( top_package ) : 
candidates = pkgutil . walk_packages ( top_package . __path__ , 
prefix = top_package . __name__ + '.' ) 
modules = [ name for _ , name , is_pkg in candidates if not is_pkg ] 
return _import_backends ( modules ) 
~~ def fetch ( self , category , filter_classified = False , ** kwargs ) : 
if category not in self . categories : 
~~ if filter_classified and self . archive : 
~~~ self . archive . init_metadata ( self . origin , self . __class__ . __name__ , self . version , category , 
kwargs ) 
~~ self . client = self . _init_client ( ) 
for item in self . fetch_items ( category , ** kwargs ) : 
~~~ if filter_classified : 
~~~ item = self . filter_classified_data ( item ) 
~~ yield self . metadata ( item , filter_classified = filter_classified ) 
~~ ~~ def fetch_from_archive ( self ) : 
if not self . archive : 
~~ self . client = self . _init_client ( from_archive = True ) 
for item in self . fetch_items ( self . archive . category , ** self . archive . backend_params ) : 
~~~ yield self . metadata ( item ) 
~~ ~~ def filter_classified_data ( self , item ) : 
item_uuid = uuid ( self . origin , self . metadata_id ( item ) ) 
for cf in self . CLASSIFIED_FIELDS : 
~~~ _remove_key_from_nested_dict ( item , cf ) 
'.' . join ( cf ) , item_uuid ) 
return item 
~~ def metadata ( self , item , filter_classified = False ) : 
item = { 
'backend_name' : self . __class__ . __name__ , 
'backend_version' : self . version , 
'perceval_version' : __version__ , 
'timestamp' : datetime_utcnow ( ) . timestamp ( ) , 
'origin' : self . origin , 
'uuid' : uuid ( self . origin , self . metadata_id ( item ) ) , 
'updated_on' : self . metadata_updated_on ( item ) , 
'classified_fields_filtered' : self . classified_fields if filter_classified else None , 
'category' : self . metadata_category ( item ) , 
'tag' : self . tag , 
'data' : item , 
~~ def parse ( self , * args ) : 
parsed_args = self . parser . parse_args ( args ) 
if parsed_args . category is None : 
~~~ delattr ( parsed_args , 'category' ) 
~~ if self . _from_date : 
~~~ parsed_args . from_date = str_to_datetime ( parsed_args . from_date ) 
~~ if self . _to_date and parsed_args . to_date : 
~~~ parsed_args . to_date = str_to_datetime ( parsed_args . to_date ) 
~~ if self . _archive and parsed_args . archived_since : 
~~~ parsed_args . archived_since = str_to_datetime ( parsed_args . archived_since ) 
~~ if self . _archive and parsed_args . fetch_archive and parsed_args . no_archive : 
~~ if self . _archive and parsed_args . fetch_archive and not parsed_args . category : 
~~ for alias , arg in self . aliases . items ( ) : 
~~~ if ( alias not in parsed_args ) and ( arg in parsed_args ) : 
~~~ value = getattr ( parsed_args , arg , None ) 
setattr ( parsed_args , alias , value ) 
~~ ~~ return parsed_args 
~~ def _set_auth_arguments ( self , basic_auth = True , token_auth = False ) : 
if basic_auth : 
~~~ group . add_argument ( '-u' , '--backend-user' , dest = 'user' , 
group . add_argument ( '-p' , '--backend-password' , dest = 'password' , 
~~ if token_auth : 
~~~ group . add_argument ( '-t' , '--api-token' , dest = 'api_token' , 
~~ ~~ def _set_archive_arguments ( self ) : 
group . add_argument ( '--archive-path' , dest = 'archive_path' , default = None , 
group . add_argument ( '--no-archive' , dest = 'no_archive' , action = 'store_true' , 
group . add_argument ( '--fetch-archive' , dest = 'fetch_archive' , action = 'store_true' , 
group . add_argument ( '--archived-since' , dest = 'archived_since' , default = '1970-01-01' , 
~~ def _set_output_arguments ( self ) : 
group . add_argument ( '-o' , '--output' , type = argparse . FileType ( 'w' ) , 
dest = 'outfile' , default = sys . stdout , 
group . add_argument ( '--json-line' , dest = 'json_line' , action = 'store_true' , 
~~ def run ( self ) : 
backend_args = vars ( self . parsed_args ) 
category = backend_args . pop ( 'category' , None ) 
filter_classified = backend_args . pop ( 'filter_classified' , False ) 
archived_since = backend_args . pop ( 'archived_since' , None ) 
if self . archive_manager and self . parsed_args . fetch_archive : 
~~~ items = fetch_from_archive ( self . BACKEND , backend_args , 
self . archive_manager , 
archived_since ) 
~~~ items = fetch ( self . BACKEND , backend_args , category , 
manager = self . archive_manager ) 
~~~ if self . json_line : 
~~~ obj = json . dumps ( item , separators = ( ',' , ':' ) , sort_keys = True ) 
~~~ obj = json . dumps ( item , indent = 4 , sort_keys = True ) 
~~ self . outfile . write ( obj ) 
self . outfile . write ( '\\n' ) 
~~ ~~ except IOError as e : 
~~~ raise RuntimeError ( str ( e ) ) 
~~ ~~ def _initialize_archive ( self ) : 
if 'archive_path' not in self . parsed_args : 
~~~ manager = None 
~~ elif self . parsed_args . no_archive : 
~~~ if not self . parsed_args . archive_path : 
~~~ archive_path = os . path . expanduser ( ARCHIVES_DEFAULT_PATH ) 
~~~ archive_path = self . parsed_args . archive_path 
~~ manager = ArchiveManager ( archive_path ) 
~~ self . archive_manager = manager 
self . uri , self . dirpath , str ( from_date ) ) 
mailing_list = MailingList ( self . uri , self . dirpath ) 
ts = item [ MBox . DATE_FIELD ] 
~~ def parse_mbox ( filepath ) : 
mbox = _MBox ( filepath , create = False ) 
for msg in mbox : 
~~~ message = message_to_dict ( msg ) 
~~ ~~ def _fetch_and_parse_messages ( self , mailing_list , from_date ) : 
nmsgs , imsgs , tmsgs = ( 0 , 0 , 0 ) 
for mbox in mailing_list . mboxes : 
~~~ tmp_path = None 
~~~ tmp_path = self . _copy_mbox ( mbox ) 
for message in self . parse_mbox ( tmp_path ) : 
~~~ tmsgs += 1 
if not self . _validate_message ( message ) : 
~~~ imsgs += 1 
~~ dt = str_to_datetime ( message [ MBox . DATE_FIELD ] ) 
if dt < from_date : 
message [ 'unixfrom' ] , str ( from_date ) ) 
tmsgs -= 1 
~~ message = self . _casedict_to_dict ( message ) 
~~ ~~ except ( OSError , EOFError ) as e : 
~~~ if tmp_path and os . path . exists ( tmp_path ) : 
~~~ os . remove ( tmp_path ) 
nmsgs , tmsgs , imsgs ) 
~~ def _copy_mbox ( self , mbox ) : 
tmp_path = tempfile . mktemp ( prefix = 'perceval_' ) 
with mbox . container as f_in : 
~~~ with open ( tmp_path , mode = 'wb' ) as f_out : 
~~~ for l in f_in : 
~~~ f_out . write ( l ) 
~~ ~~ ~~ return tmp_path 
~~ def _validate_message ( self , message ) : 
if self . MESSAGE_ID_FIELD not in message : 
message [ 'unixfrom' ] ) 
~~ if not message [ self . MESSAGE_ID_FIELD ] : 
~~ if self . DATE_FIELD not in message : 
~~ if not message [ self . DATE_FIELD ] : 
~~~ str_to_datetime ( message [ self . DATE_FIELD ] ) 
~~ except InvalidDateError : 
message [ self . DATE_FIELD ] , message [ 'unixfrom' ] ) 
~~ def _casedict_to_dict ( self , message ) : 
message_id = message . pop ( self . MESSAGE_ID_FIELD ) 
date = message . pop ( self . DATE_FIELD ) 
msg = { k : v for k , v in message . items ( ) } 
msg [ self . MESSAGE_ID_FIELD ] = message_id 
msg [ self . DATE_FIELD ] = date 
return msg 
~~ def get_message ( self , key ) : 
start , stop = self . _lookup ( key ) 
self . _file . seek ( start ) 
from_line = self . _file . readline ( ) . replace ( mailbox . linesep , b'' ) 
string = self . _file . read ( stop - self . _file . tell ( ) ) 
msg = self . _message_factory ( string . replace ( mailbox . linesep , b'\\n' ) ) 
~~~ msg . set_from ( from_line [ 5 : ] . decode ( 'ascii' ) ) 
~~~ msg . set_from ( from_line [ 5 : ] . decode ( 'utf-8' ) ) 
~~~ msg . set_from ( from_line [ 5 : ] . decode ( 'iso-8859-1' ) ) 
~~ return msg 
if os . path . isfile ( self . dirpath ) : 
~~~ archives . append ( MBoxArchive ( self . dirpath ) ) 
~~~ for root , _ , files in os . walk ( self . dirpath ) : 
~~~ for filename in sorted ( files ) : 
~~~ location = os . path . join ( root , filename ) 
archives . append ( MBoxArchive ( location ) ) 
~~ ~~ ~~ ~~ return archives 
~~ def fetch ( self , category = CATEGORY_COMMIT , from_date = DEFAULT_DATETIME , to_date = DEFAULT_LAST_DATETIME , 
branches = None , latest_items = False , no_update = False ) : 
~~ if not to_date : 
~~~ to_date = DEFAULT_LAST_DATETIME 
~~ kwargs = { 
'from_date' : from_date , 
'to_date' : to_date , 
'branches' : branches , 
'latest_items' : latest_items , 
'no_update' : no_update 
branches = kwargs [ 'branches' ] 
latest_items = kwargs [ 'latest_items' ] 
no_update = kwargs [ 'no_update' ] 
ncommits = 0 
~~~ if os . path . isfile ( self . gitpath ) : 
~~~ commits = self . __fetch_from_log ( ) 
~~~ commits = self . __fetch_from_repo ( from_date , to_date , branches , 
latest_items , no_update ) 
~~ for commit in commits : 
~~~ yield commit 
ncommits += 1 
~~ ~~ except EmptyRepositoryError : 
ncommits ) 
~~ def parse_git_log_from_file ( filepath ) : 
with open ( filepath , 'r' , errors = 'surrogateescape' , 
newline = os . linesep ) as f : 
~~~ parser = GitParser ( f ) 
for commit in parser . parse ( ) : 
~~ ~~ ~~ def _pre_init ( self ) : 
if self . parsed_args . git_log : 
~~~ git_path = self . parsed_args . git_log 
~~ elif not self . parsed_args . git_path : 
~~~ base_path = os . path . expanduser ( '~/.perceval/repositories/' ) 
processed_uri = self . parsed_args . uri . lstrip ( '/' ) 
git_path = os . path . join ( base_path , processed_uri ) + '-git' 
~~~ git_path = self . parsed_args . git_path 
~~ setattr ( self . parsed_args , 'gitpath' , git_path ) 
to_date = True ) 
group . add_argument ( '--branches' , dest = 'branches' , 
nargs = '+' , type = str , default = None , 
exgroup = group . add_mutually_exclusive_group ( ) 
exgroup . add_argument ( '--git-path' , dest = 'git_path' , 
exgroup . add_argument ( '--git-log' , dest = 'git_log' , 
exgroup_fetch = group . add_mutually_exclusive_group ( ) 
exgroup_fetch . add_argument ( '--latest-items' , dest = 'latest_items' , 
exgroup_fetch . add_argument ( '--no-update' , dest = 'no_update' , 
parser . parser . add_argument ( 'uri' , 
~~ def parse ( self ) : 
for line in self . stream : 
~~~ line = line . rstrip ( '\\n' ) 
parsed = False 
self . nline += 1 
while not parsed : 
~~~ parsed = self . handlers [ self . state ] ( line ) 
if self . state == self . COMMIT and self . commit : 
~~~ commit = self . _build_commit ( ) 
yield commit 
~~ ~~ ~~ if self . commit : 
~~ ~~ def __get_old_filepath ( self , f ) : 
i = f . find ( '{' ) 
j = f . find ( '}' ) 
if i > - 1 and j > - 1 : 
~~~ prefix = f [ 0 : i ] 
suffix = f [ j + 1 : ] 
return prefix + inner + suffix 
~~~ return f 
~~ ~~ def clone ( cls , uri , dirpath ) : 
cmd = [ 'git' , 'clone' , '--bare' , uri , dirpath ] 
env = { 
'LANG' : 'C' , 
'HOME' : os . getenv ( 'HOME' , '' ) 
cls . _exec ( cmd , env = env ) 
uri , dirpath ) 
return cls ( uri , dirpath ) 
~~ def count_objects ( self ) : 
cmd_count = [ 'git' , 'count-objects' , '-v' ] 
outs = self . _exec ( cmd_count , cwd = self . dirpath , env = self . gitenv ) 
outs = outs . decode ( 'utf-8' , errors = 'surrogateescape' ) . rstrip ( ) 
nobjs = int ( cobjs [ 'count' ] ) + int ( cobjs [ 'in-pack' ] ) 
~~ except KeyError as e : 
raise RepositoryError ( cause = error ) 
~~ except ValueError as e : 
self . uri , str ( nobjs ) ) 
return nobjs 
~~ def is_detached ( self ) : 
cmd_sym = [ 'git' , 'symbolic-ref' , 'HEAD' ] 
~~~ self . _exec ( cmd_sym , cwd = self . dirpath , env = self . gitenv ) 
~~ except RepositoryError as e : 
~~ ~~ def update ( self ) : 
cmd_update = [ 'git' , 'fetch' , 'origin' , '+refs/heads/*:refs/heads/*' , '--prune' ] 
self . _exec ( cmd_update , cwd = self . dirpath , env = self . gitenv ) 
self . uri , self . dirpath ) 
~~ def sync ( self ) : 
pack_name , refs = self . _fetch_pack ( ) 
if pack_name : 
~~~ commits = self . _read_commits_from_pack ( pack_name ) 
~~~ commits = [ ] 
~~ self . _update_references ( refs ) 
return commits 
~~ def rev_list ( self , branches = None ) : 
if self . is_empty ( ) : 
self . uri ) 
raise EmptyRepositoryError ( repository = self . uri ) 
~~ cmd_rev_list = [ 'git' , 'rev-list' , '--topo-order' ] 
if branches is None : 
~~~ cmd_rev_list . extend ( [ '--branches' , '--tags' , '--remotes=origin' ] ) 
~~ elif len ( branches ) == 0 : 
~~~ cmd_rev_list . extend ( [ '--branches' , '--tags' , '--max-count=0' ] ) 
~~~ branches = [ 'refs/heads/' + branch for branch in branches ] 
cmd_rev_list . extend ( branches ) 
~~ for line in self . _exec_nb ( cmd_rev_list , cwd = self . dirpath , env = self . gitenv ) : 
~~~ yield line . rstrip ( '\\n' ) 
~~ def log ( self , from_date = None , to_date = None , branches = None , encoding = 'utf-8' ) : 
~~ cmd_log = [ 'git' , 'log' , '--reverse' , '--topo-order' ] 
cmd_log . extend ( self . GIT_PRETTY_OUTPUT_OPTS ) 
cmd_log . append ( '--since=' + dt ) 
~~ if to_date : 
cmd_log . append ( '--until=' + dt ) 
~~ if branches is None : 
~~~ cmd_log . extend ( [ '--branches' , '--tags' , '--remotes=origin' ] ) 
~~~ cmd_log . append ( '--max-count=0' ) 
cmd_log . extend ( branches ) 
~~ for line in self . _exec_nb ( cmd_log , cwd = self . dirpath , env = self . gitenv ) : 
~~ def show ( self , commits = None , encoding = 'utf-8' ) : 
~~ if commits is None : 
~~ cmd_show = [ 'git' , 'show' ] 
cmd_show . extend ( self . GIT_PRETTY_OUTPUT_OPTS ) 
cmd_show . extend ( commits ) 
for line in self . _exec_nb ( cmd_show , cwd = self . dirpath , env = self . gitenv ) : 
~~ def _fetch_pack ( self ) : 
def prepare_refs ( refs ) : 
~~~ return [ ref . hash . encode ( 'utf-8' ) for ref in refs 
if not ref . refname . endswith ( '^{}' ) ] 
~~ def determine_wants ( refs ) : 
~~~ remote_refs = prepare_refs ( self . _discover_refs ( remote = True ) ) 
local_refs = prepare_refs ( self . _discover_refs ( ) ) 
wants = [ ref for ref in remote_refs if ref not in local_refs ] 
return wants 
~~ client , repo_path = dulwich . client . get_transport_and_path ( self . uri ) 
repo = dulwich . repo . Repo ( self . dirpath ) 
fd = io . BytesIO ( ) 
local_refs = self . _discover_refs ( ) 
graph_walker = _GraphWalker ( local_refs ) 
result = client . fetch_pack ( repo_path , 
determine_wants , 
graph_walker , 
fd . write ) 
refs = [ GitRef ( ref_hash . decode ( 'utf-8' ) , ref_name . decode ( 'utf-8' ) ) 
for ref_name , ref_hash in result . refs . items ( ) ] 
if len ( fd . getvalue ( ) ) > 0 : 
~~~ fd . seek ( 0 ) 
pack = repo . object_store . add_thin_pack ( fd . read , None ) 
pack_name = pack . name ( ) . decode ( 'utf-8' ) 
~~~ pack_name = None 
~~ return ( pack_name , refs ) 
~~ def _read_commits_from_pack ( self , packet_name ) : 
filepath = 'objects/pack/pack-' + packet_name 
cmd_verify_pack = [ 'git' , 'verify-pack' , '-v' , filepath ] 
outs = self . _exec ( cmd_verify_pack , cwd = self . dirpath , env = self . gitenv ) 
commits = [ parts [ 0 ] for parts in lines if parts [ 1 ] == 'commit' ] 
commits . reverse ( ) 
~~ def _update_references ( self , refs ) : 
new_refs = [ ref . refname for ref in refs ] 
for old_ref in self . _discover_refs ( ) : 
~~~ if not old_ref . refname . startswith ( 'refs/heads/' ) : 
~~ if old_ref . refname in new_refs : 
~~ self . _update_ref ( old_ref , delete = True ) 
~~ for new_ref in refs : 
~~~ refname = new_ref . refname 
if refname . endswith ( '^{}' ) : 
refname ) 
~~ elif not refname . startswith ( 'refs/heads/' ) and not refname . startswith ( 'refs/tags/' ) : 
~~~ self . _update_ref ( new_ref ) 
~~ ~~ cmd = [ 'git' , 'remote' , 'prune' , 'origin' ] 
self . _exec ( cmd , cwd = self . dirpath , env = self . gitenv ) 
~~ def _discover_refs ( self , remote = False ) : 
if remote : 
~~~ cmd_refs = [ 'git' , 'ls-remote' , '-h' , '-t' , '--exit-code' , 'origin' ] 
sep = '\\t' 
ignored_error_codes = [ 2 ] 
~~~ if self . is_empty ( ) : 
~~~ raise EmptyRepositoryError ( repository = self . uri ) 
~~ cmd_refs = [ 'git' , 'show-ref' , '--heads' , '--tags' ] 
ignored_error_codes = [ 1 ] 
~~ outs = self . _exec ( cmd_refs , cwd = self . dirpath , 
env = self . gitenv , 
ignored_error_codes = ignored_error_codes ) 
outs = outs . split ( '\\n' ) if outs else [ ] 
refs = [ ] 
for line in outs : 
~~~ data = line . split ( sep ) 
ref = GitRef ( data [ 0 ] , data [ 1 ] ) 
refs . append ( ref ) 
~~ return refs 
~~ def _update_ref ( self , ref , delete = False ) : 
cmd = [ 'git' , 'update-ref' ] 
if delete : 
~~~ cmd . extend ( [ '-d' , ref . refname ] ) 
action = 'deleted' 
~~~ cmd . extend ( [ ref . refname , ref . hash ] ) 
~~~ self . _exec ( cmd , cwd = self . dirpath , env = self . gitenv ) 
ref . refname , action , self . uri , self . dirpath ) 
~~ ~~ def _exec_nb ( self , cmd , cwd = None , env = None , encoding = 'utf-8' ) : 
self . failed_message = None 
~~~ self . proc = subprocess . Popen ( cmd , 
cwd = cwd , 
env = env ) 
err_thread = threading . Thread ( target = self . _read_stderr , 
kwargs = { 'encoding' : encoding } , 
daemon = True ) 
err_thread . start ( ) 
for line in self . proc . stdout : 
~~~ yield line . decode ( encoding , errors = 'surrogateescape' ) 
~~ err_thread . join ( ) 
self . proc . communicate ( ) 
self . proc . stdout . close ( ) 
self . proc . stderr . close ( ) 
~~~ err_thread . join ( ) 
raise RepositoryError ( cause = str ( e ) ) 
~~ if self . proc . returncode != 0 : 
raise RepositoryError ( cause = cause ) 
~~ ~~ def _read_stderr ( self , encoding = 'utf-8' ) : 
for line in self . proc . stderr : 
~~~ err_line = line . decode ( encoding , errors = 'surrogateescape' ) 
if self . proc . returncode != 0 : 
~~~ if self . failed_message is not None : 
~~ self . failed_message = err_line 
~~ ~~ ~~ def _exec ( cmd , cwd = None , env = None , ignored_error_codes = None , 
encoding = 'utf-8' ) : 
if ignored_error_codes is None : 
~~~ ignored_error_codes = [ ] 
~~~ proc = subprocess . Popen ( cmd , stdout = subprocess . PIPE , 
cwd = cwd , env = env ) 
( outs , errs ) = proc . communicate ( ) 
~~~ raise RepositoryError ( cause = str ( e ) ) 
~~ if proc . returncode != 0 and proc . returncode not in ignored_error_codes : 
~~~ err = errs . decode ( encoding , errors = 'surrogateescape' ) 
~~~ logger . debug ( errs . decode ( encoding , errors = 'surrogateescape' ) ) 
~~ return outs 
~~ def fetch ( self , category = CATEGORY_TWEET , since_id = None , max_id = None , 
geocode = None , lang = None , 
include_entities = True , tweets_type = TWEET_TYPE_MIXED ) : 
kwargs = { "since_id" : since_id , 
"max_id" : max_id , 
"geocode" : geocode , 
"lang" : lang , 
"include_entities" : include_entities , 
"result_type" : tweets_type } 
since_id = kwargs [ 'since_id' ] 
max_id = kwargs [ 'max_id' ] 
geocode = kwargs [ 'geocode' ] 
lang = kwargs [ 'lang' ] 
entities = kwargs [ 'include_entities' ] 
tweets_type = kwargs [ 'result_type' ] 
self . query , str ( since_id ) , 
str ( max_id ) if max_id else '--' ) 
tweets_ids = [ ] 
min_date = None 
max_date = None 
group_tweets = self . client . tweets ( self . query , since_id = since_id , max_id = max_id , geocode = geocode , 
lang = lang , include_entities = entities , result_type = tweets_type ) 
for tweets in group_tweets : 
~~~ for i in range ( len ( tweets ) ) : 
~~~ tweet = tweets [ i ] 
tweets_ids . append ( tweet [ 'id' ] ) 
if tweets [ - 1 ] == tweet : 
~~~ min_date = str_to_datetime ( tweets [ - 1 ] [ 'created_at' ] ) 
~~ if tweets [ 0 ] == tweet and not max_date : 
~~~ max_date = str_to_datetime ( tweets [ 0 ] [ 'created_at' ] ) 
~~ yield tweet 
len ( tweets_ids ) , len ( list ( set ( tweets_ids ) ) ) , min_date , max_date ) 
return TwitterClient ( self . api_token , self . max_items , 
self . sleep_for_rate , self . min_rate_to_sleep , self . sleep_time , 
self . archive , from_archive ) 
~~ def tweets ( self , query , since_id = None , max_id = None , geocode = None , lang = None , 
include_entities = True , result_type = TWEET_TYPE_MIXED ) : 
resource = self . base_url 
params = { 'q' : query , 
'count' : self . max_items } 
if since_id : 
~~~ params [ 'since_id' ] = since_id 
~~ if max_id : 
~~~ params [ 'max_id' ] = max_id 
~~ if geocode : 
~~~ params [ 'geocode' ] = geocode 
~~ if lang : 
~~~ params [ 'lang' ] = lang 
~~ params [ 'include_entities' ] = include_entities 
params [ 'result_type' ] = result_type 
~~~ raw_tweets = self . _fetch ( resource , params = params ) 
tweets = json . loads ( raw_tweets ) 
if not tweets [ 'statuses' ] : 
~~ params [ 'max_id' ] = tweets [ 'statuses' ] [ - 1 ] [ 'id' ] - 1 
yield tweets [ 'statuses' ] 
~~ ~~ def _fetch ( self , url , params ) : 
r = self . fetch ( url , payload = params , headers = headers ) 
group . add_argument ( '--no-entities' , dest = 'include_entities' , 
group . add_argument ( '--geo-code' , dest = 'geocode' , 
group . add_argument ( '--lang' , dest = 'lang' , 
group . add_argument ( '--tweets-type' , dest = 'tweets_type' , default = TWEET_TYPE_MIXED , 
default = SLEEP_TIME , type = int , 
parser . parser . add_argument ( 'query' , 
~~ def fetch ( self , category = CATEGORY_HITS ) : 
hits_raw = self . client . hits ( self . keywords ) 
hits = self . __parse_hits ( hits_raw ) 
yield hits 
return GoogleHitsClient ( self . sleep_time , self . max_retries , 
~~ def __parse_hits ( self , hit_raw ) : 
bs_result = bs4 . BeautifulSoup ( hit_raw , 'html.parser' ) 
hit_string = bs_result . find ( "div" , id = "resultStats" ) . text 
hit_string = hit_string . replace ( ',' , u'' ) 
hit_string = hit_string . replace ( '.' , u'' ) 
fetched_on = datetime_utcnow ( ) . timestamp ( ) 
id_args = self . keywords [ : ] 
id_args . append ( str ( fetched_on ) ) 
hits_json = { 
'fetched_on' : fetched_on , 
'id' : uuid ( * id_args ) , 
'keywords' : self . keywords , 
'type' : 'googleSearchHits' 
if not hit_string : 
hits_json [ 'hits' ] = 0 
return hits_json 
~~ str_hits = re . search ( r'\\d+' , hit_string ) . group ( 0 ) 
hits = int ( str_hits ) 
hits_json [ 'hits' ] = hits 
~~ def hits ( self , keywords ) : 
if len ( keywords ) == 1 : 
~~~ query_str = keywords [ 0 ] 
params = { 'q' : query_str } 
req = self . fetch ( GOOGLE_SEARCH_URL , payload = params ) 
~~ def fetch ( self , category = CATEGORY_ISSUE , from_date = DEFAULT_DATETIME , to_date = DEFAULT_LAST_DATETIME ) : 
to_date = datetime_to_utc ( to_date ) 
kwargs = { 
'to_date' : to_date 
~~~ items = self . __fetch_issues ( from_date , to_date ) 
~~ elif category == CATEGORY_PULL_REQUEST : 
~~~ items = self . __fetch_pull_requests ( from_date , to_date ) 
~~~ items = self . __fetch_repo_info ( ) 
if "forks_count" in item : 
~~~ return item [ 'fetched_on' ] 
~~~ ts = item [ 'updated_at' ] 
~~ ~~ def metadata_category ( item ) : 
if "base" in item : 
~~~ category = CATEGORY_PULL_REQUEST 
~~ elif "forks_count" in item : 
~~~ category = CATEGORY_REPO 
~~~ category = CATEGORY_ISSUE 
~~ return category 
return GitHubClient ( self . owner , self . repository , self . api_token , self . base_url , 
self . sleep_for_rate , self . min_rate_to_sleep , 
self . sleep_time , self . max_retries , 
~~ def __fetch_issues ( self , from_date , to_date ) : 
~~~ if str_to_datetime ( issue [ 'updated_at' ] ) > to_date : 
~~ self . __init_extra_issue_fields ( issue ) 
~~ if field == 'user' : 
~~~ issue [ field + '_data' ] = self . __get_user ( issue [ field ] [ 'login' ] ) 
~~ elif field == 'assignee' : 
~~~ issue [ field + '_data' ] = self . __get_issue_assignee ( issue [ field ] ) 
~~ elif field == 'assignees' : 
~~~ issue [ field + '_data' ] = self . __get_issue_assignees ( issue [ field ] ) 
~~ elif field == 'comments' : 
~~~ issue [ field + '_data' ] = self . __get_issue_comments ( issue [ 'number' ] ) 
~~ elif field == 'reactions' : 
~~~ issue [ field + '_data' ] = self . __get_issue_reactions ( issue [ 'number' ] , issue [ 'reactions' ] [ 'total_count' ] ) 
~~ ~~ ~~ def __fetch_pull_requests ( self , from_date , to_date ) : 
raw_pulls = self . client . pulls ( from_date = from_date ) 
for raw_pull in raw_pulls : 
~~~ pull = json . loads ( raw_pull ) 
if str_to_datetime ( pull [ 'updated_at' ] ) > to_date : 
~~ self . __init_extra_pull_fields ( pull ) 
for field in TARGET_PULL_FIELDS : 
~~~ if not pull [ field ] : 
~~~ pull [ field + '_data' ] = self . __get_user ( pull [ field ] [ 'login' ] ) 
~~ elif field == 'merged_by' : 
~~ elif field == 'review_comments' : 
~~~ pull [ field + '_data' ] = self . __get_pull_review_comments ( pull [ 'number' ] ) 
~~ elif field == 'requested_reviewers' : 
~~~ pull [ field + '_data' ] = self . __get_pull_requested_reviewers ( pull [ 'number' ] ) 
~~ elif field == 'commits' : 
~~~ pull [ field + '_data' ] = self . __get_pull_commits ( pull [ 'number' ] ) 
~~ ~~ yield pull 
~~ ~~ def __fetch_repo_info ( self ) : 
raw_repo = self . client . repo ( ) 
repo = json . loads ( raw_repo ) 
fetched_on = datetime_utcnow ( ) 
repo [ 'fetched_on' ] = fetched_on . timestamp ( ) 
yield repo 
~~ def __get_issue_reactions ( self , issue_number , total_count ) : 
reactions = [ ] 
if total_count == 0 : 
~~~ return reactions 
~~ group_reactions = self . client . issue_reactions ( issue_number ) 
for raw_reactions in group_reactions : 
~~~ for reaction in json . loads ( raw_reactions ) : 
~~~ reaction [ 'user_data' ] = self . __get_user ( reaction [ 'user' ] [ 'login' ] ) 
reactions . append ( reaction ) 
~~ ~~ return reactions 
~~ def __get_issue_comments ( self , issue_number ) : 
comments = [ ] 
group_comments = self . client . issue_comments ( issue_number ) 
for raw_comments in group_comments : 
~~~ for comment in json . loads ( raw_comments ) : 
~~~ comment_id = comment . get ( 'id' ) 
comment [ 'user_data' ] = self . __get_user ( comment [ 'user' ] [ 'login' ] ) 
comment [ 'reactions_data' ] = self . __get_issue_comment_reactions ( comment_id , comment [ 'reactions' ] [ 'total_count' ] ) 
comments . append ( comment ) 
~~ ~~ return comments 
~~ def __get_issue_comment_reactions ( self , comment_id , total_count ) : 
~~ group_reactions = self . client . issue_comment_reactions ( comment_id ) 
~~ def __get_issue_assignees ( self , raw_assignees ) : 
assignees = [ ] 
for ra in raw_assignees : 
~~~ assignees . append ( self . __get_user ( ra [ 'login' ] ) ) 
~~ return assignees 
~~ def __get_pull_requested_reviewers ( self , pr_number ) : 
requested_reviewers = [ ] 
group_requested_reviewers = self . client . pull_requested_reviewers ( pr_number ) 
for raw_requested_reviewers in group_requested_reviewers : 
~~~ group_requested_reviewers = json . loads ( raw_requested_reviewers ) 
for requested_reviewer in group_requested_reviewers [ 'users' ] : 
~~~ user_data = self . __get_user ( requested_reviewer [ 'login' ] ) 
requested_reviewers . append ( user_data ) 
~~ ~~ return requested_reviewers 
~~ def __get_pull_commits ( self , pr_number ) : 
hashes = [ ] 
group_pull_commits = self . client . pull_commits ( pr_number ) 
for raw_pull_commits in group_pull_commits : 
~~~ for commit in json . loads ( raw_pull_commits ) : 
~~~ commit_hash = commit [ 'sha' ] 
hashes . append ( commit_hash ) 
~~ ~~ return hashes 
~~ def __get_pull_review_comments ( self , pr_number ) : 
group_comments = self . client . pull_review_comments ( pr_number ) 
user = comment . get ( 'user' , None ) 
if not user : 
comment [ 'user_data' ] = None 
~~~ comment [ 'user_data' ] = self . __get_user ( user [ 'login' ] ) 
~~ comment [ 'reactions_data' ] = self . __get_pull_review_comment_reactions ( comment_id , comment [ 'reactions' ] [ 'total_count' ] ) 
~~ def __get_pull_review_comment_reactions ( self , comment_id , total_count ) : 
~~ group_reactions = self . client . pull_review_comment_reactions ( comment_id ) 
~~ def __get_user ( self , login ) : 
if not login : 
~~ user_raw = self . client . user ( login ) 
user_orgs_raw = self . client . user_orgs ( login ) 
user [ 'organizations' ] = json . loads ( user_orgs_raw ) 
~~ def issue_reactions ( self , issue_number ) : 
'per_page' : PER_PAGE , 
'direction' : 'asc' , 
'sort' : 'updated' 
path = urijoin ( "issues" , str ( issue_number ) , "reactions" ) 
'sort' : 'updated' } 
~~~ payload [ 'since' ] = from_date . isoformat ( ) 
~~ path = urijoin ( "issues" ) 
~~ def pulls ( self , from_date = None ) : 
issues_groups = self . issues ( from_date = from_date ) 
~~~ if "pull_request" not in issue : 
~~ pull_number = issue [ "number" ] 
path = urijoin ( self . base_url , 'repos' , self . owner , self . repository , "pulls" , pull_number ) 
r = self . fetch ( path ) 
pull = r . text 
yield pull 
~~ ~~ ~~ def repo ( self ) : 
path = urijoin ( self . base_url , 'repos' , self . owner , self . repository ) 
repo = r . text 
return repo 
~~ def pull_requested_reviewers ( self , pr_number ) : 
requested_reviewers_url = urijoin ( "pulls" , str ( pr_number ) , "requested_reviewers" ) 
return self . fetch_items ( requested_reviewers_url , { } ) 
~~ def pull_commits ( self , pr_number ) : 
commit_url = urijoin ( "pulls" , str ( pr_number ) , "commits" ) 
return self . fetch_items ( commit_url , payload ) 
~~ def pull_review_comments ( self , pr_number ) : 
comments_url = urijoin ( "pulls" , str ( pr_number ) , "comments" ) 
return self . fetch_items ( comments_url , payload ) 
~~ def pull_review_comment_reactions ( self , comment_id ) : 
path = urijoin ( "pulls" , "comments" , str ( comment_id ) , "reactions" ) 
~~ def user ( self , login ) : 
if login in self . _users : 
~~~ return self . _users [ login ] 
~~ url_user = urijoin ( self . base_url , 'users' , login ) 
r = self . fetch ( url_user ) 
user = r . text 
self . _users [ login ] = user 
~~ def user_orgs ( self , login ) : 
if login in self . _users_orgs : 
~~~ return self . _users_orgs [ login ] 
~~ url = urijoin ( self . base_url , 'users' , login , 'orgs' ) 
~~~ r = self . fetch ( url ) 
orgs = r . text 
~~~ logger . error ( "Can\ , error ) 
orgs = '[]' 
~~ ~~ self . _users_orgs [ login ] = orgs 
return orgs 
~~ def _get_token_rate_limit ( self , token ) : 
rate_url = urijoin ( self . base_url , "rate_limit" ) 
remaining = 0 
~~~ headers = super ( ) . fetch ( rate_url ) . headers 
if self . rate_limit_header in headers : 
~~~ remaining = int ( headers [ self . rate_limit_header ] ) 
~~ return remaining 
~~ def _get_tokens_rate_limits ( self ) : 
remainings = [ 0 ] * self . n_tokens 
arch = self . archive 
self . archive = None 
for idx , token in enumerate ( self . tokens ) : 
~~~ remainings [ idx ] = self . _get_token_rate_limit ( token ) 
~~ self . archive = arch 
return remainings 
~~ def _choose_best_api_token ( self ) : 
if self . n_tokens == 0 : 
~~ token_idx = 0 
if self . n_tokens > 1 : 
~~~ remainings = self . _get_tokens_rate_limits ( ) 
token_idx = remainings . index ( max ( remainings ) ) 
~~ self . current_token = self . tokens [ token_idx ] 
self . _update_current_rate_limit ( ) 
~~ def _need_check_tokens ( self ) : 
if self . n_tokens <= 1 or self . rate_limit is None : 
~~ elif self . last_rate_limit_checked is None : 
~~~ self . last_rate_limit_checked = self . rate_limit 
~~ approaching_limit = float ( self . min_rate_to_sleep ) * ( 1.0 + TOKEN_USAGE_BEFORE_SWITCH ) + 1 
if self . rate_limit <= approaching_limit : 
~~ ratio = float ( self . rate_limit ) / float ( self . last_rate_limit_checked ) 
if ratio < 1.0 - TOKEN_USAGE_BEFORE_SWITCH : 
~~ elif ratio > 1.0 : 
~~ ~~ def _update_current_rate_limit ( self ) : 
url = urijoin ( self . base_url , "rate_limit" ) 
~~~ arch = self . archive 
response = super ( ) . fetch ( url ) 
self . archive = arch 
self . last_rate_limit_checked = self . rate_limit 
~~ ~~ ~~ def init_metadata ( self , origin , backend_name , backend_version , 
category , backend_params ) : 
created_on = datetime_to_utc ( datetime_utcnow ( ) ) 
created_on_dumped = created_on . isoformat ( ) 
backend_params_dumped = pickle . dumps ( backend_params , 0 ) 
metadata = ( origin , backend_name , backend_version , category , 
backend_params_dumped , created_on_dumped , ) 
~~~ cursor = self . _db . cursor ( ) 
cursor . execute ( insert_stmt , metadata ) 
self . _db . commit ( ) 
cursor . close ( ) 
~~ except sqlite3 . DatabaseError as e : 
raise ArchiveError ( cause = msg ) 
~~ self . origin = origin 
self . backend_name = backend_name 
self . backend_version = backend_version 
self . category = category 
self . backend_params = backend_params 
self . created_on = created_on 
self . archive_path , metadata ) 
~~ def store ( self , uri , payload , headers , data ) : 
hashcode = self . make_hashcode ( uri , payload , headers ) 
payload_dump = pickle . dumps ( payload , 0 ) 
headers_dump = pickle . dumps ( headers , 0 ) 
data_dump = pickle . dumps ( data , 0 ) 
hashcode , uri , payload , headers , self . archive_path ) 
cursor . execute ( insert_stmt , ( None , hashcode , uri , 
payload_dump , headers_dump , data_dump ) ) 
~~ except sqlite3 . IntegrityError as e : 
~~ def retrieve ( self , uri , payload , headers ) : 
self . _db . row_factory = sqlite3 . Row 
cursor . execute ( select_stmt , ( hashcode , ) ) 
row = cursor . fetchone ( ) 
~~ if row : 
~~~ found = pickle . loads ( row [ 'data' ] ) 
~~ return found 
~~ def create ( cls , archive_path ) : 
if os . path . exists ( archive_path ) : 
raise ArchiveError ( cause = msg % ( archive_path ) ) 
~~ conn = sqlite3 . connect ( archive_path ) 
cursor = conn . cursor ( ) 
cursor . execute ( cls . METADATA_CREATE_STMT ) 
cursor . execute ( cls . ARCHIVE_CREATE_STMT ) 
conn . commit ( ) 
conn . close ( ) 
archive = cls ( archive_path ) 
return archive 
~~ def make_hashcode ( uri , payload , headers ) : 
def dict_to_json_str ( data ) : 
~~~ return json . dumps ( data , sort_keys = True ) 
~~ content = ':' . join ( [ uri , dict_to_json_str ( payload ) , dict_to_json_str ( headers ) ] ) 
hashcode = hashlib . sha1 ( content . encode ( 'utf-8' ) ) 
return hashcode . hexdigest ( ) 
~~ def _verify_archive ( self ) : 
nentries = self . _count_table_rows ( self . ARCHIVE_TABLE ) 
nmetadata = self . _count_table_rows ( self . METADATA_TABLE ) 
if nmetadata > 1 : 
~~ if nmetadata == 0 and nentries > 0 : 
self . archive_path , nentries , nmetadata ) 
~~ def _load_metadata ( self ) : 
cursor = self . _db . cursor ( ) 
cursor . execute ( select_stmt ) 
if row : 
~~~ self . origin = row [ 0 ] 
self . backend_name = row [ 1 ] 
self . backend_version = row [ 2 ] 
self . category = row [ 3 ] 
self . backend_params = pickle . loads ( row [ 4 ] ) 
self . created_on = str_to_datetime ( row [ 5 ] ) 
~~ def _count_table_rows ( self , table_name ) : 
~~~ cursor . execute ( select_stmt ) 
~~~ cursor . close ( ) 
~~ return row [ 0 ] 
~~ def create_archive ( self ) : 
hashcode = uuid . uuid4 ( ) . hex 
archive_dir = os . path . join ( self . dirpath , hashcode [ 0 : 2 ] ) 
archive_name = hashcode [ 2 : ] + self . STORAGE_EXT 
archive_path = os . path . join ( archive_dir , archive_name ) 
if not os . path . exists ( archive_dir ) : 
~~~ os . makedirs ( archive_dir ) 
~~~ archive = Archive . create ( archive_path ) 
~~ except ArchiveError as e : 
~~~ raise ArchiveManagerError ( cause = str ( e ) ) 
~~ return archive 
~~ def remove_archive ( self , archive_path ) : 
~~~ Archive ( archive_path ) 
~~ os . remove ( archive_path ) 
~~ def search ( self , origin , backend_name , category , archived_after ) : 
archives = self . _search_archives ( origin , backend_name , 
category , archived_after ) 
archives = [ ( fp , date ) for fp , date in archives ] 
archives = [ fp for fp , _ in sorted ( archives , key = lambda x : x [ 1 ] ) ] 
return archives 
~~ def _search_archives ( self , origin , backend_name , category , archived_after ) : 
for archive_path in self . _search_files ( ) : 
~~~ archive = Archive ( archive_path ) 
~~ except ArchiveError : 
~~ match = archive . origin == origin and archive . backend_name == backend_name and archive . category == category and archive . created_on >= archived_after 
~~ yield archive_path , archive . created_on 
~~ ~~ def _search_files ( self ) : 
for root , _ , files in os . walk ( self . dirpath ) : 
~~~ for filename in files : 
yield location 
~~ ~~ ~~ def check_compressed_file_type ( filepath ) : 
def compressed_file_type ( content ) : 
~~~ magic_dict = { 
b'\\x1f\\x8b\\x08' : 'gz' , 
b'\\x42\\x5a\\x68' : 'bz2' , 
b'PK\\x03\\x04' : 'zip' 
for magic , filetype in magic_dict . items ( ) : 
~~~ if content . startswith ( magic ) : 
~~~ return filetype 
~~ with open ( filepath , mode = 'rb' ) as f : 
~~~ magic_number = f . read ( 4 ) 
~~ return compressed_file_type ( magic_number ) 
~~ def months_range ( from_date , to_date ) : 
start = datetime . datetime ( from_date . year , from_date . month , 1 ) 
end = datetime . datetime ( to_date . year , to_date . month , 1 ) 
month_gen = dateutil . rrule . rrule ( freq = dateutil . rrule . MONTHLY , 
dtstart = start , until = end ) 
months = [ d for d in month_gen ] 
pos = 0 
for x in range ( 1 , len ( months ) ) : 
~~~ yield months [ pos ] , months [ x ] 
pos = x 
~~ ~~ def message_to_dict ( msg ) : 
def parse_headers ( msg ) : 
~~~ headers = { } 
for header , value in msg . items ( ) : 
~~~ hv = [ ] 
for text , charset in email . header . decode_header ( value ) : 
~~~ if type ( text ) == bytes : 
~~~ charset = charset if charset else 'utf-8' 
~~~ text = text . decode ( charset , errors = 'surrogateescape' ) 
~~ except ( UnicodeError , LookupError ) : 
~~~ text = text . decode ( 'ascii' , errors = 'surrogateescape' ) 
~~ ~~ hv . append ( text ) 
headers [ header ] = v if v else None 
~~ return headers 
~~ def parse_payload ( msg ) : 
~~~ body = { } 
if not msg . is_multipart ( ) : 
~~~ payload = decode_payload ( msg ) 
subtype = msg . get_content_subtype ( ) 
body [ subtype ] = [ payload ] 
~~~ for part in email . iterators . typed_subpart_iterator ( msg ) : 
~~~ payload = decode_payload ( part ) 
subtype = part . get_content_subtype ( ) 
body . setdefault ( subtype , [ ] ) . append ( payload ) 
~~ ~~ return { k : '\\n' . join ( v ) for k , v in body . items ( ) } 
~~ def decode_payload ( msg_or_part ) : 
~~~ charset = msg_or_part . get_content_charset ( 'utf-8' ) 
payload = msg_or_part . get_payload ( decode = True ) 
~~~ payload = payload . decode ( charset , errors = 'surrogateescape' ) 
~~~ payload = payload . decode ( 'ascii' , errors = 'surrogateescape' ) 
~~ message = requests . structures . CaseInsensitiveDict ( ) 
if isinstance ( msg , mailbox . mboxMessage ) : 
~~~ message [ 'unixfrom' ] = msg . get_from ( ) 
~~~ message [ 'unixfrom' ] = None 
~~~ for k , v in parse_headers ( msg ) . items ( ) : 
~~~ message [ k ] = v 
~~ message [ 'body' ] = parse_payload ( msg ) 
~~ except UnicodeError as e : 
~~~ raise ParseError ( cause = str ( e ) ) 
~~ def remove_invalid_xml_chars ( raw_xml ) : 
illegal_unichrs = [ ( 0x00 , 0x08 ) , ( 0x0B , 0x1F ) , 
( 0x7F , 0x84 ) , ( 0x86 , 0x9F ) ] 
illegal_ranges = [ '%s-%s' % ( chr ( low ) , chr ( high ) ) 
for ( low , high ) in illegal_unichrs 
if low < sys . maxunicode ] 
illegal_xml_re = re . compile ( '[%s]' % '' . join ( illegal_ranges ) ) 
purged_xml = '' 
for c in raw_xml : 
~~~ if illegal_xml_re . search ( c ) is not None : 
~~ purged_xml += c 
~~ return purged_xml 
~~ def xml_to_dict ( raw_xml ) : 
def node_to_dict ( node ) : 
~~~ d = { } 
d . update ( node . items ( ) ) 
text = getattr ( node , 'text' , None ) 
if text is not None : 
~~~ d [ '__text__' ] = text 
~~ childs = { } 
for child in node : 
~~~ childs . setdefault ( child . tag , [ ] ) . append ( node_to_dict ( child ) ) 
~~ d . update ( childs . items ( ) ) 
return d 
~~ purged_xml = remove_invalid_xml_chars ( raw_xml ) 
~~~ tree = xml . etree . ElementTree . fromstring ( purged_xml ) 
~~ except xml . etree . ElementTree . ParseError as e : 
~~ d = node_to_dict ( tree ) 
for issue_id in self . __fetch_issues_ids ( from_date ) : 
~~~ issue = self . __fetch_and_parse_issue ( issue_id ) 
for key in USER_FIELDS : 
~~~ if key not in issue : 
~~ user = self . __get_or_fetch_user ( issue [ key ] [ 'id' ] ) 
issue [ key + '_data' ] = user 
~~ for journal in issue [ 'journals' ] : 
~~~ if 'user' not in journal : 
~~ user = self . __get_or_fetch_user ( journal [ 'user' ] [ 'id' ] ) 
journal [ 'user_data' ] = user 
~~ yield issue 
~~ def parse_issues ( raw_json ) : 
results = json . loads ( raw_json ) 
issues = results [ 'issues' ] 
~~ ~~ def _init_client ( self , from_archive = False ) : 
return RedmineClient ( self . url , self . api_token , self . archive , from_archive ) 
~~ def issues ( self , from_date = DEFAULT_DATETIME , 
offset = None , max_issues = MAX_ISSUES ) : 
resource = self . RISSUES + self . CJSON 
ts = datetime_to_utc ( from_date ) 
ts = ts . strftime ( "%Y-%m-%dT%H:%M:%SZ" ) 
self . PSTATUS_ID : '*' , 
self . PSORT : self . PUPDATED_ON , 
self . PUPDATED_ON : '>=' + ts , 
self . PLIMIT : max_issues 
if offset is not None : 
~~ response = self . _call ( resource , params ) 
resource = urijoin ( self . RISSUES , str ( issue_id ) + self . CJSON ) 
self . PINCLUDE : ',' . join ( [ self . CATTACHMENTS , self . CCHANGESETS , 
self . CCHILDREN , self . CJOURNALS , 
self . CRELATIONS , self . CWATCHERS ] ) 
response = self . _call ( resource , params ) 
resource = urijoin ( self . RUSERS , str ( user_id ) + self . CJSON ) 
params = { } 
if RedmineClient . PKEY in payload : 
~~~ payload . pop ( RedmineClient . PKEY ) 
~~ def _call ( self , resource , params ) : 
~~~ params [ self . PKEY ] = self . api_token 
r = self . fetch ( url , payload = params , verify = False ) 
mailing_list = HyperKittyList ( self . url , self . dirpath ) 
mailing_list . fetch ( from_date = from_date ) 
self . client . base_url , str ( from_date ) ) 
self . client . fetch ( self . client . base_url ) 
to_end = datetime_utcnow ( ) 
to_end += dateutil . relativedelta . relativedelta ( months = 1 ) 
months = months_range ( from_date , to_end ) 
~~ tmbox = 0 
for dts in months : 
~~~ tmbox += 1 
start , end = dts [ 0 ] , dts [ 1 ] 
filename = start . strftime ( "%Y-%m.mbox.gz" ) 
filepath = os . path . join ( self . dirpath , filename ) 
url = urijoin ( self . client . base_url , 'export' , filename ) 
'start' : start . strftime ( "%Y-%m-%d" ) , 
'end' : end . strftime ( "%Y-%m-%d" ) 
success = self . _download_archive ( url , params , filepath ) 
~~~ fetched . append ( ( url , filepath ) ) 
~~ def fetch ( self , category = CATEGORY_DOCKERHUB_DATA ) : 
self . repository , self . owner ) 
raw_data = self . client . repository ( self . owner , self . repository ) 
data = self . parse_json ( raw_data ) 
data [ 'fetched_on' ] = fetched_on 
yield data 
return DockerHubClient ( archive = self . archive , from_archive = from_archive ) 
~~ def repository ( self , owner , repository ) : 
url = urijoin ( self . base_url , self . RREPOSITORY , owner , repository ) 
response = self . fetch ( url ) 
~~ def map_custom_field ( custom_fields , fields ) : 
def build_cf ( cf , v ) : 
~~~ return { 'id' : cf [ 'id' ] , 'name' : cf [ 'name' ] , 'value' : v } 
~~ return { 
k : build_cf ( custom_fields [ k ] , v ) 
for k , v in fields . items ( ) 
if k in custom_fields 
~~ def filter_custom_fields ( fields ) : 
custom_fields = { } 
sorted_fields = [ field for field in fields if field [ 'custom' ] is True ] 
for custom_field in sorted_fields : 
~~~ custom_fields [ custom_field [ 'id' ] ] = custom_field 
~~ return custom_fields 
self . url , self . project , str ( from_date ) ) 
whole_pages = self . client . get_issues ( from_date ) 
fields = json . loads ( self . client . get_fields ( ) ) 
custom_fields = filter_custom_fields ( fields ) 
for whole_page in whole_pages : 
~~~ issues = self . parse_issues ( whole_page ) 
~~~ mapping = map_custom_field ( custom_fields , issue [ 'fields' ] ) 
for k , v in mapping . items ( ) : 
~~~ issue [ 'fields' ] [ k ] = v 
~~ comments_data = self . __get_issue_comments ( issue [ 'id' ] ) 
issue [ 'comments_data' ] = comments_data 
~~ ~~ ~~ def parse_issues ( raw_page ) : 
raw_issues = json . loads ( raw_page ) 
issues = raw_issues [ 'issues' ] 
return JiraClient ( self . url , self . project , self . user , self . password , 
self . verify , self . cert , self . max_results , 
~~ def __get_issue_comments ( self , issue_id ) : 
page_comments = self . client . get_comments ( issue_id ) 
for page_comment in page_comments : 
~~~ raw_comments = json . loads ( page_comment ) 
comments . extend ( raw_comments [ 'comments' ] ) 
~~ def get_items ( self , from_date , url , expand_fields = True ) : 
start_at = 0 
req = self . fetch ( url , payload = self . __build_payload ( start_at , from_date , expand_fields ) ) 
issues = req . text 
data = req . json ( ) 
titems = data [ 'total' ] 
nitems = data [ 'maxResults' ] 
start_at += min ( nitems , titems ) 
self . __log_status ( start_at , titems , url ) 
while issues : 
~~~ yield issues 
issues = None 
if data [ 'startAt' ] + nitems < titems : 
~~~ req = self . fetch ( url , payload = self . __build_payload ( start_at , from_date , expand_fields ) ) 
start_at += nitems 
~~ ~~ ~~ def get_issues ( self , from_date ) : 
url = urijoin ( self . base_url , self . RESOURCE , self . VERSION_API , 'search' ) 
issues = self . get_items ( from_date , url ) 
return issues 
~~ def get_comments ( self , issue_id ) : 
url = urijoin ( self . base_url , self . RESOURCE , self . VERSION_API , self . ISSUE , issue_id , self . COMMENT ) 
comments = self . get_items ( DEFAULT_DATETIME , url , expand_fields = False ) 
return comments 
~~ def get_fields ( self ) : 
url = urijoin ( self . base_url , self . RESOURCE , self . VERSION_API , 'field' ) 
req = self . fetch ( url ) 
~~ def fetch ( self , category = CATEGORY_BUILD ) : 
projects = json . loads ( self . client . get_jobs ( ) ) 
jobs = projects [ 'jobs' ] 
for job in jobs : 
job [ 'url' ] , njobs , len ( jobs ) ) 
~~~ raw_builds = self . client . get_builds ( job [ 'name' ] ) 
~~~ if e . response . status_code == 500 : 
~~~ logger . warning ( e ) 
job [ 'url' ] ) 
~~ ~~ if not raw_builds : 
~~~ builds = json . loads ( raw_builds ) 
~~ builds = builds [ 'builds' ] 
for build in builds : 
~~~ yield build 
nbuilds += 1 
~~ njobs += 1 
return JenkinsClient ( self . url , self . blacklist_jobs , self . detail_depth , 
self . sleep_time , 
~~ def get_jobs ( self ) : 
url_jenkins = urijoin ( self . base_url , "api" , "json" ) 
response = self . fetch ( url_jenkins ) 
~~ def get_builds ( self , job_name ) : 
if self . blacklist_jobs and job_name in self . blacklist_jobs : 
~~ payload = { 'depth' : self . detail_depth } 
url_build = urijoin ( self . base_url , "job" , job_name , "api" , "json" ) 
response = self . fetch ( url_build , payload = payload ) 
self . site , self . tagged , str ( from_date ) ) 
whole_pages = self . client . get_questions ( from_date ) 
~~~ questions = self . parse_questions ( whole_page ) 
for question in questions : 
~~~ yield question 
~~ ~~ ~~ def parse_questions ( raw_page ) : 
raw_questions = json . loads ( raw_page ) 
questions = raw_questions [ 'items' ] 
return StackExchangeClient ( self . site , self . tagged , self . api_token , self . max_questions , 
~~ def get_questions ( self , from_date ) : 
page = 1 
url = urijoin ( self . base_url , self . VERSION_API , "questions" ) 
req = self . fetch ( url , payload = self . __build_payload ( page , from_date ) ) 
questions = req . text 
tquestions = data [ 'total' ] 
nquestions = data [ 'page_size' ] 
self . __log_status ( data [ 'quota_remaining' ] , 
data [ 'quota_max' ] , 
nquestions , 
tquestions ) 
while questions : 
~~~ yield questions 
questions = None 
if data [ 'has_more' ] : 
backoff = data . get ( 'backoff' , None ) 
if backoff : 
backoff ) 
time . sleep ( float ( backoff ) ) 
~~ req = self . fetch ( url , payload = self . __build_payload ( page , from_date ) ) 
nquestions += data [ 'page_size' ] 
if 'key' in payload : 
~~~ payload . pop ( 'key' ) 
group . add_argument ( '--site' , dest = 'site' , 
required = True , 
group . add_argument ( '--tagged' , dest = 'tagged' , 
group . add_argument ( '--max-questions' , dest = 'max_questions' , 
type = int , default = MAX_QUESTIONS , 
~~ def fetch ( self , category = CATEGORY_PAGE , from_date = DEFAULT_DATETIME , reviews_api = False ) : 
if from_date == DEFAULT_DATETIME : 
~~~ from_date = None 
~~~ from_date = datetime_to_utc ( from_date ) 
~~ kwargs = { "from_date" : from_date , "reviews_api" : reviews_api } 
reviews_api = kwargs [ 'reviews_api' ] 
mediawiki_version = self . client . get_version ( ) 
if reviews_api : 
~~~ if ( ( mediawiki_version [ 0 ] == 1 and mediawiki_version [ 1 ] >= 27 ) or mediawiki_version [ 0 ] > 1 ) : 
~~~ fetcher = self . __fetch_1_27 ( from_date ) 
fetcher = self . __fetch_pre1_27 ( from_date ) 
~~~ fetcher = self . __fetch_pre1_27 ( from_date ) 
~~ for page_reviews in fetcher : 
~~~ yield page_reviews 
return MediaWikiClient ( self . url , self . archive , from_archive ) 
~~ def __get_max_date ( self , reviews ) : 
max_ts = 0 
for review in reviews : 
~~~ ts = str_to_datetime ( review [ 'timestamp' ] ) 
ts = datetime_to_utc ( ts ) 
if ts . timestamp ( ) > max_ts : 
~~~ max_ts = ts . timestamp ( ) 
~~ ~~ return max_ts 
~~ def __fetch_1_27 ( self , from_date = None ) : 
namespaces_contents = self . __get_namespaces_contents ( ) 
while arvcontinue is not None : 
~~~ raw_pages = self . client . get_pages_from_allrevisions ( namespaces_contents , from_date , arvcontinue ) 
data_json = json . loads ( raw_pages ) 
arvcontinue = data_json [ 'continue' ] [ 'arvcontinue' ] if 'continue' in data_json else None 
pages_json = data_json [ 'query' ] [ 'allrevisions' ] 
for page in pages_json : 
~~~ if page [ 'pageid' ] in pages_done : 
~~ tpages += 1 
pages_done . append ( page [ 'pageid' ] ) 
page_reviews = self . __get_page_reviews ( page ) 
if not page_reviews : 
page [ 'title' ] , page [ 'pageid' ] ) 
~~ yield page_reviews 
npages += 1 
~~ def __fetch_pre1_27 ( self , from_date = None ) : 
def fetch_incremental_changes ( namespaces_contents ) : 
rccontinue = '' 
while rccontinue is not None : 
~~~ raw_pages = self . client . get_recent_pages ( namespaces_contents , rccontinue ) 
if 'query-continue' in data_json : 
~~~ rccontinue = data_json [ 'query-continue' ] [ 'recentchanges' ] [ 'rccontinue' ] 
~~ elif 'continue' in data_json : 
~~~ rccontinue = data_json [ 'continue' ] [ 'rccontinue' ] 
~~~ rccontinue = None 
~~ pages_json = data_json [ 'query' ] [ 'recentchanges' ] 
~~~ page_ts = dateutil . parser . parse ( page [ 'timestamp' ] ) 
if from_date >= page_ts : 
rccontinue = None 
hole_created = False 
~~ if page [ 'pageid' ] in pages_done : 
~~ ~~ if hole_created : 
~~ def fetch_all_pages ( namespaces_contents ) : 
for ns in namespaces_contents : 
while apcontinue is not None : 
~~~ raw_pages = self . client . get_pages ( ns , apcontinue ) 
~~~ apcontinue = data_json [ 'query-continue' ] [ 'allpages' ] [ 'apcontinue' ] 
~~~ apcontinue = data_json [ 'continue' ] [ 'apcontinue' ] 
~~~ apcontinue = None 
~~ pages_json = data_json [ 'query' ] [ 'allpages' ] 
~~~ if ( datetime_utcnow ( ) - from_date ) . days >= MAX_RECENT_DAYS : 
~~~ cause = "Can\ % MAX_RECENT_DAYS 
~~ ~~ namespaces_contents = self . __get_namespaces_contents ( ) 
~~~ return fetch_all_pages ( namespaces_contents ) 
~~~ return fetch_incremental_changes ( namespaces_contents ) 
~~ ~~ def call ( self , params ) : 
self . base_url , str ( params ) ) 
req = self . fetch ( self . base_url , payload = params ) 
~~ def get_pages ( self , namespace , apcontinue = '' ) : 
"action" : "query" , 
"list" : "allpages" , 
"aplimit" : self . limit , 
"apnamespace" : namespace , 
"format" : "json" 
if apcontinue : 
~~~ params [ 'apcontinue' ] = apcontinue 
~~ return self . call ( params ) 
~~ def get_recent_pages ( self , namespaces , rccontinue = '' ) : 
namespaces . sort ( ) 
"list" : "recentchanges" , 
"rclimit" : self . limit , 
"rcnamespace" : "|" . join ( namespaces ) , 
"rcprop" : "title|timestamp|ids" , 
if rccontinue : 
~~~ params [ 'rccontinue' ] = rccontinue 
~~ def fetch ( self , category = CATEGORY_MESSAGE , offset = DEFAULT_OFFSET , chats = None ) : 
if not offset : 
~~~ offset = DEFAULT_OFFSET 
~~ kwargs = { "offset" : offset , "chats" : chats } 
offset = kwargs [ 'offset' ] 
chats = kwargs [ 'chats' ] 
self . bot , offset ) 
if chats is not None : 
~~~ if len ( chats ) == 0 : 
'[' + ',' . join ( str ( ch_id ) for ch_id in chats ) + ']' ) 
~~ ~~ nmsgs = 0 
~~~ raw_json = self . client . updates ( offset = offset ) 
messages = [ msg for msg in self . parse_messages ( raw_json ) ] 
if len ( messages ) == 0 : 
~~ for msg in messages : 
~~~ offset = max ( msg [ 'update_id' ] , offset ) 
if not self . _filter_message_by_chats ( msg , chats ) : 
msg [ 'message' ] [ 'message_id' ] ) 
~~ yield msg 
~~ offset += 1 
nmsgs ) 
~~ def parse_messages ( raw_json ) : 
result = json . loads ( raw_json ) 
messages = result [ 'result' ] 
for msg in messages : 
~~~ yield msg 
return TelegramBotClient ( self . bot_token , self . archive , from_archive ) 
~~ def _filter_message_by_chats ( self , message , chats ) : 
if chats is None : 
~~ chat_id = message [ 'message' ] [ 'chat' ] [ 'id' ] 
return chat_id in chats 
~~ def updates ( self , offset = None ) : 
~~~ params [ self . OFFSET ] = offset 
~~ response = self . _call ( self . UPDATES_METHOD , params ) 
url = re . sub ( 'bot.*/' , 'botXXXXX/' , url ) 
return url , headers , payload 
~~ def _call ( self , method , params ) : 
url = self . base_url % { 'token' : self . bot_token , 'method' : method } 
method , str ( params ) ) 
self . group , self . host , str ( offset ) ) 
narts , iarts , tarts = ( 0 , 0 , 0 ) 
_ , _ , first , last , _ = self . client . group ( self . group ) 
if offset <= last : 
~~~ first = max ( first , offset ) 
_ , overview = self . client . over ( ( first , last ) ) 
~~~ overview = [ ] 
~~ tarts = len ( overview ) 
for article_id , _ in overview : 
~~~ article_raw = self . client . article ( article_id ) 
article = self . __parse_article ( article_raw ) 
~~ except ParseError : 
article_id ) 
iarts += 1 
~~ except nntplib . NNTPTemporaryError as e : 
e . response , article_id ) 
~~ yield article 
narts += 1 
~~ ~~ def metadata ( self , item , filter_classified = False ) : 
item = super ( ) . metadata ( item , filter_classified = filter_classified ) 
item [ 'offset' ] = item [ 'data' ] [ 'offset' ] 
~~ def parse_article ( raw_article ) : 
~~~ message = email . message_from_string ( raw_article ) 
article = message_to_dict ( message ) 
~~ except UnicodeEncodeError as e : 
~~ return article 
return NNTTPClient ( self . host , self . archive , from_archive ) 
~~ def _fetch ( self , method , args ) : 
~~~ data = self . _fetch_from_archive ( method , args ) 
~~~ data = self . _fetch_from_remote ( method , args ) 
~~ def _fetch_article ( self , article_id ) : 
fetched_data = self . handler . article ( article_id ) 
'number' : fetched_data [ 1 ] . number , 
'message_id' : fetched_data [ 1 ] . message_id , 
'lines' : fetched_data [ 1 ] . lines 
~~ def _fetch_from_remote ( self , method , args ) : 
~~~ if method == NNTTPClient . GROUP : 
~~~ data = self . handler . group ( args ) 
~~ elif method == NNTTPClient . OVER : 
~~~ data = self . handler . over ( args ) 
~~ elif method == NNTTPClient . ARTICLE : 
~~~ data = self . _fetch_article ( args ) 
~~ ~~ except nntplib . NNTPTemporaryError as e : 
~~~ data = e 
~~~ if self . archive : 
~~~ self . archive . store ( method , args , None , data ) 
~~ def _fetch_from_archive ( self , method , args ) : 
~~ data = self . archive . retrieve ( method , args , None ) 
if isinstance ( data , nntplib . NNTPTemporaryError ) : 
~~~ raise data 
~~ def fetch ( self , url , payload = None , headers = None , method = GET , stream = False , verify = True ) : 
~~~ response = self . _fetch_from_archive ( url , payload , headers ) 
~~~ response = self . _fetch_from_remote ( url , payload , headers , method , stream , verify ) 
~~ def _create_http_session ( self ) : 
self . session = requests . Session ( ) 
if self . headers : 
~~~ self . session . headers . update ( self . headers ) 
~~ retries = urllib3 . util . Retry ( total = self . max_retries , 
connect = self . max_retries_on_connect , 
read = self . max_retries_on_read , 
redirect = self . max_retries_on_redirect , 
status = self . max_retries_on_status , 
method_whitelist = self . method_whitelist , 
status_forcelist = self . status_forcelist , 
backoff_factor = self . sleep_time , 
raise_on_redirect = self . raise_on_redirect , 
raise_on_status = self . raise_on_status , 
respect_retry_after_header = self . respect_retry_after_header ) 
self . session . mount ( 'http://' , requests . adapters . HTTPAdapter ( max_retries = retries ) ) 
self . session . mount ( 'https://' , requests . adapters . HTTPAdapter ( max_retries = retries ) ) 
~~ def setup_rate_limit_handler ( self , sleep_for_rate = False , min_rate_to_sleep = MIN_RATE_LIMIT , 
rate_limit_header = RATE_LIMIT_HEADER , 
rate_limit_reset_header = RATE_LIMIT_RESET_HEADER ) : 
self . rate_limit = None 
self . rate_limit_reset_ts = None 
self . sleep_for_rate = sleep_for_rate 
self . rate_limit_header = rate_limit_header 
self . rate_limit_reset_header = rate_limit_reset_header 
if min_rate_to_sleep > self . MAX_RATE_LIMIT : 
self . min_rate_to_sleep = self . MAX_RATE_LIMIT 
logger . warning ( msg , min_rate_to_sleep , self . MAX_RATE_LIMIT ) 
~~~ self . min_rate_to_sleep = min_rate_to_sleep 
~~ ~~ def sleep_for_rate_limit ( self ) : 
if self . rate_limit is not None and self . rate_limit <= self . min_rate_to_sleep : 
~~~ seconds_to_reset = self . calculate_time_to_reset ( ) 
if seconds_to_reset < 0 : 
seconds_to_reset = 0 
if self . sleep_for_rate : 
time . sleep ( seconds_to_reset ) 
~~~ raise RateLimitError ( cause = cause , seconds_to_reset = seconds_to_reset ) 
~~ ~~ ~~ def update_rate_limit ( self , response ) : 
if self . rate_limit_header in response . headers : 
~~~ self . rate_limit = int ( response . headers [ self . rate_limit_header ] ) 
~~~ self . rate_limit = None 
~~ if self . rate_limit_reset_header in response . headers : 
~~~ self . rate_limit_reset_ts = int ( response . headers [ self . rate_limit_reset_header ] ) 
~~~ self . rate_limit_reset_ts = None 
nmessages = 0 
archives = self . __retrieve_archives ( from_date ) 
for archive in archives : 
for message in self . parse_supybot_log ( archive ) : 
~~~ dt = str_to_datetime ( message [ 'timestamp' ] ) 
str ( dt ) , str ( from_date ) ) 
~~ yield message 
nmessages += 1 
nmessages ) 
~~ def parse_supybot_log ( filepath ) : 
~~~ parser = SupybotParser ( f ) 
~~~ for message in parser . parse ( ) : 
~~ ~~ except ParseError as e : 
~~ ~~ ~~ def __retrieve_archives ( self , from_date ) : 
candidates = self . __list_supybot_archives ( ) 
for candidate in candidates : 
~~~ dt = self . __parse_date_from_filepath ( candidate ) 
if dt . date ( ) >= from_date . date ( ) : 
~~~ archives . append ( ( dt , candidate ) ) 
candidate , str ( from_date ) ) 
~~ ~~ archives . sort ( key = lambda x : x [ 0 ] ) 
return [ archive [ 1 ] for archive in archives ] 
~~ def __list_supybot_archives ( self ) : 
archives . append ( location ) 
~~ ~~ return archives 
if self . SUPYBOT_EMPTY_REGEX . match ( line ) : 
~~ ts , msg = self . _parse_supybot_timestamp ( line ) 
if self . SUPYBOT_EMPTY_COMMENT_REGEX . match ( msg ) : 
~~ elif self . SUPYBOT_EMPTY_COMMENT_ACTION_REGEX . match ( msg ) : 
~~ elif self . SUPYBOT_EMPTY_BOT_REGEX . match ( msg ) : 
~~ itype , nick , body = self . _parse_supybot_msg ( msg ) 
item = self . _build_item ( ts , itype , nick , body ) 
yield item 
~~ ~~ def _parse_supybot_timestamp ( self , line ) : 
m = self . SUPYBOT_TIMESTAMP_REGEX . match ( line ) 
raise ParseError ( cause = msg ) 
~~ ts = m . group ( 'ts' ) 
msg = m . group ( 'msg' ) 
return ts , msg 
~~ def _parse_supybot_msg ( self , line ) : 
patterns = [ ( self . SUPYBOT_COMMENT_REGEX , self . TCOMMENT ) , 
( self . SUPYBOT_COMMENT_ACTION_REGEX , self . TCOMMENT ) , 
( self . SUPYBOT_SERVER_REGEX , self . TSERVER ) , 
( self . SUPYBOT_BOT_REGEX , self . TCOMMENT ) ] 
for p in patterns : 
~~~ m = p [ 0 ] . match ( line ) 
~~ return p [ 1 ] , m . group ( 'nick' ) , m . group ( 'body' ) . strip ( ) 
ntopics = 0 
topics_ids = self . __fetch_and_parse_topics_ids ( from_date ) 
for topic_id in topics_ids : 
~~~ topic = self . __fetch_and_parse_topic ( topic_id ) 
ntopics += 1 
yield topic 
ntopics ) 
~~ def __parse_topics_page ( self , raw_json ) : 
topics_page = json . loads ( raw_json ) 
topics_ids = [ ] 
for topic in topics_page [ 'topic_list' ] [ 'topics' ] : 
~~~ topic_id = topic [ 'id' ] 
if topic [ 'last_posted_at' ] is None : 
~~ updated_at = str_to_datetime ( topic [ 'last_posted_at' ] ) 
pinned = topic [ 'pinned' ] 
topics_ids . append ( ( topic_id , updated_at , pinned ) ) 
~~ return topics_ids 
~~ def topics_page ( self , page = None ) : 
self . PPAGE : page 
response = self . _call ( self . ALL_TOPICS , self . TOPICS_SUMMARY , 
~~ def topic ( self , topic_id ) : 
self . PKEY : self . api_key 
response = self . _call ( self . TOPIC , topic_id , 
~~ def post ( self , post_id ) : 
response = self . _call ( self . POSTS , post_id , 
if DiscourseClient . PKEY in payload : 
~~~ payload . pop ( DiscourseClient . PKEY ) 
~~ def _call ( self , res , res_id , params ) : 
if res : 
~~~ url = urijoin ( self . base_url , res , res_id ) 
~~~ url = urijoin ( self . base_url , res_id ) 
~~ url += self . TJSON 
res , res_id , str ( params ) ) 
ntasks = 0 
for task in self . __fetch_tasks ( from_date ) : 
~~~ yield task 
ntasks += 1 
~~ def parse_tasks ( raw_json ) : 
tasks = results [ 'result' ] [ 'data' ] 
for t in tasks : 
~~~ yield t 
~~ ~~ def parse_users ( raw_json ) : 
users = results [ 'result' ] 
for u in users : 
~~~ yield u 
return ConduitClient ( self . url , self . api_token , 
self . max_retries , self . sleep_time , 
~~ def tasks ( self , from_date = DEFAULT_DATETIME ) : 
ts = int ( datetime_to_utc ( from_date ) . timestamp ( ) ) or 1 
consts = { 
self . PMODIFIED_START : ts 
attachments = { 
self . PPROJECTS : True 
self . PCONSTRAINTS : consts , 
self . PATTACHMENTS : attachments , 
self . PORDER : self . VOUTDATED , 
~~~ r = self . _call ( self . MANIPHEST_TASKS , params ) 
yield r 
j = json . loads ( r ) 
after = j [ 'result' ] [ 'cursor' ] [ 'after' ] 
if not after : 
~~ params [ self . PAFTER ] = after 
~~ ~~ def transactions ( self , * phids ) : 
self . PIDS : phids 
response = self . _call ( self . MANIPHEST_TRANSACTIONS , params ) 
~~ def users ( self , * phids ) : 
self . PHIDS : phids 
response = self . _call ( self . PHAB_USERS , params ) 
~~ def phids ( self , * phids ) : 
response = self . _call ( self . PHAB_PHIDS , params ) 
if '__conduit__' in payload [ 'params' ] : 
~~~ params = json . loads ( payload [ 'params' ] ) 
params . pop ( '__conduit__' ) 
payload [ 'params' ] = json . dumps ( params , sort_keys = True ) 
url = self . URL % { 'base' : self . base_url , 'method' : method } 
params [ '__conduit__' ] = { 'token' : self . api_token } 
'params' : json . dumps ( params , sort_keys = True ) , 
'output' : 'json' , 
'__conduit__' : True 
method , str ( data ) ) 
r = self . fetch ( url , payload = data , method = HttpClient . POST , verify = False ) 
if result [ 'error_code' ] : 
~~~ raise ConduitError ( error = result [ 'error_info' ] , 
code = result [ 'error_code' ] ) 
nhcs = 0 
contents = self . __fetch_contents_summary ( from_date ) 
contents = [ content for content in contents ] 
for content in contents : 
~~~ cid = content [ 'id' ] 
content_url = urijoin ( self . origin , content [ '_links' ] [ 'webui' ] ) 
hcs = self . __fetch_historical_contents ( cid , from_date ) 
for hc in hcs : 
~~~ hc [ 'content_url' ] = content_url 
hc [ 'ancestors' ] = content . get ( 'ancestors' , [ ] ) 
yield hc 
nhcs += 1 
nhcs ) 
cid = item [ 'id' ] 
cversion = item [ 'version' ] [ 'number' ] 
return str ( cid ) + '#v' + str ( cversion ) 
~~ def parse_contents_summary ( raw_json ) : 
summary = json . loads ( raw_json ) 
contents = summary [ 'results' ] 
for c in contents : 
~~~ yield c 
return ConfluenceClient ( self . url , archive = self . archive , from_archive = from_archive ) 
~~ def contents ( self , from_date = DEFAULT_DATETIME , 
offset = None , max_contents = MAX_CONTENTS ) : 
resource = self . RCONTENTS + '/' + self . MSEARCH 
cql = self . VCQL % { 'date' : date } 
self . PCQL : cql , 
self . PLIMIT : max_contents , 
self . PEXPAND : self . PANCESTORS 
~~~ params [ self . PSTART ] = offset 
~~ for response in self . _call ( resource , params ) : 
~~~ yield response 
~~ ~~ def historical_content ( self , content_id , version ) : 
resource = self . RCONTENTS + '/' + str ( content_id ) 
self . PVERSION : version , 
self . PSTATUS : self . VHISTORICAL , 
self . PEXPAND : ',' . join ( self . VEXPAND ) 
response = [ response for response in self . _call ( resource , params ) ] 
return response [ 0 ] 
~~~ r = self . fetch ( url , payload = params ) 
yield r . text 
j = r . json ( ) 
if '_links' not in j : 
~~ if 'next' not in j [ '_links' ] : 
~~ url = urijoin ( self . base_url , j [ '_links' ] [ 'next' ] ) 
~~ ~~ def _parse_result ( self ) : 
if self . result is not None : 
~~~ uom = testXMLAttribute ( self . result , "uom" ) 
value_str = testXMLValue ( self . result ) 
~~~ value = float ( value_str ) 
~~ self . result = Measurement ( value , uom ) 
~~ ~~ def capabilities_url ( self , service_url ) : 
qs = [ ] 
if service_url . find ( '?' ) != - 1 : 
~~~ qs = cgi . parse_qsl ( service_url . split ( '?' ) [ 1 ] ) 
~~ params = [ x [ 0 ] for x in qs ] 
if 'service' not in params : 
~~~ qs . append ( ( 'service' , 'WFS' ) ) 
~~ if 'request' not in params : 
~~~ qs . append ( ( 'request' , 'GetCapabilities' ) ) 
~~ if 'version' not in params : 
~~~ qs . append ( ( 'version' , self . version ) ) 
~~ urlqs = urlencode ( tuple ( qs ) ) 
return service_url . split ( '?' ) [ 0 ] + '?' + urlqs 
~~ def read ( self , url , timeout = 30 ) : 
request = self . capabilities_url ( url ) 
u = openURL ( request , timeout = timeout , 
username = self . username , password = self . password ) 
return etree . fromstring ( u . read ( ) ) 
~~ def readString ( self , st ) : 
if not isinstance ( st , str ) and not isinstance ( st , bytes ) : 
~~ return etree . fromstring ( st ) 
~~ def _parse_result ( self ) : 
~~~ result = self . result . find ( nspv ( 
"wml2:MeasurementTimeseries" ) ) 
self . result = MeasurementTimeseries ( result ) 
~~ ~~ def conformance ( self ) : 
url = self . _build_url ( 'conformance' ) 
response = requests . get ( url , headers = REQUEST_HEADERS ) . json ( ) 
~~ def collection ( self , collection_name ) : 
path = 'collections/{}' . format ( collection_name ) 
url = self . _build_url ( path ) 
~~ def collection_items ( self , collection_name , ** kwargs ) : 
if 'bbox' in kwargs : 
~~~ kwargs [ 'bbox' ] = ',' . join ( kwargs [ 'bbox' ] ) 
~~ path = 'collections/{}/items' . format ( collection_name ) 
response = requests . get ( url , headers = REQUEST_HEADERS , 
params = kwargs ) . json ( ) 
~~ def _build_url ( self , path = None ) : 
url = self . url 
if self . url_query_string is not None : 
url = urljoin ( url , path ) 
url = '?' . join ( [ url , self . url_query_string ] ) 
~~~ url = urljoin ( url , path ) 
return url 
~~ def get_schema ( url , typename , version = '1.0.0' , timeout = 30 , username = None , password = None ) : 
url = _get_describefeaturetype_url ( url , version , typename ) 
res = openURL ( url , timeout = timeout , username = username , password = password ) 
root = etree . fromstring ( res . read ( ) ) 
if ':' in typename : 
~~~ typename = typename . split ( ':' ) [ 1 ] 
~~ type_element = findall ( root , '{%s}element' % XS_NAMESPACE , 
attribute_name = 'name' , attribute_value = typename ) [ 0 ] 
complex_type = type_element . attrib [ 'type' ] . split ( ":" ) [ 1 ] 
elements = _get_elements ( complex_type , root ) 
nsmap = None 
if hasattr ( root , 'nsmap' ) : 
~~~ nsmap = root . nsmap 
~~ return _construct_schema ( elements , nsmap ) 
~~ def _get_elements ( complex_type , root ) : 
found_elements = [ ] 
element = findall ( root , '{%s}complexType' % XS_NAMESPACE , 
attribute_name = 'name' , attribute_value = complex_type ) [ 0 ] 
found_elements = findall ( element , '{%s}element' % XS_NAMESPACE ) 
return found_elements 
~~ def _construct_schema ( elements , nsmap ) : 
schema = { 
'properties' : { } , 
'geometry' : None 
schema_key = None 
gml_key = None 
if nsmap : 
~~~ for key in nsmap : 
~~~ if nsmap [ key ] == XS_NAMESPACE : 
~~~ schema_key = key 
~~ if nsmap [ key ] in GML_NAMESPACES : 
~~~ gml_key = key 
~~~ gml_key = 'gml' 
schema_key = 'xsd' 
~~ mappings = { 
'PointPropertyType' : 'Point' , 
'PolygonPropertyType' : 'Polygon' , 
'LineStringPropertyType' : 'LineString' , 
'MultiPointPropertyType' : 'MultiPoint' , 
'MultiLineStringPropertyType' : 'MultiLineString' , 
'MultiPolygonPropertyType' : 'MultiPolygon' , 
'MultiGeometryPropertyType' : 'MultiGeometry' , 
'GeometryPropertyType' : 'GeometryCollection' , 
for element in elements : 
~~~ data_type = element . attrib [ 'type' ] . replace ( gml_key + ':' , '' ) 
name = element . attrib [ 'name' ] 
if data_type in mappings : 
~~~ schema [ 'geometry' ] = mappings [ data_type ] 
schema [ 'geometry_column' ] = name 
~~~ schema [ 'properties' ] [ name ] = data_type . replace ( schema_key + ':' , '' ) 
~~ ~~ if schema [ 'properties' ] or schema [ 'geometry' ] : 
~~~ return schema 
~~ ~~ def _get_describefeaturetype_url ( url , version , typename ) : 
query_string = [ ] 
if url . find ( '?' ) != - 1 : 
~~~ query_string = cgi . parse_qsl ( url . split ( '?' ) [ 1 ] ) 
~~ params = [ x [ 0 ] for x in query_string ] 
~~~ query_string . append ( ( 'service' , 'WFS' ) ) 
~~~ query_string . append ( ( 'request' , 'DescribeFeatureType' ) ) 
~~~ query_string . append ( ( 'version' , version ) ) 
~~ query_string . append ( ( 'typeName' , typename ) ) 
urlqs = urlencode ( tuple ( query_string ) ) 
return url . split ( '?' ) [ 0 ] + '?' + urlqs 
~~ def complex_input_with_reference ( ) : 
wps = WebProcessingService ( 'http://localhost:8094/wps' , verbose = verbose ) 
processid = 'wordcount' 
inputs = [ ( "text" , textdoc ) ] 
outputs = [ ( "output" , True , 'some/mime-type' ) ] 
execution = wps . execute ( processid , inputs , output = outputs ) 
monitorExecution ( execution ) 
for output in execution . processOutputs : 
~~ ~~ def get_descriptors_from_module ( mdl , submodule = False ) : 
__all__ = getattr ( mdl , "__all__" , None ) 
if __all__ is None : 
~~~ __all__ = dir ( mdl ) 
~~ all_functions = ( getattr ( mdl , name ) for name in __all__ if name [ : 1 ] != "_" ) 
if submodule : 
~~~ descs = [ 
d 
for fn in all_functions 
if is_descriptor_class ( fn ) or isinstance ( fn , ModuleType ) 
for d in ( 
[ fn ] if is_descriptor_class ( fn ) 
else get_descriptors_from_module ( fn , submodule = True ) 
fn 
if is_descriptor_class ( fn ) 
~~ return descs 
~~ def get_descriptors_in_module ( mdl , submodule = True ) : 
~~ all_values = ( getattr ( mdl , name ) for name in __all__ if name [ : 1 ] != "_" ) 
~~~ for v in all_values : 
~~~ if is_descriptor_class ( v ) : 
~~~ yield v 
~~ if isinstance ( v , ModuleType ) : 
~~~ for v in get_descriptors_in_module ( v , submodule = True ) : 
~~ ~~ ~~ ~~ def register_json ( self , obj ) : 
if not isinstance ( obj , list ) : 
~~~ obj = [ obj ] 
~~ self . register ( Descriptor . from_json ( j ) for j in obj ) 
~~ def register ( self , desc , version = None , ignore_3D = False ) : 
if version is None : 
~~~ version = __version__ 
~~ version = StrictVersion ( version ) 
return self . _register ( desc , version , ignore_3D ) 
~~ def echo ( self , s , file = sys . stdout , end = "\\n" ) : 
p = getattr ( self , "_progress_bar" , None ) 
if p is not None : 
~~~ p . write ( s , file = file , end = "\\n" ) 
~~ print ( s , file = file , end = "\\n" ) 
~~ def map ( self , mols , nproc = None , nmols = None , quiet = False , ipynb = False , id = - 1 ) : 
if nproc is None : 
~~~ nproc = cpu_count ( ) 
~~ if hasattr ( mols , "__len__" ) : 
~~~ nmols = len ( mols ) 
~~ if nproc == 1 : 
~~~ return self . _serial ( mols , nmols = nmols , quiet = quiet , ipynb = ipynb , id = id ) 
~~~ return self . _parallel ( mols , nproc , nmols = nmols , quiet = quiet , ipynb = ipynb , id = id ) 
~~ ~~ def pandas ( self , mols , nproc = None , nmols = None , quiet = False , ipynb = False , id = - 1 ) : 
from . pandas_module import MordredDataFrame , Series 
if isinstance ( mols , Series ) : 
~~~ index = mols . index 
~~ return MordredDataFrame ( 
( list ( r ) for r in self . map ( mols , nproc , nmols , quiet , ipynb , id ) ) , 
columns = [ str ( d ) for d in self . descriptors ] , 
index = index , 
~~ def is_descriptor_class ( desc , include_abstract = False ) : 
return ( 
isinstance ( desc , type ) 
and issubclass ( desc , Descriptor ) 
and ( True if include_abstract else not inspect . isabstract ( desc ) ) 
~~ def to_json ( self ) : 
d , ps = self . _to_json ( ) 
if len ( ps ) == 0 : 
~~~ return { "name" : d } 
~~~ return { "name" : d , "args" : ps } 
~~ ~~ def coord ( self ) : 
if not self . require_3D : 
~~ return self . _context . get_coord ( self ) 
~~ def rethrow_zerodiv ( self ) : 
with np . errstate ( divide = "raise" , invalid = "raise" ) : 
~~ except ( FloatingPointError , ZeroDivisionError ) as e : 
~~~ self . fail ( ZeroDivisionError ( * e . args ) ) 
~~ ~~ ~~ def atomic_sa ( self , i ) : 
sa = 4.0 * np . pi * self . rads2 [ i ] 
neighbors = self . neighbors . get ( i ) 
if neighbors is None : 
~~~ return sa 
~~ XYZi = self . xyzs [ i , np . newaxis ] . T 
sphere = self . sphere * self . rads [ i ] + XYZi 
N = sphere . shape [ 1 ] 
for j , _ in neighbors : 
~~~ XYZj = self . xyzs [ j , np . newaxis ] . T 
d2 = ( sphere - XYZj ) ** 2 
mask = ( d2 [ 0 ] + d2 [ 1 ] + d2 [ 2 ] ) > self . rads2 [ j ] 
sphere = np . compress ( mask , sphere , axis = 1 ) 
~~ return sa * sphere . shape [ 1 ] / N 
~~ def surface_area ( self ) : 
return [ self . atomic_sa ( i ) for i in range ( len ( self . rads ) ) ] 
~~ def from_mol ( cls , mol , conformer = - 1 , solvent_radius = 1.4 , level = 4 ) : 
rs = atoms_to_numpy ( lambda a : vdw_radii [ a . GetAtomicNum ( ) ] + solvent_radius , mol ) 
conf = mol . GetConformer ( conformer ) 
ps = np . array ( [ list ( conf . GetAtomPosition ( i ) ) for i in range ( mol . GetNumAtoms ( ) ) ] ) 
return cls ( rs , ps , level ) 
~~ def _Descriptor_from_json ( self , obj ) : 
descs = getattr ( self , "_all_descriptors" , None ) 
if descs is None : 
~~~ from mordred import descriptors 
descs = { 
cls . __name__ : cls 
for cls in get_descriptors_in_module ( descriptors ) 
descs [ ConstDescriptor . __name__ ] = ConstDescriptor 
self . _all_descriptors = descs 
~~ return _from_json ( obj , descs ) 
~~ def fill_missing ( self , value = np . nan ) : 
return self . __class__ ( 
self . mol , 
[ ( value if is_missing ( v ) else v ) for v in self . values ( ) ] , 
self . keys ( ) , 
~~ def drop_missing ( self ) : 
newvalues = [ ] 
newdescs = [ ] 
for d , v in self . items ( ) : 
~~~ if not is_missing ( v ) : 
~~~ newvalues . append ( v ) 
newdescs . append ( d ) 
~~ ~~ return self . __class__ ( self . mol , newvalues , newdescs ) 
~~ def items ( self ) : 
return ( ( k , v ) for k , v in zip ( self . keys ( ) , self . values ( ) ) ) 
~~ def asdict ( self , rawkey = False ) : 
if rawkey : 
~~~ return dict ( self . items ( ) ) 
str ( k ) : v 
for k , v in self . items ( ) 
~~ ~~ def name ( self ) : 
if self . _name_to_value is None : 
~~~ self . _name_to_value = { str ( d ) : v for d , v in zip ( self . _descriptors , self . _values ) } 
~~ return GetValueByName ( self . _name_to_value ) 
~~ def discover_gateways ( self ) : 
_socket = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) 
_socket . settimeout ( 5.0 ) 
if self . _interface != 'any' : 
~~~ _socket . bind ( ( self . _interface , 0 ) ) 
~~ for gateway in self . _gateways_config : 
~~~ host = gateway . get ( 'host' ) 
port = gateway . get ( 'port' ) 
sid = gateway . get ( 'sid' ) 
if not ( host and port and sid ) : 
~~~ ip_address = socket . gethostbyname ( host ) 
if gateway . get ( 'disable' ) : 
~~~ _LOGGER . info ( 
self . disabled_gateways . append ( ip_address ) 
~~ _LOGGER . info ( 
sid , ip_address , port ) 
self . gateways [ ip_address ] = XiaomiGateway ( 
ip_address , port , sid , 
gateway . get ( 'key' ) , self . _device_discovery_retries , 
self . _interface , gateway . get ( 'proto' ) ) 
~~ except OSError as error : 
~~~ _LOGGER . error ( 
~~~ _socket . sendto ( \ . encode ( ) , 
( self . MULTICAST_ADDRESS , self . GATEWAY_DISCOVERY_PORT ) ) 
~~~ data , ( ip_add , _ ) = _socket . recvfrom ( 1024 ) 
if len ( data ) is None or ip_add in self . gateways : 
~~ if ip_add in self . gateways . keys ( ) or ip_add in self . disabled_gateways : 
~~ resp = json . loads ( data . decode ( ) ) 
if resp [ "cmd" ] != 'iam' : 
~~ if resp [ "model" ] not in GATEWAY_MODELS : 
~~ disabled = False 
gateway_key = None 
for gateway in self . _gateways_config : 
~~~ sid = gateway . get ( 'sid' ) 
if sid is None or sid == resp [ "sid" ] : 
~~~ gateway_key = gateway . get ( 'key' ) 
~~ if sid and sid == resp [ 'sid' ] and gateway . get ( 'disable' ) : 
~~~ disabled = True 
~~ ~~ sid = resp [ "sid" ] 
if disabled : 
sid ) 
self . disabled_gateways . append ( ip_add ) 
self . gateways [ ip_add ] = XiaomiGateway ( 
ip_add , resp [ "port" ] , sid , gateway_key , 
self . _device_discovery_retries , self . _interface , 
resp [ "proto_version" ] if "proto_version" in resp else None ) 
~~ ~~ ~~ except socket . timeout : 
_socket . close ( ) 
~~ ~~ def listen ( self ) : 
self . _mcastsocket = self . _create_mcast_socket ( ) 
self . _listening = True 
thread = Thread ( target = self . _listen_to_msg , args = ( ) ) 
self . _threads . append ( thread ) 
thread . daemon = True 
~~ def stop_listen ( self ) : 
self . _listening = False 
if self . _mcastsocket is not None : 
self . _mcastsocket . close ( ) 
self . _mcastsocket = None 
~~ for thread in self . _threads : 
~~~ thread . join ( ) 
~~ ~~ def write_to_hub ( self , sid , ** kwargs ) : 
if self . key is None : 
~~ data = { } 
for key in kwargs : 
~~~ data [ key ] = kwargs [ key ] 
~~ if not self . token : 
~~ cmd = dict ( ) 
cmd [ 'cmd' ] = 'write' 
cmd [ 'sid' ] = sid 
if int ( self . proto [ 0 : 1 ] ) == 1 : 
~~~ data [ 'key' ] = self . _get_key ( ) 
cmd [ 'data' ] = data 
~~~ cmd [ 'key' ] = self . _get_key ( ) 
cmd [ 'params' ] = [ data ] 
~~ resp = self . _send_cmd ( json . dumps ( cmd ) , "write_ack" ) if int ( self . proto [ 0 : 1 ] ) == 1 else self . _send_cmd ( json . dumps ( cmd ) , "write_rsp" ) 
if _validate_data ( resp ) : 
~~ if not _validate_keyerror ( resp ) : 
~~ resp = self . _send_cmd ( \ , "get_id_list_ack" ) if int ( self . proto [ 0 : 1 ] ) == 1 else self . _send_cmd ( \ , "discovery_rsp" ) 
if resp is None or "token" not in resp : 
~~ self . token = resp [ 'token' ] 
return _validate_data ( resp ) 
~~ def get_from_hub ( self , sid ) : 
cmd = \ + sid + \ 
resp = self . _send_cmd ( cmd , "read_ack" ) if int ( self . proto [ 0 : 1 ] ) == 1 else self . _send_cmd ( cmd , "read_rsp" ) 
return self . push_data ( resp ) 
~~ def push_data ( self , data ) : 
if not _validate_data ( data ) : 
~~ jdata = json . loads ( data [ 'data' ] ) if int ( self . proto [ 0 : 1 ] ) == 1 else _list2map ( data [ 'params' ] ) 
if jdata is None : 
~~ sid = data [ 'sid' ] 
for func in self . callbacks [ sid ] : 
~~~ func ( jdata , data ) 
~~ def _get_key ( self ) : 
init_vector = bytes ( bytearray . fromhex ( '17996d093d28ddb3ba695a2e6f58562e' ) ) 
encryptor = Cipher ( algorithms . AES ( self . key . encode ( ) ) , modes . CBC ( init_vector ) , 
backend = default_backend ( ) ) . encryptor ( ) 
ciphertext = encryptor . update ( self . token . encode ( ) ) + encryptor . finalize ( ) 
~~~ return '' . join ( '{:02x}' . format ( ord ( x ) ) for x in ciphertext ) 
~~ return '' . join ( '{:02x}' . format ( x ) for x in ciphertext ) 
~~ def train ( hparams , * args ) : 
exp = Experiment ( 
name = hparams . test_tube_exp_name , 
save_dir = hparams . log_path , 
autosave = False , 
exp . argparse ( hparams ) 
x = torch . rand ( ( 1 , hparams . x_val ) ) 
for train_step in range ( 0 , 100 ) : 
~~~ y = torch . rand ( ( hparams . x_val , 1 ) ) 
out = x . mm ( y ) 
exp . log ( { 'fake_err' : out . item ( ) } ) 
~~ exp . save ( ) 
version = hparams . hpc_exp_number , 
x = hparams . x_val 
~~~ y = hparams . y_val 
out = x * y 
~~ def compose ( chosung , joongsung , jongsung = u'' ) : 
if jongsung is None : jongsung = u'' 
~~~ chosung_index = CHO . index ( chosung ) 
joongsung_index = JOONG . index ( joongsung ) 
jongsung_index = JONG . index ( jongsung ) 
~~ return unichr ( 0xAC00 + chosung_index * NUM_JOONG * NUM_JONG + joongsung_index * NUM_JONG + jongsung_index ) 
~~ def decompose ( hangul_letter ) : 
from . import checker 
if len ( hangul_letter ) < 1 : 
~~~ raise NotLetterException ( '' ) 
~~ elif not checker . is_hangul ( hangul_letter ) : 
~~~ raise NotHangulException ( '' ) 
~~ if hangul_letter in CHO : 
~~~ return hangul_letter , '' , '' 
~~ if hangul_letter in JOONG : 
~~~ return '' , hangul_letter , '' 
~~ if hangul_letter in JONG : 
~~~ return '' , '' , hangul_letter 
~~ code = hangul_index ( hangul_letter ) 
cho , joong , jong = decompose_index ( code ) 
if cho < 0 : 
~~~ cho = 0 
~~~ return CHO [ cho ] , JOONG [ joong ] , JONG [ jong ] 
raise Exception ( ) 
~~ ~~ def has_jongsung ( letter ) : 
if len ( letter ) != 1 : 
~~ if not is_hangul ( letter ) : 
~~ code = lt . hangul_index ( letter ) 
return code % NUM_JONG > 0 
~~ def attach ( word , josa = EUN_NEUN ) : 
last_letter = word . strip ( ) [ - 1 ] 
~~~ _ , _ , letter_jong = letter . decompose ( last_letter ) 
~~ except NotHangulException : 
~~~ letter_jong = letter . get_substituent_of ( last_letter ) 
~~ if letter_jong in ( '' , josa [ 'except' ] ) : 
~~~ return word + josa [ 'has' ] 
~~ return word + josa [ 'not' ] 
~~ def bind_cache_grant ( app , provider , current_user , config_prefix = 'OAUTH2' ) : 
cache = Cache ( app , config_prefix ) 
@ provider . grantsetter 
def create_grant ( client_id , code , request , * args , ** kwargs ) : 
grant = Grant ( 
cache , 
client_id = client_id , 
code = code [ 'code' ] , 
redirect_uri = request . redirect_uri , 
scopes = request . scopes , 
user = current_user ( ) , 
cache . set ( grant . key , dict ( grant ) ) 
~~ @ provider . grantgetter 
def get ( client_id , code ) : 
grant = Grant ( cache , client_id = client_id , code = code ) 
ret = cache . get ( grant . key ) 
if not ret : 
for k , v in ret . items ( ) : 
~~~ setattr ( grant , k , v ) 
~~ return grant 
~~ ~~ def bind_sqlalchemy ( provider , session , user = None , client = None , 
token = None , grant = None , current_user = None ) : 
if user : 
~~~ user_binding = UserBinding ( user , session ) 
provider . usergetter ( user_binding . get ) 
~~ if client : 
~~~ client_binding = ClientBinding ( client , session ) 
provider . clientgetter ( client_binding . get ) 
~~ if token : 
~~~ token_binding = TokenBinding ( token , session , current_user ) 
provider . tokengetter ( token_binding . get ) 
provider . tokensetter ( token_binding . set ) 
~~ if grant : 
~~~ if not current_user : 
~~ grant_binding = GrantBinding ( grant , session , current_user ) 
provider . grantgetter ( grant_binding . get ) 
provider . grantsetter ( grant_binding . set ) 
~~ ~~ def delete ( self ) : 
log . debug ( 
self . _cache . delete ( self . key ) 
~~ def query ( self ) : 
if hasattr ( self . model , 'query' ) : 
~~~ return self . model . query 
~~~ return self . session . query ( self . model ) 
~~ ~~ def get ( self , username , password , * args , ** kwargs ) : 
user = self . query . filter_by ( username = username ) . first ( ) 
if user and user . check_password ( password ) : 
~~ def get ( self , access_token = None , refresh_token = None ) : 
if access_token : 
~~~ return self . query . filter_by ( access_token = access_token ) . first ( ) 
~~ elif refresh_token : 
~~~ return self . query . filter_by ( refresh_token = refresh_token ) . first ( ) 
~~ def set ( self , token , request , * args , ** kwargs ) : 
if hasattr ( request , 'user' ) and request . user : 
~~~ user = request . user 
~~ elif self . current_user : 
~~~ user = self . current_user ( ) 
~~ client = request . client 
tokens = self . query . filter_by ( 
client_id = client . client_id , 
user_id = user . id ) . all ( ) 
if tokens : 
~~~ for tk in tokens : 
~~~ self . session . delete ( tk ) 
~~ self . session . commit ( ) 
~~ expires_in = token . get ( 'expires_in' ) 
expires = datetime . utcnow ( ) + timedelta ( seconds = expires_in ) 
tok = self . model ( ** token ) 
tok . expires = expires 
tok . client_id = client . client_id 
tok . user_id = user . id 
self . session . add ( tok ) 
self . session . commit ( ) 
return tok 
~~ def set ( self , client_id , code , request , * args , ** kwargs ) : 
expires = datetime . utcnow ( ) + timedelta ( seconds = 100 ) 
grant = self . model ( 
client_id = request . client . client_id , 
user = self . current_user ( ) , 
expires = expires 
self . session . add ( grant ) 
~~ def get ( self , client_id , code ) : 
return self . query . filter_by ( client_id = client_id , code = code ) . first ( ) 
~~ def parse_response ( resp , content , strict = False , content_type = None ) : 
if not content_type : 
~~~ content_type = resp . headers . get ( 'content-type' , 'application/json' ) 
~~ ct , options = parse_options_header ( content_type ) 
if ct in ( 'application/json' , 'text/javascript' ) : 
~~~ if not content : 
~~ return json . loads ( content ) 
~~ if ct in ( 'application/xml' , 'text/xml' ) : 
~~~ return get_etree ( ) . fromstring ( content ) 
~~ if ct != 'application/x-www-form-urlencoded' and strict : 
~~~ return content 
~~ charset = options . get ( 'charset' , 'utf-8' ) 
return url_decode ( content , charset = charset ) . to_dict ( ) 
~~ def prepare_request ( uri , headers = None , data = None , method = None ) : 
if headers is None : 
~~ if data and not method : 
~~~ method = 'POST' 
~~ elif not method : 
~~~ method = 'GET' 
~~ if method == 'GET' and data : 
~~~ uri = add_params_to_uri ( uri , data ) 
data = None 
~~ return uri , headers , data , method 
~~ def init_app ( self , app ) : 
self . app = app 
app . extensions = getattr ( app , 'extensions' , { } ) 
app . extensions [ self . state_key ] = self 
~~ def remote_app ( self , name , register = True , ** kwargs ) : 
remote = OAuthRemoteApp ( self , name , ** kwargs ) 
if register : 
~~~ assert name not in self . remote_apps 
self . remote_apps [ name ] = remote 
~~ return remote 
~~ def request ( self , url , data = None , headers = None , format = 'urlencoded' , 
method = 'GET' , content_type = None , token = None ) : 
headers = dict ( headers or { } ) 
~~~ token = self . get_request_token ( ) 
~~ client = self . make_client ( token ) 
url = self . expand_url ( url ) 
if method == 'GET' : 
~~~ assert format == 'urlencoded' 
~~~ url = add_params_to_uri ( url , data ) 
~~~ if content_type is None : 
~~~ data , content_type = encode_request_data ( data , format ) 
~~ if content_type is not None : 
~~~ headers [ 'Content-Type' ] = content_type 
~~ ~~ if self . request_token_url : 
~~~ uri , headers , body = client . sign ( 
url , http_method = method , body = data , headers = headers 
~~~ uri , headers , body = client . add_token ( 
~~ if hasattr ( self , 'pre_request' ) : 
~~~ uri , headers , body = self . pre_request ( uri , headers , body ) 
~~ if body : 
~~~ data = to_bytes ( body , self . encoding ) 
~~~ data = None 
~~ resp , content = self . http_request ( 
uri , headers , data = to_bytes ( body , self . encoding ) , method = method 
return OAuthResponse ( resp , content , self . content_type ) 
~~ def authorize ( self , callback = None , state = None , ** kwargs ) : 
params = dict ( self . request_token_params ) or { } 
params . update ( ** kwargs ) 
if self . request_token_url : 
~~~ token = self . generate_request_token ( callback ) [ 0 ] 
url = '%s?oauth_token=%s' % ( 
self . expand_url ( self . authorize_url ) , url_quote ( token ) 
~~~ url += '&' + url_encode ( params ) 
client = self . make_client ( ) 
if 'scope' in params : 
~~~ scope = params . pop ( 'scope' ) 
~~~ scope = None 
~~ if isinstance ( scope , str ) : 
~~~ scope = _encode ( scope , self . encoding ) 
~~ if 'state' in params : 
~~~ if not state : 
~~~ state = params . pop ( 'state' ) 
~~~ params . pop ( 'state' ) 
~~ ~~ if callable ( state ) : 
~~~ state = state ( ) 
~~ session [ '%s_oauthredir' % self . name ] = callback 
url = client . prepare_request_uri ( 
self . expand_url ( self . authorize_url ) , 
redirect_uri = callback , 
scope = scope , 
state = state , 
** params 
~~ return redirect ( url ) 
~~ def handle_oauth1_response ( self , args ) : 
client . verifier = args . get ( 'oauth_verifier' ) 
tup = session . get ( '%s_oauthtok' % self . name ) 
if not tup : 
~~~ raise OAuthException ( 
type = 'token_not_found' 
~~ client . resource_owner_key = tup [ 0 ] 
client . resource_owner_secret = tup [ 1 ] 
uri , headers , data = client . sign ( 
self . expand_url ( self . access_token_url ) , 
_encode ( self . access_token_method ) 
headers . update ( self . _access_token_headers ) 
resp , content = self . http_request ( 
uri , headers , to_bytes ( data , self . encoding ) , 
method = self . access_token_method 
data = parse_response ( resp , content ) 
if resp . code not in ( 200 , 201 ) : 
type = 'invalid_response' , data = data 
~~ def handle_oauth2_response ( self , args ) : 
remote_args = { 
'code' : args . get ( 'code' ) , 
'client_secret' : self . consumer_secret , 
'redirect_uri' : session . get ( '%s_oauthredir' % self . name ) 
remote_args . update ( self . access_token_params ) 
headers = copy ( self . _access_token_headers ) 
if self . access_token_method == 'POST' : 
~~~ headers . update ( { 'Content-Type' : 'application/x-www-form-urlencoded' } ) 
body = client . prepare_request_body ( ** remote_args ) 
data = to_bytes ( body , self . encoding ) , 
method = self . access_token_method , 
~~ elif self . access_token_method == 'GET' : 
~~~ qs = client . prepare_request_body ( ** remote_args ) 
url = self . expand_url ( self . access_token_url ) 
url += ( '?' in url and '&' or '?' ) + qs 
url , 
self . access_token_method 
~~ data = parse_response ( resp , content , content_type = self . content_type ) 
~~ def authorized_response ( self , args = None ) : 
if args is None : 
~~~ args = request . args 
~~ if 'oauth_verifier' in args : 
~~~ data = self . handle_oauth1_response ( args ) 
~~ elif 'code' in args : 
~~~ data = self . handle_oauth2_response ( args ) 
~~~ data = self . handle_unknown_response ( ) 
~~ session . pop ( '%s_oauthtok' % self . name , None ) 
session . pop ( '%s_oauthredir' % self . name , None ) 
~~ def authorized_handler ( self , f ) : 
@ wraps ( f ) 
def decorated ( * args , ** kwargs ) : 
~~~ log . warn ( 
'authorized_response' 
data = self . authorized_response ( ) 
return f ( * ( ( data , ) + args ) , ** kwargs ) 
~~ return decorated 
~~ def _hash_token ( application , token ) : 
if isinstance ( token , dict ) : 
~~~ hashed_token = tuple ( sorted ( token . items ( ) ) ) 
~~ elif isinstance ( token , tuple ) : 
~~~ hashed_token = token 
~~ return ( application . __class__ . __name__ , application . name , hashed_token ) 
~~ def client ( self ) : 
token = self . obtain_token ( ) 
~~~ raise AccessTokenNotFound 
~~ return self . _make_client_with_token ( token ) 
~~ def _make_client_with_token ( self , token ) : 
cached_clients = getattr ( self , 'clients' , None ) 
hashed_token = _hash_token ( self , token ) 
if cached_clients and hashed_token in cached_clients : 
~~~ return cached_clients [ hashed_token ] 
if cached_clients : 
~~~ cached_clients [ hashed_token ] = client 
~~ return client 
~~ def make_client ( self , token ) : 
~~~ access_token = token [ 'oauth_token' ] 
access_token_secret = token [ 'oauth_token_secret' ] 
~~~ access_token , access_token_secret = token 
~~ return self . make_oauth_session ( 
resource_owner_key = access_token , 
resource_owner_secret = access_token_secret ) 
~~ def insecure_transport ( self ) : 
origin = os . environ . get ( 'OAUTHLIB_INSECURE_TRANSPORT' ) 
if current_app . debug or current_app . testing : 
~~~ os . environ [ 'OAUTHLIB_INSECURE_TRANSPORT' ] = '1' 
yield 
~~~ if origin : 
~~~ os . environ [ 'OAUTHLIB_INSECURE_TRANSPORT' ] = origin 
~~~ os . environ . pop ( 'OAUTHLIB_INSECURE_TRANSPORT' , None ) 
~~ yield 
~~ ~~ def server ( self ) : 
if hasattr ( self , '_validator' ) : 
~~~ return Server ( self . _validator ) 
~~ if hasattr ( self , '_clientgetter' ) and hasattr ( self , '_tokengetter' ) and hasattr ( self , '_tokensetter' ) and hasattr ( self , '_noncegetter' ) and hasattr ( self , '_noncesetter' ) and hasattr ( self , '_grantgetter' ) and hasattr ( self , '_grantsetter' ) and hasattr ( self , '_verifiergetter' ) and hasattr ( self , '_verifiersetter' ) : 
~~~ validator = OAuth1RequestValidator ( 
clientgetter = self . _clientgetter , 
tokengetter = self . _tokengetter , 
tokensetter = self . _tokensetter , 
grantgetter = self . _grantgetter , 
grantsetter = self . _grantsetter , 
noncegetter = self . _noncegetter , 
noncesetter = self . _noncesetter , 
verifiergetter = self . _verifiergetter , 
verifiersetter = self . _verifiersetter , 
config = self . app . config , 
self . _validator = validator 
server = Server ( validator ) 
if self . app . testing : 
~~~ server . _check_signature = lambda * args , ** kwargs : True 
~~ return server 
~~ raise RuntimeError ( 
~~ def authorize_handler ( self , f ) : 
~~~ if request . method == 'POST' : 
~~~ if not f ( * args , ** kwargs ) : 
~~~ uri = add_params_to_uri ( 
self . error_uri , [ ( 'error' , 'denied' ) ] 
return redirect ( uri ) 
~~ return self . confirm_authorization_request ( ) 
~~ server = self . server 
uri , http_method , body , headers = extract_params ( ) 
~~~ realms , credentials = server . get_realms_and_credentials ( 
uri , http_method = http_method , body = body , headers = headers 
kwargs [ 'realms' ] = realms 
kwargs . update ( credentials ) 
return f ( * args , ** kwargs ) 
~~ except errors . OAuth1Error as e : 
~~~ return redirect ( e . in_uri ( self . error_uri ) ) 
~~ except errors . InvalidClientError as e : 
~~ ~~ return decorated 
~~ def confirm_authorization_request ( self ) : 
server = self . server 
ret = server . create_authorization_response ( 
uri , http_method , body , headers , realms , credentials 
return create_response ( * ret ) 
~~ ~~ def request_token_handler ( self , f ) : 
credentials = f ( * args , ** kwargs ) 
~~~ ret = server . create_request_token_response ( 
uri , http_method , body , headers , credentials ) 
~~~ return _error_response ( e ) 
~~ def require_oauth ( self , * realms , ** kwargs ) : 
def wrapper ( f ) : 
~~~ @ wraps ( f ) 
~~~ for func in self . _before_request_funcs : 
~~~ func ( ) 
~~ if hasattr ( request , 'oauth' ) and request . oauth : 
~~~ return f ( * args , ** kwargs ) 
~~~ valid , req = server . validate_protected_resource_request ( 
uri , http_method , body , headers , realms 
e . urlencoded = urlencode ( [ ( 'error' , 'unknown' ) ] ) 
e . status_code = 400 
return _error_response ( e ) 
~~ for func in self . _after_request_funcs : 
~~~ valid , req = func ( valid , req ) 
~~ if not valid : 
~~~ return abort ( 401 ) 
~~ req . user = req . access_token . user 
request . oauth = req 
~~ def get_client_secret ( self , client_key , request ) : 
if not request . client : 
~~~ request . client = self . _clientgetter ( client_key = client_key ) 
~~ if request . client : 
~~~ return request . client . client_secret 
~~ def get_request_token_secret ( self , client_key , token , request ) : 
token , client_key ) 
tok = request . request_token or self . _grantgetter ( token = token ) 
if tok and tok . client_key == client_key : 
~~~ request . request_token = tok 
return tok . secret 
~~ def get_access_token_secret ( self , client_key , token , request ) : 
tok = request . access_token or self . _tokengetter ( 
client_key = client_key , 
token = token , 
if tok : 
~~~ request . access_token = tok 
~~ def get_default_realms ( self , client_key , request ) : 
if hasattr ( client , 'default_realms' ) : 
~~~ return client . default_realms 
~~ return [ ] 
~~ def get_realms ( self , token , request ) : 
if not tok : 
~~ request . request_token = tok 
if hasattr ( tok , 'realms' ) : 
~~~ return tok . realms or [ ] 
~~ def get_redirect_uri ( self , token , request ) : 
return tok . redirect_uri 
~~ def get_rsa_key ( self , client_key , request ) : 
~~ if hasattr ( request . client , 'rsa_key' ) : 
~~~ return request . client . rsa_key 
~~ def validate_client_key ( self , client_key , request ) : 
~~ def validate_request_token ( self , client_key , token , request ) : 
~~ def validate_access_token ( self , client_key , token , request ) : 
~~ def validate_timestamp_and_nonce ( self , client_key , timestamp , nonce , 
request , request_token = None , 
access_token = None ) : 
nonce_exists = self . _noncegetter ( 
client_key = client_key , timestamp = timestamp , 
nonce = nonce , request_token = request_token , 
access_token = access_token 
if nonce_exists : 
~~ self . _noncesetter ( 
~~ def validate_redirect_uri ( self , client_key , redirect_uri , request ) : 
~~ if not request . client : 
~~ if not request . client . redirect_uris and redirect_uri is None : 
~~ request . redirect_uri = redirect_uri 
return redirect_uri in request . client . redirect_uris 
~~ def validate_realms ( self , client_key , token , request , uri = None , 
realms = None ) : 
if request . access_token : 
~~~ tok = request . access_token 
~~~ tok = self . _tokengetter ( client_key = client_key , token = token ) 
request . access_token = tok 
~~ if not tok : 
~~ return set ( tok . realms ) . issuperset ( set ( realms ) ) 
~~ def validate_verifier ( self , client_key , token , verifier , request ) : 
data = self . _verifiergetter ( verifier = verifier , token = token ) 
if not data : 
~~ if not hasattr ( data , 'user' ) : 
~~ request . user = data . user 
if hasattr ( data , 'client_key' ) : 
~~~ return data . client_key == client_key 
~~ def verify_request_token ( self , token , request ) : 
~~ def verify_realms ( self , token , realms , request ) : 
if not hasattr ( tok , 'realms' ) : 
~~ return set ( tok . realms ) == set ( realms ) 
~~ def save_access_token ( self , token , request ) : 
self . _tokensetter ( token , request ) 
~~ def save_request_token ( self , token , request ) : 
self . _grantsetter ( token , request ) 
~~ def save_verifier ( self , token , verifier , request ) : 
self . _verifiersetter ( 
token = token , verifier = verifier , request = request 
~~ def error_uri ( self ) : 
error_uri = self . app . config . get ( 'OAUTH2_PROVIDER_ERROR_URI' ) 
if error_uri : 
~~~ return error_uri 
~~ error_endpoint = self . app . config . get ( 'OAUTH2_PROVIDER_ERROR_ENDPOINT' ) 
if error_endpoint : 
~~~ return url_for ( error_endpoint ) 
~~ return '/oauth/errors' 
~~ def server ( self ) : 
expires_in = self . app . config . get ( 'OAUTH2_PROVIDER_TOKEN_EXPIRES_IN' ) 
token_generator = self . app . config . get ( 
'OAUTH2_PROVIDER_TOKEN_GENERATOR' , None 
if token_generator and not callable ( token_generator ) : 
~~~ token_generator = import_string ( token_generator ) 
~~ refresh_token_generator = self . app . config . get ( 
'OAUTH2_PROVIDER_REFRESH_TOKEN_GENERATOR' , None 
if refresh_token_generator and not callable ( refresh_token_generator ) : 
~~~ refresh_token_generator = import_string ( refresh_token_generator ) 
~~ if hasattr ( self , '_validator' ) : 
~~~ return Server ( 
self . _validator , 
token_expires_in = expires_in , 
token_generator = token_generator , 
refresh_token_generator = refresh_token_generator , 
~~ if hasattr ( self , '_clientgetter' ) and hasattr ( self , '_tokengetter' ) and hasattr ( self , '_tokensetter' ) and hasattr ( self , '_grantgetter' ) and hasattr ( self , '_grantsetter' ) : 
~~~ usergetter = None 
if hasattr ( self , '_usergetter' ) : 
~~~ usergetter = self . _usergetter 
~~ validator_class = self . _validator_class 
if validator_class is None : 
~~~ validator_class = OAuth2RequestValidator 
~~ validator = validator_class ( 
usergetter = usergetter , 
return Server ( 
validator , 
if request . method in ( 'GET' , 'HEAD' ) : 
~~~ redirect_uri = request . args . get ( 'redirect_uri' , self . error_uri ) 
~~~ ret = server . validate_authorization_request ( 
uri , http_method , body , headers 
scopes , credentials = ret 
kwargs [ 'scopes' ] = scopes 
~~ except oauth2 . FatalClientError as e : 
return self . _on_exception ( e , e . in_uri ( self . error_uri ) ) 
~~ except oauth2 . OAuth2Error as e : 
state = request . values . get ( 'state' ) 
if state and not e . state : 
~~ return self . _on_exception ( e , e . in_uri ( redirect_uri ) ) 
~~~ log . exception ( e ) 
return self . _on_exception ( e , add_params_to_uri ( 
self . error_uri , { 'error' : str ( e ) } 
~~~ redirect_uri = request . values . get ( 
'redirect_uri' , self . error_uri 
~~~ rv = f ( * args , ** kwargs ) 
~~ if not isinstance ( rv , bool ) : 
~~~ return rv 
~~ if not rv : 
~~~ e = oauth2 . AccessDeniedError ( state = request . values . get ( 'state' ) ) 
return self . _on_exception ( e , e . in_uri ( redirect_uri ) ) 
scope = request . values . get ( 'scope' ) or '' 
scopes = scope . split ( ) 
credentials = dict ( 
client_id = request . values . get ( 'client_id' ) , 
redirect_uri = request . values . get ( 'redirect_uri' , None ) , 
response_type = request . values . get ( 'response_type' , None ) , 
state = request . values . get ( 'state' , None ) 
redirect_uri = credentials . get ( 'redirect_uri' ) 
~~~ ret = server . create_authorization_response ( 
uri , http_method , body , headers , scopes , credentials ) 
~~ return self . _on_exception ( e , e . in_uri ( redirect_uri or self . error_uri ) ) 
~~ ~~ def verify_request ( self , scopes ) : 
return self . server . verify_request ( 
uri , http_method , body , headers , scopes 
~~ def token_handler ( self , f ) : 
credentials = f ( * args , ** kwargs ) or { } 
ret = server . create_token_response ( 
uri , http_method , body , headers , credentials 
~~ def revoke_handler ( self , f ) : 
token = request . values . get ( 'token' ) 
request . token_type_hint = request . values . get ( 'token_type_hint' ) 
~~~ request . token = token 
~~ uri , http_method , body , headers = extract_params ( ) 
ret = server . create_revocation_response ( 
uri , headers = headers , body = body , http_method = http_method ) 
~~ def require_oauth ( self , * scopes ) : 
~~ valid , req = self . verify_request ( scopes ) 
for func in self . _after_request_funcs : 
~~~ if self . _invalid_response : 
~~~ return self . _invalid_response ( req ) 
~~ return abort ( 401 ) 
~~ request . oauth = req 
~~ def _get_client_creds_from_request ( self , request ) : 
if request . client_id is not None : 
~~~ return request . client_id , request . client_secret 
~~ auth = request . headers . get ( 'Authorization' ) 
if isinstance ( auth , dict ) : 
~~~ return auth [ 'username' ] , auth [ 'password' ] 
~~ return None , None 
~~ def client_authentication_required ( self , request , * args , ** kwargs ) : 
def is_confidential ( client ) : 
~~~ if hasattr ( client , 'is_confidential' ) : 
~~~ return client . is_confidential 
~~ client_type = getattr ( client , 'client_type' , None ) 
if client_type : 
~~~ return client_type == 'confidential' 
~~ grant_types = ( 'password' , 'authorization_code' , 'refresh_token' ) 
client_id , _ = self . _get_client_creds_from_request ( request ) 
if client_id and request . grant_type in grant_types : 
~~~ client = self . _clientgetter ( client_id ) 
if client : 
~~~ return is_confidential ( client ) 
~~ def authenticate_client ( self , request , * args , ** kwargs ) : 
client_id , client_secret = self . _get_client_creds_from_request ( request ) 
client = self . _clientgetter ( client_id ) 
if not client : 
~~ request . client = client 
if hasattr ( client , 'client_secret' ) and client . client_secret != client_secret : 
~~ def authenticate_client_id ( self , client_id , request , * args , ** kwargs ) : 
if client_id is None : 
~~~ client_id , _ = self . _get_client_creds_from_request ( request ) 
client = request . client or self . _clientgetter ( client_id ) 
~~ def confirm_redirect_uri ( self , client_id , code , redirect_uri , client , 
* args , ** kwargs ) : 
client = client or self . _clientgetter ( client_id ) 
client . client_id , code ) 
grant = self . _grantgetter ( client_id = client . client_id , code = code ) 
if not grant : 
~~ if hasattr ( grant , 'validate_redirect_uri' ) : 
~~~ return grant . validate_redirect_uri ( redirect_uri ) 
grant . redirect_uri , redirect_uri ) 
testing = 'OAUTHLIB_INSECURE_TRANSPORT' in os . environ 
if testing and redirect_uri is None : 
~~ return grant . redirect_uri == redirect_uri 
~~ def get_original_scopes ( self , refresh_token , request , * args , ** kwargs ) : 
tok = self . _tokengetter ( refresh_token = refresh_token ) 
return tok . scopes 
~~ def confirm_scopes ( self , refresh_token , scopes , request , * args , ** kwargs ) : 
if not scopes : 
scopes , refresh_token ) 
return set ( tok . scopes ) == set ( scopes ) 
~~ def get_default_redirect_uri ( self , client_id , request , * args , ** kwargs ) : 
request . client = request . client or self . _clientgetter ( client_id ) 
redirect_uri = request . client . default_redirect_uri 
return redirect_uri 
~~ def get_default_scopes ( self , client_id , request , * args , ** kwargs ) : 
scopes = request . client . default_scopes 
return scopes 
~~ def invalidate_authorization_code ( self , client_id , code , request , 
grant = self . _grantgetter ( client_id = client_id , code = code ) 
if grant : 
~~~ grant . delete ( ) 
~~ ~~ def save_authorization_code ( self , client_id , code , request , 
code , client_id 
self . _grantsetter ( client_id , code , request , * args , ** kwargs ) 
return request . client . default_redirect_uri 
~~ def save_bearer_token ( self , token , request , * args , ** kwargs ) : 
self . _tokensetter ( token , request , * args , ** kwargs ) 
~~ def validate_bearer_token ( self , token , scopes , request ) : 
tok = self . _tokengetter ( access_token = token ) 
request . error_message = msg 
log . debug ( msg ) 
~~ if tok . expires is not None and datetime . datetime . utcnow ( ) > tok . expires : 
~~ if scopes and not set ( tok . scopes ) & set ( scopes ) : 
~~ request . access_token = tok 
request . user = tok . user 
request . scopes = scopes 
if hasattr ( tok , 'client' ) : 
~~~ request . client = tok . client 
~~ elif hasattr ( tok , 'client_id' ) : 
~~~ request . client = self . _clientgetter ( tok . client_id ) 
~~ def validate_client_id ( self , client_id , request , * args , ** kwargs ) : 
~~~ request . client = client 
~~ def validate_code ( self , client_id , code , client , request , * args , ** kwargs ) : 
~~ if hasattr ( grant , 'expires' ) and datetime . datetime . utcnow ( ) > grant . expires : 
~~ request . state = kwargs . get ( 'state' ) 
request . user = grant . user 
request . scopes = grant . scopes 
~~ def validate_grant_type ( self , client_id , grant_type , client , request , 
if self . _usergetter is None and grant_type == 'password' : 
~~ default_grant_types = ( 
'authorization_code' , 'password' , 
'client_credentials' , 'refresh_token' , 
if hasattr ( client , 'allowed_grant_types' ) : 
~~~ if grant_type not in client . allowed_grant_types : 
~~~ if grant_type not in default_grant_types : 
~~ ~~ if grant_type == 'client_credentials' : 
~~~ if not hasattr ( client , 'user' ) : 
~~ request . user = client . user 
~~ def validate_redirect_uri ( self , client_id , redirect_uri , request , 
client = request . client 
if hasattr ( client , 'validate_redirect_uri' ) : 
~~~ return client . validate_redirect_uri ( redirect_uri ) 
~~ return redirect_uri in client . redirect_uris 
~~ def validate_refresh_token ( self , refresh_token , client , request , 
token = self . _tokengetter ( refresh_token = refresh_token ) 
if token and token . client_id == client . client_id : 
~~~ request . client_id = token . client_id 
request . user = token . user 
~~ def validate_response_type ( self , client_id , response_type , client , request , 
if response_type not in ( 'code' , 'token' ) : 
~~ if hasattr ( client , 'allowed_response_types' ) : 
~~~ return response_type in client . allowed_response_types 
~~ def validate_scopes ( self , client_id , scopes , client , request , 
if hasattr ( client , 'validate_scopes' ) : 
~~~ return client . validate_scopes ( scopes ) 
~~ return set ( client . default_scopes ) . issuperset ( set ( scopes ) ) 
~~ def validate_user ( self , username , password , client , request , 
if self . _usergetter is not None : 
~~~ user = self . _usergetter ( 
username , password , client , request , * args , ** kwargs 
~~~ request . user = user 
~~ def revoke_token ( self , token , token_type_hint , request , * args , ** kwargs ) : 
if token_type_hint : 
~~~ tok = self . _tokengetter ( ** { token_type_hint : token } ) 
~~~ tok = self . _tokengetter ( access_token = token ) 
~~~ tok = self . _tokengetter ( refresh_token = token ) 
~~ ~~ if tok : 
~~~ request . client_id = tok . client_id 
tok . delete ( ) 
~~ def json_to_dict ( x ) : 
if x . find ( b'callback' ) > - 1 : 
~~~ pos_lb = x . find ( b'{' ) 
pos_rb = x . find ( b'}' ) 
x = x [ pos_lb : pos_rb + 1 ] 
~~~ x = x . decode ( 'utf-8' ) 
~~ return json . loads ( x , encoding = 'utf-8' ) 
~~ ~~ def update_qq_api_request_data ( data = { } ) : 
'openid' : session . get ( 'qq_openid' ) , 
'access_token' : session . get ( 'qq_token' ) [ 0 ] , 
'oauth_consumer_key' : QQ_APP_ID , 
defaults . update ( data ) 
return defaults 
~~ def convert_keys_to_string ( dictionary ) : 
if not isinstance ( dictionary , dict ) : 
~~~ return dictionary 
~~ return dict ( ( str ( k ) , convert_keys_to_string ( v ) ) for k , v in dictionary . items ( ) ) 
~~ def change_weibo_header ( uri , headers , body ) : 
auth = headers . get ( 'Authorization' ) 
if auth : 
~~~ auth = auth . replace ( 'Bearer' , 'OAuth2' ) 
headers [ 'Authorization' ] = auth 
~~ return uri , headers , body 
~~ def register_to ( self , oauth , name = None , ** kwargs ) : 
kwargs = self . _process_kwargs ( 
name = ( name or self . default_name ) , ** kwargs ) 
return oauth . remote_app ( ** kwargs ) 
~~ def create ( self , oauth , ** kwargs ) : 
name = self . default_name , register = False , ** kwargs ) 
~~ def _get_uri_from_request ( request ) : 
uri = request . base_url 
if request . query_string : 
~~~ uri += '?' + request . query_string . decode ( 'utf-8' ) 
~~ return uri 
~~ def extract_params ( ) : 
uri = _get_uri_from_request ( request ) 
http_method = request . method 
headers = dict ( request . headers ) 
if 'wsgi.input' in headers : 
~~~ del headers [ 'wsgi.input' ] 
~~ if 'wsgi.errors' in headers : 
~~~ del headers [ 'wsgi.errors' ] 
~~ if request . authorization : 
~~~ headers [ 'Authorization' ] = request . authorization 
~~ body = request . form . to_dict ( ) 
return uri , http_method , body , headers 
~~ def to_bytes ( text , encoding = 'utf-8' ) : 
if not text : 
~~~ return text 
~~ if not isinstance ( text , bytes_type ) : 
~~~ text = text . encode ( encoding ) 
~~ return text 
~~ def decode_base64 ( text , encoding = 'utf-8' ) : 
text = to_bytes ( text , encoding ) 
return to_unicode ( base64 . b64decode ( text ) , encoding ) 
~~ def create_response ( headers , body , status ) : 
response = Response ( body or '' ) 
for k , v in headers . items ( ) : 
~~~ response . headers [ str ( k ) ] = v 
~~ response . status_code = status 
~~ def _simple ( self , ** kwargs ) : 
kwargs . update ( dict ( threshold = self . _config ( 'threshold' , 500 ) ) ) 
return SimpleCache ( ** kwargs ) 
~~ def _memcache ( self , ** kwargs ) : 
kwargs . update ( dict ( 
servers = self . _config ( 'MEMCACHED_SERVERS' , None ) , 
key_prefix = self . _config ( 'key_prefix' , None ) , 
return MemcachedCache ( ** kwargs ) 
~~ def _redis ( self , ** kwargs ) : 
host = self . _config ( 'REDIS_HOST' , 'localhost' ) , 
port = self . _config ( 'REDIS_PORT' , 6379 ) , 
password = self . _config ( 'REDIS_PASSWORD' , None ) , 
db = self . _config ( 'REDIS_DB' , 0 ) , 
key_prefix = self . _config ( 'KEY_PREFIX' , None ) , 
return RedisCache ( ** kwargs ) 
~~ def _filesystem ( self , ** kwargs ) : 
threshold = self . _config ( 'threshold' , 500 ) , 
return FileSystemCache ( self . _config ( 'dir' , None ) , ** kwargs ) 
~~ def get_cached_clients ( ) : 
if OAuth . state_key not in current_app . extensions : 
~~ state = current_app . extensions [ OAuth . state_key ] 
return state . cached_clients 
~~ def add_remote_app ( self , remote_app , name = None , ** kwargs ) : 
~~~ name = remote_app . name 
~~ if name != remote_app . name or kwargs : 
~~~ remote_app = copy . copy ( remote_app ) 
remote_app . name = name 
vars ( remote_app ) . update ( kwargs ) 
~~ if not hasattr ( remote_app , 'clients' ) : 
~~~ remote_app . clients = cached_clients 
~~ self . remote_apps [ name ] = remote_app 
return remote_app 
~~ def remote_app ( self , name , version = None , ** kwargs ) : 
~~~ if 'request_token_url' in kwargs : 
~~~ version = '1' 
~~~ version = '2' 
~~ ~~ if version == '1' : 
~~~ remote_app = OAuth1Application ( name , clients = cached_clients ) 
~~ elif version == '2' : 
~~~ remote_app = OAuth2Application ( name , clients = cached_clients ) 
~~ return self . add_remote_app ( remote_app , ** kwargs ) 
~~ def _print_token_factory ( col ) : 
def _helper ( msg ) : 
~~~ style = style_from_dict ( { 
Token . Color : col , 
tokens = [ 
( Token . Color , msg ) 
print_tokens ( tokens , style = style ) 
~~ def _helper_no_terminal ( msg ) : 
~~~ print ( msg ) 
~~ if sys . stdout . isatty ( ) : 
~~~ return _helper 
~~~ return _helper_no_terminal 
~~ ~~ def get_service_metadata ( self ) : 
'import_labels_as_tags' : 
self . config . get ( 'import_labels_as_tags' , False , asbool ) , 
'label_template' : 
self . config . get ( 'label_template' , DEFAULT_LABEL_TEMPLATE ) , 
~~ def issues ( self ) : 
for board in self . get_boards ( ) : 
~~~ for lst in self . get_lists ( board [ 'id' ] ) : 
~~~ listextra = dict ( boardname = board [ 'name' ] , listname = lst [ 'name' ] ) 
for card in self . get_cards ( lst [ 'id' ] ) : 
~~~ issue = self . get_issue_for_record ( card , extra = listextra ) 
issue . update_extra ( { "annotations" : self . annotations ( card ) } ) 
~~ ~~ ~~ ~~ def annotations ( self , card_json ) : 
comments = self . get_comments ( card_json [ 'id' ] ) 
annotations = self . build_annotations ( 
( ( c [ 'memberCreator' ] [ 'username' ] , c [ 'data' ] [ 'text' ] ) for c in comments ) , 
card_json [ "shortUrl" ] ) 
return annotations 
~~ def get_boards ( self ) : 
if 'include_boards' in self . config : 
~~~ for boardid in self . config . get ( 'include_boards' , to_type = aslist ) : 
~~~ yield self . api_request ( 
"/1/boards/{id}" . format ( id = boardid ) , fields = 'name' ) 
~~~ boards = self . api_request ( "/1/members/me/boards" , fields = 'name' ) 
for board in boards : 
~~~ yield board 
~~ ~~ ~~ def get_lists ( self , board ) : 
lists = self . api_request ( 
"/1/boards/{board_id}/lists/open" . format ( board_id = board ) , 
fields = 'name' ) 
include_lists = self . config . get ( 'include_lists' , to_type = aslist ) 
if include_lists : 
~~~ lists = [ l for l in lists if l [ 'name' ] in include_lists ] 
~~ exclude_lists = self . config . get ( 'exclude_lists' , to_type = aslist ) 
if exclude_lists : 
~~~ lists = [ l for l in lists if l [ 'name' ] not in exclude_lists ] 
~~ return lists 
~~ def get_cards ( self , list_id ) : 
params = { 'fields' : 'name,idShort,shortLink,shortUrl,url,labels,due' } 
member = self . config . get ( 'only_if_assigned' , None ) 
unassigned = self . config . get ( 'also_unassigned' , False , asbool ) 
if member is not None : 
~~~ params [ 'members' ] = 'true' 
params [ 'member_fields' ] = 'username' 
~~ cards = self . api_request ( 
"/1/lists/{list_id}/cards/open" . format ( list_id = list_id ) , 
** params ) 
for card in cards : 
~~~ if ( member is None 
or member in [ m [ 'username' ] for m in card [ 'members' ] ] 
or ( unassigned and not card [ 'members' ] ) ) : 
~~~ yield card 
~~ ~~ ~~ def get_comments ( self , card_id ) : 
params = { 'filter' : 'commentCard' , 'memberCreator_fields' : 'username' } 
comments = self . api_request ( 
"/1/cards/{card_id}/actions" . format ( card_id = card_id ) , 
for comment in comments : 
~~~ assert comment [ 'type' ] == 'commentCard' 
yield comment 
~~ ~~ def api_request ( self , url , ** params ) : 
params [ 'key' ] = self . config . get ( 'api_key' ) , 
params [ 'token' ] = self . config . get ( 'token' ) , 
url = "https://api.trello.com" + url 
return self . json_response ( requests . get ( url , params = params ) ) 
~~ def get_issues ( self , repo , keys ) : 
key1 , key2 = keys 
url = self . base_url + "/api/0/" + repo + "/" + key1 
response = self . session . get ( url , params = dict ( status = 'Open' ) ) 
if not bool ( response ) : 
~~~ error = response . json ( ) 
code = error [ 'error_code' ] 
if code == 'ETRACKERDISABLED' : 
~~ ~~ issues = [ ] 
for result in response . json ( ) [ key2 ] : 
~~~ idx = six . text_type ( result [ 'id' ] ) 
result [ 'html_url' ] = "/" . join ( [ self . base_url , repo , key3 , idx ] ) 
issues . append ( ( repo , result ) ) 
~~ return issues 
~~ def get_issue_generator ( self , user_id , project_id , project_name ) : 
user_tasks_data = self . call_api ( 
"/projects/" + six . text_type ( project_id ) + "/user-tasks" ) 
for key , task in enumerate ( user_tasks_data ) : 
~~~ assigned_task = self . get_task_dict ( project_id , key , task ) 
if assigned_task : 
~~~ log . debug ( 
yield assigned_task 
~~ ~~ ~~ def _api_url ( self , path , ** context ) : 
if self . host == 'github.com' : 
~~~ baseurl = "https://api.github.com" 
~~~ baseurl = "https://{}/api/v3" . format ( self . host ) 
~~ return baseurl + path . format ( ** context ) 
~~ def get_query ( self , query ) : 
url = self . _api_url ( 
"/search/issues?q={query}&per_page=100" , query = query ) 
return self . _getter ( url , subkey = 'items' ) 
~~ def _getter ( self , url , subkey = None ) : 
if 'basic' in self . auth : 
~~~ kwargs [ 'auth' ] = self . auth [ 'basic' ] 
link = dict ( next = url ) 
while 'next' in link : 
~~~ response = self . session . get ( link [ 'next' ] , ** kwargs ) 
if response . status_code == 404 and 'token' in self . auth : 
"access\ ) 
~~ json_res = self . json_response ( response ) 
if subkey is not None : 
~~~ json_res = json_res [ subkey ] 
~~ results += json_res 
link = self . _link_field_to_dict ( response . headers . get ( 'link' , None ) ) 
~~ def _link_field_to_dict ( field ) : 
if not field : 
~~~ return dict ( ) 
~~ return dict ( [ 
~~ def get_owned_repo_issues ( self , tag ) : 
issues = { } 
for issue in self . client . get_issues ( * tag . split ( '/' ) ) : 
~~~ issues [ issue [ 'url' ] ] = ( tag , issue ) 
for issue in self . client . get_query ( query ) : 
~~~ url = issue [ 'html_url' ] 
~~~ repo = self . get_repository_from_issue ( issue ) 
~~~ log . critical ( e ) 
~~~ issues [ url ] = ( repo , issue ) 
~~ ~~ return issues 
~~ def _reqs ( self , tag ) : 
( tag , i ) for i in 
self . client . get_pulls ( * tag . split ( '/' ) ) 
~~ def _aggregate_issues ( conf , main_section , target , queue , service_name ) : 
~~~ service = get_service ( service_name ) ( conf , main_section , target ) 
issue_count = 0 
for issue in service . issues ( ) : 
~~~ queue . put ( issue ) 
issue_count += 1 
~~ ~~ except SystemExit as e : 
~~~ log . critical ( str ( e ) ) 
queue . put ( ( SERVICE_FINISHED_ERROR , ( target , e ) ) ) 
~~ except BaseException as e : 
~~~ if hasattr ( e , 'request' ) and e . request : 
~~~ e . request . hooks = { } 
~~~ queue . put ( ( SERVICE_FINISHED_OK , ( target , issue_count , ) ) ) 
~~~ duration = time . time ( ) - start 
~~ ~~ def aggregate_issues ( conf , main_section , debug ) : 
targets = aslist ( conf . get ( main_section , 'targets' ) ) 
queue = multiprocessing . Queue ( ) 
processes = [ ] 
if debug : 
~~~ for target in targets : 
~~~ _aggregate_issues ( 
conf , 
main_section , 
target , 
queue , 
conf . get ( target , 'service' ) 
~~~ proc = multiprocessing . Process ( 
target = _aggregate_issues , 
args = ( conf , main_section , target , queue , conf . get ( target , 'service' ) ) 
proc . start ( ) 
processes . append ( proc ) 
~~ ~~ currently_running = len ( targets ) 
while currently_running > 0 : 
~~~ issue = queue . get ( True ) 
if isinstance ( issue , tuple ) : 
~~~ completion_type , args = issue 
if completion_type == SERVICE_FINISHED_ERROR : 
~~~ target , e = args 
for process in processes : 
~~~ process . terminate ( ) 
~~ currently_running -= 1 
~~ def _get_config_or_default ( self , key , default , as_type = lambda x : x ) : 
if self . main_config . has_option ( self . main_section , key ) : 
~~~ return as_type ( self . main_config . get ( self . main_section , key ) ) 
~~ return default 
~~ def get_templates ( self ) : 
templates = { } 
for key in six . iterkeys ( Task . FIELDS ) : 
~~~ template_key = '%s_template' % key 
if template_key in self . config : 
~~~ templates [ key ] = self . config . get ( template_key ) 
~~ ~~ return templates 
~~ def validate_config ( cls , service_config , target ) : 
if service_config . has_option ( target , 'only_if_assigned' ) : 
"\ % ( target , cls . CONFIG_PREFIX ) ) 
~~ if service_config . has_option ( target , 'also_unassigned' ) : 
~~ if service_config . has_option ( target , 'default_priority' ) : 
~~ if service_config . has_option ( target , 'add_tags' ) : 
~~ ~~ def include ( self , issue ) : 
only_if_assigned = self . config . get ( 'only_if_assigned' , None ) 
if only_if_assigned : 
~~~ owner = self . get_owner ( issue ) 
include_owners = [ only_if_assigned ] 
if self . config . get ( 'also_unassigned' , None , asbool ) : 
~~~ include_owners . append ( None ) 
~~ return owner in include_owners 
~~ only_if_author = self . config . get ( 'only_if_author' , None ) 
if only_if_author : 
~~~ return self . get_author ( issue ) == only_if_author 
~~ def make_table ( grid ) : 
cell_width = 2 + max ( 
reduce ( 
lambda x , y : x + y , [ [ len ( item ) for item in row ] for row in grid ] , [ ] 
num_cols = len ( grid [ 0 ] ) 
rst = table_div ( num_cols , cell_width , 0 ) 
header_flag = 1 
for row in grid : 
[ normalize_cell ( x , cell_width - 1 ) for x in row ] 
) + '|\\n' 
rst = rst + table_div ( num_cols , cell_width , header_flag ) 
header_flag = 0 
~~ return rst 
~~ def get_service_password ( service , username , oracle = None , interactive = False ) : 
password = None 
if not oracle or oracle == "@oracle:use_keyring" : 
~~~ keyring = get_keyring ( ) 
password = keyring . get_password ( service , username ) 
if interactive and password is None : 
~~~ oracle = "@oracle:ask_password" 
password = get_service_password ( service , username , 
oracle , interactive = True ) 
if password : 
~~~ keyring . set_password ( service , username , password ) 
~~ ~~ ~~ elif interactive and oracle == "@oracle:ask_password" : 
password = getpass . getpass ( prompt ) 
~~ elif oracle . startswith ( '@oracle:eval:' ) : 
~~~ command = oracle [ 13 : ] 
return oracle_eval ( command ) 
~~ if password is None : 
( oracle , interactive , service ) ) 
~~ return password 
~~ def oracle_eval ( command ) : 
p = subprocess . Popen ( 
command , shell = True , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) 
p . wait ( ) 
if p . returncode == 0 : 
~~~ return p . stdout . readline ( ) . strip ( ) . decode ( 'utf-8' ) 
~~~ die ( 
command = command , error = p . stderr . read ( ) . strip ( ) ) ) 
~~ ~~ def get_config_path ( ) : 
if os . environ . get ( BUGWARRIORRC ) : 
~~~ return os . environ [ BUGWARRIORRC ] 
~~ xdg_config_home = ( 
os . environ . get ( 'XDG_CONFIG_HOME' ) or os . path . expanduser ( '~/.config' ) ) 
xdg_config_dirs = ( 
( os . environ . get ( 'XDG_CONFIG_DIRS' ) or '/etc/xdg' ) . split ( ':' ) ) 
paths = [ 
os . path . join ( xdg_config_home , 'bugwarrior' , 'bugwarriorrc' ) , 
os . path . expanduser ( "~/.bugwarriorrc" ) ] 
paths += [ 
os . path . join ( d , 'bugwarrior' , 'bugwarriorrc' ) for d in xdg_config_dirs ] 
for path in paths : 
~~~ return path 
~~ ~~ return paths [ 0 ] 
~~ def fix_logging_path ( config , main_section ) : 
log_file = config . get ( main_section , 'log.file' ) 
if log_file : 
~~~ log_file = os . path . expanduser ( os . path . expandvars ( log_file ) ) 
if os . path . isabs ( log_file ) : 
~~~ log_file = os . path . relpath ( log_file ) 
~~ ~~ return log_file 
~~ def getint ( self , section , option ) : 
~~~ return super ( BugwarriorConfigParser , self ) . getint ( section , option ) 
~~~ if self . get ( section , option ) == u'' : 
section = section , option = option ) ) 
~~ ~~ ~~ def _get_bug_attr ( bug , attr ) : 
if attr in ( "longdescs" , "flags" ) : 
~~~ return getattr ( bug , attr , [ ] ) 
~~ return getattr ( bug , attr ) 
~~ def pull ( dry_run , flavor , interactive , debug ) : 
~~~ main_section = _get_section_name ( flavor ) 
config = _try_load_config ( main_section , interactive ) 
lockfile_path = os . path . join ( get_data_path ( config , main_section ) , 
'bugwarrior.lockfile' ) 
lockfile = PIDLockFile ( lockfile_path ) 
lockfile . acquire ( timeout = 10 ) 
~~~ issue_generator = aggregate_issues ( config , main_section , debug ) 
synchronize ( issue_generator , config , main_section , dry_run ) 
~~~ lockfile . release ( ) 
~~ ~~ except LockTimeout : 
~~~ log . critical ( 
lockfile_path 
~~ except RuntimeError as e : 
~~ ~~ def get_data ( self , url ) : 
return self . json_response ( requests . get ( url , ** self . requests_kwargs ) ) 
~~ def get_collection ( self , url ) : 
url = self . BASE_API2 + url 
while url is not None : 
~~~ response = self . get_data ( url ) 
for value in response [ 'values' ] : 
~~~ yield value 
~~ url = response . get ( 'next' , None ) 
~~ ~~ def hamdist ( str1 , str2 ) : 
diffs = 0 
for ch1 , ch2 in zip ( str1 , str2 ) : 
~~~ if ch1 != ch2 : 
~~~ diffs += 1 
~~ ~~ return diffs 
~~ def find_local_uuid ( tw , keys , issue , legacy_matching = False ) : 
if not issue [ 'description' ] : 
~~ possibilities = set ( [ ] ) 
if legacy_matching : 
~~~ legacy_description = issue . get_default_description ( ) . rsplit ( '..' , 1 ) [ 0 ] 
legacy_description = legacy_description . split ( "\ ) [ 0 ] 
results = tw . filter_tasks ( { 
'description.startswith' : legacy_description , 
'or' : [ 
( 'status' , 'pending' ) , 
( 'status' , 'waiting' ) , 
possibilities = possibilities | set ( [ 
task [ 'uuid' ] for task in results 
~~ for service , key_list in six . iteritems ( keys ) : 
~~~ if any ( [ key in issue for key in key_list ] ) : 
~~~ results = tw . filter_tasks ( { 
'and' : [ ( "%s.is" % key , issue [ key ] ) for key in key_list ] , 
~~ ~~ if len ( possibilities ) == 1 : 
~~~ return possibilities . pop ( ) 
~~ if len ( possibilities ) > 1 : 
~~~ raise MultipleMatches ( 
issue [ 'description' ] , 
possibilities 
~~ raise NotFound ( 
~~ def merge_left ( field , local_task , remote_issue , hamming = False ) : 
local_field = local_task . get ( field , [ ] ) 
remote_field = remote_issue . get ( field , [ ] ) 
if field not in local_task : 
~~~ local_task [ field ] = [ ] 
~~ new_count = 0 
for remote in remote_field : 
~~~ for local in local_field : 
hamming 
and get_annotation_hamming_distance ( remote , local ) == 0 
or ( 
remote == local 
local_task [ field ] . append ( remote ) 
new_count += 1 
~~ ~~ if new_count > 0 : 
new_count , field , len ( local_task [ field ] ) , ) ) 
~~ ~~ def build_uda_config_overrides ( targets ) : 
from bugwarrior . services import get_service 
targets_udas = { } 
for target in targets : 
~~~ targets_udas . update ( get_service ( target ) . ISSUE_CLASS . UDAS ) 
'uda' : targets_udas 
~~ def _parse_sprint_string ( sprint ) : 
entries = sprint [ sprint . index ( '[' ) + 1 : sprint . index ( ']' ) ] . split ( '=' ) 
fields = sum ( ( entry . rsplit ( ',' , 1 ) for entry in entries ) , [ ] ) 
return dict ( zip ( fields [ : : 2 ] , fields [ 1 : : 2 ] ) ) 
~~ def get_credentials ( self ) : 
with self . AUTHENTICATION_LOCK : 
store = oauth2client . file . Storage ( self . credentials_path ) 
credentials = store . get ( ) 
if not credentials or credentials . invalid : 
flow = oauth2client . client . flow_from_clientsecrets ( self . client_secret_path , self . SCOPES ) 
flow . user_agent = self . APPLICATION_NAME 
flags = oauth2client . tools . argparser . parse_args ( [ ] ) 
credentials = oauth2client . tools . run_flow ( flow , store , flags ) 
~~ return credentials 
~~ ~~ def multi_rouge_n ( sequences , scores_ids , n = 2 ) : 
ngrams = [ _get_word_ngrams ( n , sequence ) for sequence in sequences ] 
counts = [ len ( ngram ) for ngram in ngrams ] 
scores = [ ] 
for hyp_id , ref_id in scores_ids : 
~~~ evaluated_ngrams = ngrams [ hyp_id ] 
evaluated_count = counts [ hyp_id ] 
reference_ngrams = ngrams [ ref_id ] 
reference_count = counts [ ref_id ] 
overlapping_ngrams = evaluated_ngrams . intersection ( reference_ngrams ) 
overlapping_count = len ( overlapping_ngrams ) 
scores += [ f_r_p_rouge_n ( evaluated_count , 
reference_count , overlapping_count ) ] 
~~ return scores 
~~ def rouge_n ( evaluated_sentences , reference_sentences , n = 2 ) : 
if len ( evaluated_sentences ) <= 0 or len ( reference_sentences ) <= 0 : 
~~ evaluated_ngrams = _get_word_ngrams ( n , evaluated_sentences ) 
reference_ngrams = _get_word_ngrams ( n , reference_sentences ) 
reference_count = len ( reference_ngrams ) 
evaluated_count = len ( evaluated_ngrams ) 
return f_r_p_rouge_n ( evaluated_count , reference_count , overlapping_count ) 
~~ def _union_lcs ( evaluated_sentences , reference_sentence , prev_union = None ) : 
if prev_union is None : 
~~~ prev_union = set ( ) 
~~ if len ( evaluated_sentences ) <= 0 : 
~~ lcs_union = prev_union 
prev_count = len ( prev_union ) 
reference_words = _split_into_words ( [ reference_sentence ] ) 
combined_lcs_length = 0 
for eval_s in evaluated_sentences : 
~~~ evaluated_words = _split_into_words ( [ eval_s ] ) 
lcs = set ( _recon_lcs ( reference_words , evaluated_words ) ) 
combined_lcs_length += len ( lcs ) 
lcs_union = lcs_union . union ( lcs ) 
~~ new_lcs_count = len ( lcs_union ) - prev_count 
return new_lcs_count , lcs_union 
~~ def rouge_l_summary_level ( evaluated_sentences , reference_sentences ) : 
~~ m = len ( set ( _split_into_words ( reference_sentences ) ) ) 
n = len ( set ( _split_into_words ( evaluated_sentences ) ) ) 
union_lcs_sum_across_all_references = 0 
union = set ( ) 
for ref_s in reference_sentences : 
~~~ lcs_count , union = _union_lcs ( evaluated_sentences , 
ref_s , 
prev_union = union ) 
union_lcs_sum_across_all_references += lcs_count 
~~ llcs = union_lcs_sum_across_all_references 
r_lcs = llcs / m 
p_lcs = llcs / n 
beta = p_lcs / ( r_lcs + 1e-12 ) 
num = ( 1 + ( beta ** 2 ) ) * r_lcs * p_lcs 
denom = r_lcs + ( ( beta ** 2 ) * p_lcs ) 
f_lcs = num / ( denom + 1e-12 ) 
return { "f" : f_lcs , "p" : p_lcs , "r" : r_lcs } 
~~ def get_scores ( self , avg = False , ignore_empty = False ) : 
hyp_path , ref_path = self . hyp_path , self . ref_path 
with io . open ( hyp_path , encoding = "utf-8" , mode = "r" ) as hyp_file : 
~~~ hyps = [ line [ : - 1 ] for line in hyp_file ] 
~~ with io . open ( ref_path , encoding = "utf-8" , mode = "r" ) as ref_file : 
~~~ refs = [ line [ : - 1 ] for line in ref_file ] 
~~ return self . rouge . get_scores ( hyps , refs , avg = avg , 
ignore_empty = ignore_empty ) 
~~ def calc_pvalues ( query , gene_sets , background = 20000 , ** kwargs ) : 
k = len ( query ) 
query = set ( query ) 
vals = [ ] 
if isinstance ( background , set ) : 
query = query . intersection ( background ) 
~~ elif isinstance ( background , int ) : 
~~~ bg = background 
~~ subsets = sorted ( gene_sets . keys ( ) ) 
for s in subsets : 
~~~ category = gene_sets . get ( s ) 
m = len ( category ) 
hits = query . intersection ( set ( category ) ) 
x = len ( hits ) 
if x < 1 : continue 
vals . append ( ( s , hypergeom . sf ( x - 1 , bg , m , k ) , x , m , hits ) ) 
~~ return zip ( * vals ) 
~~ def fdrcorrection ( pvals , alpha = 0.05 ) : 
pvals = np . asarray ( pvals ) 
pvals_sortind = np . argsort ( pvals ) 
pvals_sorted = np . take ( pvals , pvals_sortind ) 
ecdffactor = _ecdf ( pvals_sorted ) 
reject = pvals_sorted <= ecdffactor * alpha 
if reject . any ( ) : 
~~~ rejectmax = max ( np . nonzero ( reject ) [ 0 ] ) 
reject [ : rejectmax ] = True 
~~ pvals_corrected_raw = pvals_sorted / ecdffactor 
pvals_corrected = np . minimum . accumulate ( pvals_corrected_raw [ : : - 1 ] ) [ : : - 1 ] 
del pvals_corrected_raw 
pvals_corrected [ pvals_corrected > 1 ] = 1 
pvals_corrected_ = np . empty_like ( pvals_corrected ) 
pvals_corrected_ [ pvals_sortind ] = pvals_corrected 
del pvals_corrected 
reject_ = np . empty_like ( reject ) 
reject_ [ pvals_sortind ] = reject 
return reject_ , pvals_corrected_ 
~~ def zscore ( data2d , axis = 0 ) : 
~~~ return data2d 
~~ assert axis in [ 0 , 1 ] 
z_scored = data2d . apply ( lambda x : ( x - x . mean ( ) ) / x . std ( ddof = 1 ) , 
axis = operator . xor ( 1 , axis ) ) 
return z_scored 
~~ def heatmap ( df , z_score = None , title = '' , figsize = ( 5 , 5 ) , cmap = 'RdBu_r' , 
xticklabels = True , yticklabels = True , ofname = None , ** kwargs ) : 
df = zscore ( df , axis = z_score ) 
df = df . iloc [ : : - 1 ] 
ny , nx = df . shape 
xticks = np . arange ( 0 , nx , 1 ) + .5 
yticks = np . arange ( 0 , ny , 1 ) + .5 
if hasattr ( sys , 'ps1' ) and ( ofname is None ) : 
~~~ fig = plt . figure ( figsize = figsize ) 
~~~ fig = Figure ( figsize = figsize ) 
canvas = FigureCanvas ( fig ) 
~~ ax = fig . add_subplot ( 111 ) 
vmin = np . percentile ( df . min ( ) , 2 ) 
vmax = np . percentile ( df . max ( ) , 98 ) 
matrix = ax . pcolormesh ( df . values , cmap = cmap , vmin = vmin , vmax = vmax ) 
ax . set_ylim ( [ 0 , len ( df ) ] ) 
ax . set ( xticks = xticks , yticks = yticks ) 
ax . set_xticklabels ( df . columns . values if xticklabels else '' , fontsize = 14 , rotation = 90 ) 
ax . set_yticklabels ( df . index . values if yticklabels else '' , fontsize = 14 ) 
ax . tick_params ( axis = 'both' , which = 'both' , bottom = False , top = False , 
right = False , left = False ) 
cbar = colorbar ( matrix ) 
cbar . ax . tick_params ( axis = 'both' , which = 'both' , bottom = False , top = False , 
for side in [ "top" , "right" , "left" , "bottom" ] : 
~~~ ax . spines [ side ] . set_visible ( False ) 
cbar . ax . spines [ side ] . set_visible ( False ) 
~~ if ofname is not None : 
~~~ fig . savefig ( ofname , bbox_inches = 'tight' , dpi = 300 ) 
~~ def gseaplot ( rank_metric , term , hits_indices , nes , pval , fdr , RES , 
pheno_pos = '' , pheno_neg = '' , figsize = ( 6 , 5.5 ) , 
cmap = 'seismic' , ofname = None , ** kwargs ) : 
norm = _MidpointNormalize ( midpoint = 0 ) 
x = np . arange ( len ( rank_metric ) ) 
rankings = rank_metric . values 
zero_score_ind = np . abs ( rankings ) . argmin ( ) 
im_matrix = np . tile ( rankings , ( 2 , 1 ) ) 
plt . rcParams . update ( { 'pdf.fonttype' : 42 , 'ps.fonttype' : 42 } ) 
gs = plt . GridSpec ( 16 , 1 ) 
~~ ax1 = fig . add_subplot ( gs [ 11 : ] ) 
module = 'tmp' if ofname is None else ofname . split ( "." ) [ - 2 ] 
if module == 'ssgsea' : 
ax1 . fill_between ( x , y1 = np . log ( rankings ) , y2 = 0 , color = '#C9D3DB' ) 
~~~ ax1 . fill_between ( x , y1 = rankings , y2 = 0 , color = '#C9D3DB' ) 
~~ ax1 . text ( .05 , .9 , phenoP_label , color = 'red' , 
horizontalalignment = 'left' , verticalalignment = 'top' , 
transform = ax1 . transAxes ) 
ax1 . text ( .95 , .05 , phenoN_label , color = 'Blue' , 
horizontalalignment = 'right' , verticalalignment = 'bottom' , 
trans1 = transforms . blended_transform_factory ( ax1 . transData , ax1 . transAxes ) 
if module != 'ssgsea' : 
~~~ ax1 . vlines ( zero_score_ind , 0 , 1 , linewidth = .5 , transform = trans1 , linestyles = '--' , color = 'grey' ) 
ax1 . text ( zero_score_ind , 0.5 , z_score_label , 
transform = trans1 ) 
ax1 . spines [ 'top' ] . set_visible ( False ) 
ax1 . tick_params ( axis = 'both' , which = 'both' , top = False , right = False , left = False ) 
ax1 . locator_params ( axis = 'y' , nbins = 5 ) 
ax1 . yaxis . set_major_formatter ( plt . FuncFormatter ( lambda tick_loc , tick_num : '{:.1f}' . format ( tick_loc ) ) ) 
ax2 = fig . add_subplot ( gs [ 8 : 10 ] , sharex = ax1 ) 
trans2 = transforms . blended_transform_factory ( ax2 . transData , ax2 . transAxes ) 
ax2 . vlines ( hits_indices , 0 , 1 , linewidth = .5 , transform = trans2 ) 
ax2 . spines [ 'bottom' ] . set_visible ( False ) 
ax2 . tick_params ( axis = 'both' , which = 'both' , bottom = False , top = False , 
labelbottom = False , right = False , left = False , labelleft = False ) 
ax3 = fig . add_subplot ( gs [ 10 ] , sharex = ax1 ) 
ax3 . spines [ 'bottom' ] . set_visible ( False ) 
ax3 . tick_params ( axis = 'both' , which = 'both' , bottom = False , top = False , 
ax4 = fig . add_subplot ( gs [ : 8 ] , sharex = ax1 ) 
ax4 . plot ( x , RES , linewidth = 4 , color = '#88C544' ) 
ax4 . text ( .1 , .1 , fdr_label , transform = ax4 . transAxes ) 
ax4 . text ( .1 , .2 , pval_label , transform = ax4 . transAxes ) 
ax4 . text ( .1 , .3 , nes_label , transform = ax4 . transAxes ) 
trans4 = transforms . blended_transform_factory ( ax4 . transAxes , ax4 . transData ) 
ax4 . hlines ( 0 , 0 , 1 , linewidth = .5 , transform = trans4 , color = 'grey' ) 
ax4 . set_xlim ( min ( x ) , max ( x ) ) 
ax4 . tick_params ( axis = 'both' , which = 'both' , bottom = False , top = False , labelbottom = False , right = False ) 
ax4 . locator_params ( axis = 'y' , nbins = 5 ) 
ax4 . yaxis . set_major_formatter ( plt . FuncFormatter ( lambda tick_loc , tick_num : '{:.1f}' . format ( tick_loc ) ) ) 
fig . suptitle ( term , fontsize = 16 , fontweight = 'bold' ) 
fig . subplots_adjust ( hspace = 0 ) 
if ofname is not None : 
sizes = None , norm = None , legend = True , figsize = ( 6 , 5.5 ) , 
cmap = 'RdBu_r' , ofname = None , ** kwargs ) : 
colname = column 
~~~ can_be_coerced = df [ colname ] . map ( isfloat ) 
if np . sum ( ~ can_be_coerced ) > 0 : 
~~~ df . loc [ : , colname ] = df [ colname ] . map ( float ) 
~~ df = df [ df [ colname ] <= cutoff ] 
if len ( df ) < 1 : 
~~ df = df . assign ( logAP = lambda x : - x [ colname ] . apply ( np . log10 ) ) 
colname = 'logAP' 
~~ df = df . sort_values ( by = colname ) . iloc [ - top_term : , : ] 
temp = df [ 'Overlap' ] . str . split ( "/" , expand = True ) . astype ( int ) 
df = df . assign ( Hits = temp . iloc [ : , 0 ] , Background = temp . iloc [ : , 1 ] ) 
df = df . assign ( Hits_ratio = lambda x : x . Hits / x . Background ) 
x = df . loc [ : , colname ] . values 
y = [ i for i in range ( 0 , len ( df ) ) ] 
ylabels = df [ 'Term' ] . values 
levels = numbers = np . sort ( df . Hits . unique ( ) ) 
if norm is None : 
~~~ norm = Normalize ( ) 
~~ elif isinstance ( norm , tuple ) : 
~~~ norm = Normalize ( * norm ) 
~~ elif not isinstance ( norm , Normalize ) : 
raise ValueError ( err ) 
~~ min_width , max_width = np . r_ [ 20 , 100 ] * plt . rcParams [ "lines.linewidth" ] 
norm . clip = True 
if not norm . scaled ( ) : 
~~~ norm ( np . asarray ( numbers ) ) 
~~ size_limits = norm . vmin , norm . vmax 
scl = norm ( numbers ) 
widths = np . asarray ( min_width + scl * ( max_width - min_width ) ) 
if scl . mask . any ( ) : 
~~~ widths [ scl . mask ] = 0 
~~ sizes = dict ( zip ( levels , widths ) ) 
df [ 'sizes' ] = df . Hits . map ( sizes ) 
area = df [ 'sizes' ] . values 
~~~ fig , ax = plt . subplots ( figsize = figsize ) 
ax = fig . add_subplot ( 111 ) 
~~ vmin = np . percentile ( combined_score . min ( ) , 2 ) 
vmax = np . percentile ( combined_score . max ( ) , 98 ) 
sc = ax . scatter ( x = x , y = y , s = area , edgecolors = 'face' , c = combined_score , 
cmap = cmap , vmin = vmin , vmax = vmax ) 
~~~ xlabel = "-log$_{10}$(%s)" % column 
~~~ xlabel = column 
~~ ax . set_xlabel ( xlabel , fontsize = 14 , fontweight = 'bold' ) 
ax . yaxis . set_major_locator ( plt . FixedLocator ( y ) ) 
ax . yaxis . set_major_formatter ( plt . FixedFormatter ( ylabels ) ) 
ax . set_yticklabels ( ylabels , fontsize = 16 ) 
ax . grid ( ) 
cax = fig . add_axes ( [ 0.95 , 0.20 , 0.03 , 0.22 ] ) 
cbar = fig . colorbar ( sc , cax = cax , ) 
cbar . ax . tick_params ( right = True ) 
cbar . ax . set_title ( 'Combined\\nScore' , loc = 'left' , fontsize = 12 ) 
if len ( df ) >= 3 : 
~~~ idx = [ area . argmax ( ) , np . abs ( area - area . mean ( ) ) . argmin ( ) , area . argmin ( ) ] 
idx = unique ( idx ) 
~~~ idx = df . index . values 
~~ label = df . iloc [ idx , df . columns . get_loc ( 'Hits' ) ] 
~~~ handles , _ = ax . get_legend_handles_labels ( ) 
legend_markers = [ ] 
for ix in idx : 
~~~ legend_markers . append ( ax . scatter ( [ ] , [ ] , s = area [ ix ] , c = 'b' ) ) 
~~ ax . legend ( legend_markers , label , title = 'Hits' ) 
~~ ax . set_title ( title , fontsize = 20 , fontweight = 'bold' ) 
~~ return ax 
figsize = ( 6.5 , 6 ) , color = 'salmon' , ofname = None , ** kwargs ) : 
~~~ df = df [ df [ colname ] <= cutoff ] 
~~ dd = df . sort_values ( by = colname ) . iloc [ - top_term : , : ] 
bar = dd . plot . barh ( x = 'Term' , y = colname , color = color , 
alpha = 0.75 , fontsize = 16 , ax = ax ) 
~~ bar . set_xlabel ( xlabel , fontsize = 16 , fontweight = 'bold' ) 
bar . set_ylabel ( "" ) 
bar . set_title ( title , fontsize = 24 , fontweight = 'bold' ) 
bar . xaxis . set_major_locator ( MaxNLocator ( integer = True ) ) 
bar . legend_ . remove ( ) 
adjust_spines ( ax , spines = [ 'left' , 'bottom' ] ) 
~~ def adjust_spines ( ax , spines ) : 
for loc , spine in ax . spines . items ( ) : 
~~~ if loc in spines : 
~~ ~~ if 'left' in spines : 
~~~ ax . yaxis . set_ticks_position ( 'left' ) 
~~~ ax . yaxis . set_ticks ( [ ] ) 
~~ if 'bottom' in spines : 
~~~ ax . xaxis . set_ticks_position ( 'bottom' ) 
~~~ ax . xaxis . set_ticks ( [ ] ) 
~~ ~~ def main ( ) : 
argparser = prepare_argparser ( ) 
args = argparser . parse_args ( ) 
subcommand = args . subcommand_name 
if subcommand == "replot" : 
~~~ from . gsea import Replot 
rep = Replot ( indir = args . indir , outdir = args . outdir , weighted_score_type = args . weight , 
figsize = args . figsize , graph_num = args . graph , 
format = args . format , verbose = args . verbose ) 
rep . run ( ) 
~~ elif subcommand == "gsea" : 
~~~ from . gsea import GSEA 
gs = GSEA ( args . data , args . gmt , args . cls , args . outdir , 
args . mins , args . maxs , args . n , args . weight , 
args . type , args . method , args . ascending , args . threads , 
args . figsize , args . format , args . graph , args . noplot , args . seed , args . verbose ) 
gs . run ( ) 
~~ elif subcommand == "prerank" : 
~~~ from . gsea import Prerank 
pre = Prerank ( args . rnk , args . gmt , args . outdir , args . label [ 0 ] , args . label [ 1 ] , 
args . mins , args . maxs , args . n , args . weight , args . ascending , args . threads , 
pre . run ( ) 
~~ elif subcommand == "ssgsea" : 
~~~ from . gsea import SingleSampleGSEA 
ss = SingleSampleGSEA ( data = args . data , gene_sets = args . gmt , outdir = args . outdir , 
sample_norm_method = args . norm , 
min_size = args . mins , max_size = args . maxs , permutation_num = args . n , 
weighted_score_type = args . weight , scale = args . scale , 
ascending = args . ascending , processes = args . threads , 
figsize = args . figsize , format = args . format , graph_num = args . graph , 
no_plot = args . noplot , seed = args . seed , verbose = args . verbose ) 
ss . run ( ) 
~~ elif subcommand == "enrichr" : 
~~~ from . enrichr import Enrichr 
enr = Enrichr ( gene_list = args . gene_list , descriptions = args . descrip , 
gene_sets = args . library , organism = args . organism , 
outdir = args . outdir , format = args . format , cutoff = args . thresh , 
background = args . bg , figsize = args . figsize , 
top_term = args . term , no_plot = args . noplot , verbose = args . verbose ) 
enr . run ( ) 
~~ elif subcommand == "biomart" : 
~~~ from . parser import Biomart 
name , value = args . filter 
if os . path . isfile ( value ) : 
~~~ with open ( value , 'r' ) as val : 
~~~ lines = val . readlines ( ) 
~~ value = [ l . strip ( ) for l in lines ] 
~~ bm = Biomart ( host = args . host , verbose = args . verbose ) 
bm . query ( dataset = args . bg , attributes = args . attrs . split ( "," ) , 
filters = { name : value } , filename = args . ofile ) 
~~~ argparser . print_help ( ) 
sys . exit ( 0 ) 
~~ ~~ def prepare_argparser ( ) : 
argparser = ap . ArgumentParser ( description = description , epilog = epilog ) 
add_gsea_parser ( subparsers ) 
add_prerank_parser ( subparsers ) 
add_singlesample_parser ( subparsers ) 
add_plot_parser ( subparsers ) 
add_enrichr_parser ( subparsers ) 
add_biomart_parser ( subparsers ) 
return argparser 
~~ def add_output_option ( parser ) : 
parser . add_argument ( "-o" , "--outdir" , dest = "outdir" , type = str , default = 'GSEApy_reports' , 
metavar = '' , action = "store" , 
parser . add_argument ( "-f" , "--format" , dest = "format" , type = str , metavar = '' , action = "store" , 
choices = ( "pdf" , "png" , "jpeg" , "eps" , "svg" ) , default = "pdf" , 
parser . add_argument ( "--fs" , "--figsize" , action = 'store' , nargs = 2 , dest = 'figsize' , 
metavar = ( 'width' , 'height' ) , type = float , default = ( 6.5 , 6 ) , 
parser . add_argument ( "--graph" , dest = "graph" , action = "store" , type = int , default = 20 , metavar = 'int' , 
parser . add_argument ( "--no-plot" , action = 'store_true' , dest = 'noplot' , default = False , 
parser . add_argument ( "-v" , "--verbose" , action = "store_true" , default = False , dest = 'verbose' , 
~~ def add_output_group ( parser , required = True ) : 
output_group = parser . add_mutually_exclusive_group ( required = required ) 
output_group . add_argument ( "-o" , "--ofile" , dest = "ofile" , type = str , default = 'GSEApy_reports' , 
output_group . add_argument ( "--o-prefix" , dest = "ofile" , type = str , default = 'GSEApy_reports' , 
~~ def add_gsea_parser ( subparsers ) : 
group_input . add_argument ( "-d" , "--data" , dest = "data" , action = "store" , type = str , required = True , 
group_input . add_argument ( "-c" , "--cls" , dest = "cls" , action = "store" , type = str , required = True , 
group_input . add_argument ( "-g" , "--gmt" , dest = "gmt" , action = "store" , type = str , required = True , 
group_input . add_argument ( "-t" , "--permu-type" , action = "store" , dest = "type" , type = str , metavar = 'perType' , 
choices = ( "gene_set" , "phenotype" ) , default = "gene_set" , 
add_output_option ( group_output ) 
group_opt . add_argument ( "-n" , "--permu-num" , dest = "n" , action = "store" , type = int , default = 1000 , metavar = 'nperm' , 
group_opt . add_argument ( "--min-size" , dest = "mins" , action = "store" , type = int , default = 15 , metavar = 'int' , 
group_opt . add_argument ( "--max-size" , dest = "maxs" , action = "store" , type = int , default = 500 , metavar = 'int' , 
group_opt . add_argument ( "-w" , "--weight" , action = 'store' , dest = 'weight' , default = 1.0 , type = float , metavar = 'float' , 
group_opt . add_argument ( "-m" , "--method" , action = "store" , dest = "method" , type = str , metavar = '' , 
choices = ( "signal_to_noise" , "t_test" , "ratio_of_classes" , "diff_of_classes" , "log2_ratio_of_classes" ) , 
default = "log2_ratio_of_classes" , 
group_opt . add_argument ( "-a" , "--ascending" , action = 'store_true' , dest = 'ascending' , default = False , 
group_opt . add_argument ( "-s" , "--seed" , dest = "seed" , action = "store" , type = int , default = None , metavar = '' , 
group_opt . add_argument ( "-p" , "--threads" , dest = "threads" , action = "store" , type = int , default = 1 , metavar = 'procs' , 
~~ def add_prerank_parser ( subparsers ) : 
prerank_input . add_argument ( "-r" , "--rnk" , dest = "rnk" , action = "store" , type = str , required = True , 
prerank_input . add_argument ( "-g" , "--gmt" , dest = "gmt" , action = "store" , type = str , required = True , 
prerank_input . add_argument ( "-l" , "--label" , action = 'store' , nargs = 2 , dest = 'label' , 
metavar = ( 'pos' , 'neg' ) , type = str , default = ( 'Pos' , 'Neg' ) , 
add_output_option ( prerank_output ) 
prerank_opt . add_argument ( "-n" , "--permu-num" , dest = "n" , action = "store" , type = int , default = 1000 , metavar = 'nperm' , 
prerank_opt . add_argument ( "--min-size" , dest = "mins" , action = "store" , type = int , default = 15 , metavar = 'int' , 
prerank_opt . add_argument ( "--max-size" , dest = "maxs" , action = "store" , type = int , default = 500 , metavar = 'int' , 
prerank_opt . add_argument ( "-w" , "--weight" , action = 'store' , dest = 'weight' , default = 1.0 , type = float , metavar = 'float' , 
prerank_opt . add_argument ( "-a" , "--ascending" , action = 'store_true' , dest = 'ascending' , default = False , 
prerank_opt . add_argument ( "-s" , "--seed" , dest = "seed" , action = "store" , type = int , default = None , metavar = '' , 
prerank_opt . add_argument ( "-p" , "--threads" , dest = "threads" , action = "store" , type = int , default = 1 , metavar = 'procs' , 
~~ def add_plot_parser ( subparsers ) : 
group_replot . add_argument ( "-i" , "--indir" , action = "store" , dest = "indir" , required = True , metavar = 'GSEA_dir' , 
add_output_option ( group_replot ) 
group_replot . add_argument ( "-w" , "--weight" , action = 'store' , dest = 'weight' , default = 1.0 , type = float , metavar = 'float' , 
~~ def add_enrichr_parser ( subparsers ) : 
enrichr_opt . add_argument ( "-i" , "--input-list" , action = "store" , dest = "gene_list" , type = str , required = True , metavar = 'IDs' , 
enrichr_opt . add_argument ( "-g" , "--gene-sets" , action = "store" , dest = "library" , type = str , required = True , metavar = 'GMT' , 
enrichr_opt . add_argument ( "--org" , "--organism" , action = "store" , dest = "organism" , type = str , default = '' , 
enrichr_opt . add_argument ( "--ds" , "--description" , action = "store" , dest = "descrip" , type = str , default = 'enrichr' , metavar = 'STRING' , 
enrichr_opt . add_argument ( "--cut" , "--cut-off" , action = "store" , dest = "thresh" , metavar = 'float' , type = float , default = 0.05 , 
enrichr_opt . add_argument ( "--bg" , "--background" , action = "store" , dest = "bg" , default = 'hsapiens_gene_ensembl' , metavar = 'BGNUM' , 
enrichr_opt . add_argument ( "-t" , "--top-term" , dest = "term" , action = "store" , type = int , default = 10 , metavar = 'int' , 
add_output_option ( enrichr_output ) 
~~ def add_biomart_parser ( subparsers ) : 
biomart_opt . add_argument ( "-f" , "--filter" , action = 'store' , nargs = 2 , dest = 'filter' , 
required = True , metavar = ( 'NAME' , 'VALUE' ) , 
biomart_opt . add_argument ( "-a" , "--attributes" , action = "store" , dest = "attrs" , type = str , required = True , metavar = 'ATTR' , 
biomart_opt . add_argument ( "-d" , "--dataset" , action = "store" , dest = "bg" , type = str , default = 'hsapiens_gene_ensembl' , metavar = 'DATA' , 
biomart_opt . add_argument ( "--host" , action = "store" , dest = "host" , type = str , default = 'www.ensembl.org' , metavar = 'HOST' , 
biomart_opt . add_argument ( "-m" , "--mart" , action = "store" , dest = "mart" , type = str , metavar = 'MART' , 
biomart_opt . add_argument ( "-v" , "--verbose" , action = "store_true" , default = False , dest = 'verbose' , 
~~ def enrichment_score ( gene_list , correl_vector , gene_set , weighted_score_type = 1 , 
nperm = 1000 , rs = np . random . RandomState ( ) , single = False , scale = False ) : 
N = len ( gene_list ) 
if weighted_score_type == 0 : 
~~~ correl_vector = np . repeat ( 1 , N ) 
~~~ correl_vector = np . abs ( correl_vector ) ** weighted_score_type 
~~ hit_ind = np . flatnonzero ( tag_indicator ) . tolist ( ) 
axis = 1 
tag_indicator = np . tile ( tag_indicator , ( nperm + 1 , 1 ) ) 
correl_vector = np . tile ( correl_vector , ( nperm + 1 , 1 ) ) 
for i in range ( nperm ) : rs . shuffle ( tag_indicator [ i ] ) 
Nhint = tag_indicator . sum ( axis = axis , keepdims = True ) 
sum_correl_tag = np . sum ( correl_vector * tag_indicator , axis = axis , keepdims = True ) 
no_tag_indicator = 1 - tag_indicator 
Nmiss = N - Nhint 
norm_tag = 1.0 / sum_correl_tag 
norm_no_tag = 1.0 / Nmiss 
RES = np . cumsum ( tag_indicator * correl_vector * norm_tag - no_tag_indicator * norm_no_tag , axis = axis ) 
if scale : RES = RES / N 
if single : 
~~~ es_vec = RES . sum ( axis = axis ) 
~~~ max_ES , min_ES = RES . max ( axis = axis ) , RES . min ( axis = axis ) 
es_vec = np . where ( np . abs ( max_ES ) > np . abs ( min_ES ) , max_ES , min_ES ) 
~~ es , esnull , RES = es_vec [ - 1 ] , es_vec [ : - 1 ] , RES [ - 1 , : ] 
return es , esnull , hit_ind , RES 
~~ def enrichment_score_tensor ( gene_mat , cor_mat , gene_sets , weighted_score_type , nperm = 1000 , 
rs = np . random . RandomState ( ) , single = False , scale = False ) : 
keys = sorted ( gene_sets . keys ( ) ) 
~~~ cor_mat = np . ones ( cor_mat . shape ) 
~~ elif weighted_score_type > 0 : 
~~ cor_mat = np . abs ( cor_mat ) 
if cor_mat . ndim == 1 : 
~~~ N , M = len ( gene_mat ) , len ( keys ) 
tag_indicator = np . vstack ( [ np . in1d ( gene_mat , gene_sets [ key ] , assume_unique = True ) for key in keys ] ) 
tag_indicator = tag_indicator . astype ( int ) 
hit_ind = [ np . flatnonzero ( tag ) . tolist ( ) for tag in tag_indicator ] 
perm_tag_tensor = np . repeat ( tag_indicator , nperm + 1 ) . reshape ( ( M , N , nperm + 1 ) ) 
if nperm : np . apply_along_axis ( lambda x : np . apply_along_axis ( rs . shuffle , 0 , x ) , 1 , perm_tag_tensor [ : , : , : - 1 ] ) 
no_tag_tensor = 1 - perm_tag_tensor 
rank_alpha = ( perm_tag_tensor * cor_mat [ np . newaxis , : , np . newaxis ] ) ** weighted_score_type 
~~ elif cor_mat . ndim == 2 : 
~~~ cor_mat = cor_mat . T 
genes , genes_ind = gene_mat 
tag_indicator = np . vstack ( [ np . in1d ( genes , gene_sets [ key ] , assume_unique = True ) for key in keys ] ) 
perm_tag_tensor = np . stack ( [ tag . take ( genes_ind ) . T for tag in tag_indicator ] , axis = 0 ) 
hit_ind = [ np . flatnonzero ( tag ) . tolist ( ) for tag in perm_tag_tensor [ : , : , - 1 ] ] 
rank_alpha = ( perm_tag_tensor * cor_mat [ np . newaxis , : , : ] ) ** weighted_score_type 
~~ axis = 1 
P_GW_denominator = np . sum ( rank_alpha , axis = axis , keepdims = True ) 
P_NG_denominator = np . sum ( no_tag_tensor , axis = axis , keepdims = True ) 
REStensor = np . cumsum ( rank_alpha / P_GW_denominator - no_tag_tensor / P_NG_denominator , axis = axis ) 
if scale : REStensor = REStensor / len ( gene_mat ) 
#ssGSEA 
~~~ esmatrix = REStensor . sum ( axis = axis ) 
#GSEA 
~~~ esmax , esmin = REStensor . max ( axis = axis ) , REStensor . min ( axis = axis ) 
esmatrix = np . where ( np . abs ( esmax ) > np . abs ( esmin ) , esmax , esmin ) 
~~ es , esnull , RES = esmatrix [ : , - 1 ] , esmatrix [ : , : - 1 ] , REStensor [ : , : , - 1 ] 
~~ def ranking_metric_tensor ( exprs , method , permutation_num , pos , neg , classes , 
ascending , rs = np . random . RandomState ( ) ) : 
G , S = exprs . shape 
expr_mat = exprs . values . T 
perm_cor_tensor = np . tile ( expr_mat , ( permutation_num + 1 , 1 , 1 ) ) 
for arr in perm_cor_tensor [ : - 1 ] : rs . shuffle ( arr ) 
classes = np . array ( classes ) 
pos = classes == pos 
neg = classes == neg 
pos_cor_mean = perm_cor_tensor [ : , pos , : ] . mean ( axis = 1 ) 
neg_cor_mean = perm_cor_tensor [ : , neg , : ] . mean ( axis = 1 ) 
pos_cor_std = perm_cor_tensor [ : , pos , : ] . std ( axis = 1 , ddof = 1 ) 
neg_cor_std = perm_cor_tensor [ : , neg , : ] . std ( axis = 1 , ddof = 1 ) 
if method == 'signal_to_noise' : 
~~~ cor_mat = ( pos_cor_mean - neg_cor_mean ) / ( pos_cor_std + neg_cor_std ) 
~~ elif method == 't_test' : 
~~~ denom = 1.0 / G 
cor_mat = ( pos_cor_mean - neg_cor_mean ) / np . sqrt ( denom * pos_cor_std ** 2 + denom * neg_cor_std ** 2 ) 
~~ elif method == 'ratio_of_classes' : 
~~~ cor_mat = pos_cor_mean / neg_cor_mean 
~~ elif method == 'diff_of_classes' : 
~~~ cor_mat = pos_cor_mean - neg_cor_mean 
~~ elif method == 'log2_ratio_of_classes' : 
~~~ cor_mat = np . log2 ( pos_cor_mean / neg_cor_mean ) 
~~ cor_mat_ind = cor_mat . argsort ( ) 
cor_mat . sort ( ) 
if ascending : return cor_mat_ind , cor_mat 
return cor_mat_ind [ : , : : - 1 ] , cor_mat [ : , : : - 1 ] 
~~ def ranking_metric ( df , method , pos , neg , classes , ascending ) : 
df_mean = df . groupby ( by = classes , axis = 1 ) . mean ( ) 
df_std = df . groupby ( by = classes , axis = 1 ) . std ( ) 
~~~ ser = ( df_mean [ pos ] - df_mean [ neg ] ) / ( df_std [ pos ] + df_std [ neg ] ) 
~~~ ser = ( df_mean [ pos ] - df_mean [ neg ] ) / np . sqrt ( df_std [ pos ] ** 2 / len ( df_std ) + df_std [ neg ] ** 2 / len ( df_std ) ) 
~~~ ser = df_mean [ pos ] / df_mean [ neg ] 
~~~ ser = df_mean [ pos ] - df_mean [ neg ] 
~~~ ser = np . log2 ( df_mean [ pos ] / df_mean [ neg ] ) 
~~ ser = ser . sort_values ( ascending = ascending ) 
return ser 
~~ def gsea_compute_tensor ( data , gmt , n , weighted_score_type , permutation_type , 
method , pheno_pos , pheno_neg , classes , ascending , 
processes = 1 , seed = None , single = False , scale = False ) : 
w = weighted_score_type 
subsets = sorted ( gmt . keys ( ) ) 
rs = np . random . RandomState ( seed ) 
genes_mat , cor_mat = data . index . values , data . values 
base = 5 if data . shape [ 0 ] >= 5000 else 10 
block = ceil ( len ( subsets ) / base ) 
if permutation_type == "phenotype" : 
genes_ind = [ ] 
cor_mat = [ ] 
temp_rnk = [ ] 
pool_rnk = Pool ( processes = processes ) 
i = 1 
while i <= block : 
~~~ rs = np . random . RandomState ( seed ) 
temp_rnk . append ( pool_rnk . apply_async ( ranking_metric_tensor , 
args = ( data , method , base , pheno_pos , pheno_neg , classes , 
ascending , rs ) ) ) 
~~ pool_rnk . close ( ) 
pool_rnk . join ( ) 
for k , temp in enumerate ( temp_rnk ) : 
~~~ gi , cor = temp . get ( ) 
if k + 1 == block : 
~~~ genes_ind . append ( gi ) 
cor_mat . append ( cor ) 
~~~ genes_ind . append ( gi [ : - 1 ] ) 
cor_mat . append ( cor [ : - 1 ] ) 
~~ ~~ genes_ind , cor_mat = np . vstack ( genes_ind ) , np . vstack ( cor_mat ) 
genes_mat = ( data . index . values , genes_ind ) 
es = [ ] 
RES = [ ] 
hit_ind = [ ] 
esnull = [ ] 
temp_esnu = [ ] 
pool_esnu = Pool ( processes = processes ) 
i , m = 1 , 0 
gmtrim = { k : gmt . get ( k ) for k in subsets [ m : base * i ] } 
temp_esnu . append ( pool_esnu . apply_async ( enrichment_score_tensor , 
args = ( genes_mat , cor_mat , 
gmtrim , w , n , rs , 
single , scale ) ) ) 
m = base * i 
~~ pool_esnu . close ( ) 
pool_esnu . join ( ) 
for si , temp in enumerate ( temp_esnu ) : 
~~~ e , enu , hit , rune = temp . get ( ) 
esnull . append ( enu ) 
es . append ( e ) 
RES . append ( rune ) 
hit_ind += hit 
~~ es , esnull , RES = np . hstack ( es ) , np . vstack ( esnull ) , np . vstack ( RES ) 
return gsea_significance ( es , esnull ) , hit_ind , RES , subsets 
~~ def gsea_compute ( data , gmt , n , weighted_score_type , permutation_type , 
esnull = [ [ ] for a in range ( len ( subsets ) ) ] 
genes_mat , cor_mat = ranking_metric_tensor ( exprs = data , method = method , 
permutation_num = n , 
pos = pheno_pos , neg = pheno_neg , 
classes = classes , 
ascending = ascending , rs = rs ) 
es , esnull , hit_ind , RES = enrichment_score_tensor ( gene_mat = genes_mat , 
cor_mat = cor_mat , 
gene_sets = gmt , 
weighted_score_type = w , 
nperm = n , rs = rs , 
single = False , scale = False , ) 
~~~ gl , cor_vec = data . index . values , data . values 
for subset in subsets : 
temp_esnu . append ( pool_esnu . apply_async ( enrichment_score , 
args = ( gl , cor_vec , gmt . get ( subset ) , w , 
n , rs , single , scale ) ) ) 
esnull [ si ] = enu 
hit_ind . append ( hit ) 
~~ ~~ return gsea_significance ( es , esnull ) , hit_ind , RES , subsets 
~~ def gsea_pval ( es , esnull ) : 
condlist = [ es < 0 , es >= 0 ] 
choicelist = [ np . sum ( esnull < es . reshape ( len ( es ) , 1 ) , axis = 1 ) / np . sum ( esnull < 0 , axis = 1 ) , 
np . sum ( esnull >= es . reshape ( len ( es ) , 1 ) , axis = 1 ) / np . sum ( esnull >= 0 , axis = 1 ) ] 
pval = np . select ( condlist , choicelist ) 
return pval 
~~ def normalize ( es , esnull ) : 
nEnrichmentScores = np . zeros ( es . shape ) 
nEnrichmentNulls = np . zeros ( esnull . shape ) 
esnull_pos = ( esnull * ( esnull >= 0 ) ) . mean ( axis = 1 ) 
esnull_neg = ( esnull * ( esnull < 0 ) ) . mean ( axis = 1 ) 
for i in range ( esnull . shape [ 0 ] ) : 
~~~ if es [ i ] >= 0 : 
~~~ nEnrichmentScores [ i ] = es [ i ] / esnull_pos [ i ] 
~~~ nEnrichmentScores [ i ] = - es [ i ] / esnull_neg [ i ] 
~~ for j in range ( esnull . shape [ 1 ] ) : 
~~~ if esnull [ i , j ] >= 0 : 
~~~ nEnrichmentNulls [ i , j ] = esnull [ i , j ] / esnull_pos [ i ] 
~~~ nEnrichmentNulls [ i , j ] = - esnull [ i , j ] / esnull_neg [ i ] 
~~ ~~ ~~ return nEnrichmentScores , nEnrichmentNulls 
~~ def gsea_significance ( enrichment_scores , enrichment_nulls ) : 
np . seterr ( divide = 'ignore' , invalid = 'ignore' ) 
es = np . array ( enrichment_scores ) 
esnull = np . array ( enrichment_nulls ) 
enrichmentPVals = gsea_pval ( es , esnull ) . tolist ( ) 
nEnrichmentScores = np . where ( es >= 0 , es / esnull_pos , - es / esnull_neg ) 
nEnrichmentNulls = np . where ( esnull >= 0 , esnull / esnull_pos [ : , np . newaxis ] , 
- esnull / esnull_neg [ : , np . newaxis ] ) 
nvals = np . sort ( nEnrichmentNulls . flatten ( ) ) 
nnes = np . sort ( nEnrichmentScores ) 
fdrs = [ ] 
for i in range ( len ( enrichment_scores ) ) : 
~~~ nes = nEnrichmentScores [ i ] 
if nes >= 0 : 
~~~ allPos = int ( len ( nvals ) - np . searchsorted ( nvals , 0 , side = "left" ) ) 
allHigherAndPos = int ( len ( nvals ) - np . searchsorted ( nvals , nes , side = "left" ) ) 
nesPos = len ( nnes ) - int ( np . searchsorted ( nnes , 0 , side = "left" ) ) 
nesHigherAndPos = len ( nnes ) - int ( np . searchsorted ( nnes , nes , side = "left" ) ) 
~~~ allPos = int ( np . searchsorted ( nvals , 0 , side = "left" ) ) 
allHigherAndPos = int ( np . searchsorted ( nvals , nes , side = "right" ) ) 
nesPos = int ( np . searchsorted ( nnes , 0 , side = "left" ) ) 
nesHigherAndPos = int ( np . searchsorted ( nnes , nes , side = "right" ) ) 
~~~ pi_norm = allHigherAndPos / float ( allPos ) 
pi_obs = nesHigherAndPos / float ( nesPos ) 
fdr = pi_norm / pi_obs 
fdrs . append ( fdr if fdr < 1 else 1.0 ) 
~~~ fdrs . append ( 1000000000.0 ) 
return zip ( enrichment_scores , nEnrichmentScores , enrichmentPVals , fdrs ) 
~~ def log_init ( outlog , log_level = logging . INFO ) : 
logging . getLogger ( "gseapy" ) . handlers = [ ] 
logging . basicConfig ( level = logging . DEBUG , 
filename = outlog , 
filemode = 'w' ) 
console = logging . StreamHandler ( ) 
console . setLevel ( log_level ) 
console . setFormatter ( formatter ) 
logging . getLogger ( "gseapy" ) . addHandler ( console ) 
logger = logging . getLogger ( "gseapy" ) 
return logger 
~~ def log_stop ( logger ) : 
handlers = logger . handlers [ : ] 
for handler in handlers : 
~~~ handler . close ( ) 
logger . removeHandler ( handler ) 
~~ ~~ def retry ( num = 5 ) : 
s = requests . Session ( ) 
retries = Retry ( total = num , backoff_factor = 0.1 , 
status_forcelist = [ 500 , 502 , 503 , 504 ] ) 
s . mount ( 'http://' , HTTPAdapter ( max_retries = retries ) ) 
~~ def gsea_cls_parser ( cls ) : 
if isinstance ( cls , list ) : 
~~~ classes = cls 
sample_name = unique ( classes ) 
~~ elif isinstance ( cls , str ) : 
~~~ with open ( cls ) as c : 
~~~ file = c . readlines ( ) 
~~ return sample_name [ 0 ] , sample_name [ 1 ] , classes 
~~ def gsea_edb_parser ( results_path , index = 0 ) : 
from bs4 import BeautifulSoup 
soup = BeautifulSoup ( open ( results_path ) , features = 'xml' ) 
tag = soup . findAll ( 'DTG' ) 
term = dict ( tag [ index ] . attrs ) 
enrich_term = term . get ( 'GENESET' ) . split ( "#" ) [ 1 ] 
es_profile = [ float ( i ) for i in es_profile ] 
hit_ind = [ float ( i ) for i in hit_ind ] 
nes = term . get ( 'NES' ) 
pval = term . get ( 'NP' ) 
fdr = term . get ( 'FDR' ) 
return enrich_term , hit_ind , nes , pval , fdr 
~~ def gsea_gmt_parser ( gmt , min_size = 3 , max_size = 1000 , gene_list = None ) : 
if gmt . lower ( ) . endswith ( ".gmt" ) : 
with open ( gmt ) as genesets : 
~~~ genesets_dict = { line . strip ( ) . split ( "\\t" ) [ 0 ] : line . strip ( ) . split ( "\\t" ) [ 2 : ] 
for line in genesets . readlines ( ) } 
if gmt in DEFAULT_LIBRARY : 
~~~ names = DEFAULT_LIBRARY 
~~~ names = get_library_name ( ) 
~~ if gmt in names : 
retries = Retry ( total = 5 , backoff_factor = 0.1 , 
ENRICHR_URL = 'http://amp.pharm.mssm.edu/Enrichr/geneSetLibrary' 
query_string = '?mode=text&libraryName=%s' 
response = s . get ( ENRICHR_URL + query_string % gmt , timeout = None ) 
~~ if not response . ok : 
~~ genesets_dict = { line . strip ( ) . split ( "\\t" ) [ 0 ] : 
list ( map ( lambda x : x . split ( "," ) [ 0 ] , line . strip ( ) . split ( "\\t" ) [ 2 : ] ) ) 
for line in response . iter_lines ( chunk_size = 1024 , decode_unicode = 'utf-8' ) } 
~~ if sys . version_info [ 0 ] >= 3 : 
~~~ genesets_filter = { k : v for k , v in genesets_dict . items ( ) if len ( v ) >= min_size and len ( v ) <= max_size } 
~~ elif sys . version_info [ 0 ] == 2 : 
~~~ genesets_filter = { k : v for k , v in genesets_dict . iteritems ( ) if len ( v ) >= min_size and len ( v ) <= max_size } 
sys . exit ( 1 ) 
~~ if gene_list is not None : 
~~~ subsets = sorted ( genesets_filter . keys ( ) ) 
~~~ tag_indicator = in1d ( gene_list , genesets_filter . get ( subset ) , assume_unique = True ) 
tag_len = sum ( tag_indicator ) 
if tag_len <= min_size or tag_len >= max_size : 
~~~ del genesets_filter [ subset ] 
~~ ~~ ~~ filsets_num = len ( genesets_dict ) - len ( genesets_filter ) 
if filsets_num == len ( genesets_dict ) : 
~~~ return genesets_filter 
~~ ~~ def get_library_name ( database = 'Human' ) : 
if database not in [ 'Human' , 'Mouse' , 'Yeast' , 'Fly' , 'Fish' , 'Worm' ] : 
~~ if database in [ 'Human' , 'Mouse' ] : database = '' 
lib_url = 'http://amp.pharm.mssm.edu/%sEnrichr/datasetStatistics' % database 
libs_json = json . loads ( requests . get ( lib_url ) . text ) 
libs = [ lib [ 'libraryName' ] for lib in libs_json [ 'statistics' ] ] 
return sorted ( libs ) 
~~ def get_marts ( self ) : 
mart_names = pd . Series ( self . names , name = "Name" ) 
mart_descriptions = pd . Series ( self . displayNames , name = "Description" ) 
return pd . concat ( [ mart_names , mart_descriptions ] , axis = 1 ) 
~~ def get_datasets ( self , mart = 'ENSEMBL_MART_ENSEMBL' ) : 
datasets = self . datasets ( mart , raw = True ) 
return pd . read_csv ( StringIO ( datasets ) , header = None , usecols = [ 1 , 2 ] , 
names = [ "Name" , "Description" ] , sep = "\\t" ) 
~~ def get_attributes ( self , dataset ) : 
attributes = self . attributes ( dataset ) 
attr_ = [ ( k , v [ 0 ] ) for k , v in attributes . items ( ) ] 
return pd . DataFrame ( attr_ , columns = [ "Attribute" , "Description" ] ) 
~~ def get_filters ( self , dataset ) : 
filters = self . filters ( dataset ) 
filt_ = [ ( k , v [ 0 ] ) for k , v in filters . items ( ) ] 
return pd . DataFrame ( filt_ , columns = [ "Filter" , "Description" ] ) 
~~ def query ( self , dataset = 'hsapiens_gene_ensembl' , attributes = [ ] , 
filters = { } , filename = None ) : 
if not attributes : 
~~~ attributes = [ 'ensembl_gene_id' , 'external_gene_name' , 'entrezgene' , 'go_id' ] 
~~ self . new_query ( ) 
self . add_dataset_to_xml ( dataset ) 
for at in attributes : 
~~~ self . add_attribute_to_xml ( at ) 
~~ if filters : 
~~~ for k , v in filters . items ( ) : 
~~~ if isinstance ( v , list ) : v = "," . join ( v ) 
self . add_filter_to_xml ( k , v ) 
~~ ~~ xml_query = self . get_xml ( ) 
results = super ( Biomart , self ) . query ( xml_query ) 
df = pd . read_csv ( StringIO ( results ) , header = None , sep = "\\t" , 
names = attributes , index_col = None ) 
if filename is None : 
~~~ mkdirs ( DEFAULT_CACHE_PATH ) 
filename = os . path . join ( DEFAULT_CACHE_PATH , "{}.background.genes.txt" . format ( dataset ) ) 
~~ df . to_csv ( filename , sep = "\\t" , index = False ) 
~~ def gsea ( data , gene_sets , cls , outdir = 'GSEA_' , min_size = 15 , max_size = 500 , permutation_num = 1000 , 
weighted_score_type = 1 , permutation_type = 'gene_set' , method = 'log2_ratio_of_classes' , 
ascending = False , processes = 1 , figsize = ( 6.5 , 6 ) , format = 'pdf' , 
graph_num = 20 , no_plot = False , seed = None , verbose = False ) : 
gs = GSEA ( data , gene_sets , cls , outdir , min_size , max_size , permutation_num , 
weighted_score_type , permutation_type , method , ascending , processes , 
figsize , format , graph_num , no_plot , seed , verbose ) 
return gs 
~~ def ssgsea ( data , gene_sets , outdir = "ssGSEA_" , sample_norm_method = 'rank' , min_size = 15 , max_size = 2000 , 
permutation_num = 0 , weighted_score_type = 0.25 , scale = True , ascending = False , processes = 1 , 
figsize = ( 7 , 6 ) , format = 'pdf' , graph_num = 20 , no_plot = False , seed = None , verbose = False ) : 
ss = SingleSampleGSEA ( data , gene_sets , outdir , sample_norm_method , min_size , max_size , 
permutation_num , weighted_score_type , scale , ascending , 
processes , figsize , format , graph_num , no_plot , seed , verbose ) 
return ss 
~~ def prerank ( rnk , gene_sets , outdir = 'GSEA_Prerank' , pheno_pos = 'Pos' , pheno_neg = 'Neg' , 
min_size = 15 , max_size = 500 , permutation_num = 1000 , weighted_score_type = 1 , 
pre = Prerank ( rnk , gene_sets , outdir , pheno_pos , pheno_neg , 
min_size , max_size , permutation_num , weighted_score_type , 
ascending , processes , figsize , format , graph_num , no_plot , seed , verbose ) 
return pre 
~~ def replot ( indir , outdir = 'GSEA_Replot' , weighted_score_type = 1 , 
min_size = 3 , max_size = 1000 , figsize = ( 6.5 , 6 ) , graph_num = 20 , format = 'pdf' , verbose = False ) : 
rep = Replot ( indir , outdir , weighted_score_type , 
min_size , max_size , figsize , graph_num , format , verbose ) 
~~ def prepare_outdir ( self ) : 
self . _outdir = self . outdir 
if self . _outdir is None : 
~~~ self . _tmpdir = TemporaryDirectory ( ) 
self . outdir = self . _tmpdir . name 
~~ elif isinstance ( self . outdir , str ) : 
~~~ mkdirs ( self . outdir ) 
~~ if isinstance ( self . gene_sets , str ) : 
~~~ _gset = os . path . split ( self . gene_sets ) [ - 1 ] . lower ( ) . rstrip ( ".gmt" ) 
~~ elif isinstance ( self . gene_sets , dict ) : 
~~~ _gset = "blank_name" 
~~ logfile = os . path . join ( self . outdir , "gseapy.%s.%s.log" % ( self . module , _gset ) ) 
return logfile 
~~ def _set_cores ( self ) : 
cpu_num = cpu_count ( ) - 1 
if self . _processes > cpu_num : 
~~~ cores = cpu_num 
~~ elif self . _processes < 1 : 
~~~ cores = 1 
~~~ cores = self . _processes 
~~ self . _processes = int ( cores ) 
~~ def _load_ranking ( self , rnk ) : 
if isinstance ( rnk , pd . DataFrame ) : 
~~~ rank_metric = rnk . copy ( ) 
if rnk . shape [ 1 ] == 1 : rank_metric = rnk . reset_index ( ) 
~~ elif isinstance ( rnk , pd . Series ) : 
~~~ rank_metric = rnk . reset_index ( ) 
~~ elif os . path . isfile ( rnk ) : 
~~~ rank_metric = pd . read_csv ( rnk , header = None , comment = '#' , sep = "\\t" ) 
~~ rank_metric . sort_values ( by = rank_metric . columns [ 1 ] , ascending = self . ascending , inplace = True ) 
if rank_metric . isnull ( ) . any ( axis = 1 ) . sum ( ) > 0 : 
NAs = rank_metric [ rank_metric . isnull ( ) . any ( axis = 1 ) ] 
rank_metric . dropna ( how = 'any' , inplace = True ) 
~~ if rank_metric . duplicated ( subset = rank_metric . columns [ 0 ] ) . sum ( ) > 0 : 
dups = rank_metric [ rank_metric . duplicated ( subset = rank_metric . columns [ 0 ] ) ] 
rank_metric . drop_duplicates ( subset = rank_metric . columns [ 0 ] , inplace = True , keep = 'first' ) 
~~ rank_metric . reset_index ( drop = True , inplace = True ) 
rank_metric . columns = [ 'gene_name' , 'rank' ] 
rankser = rank_metric . set_index ( 'gene_name' ) [ 'rank' ] 
self . ranking = rankser 
return rankser 
~~ def load_gmt ( self , gene_list , gmt ) : 
if isinstance ( gmt , dict ) : 
~~~ genesets_dict = gmt 
~~ elif isinstance ( gmt , str ) : 
~~~ genesets_dict = self . parse_gmt ( gmt ) 
~~ subsets = list ( genesets_dict . keys ( ) ) 
self . n_genesets = len ( subsets ) 
~~~ subset_list = genesets_dict . get ( subset ) 
if isinstance ( subset_list , set ) : 
~~~ subset_list = list ( subset_list ) 
genesets_dict [ subset ] = subset_list 
~~ tag_indicator = np . in1d ( gene_list , subset_list , assume_unique = True ) 
tag_len = tag_indicator . sum ( ) 
if self . min_size <= tag_len <= self . max_size : continue 
del genesets_dict [ subset ] 
~~ filsets_num = len ( subsets ) - len ( genesets_dict ) 
if filsets_num == len ( subsets ) : 
~~ self . _gmtdct = genesets_dict 
return genesets_dict 
~~ def parse_gmt ( self , gmt ) : 
~~~ with open ( gmt ) as genesets : 
~~ return genesets_dict 
~~ elif gmt in DEFAULT_LIBRARY : 
~~ elif gmt in self . get_libraries ( ) : 
~~ tmpname = "enrichr." + gmt + ".gmt" 
tempath = os . path . join ( DEFAULT_CACHE_PATH , tmpname ) 
if os . path . isfile ( tempath ) : 
return self . parse_gmt ( tempath ) 
~~~ return self . _download_libraries ( gmt ) 
~~ ~~ def get_libraries ( self , database = '' ) : 
~~ def _download_libraries ( self , libname ) : 
s = retry ( 5 ) 
response = s . get ( ENRICHR_URL + query_string % libname , timeout = None ) 
if not response . ok : 
~~ mkdirs ( DEFAULT_CACHE_PATH ) 
genesets_dict = { } 
outname = "enrichr.%s.gmt" % libname 
gmtout = open ( os . path . join ( DEFAULT_CACHE_PATH , outname ) , "w" ) 
for line in response . iter_lines ( chunk_size = 1024 , decode_unicode = 'utf-8' ) : 
k = line . split ( "\\t" ) [ 0 ] 
v = list ( map ( lambda x : x . split ( "," ) [ 0 ] , line . split ( "\\t" ) [ 2 : ] ) ) 
genesets_dict . update ( { k : v } ) 
outline = "%s\\t\\t%s\\n" % ( k , "\\t" . join ( v ) ) 
gmtout . write ( outline ) 
~~ gmtout . close ( ) 
~~ def _heatmat ( self , df , classes , pheno_pos , pheno_neg ) : 
width = len ( classes ) if len ( classes ) >= 6 else 5 
cls_booA = list ( map ( lambda x : True if x == pheno_pos else False , classes ) ) 
cls_booB = list ( map ( lambda x : True if x == pheno_neg else False , classes ) ) 
datA = df . loc [ : , cls_booA ] 
datB = df . loc [ : , cls_booB ] 
datAB = pd . concat ( [ datA , datB ] , axis = 1 ) 
self . _width = width 
self . heatmat = datAB 
~~ def _plotting ( self , rank_metric , results , graph_num , outdir , 
format , figsize , pheno_pos = '' , pheno_neg = '' ) : 
if self . _outdir is None : return 
#Plotting 
top_term = self . res2d . index [ : graph_num ] 
pool = Pool ( self . _processes ) 
for gs in top_term : 
~~~ hit = results . get ( gs ) [ 'hits_indices' ] 
NES = 'nes' if self . module != 'ssgsea' else 'es' 
term = gs . replace ( '/' , '_' ) . replace ( ":" , "_" ) 
outfile = '{0}/{1}.{2}.{3}' . format ( self . outdir , term , self . module , self . format ) 
pool . apply_async ( gseaplot , args = ( rank_metric , term , hit , results . get ( gs ) [ NES ] , 
results . get ( gs ) [ 'pval' ] , results . get ( gs ) [ 'fdr' ] , 
results . get ( gs ) [ 'RES' ] , 
pheno_pos , pheno_neg , 
figsize , 'seismic' , outfile ) ) 
if self . module == 'gsea' : 
~~~ outfile2 = "{0}/{1}.heatmap.{2}" . format ( self . outdir , term , self . format ) 
pool . apply_async ( heatmap , args = ( self . heatmat . iloc [ hit , : ] , 0 , term , 
( self . _width , len ( hit ) / 2 + 2 ) , 'RdBu_r' , 
True , True , outfile2 ) ) 
~~ ~~ pool . close ( ) 
~~ def _save_results ( self , zipdata , outdir , module , gmt , rank_metric , permutation_type ) : 
res = OrderedDict ( ) 
for gs , gseale , ind , RES in zipdata : 
~~~ rdict = OrderedDict ( ) 
rdict [ 'es' ] = gseale [ 0 ] 
rdict [ 'nes' ] = gseale [ 1 ] 
rdict [ 'pval' ] = gseale [ 2 ] 
rdict [ 'fdr' ] = gseale [ 3 ] 
rdict [ 'geneset_size' ] = len ( gmt [ gs ] ) 
rdict [ 'matched_size' ] = len ( ind ) 
_genes = rank_metric . index . values [ ind ] 
rdict [ 'genes' ] = ";" . join ( [ str ( g ) . strip ( ) for g in _genes ] ) 
if self . module != 'ssgsea' : 
~~~ if rdict [ 'es' ] > 0 : 
~~~ idx = RES . argmax ( ) 
ldg_pos = list ( filter ( lambda x : x <= idx , ind ) ) 
~~ elif rdict [ 'es' ] < 0 : 
~~~ idx = RES . argmin ( ) 
ldg_pos = list ( filter ( lambda x : x >= idx , ind ) ) 
~~ rdict [ 'ledge_genes' ] = ';' . join ( list ( map ( str , rank_metric . iloc [ ldg_pos ] . index ) ) ) 
~~ rdict [ 'RES' ] = RES 
rdict [ 'hits_indices' ] = ind 
res [ gs ] = rdict 
~~ self . results = res 
res_df = pd . DataFrame . from_dict ( res , orient = 'index' ) 
res_df . index . name = 'Term' 
res_df . drop ( [ 'RES' , 'hits_indices' ] , axis = 1 , inplace = True ) 
res_df . sort_values ( by = [ 'fdr' , 'pval' ] , inplace = True ) 
self . res2d = res_df 
out = os . path . join ( outdir , 'gseapy.{b}.{c}.report.csv' . format ( b = module , c = permutation_type ) ) 
if self . module == 'ssgsea' : 
~~~ out = out . replace ( ".csv" , ".txt" ) 
with open ( out , 'a' ) as f : 
res_df . to_csv ( f , sep = '\\t' ) 
~~~ res_df . to_csv ( out ) 
~~ def load_data ( self , cls_vec ) : 
if isinstance ( self . data , pd . DataFrame ) : 
~~~ exprs = self . data . copy ( ) 
if exprs . index . dtype == 'O' : 
~~~ exprs = exprs . reset_index ( ) 
~~ ~~ elif os . path . isfile ( self . data ) : 
~~~ if self . data . endswith ( "gct" ) : 
~~~ exprs = pd . read_csv ( self . data , skiprows = 1 , comment = '#' , sep = "\\t" ) 
~~~ exprs = pd . read_csv ( self . data , comment = '#' , sep = "\\t" ) 
~~ if exprs . iloc [ : , 0 ] . duplicated ( ) . sum ( ) > 0 : 
~~ if exprs . isnull ( ) . any ( ) . sum ( ) > 0 : 
exprs = exprs . fillna ( 0 ) 
~~ exprs . set_index ( keys = exprs . columns [ 0 ] , inplace = True ) 
df = exprs . select_dtypes ( include = [ np . number ] ) 
df_std = df . groupby ( by = cls_vec , axis = 1 ) . std ( ) 
df = df [ ~ df_std . isin ( [ 0 ] ) . any ( axis = 1 ) ] 
assert self . permutation_type in [ "phenotype" , "gene_set" ] 
assert self . min_size <= self . max_size 
phenoPos , phenoNeg , cls_vector = gsea_cls_parser ( self . classes ) 
dat = self . load_data ( cls_vector ) 
assert len ( dat ) > 1 
dat2 = ranking_metric ( df = dat , method = self . method , pos = phenoPos , neg = phenoNeg , 
classes = cls_vector , ascending = self . ascending ) 
self . ranking = dat2 
gmt = self . load_gmt ( gene_list = dat2 . index . values , gmt = self . gene_sets ) 
self . _set_cores ( ) 
dataset = dat if self . permutation_type == 'phenotype' else dat2 
gsea_results , hit_ind , rank_ES , subsets = gsea_compute_tensor ( data = dataset , gmt = gmt , n = self . permutation_num , 
weighted_score_type = self . weighted_score_type , 
permutation_type = self . permutation_type , 
pheno_pos = phenoPos , pheno_neg = phenoNeg , 
classes = cls_vector , ascending = self . ascending , 
processes = self . _processes , seed = self . seed ) 
res_zip = zip ( subsets , list ( gsea_results ) , hit_ind , rank_ES ) 
self . _save_results ( zipdata = res_zip , outdir = self . outdir , module = self . module , 
gmt = gmt , rank_metric = dat2 , permutation_type = self . permutation_type ) 
self . _heatmat ( df = dat . loc [ dat2 . index ] , classes = cls_vector , 
pheno_pos = phenoPos , pheno_neg = phenoNeg ) 
if not self . _noplot : 
~~~ self . _plotting ( rank_metric = dat2 , results = self . results , 
graph_num = self . graph_num , outdir = self . outdir , 
figsize = self . figsize , format = self . format , 
~~~ self . _tmpdir . cleanup ( ) 
dat2 = self . _load_ranking ( self . rnk ) 
assert len ( dat2 ) > 1 
gsea_results , hit_ind , rank_ES , subsets = gsea_compute ( data = dat2 , n = self . permutation_num , gmt = gmt , 
permutation_type = 'gene_set' , method = None , 
pheno_pos = self . pheno_pos , pheno_neg = self . pheno_neg , 
classes = None , ascending = self . ascending , 
gmt = gmt , rank_metric = dat2 , permutation_type = "gene_sets" ) 
pheno_pos = self . pheno_pos , pheno_neg = self . pheno_neg ) 
~~ def norm_samples ( self , dat ) : 
if self . sample_norm_method == 'rank' : 
~~~ data = dat . rank ( axis = 0 , method = 'average' , na_option = 'bottom' ) 
data = 10000 * data / data . shape [ 0 ] 
~~ elif self . sample_norm_method == 'log_rank' : 
data = log ( 10000 * data / data . shape [ 0 ] + exp ( 1 ) ) 
~~ elif self . sample_norm_method == 'log' : 
~~~ dat [ dat < 1 ] = 1 
data = log ( dat + exp ( 1 ) ) 
~~ elif self . sample_norm_method == 'custom' : 
data = dat 
data = self . load_data ( ) 
normdat = self . norm_samples ( data ) 
gmt = self . load_gmt ( gene_list = normdat . index . values , gmt = self . gene_sets ) 
if self . permutation_num == 0 : 
~~~ self . runSamples ( df = normdat , gmt = gmt ) 
self . runSamplesPermu ( df = normdat , gmt = gmt ) 
~~ if self . _outdir is None : 
~~ ~~ def runSamplesPermu ( self , df , gmt = None ) : 
mkdirs ( self . outdir ) 
self . resultsOnSamples = OrderedDict ( ) 
outdir = self . outdir 
for name , ser in df . iteritems ( ) : 
~~~ self . outdir = os . path . join ( outdir , str ( name ) ) 
dat2 = ser . sort_values ( ascending = self . ascending ) 
pheno_pos = '' , pheno_neg = '' , 
processes = self . _processes , 
seed = self . seed , single = True , scale = self . scale ) 
self . resultsOnSamples [ name ] = self . res2d . es 
if self . _noplot : continue 
self . _plotting ( rank_metric = dat2 , results = self . results , 
figsize = self . figsize , format = self . format ) 
~~ self . _save ( outdir ) 
~~ def runSamples ( self , df , gmt = None ) : 
#multi-threading 
tempes = [ ] 
rankings = [ ] 
pool = Pool ( processes = self . _processes ) 
~~~ dat = ser . sort_values ( ascending = self . ascending ) 
rankings . append ( dat ) 
names . append ( name ) 
genes_sorted , cor_vec = dat . index . values , dat . values 
rs = np . random . RandomState ( self . seed ) 
tempes . append ( pool . apply_async ( enrichment_score_tensor , 
args = ( genes_sorted , cor_vec , gmt , 
self . weighted_score_type , 
self . permutation_num , rs , True , 
self . scale ) ) ) 
~~ pool . close ( ) 
for i , temp in enumerate ( tempes ) : 
~~~ name , rnk = names [ i ] , rankings [ i ] 
es , esnull , hit_ind , RES = temp . get ( ) 
self . outdir = os . path . join ( outdir , str ( name ) ) 
self . resultsOnSamples [ name ] = pd . Series ( data = es , index = subsets , name = name ) 
for i , term in enumerate ( subsets ) : 
~~~ term = term . replace ( '/' , '_' ) . replace ( ":" , "_" ) 
gseaplot ( rank_metric = rnk , term = term , 
hits_indices = hit_ind [ i ] , nes = es [ i ] , pval = 1 , fdr = 1 , 
RES = RES [ i ] , pheno_pos = '' , pheno_neg = '' , 
figsize = self . figsize , ofname = outfile ) 
~~ ~~ self . _save ( outdir ) 
~~ def _save ( self , outdir ) : 
samplesRawES = pd . DataFrame ( self . resultsOnSamples ) 
samplesRawES . index . name = 'Term|ES' 
samplesNES = samplesRawES / ( samplesRawES . values . max ( ) - samplesRawES . values . min ( ) ) 
samplesNES = samplesNES . copy ( ) 
samplesNES . index . rename ( 'Term|NES' , inplace = True ) 
self . res2d = samplesNES 
outESfile = os . path . join ( outdir , "gseapy.samples.raw.es.txt" ) 
with open ( outESfile , 'a' ) as f : 
~~~ if self . scale : 
~~ samplesRawES . to_csv ( f , sep = '\\t' ) 
~~ outNESfile = os . path . join ( outdir , "gseapy.samples.normalized.es.txt" ) 
with open ( outNESfile , 'a' ) as f : 
samplesNES . to_csv ( f , sep = '\\t' ) 
assert self . fignum > 0 
~~~ results_path = glob . glob ( self . indir + '*/edb/results.edb' ) [ 0 ] 
rank_path = glob . glob ( self . indir + '*/edb/*.rnk' ) [ 0 ] 
gene_set_path = glob . glob ( self . indir + '*/edb/gene_sets.gmt' ) [ 0 ] 
~~ except IndexError as e : 
~~ cls_path = glob . glob ( self . indir + '*/edb/*.cls' ) 
if cls_path : 
~~~ pos , neg , classes = gsea_cls_parser ( cls_path [ 0 ] ) 
~~~ pos , neg = '' , '' 
~~ self . gene_sets = gene_set_path 
gene_set_dict = self . parse_gmt ( gmt = gene_set_path ) 
rank_metric = self . _load_ranking ( rank_path ) 
correl_vector = rank_metric . values 
gene_list = rank_metric . index . values 
database = BeautifulSoup ( open ( results_path ) , features = 'xml' ) 
length = len ( database . findAll ( 'DTG' ) ) 
fig_num = self . fignum if self . fignum <= length else length 
for idx in range ( fig_num ) : 
~~~ enrich_term , hit_ind , nes , pval , fdr = gsea_edb_parser ( results_path , index = idx ) 
gene_set = gene_set_dict . get ( enrich_term ) 
RES = enrichment_score ( gene_list = gene_list , 
correl_vector = correl_vector , 
gene_set = gene_set , 
nperm = 0 ) [ - 1 ] 
term = enrich_term . replace ( '/' , '_' ) . replace ( ":" , "_" ) 
gseaplot ( rank_metric = rank_metric , term = enrich_term , 
hits_indices = hit_ind , nes = nes , pval = pval , fdr = fdr , 
RES = RES , pheno_pos = pos , pheno_neg = neg , 
~~ def enrichr ( gene_list , gene_sets , organism = 'human' , description = '' , 
outdir = 'Enrichr' , background = 'hsapiens_gene_ensembl' , cutoff = 0.05 , 
format = 'pdf' , figsize = ( 8 , 6 ) , top_term = 10 , no_plot = False , verbose = False ) : 
enr = Enrichr ( gene_list , gene_sets , organism , description , outdir , 
cutoff , background , format , figsize , top_term , no_plot , verbose ) 
return enr 
~~ logfile = os . path . join ( self . outdir , "gseapy.%s.%s.log" % ( self . module , self . descriptions ) ) 
~~ def parse_genesets ( self ) : 
enrichr_library = self . get_libraries ( ) 
if isinstance ( self . gene_sets , list ) : 
~~~ gss = self . gene_sets 
~~ elif isinstance ( self . gene_sets , str ) : 
~~~ gss = [ g . strip ( ) for g in self . gene_sets . strip ( ) . split ( "," ) ] 
~~~ gss = [ self . gene_sets ] 
~~ gss_exist = [ ] 
for g in gss : 
~~~ if isinstance ( g , dict ) : 
~~~ gss_exist . append ( g ) 
~~ if isinstance ( g , str ) : 
~~~ if g in enrichr_library : 
~~ if g . lower ( ) . endswith ( ".gmt" ) and os . path . exists ( g ) : 
with open ( g ) as genesets : 
~~~ g_dict = { line . strip ( ) . split ( "\\t" ) [ 0 ] : line . strip ( ) . split ( "\\t" ) [ 2 : ] 
~~ gss_exist . append ( g_dict ) 
~~ ~~ ~~ return gss_exist 
~~ def parse_genelists ( self ) : 
if isinstance ( self . gene_list , list ) : 
~~~ genes = self . gene_list 
~~ elif isinstance ( self . gene_list , pd . DataFrame ) : 
~~~ if self . gene_list . shape [ 1 ] >= 3 : 
~~~ genes = self . gene_list . iloc [ : , : 3 ] . apply ( lambda x : "\\t" . join ( [ str ( i ) for i in x ] ) , axis = 1 ) . tolist ( ) 
~~ elif self . gene_list . shape [ 1 ] == 2 : 
~~~ genes = self . gene_list . apply ( lambda x : "," . join ( [ str ( i ) for i in x ] ) , axis = 1 ) . tolist ( ) 
~~~ genes = self . gene_list . squeeze ( ) . tolist ( ) 
~~ ~~ elif isinstance ( self . gene_list , pd . Series ) : 
~~~ genes = [ ] 
with open ( self . gene_list ) as f : 
~~~ for gene in f : 
~~~ genes . append ( gene . strip ( ) ) 
~~ ~~ ~~ self . _isezid = all ( map ( self . _is_entrez_id , genes ) ) 
if self . _isezid : 
~~~ self . _gls = set ( map ( int , self . _gls ) ) 
~~~ self . _gls = genes 
~~ return '\\n' . join ( genes ) 
~~ def send_genes ( self , gene_list , url ) : 
'list' : ( None , gene_list ) , 
'description' : ( None , self . descriptions ) 
response = requests . post ( url , files = payload ) 
~~ sleep ( 1 ) 
job_id = json . loads ( response . text ) 
return job_id 
~~ def check_genes ( self , gene_list , usr_list_id ) : 
response = requests . get ( 'http://amp.pharm.mssm.edu/Enrichr/view?userListId=%s' % usr_list_id ) 
~~ returnedL = json . loads ( response . text ) [ "genes" ] 
returnedN = sum ( [ 1 for gene in gene_list if gene in returnedL ] ) 
~~ def get_results ( self , gene_list ) : 
ADDLIST_URL = 'http://amp.pharm.mssm.edu/%sEnrichr/addList' % self . _organism 
job_id = self . send_genes ( gene_list , ADDLIST_URL ) 
user_list_id = job_id [ 'userListId' ] 
RESULTS_URL = 'http://amp.pharm.mssm.edu/%sEnrichr/export' % self . _organism 
query_string = '?userListId=%s&filename=%s&backgroundType=%s' 
s = retry ( num = 5 ) 
filename = "%s.%s.reports" % ( self . _gs , self . descriptions ) 
url = RESULTS_URL + query_string % ( user_list_id , filename , self . _gs ) 
response = s . get ( url , stream = True , timeout = None ) 
res = pd . read_csv ( StringIO ( response . content . decode ( 'utf-8' ) ) , sep = "\\t" ) 
return [ job_id [ 'shortId' ] , res ] 
~~ def get_background ( self ) : 
if os . path . isfile ( self . background ) : 
~~~ with open ( self . background ) as b : 
~~~ bg2 = b . readlines ( ) 
~~ bg = [ g . strip ( ) for g in bg2 ] 
return set ( bg ) 
~~ DB_FILE = resource_filename ( "gseapy" , "data/{}.background.genes.txt" . format ( self . background ) ) 
filename = os . path . join ( DEFAULT_CACHE_PATH , "{}.background.genes.txt" . format ( self . background ) ) 
if os . path . exists ( filename ) : 
~~~ df = pd . read_csv ( filename , sep = "\\t" ) 
~~ elif os . path . exists ( DB_FILE ) : 
~~~ df = pd . read_csv ( DB_FILE , sep = "\\t" ) 
bm = Biomart ( ) 
df = bm . query ( dataset = self . background ) 
df . dropna ( subset = [ 'go_id' ] , inplace = True ) 
df . dropna ( subset = [ 'entrezgene' ] , inplace = True ) 
~~~ bg = df [ 'entrezgene' ] . astype ( int ) 
~~~ bg = df [ 'external_gene_name' ] 
~~ return set ( bg ) 
~~ def get_organism ( self ) : 
organism = { 'default' : [ '' , 'hs' , 'mm' , 'human' , 'mouse' , 
for k , v in organism . items ( ) : 
~~~ if self . organism . lower ( ) in v : 
~~~ self . _organism = k 
~~ ~~ if self . _organism is None : 
~~ if self . _organism == 'default' : 
~~~ self . _organism = '' 
~~ def enrich ( self , gmt ) : 
if isscalar ( self . background ) : 
~~~ if isinstance ( self . background , int ) or self . background . isdigit ( ) : 
~~~ self . _bg = int ( self . background ) 
~~ elif isinstance ( self . background , str ) : 
~~~ self . _bg = self . get_background ( ) 
~~~ it = iter ( self . background ) 
self . _bg = set ( self . background ) 
~~ ~~ hgtest = list ( calc_pvalues ( query = self . _gls , gene_sets = gmt , 
background = self . _bg ) ) 
if len ( hgtest ) > 0 : 
~~~ terms , pvals , olsz , gsetsz , genes = hgtest 
fdrs , rej = multiple_testing_correction ( ps = pvals , 
alpha = self . cutoff , 
method = 'benjamini-hochberg' ) 
odict = OrderedDict ( ) 
odict [ 'Term' ] = terms 
odict [ 'Overlap' ] = list ( map ( lambda h , g : "%s/%s" % ( h , g ) , olsz , gsetsz ) ) 
odict [ 'P-value' ] = pvals 
odict [ 'Genes' ] = [ ";" . join ( g ) for g in genes ] 
res = pd . DataFrame ( odict ) 
self . get_organism ( ) 
genes_list = self . parse_genelists ( ) 
gss = self . parse_genesets ( ) 
if len ( gss ) < 1 : 
~~ self . results = pd . DataFrame ( ) 
~~~ res = self . enrich ( g ) 
shortID , self . _gs = str ( id ( g ) ) , "CUSTOM%s" % id ( g ) 
if res is None : 
~~~ self . _gs = str ( g ) 
shortID , res = self . get_results ( genes_list ) 
~~ res . insert ( 0 , "Gene_set" , self . _gs ) 
self . results = self . results . append ( res , ignore_index = True , sort = True ) 
self . res2d = res 
if self . _outdir is None : continue 
outfile = "%s/%s.%s.%s.reports.txt" % ( self . outdir , self . _gs , self . descriptions , self . module ) 
self . res2d . to_csv ( outfile , index = False , encoding = 'utf-8' , sep = "\\t" ) 
if not self . __no_plot : 
~~~ msg = barplot ( df = res , cutoff = self . cutoff , figsize = self . figsize , 
top_term = self . __top_term , color = 'salmon' , 
title = self . _gs , 
ofname = outfile . replace ( "txt" , self . format ) ) 
if msg is not None : self . _logger . warning ( msg ) 
~~ self . _logger . info ( 'Done.\\n' ) 
~~ if self . _outdir is None : self . _tmpdir . cleanup ( ) 
~~ def cube ( script , size = 1.0 , center = False , color = None ) : 
size = util . make_list ( size , 3 ) 
if script . ml_version == '1.3.4BETA' : 
~~~ filter_name = 'Box' 
~~~ filter_name = 'Box/Cube' 
~~ filter_xml = '' . join ( [ 
\ . format ( filter_name ) , 
'/>\\n' , 
util . write_filter ( script , filter_xml ) 
if isinstance ( script , FilterScript ) : 
~~~ script . add_layer ( 'Cube' , change_layer = True ) 
~~ transform . scale ( script , value = size ) 
if not center : 
~~~ transform . translate ( script , value = [ size [ 0 ] / 2 , size [ 1 ] / 2 , size [ 2 ] / 2 ] ) 
~~ if color is not None : 
~~~ vert_color . function ( script , color = color ) 
~~ def cylinder ( script , up = 'z' , height = 1.0 , radius = None , radius1 = None , 
radius2 = None , diameter = None , diameter1 = None , diameter2 = None , 
center = False , cir_segments = 32 , color = None ) : 
if radius is not None and diameter is None : 
~~~ if radius1 is None and diameter1 is None : 
~~~ radius1 = radius 
~~ if radius2 is None and diameter2 is None : 
~~~ radius2 = radius 
~~ ~~ if diameter is not None : 
~~~ radius1 = diameter / 2 
~~~ radius2 = diameter / 2 
~~ ~~ if diameter1 is not None : 
~~~ radius1 = diameter1 / 2 
~~ if diameter2 is not None : 
~~~ radius2 = diameter2 / 2 
~~ if radius1 is None : 
~~~ radius1 = 1.0 
~~ if radius2 is None : 
~~~ radius2 = radius1 
\ % height , 
\ % radius1 , 
\ % radius2 , 
\ % cir_segments , 
~~~ script . add_layer ( 'Cone' , change_layer = True ) 
~~ if not center : 
~~~ transform . translate ( script , [ 0 , height / 2 , 0 ] ) 
~~ if up . lower ( ) == 'z' : 
~~ def icosphere ( script , radius = 1.0 , diameter = None , subdivisions = 3 , color = None ) : 
if diameter is not None : 
~~~ radius = diameter / 2 
\ % radius , 
\ % subdivisions , 
~~~ script . add_layer ( 'Sphere' , change_layer = True ) 
~~ def sphere_cap ( script , angle = 1.0 , subdivisions = 3 , color = None ) : 
filter_xml = '' . join ( [ 
\ % angle , 
~~ def torus ( script , major_radius = 3.0 , minor_radius = 1.0 , inner_diameter = None , 
outer_diameter = None , major_segments = 48 , minor_segments = 12 , 
color = None ) : 
if inner_diameter is not None and outer_diameter is not None : 
~~~ major_radius = ( inner_diameter + outer_diameter ) / 4 
minor_radius = major_radius - inner_diameter / 2 
\ % major_radius , 
\ % minor_radius , 
\ % major_segments , 
\ % minor_segments , 
~~~ script . add_layer ( 'Torus' , change_layer = True ) 
~~ def grid ( script , size = 1.0 , x_segments = 1 , y_segments = 1 , center = False , 
size = util . make_list ( size , 2 ) 
\ . format ( size [ 0 ] ) , 
\ . format ( size [ 1 ] ) , 
\ . format ( x_segments + 1 ) , 
\ . format ( y_segments + 1 ) , 
transform . vert_function ( script , z_func = 'rint(z)' ) 
~~~ transform . translate ( script , value = [ size [ 0 ] / 2 , - size [ 1 ] / 2 , 0 ] ) 
~~~ transform . translate ( script , value = [ size [ 0 ] , 0 , 0 ] ) 
~~ def annulus ( script , radius = None , radius1 = None , radius2 = None , diameter = None , 
diameter1 = None , diameter2 = None , cir_segments = 32 , color = None ) : 
~~~ radius2 = 0 
~~~ radius1 = 1 
~~~ script . add_layer ( 'Annulus' , change_layer = True ) 
~~ def cylinder_open_hires ( script , height = 1.0 , radius = 1 , diameter = None , 
cir_segments = 48 , height_segments = 1 , 
invert_normals = False , center = False , color = None ) : 
~~~ z_translate = - height / 2 
~~~ z_translate = 0.0 
~~ grid ( script , 
[ 2 * math . pi * radius , height ] , 
x_segments = cir_segments , 
y_segments = height_segments ) 
transform . rotate ( script , 'x' , 90 ) 
transform . translate ( script , [ math . pi * radius / 2 , 0 , z_translate ] ) 
if not invert_normals : 
~~~ transform . rotate ( script , 'z' , 180 ) 
~~ transform . wrap2cylinder ( script , radius ) 
clean . merge_vert ( script , threshold = 0.00002 ) 
if color is not None : 
~~ def cube_open_hires_old ( script , size = 1.0 , x_segments = 1 , y_segments = 1 , z_segments = 1 , 
center = False , color = None ) : 
grid ( script , [ size [ 0 ] , size [ 2 ] ] , 
x_segments = x_segments , 
y_segments = z_segments ) 
layers . duplicate ( script ) 
transform . rotate ( script , 'z' , 180 ) 
transform . translate ( script , [ size [ 0 ] , size [ 1 ] , 0 ] ) 
grid ( script , [ size [ 2 ] , size [ 1 ] ] , 
x_segments = z_segments , 
y_segments = y_segments ) 
transform . rotate ( script , 'y' , - 90 ) 
layers . join ( script ) 
~~~ transform . translate ( script , [ - size [ 0 ] / 2 , - size [ 1 ] / 2 , - size [ 2 ] / 2 ] ) 
~~ def cube_open_hires ( script , size = 1.0 , x_segments = 1 , y_segments = 1 , z_segments = 1 , 
grid ( script , [ 2 * ( x_segments + y_segments ) , z_segments ] , 
x_segments = 2 * ( x_segments + y_segments ) , 
~~~ transform . vert_function ( script , 
z_func = 'z' ) 
transform . vert_function ( script , 
~~ clean . merge_vert ( script , threshold = 0.00002 ) 
transform . scale ( script , [ size [ 0 ] / x_segments , size [ 1 ] / y_segments , size [ 2 ] / z_segments ] ) 
~~ def plane_hires_edges ( script , size = 1.0 , x_segments = 1 , y_segments = 1 , 
grid ( script , size = [ x_segments + y_segments - 1 , 1 ] , 
x_segments = ( x_segments + y_segments - 1 ) , y_segments = 1 ) 
if ml_script1 . ml_version == '1.3.4BETA' : 
~~~ and_val = 'and' 
~~~ and_val = '&&' 
~~~ transform . vert_function ( 
script , 
y_segments , size [ 1 ] / y_segments ) ) 
transform . vert_function ( 
y_segments , y_segments , size [ 0 ] / x_segments ) , 
x_segments , size [ 0 ] ) , 
x_segments , x_segments , size [ 1 ] / y_segments ) ) 
x_segments , size [ 0 ] / x_segments ) , 
~~~ transform . translate ( script , [ - size [ 0 ] / 2 , - size [ 1 ] / 2 ] ) 
~~ def cube_hires ( script , size = 1.0 , x_segments = 1 , y_segments = 1 , z_segments = 1 , 
simple_bottom = True , center = False , color = None ) : 
grid ( script , 
size , 
x_segments , 
y_segments ) 
transform . translate ( script , [ 0 , 0 , size [ 2 ] ] ) 
if simple_bottom : 
~~~ plane_hires_edges ( 
script , size , x_segments , y_segments ) 
~~~ layers . duplicate ( script ) 
transform . translate ( script , [ 0 , 0 , - size [ 2 ] ] ) 
~~ transform . rotate ( script , 'x' , 180 ) 
transform . translate ( script , [ 0 , size [ 1 ] , 0 ] ) 
cube_open_hires ( 
script = script , size = size , x_segments = x_segments , 
y_segments = y_segments , z_segments = z_segments ) 
~~ def annulus_hires ( script , radius = None , radius1 = None , radius2 = None , 
diameter = None , diameter1 = None , diameter2 = None , 
cir_segments = 48 , rad_segments = 1 , color = None ) : 
~~ ring = ( radius1 - radius2 ) / rad_segments 
for i in range ( 0 , rad_segments ) : 
~~~ annulus ( script , 
radius1 = radius1 - i * ring , 
radius2 = radius1 - ( i + 1 ) * ring , 
cir_segments = cir_segments ) 
~~ layers . join ( script , merge_vert = True ) 
~~ def tube_hires ( script , height = 1.0 , radius = None , radius1 = None , radius2 = None , 
diameter = None , diameter1 = None , diameter2 = None , cir_segments = 32 , 
rad_segments = 1 , height_segments = 1 , center = False , 
simple_bottom = False , color = None ) : 
~~ annulus_hires ( script , 
radius1 = radius1 , 
radius2 = radius2 , 
cir_segments = cir_segments , 
rad_segments = rad_segments ) 
transform . translate ( script , [ 0 , 0 , height ] ) 
transform . translate ( script , [ 0 , 0 , - height ] ) 
cylinder_open_hires ( script , height , radius1 , 
height_segments = height_segments ) 
if radius2 != 0 : 
~~~ cylinder_open_hires ( script , height , radius2 , 
height_segments = height_segments , 
invert_normals = True ) 
~~ layers . join ( script ) 
~~~ transform . translate ( script , [ 0 , 0 , - height / 2 ] ) 
~~ def color_values ( color ) : 
this_dir = os . path . dirname ( 
inspect . getsourcefile ( 
lambda : 0 ) ) ) 
color_name_file = os . path . join ( this_dir , 'color_names.txt' ) 
found = False 
for line in open ( color_name_file , 'r' ) : 
~~~ line = line . rstrip ( ) 
if color . lower ( ) == line . split ( ) [ 0 ] : 
~~~ red = line . split ( ) [ 2 ] 
green = line . split ( ) [ 3 ] 
blue = line . split ( ) [ 4 ] 
found = True 
~~ ~~ if not found : 
~~~ print ( \ % color ) 
red = 255 
green = 255 
blue = 255 
~~ return red , green , blue 
~~ def check_list ( var , num_terms ) : 
if not isinstance ( var , list ) : 
~~~ if isinstance ( var , tuple ) : 
~~~ var = list ( var ) 
~~~ var = [ var ] 
~~ for _ in range ( 1 , num_terms ) : 
~~~ var . append ( var [ 0 ] ) 
~~ ~~ if len ( var ) != num_terms : 
~~~ print ( 
\ % 
( var , num_terms ) ) 
~~ return var 
~~ def make_list ( var , num_terms = 1 ) : 
for _ in range ( 1 , num_terms ) : 
~~ ~~ ~~ return var 
~~ def write_filter ( script , filter_xml ) : 
if isinstance ( script , mlx . FilterScript ) : 
~~~ script . filters . append ( filter_xml ) 
~~ elif isinstance ( script , str ) : 
~~~ script_file = open ( script , 'a' ) 
script_file . write ( filter_xml ) 
script_file . close ( ) 
~~~ print ( filter_xml ) 
~~ def ls3loop ( script , iterations = 1 , loop_weight = 0 , edge_threshold = 0 , 
selected = False ) : 
\ . format ( loop_weight ) , 
\ . format ( iterations ) , 
\ . format ( edge_threshold ) , 
\ . format ( str ( selected ) . lower ( ) ) , 
~~ def merge_vert ( script , threshold = 0.0 ) : 
\ . format ( threshold ) , 
~~ def close_holes ( script , hole_max_edge = 30 , selected = False , 
sel_new_face = True , self_intersection = True ) : 
\ . format ( hole_max_edge ) , 
\ . format ( str ( sel_new_face ) . lower ( ) ) , 
\ . format ( str ( self_intersection ) . lower ( ) ) , 
~~ def split_vert_on_nonmanifold_face ( script , vert_displacement_ratio = 0.0 ) : 
\ . format ( vert_displacement_ratio ) , 
~~ def snap_mismatched_borders ( script , edge_dist_ratio = 0.01 , unify_vert = True ) : 
\ . format ( edge_dist_ratio ) , 
\ . format ( str ( unify_vert ) . lower ( ) ) , 
~~ def translate ( script , value = ( 0.0 , 0.0 , 0.0 ) ) : 
if not isinstance ( value , list ) : 
~~~ value = list ( value ) 
~~ vert_function ( script , 
x_func = 'x+(%s)' % value [ 0 ] , 
y_func = 'y+(%s)' % value [ 1 ] , 
z_func = 'z+(%s)' % value [ 2 ] ) 
~~ def rotate ( script , axis = 'z' , angle = 0.0 ) : 
angle = math . radians ( angle ) 
if axis . lower ( ) == 'x' : 
~~~ vert_function ( script , 
x_func = 'x' , 
y_func = 'y*cos({angle})-z*sin({angle})' . format ( angle = angle ) , 
z_func = 'y*sin({angle})+z*cos({angle})' . format ( angle = angle ) ) 
~~ elif axis . lower ( ) == 'y' : 
x_func = 'z*sin({angle})+x*cos({angle})' . format ( angle = angle ) , 
y_func = 'y' , 
z_func = 'z*cos({angle})-x*sin({angle})' . format ( angle = angle ) ) 
~~ elif axis . lower ( ) == 'z' : 
x_func = 'x*cos({angle})-y*sin({angle})' . format ( angle = angle ) , 
y_func = 'x*sin({angle})+y*cos({angle})' . format ( angle = angle ) , 
~~ def scale ( script , value = 1.0 ) : 
value = util . make_list ( value , 3 ) 
vert_function ( script , 
x_func = 'x*(%s)' % value [ 0 ] , 
y_func = 'y*(%s)' % value [ 1 ] , 
z_func = 'z*(%s)' % value [ 2 ] ) 
~~ def freeze_matrix ( script , all_layers = False ) : 
\ % str ( all_layers ) . lower ( ) , 
~~ def function ( script , x_func = 'x' , y_func = 'y' , z_func = 'z' ) : 
\ . format ( str ( x_func ) . replace ( '&' , '&amp;' ) . replace ( '<' , '&lt;' ) ) , 
\ . format ( str ( y_func ) . replace ( '&' , '&amp;' ) . replace ( '<' , '&lt;' ) ) , 
\ . format ( str ( z_func ) . replace ( '&' , '&amp;' ) . replace ( '<' , '&lt;' ) ) , 
~~ def vert_function ( script , x_func = 'x' , y_func = 'y' , z_func = 'z' , selected = False ) : 
~~~ filter_xml = '' . join ( [ 
\ % str ( selected ) . lower ( ) , 
~~ util . write_filter ( script , filter_xml ) 
~~ def function_cyl_co ( script , r_func = 'r' , theta_func = 'theta' , z_func = 'z' ) : 
r = 'sqrt(x^2+y^2)' 
if isinstance ( script , FilterScript ) and script . ml_version >= '2016.12' : 
~~~ theta = mp_func . mp_atan2 ( 'y' , 'x' ) 
~~ r_func = re . sub ( r"\\br\\b" , r , r_func ) . replace ( 'theta' , theta ) 
theta_func = re . sub ( r"\\br\\b" , r , theta_func ) . replace ( 'theta' , theta ) 
z_func = re . sub ( r"\\br\\b" , r , z_func ) . replace ( 'theta' , theta ) 
x_func = '(r)*cos(theta)' . replace ( 'r' , r_func ) . replace ( 'theta' , theta_func ) 
y_func = '(r)*sin(theta)' . replace ( 'r' , r_func ) . replace ( 'theta' , theta_func ) 
vert_function ( script , x_func , y_func , z_func ) 
~~ def radial_flare2 ( script , flare_radius = None , start_radius = None , end_radius = None , 
end_height = None ) : 
if ( end_radius is not None ) and ( end_height is not None ) : 
~~~ if ( end_radius - start_radius ) < end_height : 
~~~ flare_radius = - ( ( start_radius - end_radius ) ** 2 + end_height ** 2 ) / ( 2 * ( start_radius - end_radius ) ) 
function_cyl_co ( script , r_func ) 
~~ def radial_flare ( script , flare_radius = None , start_radius = None , end_radius = None , 
r_func = r_func . replace ( 'effective_radius' , str ( effective_radius ) ) . replace ( 'start_radius' , str ( start_radius ) ) . replace ( 'flare_radius' , str ( flare_radius ) ) 
z_func = z_func . replace ( 'effective_radius' , str ( effective_radius ) ) . replace ( 'start_radius' , str ( start_radius ) ) . replace ( 'flare_radius' , str ( flare_radius ) ) 
function_cyl_co ( script = script , r_func = r_func , z_func = z_func ) 
~~ def curl_rim ( script , curl_radius = None , start_radius = None , end_radius = None , 
r_func = r_func . replace ( 'effective_radius' , str ( effective_radius ) ) . replace ( 'start_radius' , str ( start_radius ) ) . replace ( 'curl_radius' , str ( curl_radius ) ) 
z_func = z_func . replace ( 'effective_radius' , str ( effective_radius ) ) . replace ( 'start_radius' , str ( start_radius ) ) . replace ( 'curl_radius' , str ( curl_radius ) ) 
~~ def wrap2cylinder ( script , radius = 1 , pitch = 0 , taper = 0 , pitch_func = None , 
taper_func = None ) : 
if pitch_func is None : 
~~~ pitch_func = '-(pitch)*x/(2*pi*(radius))' 
~~ pitch_func = pitch_func . replace ( 
'pitch' , str ( pitch ) ) . replace ( 
'pi' , str ( math . pi ) ) . replace ( 
'radius' , str ( radius ) ) 
if taper_func is None : 
~~~ taper_func = '-(taper)*(pitch_func)' 
~~ taper_func = taper_func . replace ( 
'taper' , str ( taper ) ) . replace ( 
'pitch_func' , str ( pitch_func ) ) . replace ( 
'pi' , str ( math . pi ) ) 
x_func = '(y+(radius)+(taper_func))*sin(x/(radius))' . replace ( 
'radius' , str ( radius ) ) . replace ( 'taper_func' , str ( taper_func ) ) 
y_func = '(y+(radius)+(taper_func))*cos(x/(radius))' . replace ( 
z_func = 'z+(pitch_func)' . replace ( 'pitch_func' , str ( pitch_func ) ) 
~~ def bend ( script , radius = 1 , pitch = 0 , taper = 0 , angle = 0 , straght_start = True , 
straght_end = False , radius_limit = None , outside_limit_end = True ) : 
if radius_limit is None : 
~~~ radius_limit = 2 * radius 
~~ angle = math . radians ( angle ) 
segment = radius * angle 
pitch_func = '-(pitch)*x/(2*pi*(radius))' . replace ( 
taper_func = '(taper)*(pitch_func)' . replace ( 
if outside_limit_end : 
~~ x_func = x_func . replace ( 
'segment' , str ( segment ) ) . replace ( 
'radius_limit' , str ( radius_limit ) ) . replace ( 
'radius' , str ( radius ) ) . replace ( 
'taper_func' , str ( taper_func ) ) . replace ( 
'angle' , str ( angle ) ) 
~~ y_func = y_func . replace ( 
if straght_start : 
~~~ start = 'z' 
~~~ start = 'z+(pitch_func)' 
~~ if straght_end : 
~~~ end = 'z-(pitch)*(angle)/(2*pi)' 
~~~ end = 'z+(pitch_func)' 
~~ if outside_limit_end : 
~~ z_func = z_func . replace ( 
'start' , str ( start ) ) . replace ( 
'end' , str ( end ) ) . replace ( 
'angle' , str ( angle ) ) . replace ( 
vert_function ( script , x_func = x_func , y_func = y_func , z_func = z_func ) 
~~ def deform2curve ( script , curve = mp_func . torus_knot ( 't' ) , step = 0.001 ) : 
curve_step = [ ] 
for idx , val in enumerate ( curve ) : 
~~~ curve [ idx ] = val . replace ( 't' , 'z' ) 
curve_step . append ( val . replace ( 't' , 'z+{}' . format ( step ) ) ) 
~~ tangent = mp_func . v_subtract ( curve_step , curve ) 
normal1 = mp_func . v_add ( curve_step , curve ) 
bee = mp_func . v_cross ( tangent , normal1 ) 
normal = mp_func . v_cross ( bee , tangent ) 
bee = mp_func . v_normalize ( bee ) 
normal = mp_func . v_normalize ( normal ) 
new_point = mp_func . v_add ( mp_func . v_multiply ( 'x' , normal ) , mp_func . v_multiply ( 'y' , bee ) ) 
function = mp_func . v_add ( curve , new_point ) 
vert_function ( script , x_func = function [ 0 ] , y_func = function [ 1 ] , z_func = function [ 2 ] ) 
return function 
~~ def vc2tex ( script , tex_name = 'TEMP3D_texture.png' , tex_width = 1024 , 
tex_height = 1024 , overwrite_tex = False , assign_tex = False , 
fill_tex = True ) : 
\ % tex_name , 
\ % tex_width , 
\ % tex_height , 
\ % str ( overwrite_tex ) . lower ( ) , 
\ % str ( assign_tex ) . lower ( ) , 
\ % str ( fill_tex ) . lower ( ) , 
~~ def mesh2fc ( script , all_visible_layers = False ) : 
\ % str ( all_visible_layers ) . lower ( ) , 
~~ def vert_attr_2_meshes ( script , source_mesh = 0 , target_mesh = 1 , 
geometry = False , normal = False , color = True , 
quality = False , selection = False , 
quality_distance = False , max_distance = 0.5 ) : 
\ . format ( source_mesh ) , 
\ . format ( target_mesh ) , 
\ . format ( str ( geometry ) . lower ( ) ) , 
\ . format ( str ( normal ) . lower ( ) ) , 
\ . format ( str ( color ) . lower ( ) ) , 
\ . format ( str ( quality ) . lower ( ) ) , 
\ . format ( str ( selection ) . lower ( ) ) , 
\ . format ( str ( quality_distance ) . lower ( ) ) , 
\ . format ( max_distance ) , 
~~ def vert_attr2tex_2_meshes ( script , source_mesh = 0 , target_mesh = 1 , attribute = 0 , 
max_distance = 0.5 , tex_name = 'TEMP3D_texture.png' , 
tex_width = 1024 , tex_height = 1024 , 
overwrite_tex = True , assign_tex = False , 
\ % source_mesh , 
\ % target_mesh , 
\ % attribute , 
\ % max_distance , 
~~ def tex2vc_2_meshes ( script , source_mesh = 0 , target_mesh = 1 , max_distance = 0.5 ) : 
~~ def simplify ( script , texture = True , faces = 25000 , target_perc = 0.0 , 
quality_thr = 0.3 , preserve_boundary = False , boundary_weight = 1.0 , 
optimal_placement = True , preserve_normal = False , 
planar_quadric = False , selected = False , extra_tex_coord_weight = 1.0 , 
preserve_topology = True , quality_weight = False , autoclean = True ) : 
if texture : 
~~~ if isinstance ( script , FilterScript ) and ( script . ml_version == '2016.12' ) : 
~~~ filter_xml = \ 
~~ ~~ filter_xml = '' . join ( [ 
filter_xml , 
\ . format ( faces ) , 
\ . format ( target_perc ) , 
\ . format ( quality_thr ) , 
\ . format ( str ( preserve_boundary ) . lower ( ) ) , 
\ . format ( boundary_weight ) , 
\ . format ( str ( optimal_placement ) . lower ( ) ) , 
\ . format ( str ( preserve_normal ) . lower ( ) ) , 
\ . format ( str ( planar_quadric ) . lower ( ) ) , 
'/>\\n' ] ) 
\ . format ( extra_tex_coord_weight ) , 
\ . format ( str ( preserve_topology ) . lower ( ) ) , 
\ . format ( str ( quality_weight ) . lower ( ) ) , 
\ . format ( str ( autoclean ) . lower ( ) ) , 
~~ def uniform_resampling ( script , voxel = 1.0 , offset = 0.0 , merge_vert = True , 
discretize = False , multisample = False , thicken = False ) : 
\ . format ( voxel ) , 
\ . format ( offset ) , 
\ . format ( str ( merge_vert ) . lower ( ) ) , 
\ . format ( str ( discretize ) . lower ( ) ) , 
\ . format ( str ( multisample ) . lower ( ) ) , 
\ . format ( str ( thicken ) . lower ( ) ) , 
~~ def hull ( script , reorient_normal = True ) : 
\ . format ( str ( reorient_normal ) . lower ( ) ) , 
~~ def surface_poisson ( script , octree_depth = 10 , solver_divide = 8 , 
samples_per_node = 1.0 , offset = 1.0 ) : 
\ . format ( octree_depth ) , 
\ . format ( solver_divide ) , 
\ . format ( samples_per_node ) , 
~~ def surface_poisson_screened ( script , visible_layer = False , depth = 8 , 
full_depth = 5 , cg_depth = 0 , scale = 1.1 , 
samples_per_node = 1.5 , point_weight = 4.0 , 
iterations = 8 , confidence = False , pre_clean = False ) : 
\ . format ( cg_depth ) , 
\ . format ( str ( confidence ) . lower ( ) ) , 
\ . format ( depth ) , 
\ . format ( full_depth ) , 
\ . format ( point_weight ) , 
\ . format ( str ( pre_clean ) . lower ( ) ) , 
\ . format ( scale ) , 
\ . format ( str ( visible_layer ) . lower ( ) ) , 
~~ def curvature_flipping ( script , angle_threshold = 1.0 , curve_type = 0 , 
\ . format ( angle_threshold ) , 
\ . format ( curve_type ) , 
~~ def voronoi ( script , hole_num = 50 , target_layer = None , sample_layer = None , thickness = 0.5 , backward = True ) : 
if target_layer is None : 
~~~ target_layer = script . current_layer ( ) 
~~ if sample_layer is None : 
~~~ sampling . poisson_disk ( script , sample_num = hole_num ) 
sample_layer = script . last_layer ( ) 
~~ vert_color . voronoi ( script , target_layer = target_layer , source_layer = sample_layer , backward = backward ) 
select . vert_quality ( script , min_quality = 0.0 , max_quality = thickness ) 
if backward : 
~~~ select . invert ( script ) 
~~ delete . selected ( script ) 
smooth . laplacian ( script , iterations = 3 ) 
~~ def all ( script , face = True , vert = True ) : 
\ . format ( str ( face ) . lower ( ) ) , 
\ . format ( str ( vert ) . lower ( ) ) , 
~~ def grow ( script , iterations = 1 ) : 
filter_xml = \ 
for _ in range ( iterations ) : 
~~~ util . write_filter ( script , filter_xml ) 
~~ def shrink ( script , iterations = 1 ) : 
~~ def small_parts ( script , ratio = 0.2 , non_closed_only = False ) : 
\ . format ( ratio ) , 
\ . format ( str ( non_closed_only ) . lower ( ) ) , 
~~ def vert_quality ( script , min_quality = 0.0 , max_quality = 0.05 , inclusive = True ) : 
\ . format ( min_quality ) , 
\ . format ( 2 * max_quality ) , 
\ . format ( max_quality ) , 
\ . format ( str ( inclusive ) . lower ( ) ) , 
\ . format ( str ( function ) . replace ( '&' , '&amp;' ) . replace ( '<' , '&lt;' ) ) , 
~~~ strict_select = '' . join ( [ 
\ . format ( str ( strict_face_select ) . lower ( ) ) , 
~~~ strict_select = '' 
strict_select , 
~~ def cylindrical_vert ( script , radius = 1.0 , inside = True ) : 
if inside : 
~~~ function = 'sqrt(x^2+y^2)<={}' . format ( radius ) 
~~~ function = 'sqrt(x^2+y^2)>={}' . format ( radius ) 
~~ vert_function ( script , function = function ) 
~~ def spherical_vert ( script , radius = 1.0 , center_pt = ( 0.0 , 0.0 , 0.0 ) ) : 
function = 'sqrt((x-{})^2+(y-{})^2+(z-{})^2)<={}' . format ( 
center_pt [ 0 ] , center_pt [ 1 ] , center_pt [ 2 ] , radius ) 
vert_function ( script , function = function ) 
~~ def join ( script , merge_visible = True , merge_vert = False , delete_layer = True , 
keep_unreferenced_vert = False ) : 
\ . format ( str ( merge_visible ) . lower ( ) ) , 
\ . format ( str ( delete_layer ) . lower ( ) ) , 
\ . format ( str ( keep_unreferenced_vert ) . lower ( ) ) , 
if delete_layer : 
~~~ for i in range ( script . last_layer ( ) ) : 
~~~ script . del_layer ( 0 ) 
~~ ~~ ~~ return None 
~~ def delete ( script , layer_num = None ) : 
~~~ if ( layer_num is None ) or ( layer_num == script . current_layer ( ) ) : 
script . del_layer ( script . current_layer ( ) ) 
~~~ cur_layer = script . current_layer ( ) 
change ( script , layer_num ) 
if layer_num < script . current_layer ( ) : 
~~~ change ( script , cur_layer - 1 ) 
~~~ change ( script , cur_layer ) 
~~ script . del_layer ( layer_num ) 
~~ def rename ( script , label = 'blank' , layer_num = None ) : 
\ . format ( label ) , 
script . layer_stack [ script . current_layer ( ) ] = label 
change ( script , cur_layer ) 
script . layer_stack [ layer_num ] = label 
~~ def change ( script , layer_num = None ) : 
if layer_num is None : 
~~~ if isinstance ( script , mlx . FilterScript ) : 
~~~ layer_num = script . last_layer ( ) 
~~~ layer_num = 0 
\ . format ( layer_num ) , 
~~~ script . set_current_layer ( layer_num ) 
~~ def duplicate ( script , layer_num = None ) : 
script . add_layer ( '{}_copy' . format ( script . layer_stack [ script . current_layer ( ) ] ) , True ) 
~~~ change ( script , layer_num ) 
script . add_layer ( '{}_copy' . format ( script . layer_stack [ layer_num ] ) , True ) 
~~ def split_parts ( script , part_num = None , layer_num = None ) : 
~~~ if ( layer_num is not None ) and ( layer_num != script . current_layer ( ) ) : 
if part_num is not None : 
~~~ for i in range ( part_num ) : 
'incorrect!' ) 
~~ def delete_lower ( script , layer_num = None ) : 
~~~ layer_num = script . current_layer ( ) 
~~ if layer_num != 0 : 
~~~ change ( script , 0 ) 
~~ for i in range ( layer_num ) : 
~~~ delete ( script , 0 ) 
~~ def handle_error ( program_name , cmd , log = None ) : 
print ( \ % ( program_name , cmd ) ) 
if log is not None : 
~~~ print ( \ % log ) 
if choice not in ( 'r' , 'c' , 'x' , 'xd' ) : 
~~~ choice = 'x' 
#else: 
~~ break 
~~ if choice == 'x' : 
~~ elif choice == 'xd' : 
util . delete_all ( 'TEMP3D*' ) 
~~~ os . remove ( log ) 
~~ sys . exit ( 1 ) 
~~ elif choice == 'c' : 
break_now = True 
~~ elif choice == 'r' : 
break_now = False 
~~ return break_now 
~~ def run ( script = 'TEMP3D_default.mlx' , log = None , ml_log = None , 
mlp_in = None , mlp_out = None , overwrite = False , file_in = None , 
file_out = None , output_mask = None , cmd = None , ml_version = ML_VERSION , 
print_meshlabserver_output = True ) : 
if cmd is None : 
~~~ cmd = 'meshlabserver' 
if ml_log is not None : 
~~~ ml_log_file = open ( ml_log , 'w' ) 
ml_log_file . close ( ) 
~~ if mlp_in is not None : 
~~~ mlp_in = util . make_list ( mlp_in ) 
for val in mlp_in : 
~~~ cmd += \ % val 
~~ ~~ if mlp_out is not None : 
if overwrite : 
~~ ~~ if ( mlp_in is None ) and ( file_in is None ) : 
~~~ file_in = [ 'TEMP3D.xyz' ] 
~~ if file_in is not None : 
~~~ file_in = util . make_list ( file_in ) 
for val in file_in : 
~~~ if val == 'bunny' : 
~~~ cmd += \ % os . path . join ( THIS_MODULEPATH , os . pardir , 
'models' , 'bunny_flat(1Z).ply' ) 
~~ elif val == 'bunny_raw' : 
'models' , 'bunny_raw(-1250Y).ply' ) 
~~ ~~ ~~ if file_out is not None : 
~~~ file_out = util . make_list ( file_out ) 
if output_mask is not None : 
~~~ output_mask = util . make_list ( output_mask ) 
~~~ output_mask = [ ] 
~~ for index , val in enumerate ( file_out ) : 
~~ ~~ ~~ if script is not None : 
~~~ cmd += \ % script 
~~ ~~ if log is not None : 
~~~ log_file = open ( log , 'a' ) 
log_file . close ( ) 
log_file = open ( log , 'a' ) 
~~~ if print_meshlabserver_output : 
~~~ log_file = None 
~~~ log_file = open ( os . devnull , 'w' ) 
~~ ~~ while True : 
~~~ return_code = subprocess . call ( cmd , shell = True , 
stdout = log_file , stderr = log_file , 
universal_newlines = True ) 
~~~ log_file . close ( ) 
~~ if ( return_code == 0 ) or handle_error ( program_name = 'MeshLab' , cmd = cmd , log = log ) : 
~~ return return_code 
~~ def find_texture_files ( fbasename , log = None ) : 
fext = os . path . splitext ( fbasename ) [ 1 ] [ 1 : ] . strip ( ) . lower ( ) 
material_file = None 
texture_files = [ ] 
vert_colors = False 
face_colors = False 
if fext == 'obj' : 
~~~ with open ( fbasename , 'r' ) as fread : 
~~~ for line in fread : 
~~~ if 'mtllib' in line : 
~~~ material_file = os . path . basename ( line . split ( ) [ 1 ] ) 
~~ ~~ ~~ if material_file is not None : 
~~~ with open ( material_file , 'r' ) as fread : 
~~~ if 'map_Kd' in line : 
~~~ texture_files . append ( os . path . basename ( line . split ( ) [ 1 ] ) ) 
~~ ~~ ~~ ~~ ~~ elif fext == 'ply' : 
~~~ face_element = False 
with open ( fbasename , 'rb' ) as fread : 
~~~ line = fread . readline ( ) . strip ( ) . decode ( 'ascii' ) 
~~~ face_element = True 
~~ if 'red' in line : 
~~~ if face_element : 
~~~ face_colors = True 
~~~ vert_colors = True 
~~ ~~ if 'TextureFile' in line : 
~~~ texture_files . append ( os . path . basename ( line . split ( ) [ 2 ] ) ) 
~~ if 'end_header' in line : 
~~~ namespace = 'http://www.collada.org/2005/11/COLLADASchema' 
tree = ET . parse ( fbasename ) 
for elem in tree . findall ( 
'{%s}library_images/{%s}image/{%s}init_from' % ( namespace , namespace , namespace ) ) : 
~~~ texture_files . append ( elem . text ) 
~~ ~~ elif fext == 'x3d' : 
~~~ tree = ET . parse ( fbasename ) 
for elem in tree . iter ( tag = 'ImageTexture' ) : 
~~~ texture_files . append ( elem . attrib [ 'url' ] ) 
~~ ~~ elif fext == 'wrl' : 
~~~ if 'ImageTexture' in line : 
~~~ texture_files . append ( os . path . basename ( line . split ( \ ) [ 1 ] ) ) 
~~ texture_files_unique = list ( set ( texture_files ) ) 
log_file . write ( 
len ( texture_files_unique ) ) 
~~ colors = { 'texture' : bool ( texture_files ) , 'vert_colors' : vert_colors , 'face_colors' : face_colors } 
return texture_files , texture_files_unique , material_file , colors 
~~ def default_output_mask ( file_out , texture = True , vert_normals = True , vert_colors = False , 
face_colors = False , ml_version = ML_VERSION ) : 
vn = '' 
wt = '' 
vc = '' 
fc = '' 
if ml_version < '1.3.4' : 
~~~ om = '-om' 
~~~ om = '-m' 
~~ fext = os . path . splitext ( file_out ) [ 1 ] [ 1 : ] . strip ( ) . lower ( ) 
if fext in [ 'stl' , 'dxf' , 'xyz' ] : 
~~~ om = '' 
texture = False 
vert_normals = False 
~~ if vert_normals : 
~~ if texture : 
~~ if vert_colors : 
~~ if face_colors : 
~~ output_mask = '{}{}{}{}{}' . format ( om , vn , wt , vc , fc ) 
return output_mask 
~~ def begin ( script = 'TEMP3D_default.mlx' , file_in = None , mlp_in = None ) : 
script_file = open ( script , 'w' ) 
'<FilterScript>\\n' ] ) ) 
current_layer = - 1 
last_layer = - 1 
stl = False 
if mlp_in is not None : 
~~~ if not isinstance ( mlp_in , list ) : 
~~~ mlp_in = [ mlp_in ] 
~~ for val in mlp_in : 
~~~ tree = ET . parse ( val ) 
for elem in tree . iter ( tag = 'MLMesh' ) : 
~~~ filename = ( elem . attrib [ 'filename' ] ) 
current_layer += 1 
last_layer += 1 
if os . path . splitext ( filename ) [ 1 ] [ 1 : ] . strip ( ) . lower ( ) == 'stl' : 
~~~ layers . change ( script , current_layer ) 
clean . merge_vert ( script ) 
stl = True 
~~ ~~ ~~ ~~ if file_in is not None : 
~~~ if not isinstance ( file_in , list ) : 
~~~ file_in = [ file_in ] 
~~ for val in file_in : 
~~~ current_layer += 1 
if os . path . splitext ( val ) [ 1 ] [ 1 : ] . strip ( ) . lower ( ) == 'stl' : 
~~ ~~ ~~ if stl : 
~~ elif last_layer == - 1 : 
file_in_descriptor = open ( file_in [ 0 ] , 'w' ) 
file_in_descriptor . close ( ) 
layers . delete ( script ) 
~~ return current_layer , last_layer 
~~ def create_mlp ( file_out , mlp_mesh = None , mlp_raster = None ) : 
mlp_file = open ( file_out , 'w' ) 
mlp_file . write ( '\\n' . join ( [ 
'<MeshLabProject>\\n' ] ) ) 
mlp_file . close ( ) 
if mlp_mesh is not None : 
~~~ mlp_file = open ( file_out , 'a' ) 
for i , val in enumerate ( mlp_mesh ) : 
~~~ if 'label' not in mlp_mesh [ i ] : 
~~~ mlp_mesh [ i ] [ 'label' ] = mlp_mesh [ i ] [ 'filename' ] 
~~ if 'matrix' not in mlp_mesh [ i ] : 
~~~ mlp_mesh [ i ] [ 'matrix' ] = [ [ 1 , 0 , 0 , 0 ] , [ 0 , 1 , 0 , 0 ] , [ 0 , 0 , 1 , 0 ] , [ 0 , 0 , 0 , 1 ] ] 
~~ mlp_file . write ( \ . format ( mlp_mesh [ i ] [ 'filename' ] , mlp_mesh [ i ] [ 'label' ] ) ) 
'</MLMatrix44>' , 
~~ if mlp_raster is not None : 
for i , val in enumerate ( mlp_raster ) : 
~~~ if 'label' not in mlp_raster [ i ] : 
~~~ mlp_raster [ i ] [ 'label' ] = mlp_raster [ i ] [ 'filename' ] 
~~ if 'semantic' not in mlp_raster [ i ] : 
~~~ mlp_raster [ i ] [ 'semantic' ] = 1 
~~ if 'lens_distortion' not in mlp_raster [ i ] [ 'camera' ] : 
~~~ mlp_raster [ i ] [ 'camera' ] [ 'lens_distortion' ] = [ 0 , 0 ] 
~~ if 'center_px' not in mlp_raster [ i ] [ 'camera' ] : 
~~~ mlp_raster [ i ] [ 'camera' ] [ 'center_px' ] = [ int ( mlp_raster [ i ] [ 'camera' ] [ 'image_px' ] [ 0 ] / 2 ) , int ( mlp_raster [ i ] [ 'camera' ] [ 'image_px' ] [ 1 ] / 2 ) ] 
~~ mlp_file . write ( \ . format ( mlp_raster [ i ] [ 'label' ] ) ) 
\ . format ( m = mlp_raster [ i ] [ 'camera' ] [ 'trans_vector' ] ) , 
\ . format ( m = mlp_raster [ i ] [ 'camera' ] [ 'rotation_matrix' ] ) , 
\ . format ( mlp_raster [ i ] [ 'camera' ] [ 'focal_length' ] ) , 
\ . format ( m = mlp_raster [ i ] [ 'camera' ] [ 'image_px' ] ) , 
\ . format ( m = mlp_raster [ i ] [ 'camera' ] [ 'image_res_mm_per_px' ] ) , 
\ . format ( m = mlp_raster [ i ] [ 'camera' ] [ 'lens_distortion' ] ) , 
\ . format ( m = mlp_raster [ i ] [ 'camera' ] [ 'center_px' ] ) , 
'/>\\n' ] ) ) 
mlp_file . write ( \ . format ( mlp_raster [ i ] [ 'semantic' ] , mlp_raster [ i ] [ 'filename' ] ) ) 
~~ mlp_file = open ( file_out , 'a' ) 
mlp_file . write ( '</MeshLabProject>\\n' ) 
~~ def add_layer ( self , label , change_layer = True ) : 
self . layer_stack . insert ( self . last_layer ( ) + 1 , label ) 
if change_layer : 
~~~ self . set_current_layer ( self . last_layer ( ) ) 
~~ def del_layer ( self , layer_num ) : 
del self . layer_stack [ layer_num ] 
if layer_num < self . current_layer ( ) : 
~~~ self . set_current_layer ( self . current_layer ( ) - 1 ) 
~~ def save_to_file ( self , script_file ) : 
if not self . filters : 
~~ script_file_descriptor = open ( script_file , 'w' ) 
script_file_descriptor . write ( '' . join ( self . opening + self . filters + self . closing ) ) 
script_file_descriptor . close ( ) 
~~ def run_script ( self , log = None , ml_log = None , mlp_out = None , overwrite = False , 
file_out = None , output_mask = None , script_file = None , print_meshlabserver_output = True ) : 
temp_script = False 
temp_ml_log = False 
if self . __no_file_in : 
~~~ temp_file_in_file = tempfile . NamedTemporaryFile ( delete = False , suffix = '.xyz' , dir = os . getcwd ( ) ) 
temp_file_in_file . close ( ) 
self . file_in = [ temp_file_in_file . name ] 
~~ if not self . filters : 
~~~ script_file = None 
~~ elif script_file is None : 
~~~ temp_script = True 
temp_script_file = tempfile . NamedTemporaryFile ( delete = False , suffix = '.mlx' ) 
temp_script_file . close ( ) 
self . save_to_file ( temp_script_file . name ) 
script_file = temp_script_file . name 
~~ if ( self . parse_geometry or self . parse_topology or self . parse_hausdorff ) and ( ml_log is None ) : 
~~~ temp_ml_log = True 
ml_log_file = tempfile . NamedTemporaryFile ( delete = False , suffix = '.txt' ) 
ml_log = ml_log_file . name 
~~ if file_out is None : 
~~~ file_out = self . file_out 
~~ run ( script = script_file , log = log , ml_log = ml_log , 
mlp_in = self . mlp_in , mlp_out = mlp_out , overwrite = overwrite , 
file_in = self . file_in , file_out = file_out , output_mask = output_mask , ml_version = self . ml_version , 
print_meshlabserver_output = print_meshlabserver_output ) 
if self . parse_geometry : 
~~~ self . geometry = compute . parse_geometry ( ml_log , log , print_output = print_meshlabserver_output ) 
~~ if self . parse_topology : 
~~~ self . topology = compute . parse_topology ( ml_log , log , print_output = print_meshlabserver_output ) 
~~ if self . parse_hausdorff : 
~~~ self . hausdorff_distance = compute . parse_hausdorff ( ml_log , log , print_output = print_meshlabserver_output ) 
~~ if self . __no_file_in : 
~~~ os . remove ( temp_file_in_file . name ) 
~~ if temp_script : 
~~~ os . remove ( temp_script_file . name ) 
~~ if temp_ml_log : 
~~~ os . remove ( ml_log_file . name ) 
segments = 50 
star_points = 5 
star_radius = 2 
ring_thickness = 1 
sphere_radius = 2 * ( star_radius + 3 * ring_thickness ) 
polygon_radius = star_radius / ( 1 + math . tan ( math . radians ( 180 / star_points ) ) / 
math . tan ( math . radians ( 90 / star_points ) ) ) 
width = polygon_radius * math . tan ( math . radians ( 180 / star_points ) ) 
height = width / math . tan ( math . radians ( 90 / star_points ) ) 
shield = mlx . FilterScript ( file_out = "shield.ply" ) 
mlx . create . annulus ( shield , radius = star_radius , cir_segments = segments , color = 'blue' ) 
mlx . create . annulus ( shield , 
radius1 = star_radius + ring_thickness , 
radius2 = star_radius , 
cir_segments = segments , 
color = 'red' ) 
radius1 = star_radius + 2 * ring_thickness , 
radius2 = star_radius + ring_thickness , 
color = 'white' ) 
radius1 = star_radius + 3 * ring_thickness , 
radius2 = star_radius + 2 * ring_thickness , 
mlx . layers . join ( shield ) 
mlx . subdivide . midpoint ( shield , iterations = 2 ) 
color = 'silver' ) 
mlx . transform . rotate ( shield , axis = 'y' , angle = 180 ) 
mlx . transform . translate ( shield , value = [ 0 , 0 , - 0.005 ] ) 
mlx . subdivide . midpoint ( shield , iterations = 4 ) 
mlx . create . grid ( shield , 
size = math . sqrt ( 2 ) , 
x_segments = 10 , 
y_segments = 10 , 
center = True , 
mlx . transform . rotate ( shield , axis = 'z' , angle = 45 ) 
mlx . transform . scale ( shield , value = [ width , height , 1 ] ) 
mlx . transform . translate ( shield , value = [ 0 , polygon_radius , 0.001 ] ) 
for _ in range ( 1 , star_points ) : 
~~~ mlx . layers . duplicate ( shield ) 
mlx . transform . rotate ( shield , axis = 'z' , angle = 360 / star_points ) 
~~ mlx . layers . join ( shield ) 
mlx . transform . vert_function ( shield , 
z_func = 'sqrt(%s-x^2-y^2)-%s+z' % 
( sphere_radius ** 2 , sphere_radius ) ) 
shield . run_script ( ) 
select . small_parts ( script , ratio , non_closed_only ) 
selected ( script ) 
~~ def selected ( script , face = True , vert = True ) : 
if face and vert : 
~~ elif face and not vert : 
~~ elif not face and vert : 
~~ def unreferenced_vert ( script ) : 
~~ def duplicate_verts ( script ) : 
~~ def hausdorff_distance ( script , sampled_layer = 1 , target_layer = 0 , 
save_sample = False , sample_vert = True , sample_edge = True , 
sample_faux_edge = False , sample_face = True , 
sample_num = 1000 , maxdist = 10 ) : 
maxdist_max = 2 * maxdist 
\ . format ( sampled_layer ) , 
\ . format ( target_layer ) , 
\ . format ( str ( save_sample ) . lower ( ) ) , 
\ . format ( str ( sample_vert ) . lower ( ) ) , 
\ . format ( str ( sample_edge ) . lower ( ) ) , 
\ . format ( str ( sample_faux_edge ) . lower ( ) ) , 
\ . format ( str ( sample_face ) . lower ( ) ) , 
\ % str ( sample_face ) . lower ( ) + 
\ . format ( sample_num ) , 
\ . format ( maxdist ) , 
\ % maxdist + 
\ . format ( maxdist_max ) , 
~~~ script . parse_hausdorff = True 
~~ if isinstance ( script , FilterScript ) and save_sample : 
~~ def poisson_disk ( script , sample_num = 1000 , radius = 0.0 , 
montecarlo_rate = 20 , save_montecarlo = False , 
approx_geodesic_dist = False , subsample = False , refine = False , 
refine_layer = 0 , best_sample = True , best_sample_pool = 10 , 
exact_num = False , radius_variance = 1.0 ) : 
\ . format ( radius ) , 
\ . format ( montecarlo_rate ) , 
\ . format ( str ( save_montecarlo ) . lower ( ) ) , 
\ . format ( str ( approx_geodesic_dist ) . lower ( ) ) , 
\ . format ( str ( subsample ) . lower ( ) ) , 
\ . format ( str ( refine ) . lower ( ) ) , 
\ . format ( refine_layer ) , 
\ . format ( str ( best_sample ) . lower ( ) ) , 
\ . format ( best_sample_pool ) , 
\ . format ( str ( exact_num ) . lower ( ) ) , 
\ . format ( radius_variance ) , 
if save_montecarlo : 
~~ def mesh_element ( script , sample_num = 1000 , element = 'VERT' ) : 
if element . lower ( ) == 'vert' : 
~~~ element_num = 0 
~~ elif element . lower ( ) == 'edge' : 
~~~ element_num = 1 
~~ elif element . lower ( ) == 'face' : 
~~~ element_num = 2 
\ . format ( element_num ) , 
~~ def clustered_vert ( script , cell_size = 1.0 , strategy = 'AVERAGE' , selected = False ) : 
if strategy . lower ( ) == 'average' : 
~~~ strategy_num = 0 
~~ elif strategy . lower ( ) == 'center' : 
~~~ strategy_num = 1 
\ . format ( cell_size ) , 
\ . format ( strategy_num ) , 
~~ def flat_plane ( script , plane = 0 , aspect_ratio = False ) : 
\ % plane , 
\ % str ( aspect_ratio ) . lower ( ) , 
~~ def per_triangle ( script , sidedim = 0 , textdim = 1024 , border = 2 , method = 1 ) : 
\ % sidedim , 
\ % textdim , 
\ % border , 
\ % method , 
\ 
~~ def voronoi ( script , region_num = 10 , overlap = False ) : 
\ % region_num , 
\ % str ( overlap ) . lower ( ) , 
~~ def isometric ( script , targetAbstractMinFaceNum = 140 , targetAbstractMaxFaceNum = 180 , 
stopCriteria = 1 , convergenceSpeed = 1 , DoubleStep = True ) : 
\ % targetAbstractMinFaceNum , 
\ % targetAbstractMaxFaceNum , 
\ % stopCriteria , 
\ % convergenceSpeed , 
\ % str ( DoubleStep ) . lower ( ) , 
~~ def isometric_build_atlased_mesh ( script , BorderSize = 0.1 ) : 
\ % BorderSize , 
~~ def isometric_save ( script , AbsName = "TEMP3D.abs" ) : 
\ % AbsName , 
~~ def isometric_load ( script , AbsName = "TEMP3D.abs" ) : 
~~ def isometric_transfer ( script , sourceMesh = 0 , targetMesh = 1 ) : 
\ % sourceMesh , 
\ % targetMesh , 
~~ def isometric_remesh ( script , SamplingRate = 10 ) : 
\ % SamplingRate , 
~~ def set_texture ( script , textName = "TEMP3D.png" , textDim = 1024 ) : 
\ % textName , 
\ % textDim , 
~~ def project_rasters ( script , tex_file_out = "TEMP3D.png" , tex_size = 1024 , 
fill_atlas_gaps = False , depth_threshold = 0.5 , 
selected = False , use_angle = True , use_distance = True , 
use_borders = True , use_silhouettes = True , use_alpha = False ) : 
\ % tex_file_out , 
\ % tex_size , 
\ % str ( fill_atlas_gaps ) . lower ( ) , 
\ % depth_threshold , 
\ % str ( use_angle ) . lower ( ) , 
\ % str ( use_distance ) . lower ( ) , 
\ % str ( use_borders ) . lower ( ) , 
\ % str ( use_silhouettes ) . lower ( ) , 
\ % str ( use_alpha ) . lower ( ) , 
~~ def param_texture_from_rasters ( script , textName = "TEMP3D.png" , texsize = 1024 , 
colorCorrection = True , colorCorrectionFilterSize = 1 , 
useDistanceWeight = True , useImgBorderWeight = True , 
useAlphaWeight = False , cleanIsolatedTriangles = True , 
stretchingAllowed = False , textureGutter = 4 ) : 
\ % texsize , 
\ % str ( colorCorrection ) . lower ( ) , 
\ % colorCorrectionFilterSize , 
\ % str ( useDistanceWeight ) . lower ( ) , 
\ % str ( useImgBorderWeight ) . lower ( ) , 
\ % str ( useAlphaWeight ) . lower ( ) , 
\ % str ( cleanIsolatedTriangles ) . lower ( ) , 
\ % str ( stretchingAllowed ) . lower ( ) , 
\ % textureGutter , 
~~ def param_from_rasters ( script , useDistanceWeight = True , useImgBorderWeight = True , 
~~ def section ( script , axis = 'z' , offset = 0.0 , surface = False , custom_axis = None , 
planeref = 2 ) : 
~~~ axis_num = 0 
axis_name = 'X' 
~~~ axis_num = 1 
axis_name = 'Y' 
~~~ axis_num = 2 
axis_name = 'Z' 
~~~ axis_num = 3 
axis_name = 'custom' 
if custom_axis is None : 
~~ ~~ if custom_axis is None : 
~~~ custom_axis = ( 0.0 , 0.0 , 1.0 ) 
\ . format ( axis_num ) , 
\ . format ( custom_axis [ 0 ] , custom_axis [ 1 ] , 
custom_axis [ 2 ] ) , 
\ . format ( planeref ) , 
\ . format ( str ( surface ) . lower ( ) ) , 
~~~ current_layer_label = script . layer_stack [ script . current_layer ( ) ] 
script . add_layer ( '{}_sect_{}_{}' . format ( current_layer_label , axis_name , 
int ( offset ) ) ) 
if surface : 
~~~ script . add_layer ( '{}_sect_{}_{}_mesh' . format ( current_layer_label , 
axis_name , int ( offset ) ) ) 
~~ def measure_geometry ( script ) : 
~~~ script . parse_geometry = True 
~~ def measure_topology ( script ) : 
~~~ script . parse_topology = True 
~~ def parse_geometry ( ml_log , log = None , ml_version = '2016.12' , print_output = False ) : 
aabb = { } 
geometry = { 'aabb' : aabb } 
with open ( ml_log ) as fread : 
~~~ geometry [ 'aabb' ] [ 'min' ] = ( line . split ( ) [ 4 : 7 ] ) 
geometry [ 'aabb' ] [ 'min' ] = [ util . to_float ( val ) for val in geometry [ 'aabb' ] [ 'min' ] ] 
~~~ geometry [ 'aabb' ] [ 'max' ] = ( line . split ( ) [ 4 : 7 ] ) 
geometry [ 'aabb' ] [ 'max' ] = [ util . to_float ( val ) for val in geometry [ 'aabb' ] [ 'max' ] ] 
~~~ geometry [ 'aabb' ] [ 'size' ] = ( line . split ( ) [ 4 : 7 ] ) 
geometry [ 'aabb' ] [ 'size' ] = [ util . to_float ( val ) for val in geometry [ 'aabb' ] [ 'size' ] ] 
~~~ geometry [ 'aabb' ] [ 'diagonal' ] = util . to_float ( line . split ( ) [ 4 ] ) 
~~~ geometry [ 'volume_mm3' ] = util . to_float ( line . split ( ) [ 3 ] ) 
geometry [ 'volume_cm3' ] = geometry [ 'volume_mm3' ] * 0.001 
~~~ if ml_version == '1.3.4BETA' : 
~~~ geometry [ 'area_mm2' ] = util . to_float ( line . split ( ) [ 3 ] ) 
~~~ geometry [ 'area_mm2' ] = util . to_float ( line . split ( ) [ 4 ] ) 
~~ geometry [ 'area_cm2' ] = geometry [ 'area_mm2' ] * 0.01 
~~~ geometry [ 'total_edge_length_incl_faux' ] = util . to_float ( 
line . split ( ) [ 7 ] ) 
~~~ geometry [ 'total_edge_length' ] = util . to_float ( 
~~~ geometry [ 'barycenter' ] = ( line . split ( ) [ 3 : 6 ] ) 
geometry [ 'barycenter' ] = [ util . to_float ( val ) for val in geometry [ 'barycenter' ] ] 
~~~ geometry [ 'barycenter' ] = ( line . split ( ) [ 4 : 7 ] ) 
~~~ geometry [ 'vert_barycenter' ] = ( line . split ( ) [ 2 : 5 ] ) 
geometry [ 'vert_barycenter' ] = [ util . to_float ( val ) for val in geometry [ 'vert_barycenter' ] ] 
~~~ geometry [ 'center_of_mass' ] = ( line . split ( ) [ 4 : 7 ] ) 
geometry [ 'center_of_mass' ] = [ util . to_float ( val ) for val in geometry [ 'center_of_mass' ] ] 
~~~ geometry [ 'inertia_tensor' ] = [ ] 
for val in range ( 3 ) : 
~~~ row = ( next ( fread , val ) . split ( ) [ 1 : 4 ] ) 
row = [ util . to_float ( b ) for b in row ] 
geometry [ 'inertia_tensor' ] . append ( row ) 
~~~ geometry [ 'principal_axes' ] = [ ] 
geometry [ 'principal_axes' ] . append ( row ) 
~~~ geometry [ 'axis_momenta' ] = ( next ( fread ) . split ( ) [ 1 : 4 ] ) 
geometry [ 'axis_momenta' ] = [ util . to_float ( val ) for val in geometry [ 'axis_momenta' ] ] 
~~ ~~ ~~ for key , value in geometry . items ( ) : 
~~~ if log is not None : 
~~ elif print_output : 
~~ ~~ return geometry 
~~ def parse_topology ( ml_log , log = None , ml_version = '1.3.4BETA' , print_output = False ) : 
topology = { 'manifold' : True , 'non_manifold_E' : 0 , 'non_manifold_V' : 0 } 
~~~ if 'V:' in line : 
topology [ 'vert_num' ] = int ( vert_edge_face [ 0 ] ) 
topology [ 'edge_num' ] = int ( vert_edge_face [ 1 ] ) 
topology [ 'face_num' ] = int ( vert_edge_face [ 2 ] ) 
~~~ topology [ 'unref_vert_num' ] = int ( line . split ( ) [ 2 ] ) 
~~~ topology [ 'boundry_edge_num' ] = int ( line . split ( ) [ 2 ] ) 
~~~ topology [ 'part_num' ] = int ( line . split ( ) [ 4 ] ) 
~~~ topology [ 'manifold' ] = False 
~~~ topology [ 'non_manifold_edge' ] = int ( line . split ( ) [ 2 ] ) 
~~~ topology [ 'non_manifold_vert' ] = int ( line . split ( ) [ 2 ] ) 
~~~ topology [ 'genus' ] = line . split ( ) [ 2 ] 
if topology [ 'genus' ] != 'undefined' : 
~~~ topology [ 'genus' ] = int ( topology [ 'genus' ] ) 
~~ ~~ if 'holes' in line : 
~~~ topology [ 'hole_num' ] = line . split ( ) [ 2 ] 
if topology [ 'hole_num' ] == 'a' : 
~~~ topology [ 'hole_num' ] = 'undefined' 
~~~ topology [ 'hole_num' ] = int ( topology [ 'hole_num' ] ) 
~~ ~~ ~~ ~~ for key , value in topology . items ( ) : 
~~ ~~ return topology 
~~ def parse_hausdorff ( ml_log , log = None , print_output = False ) : 
hausdorff_distance = { "min_distance" : 0.0 , 
"max_distance" : 0.0 , 
"mean_distance" : 0.0 , 
"rms_distance" : 0.0 , 
"number_points" : 0 } 
~~~ result = fread . readlines ( ) 
data = "" 
for idx , line in enumerate ( result ) : 
if m is not None : 
~~~ hausdorff_distance [ "number_points" ] = int ( m . group ( 1 ) ) 
~~~ data = result [ idx + 2 ] 
~~ ~~ m = re . match ( r"\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)\\D+(\\d+\\.*\\d*)" , data ) 
hausdorff_distance [ "min_distance" ] = float ( m . group ( 1 ) ) 
hausdorff_distance [ "max_distance" ] = float ( m . group ( 2 ) ) 
hausdorff_distance [ "mean_distance" ] = float ( m . group ( 3 ) ) 
hausdorff_distance [ "rms_distance" ] = float ( m . group ( 4 ) ) 
for key , value in hausdorff_distance . items ( ) : 
~~ ~~ return hausdorff_distance 
~~ ~~ def function ( script , red = 255 , green = 255 , blue = 255 , alpha = 255 , color = None ) : 
~~~ red , green , blue , _ = color_name [ color . lower ( ) ] 
\ . format ( str ( red ) . replace ( '&' , '&amp;' ) . replace ( '<' , '&lt;' ) ) , 
\ . format ( str ( green ) . replace ( '&' , '&amp;' ) . replace ( '<' , '&lt;' ) ) , 
\ . format ( str ( blue ) . replace ( '&' , '&amp;' ) . replace ( '<' , '&lt;' ) ) , 
\ . format ( str ( alpha ) . replace ( '&' , '&amp;' ) . replace ( '<' , '&lt;' ) ) , 
~~ def voronoi ( script , target_layer = 0 , source_layer = 1 , backward = True ) : 
\ . format ( source_layer ) , 
\ . format ( str ( backward ) . lower ( ) ) , 
~~ def cyclic_rainbow ( script , direction = 'sphere' , start_pt = ( 0 , 0 , 0 ) , 
amplitude = 255 / 2 , center = 255 / 2 , freq = 0.8 , 
phase = ( 0 , 120 , 240 , 0 ) , alpha = False ) : 
start_pt = util . make_list ( start_pt , 3 ) 
amplitude = util . make_list ( amplitude , 4 ) 
center = util . make_list ( center , 4 ) 
freq = util . make_list ( freq , 4 ) 
phase = util . make_list ( phase , 4 ) 
if direction . lower ( ) == 'sphere' : 
~~~ increment = 'sqrt((x-{})^2+(y-{})^2+(z-{})^2)' . format ( 
start_pt [ 0 ] , start_pt [ 1 ] , start_pt [ 2 ] ) 
~~ elif direction . lower ( ) == 'x' : 
~~ elif direction . lower ( ) == 'y' : 
~~ elif direction . lower ( ) == 'z' : 
~~~ increment = direction 
f = freq [ 0 ] , i = increment , p = math . radians ( phase [ 0 ] ) , 
a = amplitude [ 0 ] , c = center [ 0 ] ) 
f = freq [ 1 ] , i = increment , p = math . radians ( phase [ 1 ] ) , 
a = amplitude [ 1 ] , c = center [ 1 ] ) 
f = freq [ 2 ] , i = increment , p = math . radians ( phase [ 2 ] ) , 
a = amplitude [ 2 ] , c = center [ 2 ] ) 
if alpha : 
f = freq [ 3 ] , i = increment , p = math . radians ( phase [ 3 ] ) , 
a = amplitude [ 3 ] , c = center [ 3 ] ) 
~~~ alpha_func = 255 
~~ function ( script , red = red_func , green = green_func , blue = blue_func , 
alpha = alpha_func ) 
~~ def mp_atan2 ( y , x ) : 
'pi' , str ( math . pi ) ) . replace ( 'y' , y ) . replace ( 'x' , x ) 
~~ def v_cross ( u , v ) : 
return [ i , j , k ] 
~~ def v_multiply ( scalar , v1 ) : 
vector = [ ] 
for i , x in enumerate ( v1 ) : 
~~~ vector . append ( '(({})*({}))' . format ( scalar , v1 [ i ] ) ) 
~~ return vector 
~~ def torus_knot ( t , p = 3 , q = 4 , scale = 1.0 , radius = 2.0 ) : 
'{scale}*(-sin({q}*({t})))' . format ( t = t , q = q , scale = scale ) ] 
\ . format ( name ) , 
~~ def vq_function ( script , function = 'vi' , normalize = False , color = False ) : 
\ . format ( str ( normalize ) . lower ( ) ) , 
~~ def quatrefoil ( ) : 
os . chdir ( THIS_SCRIPTPATH ) 
ml_version = '2016.12' 
os . environ [ 'PATH' ] = meshlabserver_path + os . pathsep + os . environ [ 'PATH' ] 
length = math . radians ( 360 ) 
tube_size = [ 10 , 10 , length ] 
segments = [ 64 , 64 , 720 * 2 ] 
inner_radius = 2.0 
amplitude = 4.2 
freq = 4 
phase = 270 
center = 'r' 
start_pt = 0 
increment = 'z-{}' . format ( start_pt ) 
c_start_pt = 0 
c_freq = 5 
c_phase = ( 0 + c_phase_shift , 120 + c_phase_shift , 240 + c_phase_shift , 0 ) 
web_thickness = 0.5 
faces_surface = 50000 
voxel = 0.5 
thickness = 2.5 
faces_solid = 200000 
scale = ( size - 2 * ( thickness + amplitude ) - tube_size [ 1 ] ) / curve_max_size 
file_color = 'quatrefoil_color.ply' 
file_voronoi_surf = 'quatrefoil_voronoi_surf.ply' 
file_voronoi_solid = 'quatrefoil_voronoi_solid.ply' 
file_voronoi_color = 'quatrefoil_voronoi_final.ply' 
quatrefoil_color = mlx . FilterScript ( 
file_in = None , file_out = file_color , ml_version = ml_version ) 
quatrefoil_voronoi_surf = mlx . FilterScript ( 
file_in = file_color , file_out = file_voronoi_surf , ml_version = ml_version ) 
quatrefoil_voronoi_solid = mlx . FilterScript ( 
file_in = file_voronoi_surf , file_out = file_voronoi_solid , 
ml_version = ml_version ) 
quatrefoil_voronoi_color = mlx . FilterScript ( 
file_in = [ file_color , file_voronoi_solid ] , file_out = file_voronoi_color , 
mlx . create . cube_open_hires ( 
quatrefoil_color , size = tube_size , x_segments = segments [ 0 ] , 
y_segments = segments [ 1 ] , z_segments = segments [ 2 ] , center = True ) 
mlx . transform . translate ( quatrefoil_color , [ 0 , 0 , length / 2 ] ) 
f = freq , i = increment , p = math . radians ( phase ) , a = amplitude , c = center ) 
mlx . transform . function_cyl_co ( 
quatrefoil_color , r_func = r_func , theta_func = 'theta' , z_func = 'z' ) 
f = freq , i = increment , p = math . radians ( phase ) , a = amplitude , c = max_radius ) 
mlx . mp_func . vq_function ( quatrefoil_color , function = q_func ) 
mlx . vert_color . cyclic_rainbow ( 
quatrefoil_color , direction = 'z' , start_pt = c_start_pt , amplitude = 255 / 2 , 
center = 255 / 2 , freq = c_freq , phase = c_phase ) 
quatrefoil_func = mlx . transform . deform2curve ( 
quatrefoil_color , 
curve = mlx . mp_func . torus_knot ( 't' , p = 3 , q = 4 , scale = scale , 
radius = inner_radius ) ) 
mlx . clean . merge_vert ( quatrefoil_color , threshold = 0.0001 ) 
mlx . layers . delete_lower ( quatrefoil_color ) 
mlx . mp_func . vert_attr ( quatrefoil_voronoi_surf , name = 'radius' , function = 'q' ) 
quatrefoil_voronoi_surf , size = tube_size , x_segments = holes [ 0 ] + 1 , 
y_segments = holes [ 1 ] + 1 , z_segments = holes [ 2 ] + 1 , center = True ) 
mlx . select . all ( quatrefoil_voronoi_surf , vert = False ) 
mlx . delete . selected ( quatrefoil_voronoi_surf , vert = False ) 
mlx . select . cylindrical_vert ( quatrefoil_voronoi_surf , 
radius = max_radius - 0.0001 , inside = False ) 
mlx . transform . translate ( quatrefoil_voronoi_surf , [ 0 , 0 , 20 ] ) 
mlx . delete . selected ( quatrefoil_voronoi_surf , face = False ) 
mlx . transform . function_cyl_co ( quatrefoil_voronoi_surf , r_func = r_func , 
theta_func = 'theta' , z_func = 'z' ) 
mlx . transform . vert_function ( 
quatrefoil_voronoi_surf , x_func = quatrefoil_func [ 0 ] , 
y_func = quatrefoil_func [ 1 ] , z_func = quatrefoil_func [ 2 ] ) 
mlx . layers . change ( quatrefoil_voronoi_surf , 0 ) 
mlx . vert_color . voronoi ( quatrefoil_voronoi_surf ) 
if quatrefoil_voronoi_surf . ml_version == '1.3.4BETA' : 
~~ mlx . select . vert_function ( quatrefoil_voronoi_surf , function = sel_func ) 
mlx . select . invert ( quatrefoil_voronoi_surf , face = False ) 
mlx . smooth . laplacian ( quatrefoil_voronoi_surf , iterations = 3 ) 
mlx . remesh . simplify ( quatrefoil_voronoi_surf , texture = False , faces = faces_surface ) 
mlx . layers . delete_lower ( quatrefoil_voronoi_surf ) 
#quatrefoil_voronoi_surf.save_to_file('temp_script.mlx') 
mlx . remesh . uniform_resampling ( quatrefoil_voronoi_solid , voxel = voxel , 
offset = thickness / 2 , thicken = True ) 
mlx . layers . delete_lower ( quatrefoil_voronoi_solid ) 
quatrefoil_voronoi_solid . run_script ( ) 
mlx . delete . small_parts ( quatrefoil_voronoi_color ) 
mlx . delete . unreferenced_vert ( quatrefoil_voronoi_color ) 
mlx . delete . faces_from_nonmanifold_edges ( quatrefoil_voronoi_color ) 
mlx . clean . split_vert_on_nonmanifold_face ( quatrefoil_voronoi_color ) 
mlx . clean . close_holes ( quatrefoil_voronoi_color ) 
mlx . remesh . simplify ( quatrefoil_voronoi_color , texture = False , faces = faces_solid ) 
mlx . subdivide . ls3loop ( quatrefoil_voronoi_color , iterations = 1 ) 
mlx . smooth . laplacian ( quatrefoil_voronoi_color , iterations = 3 ) 
mlx . transfer . vert_attr_2_meshes ( 
quatrefoil_voronoi_color , source_mesh = 0 , target_mesh = 1 , color = True , 
max_distance = 7 ) 
mlx . layers . delete_lower ( quatrefoil_voronoi_color ) 
quatrefoil_voronoi_color . run_script ( script_file = None ) 
~~ def flip ( script , force_flip = False , selected = False ) : 
\ . format ( str ( force_flip ) . lower ( ) ) , 
~~ def point_sets ( script , neighbors = 10 , smooth_iteration = 0 , flip = False , 
viewpoint_pos = ( 0.0 , 0.0 , 0.0 ) ) : 
\ . format ( neighbors ) , 
\ . format ( smooth_iteration ) , 
\ . format ( str ( flip ) . lower ( ) ) , 
\ . format ( viewpoint_pos [ 0 ] , viewpoint_pos [ 1 ] , 
viewpoint_pos [ 2 ] , ) , 
~~ def laplacian ( script , iterations = 1 , boundary = True , cotangent_weight = True , 
\ . format ( str ( boundary ) . lower ( ) ) , 
\ . format ( str ( cotangent_weight ) . lower ( ) ) , 
~~ def taubin ( script , iterations = 10 , t_lambda = 0.5 , t_mu = - 0.53 , selected = False ) : 
\ . format ( t_lambda ) , 
\ . format ( t_mu ) , 
~~ def twostep ( script , iterations = 3 , angle_threshold = 60 , normal_steps = 20 , fit_steps = 20 , 
\ . format ( normal_steps ) , 
\ . format ( fit_steps ) , 
~~ def depth ( script , iterations = 3 , viewpoint = ( 0 , 0 , 0 ) , selected = False ) : 
\ . format ( viewpoint [ 0 ] ) , 
\ . format ( viewpoint [ 1 ] ) , 
\ . format ( viewpoint [ 2 ] ) , 
~~ def measure_aabb ( fbasename = None , log = None , coord_system = 'CARTESIAN' ) : 
if fext != 'xyz' : 
~~~ fin = 'TEMP3D_aabb.xyz' 
run ( log = log , file_in = fbasename , file_out = fin , script = None ) 
~~~ fin = fbasename 
~~ fread = open ( fin , 'r' ) 
aabb = { 'min' : [ 999999.0 , 999999.0 , 999999.0 ] , 'max' : [ - 999999.0 , - 999999.0 , - 999999.0 ] } 
for line in fread : 
~~~ x_co , y_co , z_co = line . split ( ) 
x_co = util . to_float ( x_co ) 
y_co = util . to_float ( y_co ) 
z_co = util . to_float ( z_co ) 
if coord_system == 'CARTESIAN' : 
~~~ if x_co < aabb [ 'min' ] [ 0 ] : 
~~~ aabb [ 'min' ] [ 0 ] = x_co 
~~ if y_co < aabb [ 'min' ] [ 1 ] : 
~~~ aabb [ 'min' ] [ 1 ] = y_co 
~~ if z_co < aabb [ 'min' ] [ 2 ] : 
~~~ aabb [ 'min' ] [ 2 ] = z_co 
~~ if x_co > aabb [ 'max' ] [ 0 ] : 
~~~ aabb [ 'max' ] [ 0 ] = x_co 
~~ if y_co > aabb [ 'max' ] [ 1 ] : 
~~~ aabb [ 'max' ] [ 1 ] = y_co 
~~ if z_co > aabb [ 'max' ] [ 2 ] : 
~~~ aabb [ 'max' ] [ 2 ] = z_co 
~~ ~~ elif coord_system == 'CYLINDRICAL' : 
~~~ radius = math . sqrt ( x_co ** 2 + y_co ** 2 ) 
theta = math . degrees ( math . atan2 ( y_co , x_co ) ) 
if radius < aabb [ 'min' ] [ 0 ] : 
~~~ aabb [ 'min' ] [ 0 ] = radius 
~~ if theta < aabb [ 'min' ] [ 1 ] : 
~~~ aabb [ 'min' ] [ 1 ] = theta 
~~ if radius > aabb [ 'max' ] [ 0 ] : 
~~~ aabb [ 'max' ] [ 0 ] = radius 
~~ if theta > aabb [ 'max' ] [ 1 ] : 
~~~ aabb [ 'max' ] [ 1 ] = theta 
~~ ~~ ~~ fread . close ( ) 
~~~ aabb [ 'center' ] = [ ( aabb [ 'max' ] [ 0 ] + aabb [ 'min' ] [ 0 ] ) / 2 , 
( aabb [ 'max' ] [ 1 ] + aabb [ 'min' ] [ 1 ] ) / 2 , 
( aabb [ 'max' ] [ 2 ] + aabb [ 'min' ] [ 2 ] ) / 2 ] 
aabb [ 'size' ] = [ aabb [ 'max' ] [ 0 ] - aabb [ 'min' ] [ 0 ] , aabb [ 'max' ] [ 1 ] - aabb [ 'min' ] [ 1 ] , 
aabb [ 'max' ] [ 2 ] - aabb [ 'min' ] [ 2 ] ] 
aabb [ 'diagonal' ] = math . sqrt ( 
aabb [ 'size' ] [ 0 ] ** 2 + 
aabb [ 'size' ] [ 1 ] ** 2 + 
aabb [ 'size' ] [ 2 ] ** 2 ) 
~~ except UnboundLocalError : 
~~ for key , value in aabb . items ( ) : 
~~~ if log is None : 
return aabb 
~~ def measure_section ( fbasename = None , log = None , axis = 'z' , offset = 0.0 , 
rotate_x_angle = None , ml_version = ml_version ) : 
ml_script1_file = 'TEMP3D_measure_section.mlx' 
file_out = 'TEMP3D_sect_aabb.xyz' 
ml_script1 = mlx . FilterScript ( file_in = fbasename , file_out = file_out , ml_version = ml_version ) 
if rotate_x_angle is not None : 
~~~ transform . rotate ( ml_script1 , axis = 'x' , angle = rotate_x_angle ) 
~~ compute . section ( ml_script1 , axis = axis , offset = offset ) 
layers . delete_lower ( ml_script1 ) 
ml_script1 . save_to_file ( ml_script1_file ) 
ml_script1 . run_script ( log = log , script_file = ml_script1_file ) 
aabb = measure_aabb ( file_out , log ) 
~~ def polylinesort ( fbasename = None , log = None ) : 
if fext != 'obj' : 
~~ fread = open ( fbasename , 'r' ) 
first = True 
polyline_vertices = [ ] 
line_segments = [ ] 
~~~ element , x_co , y_co , z_co = line . split ( ) 
if element == 'v' : 
~~~ polyline_vertices . append ( 
[ util . to_float ( x_co ) , util . to_float ( y_co ) , util . to_float ( z_co ) ] ) 
~~ elif element == 'l' : 
~~~ p1 = x_co 
p2 = y_co 
line_segments . append ( [ int ( p1 ) , int ( p2 ) ] ) 
~~ ~~ fread . close ( ) 
#log_file.write(\ 
"""log_file.write(\ 
~~ def measure_topology ( fbasename = None , log = None , ml_version = ml_version ) : 
ml_script1_file = 'TEMP3D_measure_topology.mlx' 
ml_script1 = mlx . FilterScript ( file_in = fbasename , ml_version = ml_version ) 
compute . measure_topology ( ml_script1 ) 
topology = ml_script1 . topology 
return topology 
~~ def measure_all ( fbasename = None , log = None , ml_version = ml_version ) : 
ml_script1_file = 'TEMP3D_measure_gAndT.mlx' 
if ml_version == '1.3.4BETA' : 
~~~ file_out = 'TEMP3D_aabb.xyz' 
~~~ file_out = None 
~~ ml_script1 = mlx . FilterScript ( file_in = fbasename , file_out = file_out , ml_version = ml_version ) 
compute . measure_geometry ( ml_script1 ) 
geometry = ml_script1 . geometry 
fbasename ) 
~~ aabb = measure_aabb ( file_out , log ) 
~~~ aabb = geometry [ 'aabb' ] 
~~ return aabb , geometry , topology 
~~ def measure_dimension ( fbasename = None , log = None , axis1 = None , offset1 = 0.0 , 
axis2 = None , offset2 = 0.0 , ml_version = ml_version ) : 
axis1 = axis1 . lower ( ) 
axis2 = axis2 . lower ( ) 
ml_script1_file = 'TEMP3D_measure_dimension.mlx' 
file_out = 'TEMP3D_measure_dimension.xyz' 
compute . section ( ml_script1 , axis1 , offset1 , surface = True ) 
compute . section ( ml_script1 , axis2 , offset2 , surface = False ) 
for val in ( 'x' , 'y' , 'z' ) : 
~~~ if val not in ( axis1 , axis2 ) : 
~~~ axis = val 
~~ ~~ axis_num = ord ( axis ) - ord ( 'x' ) 
dimension = { 'min' : aabb [ 'min' ] [ axis_num ] , 'max' : aabb [ 'max' ] [ axis_num ] , 
'length' : aabb [ 'size' ] [ axis_num ] , 'axis' : axis } 
if log is None : 
~~~ print ( \ % fbasename ) 
axis2 , offset2 ) ) 
dimension [ 'max' ] , dimension [ 'length' ] ) ) 
log_file . write ( \ % fbasename ) 
~~ return dimension 
~~ def bitperm ( s , perm , pos ) : 
perm = perm . upper ( ) 
pos = pos . upper ( ) 
assert perm in [ 'R' , 'W' , 'X' ] 
assert pos in [ 'USR' , 'GRP' , 'OTH' ] 
return s . st_mode & getattr ( stat , 'S_I{}{}' . format ( perm , pos ) ) 
~~ def only_root_write ( path ) : 
s = os . stat ( path ) 
for ug , bp in [ ( s . st_uid , bitperm ( s , 'w' , 'usr' ) ) , ( s . st_gid , bitperm ( s , 'w' , 'grp' ) ) ] : 
~~~ if ug and bp : 
~~ ~~ if bitperm ( s , 'w' , 'oth' ) : 
~~ def check_config ( file , printfn = print ) : 
Config ( file ) . read ( ) 
printfn ( \ . format ( file ) ) 
~~~ data = load ( open ( self . file ) , Loader ) 
~~ except ( UnicodeDecodeError , YAMLError ) as e : 
~~~ raise InvalidConfig ( self . file , '{}' . format ( e ) ) 
~~~ validate ( data , SCHEMA ) 
~~ except ValidationError as e : 
~~~ raise InvalidConfig ( self . file , e ) 
~~ self . update ( data ) 
~~ def run_as_cmd ( cmd , user , shell = 'bash' ) : 
to_execute = get_shell ( shell ) + [ EXECUTE_SHELL_PARAM , cmd ] 
if user == 'root' : 
~~~ return to_execute 
~~ return [ 'sudo' , '-s' , '--set-home' , '-u' , user ] + to_execute 
~~ def execute_cmd ( cmd , cwd = None , timeout = 5 ) : 
p = subprocess . Popen ( cmd , cwd = cwd , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) 
~~~ p . wait ( timeout = timeout ) 
~~ except subprocess . TimeoutExpired : 
~~~ stdout , stderr = p . stdout . read ( ) , p . stderr . read ( ) 
if sys . version_info >= ( 3 , ) : 
~~~ stdout , stderr = stdout . decode ( 'utf-8' , errors = 'ignore' ) , stderr . decode ( 'utf-8' , errors = 'ignore' ) 
~~ if p . returncode : 
~~~ return stdout , stderr 
~~ ~~ ~~ def execute_over_ssh ( cmd , ssh , cwd = None , shell = 'bash' ) : 
port = None 
parts = ssh . split ( ':' , 1 ) 
if len ( parts ) > 1 and not parts [ 1 ] . isdigit ( ) : 
~~ elif len ( parts ) > 1 : 
~~~ port = parts [ 1 ] 
return [ 'ssh' , parts [ 0 ] ] + ( [ '-p' , port ] if port else [ ] ) + [ '-C' ] + [ remote_cmd ] 
~~ def execute ( self , root_allowed = False ) : 
if self . user == ROOT_USER and not root_allowed and not self . data . get ( 'ssh' ) : 
~~ if self . data . get ( 'user' ) and self . data . get ( 'ssh' ) : 
~~ if self . data . get ( 'ssh' ) : 
~~~ cmd = execute_over_ssh ( self . data [ 'cmd' ] , self . data [ 'ssh' ] , self . data . get ( 'cwd' ) ) 
output = execute_cmd ( cmd ) 
~~~ cmd = run_as_cmd ( self . data [ 'cmd' ] , self . user ) 
output = execute_cmd ( cmd , self . data . get ( 'cwd' ) ) 
~~ if output : 
~~~ return output [ 0 ] 
~~ ~~ def validate ( self ) : 
if ( self . data . get ( 'content-type' ) or self . data . get ( 'body' ) ) and self . data . get ( 'method' , '' ) . lower ( ) not in CONTENT_TYPE_METHODS : 
~~~ raise InvalidConfig ( 
~~ self . data [ 'content-type' ] = CONTENT_TYPE_ALIASES . get ( self . data . get ( 'content-type' ) , 
self . data . get ( 'content-type' ) ) 
form_type = CONTENT_TYPE_ALIASES [ 'form' ] 
if self . data . get ( 'body' ) and ( self . data . get ( 'content-type' ) or form_type ) == form_type : 
~~~ self . data [ 'body' ] = json . loads ( self . data [ 'body' ] ) 
~~ except JSONDecodeError : 
~~ ~~ ~~ def execute ( self , root_allowed = False ) : 
kwargs = { 'stream' : True , 'timeout' : 15 , 
'headers' : self . data . get ( 'headers' , { } ) } 
if self . data . get ( 'content-type' ) : 
~~~ kwargs [ 'headers' ] [ 'content-type' ] = self . data [ 'content-type' ] 
~~ if self . data . get ( 'body' ) : 
~~~ kwargs [ 'data' ] = self . data [ 'body' ] 
~~ if self . data . get ( 'auth' ) : 
~~~ kwargs [ 'auth' ] = tuple ( self . data [ 'auth' ] . split ( ':' , 1 ) ) 
~~~ resp = request ( self . data . get ( 'method' , 'get' ) . lower ( ) , self . data [ 'url' ] , 
verify = self . data . get ( 'verify' , True ) , 
~~ except RequestException as e : 
~~ if resp . status_code >= 400 : 
~~~ raise ExecuteError ( \ . format ( self . data [ 'url' ] , resp . status_code ) ) 
~~ data = resp . raw . read ( 1000 , decode_content = True ) 
~~~ data = data . decode ( 'utf-8' , errors = 'ignore' ) 
~~ def get_headers ( self ) : 
headers = copy . copy ( self . default_headers or { } ) 
headers . update ( self . data . get ( 'headers' ) or { } ) 
return headers 
~~ def get_url ( self ) : 
url = self . data [ self . execute_name ] 
parsed = urlparse ( url ) 
if not parsed . scheme : 
~~~ url = '{}://{}' . format ( self . default_protocol , url ) 
~~ if not url . split ( ':' ) [ - 1 ] . isalnum ( ) : 
~~~ url += ':{}' . format ( self . default_port ) 
~~ def get_body ( self ) : 
if self . default_body : 
~~~ return self . default_body 
~~ data = self . data . get ( 'data' ) 
~~~ return json . dumps ( data ) 
url = super ( ExecuteHomeAssistant , self ) . get_url ( ) 
if not self . data . get ( 'event' ) : 
~~ url += '/api/events/{}' . format ( self . data [ 'event' ] ) 
if not self . data [ self . execute_name ] : 
'https://ifttt.com/services/maker_webhooks/settings' . format ( self . name ) ) 
~~ if not self . data . get ( 'event' ) : 
'applet' . format ( self . name ) ) 
~~ url = self . url_pattern . format ( event = self . data [ 'event' ] , key = self . data [ self . execute_name ] ) 
~~ def pkt_text ( pkt ) : 
if pkt . src . upper ( ) in BANNED_DEVICES : 
~~~ body = '' 
~~ elif pkt . src . upper ( ) [ : 8 ] in AMAZON_DEVICES : 
~~~ body = pkt . src 
~~ return body 
~~ def discovery_print ( pkt ) : 
if pkt . src in mac_id_list : 
~~ mac_id_list . append ( pkt . src ) 
text = pkt_text ( pkt ) 
click . secho ( text , fg = 'magenta' ) if 'Amazon' in text else click . echo ( text ) 
~~ def discover ( interface = None ) : 
click . secho ( HELP , fg = 'yellow' ) 
scan_devices ( discovery_print , lfilter = lambda d : d . src not in mac_id_list , iface = interface ) 
if not self . execute_instance : 
logger . warning ( msg , self . name ) 
self . send_confirmation ( msg % self . name , False ) 
~~~ result = self . execute_instance . execute ( root_allowed ) 
self . send_confirmation ( result ) 
~~ def send_confirmation ( self , message , success = True ) : 
message = message . strip ( ) 
if not self . confirmation : 
~~~ self . confirmation . send ( message , success ) 
~~ ~~ def on_push ( self , device ) : 
src = device . src . lower ( ) 
if last_execution [ src ] + self . settings . get ( 'delay' , DEFAULT_DELAY ) > time . time ( ) : 
~~ last_execution [ src ] = time . time ( ) 
self . execute ( device ) 
~~ def execute ( self , device ) : 
device = self . devices [ src ] 
threading . Thread ( target = device . execute , kwargs = { 
'root_allowed' : self . root_allowed 
} ) . start ( ) 
~~ def run ( self , root_allowed = False ) : 
self . root_allowed = root_allowed 
scan_devices ( self . on_push , lambda d : d . src . lower ( ) in self . devices , self . settings . get ( 'interface' ) ) 
~~ def scan_devices ( fn , lfilter , iface = None ) : 
~~~ sniff ( prn = fn , store = 0 , 
lfilter = lfilter , iface = iface ) 
~~ except PermissionError : 
~~~ raise SocketPermissionError 
~~ ~~ def check ( func ) : 
~~~ check_name = func . __name__ 
arg_name = None 
~~~ arg_name = args [ 0 ] 
~~~ if arg_name : 
~~ response = func ( * args , ** kwargs ) 
~~~ message = str ( e ) 
"ok" : False , 
"error" : message , 
"stacktrace" : traceback . format_exc ( ) , 
if arg_name : 
~~~ response = { arg_name : response } 
logger . exception ( 
check_name , 
arg_name , 
message 
~~~ logger . exception ( 
~~ def token_required ( view_func ) : 
def _parse_auth_header ( auth_header ) : 
reg = re . compile ( \ ) 
header_dict = dict ( reg . findall ( auth_header ) ) 
return header_dict [ 'Token' ] 
~~ def _get_passed_token ( request ) : 
~~~ auth_header = request . META [ 'HTTP_AUTHORIZATION' ] 
token = _parse_auth_header ( auth_header ) 
~~~ token = request . GET . get ( settings . WATCHMAN_TOKEN_NAME ) 
~~ return token 
~~ def _validate_token ( request ) : 
~~~ if settings . WATCHMAN_TOKENS : 
~~~ watchman_tokens = settings . WATCHMAN_TOKENS . split ( ',' ) 
~~ elif settings . WATCHMAN_TOKEN : 
~~~ watchman_tokens = [ settings . WATCHMAN_TOKEN , ] 
~~ return _get_passed_token ( request ) in watchman_tokens 
~~ @ csrf_exempt 
@ wraps ( view_func ) 
def _wrapped_view ( request , * args , ** kwargs ) : 
~~~ if _validate_token ( request ) : 
~~~ return view_func ( request , * args , ** kwargs ) 
~~ return HttpResponseForbidden ( ) 
~~ return _wrapped_view 
~~ def set_hosts ( hosts , use_ssl = False , ssl_cert_path = None ) : 
if type ( hosts ) != list : 
~~~ hosts = [ hosts ] 
~~ conn_params = { 
"hosts" : hosts , 
"timeout" : 20 
if use_ssl : 
~~~ conn_params [ 'use_ssl' ] = True 
if ssl_cert_path : 
~~~ conn_params [ 'verify_certs' ] = True 
conn_params [ 'ca_certs' ] = ssl_cert_path 
~~~ conn_params [ 'verify_certs' ] = False 
~~ ~~ connections . create_connection ( ** conn_params ) 
~~ def create_indexes ( names , settings = None ) : 
~~~ index = Index ( name ) 
~~~ if not index . exists ( ) : 
if settings is None : 
~~~ index . settings ( number_of_shards = 1 , 
number_of_replicas = 1 ) 
~~~ index . settings ( ** settings ) 
~~ index . create ( ) 
~~~ raise ElasticsearchError ( 
~~ ~~ ~~ def migrate_indexes ( aggregate_indexes = None , forensic_indexes = None ) : 
version = 2 
if aggregate_indexes is None : 
~~~ aggregate_indexes = [ ] 
~~ if forensic_indexes is None : 
~~~ forensic_indexes = [ ] 
~~ for aggregate_index_name in aggregate_indexes : 
~~~ if not Index ( aggregate_index_name ) . exists ( ) : 
~~ aggregate_index = Index ( aggregate_index_name ) 
doc = "doc" 
fo_field = "published_policy.fo" 
fo = "fo" 
fo_mapping = aggregate_index . get_field_mapping ( fields = [ fo_field ] ) 
fo_mapping = fo_mapping [ list ( fo_mapping . keys ( ) ) [ 0 ] ] [ "mappings" ] 
if doc not in fo_mapping : 
~~ fo_mapping = fo_mapping [ doc ] [ fo_field ] [ "mapping" ] [ fo ] 
fo_type = fo_mapping [ "type" ] 
if fo_type == "long" : 
~~~ new_index_name = "{0}-v{1}" . format ( aggregate_index_name , version ) 
body = { "properties" : { "published_policy.fo" : { 
"type" : "text" , 
"fields" : { 
"keyword" : { 
"type" : "keyword" , 
"ignore_above" : 256 
Index ( new_index_name ) . create ( ) 
Index ( new_index_name ) . put_mapping ( doc_type = doc , body = body ) 
reindex ( connections . get_connection ( ) , aggregate_index_name , 
new_index_name ) 
Index ( aggregate_index_name ) . delete ( ) 
~~ ~~ for forensic_index in forensic_indexes : 
~~ ~~ def save_aggregate_report_to_elasticsearch ( aggregate_report , 
index_suffix = None , 
monthly_indexes = False ) : 
aggregate_report = aggregate_report . copy ( ) 
metadata = aggregate_report [ "report_metadata" ] 
org_name = metadata [ "org_name" ] 
report_id = metadata [ "report_id" ] 
domain = aggregate_report [ "policy_published" ] [ "domain" ] 
begin_date = human_timestamp_to_datetime ( metadata [ "begin_date" ] ) 
end_date = human_timestamp_to_datetime ( metadata [ "end_date" ] ) 
if monthly_indexes : 
~~~ index_date = begin_date . strftime ( "%Y-%m" ) 
~~~ index_date = begin_date . strftime ( "%Y-%m-%d" ) 
~~ aggregate_report [ "begin_date" ] = begin_date 
aggregate_report [ "end_date" ] = end_date 
date_range = [ aggregate_report [ "begin_date" ] , 
aggregate_report [ "end_date" ] ] 
org_name_query = Q ( dict ( match = dict ( org_name = org_name ) ) ) 
report_id_query = Q ( dict ( match = dict ( report_id = report_id ) ) ) 
domain_query = Q ( dict ( match = { "published_policy.domain" : domain } ) ) 
begin_date_query = Q ( dict ( match = dict ( date_range = begin_date ) ) ) 
end_date_query = Q ( dict ( match = dict ( date_range = end_date ) ) ) 
search = Search ( index = "dmarc_aggregate*" ) 
query = org_name_query & report_id_query & domain_query 
query = query & begin_date_query & end_date_query 
search . query = query 
existing = search . execute ( ) 
if len ( existing ) > 0 : 
"Elasticsearch" . format ( report_id , 
org_name , 
domain , 
begin_date_human , 
end_date_human ) ) 
~~ published_policy = _PublishedPolicy ( 
domain = aggregate_report [ "policy_published" ] [ "domain" ] , 
adkim = aggregate_report [ "policy_published" ] [ "adkim" ] , 
aspf = aggregate_report [ "policy_published" ] [ "aspf" ] , 
p = aggregate_report [ "policy_published" ] [ "p" ] , 
sp = aggregate_report [ "policy_published" ] [ "sp" ] , 
pct = aggregate_report [ "policy_published" ] [ "pct" ] , 
fo = aggregate_report [ "policy_published" ] [ "fo" ] 
for record in aggregate_report [ "records" ] : 
~~~ agg_doc = _AggregateReportDoc ( 
xml_schemea = aggregate_report [ "xml_schema" ] , 
org_name = metadata [ "org_name" ] , 
org_email = metadata [ "org_email" ] , 
org_extra_contact_info = metadata [ "org_extra_contact_info" ] , 
report_id = metadata [ "report_id" ] , 
date_range = date_range , 
errors = metadata [ "errors" ] , 
published_policy = published_policy , 
source_ip_address = record [ "source" ] [ "ip_address" ] , 
source_country = record [ "source" ] [ "country" ] , 
source_reverse_dns = record [ "source" ] [ "reverse_dns" ] , 
source_base_domain = record [ "source" ] [ "base_domain" ] , 
message_count = record [ "count" ] , 
disposition = record [ "policy_evaluated" ] [ "disposition" ] , 
dkim_aligned = record [ "policy_evaluated" ] [ "dkim" ] == "pass" , 
spf_aligned = record [ "policy_evaluated" ] [ "spf" ] == "pass" , 
header_from = record [ "identifiers" ] [ "header_from" ] , 
envelope_from = record [ "identifiers" ] [ "envelope_from" ] , 
envelope_to = record [ "identifiers" ] [ "envelope_to" ] 
for override in record [ "policy_evaluated" ] [ "policy_override_reasons" ] : 
~~~ agg_doc . add_policy_override ( type_ = override [ "type" ] , 
comment = override [ "comment" ] ) 
~~ for dkim_result in record [ "auth_results" ] [ "dkim" ] : 
~~~ agg_doc . add_dkim_result ( domain = dkim_result [ "domain" ] , 
selector = dkim_result [ "selector" ] , 
result = dkim_result [ "result" ] ) 
~~ for spf_result in record [ "auth_results" ] [ "spf" ] : 
~~~ agg_doc . add_spf_result ( domain = spf_result [ "domain" ] , 
scope = spf_result [ "scope" ] , 
result = spf_result [ "result" ] ) 
~~ index = "dmarc_aggregate" 
if index_suffix : 
~~~ index = "{0}_{1}" . format ( index , index_suffix ) 
~~ index = "{0}-{1}" . format ( index , index_date ) 
create_indexes ( [ index ] ) 
agg_doc . meta . index = index 
~~~ agg_doc . save ( ) 
~~ ~~ ~~ def save_forensic_report_to_elasticsearch ( forensic_report , 
forensic_report = forensic_report . copy ( ) 
sample_date = None 
if forensic_report [ "parsed_sample" ] [ "date" ] is not None : 
~~~ sample_date = forensic_report [ "parsed_sample" ] [ "date" ] 
sample_date = human_timestamp_to_datetime ( sample_date ) 
~~ original_headers = forensic_report [ "parsed_sample" ] [ "headers" ] 
headers = OrderedDict ( ) 
for original_header in original_headers : 
~~~ headers [ original_header . lower ( ) ] = original_headers [ original_header ] 
~~ arrival_date_human = forensic_report [ "arrival_date_utc" ] 
arrival_date = human_timestamp_to_datetime ( arrival_date_human ) 
search = Search ( index = "dmarc_forensic*" ) 
arrival_query = { "match" : { "arrival_date" : arrival_date } } 
q = Q ( arrival_query ) 
from_ = None 
to_ = None 
subject = None 
if "from" in headers : 
~~~ from_ = headers [ "from" ] 
from_query = { "match" : { "sample.headers.from" : from_ } } 
q = q & Q ( from_query ) 
~~ if "to" in headers : 
~~~ to_ = headers [ "to" ] 
to_query = { "match" : { "sample.headers.to" : to_ } } 
q = q & Q ( to_query ) 
~~ if "subject" in headers : 
~~~ subject = headers [ "subject" ] 
subject_query = { "match" : { "sample.headers.subject" : subject } } 
q = q & Q ( subject_query ) 
~~ search . query = q 
"Elasticsearch" . format ( to_ , 
from_ , 
subject , 
arrival_date_human 
~~ parsed_sample = forensic_report [ "parsed_sample" ] 
sample = _ForensicSampleDoc ( 
raw = forensic_report [ "sample" ] , 
headers_only = forensic_report [ "sample_headers_only" ] , 
date = sample_date , 
subject = forensic_report [ "parsed_sample" ] [ "subject" ] , 
filename_safe_subject = parsed_sample [ "filename_safe_subject" ] , 
body = forensic_report [ "parsed_sample" ] [ "body" ] 
for address in forensic_report [ "parsed_sample" ] [ "to" ] : 
~~~ sample . add_to ( display_name = address [ "display_name" ] , 
address = address [ "address" ] ) 
~~ for address in forensic_report [ "parsed_sample" ] [ "reply_to" ] : 
~~~ sample . add_reply_to ( display_name = address [ "display_name" ] , 
~~ for address in forensic_report [ "parsed_sample" ] [ "cc" ] : 
~~~ sample . add_cc ( display_name = address [ "display_name" ] , 
~~ for address in forensic_report [ "parsed_sample" ] [ "bcc" ] : 
~~~ sample . add_bcc ( display_name = address [ "display_name" ] , 
~~ for attachment in forensic_report [ "parsed_sample" ] [ "attachments" ] : 
~~~ sample . add_attachment ( filename = attachment [ "filename" ] , 
content_type = attachment [ "mail_content_type" ] , 
sha256 = attachment [ "sha256" ] ) 
~~~ forensic_doc = _ForensicReportDoc ( 
feedback_type = forensic_report [ "feedback_type" ] , 
user_agent = forensic_report [ "user_agent" ] , 
version = forensic_report [ "version" ] , 
original_mail_from = forensic_report [ "original_mail_from" ] , 
arrival_date = arrival_date , 
domain = forensic_report [ "reported_domain" ] , 
original_envelope_id = forensic_report [ "original_envelope_id" ] , 
authentication_results = forensic_report [ "authentication_results" ] , 
delivery_results = forensic_report [ "delivery_result" ] , 
source_ip_address = forensic_report [ "source" ] [ "ip_address" ] , 
source_country = forensic_report [ "source" ] [ "country" ] , 
source_reverse_dns = forensic_report [ "source" ] [ "reverse_dns" ] , 
source_base_domain = forensic_report [ "source" ] [ "base_domain" ] , 
authentication_mechanisms = forensic_report [ 
"authentication_mechanisms" ] , 
auth_failure = forensic_report [ "auth_failure" ] , 
dkim_domain = forensic_report [ "dkim_domain" ] , 
original_rcpt_to = forensic_report [ "original_rcpt_to" ] , 
sample = sample 
index = "dmarc_forensic" 
~~ if monthly_indexes : 
~~~ index_date = arrival_date . strftime ( "%Y-%m" ) 
~~~ index_date = arrival_date . strftime ( "%Y-%m-%d" ) 
forensic_doc . meta . index = index 
~~~ forensic_doc . save ( ) 
~~ ~~ except KeyError as e : 
~~~ raise InvalidForensicReport ( 
~~ ~~ def strip_metadata ( report ) : 
report [ 'org_name' ] = report [ 'report_metadata' ] [ 'org_name' ] 
report [ 'org_email' ] = report [ 'report_metadata' ] [ 'org_email' ] 
report [ 'report_id' ] = report [ 'report_metadata' ] [ 'report_id' ] 
report . pop ( 'report_metadata' ) 
return report 
~~ def generate_daterange ( report ) : 
metadata = report [ "report_metadata" ] 
begin_date_human = begin_date . strftime ( "%Y-%m-%dT%H:%M:%S" ) 
end_date_human = end_date . strftime ( "%Y-%m-%dT%H:%M:%S" ) 
date_range = [ begin_date_human , 
end_date_human ] 
return date_range 
~~ def save_aggregate_reports_to_kafka ( self , aggregate_reports , 
aggregate_topic ) : 
if ( type ( aggregate_reports ) == dict or 
type ( aggregate_reports ) == OrderedDict ) : 
~~~ aggregate_reports = [ aggregate_reports ] 
~~ if len ( aggregate_reports ) < 1 : 
~~ for report in aggregate_reports : 
~~~ report [ 'date_range' ] = self . generate_daterange ( report ) 
report = self . strip_metadata ( report ) 
for slice in report [ 'records' ] : 
~~~ slice [ 'date_range' ] = report [ 'date_range' ] 
slice [ 'org_name' ] = report [ 'org_name' ] 
slice [ 'org_email' ] = report [ 'org_email' ] 
slice [ 'policy_published' ] = report [ 'policy_published' ] 
slice [ 'report_id' ] = report [ 'report_id' ] 
self . producer . send ( aggregate_topic , slice ) 
~~ except UnknownTopicOrPartitionError : 
~~~ raise KafkaError ( 
~~~ self . producer . flush ( ) 
~~ ~~ ~~ ~~ def save_forensic_reports_to_kafka ( self , forensic_reports , forensic_topic ) : 
if type ( forensic_reports ) == dict : 
~~~ forensic_reports = [ forensic_reports ] 
~~ if len ( forensic_reports ) < 1 : 
self . producer . send ( forensic_topic , forensic_reports ) 
~~ ~~ def _parse_report_record ( record , nameservers = None , dns_timeout = 2.0 , 
parallel = False ) : 
if nameservers is None : 
~~~ nameservers = [ "1.1.1.1" , "1.0.0.1" , 
"2606:4700:4700::1111" , "2606:4700:4700::1001" , 
~~ record = record . copy ( ) 
new_record = OrderedDict ( ) 
new_record_source = get_ip_address_info ( record [ "row" ] [ "source_ip" ] , 
cache = IP_ADDRESS_CACHE , 
nameservers = nameservers , 
timeout = dns_timeout , 
parallel = parallel ) 
new_record [ "source" ] = new_record_source 
new_record [ "count" ] = int ( record [ "row" ] [ "count" ] ) 
policy_evaluated = record [ "row" ] [ "policy_evaluated" ] . copy ( ) 
new_policy_evaluated = OrderedDict ( [ ( "disposition" , "none" ) , 
( "dkim" , "fail" ) , 
( "spf" , "fail" ) , 
( "policy_override_reasons" , [ ] ) 
if "disposition" in policy_evaluated : 
~~~ new_policy_evaluated [ "disposition" ] = policy_evaluated [ "disposition" ] 
if new_policy_evaluated [ "disposition" ] . strip ( ) . lower ( ) == "pass" : 
~~~ new_policy_evaluated [ "disposition" ] = "none" 
~~ ~~ if "dkim" in policy_evaluated : 
~~~ new_policy_evaluated [ "dkim" ] = policy_evaluated [ "dkim" ] 
~~ if "spf" in policy_evaluated : 
~~~ new_policy_evaluated [ "spf" ] = policy_evaluated [ "spf" ] 
~~ reasons = [ ] 
spf_aligned = policy_evaluated [ "spf" ] == "pass" 
dkim_aligned = policy_evaluated [ "dkim" ] == "pass" 
dmarc_aligned = spf_aligned or dkim_aligned 
new_record [ "alignment" ] = dict ( ) 
new_record [ "alignment" ] [ "spf" ] = spf_aligned 
new_record [ "alignment" ] [ "dkim" ] = dkim_aligned 
new_record [ "alignment" ] [ "dmarc" ] = dmarc_aligned 
if "reason" in policy_evaluated : 
~~~ if type ( policy_evaluated [ "reason" ] ) == list : 
~~~ reasons = policy_evaluated [ "reason" ] 
~~~ reasons = [ policy_evaluated [ "reason" ] ] 
~~ ~~ for reason in reasons : 
~~~ if "comment" not in reason : 
~~~ reason [ "comment" ] = None 
~~ ~~ new_policy_evaluated [ "policy_override_reasons" ] = reasons 
new_record [ "policy_evaluated" ] = new_policy_evaluated 
new_record [ "identifiers" ] = record [ "identifiers" ] . copy ( ) 
new_record [ "auth_results" ] = OrderedDict ( [ ( "dkim" , [ ] ) , ( "spf" , [ ] ) ] ) 
if record [ "auth_results" ] is not None : 
~~~ auth_results = record [ "auth_results" ] . copy ( ) 
if "spf" not in auth_results : 
~~~ auth_results [ "spf" ] = [ ] 
~~ if "dkim" not in auth_results : 
~~~ auth_results [ "dkim" ] = [ ] 
~~~ auth_results = new_record [ "auth_results" ] . copy ( ) 
~~ if type ( auth_results [ "dkim" ] ) != list : 
~~~ auth_results [ "dkim" ] = [ auth_results [ "dkim" ] ] 
~~ for result in auth_results [ "dkim" ] : 
~~~ if "domain" in result and result [ "domain" ] is not None : 
~~~ new_result = OrderedDict ( [ ( "domain" , result [ "domain" ] ) ] ) 
if "selector" in result and result [ "selector" ] is not None : 
~~~ new_result [ "selector" ] = result [ "selector" ] 
~~~ new_result [ "selector" ] = "none" 
~~ if "result" in result and result [ "result" ] is not None : 
~~~ new_result [ "result" ] = result [ "result" ] 
~~~ new_result [ "result" ] = "none" 
~~ new_record [ "auth_results" ] [ "dkim" ] . append ( new_result ) 
~~ ~~ if type ( auth_results [ "spf" ] ) != list : 
~~~ auth_results [ "spf" ] = [ auth_results [ "spf" ] ] 
~~ for result in auth_results [ "spf" ] : 
if "scope" in result and result [ "scope" ] is not None : 
~~~ new_result [ "scope" ] = result [ "scope" ] 
~~~ new_result [ "scope" ] = "mfrom" 
~~ new_record [ "auth_results" ] [ "spf" ] . append ( new_result ) 
~~ if "envelope_from" not in new_record [ "identifiers" ] : 
~~~ envelope_from = None 
if len ( auth_results [ "spf" ] ) > 0 : 
~~~ envelope_from = new_record [ "auth_results" ] [ "spf" ] [ - 1 ] [ "domain" ] 
~~ if envelope_from is not None : 
~~~ envelope_from = str ( envelope_from ) . lower ( ) 
~~ new_record [ "identifiers" ] [ "envelope_from" ] = envelope_from 
~~ elif new_record [ "identifiers" ] [ "envelope_from" ] is None : 
~~~ if len ( auth_results [ "spf" ] ) > 0 : 
if envelope_from is not None : 
~~ ~~ envelope_to = None 
if "envelope_to" in new_record [ "identifiers" ] : 
~~~ envelope_to = new_record [ "identifiers" ] [ "envelope_to" ] 
del new_record [ "identifiers" ] [ "envelope_to" ] 
~~ new_record [ "identifiers" ] [ "envelope_to" ] = envelope_to 
return new_record 
~~ def parse_aggregate_report_xml ( xml , nameservers = None , timeout = 2.0 , 
~~~ xmltodict . parse ( xml ) [ "feedback" ] 
~~~ errors . append ( e . __str__ ( ) ) 
xml = xml_schema_regex . sub ( '' , xml ) 
report = xmltodict . parse ( xml ) [ "feedback" ] 
report_metadata = report [ "report_metadata" ] 
schema = "draft" 
if "version" in report : 
~~~ schema = report [ "version" ] 
~~ new_report = OrderedDict ( [ ( "xml_schema" , schema ) ] ) 
new_report_metadata = OrderedDict ( ) 
if report_metadata [ "org_name" ] is None : 
~~~ if report_metadata [ "email" ] is not None : 
~~~ report_metadata [ "org_name" ] = report_metadata [ 
"email" ] . split ( "@" ) [ - 1 ] 
~~ ~~ org_name = report_metadata [ "org_name" ] 
if org_name is not None : 
~~~ org_name = get_base_domain ( org_name ) 
~~ new_report_metadata [ "org_name" ] = org_name 
new_report_metadata [ "org_email" ] = report_metadata [ "email" ] 
extra = None 
if "extra_contact_info" in report_metadata : 
~~~ extra = report_metadata [ "extra_contact_info" ] 
~~ new_report_metadata [ "org_extra_contact_info" ] = extra 
new_report_metadata [ "report_id" ] = report_metadata [ "report_id" ] 
report_id = new_report_metadata [ "report_id" ] 
report_id = report_id . replace ( "<" , 
"" ) . replace ( ">" , "" ) . split ( "@" ) [ 0 ] 
new_report_metadata [ "report_id" ] = report_id 
date_range = report [ "report_metadata" ] [ "date_range" ] 
date_range [ "begin" ] = timestamp_to_human ( date_range [ "begin" ] ) 
date_range [ "end" ] = timestamp_to_human ( date_range [ "end" ] ) 
new_report_metadata [ "begin_date" ] = date_range [ "begin" ] 
new_report_metadata [ "end_date" ] = date_range [ "end" ] 
if "error" in report [ "report_metadata" ] : 
~~~ if type ( report [ "report_metadata" ] [ "error" ] ) != list : 
~~~ errors = [ report [ "report_metadata" ] [ "error" ] ] 
~~~ errors = report [ "report_metadata" ] [ "error" ] 
~~ ~~ new_report_metadata [ "errors" ] = errors 
new_report [ "report_metadata" ] = new_report_metadata 
records = [ ] 
policy_published = report [ "policy_published" ] 
new_policy_published = OrderedDict ( ) 
new_policy_published [ "domain" ] = policy_published [ "domain" ] 
adkim = "r" 
if "adkim" in policy_published : 
~~~ if policy_published [ "adkim" ] is not None : 
~~~ adkim = policy_published [ "adkim" ] 
~~ ~~ new_policy_published [ "adkim" ] = adkim 
aspf = "r" 
if "aspf" in policy_published : 
~~~ if policy_published [ "aspf" ] is not None : 
~~~ aspf = policy_published [ "aspf" ] 
~~ ~~ new_policy_published [ "aspf" ] = aspf 
new_policy_published [ "p" ] = policy_published [ "p" ] 
sp = new_policy_published [ "p" ] 
if "sp" in policy_published : 
~~~ if policy_published [ "sp" ] is not None : 
~~~ sp = report [ "policy_published" ] [ "sp" ] 
~~ ~~ new_policy_published [ "sp" ] = sp 
pct = "100" 
if "pct" in policy_published : 
~~~ if policy_published [ "pct" ] is not None : 
~~~ pct = report [ "policy_published" ] [ "pct" ] 
~~ ~~ new_policy_published [ "pct" ] = pct 
fo = "0" 
if "fo" in policy_published : 
~~~ if policy_published [ "fo" ] is not None : 
~~~ fo = report [ "policy_published" ] [ "fo" ] 
~~ ~~ new_policy_published [ "fo" ] = fo 
new_report [ "policy_published" ] = new_policy_published 
if type ( report [ "record" ] ) == list : 
~~~ for record in report [ "record" ] : 
~~~ report_record = _parse_report_record ( record , 
dns_timeout = timeout , 
records . append ( report_record ) 
~~~ report_record = _parse_report_record ( report [ "record" ] , 
~~ new_report [ "records" ] = records 
return new_report 
~~ except expat . ExpatError as error : 
~~~ raise InvalidAggregateReport ( 
~~ except KeyError as error : 
~~ except Exception as error : 
~~ ~~ def extract_xml ( input_ ) : 
if type ( input_ ) == str : 
~~~ file_object = open ( input_ , "rb" ) 
~~ elif type ( input_ ) == bytes : 
~~~ file_object = BytesIO ( input_ ) 
~~~ file_object = input_ 
~~~ header = file_object . read ( 6 ) 
file_object . seek ( 0 ) 
if header . startswith ( MAGIC_ZIP ) : 
~~~ _zip = zipfile . ZipFile ( file_object ) 
xml = _zip . open ( _zip . namelist ( ) [ 0 ] ) . read ( ) . decode ( ) 
~~ elif header . startswith ( MAGIC_GZIP ) : 
~~~ xml = GzipFile ( fileobj = file_object ) . read ( ) . decode ( ) 
~~ elif header . startswith ( MAGIC_XML ) : 
~~~ xml = file_object . read ( ) . decode ( ) 
~~~ file_object . close ( ) 
~~ file_object . close ( ) 
~~ return xml 
~~ def parse_aggregate_report_file ( _input , nameservers = None , dns_timeout = 2.0 , 
xml = extract_xml ( _input ) 
return parse_aggregate_report_xml ( xml , 
~~ def parsed_aggregate_reports_to_csv ( reports ) : 
def to_str ( obj ) : 
~~~ return str ( obj ) . lower ( ) 
~~ fields = [ "xml_schema" , "org_name" , "org_email" , 
"org_extra_contact_info" , "report_id" , "begin_date" , "end_date" , 
"errors" , "domain" , "adkim" , "aspf" , "p" , "sp" , "pct" , "fo" , 
"source_ip_address" , "source_country" , "source_reverse_dns" , 
"source_base_domain" , "count" , "disposition" , "dkim_alignment" , 
"spf_alignment" , "policy_override_reasons" , 
"policy_override_comments" , "envelope_from" , "header_from" , 
"envelope_to" , "dkim_domains" , "dkim_selectors" , "dkim_results" , 
"spf_domains" , "spf_scopes" , "spf_results" ] 
csv_file_object = StringIO ( newline = "\\n" ) 
writer = DictWriter ( csv_file_object , fields ) 
writer . writeheader ( ) 
if type ( reports ) == OrderedDict : 
~~~ reports = [ reports ] 
~~ for report in reports : 
~~~ xml_schema = report [ "xml_schema" ] 
org_name = report [ "report_metadata" ] [ "org_name" ] 
org_email = report [ "report_metadata" ] [ "org_email" ] 
org_extra_contact = report [ "report_metadata" ] [ "org_extra_contact_info" ] 
report_id = report [ "report_metadata" ] [ "report_id" ] 
begin_date = report [ "report_metadata" ] [ "begin_date" ] 
end_date = report [ "report_metadata" ] [ "end_date" ] 
errors = "|" . join ( report [ "report_metadata" ] [ "errors" ] ) 
domain = report [ "policy_published" ] [ "domain" ] 
adkim = report [ "policy_published" ] [ "adkim" ] 
aspf = report [ "policy_published" ] [ "aspf" ] 
p = report [ "policy_published" ] [ "p" ] 
sp = report [ "policy_published" ] [ "sp" ] 
pct = report [ "policy_published" ] [ "pct" ] 
fo = report [ "policy_published" ] [ "fo" ] 
report_dict = dict ( xml_schema = xml_schema , org_name = org_name , 
org_email = org_email , 
org_extra_contact_info = org_extra_contact , 
report_id = report_id , begin_date = begin_date , 
end_date = end_date , errors = errors , domain = domain , 
adkim = adkim , aspf = aspf , p = p , sp = sp , pct = pct , fo = fo ) 
for record in report [ "records" ] : 
~~~ row = report_dict 
row [ "source_ip_address" ] = record [ "source" ] [ "ip_address" ] 
row [ "source_country" ] = record [ "source" ] [ "country" ] 
row [ "source_reverse_dns" ] = record [ "source" ] [ "reverse_dns" ] 
row [ "source_base_domain" ] = record [ "source" ] [ "base_domain" ] 
row [ "count" ] = record [ "count" ] 
row [ "disposition" ] = record [ "policy_evaluated" ] [ "disposition" ] 
row [ "spf_alignment" ] = record [ "policy_evaluated" ] [ "spf" ] 
row [ "dkim_alignment" ] = record [ "policy_evaluated" ] [ "dkim" ] 
policy_override_reasons = list ( map ( 
lambda r : r [ "type" ] , 
record [ "policy_evaluated" ] 
[ "policy_override_reasons" ] ) ) 
policy_override_comments = list ( map ( 
lambda r : r [ "comment" ] or "none" , 
row [ "policy_override_reasons" ] = "," . join ( 
policy_override_reasons ) 
row [ "policy_override_comments" ] = "|" . join ( 
policy_override_comments ) 
row [ "envelope_from" ] = record [ "identifiers" ] [ "envelope_from" ] 
row [ "header_from" ] = record [ "identifiers" ] [ "header_from" ] 
row [ "envelope_to" ] = envelope_to 
dkim_domains = [ ] 
dkim_selectors = [ ] 
dkim_results = [ ] 
for dkim_result in record [ "auth_results" ] [ "dkim" ] : 
~~~ dkim_domains . append ( dkim_result [ "domain" ] ) 
if "selector" in dkim_result : 
~~~ dkim_selectors . append ( dkim_result [ "selector" ] ) 
~~ dkim_results . append ( dkim_result [ "result" ] ) 
~~ row [ "dkim_domains" ] = "," . join ( map ( to_str , dkim_domains ) ) 
row [ "dkim_selectors" ] = "," . join ( map ( to_str , dkim_selectors ) ) 
row [ "dkim_results" ] = "," . join ( map ( to_str , dkim_results ) ) 
spf_domains = [ ] 
spf_scopes = [ ] 
spf_results = [ ] 
for spf_result in record [ "auth_results" ] [ "spf" ] : 
~~~ spf_domains . append ( spf_result [ "domain" ] ) 
spf_scopes . append ( spf_result [ "scope" ] ) 
spf_results . append ( spf_result [ "result" ] ) 
~~ row [ "spf_domains" ] = "," . join ( map ( to_str , spf_domains ) ) 
row [ "spf_scopes" ] = "," . join ( map ( to_str , spf_scopes ) ) 
row [ "spf_results" ] = "," . join ( map ( to_str , dkim_results ) ) 
writer . writerow ( row ) 
csv_file_object . flush ( ) 
~~ ~~ return csv_file_object . getvalue ( ) 
~~ def parse_forensic_report ( feedback_report , sample , msg_date , 
nameservers = None , dns_timeout = 2.0 , 
strip_attachment_payloads = False , 
delivery_results = [ "delivered" , "spam" , "policy" , "reject" , "other" ] 
~~~ parsed_report = OrderedDict ( ) 
report_values = feedback_report_regex . findall ( feedback_report ) 
for report_value in report_values : 
~~~ key = report_value [ 0 ] . lower ( ) . replace ( "-" , "_" ) 
parsed_report [ key ] = report_value [ 1 ] 
~~ if "arrival_date" not in parsed_report : 
~~~ if msg_date is None : 
~~ parsed_report [ "arrival_date" ] = msg_date . isoformat ( ) 
~~ if "version" not in parsed_report : 
~~~ parsed_report [ "version" ] = 1 
~~ if "user_agent" not in parsed_report : 
~~~ parsed_report [ "user_agent" ] = None 
~~ if "delivery_result" not in parsed_report : 
~~~ parsed_report [ "delivery_result" ] = None 
~~~ for delivery_result in delivery_results : 
~~~ if delivery_result in parsed_report [ "delivery_result" ] . lower ( ) : 
~~~ parsed_report [ "delivery_result" ] = delivery_result 
~~ ~~ ~~ if parsed_report [ "delivery_result" ] not in delivery_results : 
~~~ parsed_report [ "delivery_result" ] = "other" 
~~ arrival_utc = human_timestamp_to_datetime ( 
parsed_report [ "arrival_date" ] , to_utc = True ) 
parsed_report [ "arrival_date_utc" ] = arrival_utc 
ip_address = parsed_report [ "source_ip" ] 
parsed_report_source = get_ip_address_info ( ip_address , 
parsed_report [ "source" ] = parsed_report_source 
del parsed_report [ "source_ip" ] 
if "identity_alignment" not in parsed_report : 
~~~ parsed_report [ "authentication_mechanisms" ] = [ ] 
~~ elif parsed_report [ "identity_alignment" ] == "none" : 
del parsed_report [ "identity_alignment" ] 
~~~ auth_mechanisms = parsed_report [ "identity_alignment" ] 
auth_mechanisms = auth_mechanisms . split ( "," ) 
parsed_report [ "authentication_mechanisms" ] = auth_mechanisms 
~~ if "auth_failure" not in parsed_report : 
~~~ parsed_report [ "auth_failure" ] = "dmarc" 
~~ auth_failure = parsed_report [ "auth_failure" ] . split ( "," ) 
parsed_report [ "auth_failure" ] = auth_failure 
optional_fields = [ "original_envelope_id" , "dkim_domain" , 
"original_mail_from" , "original_rcpt_to" ] 
for optional_field in optional_fields : 
~~~ if optional_field not in parsed_report : 
~~~ parsed_report [ optional_field ] = None 
~~ ~~ parsed_sample = parse_email ( 
sample , 
strip_attachment_payloads = strip_attachment_payloads ) 
if "reported_domain" not in parsed_report : 
~~~ parsed_report [ "reported_domain" ] = parsed_sample [ "from" ] [ "domain" ] 
~~ sample_headers_only = False 
number_of_attachments = len ( parsed_sample [ "attachments" ] ) 
if number_of_attachments < 1 and parsed_sample [ "body" ] is None : 
~~~ sample_headers_only = True 
~~ if sample_headers_only and parsed_sample [ "has_defects" ] : 
~~~ del parsed_sample [ "defects" ] 
del parsed_sample [ "defects_categories" ] 
del parsed_sample [ "has_defects" ] 
~~ parsed_report [ "sample_headers_only" ] = sample_headers_only 
parsed_report [ "sample" ] = sample 
parsed_report [ "parsed_sample" ] = parsed_sample 
return parsed_report 
error . __str__ ( ) ) ) 
~~ ~~ def parsed_forensic_reports_to_csv ( reports ) : 
fields = [ "feedback_type" , "user_agent" , "version" , "original_envelope_id" , 
"original_mail_from" , "original_rcpt_to" , "arrival_date" , 
"arrival_date_utc" , "subject" , "message_id" , 
"authentication_results" , "dkim_domain" , "source_ip_address" , 
"source_country" , "source_reverse_dns" , "source_base_domain" , 
"delivery_result" , "auth_failure" , "reported_domain" , 
"authentication_mechanisms" , "sample_headers_only" ] 
~~ csv_file = StringIO ( ) 
csv_writer = DictWriter ( csv_file , fieldnames = fields ) 
csv_writer . writeheader ( ) 
for report in reports : 
~~~ row = report . copy ( ) 
row [ "source_ip_address" ] = report [ "source" ] [ "ip_address" ] 
row [ "source_reverse_dns" ] = report [ "source" ] [ "reverse_dns" ] 
row [ "source_base_domain" ] = report [ "source" ] [ "base_domain" ] 
row [ "source_country" ] = report [ "source" ] [ "country" ] 
del row [ "source" ] 
row [ "subject" ] = report [ "parsed_sample" ] [ "subject" ] 
row [ "auth_failure" ] = "," . join ( report [ "auth_failure" ] ) 
authentication_mechanisms = report [ "authentication_mechanisms" ] 
row [ "authentication_mechanisms" ] = "," . join ( 
authentication_mechanisms ) 
del row [ "sample" ] 
del row [ "parsed_sample" ] 
csv_writer . writerow ( row ) 
~~ return csv_file . getvalue ( ) 
~~ def parse_report_email ( input_ , nameservers = None , dns_timeout = 2.0 , 
strip_attachment_payloads = False , parallel = False ) : 
result = None 
~~~ if is_outlook_msg ( input_ ) : 
~~~ input_ = convert_outlook_msg ( input_ ) 
~~ if type ( input_ ) == bytes : 
~~~ input_ = input_ . decode ( encoding = "utf8" ) 
~~ msg = mailparser . parse_from_string ( input_ ) 
msg_headers = json . loads ( msg . headers_json ) 
date = email . utils . format_datetime ( datetime . utcnow ( ) ) 
if "Date" in msg_headers : 
~~~ date = human_timestamp_to_datetime ( 
msg_headers [ "Date" ] ) 
~~ msg = email . message_from_string ( input_ ) 
~~~ raise InvalidDMARCReport ( e . __str__ ( ) ) 
~~ subject = None 
feedback_report = None 
sample = None 
if "Subject" in msg_headers : 
~~~ subject = msg_headers [ "Subject" ] 
~~ for part in msg . walk ( ) : 
~~~ content_type = part . get_content_type ( ) 
payload = part . get_payload ( ) 
if type ( payload ) != list : 
~~~ payload = [ payload ] 
~~ payload = payload [ 0 ] . __str__ ( ) 
if content_type == "message/feedback-report" : 
~~~ if "Feedback-Type" in payload : 
~~~ feedback_report = payload 
~~~ feedback_report = b64decode ( payload ) . __str__ ( ) 
~~ feedback_report = feedback_report . lstrip ( 
"b\ ) . rstrip ( "\ ) 
feedback_report = feedback_report . replace ( "\\\\r" , "" ) 
feedback_report = feedback_report . replace ( "\\\\n" , "\\n" ) 
~~ except ( ValueError , TypeError , binascii . Error ) : 
~~ ~~ elif content_type == "text/rfc822-headers" : 
~~~ sample = payload 
~~ elif content_type == "message/rfc822" : 
~~~ payload = b64decode ( payload ) 
if payload . startswith ( MAGIC_ZIP ) or payload . startswith ( MAGIC_GZIP ) or payload . startswith ( MAGIC_XML ) : 
~~~ ns = nameservers 
aggregate_report = parse_aggregate_report_file ( 
payload , 
nameservers = ns , 
dns_timeout = dns_timeout , 
result = OrderedDict ( [ ( "report_type" , "aggregate" ) , 
( "report" , aggregate_report ) ] ) 
~~ ~~ except ( TypeError , ValueError , binascii . Error ) : 
~~ except InvalidAggregateReport as e : 
raise InvalidAggregateReport ( error ) 
~~ except FileNotFoundError as e : 
raise InvalidDMARCReport ( error ) 
~~ ~~ ~~ if feedback_report and sample : 
~~~ forensic_report = parse_forensic_report ( 
feedback_report , 
date , 
strip_attachment_payloads = strip_attachment_payloads , 
~~ except InvalidForensicReport as e : 
raise InvalidForensicReport ( error ) 
~~~ raise InvalidForensicReport ( e . __str__ ( ) ) 
~~ result = OrderedDict ( [ ( "report_type" , "forensic" ) , 
( "report" , forensic_report ) ] ) 
~~ if result is None : 
~~ ~~ def parse_report_file ( input_ , nameservers = None , dns_timeout = 2.0 , 
~~ content = file_object . read ( ) 
~~~ report = parse_aggregate_report_file ( content , nameservers = nameservers , 
results = OrderedDict ( [ ( "report_type" , "aggregate" ) , 
( "report" , report ) ] ) 
~~ except InvalidAggregateReport : 
~~~ sa = strip_attachment_payloads 
results = parse_report_email ( content , 
strip_attachment_payloads = sa , 
~~ except InvalidDMARCReport : 
"report" ) 
~~ def get_imap_capabilities ( server ) : 
capabilities = list ( map ( str , list ( server . capabilities ( ) ) ) ) 
for i in range ( len ( capabilities ) ) : 
~~~ capabilities [ i ] = str ( capabilities [ i ] ) . replace ( "b\ , 
"" ) . replace ( "\ , 
"" ) 
return capabilities 
~~ def get_dmarc_reports_from_inbox ( host = None , 
user = None , 
password = None , 
connection = None , 
port = None , 
ssl = True , 
ssl_context = None , 
move_supported = None , 
reports_folder = "INBOX" , 
archive_folder = "Archive" , 
delete = False , 
test = False , 
nameservers = None , 
dns_timeout = 6.0 , 
results = None ) : 
def chunks ( l , n ) : 
for i in range ( 0 , len ( l ) , n ) : 
~~~ yield l [ i : i + n ] 
~~ ~~ if delete and test : 
~~ if connection is None and ( user is None or password is None ) : 
"password" ) 
~~ aggregate_reports = [ ] 
forensic_reports = [ ] 
aggregate_report_msg_uids = [ ] 
forensic_report_msg_uids = [ ] 
aggregate_reports_folder = "{0}/Aggregate" . format ( archive_folder ) 
forensic_reports_folder = "{0}/Forensic" . format ( archive_folder ) 
invalid_reports_folder = "{0}/Invalid" . format ( archive_folder ) 
if results : 
~~~ aggregate_reports = results [ "aggregate_reports" ] . copy ( ) 
forensic_reports = results [ "forensic_reports" ] . copy ( ) 
~~~ if connection : 
~~~ server = connection 
~~~ if not ssl : 
~~ if ssl_context is None : 
~~~ ssl_context = create_default_context ( ) 
~~ server = imapclient . IMAPClient ( host , 
port = port , 
ssl = ssl , 
ssl_context = ssl_context , 
use_uid = True ) 
server . login ( user , password ) 
~~ if move_supported is None : 
~~~ server_capabilities = get_imap_capabilities ( server ) 
move_supported = "MOVE" in server_capabilities 
~~ def delete_messages ( msg_uids ) : 
str ( uid ) for uid in msg_uids ) ) ) 
if type ( msg_uids ) == str or type ( msg_uids ) == int : 
~~~ msg_uids = [ int ( msg_uids ) ] 
~~ server . delete_messages ( msg_uids , silent = True ) 
server . expunge ( msg_uids ) 
~~ def move_messages ( msg_uids , folder ) : 
~~~ if type ( msg_uids ) == str or type ( msg_uids ) == int : 
~~ for chunk in chunks ( msg_uids , 100 ) : 
~~~ if move_supported : 
"," . join ( str ( uid ) for uid in chunk ) , folder 
server . move ( chunk , folder ) 
server . copy ( msg_uids , folder ) 
delete_messages ( msg_uids ) 
~~ ~~ ~~ if not server . folder_exists ( archive_folder ) : 
server . create_folder ( archive_folder ) 
~~~ if not server . folder_exists ( aggregate_reports_folder ) : 
~~~ server . create_folder ( aggregate_reports_folder ) 
aggregate_reports_folder ) ) 
~~ ~~ except imapclient . exceptions . IMAPClientError : 
~~~ aggregate_reports_folder = aggregate_reports_folder . replace ( "/" , 
"." ) 
forensic_reports_folder = forensic_reports_folder . replace ( "/" , 
invalid_reports_folder = invalid_reports_folder . replace ( "/" , 
~~ subfolders = [ aggregate_reports_folder , 
forensic_reports_folder , 
invalid_reports_folder ] 
for subfolder in subfolders : 
~~~ if not server . folder_exists ( subfolder ) : 
server . create_folder ( subfolder ) 
~~ ~~ server . select_folder ( reports_folder ) 
messages = server . search ( ) 
total_messages = len ( messages ) 
len ( messages ) , reports_folder ) ) 
for i in range ( len ( messages ) ) : 
~~~ msg_uid = messages [ i ] 
i + 1 , 
total_messages , 
msg_uid 
~~~ raw_msg = server . fetch ( msg_uid , 
[ "RFC822" ] ) [ msg_uid ] 
msg_keys = [ b'RFC822' , b'BODY[NULL]' , b'BODY[]' ] 
msg_key = '' 
for key in msg_keys : 
~~~ if key in raw_msg . keys ( ) : 
~~~ msg_key = key 
~~ ~~ raw_msg = raw_msg [ msg_key ] 
~~ except ( ConnectionResetError , socket . error , 
TimeoutError , 
imapclient . exceptions . IMAPClientError ) as error : 
~~~ error = error . __str__ ( ) . lstrip ( "b\ ) . rstrip ( "\ ) . rstrip ( 
~~~ server . shutdown ( ) 
~~ if not ssl : 
server . select_folder ( reports_folder ) 
raw_msg = server . fetch ( msg_uid , 
[ "RFC822" ] ) [ msg_uid ] [ b"RFC822" ] 
~~ msg_content = raw_msg . decode ( "utf-8" , errors = "replace" ) 
sa = strip_attachment_payloads 
parsed_email = parse_report_email ( msg_content , 
strip_attachment_payloads = sa ) 
if parsed_email [ "report_type" ] == "aggregate" : 
~~~ aggregate_reports . append ( parsed_email [ "report" ] ) 
aggregate_report_msg_uids . append ( msg_uid ) 
~~ elif parsed_email [ "report_type" ] == "forensic" : 
~~~ forensic_reports . append ( parsed_email [ "report" ] ) 
forensic_report_msg_uids . append ( msg_uid ) 
~~ ~~ except InvalidDMARCReport as error : 
~~~ logger . warning ( error . __str__ ( ) ) 
if not test : 
~~~ if delete : 
delete_messages ( [ msg_uid ] ) 
msg_uid , invalid_reports_folder ) ) 
move_messages ( [ msg_uid ] , invalid_reports_folder ) 
~~ ~~ ~~ ~~ if not test : 
~~~ processed_messages = aggregate_report_msg_uids + forensic_report_msg_uids 
number_of_processed_msgs = len ( processed_messages ) 
for i in range ( number_of_processed_msgs ) : 
~~~ msg_uid = processed_messages [ i ] 
i + 1 , number_of_processed_msgs , msg_uid ) ) 
~~~ delete_messages ( [ msg_uid ] ) 
~~ except imapclient . exceptions . IMAPClientError as e : 
~~~ e = e . __str__ ( ) . lstrip ( "b\ ) . rstrip ( 
"\ ) . rstrip ( "." ) 
TimeoutError ) as e : 
~~~ if len ( aggregate_report_msg_uids ) > 0 : 
log_message , reports_folder , 
number_of_agg_report_msgs = len ( aggregate_report_msg_uids ) 
for i in range ( number_of_agg_report_msgs ) : 
~~~ msg_uid = aggregate_report_msg_uids [ i ] 
i + 1 , number_of_agg_report_msgs , msg_uid ) ) 
~~~ move_messages ( [ msg_uid ] , 
aggregate_reports_folder ) 
TimeoutError ) as error : 
e . __str__ ( ) ) ) 
~~ server = imapclient . IMAPClient ( 
host , 
use_uid = True 
move_messages ( [ msg_uid ] , 
~~ ~~ ~~ if len ( forensic_report_msg_uids ) > 0 : 
reports_folder , 
forensic_reports_folder ) ) 
number_of_forensic_msgs = len ( forensic_report_msg_uids ) 
for i in range ( number_of_forensic_msgs ) : 
~~~ msg_uid = forensic_report_msg_uids [ i ] 
message , 
i + 1 , number_of_forensic_msgs , msg_uid ) ) 
forensic_reports_folder ) 
msg_uid , e ) 
~~ except ( ConnectionResetError , TimeoutError ) as error : 
~~ ~~ ~~ ~~ ~~ results = OrderedDict ( [ ( "aggregate_reports" , aggregate_reports ) , 
( "forensic_reports" , forensic_reports ) ] ) 
if not test and total_messages > 0 : 
~~~ results = get_dmarc_reports_from_inbox ( 
host = host , 
user = user , 
password = password , 
connection = connection , 
move_supported = move_supported , 
reports_folder = reports_folder , 
archive_folder = archive_folder , 
delete = delete , 
test = test , 
results = results 
~~ except imapclient . exceptions . IMAPClientError as error : 
~~~ error = error . __str__ ( ) . lstrip ( "b\ ) . rstrip ( "\ ) . rstrip ( "." ) 
~~~ sleep_minutes = 5 
error , 
sleep_minutes ) ) 
time . sleep ( sleep_minutes * 60 ) 
results = get_dmarc_reports_from_inbox ( 
~~ raise IMAPError ( error ) 
~~ except socket . gaierror : 
~~ except ConnectionRefusedError : 
~~ except ConnectionResetError : 
~~ except ConnectionAbortedError : 
~~ except TimeoutError : 
~~ except SSLError as error : 
~~ except CertificateError as error : 
~~ ~~ def save_output ( results , output_directory = "output" ) : 
aggregate_reports = results [ "aggregate_reports" ] 
forensic_reports = results [ "forensic_reports" ] 
if os . path . exists ( output_directory ) : 
~~~ if not os . path . isdir ( output_directory ) : 
~~~ os . makedirs ( output_directory ) 
~~ with open ( "{0}" . format ( os . path . join ( output_directory , "aggregate.json" ) ) , 
"w" , newline = "\\n" , encoding = "utf-8" ) as agg_json : 
~~~ agg_json . write ( json . dumps ( aggregate_reports , ensure_ascii = False , 
indent = 2 ) ) 
~~ with open ( "{0}" . format ( os . path . join ( output_directory , "aggregate.csv" ) ) , 
"w" , newline = "\\n" , encoding = "utf-8" ) as agg_csv : 
~~~ csv = parsed_aggregate_reports_to_csv ( aggregate_reports ) 
agg_csv . write ( csv ) 
~~ with open ( "{0}" . format ( os . path . join ( output_directory , "forensic.json" ) ) , 
"w" , newline = "\\n" , encoding = "utf-8" ) as for_json : 
~~~ for_json . write ( json . dumps ( forensic_reports , ensure_ascii = False , 
~~ with open ( "{0}" . format ( os . path . join ( output_directory , "forensic.csv" ) ) , 
"w" , newline = "\\n" , encoding = "utf-8" ) as for_csv : 
~~~ csv = parsed_forensic_reports_to_csv ( forensic_reports ) 
for_csv . write ( csv ) 
~~ samples_directory = os . path . join ( output_directory , "samples" ) 
if not os . path . exists ( samples_directory ) : 
~~~ os . makedirs ( samples_directory ) 
~~ sample_filenames = [ ] 
for forensic_report in forensic_reports : 
~~~ sample = forensic_report [ "sample" ] 
message_count = 0 
parsed_sample = forensic_report [ "parsed_sample" ] 
subject = parsed_sample [ "filename_safe_subject" ] 
filename = subject 
while filename in sample_filenames : 
~~~ message_count += 1 
~~ sample_filenames . append ( filename ) 
filename = "{0}.eml" . format ( filename ) 
path = os . path . join ( samples_directory , filename ) 
with open ( path , "w" , newline = "\\n" , encoding = "utf-8" ) as sample_file : 
~~~ sample_file . write ( sample ) 
~~ ~~ ~~ def get_report_zip ( results ) : 
def add_subdir ( root_path , subdir ) : 
~~~ subdir_path = os . path . join ( root_path , subdir ) 
for subdir_root , subdir_dirs , subdir_files in os . walk ( subdir_path ) : 
~~~ for subdir_file in subdir_files : 
~~~ subdir_file_path = os . path . join ( root_path , subdir , subdir_file ) 
if os . path . isfile ( subdir_file_path ) : 
~~~ rel_path = os . path . relpath ( subdir_root , subdir_file_path ) 
subdir_arc_name = os . path . join ( rel_path , subdir_file ) 
zip_file . write ( subdir_file_path , subdir_arc_name ) 
~~ ~~ for subdir in subdir_dirs : 
~~~ add_subdir ( subdir_path , subdir ) 
~~ ~~ ~~ storage = BytesIO ( ) 
tmp_dir = tempfile . mkdtemp ( ) 
~~~ save_output ( results , tmp_dir ) 
with zipfile . ZipFile ( storage , 'w' , zipfile . ZIP_DEFLATED ) as zip_file : 
~~~ for root , dirs , files in os . walk ( tmp_dir ) : 
~~~ for file in files : 
~~~ file_path = os . path . join ( root , file ) 
if os . path . isfile ( file_path ) : 
~~~ arcname = os . path . join ( os . path . relpath ( root , tmp_dir ) , 
file ) 
zip_file . write ( file_path , arcname ) 
~~ ~~ for directory in dirs : 
~~~ dir_path = os . path . join ( root , directory ) 
if os . path . isdir ( dir_path ) : 
~~~ zip_file . write ( dir_path , directory ) 
add_subdir ( root , directory ) 
~~ ~~ ~~ ~~ ~~ finally : 
~~~ shutil . rmtree ( tmp_dir ) 
~~ return storage . getvalue ( ) 
~~ def email_results ( results , host , mail_from , mail_to , port = 0 , 
ssl = False , user = None , password = None , subject = None , 
attachment_filename = None , message = None , ssl_context = None ) : 
date_string = datetime . now ( ) . strftime ( "%Y-%m-%d" ) 
if attachment_filename : 
~~~ if not attachment_filename . lower ( ) . endswith ( ".zip" ) : 
~~~ attachment_filename += ".zip" 
~~ filename = attachment_filename 
~~~ filename = "DMARC-{0}.zip" . format ( date_string ) 
~~ assert isinstance ( mail_to , list ) 
msg = MIMEMultipart ( ) 
msg [ 'From' ] = mail_from 
msg [ 'Date' ] = email . utils . formatdate ( localtime = True ) 
msg . attach ( MIMEText ( text ) ) 
zip_bytes = get_report_zip ( results ) 
part = MIMEApplication ( zip_bytes , Name = filename ) 
part [ 'Content-Disposition' ] = \ . format ( filename ) 
msg . attach ( part ) 
~~~ if ssl_context is None : 
~~ if ssl : 
~~~ server = smtplib . SMTP_SSL ( host , port = port , context = ssl_context ) 
server . connect ( host , port ) 
server . ehlo_or_helo_if_needed ( ) 
~~~ server = smtplib . SMTP ( host , port = port ) 
if server . has_extn ( "starttls" ) : 
~~~ server . starttls ( context = ssl_context ) 
server . ehlo ( ) 
~~ ~~ if user and password : 
~~~ server . login ( user , password ) 
~~ server . sendmail ( mail_from , mail_to , msg . as_string ( ) ) 
~~ except smtplib . SMTPException as error : 
raise SMTPError ( error ) 
~~ ~~ def watch_inbox ( host , username , password , callback , port = None , ssl = True , 
ssl_context = None , reports_folder = "INBOX" , 
archive_folder = "Archive" , delete = False , test = False , wait = 30 , 
nameservers = None , dns_timeout = 6.0 , 
strip_attachment_payloads = False ) : 
rf = reports_folder 
af = archive_folder 
ns = nameservers 
dt = dns_timeout 
if ssl_context is None : 
~~ server = imapclient . IMAPClient ( host , port = port , ssl = ssl , 
~~~ server . login ( username , password ) 
imap_capabilities = get_imap_capabilities ( server ) 
if "IDLE" not in imap_capabilities : 
~~ ms = "MOVE" in imap_capabilities 
server . select_folder ( rf ) 
idle_start_time = time . monotonic ( ) 
server . idle ( ) 
~~~ error = error . __str__ ( ) . replace ( "b\ , "" ) . replace ( "\ , "" ) 
~~~ server . logout ( ) 
~~ server = imapclient . IMAPClient ( host ) 
server . login ( username , password ) 
ms = "MOVE" in get_imap_capabilities ( server ) 
res = get_dmarc_reports_from_inbox ( connection = server , 
move_supported = ms , 
reports_folder = rf , 
archive_folder = af , 
dns_timeout = dt , 
callback ( res ) 
~~~ raise IMAPError ( error ) 
~~ ~~ except socket . gaierror : 
dns_timeout = dt ) 
~~ except BrokenPipeError : 
~~~ if time . monotonic ( ) - idle_start_time > 5 * 60 : 
server . idle_done ( ) 
~~ responses = server . idle_check ( timeout = wait ) 
if responses is not None : 
~~~ if len ( responses ) == 0 : 
~~~ server . idle_done ( ) 
~~ for response in responses : 
if response [ 0 ] > 0 and response [ 1 ] == b'RECENT' : 
~~ ~~ ~~ ~~ except imapclient . exceptions . IMAPClientError as error : 
~~ except ( KeyError , socket . error , BrokenPipeError , ConnectionResetError ) : 
~~ ~~ def save_aggregate_reports_to_splunk ( self , aggregate_reports ) : 
if type ( aggregate_reports ) == dict : 
~~ data = self . _common_data . copy ( ) 
json_str = "" 
for report in aggregate_reports : 
~~~ for record in report [ "records" ] : 
~~~ new_report = dict ( ) 
for metadata in report [ "report_metadata" ] : 
~~~ new_report [ metadata ] = report [ "report_metadata" ] [ metadata ] 
~~ new_report [ "published_policy" ] = report [ "policy_published" ] 
new_report [ "source_ip_address" ] = record [ "source" ] [ 
"ip_address" ] 
new_report [ "source_country" ] = record [ "source" ] [ "country" ] 
new_report [ "source_reverse_dns" ] = record [ "source" ] [ 
"reverse_dns" ] 
new_report [ "source_base_domain" ] = record [ "source" ] [ 
"base_domain" ] 
new_report [ "message_count" ] = record [ "count" ] 
new_report [ "disposition" ] = record [ "policy_evaluated" ] [ 
"disposition" 
new_report [ "spf_aligned" ] = record [ "alignment" ] [ "spf" ] 
new_report [ "dkim_aligned" ] = record [ "alignment" ] [ "dkim" ] 
new_report [ "passed_dmarc" ] = record [ "alignment" ] [ "dmarc" ] 
new_report [ "header_from" ] = record [ "identifiers" ] [ 
"header_from" ] 
new_report [ "envelope_from" ] = record [ "identifiers" ] [ 
"envelope_from" ] 
if "dkim" in record [ "auth_results" ] : 
~~~ new_report [ "dkim_results" ] = record [ "auth_results" ] [ 
"dkim" ] 
~~ if "spf" in record [ "auth_results" ] : 
~~~ new_report [ "spf_results" ] = record [ "auth_results" ] [ 
"spf" ] 
~~ data [ "sourcetype" ] = "dmarc:aggregate" 
timestamp = human_timestamp_to_timestamp ( 
new_report [ "begin_date" ] ) 
data [ "time" ] = timestamp 
data [ "event" ] = new_report . copy ( ) 
json_str += "{0}\\n" . format ( json . dumps ( data ) ) 
~~ ~~ if not self . session . verify : 
~~~ response = self . session . post ( self . url , data = json_str , 
timeout = self . timeout ) 
response = response . json ( ) 
~~~ raise SplunkError ( e . __str__ ( ) ) 
~~ if response [ "code" ] != 0 : 
~~~ raise SplunkError ( response [ "text" ] ) 
~~ ~~ def save_forensic_reports_to_splunk ( self , forensic_reports ) : 
~~ json_str = "" 
for report in forensic_reports : 
~~~ data = self . _common_data . copy ( ) 
data [ "sourcetype" ] = "dmarc:forensic" 
report [ "arrival_date_utc" ] ) 
data [ "event" ] = report . copy ( ) 
~~ if not self . session . verify : 
~~ ~~ def decode_base64 ( data ) : 
data = bytes ( data , encoding = "ascii" ) 
missing_padding = len ( data ) % 4 
if missing_padding != 0 : 
~~~ data += b'=' * ( 4 - missing_padding ) 
~~ return base64 . b64decode ( data ) 
~~ def get_base_domain ( domain , use_fresh_psl = False ) : 
psl_path = os . path . join ( tempdir , "public_suffix_list.dat" ) 
def download_psl ( ) : 
~~~ url = "https://publicsuffix.org/list/public_suffix_list.dat" 
headers = { "User-Agent" : USER_AGENT } 
fresh_psl = requests . get ( url , headers = headers ) . text 
with open ( psl_path , "w" , encoding = "utf-8" ) as fresh_psl_file : 
~~~ fresh_psl_file . write ( fresh_psl ) 
~~ ~~ if use_fresh_psl : 
~~~ if not os . path . exists ( psl_path ) : 
~~~ download_psl ( ) 
~~~ psl_age = datetime . now ( ) - datetime . fromtimestamp ( 
os . stat ( psl_path ) . st_mtime ) 
if psl_age > timedelta ( hours = 24 ) : 
~~ ~~ ~~ with open ( psl_path , encoding = "utf-8" ) as psl_file : 
~~~ psl = publicsuffix2 . PublicSuffixList ( psl_file ) 
~~ return psl . get_public_suffix ( domain ) 
~~~ return publicsuffix2 . get_public_suffix ( domain ) 
~~ ~~ def query_dns ( domain , record_type , cache = None , nameservers = None , timeout = 2.0 ) : 
domain = str ( domain ) . lower ( ) 
record_type = record_type . upper ( ) 
cache_key = "{0}_{1}" . format ( domain , record_type ) 
if cache : 
~~~ records = cache . get ( cache_key , None ) 
if records : 
~~~ return records 
~~ ~~ resolver = dns . resolver . Resolver ( ) 
timeout = float ( timeout ) 
~~ resolver . nameservers = nameservers 
resolver . timeout = timeout 
resolver . lifetime = timeout 
if record_type == "TXT" : 
~~~ resource_records = list ( map ( 
lambda r : r . strings , 
resolver . query ( domain , record_type , tcp = True ) ) ) 
_resource_record = [ 
resource_record [ 0 ] [ : 0 ] . join ( resource_record ) 
for resource_record in resource_records if resource_record ] 
records = [ r . decode ( ) for r in _resource_record ] 
~~~ records = list ( map ( 
lambda r : r . to_text ( ) . replace ( \ , '' ) . rstrip ( "." ) , 
~~ if cache : 
~~~ cache [ cache_key ] = records 
~~ return records 
~~ def get_reverse_dns ( ip_address , cache = None , nameservers = None , timeout = 2.0 ) : 
hostname = None 
~~~ address = dns . reversename . from_address ( ip_address ) 
hostname = query_dns ( address , "PTR" , cache = cache , 
timeout = timeout ) [ 0 ] 
~~ except dns . exception . DNSException : 
~~ return hostname 
~~ def human_timestamp_to_datetime ( human_timestamp , to_utc = False ) : 
settings = { } 
if to_utc : 
~~~ settings = { "TO_TIMEZONE" : "UTC" } 
~~ return dateparser . parse ( human_timestamp , settings = settings ) 
~~ def get_ip_address_country ( ip_address , parallel = False ) : 
def download_country_database ( location = "GeoLite2-Country.mmdb" ) : 
if parallel : 
~~ url = "https://geolite.maxmind.com/download/geoip/database/" "GeoLite2-Country.tar.gz" 
original_filename = "GeoLite2-Country.mmdb" 
~~~ response = requests . get ( url , headers = headers ) 
response . raise_for_status ( ) 
tar_bytes = response . content 
tar_file = tarfile . open ( fileobj = BytesIO ( tar_bytes ) , mode = "r:gz" ) 
tar_dir = tar_file . getnames ( ) [ 0 ] 
tar_path = "{0}/{1}" . format ( tar_dir , original_filename ) 
tar_file . extract ( tar_path ) 
shutil . move ( tar_path , location ) 
shutil . rmtree ( tar_dir ) 
~~ ~~ system_paths = [ 
"GeoLite2-Country.mmdb" , 
"/usr/local/share/GeoIP/GeoLite2-Country.mmdb" , 
"/usr/share/GeoIP/GeoLite2-Country.mmdb" , 
"/var/lib/GeoIP/GeoLite2-Country.mmdb" , 
"/var/local/lib/GeoIP/GeoLite2-Country.mmdb" , 
"C:\\\\GeoIP\\\\GeoLite2-Country.mmdb" 
db_path = None 
for system_path in system_paths : 
~~~ if os . path . exists ( system_path ) : 
~~~ db_path = system_path 
~~ ~~ if db_path is None : 
~~~ db_path = os . path . join ( tempdir , "GeoLite2-Country.mmdb" ) 
if not os . path . exists ( db_path ) : 
~~~ download_country_database ( db_path ) 
~~~ db_age = datetime . now ( ) - datetime . fromtimestamp ( 
os . stat ( db_path ) . st_mtime ) 
if db_age > timedelta ( days = 7 ) : 
~~~ download_country_database ( ) 
~~ ~~ db_path = db_path 
~~ db_reader = geoip2 . database . Reader ( db_path ) 
country = None 
~~~ country = db_reader . country ( ip_address ) . country . iso_code 
~~ except geoip2 . errors . AddressNotFoundError : 
~~ return country 
~~ def get_ip_address_info ( ip_address , cache = None , nameservers = None , 
timeout = 2.0 , parallel = False ) : 
ip_address = ip_address . lower ( ) 
~~~ info = cache . get ( ip_address , None ) 
~~~ return info 
~~ ~~ info = OrderedDict ( ) 
info [ "ip_address" ] = ip_address 
reverse_dns = get_reverse_dns ( ip_address , 
timeout = timeout ) 
country = get_ip_address_country ( ip_address , parallel = parallel ) 
info [ "country" ] = country 
info [ "reverse_dns" ] = reverse_dns 
info [ "base_domain" ] = None 
if reverse_dns is not None : 
~~~ base_domain = get_base_domain ( reverse_dns ) 
info [ "base_domain" ] = base_domain 
~~ return info 
~~ def get_filename_safe_string ( string ) : 
invalid_filename_chars = [ '\\\\' , '/' , ':' , \ , '*' , '?' , '|' , '\\n' , 
'\\r' ] 
if string is None : 
~~~ string = "None" 
~~ for char in invalid_filename_chars : 
~~~ string = string . replace ( char , "" ) 
~~ string = string . rstrip ( "." ) 
~~ def convert_outlook_msg ( msg_bytes ) : 
if not is_outlook_msg ( msg_bytes ) : 
~~ orig_dir = os . getcwd ( ) 
os . chdir ( tmp_dir ) 
with open ( "sample.msg" , "wb" ) as msg_file : 
~~~ msg_file . write ( msg_bytes ) 
~~~ subprocess . check_call ( [ "msgconvert" , "sample.msg" ] , 
stdout = null_file , stderr = null_file ) 
eml_path = "sample.eml" 
with open ( eml_path , "rb" ) as eml_file : 
~~~ rfc822 = eml_file . read ( ) 
~~ ~~ except FileNotFoundError : 
~~~ raise EmailParserError ( 
~~~ os . chdir ( orig_dir ) 
shutil . rmtree ( tmp_dir ) 
~~ return rfc822 
~~ def parse_email ( data , strip_attachment_payloads = False ) : 
if type ( data ) == bytes : 
~~~ if is_outlook_msg ( data ) : 
~~~ data = convert_outlook_msg ( data ) 
~~ data = data . decode ( "utf-8" , errors = "replace" ) 
~~ parsed_email = mailparser . parse_from_string ( data ) 
headers = json . loads ( parsed_email . headers_json ) . copy ( ) 
parsed_email = json . loads ( parsed_email . mail_json ) . copy ( ) 
parsed_email [ "headers" ] = headers 
if "received" in parsed_email : 
~~~ for received in parsed_email [ "received" ] : 
~~~ if "date_utc" in received : 
~~~ if received [ "date_utc" ] is None : 
~~~ del received [ "date_utc" ] 
~~~ received [ "date_utc" ] = received [ "date_utc" ] . replace ( "T" , 
~~ ~~ ~~ ~~ if "from" not in parsed_email : 
~~~ if "From" in parsed_email [ "headers" ] : 
~~~ parsed_email [ "from" ] = parsed_email [ "Headers" ] [ "From" ] 
~~~ parsed_email [ "from" ] = None 
~~ ~~ if parsed_email [ "from" ] is not None : 
~~~ parsed_email [ "from" ] = parse_email_address ( parsed_email [ "from" ] [ 0 ] ) 
~~ if "date" in parsed_email : 
~~~ parsed_email [ "date" ] = None 
~~ if "reply_to" in parsed_email : 
~~~ parsed_email [ "reply_to" ] = list ( map ( lambda x : parse_email_address ( x ) , 
parsed_email [ "reply_to" ] ) ) 
~~~ parsed_email [ "reply_to" ] = [ ] 
~~ if "to" in parsed_email : 
~~~ parsed_email [ "to" ] = list ( map ( lambda x : parse_email_address ( x ) , 
parsed_email [ "to" ] ) ) 
~~~ parsed_email [ "to" ] = [ ] 
~~ if "cc" in parsed_email : 
~~~ parsed_email [ "cc" ] = list ( map ( lambda x : parse_email_address ( x ) , 
parsed_email [ "cc" ] ) ) 
~~~ parsed_email [ "cc" ] = [ ] 
~~ if "bcc" in parsed_email : 
~~~ parsed_email [ "bcc" ] = list ( map ( lambda x : parse_email_address ( x ) , 
parsed_email [ "bcc" ] ) ) 
~~~ parsed_email [ "bcc" ] = [ ] 
~~ if "delivered_to" in parsed_email : 
~~~ parsed_email [ "delivered_to" ] = list ( 
map ( lambda x : parse_email_address ( x ) , 
parsed_email [ "delivered_to" ] ) 
~~ if "attachments" not in parsed_email : 
~~~ parsed_email [ "attachments" ] = [ ] 
~~~ for attachment in parsed_email [ "attachments" ] : 
~~~ if "payload" in attachment : 
~~~ payload = attachment [ "payload" ] 
~~~ if "content_transfer_encoding" in attachment : 
~~~ if attachment [ "content_transfer_encoding" ] == "base64" : 
~~~ payload = decode_base64 ( payload ) 
~~~ payload = str . encode ( payload ) 
~~ ~~ attachment [ "sha256" ] = hashlib . sha256 ( payload ) . hexdigest ( ) 
e . __str__ ( ) 
~~ ~~ ~~ if strip_attachment_payloads : 
~~~ del attachment [ "payload" ] 
~~ ~~ ~~ ~~ if "subject" not in parsed_email : 
~~~ parsed_email [ "subject" ] = None 
~~ parsed_email [ "filename_safe_subject" ] = get_filename_safe_string ( 
parsed_email [ "subject" ] ) 
if "body" not in parsed_email : 
~~~ parsed_email [ "body" ] = None 
~~ return parsed_email 
~~ def _str_to_list ( s ) : 
_list = s . split ( "," ) 
return list ( map ( lambda i : i . lstrip ( ) , _list ) ) 
~~ def cli_parse ( file_path , sa , nameservers , dns_timeout , parallel = False ) : 
~~~ file_results = parse_report_file ( file_path , 
~~ except ParserError as error : 
~~~ return error , file_path 
~~~ global counter 
with counter . get_lock ( ) : 
~~~ counter . value += 1 
~~ ~~ return file_results , file_path 
~~ def _main ( ) : 
def process_reports ( reports_ ) : 
~~~ output_str = "{0}\\n" . format ( json . dumps ( reports_ , 
ensure_ascii = False , 
if not opts . silent : 
~~~ print ( output_str ) 
~~ if opts . kafka_hosts : 
if opts . kafka_skip_certificate_verification : 
ssl_context = create_default_context ( ) 
ssl_context . check_hostname = False 
ssl_context . verify_mode = CERT_NONE 
~~ kafka_client = kafkaclient . KafkaClient ( 
opts . kafka_hosts , 
username = opts . kafka_username , 
password = opts . kafka_password , 
ssl_context = ssl_context 
~~ except Exception as error_ : 
~~ ~~ if opts . save_aggregate : 
~~~ for report in reports_ [ "aggregate_reports" ] : 
~~~ if opts . elasticsearch_hosts : 
~~~ elastic . save_aggregate_report_to_elasticsearch ( 
report , 
index_suffix = opts . elasticsearch_index_suffix , 
monthly_indexes = opts . elasticsearch_monthly_indexes ) 
~~ ~~ except elastic . AlreadySaved as warning : 
~~~ logger . warning ( warning . __str__ ( ) ) 
~~ except elastic . ElasticsearchError as error_ : 
error_ . __str__ ( ) ) ) 
~~~ if opts . kafka_hosts : 
~~~ kafka_client . save_aggregate_reports_to_kafka ( 
report , kafka_aggregate_topic ) 
~~ ~~ except Exception as error_ : 
~~ ~~ if opts . hec : 
~~~ aggregate_reports_ = reports_ [ "aggregate_reports" ] 
if len ( aggregate_reports_ ) > 0 : 
~~~ hec_client . save_aggregate_reports_to_splunk ( 
aggregate_reports_ ) 
~~ ~~ except splunk . SplunkError as e : 
~~ ~~ ~~ if opts . save_forensic : 
~~~ for report in reports_ [ "forensic_reports" ] : 
~~~ elastic . save_forensic_report_to_elasticsearch ( 
~~ except InvalidDMARCReport as error_ : 
~~~ logger . error ( error_ . __str__ ( ) ) 
~~~ kafka_client . save_forensic_reports_to_kafka ( 
report , kafka_forensic_topic ) 
~~~ forensic_reports_ = reports_ [ "forensic_reports" ] 
if len ( forensic_reports_ ) > 0 : 
~~~ hec_client . save_forensic_reports_to_splunk ( 
forensic_reports_ ) 
arg_parser . add_argument ( "-c" , "--config-file" , 
arg_parser . add_argument ( "file_path" , nargs = "*" , 
arg_parser . add_argument ( "--strip-attachment-payloads" , 
help = strip_attachment_help , action = "store_true" ) 
arg_parser . add_argument ( "-o" , "--output" , 
arg_parser . add_argument ( "-n" , "--nameservers" , nargs = "+" , 
arg_parser . add_argument ( "-t" , "--dns_timeout" , 
default = 6.0 ) 
arg_parser . add_argument ( "-s" , "--silent" , action = "store_true" , 
arg_parser . add_argument ( "--debug" , action = "store_true" , 
arg_parser . add_argument ( "--log-file" , default = None , 
arg_parser . add_argument ( "-v" , "--version" , action = "version" , 
version = __version__ ) 
aggregate_reports = [ ] 
args = arg_parser . parse_args ( ) 
opts = Namespace ( file_path = args . file_path , 
config_file = args . config_file , 
strip_attachment_payloads = args . strip_attachment_payloads , 
output = args . output , 
nameservers = args . nameservers , 
silent = args . silent , 
dns_timeout = args . dns_timeout , 
debug = args . debug , 
save_aggregate = False , 
save_forensic = False , 
imap_host = None , 
imap_skip_certificate_verification = False , 
imap_ssl = True , 
imap_port = 993 , 
imap_user = None , 
imap_password = None , 
imap_reports_folder = "INBOX" , 
imap_archive_folder = "Archive" , 
imap_watch = False , 
imap_delete = False , 
imap_test = False , 
hec = None , 
hec_token = None , 
hec_index = None , 
hec_skip_certificate_verification = False , 
elasticsearch_hosts = None , 
elasticsearch_index_suffix = None , 
elasticsearch_ssl = True , 
elasticsearch_ssl_cert_path = None , 
elasticsearch_monthly_indexes = False , 
kafka_hosts = None , 
kafka_username = None , 
kafka_password = None , 
kafka_aggregate_topic = None , 
kafka_forensic_topic = None , 
kafka_ssl = False , 
kafka_skip_certificate_verification = False , 
smtp_host = None , 
smtp_port = 25 , 
smtp_ssl = False , 
smtp_skip_certificate_verification = False , 
smtp_user = None , 
smtp_password = None , 
smtp_from = None , 
smtp_to = [ ] , 
log_file = args . log_file , 
n_procs = 1 , 
chunk_size = 1 
if args . config_file : 
~~~ abs_path = os . path . abspath ( args . config_file ) 
if not os . path . exists ( abs_path ) : 
exit ( - 1 ) 
~~ opts . silent = True 
config = ConfigParser ( ) 
config . read ( args . config_file ) 
if "general" in config . sections ( ) : 
~~~ general_config = config [ "general" ] 
if "strip_attachment_payloads" in general_config : 
~~~ opts . strip_attachment_payloads = general_config [ 
"strip_attachment_payloads" ] 
~~ if "output" in general_config : 
~~~ opts . output = general_config [ "output" ] 
~~ if "nameservers" in general_config : 
~~~ opts . nameservers = _str_to_list ( general_config [ "nameservers" ] ) 
~~ if "dns_timeout" in general_config : 
~~~ opts . dns_timeout = general_config . getfloat ( "dns_timeout" ) 
~~ if "save_aggregate" in general_config : 
~~~ opts . save_aggregate = general_config [ "save_aggregate" ] 
~~ if "save_forensic" in general_config : 
~~~ opts . save_forensic = general_config [ "save_forensic" ] 
~~ if "debug" in general_config : 
~~~ opts . debug = general_config . getboolean ( "debug" ) 
~~ if "silent" in general_config : 
~~~ opts . silent = general_config . getboolean ( "silent" ) 
~~ if "log_file" in general_config : 
~~~ opts . log_file = general_config [ "log_file" ] 
~~ if "n_procs" in general_config : 
~~~ opts . n_procs = general_config . getint ( "n_procs" ) 
~~ if "chunk_size" in general_config : 
~~~ opts . chunk_size = general_config . getint ( "chunk_size" ) 
~~ ~~ if "imap" in config . sections ( ) : 
~~~ imap_config = config [ "imap" ] 
if "host" in imap_config : 
~~~ opts . imap_host = imap_config [ "host" ] 
~~ if "port" in imap_config : 
~~~ opts . imap_port = imap_config [ "port" ] 
~~ if "ssl" in imap_config : 
~~~ opts . imap_ssl = imap_config . getboolean ( "ssl" ) 
~~ if "skip_certificate_verification" in imap_config : 
~~~ imap_verify = imap_config . getboolean ( 
"skip_certificate_verification" ) 
opts . imap_skip_certificate_verification = imap_verify 
~~ if "user" in imap_config : 
~~~ opts . imap_user = imap_config [ "user" ] 
~~ if "password" in imap_config : 
~~~ opts . imap_password = imap_config [ "password" ] 
~~ if "reports_folder" in imap_config : 
~~~ opts . imap_reports_folder = imap_config [ "reports_folder" ] 
~~ if "archive_folder" in imap_config : 
~~~ opts . imap_archive_folder = imap_config [ "archive_folder" ] 
~~ if "watch" in imap_config : 
~~~ opts . imap_watch = imap_config . getboolean ( "watch" ) 
~~ if "delete" in imap_config : 
~~~ opts . imap_delete = imap_config . getboolean ( "delete" ) 
~~ if "test" in imap_config : 
~~~ opts . imap_test = imap_config . getboolean ( "test" ) 
~~ ~~ if "elasticsearch" in config : 
~~~ elasticsearch_config = config [ "elasticsearch" ] 
if "hosts" in elasticsearch_config : 
~~~ opts . elasticsearch_hosts = _str_to_list ( elasticsearch_config [ 
"hosts" ] ) 
~~ if "index_suffix" in elasticsearch_config : 
~~~ opts . elasticsearch_index_suffix = elasticsearch_config [ 
"index_suffix" ] 
~~ if "monthly_indexes" in elasticsearch_config : 
~~~ monthly = elasticsearch_config . getboolean ( "monthly_indexes" ) 
opts . elasticsearch_monthly_indexes = monthly 
~~ if "ssl" in elasticsearch_config : 
~~~ opts . elasticsearch_ssl = elasticsearch_config . getboolean ( 
"ssl" ) 
~~ if "cert_path" in elasticsearch_config : 
~~~ opts . elasticsearch_ssl_cert_path = elasticsearch_config [ 
"cert_path" ] 
~~ ~~ if "splunk_hec" in config . sections ( ) : 
~~~ hec_config = config [ "splunk_hec" ] 
if "url" in hec_config : 
~~~ opts . hec = hec_config [ "url" ] 
~~ if "token" in hec_config : 
~~~ opts . hec_token = hec_config [ "token" ] 
~~ if "index" in hec_config : 
~~~ opts . hec_index = hec_config [ "index" ] 
~~ if "skip_certificate_verification" in hec_config : 
~~~ opts . hec_skip_certificate_verification = hec_config [ 
"skip_certificate_verification" ] 
~~ ~~ if "kafka" in config . sections ( ) : 
~~~ kafka_config = config [ "kafka" ] 
if "hosts" in kafka_config : 
~~~ opts . kafka_hosts = _str_to_list ( kafka_config [ "hosts" ] ) 
~~ if "user" in kafka_config : 
~~~ opts . kafka_username = kafka_config [ "user" ] 
~~ if "password" in kafka_config : 
~~~ opts . kafka_password = kafka_config [ "password" ] 
~~ if "ssl" in kafka_config : 
~~~ opts . kafka_ssl = kafka_config [ "ssl" ] . getboolean ( ) 
~~ if "skip_certificate_verification" in kafka_config : 
~~~ kafka_verify = kafka_config . getboolean ( 
opts . kafka_skip_certificate_verification = kafka_verify 
~~ if "aggregate_topic" in kafka_config : 
~~~ opts . kafka_aggregate = kafka_config [ "aggregate_topic" ] 
~~ if "forensic_topic" in kafka_config : 
~~~ opts . kafka_username = kafka_config [ "forensic_topic" ] 
~~ ~~ if "smtp" in config . sections ( ) : 
~~~ smtp_config = config [ "smtp" ] 
if "host" in smtp_config : 
~~~ opts . smtp_host = smtp_config [ "host" ] 
~~ if "port" in smtp_config : 
~~~ opts . smtp_port = smtp_config [ "port" ] 
~~ if "ssl" in smtp_config : 
~~~ opts . smtp_ssl = smtp_config . getboolean ( "ssl" ) 
~~ if "skip_certificate_verification" in smtp_config : 
~~~ smtp_verify = smtp_config . getboolean ( 
opts . smtp_skip_certificate_verification = smtp_verify 
~~ if "user" in smtp_config : 
~~~ opts . smtp_user = smtp_config [ "user" ] 
~~ if "password" in smtp_config : 
~~~ opts . smtp_password = smtp_config [ "password" ] 
~~ if "from" in smtp_config : 
~~~ opts . smtp_from = smtp_config [ "from" ] 
~~ if "to" in smtp_config : 
~~~ opts . smtp_to = _str_to_list ( smtp_config [ "to" ] ) 
~~ if "subject" in smtp_config : 
~~~ opts . smtp_subject = smtp_config [ "subject" ] 
~~ if "attachment" in smtp_config : 
~~~ opts . smtp_attachment = smtp_config [ "attachment" ] 
~~ if "message" in smtp_config : 
~~~ opts . smtp_message = smtp_config [ "message" ] 
~~ ~~ ~~ logging . basicConfig ( level = logging . WARNING ) 
logger . setLevel ( logging . WARNING ) 
if opts . debug : 
~~~ logging . basicConfig ( level = logging . DEBUG ) 
logger . setLevel ( logging . DEBUG ) 
~~ if opts . log_file : 
~~~ fh = logging . FileHandler ( opts . log_file ) 
formatter = logging . Formatter ( 
fh . setFormatter ( formatter ) 
logger . addHandler ( fh ) 
~~ if opts . imap_host is None and len ( opts . file_path ) == 0 : 
exit ( 1 ) 
~~ if opts . save_aggregate or opts . save_forensic : 
~~~ es_aggregate_index = "dmarc_aggregate" 
es_forensic_index = "dmarc_forensic" 
if opts . elasticsearch_index_suffix : 
~~~ suffix = opts . elasticsearch_index_suffix 
es_aggregate_index = "{0}_{1}" . format ( 
es_aggregate_index , suffix ) 
es_forensic_index = "{0}_{1}" . format ( 
es_forensic_index , suffix ) 
~~ elastic . set_hosts ( opts . elasticsearch_hosts , 
opts . elasticsearch_ssl , 
opts . elasticsearch_ssl_cert_path ) 
elastic . migrate_indexes ( aggregate_indexes = [ es_aggregate_index ] , 
forensic_indexes = [ es_forensic_index ] ) 
~~ ~~ except elastic . ElasticsearchError as error : 
~~~ if opts . hec_token is None or opts . hec_index is None : 
~~ verify = True 
if opts . hec_skip_certificate_verification : 
~~~ verify = False 
~~ hec_client = splunk . HECClient ( opts . hec , opts . hec_token , 
opts . hec_index , 
verify = verify ) 
~~ kafka_aggregate_topic = opts . kafka_aggregate_topic 
kafka_forensic_topic = opts . kafka_forensic_topic 
file_paths = [ ] 
for file_path in args . file_path : 
~~~ file_paths += glob ( file_path ) 
~~ file_paths = list ( set ( file_paths ) ) 
counter = Value ( 'i' , 0 ) 
pool = Pool ( opts . n_procs , initializer = init , initargs = ( counter , ) ) 
results = pool . starmap_async ( cli_parse , 
zip ( file_paths , 
repeat ( opts . strip_attachment_payloads ) , 
repeat ( opts . nameservers ) , 
repeat ( opts . dns_timeout ) , 
repeat ( opts . n_procs >= 1 ) ) , 
opts . chunk_size ) 
pbar = tqdm ( total = len ( file_paths ) ) 
while not results . ready ( ) : 
~~~ pbar . update ( counter . value - pbar . n ) 
time . sleep ( 0.1 ) 
~~ pbar . close ( ) 
results = results . get ( ) 
~~~ if type ( result [ 0 ] ) is InvalidDMARCReport : 
result [ 0 ] ) ) 
~~~ if result [ 0 ] [ "report_type" ] == "aggregate" : 
~~~ aggregate_reports . append ( result [ 0 ] [ "report" ] ) 
~~ elif result [ 0 ] [ "report_type" ] == "forensic" : 
~~~ forensic_reports . append ( result [ 0 ] [ "report" ] ) 
~~ ~~ ~~ if opts . imap_host : 
~~~ if opts . imap_user is None or opts . imap_password is None : 
~~ rf = opts . imap_reports_folder 
af = opts . imap_archive_folder 
ns = opts . nameservers 
sa = opts . strip_attachment_payloads 
ssl = True 
ssl_context = None 
if opts . imap_skip_certificate_verification : 
~~ if opts . imap_ssl is False : 
~~~ ssl = False 
~~ reports = get_dmarc_reports_from_inbox ( host = opts . imap_host , 
port = opts . imap_port , 
user = opts . imap_user , 
password = opts . imap_password , 
delete = opts . imap_delete , 
test = opts . imap_test , 
strip_attachment_payloads = sa 
aggregate_reports += reports [ "aggregate_reports" ] 
forensic_reports += reports [ "forensic_reports" ] 
~~ except IMAPError as error : 
~~ ~~ results = OrderedDict ( [ ( "aggregate_reports" , aggregate_reports ) , 
if opts . output : 
~~~ save_output ( results , output_directory = opts . output ) 
~~ process_reports ( results ) 
if opts . smtp_host : 
if opts . smtp_skip_certificate_verification : 
~~ email_results ( results , opts . smtp_host , opts . smtp_from , 
opts . smtp_to , ssl = opts . smtp_ssl , 
user = opts . smtp_user , 
password = opts . smtp_password , 
subject = opts . smtp_subject , 
ssl_context = ssl_context ) 
~~ except SMTPError as error : 
~~ ~~ if opts . imap_host and opts . imap_watch : 
~~~ sa = opts . strip_attachment_payloads 
watch_inbox ( opts . imap_host , opts . imap_user , opts . imap_password , 
process_reports , port = opts . imap_port , ssl = ssl , 
reports_folder = opts . imap_reports_folder , 
archive_folder = opts . imap_archive_folder , 
test = opts . imap_test , nameservers = opts . nameservers , 
dns_timeout = opts . dns_timeout , 
~~ ~~ ~~ def drain ( self , sid = None ) : 
if self . is_draining : 
~~ if self . is_closed : 
~~~ raise ErrConnectionClosed 
~~ if self . is_connecting or self . is_reconnecting : 
~~~ raise ErrConnectionReconnecting 
~~ if sid is not None : 
~~~ return self . _drain_sub ( sid ) 
~~ self . _status = Client . DRAINING_SUBS 
drain_tasks = [ ] 
for ssid , sub in self . _subs . items ( ) : 
~~~ task = self . _drain_sub ( ssid ) 
drain_tasks . append ( task ) 
~~ drain_is_done = asyncio . gather ( * drain_tasks ) 
~~~ yield from asyncio . wait_for ( drain_is_done , self . options [ "drain_timeout" ] ) 
~~ except asyncio . TimeoutError : 
~~~ drain_is_done . exception ( ) 
drain_is_done . cancel ( ) 
if self . _error_cb is not None : 
~~~ yield from self . _error_cb ( ErrDrainTimeout ) 
~~ ~~ except asyncio . CancelledError : 
~~~ self . _status = Client . DRAINING_PUBS 
yield from self . flush ( ) 
yield from self . _close ( Client . CLOSED ) 
~~ ~~ def publish ( self , subject , payload ) : 
if self . is_closed : 
~~ if self . is_draining_pubs : 
~~~ raise ErrConnectionDraining 
~~ payload_size = len ( payload ) 
if payload_size > self . _max_payload : 
~~~ raise ErrMaxPayload 
~~ yield from self . _publish ( subject , _EMPTY_ , payload , payload_size ) 
~~ def publish_request ( self , subject , reply , payload ) : 
~~ yield from self . _publish ( subject , reply . encode ( ) , payload , payload_size ) 
~~ def _publish ( self , subject , reply , payload , payload_size ) : 
if subject == "" : 
~~~ raise ErrBadSubject 
~~ payload_size_bytes = ( "%d" % payload_size ) . encode ( ) 
pub_cmd = b'' . join ( [ PUB_OP , _SPC_ , subject . encode ( 
) , _SPC_ , reply , _SPC_ , payload_size_bytes , _CRLF_ , payload , _CRLF_ ] ) 
self . stats [ 'out_msgs' ] += 1 
self . stats [ 'out_bytes' ] += payload_size 
yield from self . _send_command ( pub_cmd ) 
if self . _flush_queue . empty ( ) : 
~~~ yield from self . _flush_pending ( ) 
~~ ~~ def subscribe ( self , subject , 
queue = "" , 
cb = None , 
future = None , 
max_msgs = 0 , 
is_async = False , 
pending_msgs_limit = DEFAULT_SUB_PENDING_MSGS_LIMIT , 
pending_bytes_limit = DEFAULT_SUB_PENDING_BYTES_LIMIT , 
~~ if self . is_draining : 
~~ sub = Subscription ( subject = subject , 
queue = queue , 
max_msgs = max_msgs , 
is_async = is_async , 
if cb is not None : 
~~~ if asyncio . iscoroutinefunction ( cb ) : 
~~~ sub . coro = cb 
~~ elif sub . is_async : 
~~~ raise NatsError ( 
~~~ sub . cb = cb 
~~ sub . pending_msgs_limit = pending_msgs_limit 
sub . pending_bytes_limit = pending_bytes_limit 
sub . pending_queue = asyncio . Queue ( 
maxsize = pending_msgs_limit , 
loop = self . _loop , 
err_cb = self . _error_cb 
@ asyncio . coroutine 
def wait_for_msgs ( ) : 
~~~ nonlocal sub 
nonlocal err_cb 
~~~ msg = yield from sub . pending_queue . get ( ) 
sub . pending_size -= len ( msg . data ) 
~~~ if sub . coro is not None : 
~~~ if sub . is_async : 
~~~ self . _loop . create_task ( sub . coro ( msg ) ) 
~~~ yield from sub . coro ( msg ) 
~~ ~~ elif sub . cb is not None : 
~~~ self . _loop . call_soon ( sub . cb , msg ) 
~~ ~~ ~~ except asyncio . CancelledError : 
~~~ if err_cb is not None : 
~~~ yield from err_cb ( e ) 
~~ ~~ ~~ sub . wait_for_msgs_task = self . _loop . create_task ( 
wait_for_msgs ( ) ) 
~~ elif future is not None : 
~~~ sub . future = future 
~~ self . _ssid += 1 
ssid = self . _ssid 
self . _subs [ ssid ] = sub 
yield from self . _subscribe ( sub , ssid ) 
return ssid 
~~ def subscribe_async ( self , subject , ** kwargs ) : 
kwargs [ "is_async" ] = True 
sid = yield from self . subscribe ( subject , ** kwargs ) 
return sid 
~~ def unsubscribe ( self , ssid , max_msgs = 0 ) : 
~~ self . _remove_sub ( ssid , max_msgs ) 
if not self . is_reconnecting : 
~~~ yield from self . auto_unsubscribe ( ssid , max_msgs ) 
~~ ~~ def request ( self , subject , payload , timeout = 0.5 , expected = 1 , cb = None ) : 
if self . is_draining_pubs : 
~~ if cb is not None : 
~~~ next_inbox = INBOX_PREFIX [ : ] 
next_inbox . extend ( self . _nuid . next ( ) ) 
inbox = next_inbox . decode ( ) 
sid = yield from self . subscribe ( inbox , cb = cb ) 
yield from self . auto_unsubscribe ( sid , expected ) 
yield from self . publish_request ( subject , inbox , payload ) 
~~ if self . _resp_sub_prefix is None : 
~~~ self . _resp_map = { } 
self . _resp_sub_prefix = INBOX_PREFIX [ : ] 
self . _resp_sub_prefix . extend ( self . _nuid . next ( ) ) 
self . _resp_sub_prefix . extend ( b'.' ) 
resp_mux_subject = self . _resp_sub_prefix [ : ] 
resp_mux_subject . extend ( b'*' ) 
sub = Subscription ( subject = resp_mux_subject . decode ( ) ) 
sub . pending_msgs_limit = DEFAULT_SUB_PENDING_MSGS_LIMIT 
sub . pending_bytes_limit = DEFAULT_SUB_PENDING_BYTES_LIMIT 
maxsize = sub . pending_msgs_limit , 
token = msg . subject [ INBOX_PREFIX_LEN : ] 
~~~ fut = self . _resp_map [ token ] 
fut . set_result ( msg ) 
del self . _resp_map [ token ] 
~~ except ( asyncio . CancelledError , asyncio . InvalidStateError ) : 
~~~ del self . _resp_map [ token ] 
self . _ssid += 1 
~~ token = self . _nuid . next ( ) 
inbox = self . _resp_sub_prefix [ : ] 
inbox . extend ( token ) 
future = asyncio . Future ( loop = self . _loop ) 
self . _resp_map [ token . decode ( ) ] = future 
yield from self . publish_request ( subject , inbox . decode ( ) , payload ) 
~~~ msg = yield from asyncio . wait_for ( future , timeout , loop = self . _loop ) 
~~~ future . cancel ( ) 
raise ErrTimeout 
~~ ~~ def timed_request ( self , subject , payload , timeout = 0.5 ) : 
next_inbox = INBOX_PREFIX [ : ] 
sid = yield from self . subscribe ( inbox , future = future , max_msgs = 1 ) 
yield from self . auto_unsubscribe ( sid , 1 ) 
~~ ~~ def flush ( self , timeout = 60 ) : 
if timeout <= 0 : 
~~~ raise ErrBadTimeout 
~~ future = asyncio . Future ( loop = self . _loop ) 
~~~ yield from self . _send_ping ( future ) 
yield from asyncio . wait_for ( future , timeout , loop = self . _loop ) 
~~ ~~ def _select_next_server ( self ) : 
~~~ if len ( self . _server_pool ) == 0 : 
~~~ self . _current_server = None 
raise ErrNoServers 
~~ now = time . monotonic ( ) 
s = self . _server_pool . pop ( 0 ) 
if self . options [ "max_reconnect_attempts" ] > 0 : 
~~~ if s . reconnects > self . options [ "max_reconnect_attempts" ] : 
~~ ~~ self . _server_pool . append ( s ) 
if s . last_attempt is not None and now < s . last_attempt + self . options [ "reconnect_time_wait" ] : 
~~~ yield from asyncio . sleep ( self . options [ "reconnect_time_wait" ] , loop = self . _loop ) 
~~~ s . last_attempt = time . monotonic ( ) 
r , w = yield from asyncio . open_connection ( 
s . uri . hostname , 
s . uri . port , 
limit = DEFAULT_BUFFER_SIZE ) 
self . _current_server = s 
self . _bare_io_reader = self . _io_reader = r 
self . _bare_io_writer = self . _io_writer = w 
s . reconnects += 1 
self . _err = e 
~~~ yield from self . _error_cb ( e ) 
~~ ~~ ~~ def _process_err ( self , err_msg ) : 
if STALE_CONNECTION in err_msg : 
~~~ yield from self . _process_op_err ( ErrStaleConnection ) 
~~ if AUTHORIZATION_VIOLATION in err_msg : 
~~~ self . _err = ErrAuthorization 
self . _err = NatsError ( m . decode ( ) ) 
~~ do_cbs = False 
if not self . is_connecting : 
~~~ do_cbs = True 
~~ self . _loop . create_task ( self . _close ( Client . CLOSED , do_cbs ) ) 
~~ def _process_op_err ( self , e ) : 
if self . is_connecting or self . is_closed or self . is_reconnecting : 
~~ if self . options [ "allow_reconnect" ] and self . is_connected : 
~~~ self . _status = Client . RECONNECTING 
self . _ps . reset ( ) 
if self . _reconnection_task is not None and not self . _reconnection_task . cancelled ( ) : 
~~~ self . _reconnection_task . cancel ( ) 
~~ self . _reconnection_task = self . _loop . create_task ( self . _attempt_reconnect ( ) ) 
~~~ self . _process_disconnect ( ) 
yield from self . _close ( Client . CLOSED , True ) 
~~ ~~ def _connect_command ( self ) : 
options = { 
"verbose" : self . options [ "verbose" ] , 
"pedantic" : self . options [ "pedantic" ] , 
"lang" : __lang__ , 
"version" : __version__ , 
"protocol" : PROTOCOL 
if "auth_required" in self . _server_info : 
~~~ if self . _server_info [ "auth_required" ] : 
~~~ if self . options [ "user" ] is not None and self . options [ "password" ] is not None : 
~~~ options [ "user" ] = self . options [ "user" ] 
options [ "pass" ] = self . options [ "password" ] 
~~ elif self . options [ "token" ] is not None : 
~~~ options [ "auth_token" ] = self . options [ "token" ] 
~~ elif self . _current_server . uri . password is None : 
~~~ options [ "auth_token" ] = self . _current_server . uri . username 
~~~ options [ "user" ] = self . _current_server . uri . username 
options [ "pass" ] = self . _current_server . uri . password 
~~ ~~ ~~ if self . options [ "name" ] is not None : 
~~~ options [ "name" ] = self . options [ "name" ] 
~~ if self . options [ "no_echo" ] is not None : 
~~~ options [ "echo" ] = not self . options [ "no_echo" ] 
~~ connect_opts = json . dumps ( options , sort_keys = True ) 
return b'' . join ( [ CONNECT_OP + _SPC_ + connect_opts . encode ( ) + _CRLF_ ] ) 
~~ def _process_pong ( self ) : 
if len ( self . _pongs ) > 0 : 
~~~ future = self . _pongs . pop ( 0 ) 
future . set_result ( True ) 
self . _pongs_received += 1 
self . _pings_outstanding -= 1 
~~ ~~ def _process_msg ( self , sid , subject , reply , data ) : 
payload_size = len ( data ) 
self . stats [ 'in_msgs' ] += 1 
self . stats [ 'in_bytes' ] += payload_size 
sub = self . _subs . get ( sid ) 
if sub is None : 
~~ sub . received += 1 
if sub . max_msgs > 0 and sub . received >= sub . max_msgs : 
~~~ self . _subs . pop ( sid , None ) 
~~ msg = self . _build_message ( subject , reply , data ) 
if sub . future is not None : 
~~~ if sub . future . cancelled ( ) : 
~~ sub . future . set_result ( msg ) 
~~~ sub . pending_size += payload_size 
if sub . pending_size >= sub . pending_bytes_limit : 
~~~ sub . pending_size -= payload_size 
~~~ yield from self . _error_cb ( 
ErrSlowConsumer ( subject = subject , sid = sid ) ) 
~~ sub . pending_queue . put_nowait ( msg ) 
~~ except asyncio . QueueFull : 
~~~ if self . _error_cb is not None : 
~~ ~~ ~~ def _process_info ( self , info ) : 
if 'connect_urls' in info : 
~~~ if info [ 'connect_urls' ] : 
~~~ connect_urls = [ ] 
for connect_url in info [ 'connect_urls' ] : 
~~~ uri = urlparse ( "nats://%s" % connect_url ) 
srv = Srv ( uri ) 
srv . discovered = True 
should_add = True 
for s in self . _server_pool : 
~~~ if uri . netloc == s . uri . netloc : 
~~~ should_add = False 
~~ ~~ if should_add : 
~~~ connect_urls . append ( srv ) 
~~ ~~ if self . options [ "dont_randomize" ] is not True : 
~~~ shuffle ( connect_urls ) 
~~ for srv in connect_urls : 
~~~ self . _server_pool . append ( srv ) 
~~ ~~ ~~ ~~ def _process_connect_init ( self ) : 
self . _status = Client . CONNECTING 
connection_completed = self . _io_reader . readline ( ) 
info_line = yield from asyncio . wait_for ( connection_completed , self . options [ "connect_timeout" ] ) 
if INFO_OP not in info_line : 
~~ _ , info = info_line . split ( INFO_OP + _SPC_ , 1 ) 
~~~ srv_info = json . loads ( info . decode ( ) ) 
~~ self . _process_info ( srv_info ) 
self . _server_info = srv_info 
if 'max_payload' in self . _server_info : 
~~~ self . _max_payload = self . _server_info [ "max_payload" ] 
~~ if 'tls_required' in self . _server_info and self . _server_info [ 'tls_required' ] : 
if "tls" in self . options : 
~~~ ssl_context = self . options . get ( 'tls' ) 
~~ elif self . _current_server . uri . scheme == 'tls' : 
~~~ ssl_context = ssl . create_default_context ( ) 
~~ transport = self . _io_writer . transport 
sock = transport . get_extra_info ( 'socket' ) 
if not sock : 
self . _io_reader , self . _io_writer = yield from asyncio . open_connection ( 
limit = DEFAULT_BUFFER_SIZE , 
sock = sock , 
ssl = ssl_context , 
server_hostname = self . _current_server . uri . hostname , 
~~ if self . is_reconnecting : 
~~~ self . _ps . reset ( ) 
~~ connect_cmd = self . _connect_command ( ) 
self . _io_writer . write ( connect_cmd ) 
self . _io_writer . write ( PING_PROTO ) 
yield from self . _io_writer . drain ( ) 
next_op = yield from self . _io_reader . readline ( ) 
if self . options [ "verbose" ] and OK_OP in next_op : 
~~~ next_op = yield from self . _io_reader . readline ( ) 
~~ if ERR_OP in next_op : 
~~~ err_line = next_op . decode ( ) 
~~ if PONG_PROTO in next_op : 
~~~ self . _status = Client . CONNECTED 
~~ self . _reading_task = self . _loop . create_task ( self . _read_loop ( ) ) 
self . _pongs = [ ] 
self . _pings_outstanding = 0 
self . _ping_interval_task = self . _loop . create_task ( 
self . _ping_interval ( ) ) 
self . _flusher_task = self . _loop . create_task ( self . _flusher ( ) ) 
~~ def _flusher ( self ) : 
~~~ if not self . is_connected or self . is_connecting : 
~~~ yield from self . _flush_queue . get ( ) 
if self . _pending_data_size > 0 : 
~~~ self . _io_writer . writelines ( self . _pending [ : ] ) 
self . _pending = [ ] 
self . _pending_data_size = 0 
~~ ~~ except OSError as e : 
~~ yield from self . _process_op_err ( e ) 
~~ except asyncio . CancelledError : 
~~ ~~ ~~ def _read_loop ( self ) : 
~~~ should_bail = self . is_closed or self . is_reconnecting 
if should_bail or self . _io_reader is None : 
~~ if self . is_connected and self . _io_reader . at_eof ( ) : 
~~~ yield from self . _error_cb ( ErrStaleConnection ) 
~~ yield from self . _process_op_err ( ErrStaleConnection ) 
~~ b = yield from self . _io_reader . read ( DEFAULT_BUFFER_SIZE ) 
yield from self . _ps . parse ( b ) 
~~ except ErrProtocol : 
~~~ yield from self . _process_op_err ( ErrProtocol ) 
~~~ yield from self . _process_op_err ( e ) 
~~ ~~ ~~ async def get_bearer_info ( self ) : 
if self . client_id is None : 
~~~ raise SpotifyException ( _GET_BEARER_ERR % 'client_id' ) 
~~ elif self . client_secret is None : 
~~~ raise SpotifyException ( _GET_BEARER_ERR % 'client_secret' ) 
~~ token = b64encode ( ':' . join ( ( self . client_id , self . client_secret ) ) . encode ( ) ) 
'url' : 'https://accounts.spotify.com/api/token' , 
'data' : { 'grant_type' : 'client_credentials' } , 
async with self . _session . post ( ** kwargs ) as resp : 
~~~ return json . loads ( await resp . text ( encoding = 'utf-8' ) ) 
~~ ~~ async def request ( self , route , ** kwargs ) : 
if isinstance ( route , tuple ) : 
~~~ method , url = route 
~~~ method = route . method 
url = route . url 
~~ if self . bearer_info is None : 
~~~ self . bearer_info = bearer_info = await self . get_bearer_info ( ) 
access_token = bearer_info [ 'access_token' ] 
~~~ access_token = self . bearer_info [ 'access_token' ] 
~~ headers = { 
'Content-Type' : kwargs . get ( 'content_type' , 'application/json' ) , 
** kwargs . pop ( 'headers' , { } ) 
for _ in range ( self . RETRY_AMOUNT ) : 
~~~ r = await self . _session . request ( method , url , headers = headers , ** kwargs ) 
~~~ status = r . status 
~~~ data = json . loads ( await r . text ( encoding = 'utf-8' ) ) 
~~ except json . decoder . JSONDecodeError : 
~~~ data = { } 
~~ if 300 > status >= 200 : 
~~ if status == 401 : 
~~ if status == 429 : 
~~~ amount = r . headers . get ( 'Retry-After' ) 
await asyncio . sleep ( int ( amount ) , loop = self . loop ) 
~~ if status in ( 502 , 503 ) : 
~~ if status == 403 : 
~~~ raise Forbidden ( r , data ) 
~~ elif status == 404 : 
~~~ raise NotFound ( r , data ) 
~~ ~~ finally : 
~~~ await r . release ( ) 
~~~ raise HTTPException ( r , data ) 
~~ ~~ def album ( self , spotify_id , market = 'US' ) : 
route = Route ( 'GET' , '/albums/{spotify_id}' , spotify_id = spotify_id ) 
payload = { } 
if market : 
~~~ payload [ 'market' ] = market 
~~ return self . request ( route , params = payload ) 
~~ def album_tracks ( self , spotify_id , limit = 20 , offset = 0 , market = 'US' ) : 
route = Route ( 'GET' , '/albums/{spotify_id}/tracks' , spotify_id = spotify_id ) 
payload = { 'limit' : limit , 'offset' : offset } 
~~ def albums ( self , spotify_ids , market = 'US' ) : 
route = Route ( 'GET' , '/albums/' ) 
payload = { 'ids' : spotify_ids } 
~~ def artist ( self , spotify_id ) : 
route = Route ( 'GET' , '/artists/{spotify_id}' , spotify_id = spotify_id ) 
return self . request ( route ) 
~~ def artist_albums ( self , spotify_id , include_groups = None , limit = 20 , offset = 0 , market = 'US' ) : 
route = Route ( 'GET' , '/artists/{spotify_id}/albums' , spotify_id = spotify_id ) 
if include_groups : 
~~~ payload [ 'include_groups' ] = include_groups 
~~ if market : 
~~ def artist_top_tracks ( self , spotify_id , country ) : 
route = Route ( 'GET' , '/artists/{spotify_id}/top-tracks' , spotify_id = spotify_id ) 
payload = { 'country' : country } 
return self . request ( route , params = payload ) 
~~ def artist_related_artists ( self , spotify_id ) : 
route = Route ( 'GET' , '/artists/{spotify_id}/related-artists' , spotify_id = spotify_id ) 
~~ def artists ( self , spotify_ids ) : 
route = Route ( 'GET' , '/artists' ) 
~~ def category ( self , category_id , country = None , locale = None ) : 
route = Route ( 'GET' , '/browse/categories/{category_id}' , category_id = category_id ) 
if country : 
~~~ payload [ 'country' ] = country 
~~ if locale : 
~~~ payload [ 'locale' ] = locale 
~~ def category_playlists ( self , category_id , limit = 20 , offset = 0 , country = None ) : 
route = Route ( 'GET' , '/browse/categories/{category_id}/playlists' , category_id = category_id ) 
~~ def categories ( self , limit = 20 , offset = 0 , country = None , locale = None ) : 
route = Route ( 'GET' , '/browse/categories' ) 
~~ def featured_playlists ( self , locale = None , country = None , timestamp = None , limit = 20 , offset = 0 ) : 
route = Route ( 'GET' , '/browse/featured-playlists' ) 
~~~ payload [ 'timestamp' ] = timestamp 
~~ def new_releases ( self , * , country = None , limit = 20 , offset = 0 ) : 
route = Route ( 'GET' , '/browse/new-releases' ) 
~~ def recommendations ( self , seed_artists , seed_genres , seed_tracks , * , limit = 20 , market = None , ** filters ) : 
route = Route ( 'GET' , '/recommendations' ) 
payload = { 'seed_artists' : seed_artists , 'seed_genres' : seed_genres , 'seed_tracks' : seed_tracks , 'limit' : limit } 
~~~ payload . update ( filters ) 
~~ return self . request ( route , param = payload ) 
~~ def following_artists_or_users ( self , ids , * , type = 'artist' ) : 
route = Route ( 'GET' , '/me/following/contains' ) 
payload = { 'ids' : ids , 'type' : type } 
~~ async def get_albums ( self , * , limit : Optional [ int ] = 20 , offset : Optional [ int ] = 0 , include_groups = None , market : Optional [ str ] = None ) -> List [ Album ] : 
from . album import Album 
data = await self . __client . http . artist_albums ( self . id , limit = limit , offset = offset , include_groups = include_groups , market = market ) 
return list ( Album ( self . __client , item ) for item in data [ 'items' ] ) 
~~ async def get_all_albums ( self , * , market = 'US' ) -> List [ Album ] : 
albums = [ ] 
total = await self . total_albums ( market = market ) 
while len ( albums ) < total : 
~~~ data = await self . __client . http . artist_albums ( self . id , limit = 50 , offset = offset , market = market ) 
offset += 50 
albums += list ( Album ( self . __client , item ) for item in data [ 'items' ] ) 
~~ return albums 
~~ async def total_albums ( self , * , market : str = None ) -> int : 
data = await self . __client . http . artist_albums ( self . id , limit = 1 , offset = 0 , market = market ) 
return data [ 'total' ] 
~~ async def top_tracks ( self , country : str = 'US' ) -> List [ Track ] : 
from . track import Track 
top = await self . __client . http . artist_top_tracks ( self . id , country = country ) 
return list ( Track ( self . __client , item ) for item in top [ 'tracks' ] ) 
~~ async def related_artists ( self ) -> List [ Artist ] : 
related = await self . __client . http . artist_related_artists ( self . id ) 
return list ( Artist ( self . __client , item ) for item in related [ 'artists' ] ) 
~~ async def currently_playing ( self ) -> Tuple [ Context , Track ] : 
data = await self . http . currently_playing ( ) 
if data . get ( 'item' ) : 
~~~ data [ 'Context' ] = Context ( data . get ( 'context' ) ) 
data [ 'item' ] = Track ( self . __client , data . get ( 'item' ) ) 
~~ async def get_player ( self ) -> Player : 
self . _player = player = Player ( self . __client , self , await self . http . current_player ( ) ) 
return player 
~~ async def get_devices ( self ) -> List [ Device ] : 
data = await self . http . available_devices ( ) 
return [ Device ( item ) for item in data [ 'devices' ] ] 
~~ async def recently_played ( self ) -> List [ Dict [ str , Union [ Track , Context , str ] ] ] : 
data = await self . http . recently_played ( ) 
f = lambda data : { 'context' : Context ( data . get ( 'context' ) ) , 'track' : Track ( self . __client , data . get ( 'track' ) ) } 
return [ { 'timestamp' : track [ 'timestamp' ] , ** f ( track ) } for track in data [ 'items' ] ] 
~~ async def add_tracks ( self , playlist : Union [ str , Playlist ] , * tracks ) -> str : 
tracks = [ str ( track ) for track in tracks ] 
data = await self . http . add_playlist_tracks ( self . id , str ( playlist ) , tracks = ',' . join ( tracks ) ) 
return data [ 'snapshot_id' ] 
~~ async def replace_tracks ( self , playlist , * tracks ) -> str : 
await self . http . replace_playlist_tracks ( self . id , str ( playlist ) , tracks = ',' . join ( tracks ) ) 
~~ async def remove_tracks ( self , playlist , * tracks ) : 
data = await self . http . remove_playlist_tracks ( self . id , str ( playlist ) , tracks = ',' . join ( tracks ) ) 
~~ async def reorder_tracks ( self , playlist , start , insert_before , length = 1 , * , snapshot_id = None ) : 
data = await self . http . reorder_playlists_tracks ( self . id , str ( playlist ) , start , length , insert_before , snapshot_id = snapshot_id ) 
~~ async def edit_playlist ( self , playlist , * , name = None , public = None , collaborative = None , description = None ) : 
data = { } 
~~~ data [ 'name' ] = name 
~~ if public : 
~~~ data [ 'public' ] = public 
~~ if collaborative : 
~~~ data [ 'collaborative' ] = collaborative 
~~ if description : 
~~~ data [ 'description' ] = description 
~~ await self . http . change_playlist_details ( self . id , str ( playlist ) , data ) 
~~ async def create_playlist ( self , name , * , public = True , collaborative = False , description = None ) : 
'name' : name , 
'public' : public , 
'collaborative' : collaborative 
if description : 
~~ playlist_data = await self . http . create_playlist ( self . id , data ) 
return Playlist ( self . __client , playlist_data ) 
~~ async def get_playlists ( self , * , limit = 20 , offset = 0 ) : 
if hasattr ( self , 'http' ) : 
~~~ http = self . http 
~~~ http = self . __client . http 
~~ data = await http . get_playlists ( self . id , limit = limit , offset = offset ) 
return [ Playlist ( self . __client , playlist_data ) for playlist_data in data [ 'items' ] ] 
~~ async def get_tracks ( self , * , limit : Optional [ int ] = 20 , offset : Optional [ int ] = 0 ) -> List [ Track ] : 
data = await self . __client . http . album_tracks ( self . id , limit = limit , offset = offset ) 
return list ( Track ( self . __client , item ) for item in data [ 'items' ] ) 
~~ async def get_all_tracks ( self , * , market : Optional [ str ] = 'US' ) -> List [ Track ] : 
tracks = [ ] 
total = self . total_tracks or None 
~~~ data = await self . __client . http . album_tracks ( self . id , limit = 50 , offset = offset , market = market ) 
if total is None : 
~~~ total = data [ 'total' ] 
~~ offset += 50 
tracks += list ( Track ( self . __client , item ) for item in data [ 'items' ] ) 
if len ( tracks ) >= total : 
~~ ~~ return tracks 
~~ def oauth2_url ( self , redirect_uri : str , scope : Optional [ str ] = None , state : Optional [ str ] = None ) -> str : 
return OAuth2 . url_ ( self . http . client_id , redirect_uri , scope = scope , state = state ) 
~~ async def get_album ( self , spotify_id : str , * , market : str = 'US' ) -> Album : 
data = await self . http . album ( to_id ( spotify_id ) , market = market ) 
return Album ( self , data ) 
~~ async def get_artist ( self , spotify_id : str ) -> Artist : 
data = await self . http . artist ( to_id ( spotify_id ) ) 
return Artist ( self , data ) 
~~ async def get_track ( self , spotify_id : str ) -> Track : 
data = await self . http . track ( to_id ( spotify_id ) ) 
return Track ( self , data ) 
~~ async def get_user ( self , spotify_id : str ) -> User : 
data = await self . http . user ( to_id ( spotify_id ) ) 
return User ( self , data ) 
~~ async def get_albums ( self , * ids : List [ str ] , market : str = 'US' ) -> List [ Album ] : 
data = await self . http . albums ( ',' . join ( to_id ( _id ) for _id in ids ) , market = market ) 
return list ( Album ( self , album ) for album in data [ 'albums' ] ) 
~~ async def get_artists ( self , * ids : List [ str ] ) -> List [ Artist ] : 
data = await self . http . artists ( ',' . join ( to_id ( _id ) for _id in ids ) ) 
return list ( Artist ( self , artist ) for artist in data [ 'artists' ] ) 
~~ async def search ( self , q : str , * , types : Optional [ Iterable [ str ] ] = [ 'track' , 'playlist' , 'artist' , 'album' ] , limit : Optional [ int ] = 20 , offset : Optional [ int ] = 0 , market : Optional [ str ] = None ) -> Dict [ str , List [ Union [ Track , Playlist , Artist , Album ] ] ] : 
if not hasattr ( types , '__iter__' ) : 
~~ elif not isinstance ( types , list ) : 
~~~ types = list ( item for item in types ) 
~~ types_ = set ( types ) 
if not types_ . issubset ( _SEARCH_TYPES ) : 
~~~ raise ValueError ( _SEARCH_TYPE_ERR % types_ . difference ( _SEARCH_TYPES ) . pop ( ) ) 
'queary_type' : ',' . join ( tp . strip ( ) for tp in types ) , 
'market' : market , 
'limit' : limit , 
'offset' : offset 
data = await self . http . search ( ** kwargs ) 
return { key : [ _TYPES [ obj [ 'type' ] ] ( self , obj ) for obj in value [ 'items' ] ] for key , value in data . items ( ) } 
~~ async def contains_albums ( self , * albums : Sequence [ Union [ str , Album ] ] ) -> List [ bool ] : 
_albums = [ ( obj if isinstance ( obj , str ) else obj . id ) for obj in albums ] 
return await self . user . http . is_saved_album ( _albums ) 
~~ async def contains_tracks ( self , * tracks : Sequence [ Union [ str , Track ] ] ) -> List [ bool ] : 
_tracks = [ ( obj if isinstance ( obj , str ) else obj . id ) for obj in tracks ] 
return await self . user . http . is_saved_track ( _tracks ) 
~~ async def get_tracks ( self , * , limit = 20 , offset = 0 ) -> List [ Track ] : 
data = await self . user . http . saved_tracks ( limit = limit , offset = offset ) 
return [ Track ( self . __client , item [ 'track' ] ) for item in data [ 'items' ] ] 
~~ async def get_albums ( self , * , limit = 20 , offset = 0 ) -> List [ Album ] : 
data = await self . user . http . saved_albums ( limit = limit , offset = offset ) 
return [ Album ( self . __client , item [ 'album' ] ) for item in data [ 'items' ] ] 
~~ async def remove_albums ( self , * albums ) : 
await self . user . http . delete_saved_albums ( ',' . join ( _albums ) ) 
~~ async def remove_tracks ( self , * tracks ) : 
await self . user . http . delete_saved_tracks ( ',' . join ( _tracks ) ) 
~~ async def save_albums ( self , * albums ) : 
await self . user . http . save_albums ( ',' . join ( _albums ) ) 
~~ async def save_tracks ( self , * tracks ) : 
await self . user . http . save_tracks ( ',' . join ( _tracks ) ) 
~~ def to_id ( string : str ) -> str : 
string = string . strip ( ) 
match = _URI_RE . match ( string ) 
~~~ match = _OPEN_RE . match ( string ) 
~~~ return match . group ( 2 ) 
~~ ~~ def assert_hasattr ( attr : str , msg : str , tp : BaseException = SpotifyException ) -> Callable : 
def decorator ( func : Callable ) -> Callable : 
def decorated ( self , * args , ** kwargs ) : 
~~~ if not hasattr ( self , attr ) : 
~~~ raise tp ( msg ) 
~~ if inspect . iscoroutinefunction ( func ) : 
async def decorated ( * args , ** kwargs ) : 
~~~ return await decorated ( * args , ** kwargs ) 
~~ return decorator 
~~ def from_client ( cls , client , * args , ** kwargs ) : 
return cls ( client . http . client_id , * args , ** kwargs ) 
~~ def url_ ( client_id : str , redirect_uri : str , * , scope : str = None , state : str = None , secure : bool = True ) -> str : 
attrs = { 
'client_id' : client_id , 
'redirect_uri' : quote ( redirect_uri ) 
if scope is not None : 
~~~ attrs [ 'scope' ] = quote ( scope ) 
~~ if state is not None : 
~~~ attrs [ 'state' ] = state 
~~ parameters = '&' . join ( '{0}={1}' . format ( * item ) for item in attrs . items ( ) ) 
return OAuth2 . _BASE . format ( parameters = parameters ) 
~~ def attrs ( self ) : 
'client_id' : self . client_id , 
'redirect_uri' : quote ( self . redirect_uri ) , 
if self . scope is not None : 
~~~ data [ 'scope' ] = quote ( self . scope ) 
~~ if self . state is not None : 
~~~ data [ 'state' ] = self . state 
~~ def parameters ( self ) -> str : 
return '&' . join ( '{0}={1}' . format ( * item ) for item in self . attrs . items ( ) ) 
~~ async def build ( self ) : 
data = await self . __func ( ) 
return list ( PlaylistTrack ( self . __client , track ) for track in data [ 'items' ] ) 
~~ async def get_all_tracks ( self ) -> List [ PlaylistTrack ] : 
if isinstance ( self . _tracks , PartialTracks ) : 
~~~ return await self . _tracks . build ( ) 
~~ _tracks = [ ] 
while len ( self . tracks ) < self . total_tracks : 
~~~ data = await self . __client . http . get_playlist_tracks ( self . owner . id , self . id , limit = 50 , offset = offset ) 
_tracks += [ PlaylistTrack ( self . __client , item ) for item in data [ 'items' ] ] 
~~ self . total_tracks = len ( self . _tracks ) 
return list ( self . _tracks ) 
~~ async def pause ( self , * , device : Optional [ SomeDevice ] = None ) : 
await self . _user . http . pause_playback ( device_id = str ( device ) ) 
~~ async def resume ( self , * , device : Optional [ SomeDevice ] = None ) : 
await self . _user . http . play_playback ( None , device_id = str ( device ) ) 
~~ async def seek ( self , pos , * , device : Optional [ SomeDevice ] = None ) : 
await self . _user . http . seek_playback ( pos , device_id = str ( device ) ) 
~~ async def set_repeat ( self , state , * , device : Optional [ SomeDevice ] = None ) : 
await self . _user . http . repeat_playback ( state , device_id = str ( device ) ) 
~~ async def set_volume ( self , volume : int , * , device : Optional [ SomeDevice ] = None ) : 
await self . _user . http . set_playback_volume ( volume , device_id = str ( device ) ) 
~~ async def next ( self , * , device : Optional [ SomeDevice ] = None ) : 
await self . _user . http . skip_next ( device_id = str ( device ) ) 
~~ async def previous ( self , * , device : Optional [ SomeDevice ] = None ) : 
return await self . _user . http . skip_previous ( device_id = str ( device ) ) 
~~ async def play ( self , * uris : SomeURIs , offset : Optional [ Offset ] = 0 , device : Optional [ SomeDevice ] = None ) : 
if len ( uris ) > 1 : 
~~~ context_uri = list ( str ( uri ) for uri in uris ) 
~~~ context_uri = str ( uris [ 0 ] ) 
~~ if device is not None : 
~~~ if not isinstance ( device , ( Device , str ) ) : 
~~~ device = device . id 
~~ ~~ await self . _user . http . play_playback ( context_uri , offset = offset , device_id = device ) 
~~ async def shuffle ( self , state : Optional [ bool ] = None , * , device : Optional [ SomeDevice ] = None ) : 
await self . __user . http . shuffle_playback ( state ) 
~~ async def transfer ( self , device : SomeDevice , ensure_playback : bool = False ) : 
await self . _user . http . transfer_player ( str ( device ) , play = ensure_playback ) 
~~ async def from_href ( self ) : 
if not hasattr ( self , 'href' ) : 
~~ elif hasattr ( self , 'http' ) : 
~~~ return await self . http . request ( ( 'GET' , self . href ) ) 
~~~ cls = type ( self ) 
~~~ client = getattr ( self , '_{0}__client' . format ( cls . __name__ ) ) 
~~~ http = client . http 
~~ data = await http . request ( ( 'GET' , self . href ) ) 
return cls ( client , data ) 
domain_validation = self . checker . is_domain_valid ( ) 
ip_validation = self . checker . is_ip_valid ( ) 
if "current_test_data" in PyFunceble . INTERN : 
~~~ PyFunceble . INTERN [ "current_test_data" ] . update ( 
"domain_syntax_validation" : domain_validation , 
"ip4_syntax_validation" : ip_validation , 
~~ if ( 
domain_validation 
and not ip_validation 
or domain_validation 
or PyFunceble . CONFIGURATION [ "local" ] 
~~~ PyFunceble . INTERN . update ( 
{ "http_code" : HTTPCode ( ) . get ( ) , "referer" : Referer ( ) . get ( ) } 
if not PyFunceble . INTERN [ "referer" ] : 
~~~ return PyFunceble . INTERN [ "referer" ] 
~~ if PyFunceble . INTERN [ "referer" ] and not self . checker . is_subdomain ( ) : 
~~~ return self . _extract ( ) 
~~ Logs ( ) . whois ( self . whois_record ) 
ip_validation 
and not domain_validation 
or ip_validation 
~~~ PyFunceble . INTERN [ "http_code" ] = HTTPCode ( ) . get ( ) 
Logs ( ) . whois ( self . whois_record ) 
~~ def _convert_or_shorten_month ( cls , data ) : 
short_month = { 
"jan" : [ str ( 1 ) , "01" , "Jan" , "January" ] , 
"feb" : [ str ( 2 ) , "02" , "Feb" , "February" ] , 
"mar" : [ str ( 3 ) , "03" , "Mar" , "March" ] , 
"apr" : [ str ( 4 ) , "04" , "Apr" , "April" ] , 
"may" : [ str ( 5 ) , "05" , "May" ] , 
"jun" : [ str ( 6 ) , "06" , "Jun" , "June" ] , 
"jul" : [ str ( 7 ) , "07" , "Jul" , "July" ] , 
"aug" : [ str ( 8 ) , "08" , "Aug" , "August" ] , 
"sep" : [ str ( 9 ) , "09" , "Sep" , "September" ] , 
"oct" : [ str ( 10 ) , "Oct" , "October" ] , 
"nov" : [ str ( 11 ) , "Nov" , "November" ] , 
"dec" : [ str ( 12 ) , "Dec" , "December" ] , 
for month in short_month : 
~~~ if data in short_month [ month ] : 
~~~ return month 
~~ def _cases_management ( self , regex_number , matched_result ) : 
cases = { 
"first" : [ [ 1 , 2 , 3 , 10 , 11 , 22 , 26 , 27 , 28 , 29 , 32 , 34 , 38 ] , [ 0 , 1 , 2 ] ] , 
"second" : [ [ 14 , 15 , 31 , 33 , 36 , 37 ] , [ 1 , 0 , 2 ] ] , 
"third" : [ 
[ 4 , 5 , 6 , 7 , 8 , 9 , 12 , 13 , 16 , 17 , 18 , 19 , 20 , 21 , 23 , 24 , 25 , 30 , 35 ] , 
[ 2 , 1 , 0 ] , 
for case in cases : 
~~~ case_data = cases [ case ] 
if int ( regex_number ) in case_data [ 0 ] : 
self . _convert_1_to_2_digits ( matched_result [ case_data [ 1 ] [ 0 ] ] ) , 
self . _convert_or_shorten_month ( matched_result [ case_data [ 1 ] [ 1 ] ] ) , 
str ( matched_result [ case_data [ 1 ] [ 2 ] ] ) , 
~~ ~~ return matched_result 
~~ def _format ( self , date_to_convert = None ) : 
~~~ date_to_convert = self . expiration_date 
~~ regex_dates = { 
"1" : r"([0-9]{2})-([a-z]{3})-([0-9]{4})" , 
"2" : r"([0-9]{2})\\.([0-9]{2})\\.([0-9]{4})$" , 
"3" : r"([0-3][0-9])\\/(0[1-9]|1[012])\\/([0-9]{4})" , 
"4" : r"([0-9]{4})-([0-9]{2})-([0-9]{2})$" , 
"5" : r"([0-9]{4})\\.([0-9]{2})\\.([0-9]{2})$" , 
"6" : r"([0-9]{4})\\/([0-9]{2})\\/([0-9]{2})$" , 
"7" : r"([0-9]{4})\\.([0-9]{2})\\.([0-9]{2})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}" , 
"8" : r"([0-9]{4})([0-9]{2})([0-9]{2})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}" , 
"9" : r"([0-9]{4})-([0-9]{2})-([0-9]{2})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}" , 
"10" : r"([0-9]{2})\\.([0-9]{2})\\.([0-9]{4})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}" , 
"12" : r"([0-9]{4})\\/([0-9]{2})\\/([0-9]{2})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}\\s\\(.*\\)" , 
"13" : r"([0-9]{4})\\/([0-9]{2})\\/([0-9]{2})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}$" , 
"15" : r"[a-zA-Z]{3}\\s([a-zA-Z]{3})\\s([0-9]{2})\\s([0-9]{4})" , 
"16" : r"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}$" , 
"17" : r"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}[A-Z].*" , 
"18" : r"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}[+-][0-9]{4}" , 
"20" : r"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}\\.[0-9]{6}$" , 
"21" : r"([0-9]{4})-([0-9]{2})-([0-9]{2})T[0-9]{2}:[0-9]{2}:[0-9]{2}\\.[0-9].*[A-Z]" , 
"22" : r"([0-9]{2})-([0-9]{2})-([0-9]{4})" , 
"23" : r"([0-9]{4})\\.\\s([0-9]{2})\\.\\s([0-9]{2})\\." , 
"25" : r"(?=[0-9]{8})(?=([0-9]{4})([0-9]{2})([0-9]{2}))" , 
"26" : r"([0-9]{2})-([A-Z]{1}[a-z]{2})-([0-9]{4})$" , 
"27" : r"([0-9]{2})\\.([0-9]{1})\\.([0-9]{4})" , 
"28" : r"([0-9]{1,2})\\s([A-Z]{1}[a-z]{2})\\s([0-9]{4})" , 
"29" : r"([0-9]{2})-([A-Z]{1}[a-z]*)-([0-9]{4})" , 
"30" : r"([0-9]{4})-([A-Z]{1}[a-z]{2})-([0-9]{2})\\." , 
"32" : r"()[a-zA-Z]{3}\\s([a-zA-Z]{3})\\s([0-9]{4})\\s[0-9]{2}:[0-9]{2}:[0-9]{2}" , 
"33" : r"([A-Z]{1}[a-z]*)\\s([0-9]{1,2})\\s([0-9]{4})" , 
"34" : r"([0-9]{1,2})\\.([0-9]{1,2})\\.([0-9]{4})" , 
"35" : r"([0-9]{4})([0-9]{2})([0-9]{2})[0-9]+" , 
"36" : r"(0[1-9]|1[012])\\/([0-3][0-9])\\/([0-9]{4})" , 
"37" : r"([A-Z]{1}[a-z].*)\\s\\s([0-9]{1,2})\\s([0-9]{4})" , 
"38" : r"([0-9]{1,})[a-z]{1,}\\s([A-Z].*)\\s(2[0-9]{3})" , 
for regx in regex_dates : 
~~~ matched_result = Regex ( 
date_to_convert , regex_dates [ regx ] , return_data = True , rematch = True 
) . match ( ) 
if matched_result : 
~~~ date = self . _cases_management ( regx , matched_result ) 
if date : 
~~~ return "-" . join ( date ) 
~~ ~~ ~~ return "" 
expiration_date_from_database = Whois ( ) . get_expiration_date ( ) 
if expiration_date_from_database : 
~~~ Generate ( 
PyFunceble . STATUS [ "official" ] [ "up" ] , 
"WHOIS" , 
expiration_date_from_database , 
) . status_file ( ) 
return PyFunceble . STATUS [ "official" ] [ "up" ] 
~~ self . whois_record = Lookup ( ) . whois ( PyFunceble . INTERN [ "referer" ] ) 
to_match = [ 
r"expire:(.*)" , 
r"free-date(.*)" , 
r"expires:(.*)" , 
r"Expires:(.*)" , 
r"validity:(.*)" , 
r"domain_datebilleduntil:(.*)" , 
r"renewal:(.*)" , 
r"expires............:(.*)" , 
r"expire-date:(.*)" , 
r"Valid-date(.*)" , 
r"Expiration:.........(.*)" , 
r"Expired:(.*)" , 
if self . whois_record : 
~~~ if "current_test_data" in PyFunceble . INTERN : 
~~~ PyFunceble . INTERN [ "current_test_data" ] [ 
"whois_record" 
] = self . whois_record 
~~ for string in to_match : 
~~~ expiration_date = Regex ( 
self . whois_record , string , return_data = True , rematch = True , group = 0 
if expiration_date : 
~~~ self . expiration_date = expiration_date [ 0 ] . strip ( ) 
regex_rumbers = r"[0-9]" 
if Regex ( 
self . expiration_date , regex_rumbers , return_data = False 
) . match ( ) : 
~~~ self . expiration_date = self . _format ( ) 
self . expiration_date 
and not Regex ( 
self . expiration_date , 
r"[0-9]{2}\\-[a-z]{3}\\-2[0-9]{3}" , 
return_data = False , 
~~~ Logs ( ) . expiration_date ( self . expiration_date ) 
~~ if "current_test_data" in PyFunceble . INTERN : 
"expiration_date" 
] = self . expiration_date 
~~ Generate ( 
Whois ( expiration_date = self . expiration_date ) . add ( ) 
~~ def _update_code_urls ( self ) : 
to_ignore = [ ".gitignore" , ".keep" ] 
for root , _ , files in PyFunceble . walk ( 
PyFunceble . CURRENT_DIRECTORY 
+ PyFunceble . directory_separator 
+ "PyFunceble" 
~~~ if file not in to_ignore and "__pycache__" not in root : 
~~~ if root . endswith ( PyFunceble . directory_separator ) : 
~~~ self . _update_docs ( root + file ) 
~~~ self . _update_docs ( root + PyFunceble . directory_separator + file ) 
~~ ~~ ~~ ~~ for root , _ , files in PyFunceble . walk ( 
+ "tests" 
~~ ~~ ~~ ~~ ~~ def _is_version_greater ( self ) : 
checked = Version ( True ) . check_versions ( 
self . current_version [ 0 ] , self . version_yaml 
if checked is not None and not checked : 
~~ def is_dev_version ( cls ) : 
command_result = Command ( command ) . execute ( ) 
for branch in command_result . split ( "\\n" ) : 
~~~ if branch . startswith ( "*" ) and "dev" in branch : 
~~ def _does_require_deprecation ( self ) : 
for index , version_number in enumerate ( self . current_version [ 0 ] [ : 2 ] ) : 
~~~ if version_number > self . version_yaml [ index ] : 
~~ def _update_docs ( self , file_to_update ) : 
if self . is_dev_version ( ) : 
~~~ regexes = { 
"/%s/" % "dev" : r"\\/%s\\/" % "master" , 
"=%s" % "dev" : "=%s" % "master" , 
~~ elif self . is_master_version ( ) : 
"/%s/" % "master" : r"\\/%s\\/" % "dev" , 
"=%s" % "master" : "=%s" % "dev" , 
~~ to_update = File ( file_to_update ) . read ( ) 
for replacement , regex in regexes . items ( ) : 
~~~ to_update = Regex ( to_update , regex , replace_with = replacement ) . replace ( ) 
~~ File ( file_to_update ) . write ( to_update , overwrite = True ) 
~~ def _update_setup_py ( self ) : 
setup_py_path = PyFunceble . CURRENT_DIRECTORY + "setup.py" 
\ : r\ , 
~~ to_update = File ( setup_py_path ) . read ( ) 
~~ File ( setup_py_path ) . write ( to_update , overwrite = True ) 
~~ def _update_travis_yml ( self ) : 
travis_yml_path = PyFunceble . CURRENT_DIRECTORY + ".travis.yml" 
~~ to_update = File ( travis_yml_path ) . read ( ) 
~~ File ( travis_yml_path ) . write ( to_update , overwrite = True ) 
~~ def backup ( self ) : 
if PyFunceble . CONFIGURATION [ "auto_continue" ] : 
~~~ data_to_backup = { } 
configuration_counter = PyFunceble . INTERN [ "counter" ] [ "number" ] 
data_to_backup [ PyFunceble . INTERN [ "file_to_test" ] ] = { 
"tested" : configuration_counter [ "tested" ] , 
"up" : configuration_counter [ "up" ] , 
"down" : configuration_counter [ "down" ] , 
"invalid" : configuration_counter [ "invalid" ] , 
to_save = { } 
to_save . update ( self . backup_content ) 
to_save . update ( data_to_backup ) 
Dict ( to_save ) . to_json ( self . autocontinue_log_file ) 
~~ ~~ def restore ( self ) : 
if PyFunceble . CONFIGURATION [ "auto_continue" ] and self . backup_content : 
~~~ file_to_restore = PyFunceble . INTERN [ "file_to_test" ] 
if file_to_restore in self . backup_content : 
~~~ to_initiate = [ "up" , "down" , "invalid" , "tested" ] 
alternatives = { 
"up" : "number_of_up" , 
"down" : "number_of_down" , 
"invalid" : "number_of_invalid" , 
"tested" : "number_of_tested" , 
for string in to_initiate : 
~~~ PyFunceble . INTERN [ "counter" ] [ "number" ] . update ( 
{ string : self . backup_content [ file_to_restore ] [ string ] } 
string : self . backup_content [ file_to_restore ] [ 
alternatives [ string ] 
~~ ~~ ~~ ~~ ~~ def _is_to_ignore ( cls , line ) : 
for element in to_ignore : 
~~~ if Regex ( line , element , return_data = False ) . match ( ) : 
~~ def _handle_options ( self , options ) : 
regex_domain_option = r"domain=(.*)" 
for option in options : 
~~~ domains = Regex ( 
option , regex_domain_option , return_data = True , rematch = True , group = 0 
) . match ( ) [ - 1 ] 
if domains : 
~~~ result . extend ( 
x 
for x in domains . split ( "|" ) 
if x and not x . startswith ( "~" ) 
~~ ~~ ~~ except TypeError : 
~~ ~~ return result 
~~ def _extract_base ( self , element ) : 
if isinstance ( element , list ) : 
~~~ return [ self . _extract_base ( x ) for x in element ] 
~~ base = self . checker . is_url_valid ( url = element , return_base = True ) 
if base : 
~~~ return base 
~~ if "/" in element : 
~~~ return element . split ( "/" ) [ 0 ] 
~~ return element 
~~ def decode ( self ) : 
regex = r"^(?:.*\\|\\|)([^\\/\\$\\^]{1,}).*$" 
regex_v3 = ( 
r"(?:#+(?:[a-z]+?)?\\[[a-z]+(?:\\^|\\*)\\=(?:\\\ 
regex_v4 = r"^\\|(.*\\..*)\\|$" 
for line in self . to_format : 
~~~ rematch = rematch_v3 = rematch_v4 = None 
rematch = Regex ( 
line , regex , return_data = True , rematch = True , group = 0 
rematch_v4 = Regex ( 
line , regex_v4 , return_data = True , rematch = True , group = 0 
rematch_v3 = Regex ( 
line , regex_v3 , return_data = True , rematch = True , group = 0 
if rematch : 
~~~ if self . options_separator in line : 
~~~ options = line . split ( self . options_separator ) [ - 1 ] . split ( 
self . option_separator 
not options [ - 1 ] 
or "third-party" in options 
or "script" in options 
or "popup" in options 
or "xmlhttprequest" in options 
~~~ result . extend ( self . _extract_base ( rematch ) ) 
~~ extra = self . _handle_options ( options ) 
~~~ extra . extend ( self . _extract_base ( rematch ) ) 
result . extend ( self . _extract_base ( extra ) ) 
~~ elif extra : 
~~ ~~ if rematch_v4 : 
~~~ result . extend ( List ( self . _format_decoded ( rematch_v4 ) ) . format ( ) ) 
~~ if rematch_v3 : 
~~~ result . extend ( List ( self . _format_decoded ( rematch_v3 ) ) . format ( ) ) 
~~ ~~ return List ( result ) . format ( ) 
if not result : 
~~ for data in List ( to_format ) . format ( ) : 
~~~ if data : 
~~~ if "^" in data : 
~~~ return self . _format_decoded ( data . split ( "^" ) , result ) 
~~ if "#" in data : 
~~~ return self . _format_decoded ( data . split ( "#" ) , result ) 
~~ if "," in data : 
~~~ return self . _format_decoded ( data . split ( "," ) , result ) 
~~ if "!" in data : 
~~~ return self . _format_decoded ( data . split ( "!" ) , result ) 
~~ if "|" in data : 
~~~ return self . _format_decoded ( data . split ( "|" ) , result ) 
~~ if data : 
~~~ data = self . _extract_base ( data ) 
if data and ( 
self . checker . is_domain_valid ( data ) 
or self . checker . is_ip_valid ( data ) 
~~~ result . append ( data ) 
~~ elif data : 
~~~ url_base = self . checker . is_url_valid ( data , return_base = True ) 
if url_base : 
~~~ result . append ( url_base ) 
~~ ~~ ~~ ~~ ~~ return result 
~~~ if PyFunceble . INTERN [ "to_test_type" ] == "url" : 
~~~ req = PyFunceble . requests . head ( 
self . to_get , 
timeout = PyFunceble . CONFIGURATION [ "seconds_before_http_timeout" ] , 
headers = self . headers , 
verify = PyFunceble . CONFIGURATION [ "verify_ssl_certificate" ] , 
~~ return req . status_code 
~~ except ( 
PyFunceble . requests . exceptions . InvalidURL , 
PyFunceble . socket . timeout , 
PyFunceble . requests . exceptions . Timeout , 
PyFunceble . requests . ConnectionError , 
urllib3_exceptions . InvalidHeader , 
~~ ~~ def get ( self ) : 
if PyFunceble . HTTP_CODE [ "active" ] : 
~~~ http_code = self . _access ( ) 
list_of_valid_http_code = [ ] 
for codes in [ 
PyFunceble . HTTP_CODE [ "list" ] [ "up" ] , 
PyFunceble . HTTP_CODE [ "list" ] [ "potentially_down" ] , 
PyFunceble . HTTP_CODE [ "list" ] [ "potentially_up" ] , 
~~~ list_of_valid_http_code . extend ( codes ) 
~~ if http_code not in list_of_valid_http_code or http_code is None : 
~~~ return "*" * 3 
~~ return http_code 
if domain and isinstance ( domain , str ) : 
~~~ load_config ( True ) 
return Check ( domain ) . is_domain_valid ( ) 
return Check ( domain ) . is_subdomain ( ) 
if ip and isinstance ( ip , str ) : 
return Check ( ip ) . is_ip_valid ( ) 
return Check ( ip ) . is_ip_range ( ) 
if url and isinstance ( url , str ) : 
return Check ( url ) . is_url_valid ( ) 
if "config_loaded" not in INTERN : 
~~~ Load ( CURRENT_DIRECTORY ) 
if not under_test : 
~~~ DirectoryStructure ( ) 
~~ INTERN . update ( { "config_loaded" : True } ) 
if custom and isinstance ( custom , dict ) : 
~~~ CONFIGURATION . update ( custom ) 
random = int ( choice ( str ( int ( time ( ) ) ) ) ) 
if not CONFIGURATION [ "quiet" ] and random % 3 == 0 : 
print ( 
Fore . YELLOW 
+ Style . BRIGHT 
+ Fore . CYAN 
+ "Twitter" 
+ Fore . YELLOW 
+ "#PyFunceble" 
+ "!" 
Fore . GREEN 
+ "GitHub" 
if __name__ == "PyFunceble" : 
~~~ initiate ( autoreset = True ) 
load_config ( True ) 
~~~ PARSER = argparse . ArgumentParser ( 
Fore . RED + "♥" + Fore . RESET , 
Style . BRIGHT 
+ Style . RESET_ALL 
+ Fore . GREEN 
+ "https://pyfunceble.rtfd.io/en/master/special-thanks.html" , 
add_help = False , 
CURRENT_VALUE_FORMAT = ( 
PARSER . add_argument ( 
"-ad" , 
"--adblock" , 
action = "store_true" , 
CURRENT_VALUE_FORMAT 
+ repr ( CONFIGURATION [ "adblock" ] ) 
"-a" , 
"--all" , 
action = "store_false" , 
+ repr ( CONFIGURATION [ "less" ] ) 
"" "-c" , 
"--auto-continue" , 
"--continue" , 
+ repr ( CONFIGURATION [ "auto_continue" ] ) 
"--autosave-minutes" , 
type = int , 
+ repr ( CONFIGURATION [ "travis_autosave_minutes" ] ) 
"--clean-all" , 
"--cmd" , 
type = str , 
+ repr ( CONFIGURATION [ "command_before_end" ] ) 
"--cmd-before-end" , 
"--commit-autosave-message" , 
+ repr ( CONFIGURATION [ "travis_autosave_commit" ] ) 
"--commit-results-message" , 
+ repr ( CONFIGURATION [ "travis_autosave_final_commit" ] ) 
"-db" , 
"--database" , 
+ repr ( CONFIGURATION [ "inactive_database" ] ) 
"-dbr" , 
"--days-between-db-retest" , 
+ repr ( CONFIGURATION [ "days_between_db_retest" ] ) 
"--debug" , 
+ repr ( CONFIGURATION [ "debug" ] ) 
"--directory-structure" , 
"-ex" , 
"--execution" , 
+ repr ( CONFIGURATION [ "show_execution_time" ] ) 
"-f" , 
"--file" , 
"--help" , 
action = "help" , 
default = argparse . SUPPRESS , 
"--hierarchical" , 
+ repr ( CONFIGURATION [ "hierarchical_sorting" ] ) 
"-h" , 
"--host" , 
+ repr ( CONFIGURATION [ "generate_hosts" ] ) 
"--http" , 
+ repr ( HTTP_CODE [ "active" ] ) 
"--iana" , 
"--idna" , 
+ repr ( CONFIGURATION [ "idna_conversion" ] ) 
"-ip" , 
+ repr ( CONFIGURATION [ "custom_ip" ] ) 
"--json" , 
+ repr ( CONFIGURATION [ "generate_json" ] ) 
"--less" , 
+ repr ( Core . switch ( "less" ) ) 
"--local" , 
+ repr ( Core . switch ( "local" ) ) 
"-m" , 
"--mining" , 
+ repr ( CONFIGURATION [ "mining" ] ) 
"-n" , 
"--no-files" , 
+ repr ( CONFIGURATION [ "no_files" ] ) 
"-nl" , 
"--no-logs" , 
+ repr ( not CONFIGURATION [ "logs" ] ) 
"-ns" , 
"--no-special" , 
+ repr ( CONFIGURATION [ "no_special" ] ) 
"-nu" , 
"--no-unified" , 
+ repr ( CONFIGURATION [ "unified" ] ) 
"-nw" , 
"--no-whois" , 
+ repr ( CONFIGURATION [ "no_whois" ] ) 
"-p" , 
"--percentage" , 
+ repr ( CONFIGURATION [ "show_percentage" ] ) 
"--plain" , 
+ repr ( CONFIGURATION [ "plain_list_domain" ] ) 
"--production" , 
"-psl" , 
"--public-suffix" , 
"-q" , 
"--quiet" , 
+ repr ( CONFIGURATION [ "quiet" ] ) 
"--share-logs" , 
+ repr ( CONFIGURATION [ "share_logs" ] ) 
"-s" , 
"--simple" , 
+ repr ( CONFIGURATION [ "simple" ] ) 
"--split" , 
"--syntax" , 
+ repr ( CONFIGURATION [ "syntax" ] ) 
"-t" , 
"--timeout" , 
default = 3 , 
+ repr ( CONFIGURATION [ "seconds_before_http_timeout" ] ) 
"--travis" , 
+ repr ( CONFIGURATION [ "travis" ] ) 
"--travis-branch" , 
default = "master" , 
+ repr ( CONFIGURATION [ "travis_branch" ] ) 
"-uf" , 
"--url-file" , 
"-ua" , 
"--user-agent" , 
"-v" , 
"--version" , 
action = "version" , 
"-vsc" , 
"--verify-ssl-certificate" , 
+ repr ( CONFIGURATION [ "verify_ssl_certificate" ] ) 
"-wdb" , 
"--whois-database" , 
+ repr ( CONFIGURATION [ "whois_database" ] ) 
ARGS = PARSER . parse_args ( ) 
if ARGS . less : 
~~~ CONFIGURATION . update ( { "less" : ARGS . less } ) 
~~ elif not ARGS . all : 
~~~ CONFIGURATION . update ( { "less" : ARGS . all } ) 
~~ if ARGS . adblock : 
~~~ CONFIGURATION . update ( { "adblock" : Core . switch ( "adblock" ) } ) 
~~ if ARGS . auto_continue : 
~~~ CONFIGURATION . update ( 
{ "auto_continue" : Core . switch ( "auto_continue" ) } 
~~ if ARGS . autosave_minutes : 
{ "travis_autosave_minutes" : ARGS . autosave_minutes } 
~~ if ARGS . clean : 
~~~ Clean ( None ) 
~~ if ARGS . clean_all : 
~~~ Clean ( None , ARGS . clean_all ) 
~~ if ARGS . cmd : 
~~~ CONFIGURATION . update ( { "command" : ARGS . cmd } ) 
~~ if ARGS . cmd_before_end : 
~~~ CONFIGURATION . update ( { "command_before_end" : ARGS . cmd_before_end } ) 
~~ if ARGS . commit_autosave_message : 
{ "travis_autosave_commit" : ARGS . commit_autosave_message } 
~~ if ARGS . commit_results_message : 
{ "travis_autosave_final_commit" : ARGS . commit_results_message } 
~~ if ARGS . database : 
{ "inactive_database" : Core . switch ( "inactive_database" ) } 
~~ if ARGS . days_between_db_retest : 
{ "days_between_db_retest" : ARGS . days_between_db_retest } 
~~ if ARGS . debug : 
~~~ CONFIGURATION . update ( { "debug" : Core . switch ( "debug" ) } ) 
~~ if ARGS . directory_structure : 
~~ if ARGS . execution : 
{ "show_execution_time" : Core . switch ( "show_execution_time" ) } 
~~ if ARGS . filter : 
~~~ CONFIGURATION . update ( { "filter" : ARGS . filter } ) 
~~ if ARGS . hierarchical : 
{ "hierarchical_sorting" : Core . switch ( "hierarchical_sorting" ) } 
~~ if ARGS . host : 
{ "generate_hosts" : Core . switch ( "generate_hosts" ) } 
~~ if ARGS . http : 
~~~ HTTP_CODE . update ( { "active" : Core . switch ( HTTP_CODE [ "active" ] , True ) } ) 
~~ if ARGS . iana : 
~~~ IANA ( ) . update ( ) 
~~ if ARGS . idna : 
{ "idna_conversion" : Core . switch ( "idna_conversion" ) } 
~~ if ARGS . ip : 
~~~ CONFIGURATION . update ( { "custom_ip" : ARGS . ip } ) 
~~ if ARGS . json : 
{ "generate_json" : Core . switch ( "generate_json" ) } 
~~ if ARGS . local : 
~~~ CONFIGURATION . update ( { "local" : Core . switch ( "local" ) } ) 
~~ if ARGS . mining : 
~~~ CONFIGURATION . update ( { "mining" : Core . switch ( "mining" ) } ) 
~~ if ARGS . no_files : 
~~~ CONFIGURATION . update ( { "no_files" : Core . switch ( "no_files" ) } ) 
~~ if ARGS . no_logs : 
~~~ CONFIGURATION . update ( { "logs" : Core . switch ( "logs" ) } ) 
~~ if ARGS . no_special : 
~~~ CONFIGURATION . update ( { "no_special" : Core . switch ( "no_special" ) } ) 
~~ if ARGS . no_unified : 
~~~ CONFIGURATION . update ( { "unified" : Core . switch ( "unified" ) } ) 
~~ if ARGS . no_whois : 
~~~ CONFIGURATION . update ( { "no_whois" : Core . switch ( "no_whois" ) } ) 
~~ if ARGS . percentage : 
{ "show_percentage" : Core . switch ( "show_percentage" ) } 
~~ if ARGS . plain : 
{ "plain_list_domain" : Core . switch ( "plain_list_domain" ) } 
~~ if ARGS . production : 
~~~ Production ( ) 
~~ if ARGS . public_suffix : 
~~~ PublicSuffix ( ) . update ( ) 
~~ if ARGS . quiet : 
~~~ CONFIGURATION . update ( { "quiet" : Core . switch ( "quiet" ) } ) 
~~ if ARGS . share_logs : 
~~~ CONFIGURATION . update ( { "share_logs" : Core . switch ( "share_logs" ) } ) 
~~ if ARGS . simple : 
{ "simple" : Core . switch ( "simple" ) , "quiet" : Core . switch ( "quiet" ) } 
~~ if ARGS . split : 
~~~ CONFIGURATION . update ( { "split" : Core . switch ( "split" ) } ) 
~~ if ARGS . syntax : 
~~~ CONFIGURATION . update ( { "syntax" : Core . switch ( "syntax" ) } ) 
~~ if ARGS . timeout and ARGS . timeout % 3 == 0 : 
~~~ CONFIGURATION . update ( { "seconds_before_http_timeout" : ARGS . timeout } ) 
~~ if ARGS . travis : 
~~~ CONFIGURATION . update ( { "travis" : Core . switch ( "travis" ) } ) 
~~ if ARGS . travis_branch : 
~~~ CONFIGURATION . update ( { "travis_branch" : ARGS . travis_branch } ) 
~~ if ARGS . user_agent : 
~~~ CONFIGURATION . update ( { "user_agent" : ARGS . user_agent } ) 
~~ if ARGS . verify_ssl_certificate : 
{ "verify_ssl_certificate" : ARGS . verify_ssl_certificate } 
~~ if ARGS . whois_database : 
{ "whois_database" : Core . switch ( "whois_database" ) } 
~~ if not CONFIGURATION [ "quiet" ] : 
~~~ Core . colorify_logo ( home = True ) 
~~ Version ( ) . compare ( ) 
Core ( 
domain_or_ip_to_test = ARGS . domain , 
file_path = ARGS . file , 
url_to_test = ARGS . url , 
url_file = ARGS . url_file , 
link_to_test = ARGS . link , 
~~~ if not Version ( True ) . is_cloned ( ) : 
~~~ Merge ( CURRENT_DIRECTORY ) 
~~ ~~ ~~ except KeyboardInterrupt : 
~~~ stay_safe ( ) 
~~ ~~ ~~ def _entry_management_url_download ( self , passed ) : 
if passed and self . checker . is_url_valid ( passed ) : 
~~~ file_to_test = passed . split ( "/" ) [ - 1 ] 
not PyFunceble . path . isfile ( file_to_test ) 
or PyFunceble . INTERN [ "counter" ] [ "number" ] [ "tested" ] == 0 
~~~ Download ( passed , file_to_test ) . text ( ) 
~~ PyFunceble . INTERN [ "file_to_test" ] = file_to_test 
~~ def _entry_management_url ( self ) : 
and not self . _entry_management_url_download ( 
~~~ PyFunceble . INTERN [ 
"file_to_test" 
] = self . url_file 
self . _entry_management_url ( ) 
AutoSave ( ) . travis_permissions ( ) 
self . bypass ( ) 
ExecutionTime ( "start" ) 
if PyFunceble . CONFIGURATION [ "syntax" ] : 
~~~ PyFunceble . HTTP_CODE [ "active" ] = False 
~~~ PyFunceble . INTERN [ "to_test_type" ] = "domain" 
PyFunceble . CONFIGURATION [ "show_percentage" ] = False 
PyFunceble . CONFIGURATION [ "whois_database" ] = False 
if PyFunceble . CONFIGURATION [ "idna_conversion" ] : 
~~~ domain_or_ip_to_test = domain2idna ( 
~~~ domain_or_ip_to_test = ( 
~~ self . domain ( domain_or_ip_to_test ) 
~~~ PyFunceble . INTERN [ "to_test_type" ] = "url" 
self . url ( 
self . checker . is_url_valid ( 
return_formatted = True , 
~~ elif ( 
self . _entry_management_url_download ( 
~~~ PyFunceble . CONFIGURATION [ "no_whois" ] = PyFunceble . CONFIGURATION [ 
"plain_list_domain" 
] = PyFunceble . CONFIGURATION [ "split" ] = True 
PyFunceble . CONFIGURATION [ "generate_hosts" ] = False 
PyFunceble . INTERN [ "to_test_type" ] = "url" 
self . file_url ( ) 
or self . _entry_management_url_download ( 
self . file ( ) 
~~~ ExecutionTime ( "stop" , last = True ) 
self . percentage . log ( ) 
self . colorify_logo ( ) 
~~ PyFunceble . stay_safe ( ) 
~~~ PyFunceble . CONFIGURATION [ "simple" ] = PyFunceble . CONFIGURATION [ 
"quiet" 
] = PyFunceble . CONFIGURATION [ "no_files" ] = True 
PyFunceble . CONFIGURATION [ "whois_database" ] = PyFunceble . CONFIGURATION [ 
"inactive_database" 
] = PyFunceble . CONFIGURATION [ "auto_continue" ] = PyFunceble . CONFIGURATION [ 
"show_execution_time" 
] = False 
PyFunceble . INTERN [ 
"to_test" 
] = self . url_to_test 
~~ ~~ ~~ def bypass ( cls ) : 
regex_bypass = r"\\[PyFunceble\\sskip\\]" 
PyFunceble . CONFIGURATION [ "travis" ] 
and Regex ( 
~~~ AutoSave ( True , is_bypass = True ) 
~~ ~~ def _print_header ( cls ) : 
not PyFunceble . CONFIGURATION [ "quiet" ] 
and not PyFunceble . CONFIGURATION [ "header_printed" ] 
~~~ print ( "\\n" ) 
if PyFunceble . CONFIGURATION [ "less" ] : 
~~~ Prints ( None , "Less" ) . header ( ) 
~~~ Prints ( None , "Generic" ) . header ( ) 
~~ PyFunceble . CONFIGURATION [ "header_printed" ] = True 
~~ ~~ def _file_decision ( self , current , last , status = None ) : 
status 
and not PyFunceble . CONFIGURATION [ "simple" ] 
and PyFunceble . INTERN [ "file_to_test" ] 
~~~ self . mining . process ( ) 
self . mining . remove ( ) 
status . lower ( ) in PyFunceble . STATUS [ "list" ] [ "up" ] 
or status . lower ( ) in PyFunceble . STATUS [ "list" ] [ "valid" ] 
~~~ if self . inactive_database . is_present ( ) : 
~~~ Generate ( PyFunceble . STATUS [ "official" ] [ "up" ] ) . analytic_file ( 
"suspicious" 
self . inactive_database . remove ( ) 
~~~ self . inactive_database . add ( ) 
~~ self . auto_continue . backup ( ) 
if current != last : 
~~~ AutoSave ( ) 
self . reset_counters ( ) 
self . auto_continue . backup ( ) 
AutoSave ( True ) 
~~ ~~ for index in [ "http_code" , "referer" ] : 
~~~ if index in PyFunceble . INTERN : 
~~~ PyFunceble . INTERN [ index ] = "" 
~~ ~~ ~~ def domain ( self , domain = None , last_domain = None ) : 
self . _print_header ( ) 
if domain : 
~~~ PyFunceble . INTERN [ "to_test" ] = self . _format_domain ( domain ) 
~~~ PyFunceble . INTERN [ "to_test" ] = None 
~~ if PyFunceble . INTERN [ "to_test" ] : 
~~~ if PyFunceble . CONFIGURATION [ "syntax" ] : 
~~~ status = self . syntax_status . get ( ) 
~~~ status , _ = self . status . get ( ) 
~~ self . _file_decision ( PyFunceble . INTERN [ "to_test" ] , last_domain , status ) 
if PyFunceble . CONFIGURATION [ "simple" ] : 
~~~ print ( PyFunceble . INTERN [ "to_test" ] , status ) 
~~ return PyFunceble . INTERN [ "to_test" ] , status 
~~ def url ( self , url_to_test = None , last_url = None ) : 
if url_to_test : 
~~~ PyFunceble . INTERN [ "to_test" ] = url_to_test 
~~~ status = self . url_status . get ( ) 
~~ self . _file_decision ( PyFunceble . INTERN [ "to_test" ] , last_url , status ) 
~~ def colorify_logo ( cls , home = False ) : 
if not PyFunceble . CONFIGURATION [ "quiet" ] : 
~~~ to_print = [ ] 
if home : 
~~~ for line in PyFunceble . ASCII_PYFUNCEBLE . split ( "\\n" ) : 
~~~ to_print . append ( 
PyFunceble . Fore . YELLOW + line + PyFunceble . Fore . RESET 
~~ ~~ elif PyFunceble . INTERN [ "counter" ] [ "percentage" ] [ "up" ] >= 50 : 
PyFunceble . Fore . GREEN + line + PyFunceble . Fore . RESET 
~~~ to_print . append ( PyFunceble . Fore . RED + line + PyFunceble . Fore . RESET ) 
~~ ~~ print ( "\\n" . join ( to_print ) ) 
~~ ~~ def _format_domain ( cls , extracted_domain ) : 
if not extracted_domain . startswith ( "#" ) : 
~~~ if "#" in extracted_domain : 
~~~ extracted_domain = extracted_domain [ 
: extracted_domain . find ( "#" ) 
] . strip ( ) 
~~~ splited_line = extracted_domain . split ( ) 
index = 1 
while index < len ( splited_line ) : 
~~~ if splited_line [ index ] : 
~~ index += 1 
~~ return splited_line [ index ] 
~~ return extracted_domain 
~~ return "" 
~~ def _extract_domain_from_file ( cls ) : 
if PyFunceble . path . isfile ( PyFunceble . INTERN [ "file_to_test" ] ) : 
~~~ with open ( PyFunceble . INTERN [ "file_to_test" ] ) as file : 
~~~ for line in file : 
~~~ if not line . startswith ( "#" ) : 
~~~ result . append ( line . rstrip ( "\\n" ) . strip ( ) ) 
~~ ~~ ~~ ~~ except UnicodeDecodeError : 
~~~ with open ( PyFunceble . INTERN [ "file_to_test" ] , encoding = "utf-8" ) as file : 
~~ ~~ ~~ ~~ ~~ else : 
~~~ raise FileNotFoundError ( PyFunceble . INTERN [ "file_to_test" ] ) 
~~ def file ( self ) : 
list_to_test = self . _file_list_to_test_filtering ( ) 
~~~ list_to_test = domain2idna ( list_to_test ) 
if PyFunceble . CONFIGURATION [ "hierarchical_sorting" ] : 
~~~ list_to_test = List ( list_to_test ) . custom_format ( Sort . hierarchical ) 
~~~ list_to_test = List ( list_to_test ) . custom_format ( Sort . standard ) 
~~ ~~ not_filtered = list_to_test 
~~~ list_to_test = List ( 
list ( 
set ( 
list_to_test [ PyFunceble . INTERN [ "counter" ] [ "number" ] [ "tested" ] : ] 
- set ( PyFunceble . INTERN [ "flatten_inactive_db" ] ) 
) . format ( ) 
_ = list_to_test [ - 1 ] 
~~~ list_to_test = not_filtered [ 
PyFunceble . INTERN [ "counter" ] [ "number" ] [ "tested" ] : 
del not_filtered 
~~ if PyFunceble . CONFIGURATION [ "hierarchical_sorting" ] : 
~~~ list_to_test = List ( list ( list_to_test ) ) . custom_format ( Sort . hierarchical ) 
~~~ return [ self . domain ( x , list_to_test [ - 1 ] ) for x in list_to_test if x ] 
~~ ~~ def file_url ( self ) : 
not_filtered = list_to_test 
~~~ return [ self . url ( x , list_to_test [ - 1 ] ) for x in list_to_test if x ] 
~~ ~~ def switch ( 
cls , variable , custom = False 
if not custom : 
~~~ current_state = dict . get ( PyFunceble . CONFIGURATION , variable ) 
~~~ current_state = variable 
~~ if isinstance ( current_state , bool ) : 
~~~ if current_state : 
raise Exception ( 
to_print % ( repr ( variable ) , PyFunceble . LINKS [ "repo" ] + "/issues." ) 
~~ def get ( cls ) : 
if "to_test" in PyFunceble . INTERN and PyFunceble . INTERN [ "to_test" ] : 
~~~ expiration_date = ExpirationDate ( ) . get ( ) 
if expiration_date is False : 
~~~ return cls . handle ( status = "invalid" ) 
~~ if expiration_date == PyFunceble . STATUS [ "official" ] [ "up" ] : 
~~~ return expiration_date , "WHOIS" 
~~ return cls . handle ( status = "inactive" ) 
~~ def handle ( cls , status , invalid_source = "IANA" ) : 
if status . lower ( ) not in PyFunceble . STATUS [ "list" ] [ "invalid" ] : 
~~~ source = "NSLOOKUP" 
if Lookup ( ) . nslookup ( ) : 
~~~ status , source = cls . extra_rules . handle ( 
PyFunceble . STATUS [ "official" ] [ "up" ] , source 
Generate ( status , source ) . status_file ( ) 
return status , source 
~~ status , source = cls . extra_rules . handle ( 
PyFunceble . STATUS [ "official" ] [ "down" ] , source 
PyFunceble . STATUS [ "official" ] [ "invalid" ] , invalid_source 
~~ def handle ( self ) : 
source = "URL" 
if self . catched . lower ( ) not in PyFunceble . STATUS [ "list" ] [ "invalid" ] : 
~~~ Generate ( self . catched , source ) . status_file ( ) 
~~~ Generate ( self . catched , "SYNTAX" ) . status_file ( ) 
~~ return self . catched 
output_path = self . base + PyFunceble . OUTPUTS [ "parent_directory" ] 
result = { PyFunceble . OUTPUTS [ "parent_directory" ] : { } } 
for root , _ , files in PyFunceble . walk ( output_path ) : 
~~~ directories = Directory ( root . split ( output_path ) [ 1 ] ) . fix_path ( ) 
local_result = result [ PyFunceble . OUTPUTS [ "parent_directory" ] ] 
for file in files : 
~~~ file_path = root + PyFunceble . directory_separator + file 
file_hash = Hash ( file_path , "sha512" , True ) . get ( ) 
lines_in_list = [ line . rstrip ( "\\n" ) for line in open ( file_path ) ] 
formatted_content = "@@@" . join ( lines_in_list ) 
local_result = local_result . setdefault ( 
directories , 
{ file : { "sha512" : file_hash , "content" : formatted_content } } , 
~~ ~~ Dict ( result ) . to_json ( self . base + "dir_structure_production.json" ) 
~~ def _restore_replace ( self ) : 
if PyFunceble . path . isdir ( self . base + ".git" ) : 
~~ def _update_structure_from_config ( self , structure ) : 
to_replace_base_map = { "output/" : PyFunceble . OUTPUTS [ "parent_directory" ] } 
to_replace_map = { 
######################################################################### 
"HTTP_Analytic/" : PyFunceble . OUTPUTS [ "analytic" ] [ "directories" ] [ "parent" ] , 
"HTTP_Analytic/ACTIVE/" : PyFunceble . OUTPUTS [ "analytic" ] [ "directories" ] [ 
"parent" 
+ PyFunceble . OUTPUTS [ "analytic" ] [ "directories" ] [ "up" ] , 
"HTTP_Analytic/POTENTIALLY_ACTIVE/" : PyFunceble . OUTPUTS [ "analytic" ] [ 
"directories" 
] [ "parent" ] 
+ PyFunceble . OUTPUTS [ "analytic" ] [ "directories" ] [ "potentially_up" ] , 
"HTTP_Analytic/POTENTIALLY_INACTIVE/" : PyFunceble . OUTPUTS [ "analytic" ] [ 
+ PyFunceble . OUTPUTS [ "analytic" ] [ "directories" ] [ "potentially_down" ] , 
"Analytic/" : PyFunceble . OUTPUTS [ "analytic" ] [ "directories" ] [ "parent" ] , 
"Analytic/ACTIVE/" : PyFunceble . OUTPUTS [ "analytic" ] [ "directories" ] [ "parent" ] 
"Analytic/POTENTIALLY_ACTIVE/" : PyFunceble . OUTPUTS [ "analytic" ] [ 
"Analytic/POTENTIALLY_INACTIVE/" : PyFunceble . OUTPUTS [ "analytic" ] [ 
"Analytic/SUSPICIOUS/" : PyFunceble . OUTPUTS [ "analytic" ] [ "directories" ] [ 
+ PyFunceble . OUTPUTS [ "analytic" ] [ "directories" ] [ "suspicious" ] , 
"domains/" : PyFunceble . OUTPUTS [ "domains" ] [ "directory" ] , 
"domains/ACTIVE/" : PyFunceble . OUTPUTS [ "domains" ] [ "directory" ] 
+ PyFunceble . STATUS [ "official" ] [ "up" ] , 
"domains/INACTIVE/" : PyFunceble . OUTPUTS [ "domains" ] [ "directory" ] 
+ PyFunceble . STATUS [ "official" ] [ "down" ] , 
"domains/INVALID/" : PyFunceble . OUTPUTS [ "domains" ] [ "directory" ] 
+ PyFunceble . STATUS [ "official" ] [ "invalid" ] , 
"domains/VALID/" : PyFunceble . OUTPUTS [ "domains" ] [ "directory" ] 
+ PyFunceble . STATUS [ "official" ] [ "valid" ] , 
"hosts/" : PyFunceble . OUTPUTS [ "hosts" ] [ "directory" ] , 
"hosts/ACTIVE/" : PyFunceble . OUTPUTS [ "hosts" ] [ "directory" ] 
"hosts/INACTIVE/" : PyFunceble . OUTPUTS [ "hosts" ] [ "directory" ] 
"hosts/INVALID/" : PyFunceble . OUTPUTS [ "hosts" ] [ "directory" ] 
"hosts/VALID/" : PyFunceble . OUTPUTS [ "hosts" ] [ "directory" ] 
"json/" : PyFunceble . OUTPUTS [ "json" ] [ "directory" ] , 
"json/ACTIVE/" : PyFunceble . OUTPUTS [ "json" ] [ "directory" ] 
"json/INACTIVE/" : PyFunceble . OUTPUTS [ "json" ] [ "directory" ] 
"json/INVALID/" : PyFunceble . OUTPUTS [ "json" ] [ "directory" ] 
"json/VALID/" : PyFunceble . OUTPUTS [ "json" ] [ "directory" ] 
"logs/" : PyFunceble . OUTPUTS [ "logs" ] [ "directories" ] [ "parent" ] , 
"logs/percentage/" : PyFunceble . OUTPUTS [ "logs" ] [ "directories" ] [ "parent" ] 
+ PyFunceble . OUTPUTS [ "logs" ] [ "directories" ] [ "percentage" ] , 
"splited/" : PyFunceble . OUTPUTS [ "splited" ] [ "directory" ] , 
to_replace = { } 
for mapped , declared in to_replace_map . items ( ) : 
~~~ declared = Directory ( declared ) . fix_path ( ) 
to_replace . update ( { mapped : declared } ) 
~~ to_replace_base = { } 
for mapped , declared in to_replace_base_map . items ( ) : 
to_replace_base . update ( { mapped : declared } ) 
~~ structure = Dict ( structure ) . rename_key ( to_replace_base ) 
structure [ PyFunceble . OUTPUTS [ "parent_directory" ] ] = Dict ( 
structure [ PyFunceble . OUTPUTS [ "parent_directory" ] ] 
) . rename_key ( to_replace ) 
~~~ Dict ( structure ) . to_json ( self . structure ) 
~~ except FileNotFoundError : 
~~~ PyFunceble . mkdir ( 
PyFunceble . directory_separator . join ( 
self . structure . split ( PyFunceble . directory_separator ) [ : - 1 ] 
Dict ( structure ) . to_json ( self . structure ) 
~~ return structure 
~~ def _get_structure ( self ) : 
structure_file = "" 
req = "" 
if PyFunceble . path . isfile ( self . structure ) : 
~~~ structure_file = self . structure 
~~ elif PyFunceble . path . isfile ( self . base + "dir_structure_production.json" ) : 
~~~ structure_file = self . base + "dir_structure_production.json" 
~~~ if "dev" not in PyFunceble . VERSION : 
~~~ req = PyFunceble . requests . get ( 
PyFunceble . LINKS [ "dir_structure" ] . replace ( "dev" , "master" ) 
PyFunceble . LINKS [ "dir_structure" ] . replace ( "master" , "dev" ) 
~~ ~~ if structure_file . endswith ( "_production.json" ) : 
~~~ return self . _update_structure_from_config ( 
Dict ( ) . from_json ( File ( structure_file ) . read ( ) ) 
~~ if structure_file . endswith ( ".json" ) : 
~~ return self . _update_structure_from_config ( Dict ( ) . from_json ( req . text ) ) 
~~ def _create_directory ( cls , directory , loop = False ) : 
if not loop and PyFunceble . directory_separator in directory : 
~~~ splited_directory = directory . split ( PyFunceble . directory_separator ) 
full_path_to_create = "" 
for single_directory in splited_directory : 
~~~ full_path_to_create += single_directory + PyFunceble . directory_separator 
cls . _create_directory ( full_path_to_create , True ) 
~~ ~~ if not PyFunceble . path . isdir ( directory ) : 
~~~ AutoSave . travis_permissions ( ) 
PyFunceble . mkdir ( directory ) 
AutoSave . travis_permissions ( ) 
structure = self . _get_structure ( ) 
list_of_key = list ( structure . keys ( ) ) 
structure = structure [ list_of_key [ 0 ] ] 
parent_path = list_of_key [ 0 ] 
if not parent_path . endswith ( PyFunceble . directory_separator ) : 
~~~ parent_path += PyFunceble . directory_separator 
~~ replacement_status = self . _restore_replace ( ) 
for directory in structure : 
~~~ base = self . base + parent_path + directory 
if not base . endswith ( PyFunceble . directory_separator ) : 
~~~ base += PyFunceble . directory_separator 
~~ self . _create_directory ( base ) 
for file in structure [ directory ] : 
~~~ file_path = base + file 
content_to_write = structure [ directory ] [ file ] [ "content" ] 
online_sha = structure [ directory ] [ file ] [ "sha512" ] 
content_to_write = Regex ( 
content_to_write , "@@@" , escape = True , replace_with = "\\\\n" 
) . replace ( ) 
git_to_keep = file_path . replace ( "gitignore" , "keep" ) 
keep_to_git = file_path . replace ( "keep" , "gitignore" ) 
if replacement_status : 
PyFunceble . path . isfile ( file_path ) 
and Hash ( file_path , "sha512" , True ) . get ( ) == online_sha 
~~~ PyFunceble . rename ( file_path , git_to_keep ) 
write = False 
~~~ File ( file_path ) . delete ( ) 
file_path = git_to_keep 
write = True 
PyFunceble . path . isfile ( keep_to_git ) 
~~~ PyFunceble . rename ( file_path , keep_to_git ) 
~~~ File ( keep_to_git ) . delete ( ) 
file_path = keep_to_git 
~~ ~~ if write : 
~~~ File ( file_path ) . write ( content_to_write + "\\n" , True ) 
~~ ~~ ~~ self . delete_uneeded ( ) 
~~ def delete_uneeded ( self ) : 
~~ for root , _ , _ in PyFunceble . walk ( parent_path ) : 
~~~ root = Directory ( root ) . fix_path ( ) 
if root . replace ( parent_path , "" ) not in structure : 
~~~ PyFunceble . rmtree ( root ) 
~~ ~~ ~~ def _set_path_to_configs ( cls , path_to_config ) : 
if not path_to_config . endswith ( PyFunceble . directory_separator ) : 
~~~ default = parsed = path_to_config + PyFunceble . directory_separator 
~~~ default = parsed = path_to_config 
~~ parsed += PyFunceble . CONFIGURATION_FILENAME 
default += PyFunceble . DEFAULT_CONFIGURATION_FILENAME 
return ( parsed , default ) 
~~ def _load_config_file ( self ) : 
~~~ PyFunceble . CONFIGURATION . update ( 
Dict . from_yaml ( File ( self . path_to_config ) . read ( ) ) 
self . _install_iana_config ( ) 
self . _install_psl_config ( ) 
self . _install_directory_structure_file ( ) 
~~ except FileNotFoundError as exception : 
~~~ if PyFunceble . path . isfile ( self . path_to_default_config ) : 
~~~ File ( self . path_to_default_config ) . copy ( self . path_to_config ) 
self . _load_config_file ( ) 
~~~ raise exception 
~~ ~~ ~~ def _install_production_config ( self ) : 
production_config_link = Version ( True ) . right_url_from_version ( 
production_config_link 
if not Version ( True ) . is_cloned ( ) : 
~~~ Download ( production_config_link , self . path_to_default_config ) . text ( ) 
~~ return Download ( production_config_link , self . path_to_config ) . text ( ) 
~~ def _install_iana_config ( cls ) : 
iana_link = PyFunceble . CONFIGURATION [ "links" ] [ "iana" ] 
iana_link = Version ( True ) . right_url_from_version ( iana_link ) 
destination = PyFunceble . CURRENT_DIRECTORY + "iana-domains-db.json" 
if not Version ( True ) . is_cloned ( ) or not PyFunceble . path . isfile ( destination ) : 
~~~ return Download ( iana_link , destination ) . text ( ) 
~~ def _install_psl_config ( cls ) : 
psl_link = PyFunceble . CONFIGURATION [ "links" ] [ "psl" ] 
psl_link = Version ( True ) . right_url_from_version ( psl_link ) 
destination = ( 
+ PyFunceble . CONFIGURATION [ "outputs" ] [ "default_files" ] [ "public_suffix" ] 
~~~ return Download ( psl_link , destination ) . text ( ) 
~~ def _install_directory_structure_file ( cls ) : 
dir_structure_link = PyFunceble . CONFIGURATION [ "links" ] [ "dir_structure" ] 
dir_structure_link = Version ( True ) . right_url_from_version ( dir_structure_link ) 
+ PyFunceble . CONFIGURATION [ "outputs" ] [ "default_files" ] [ "dir_structure" ] 
~~~ data = Download ( dir_structure_link , destination , return_data = True ) . text ( ) 
File ( destination ) . write ( data , overwrite = True ) 
~~ def _merge_values ( self ) : 
to_remove = [ ] 
self . new_config = Dict ( 
Dict ( self . upstream_config ) . merge ( PyFunceble . CONFIGURATION ) 
) . remove_key ( to_remove ) 
~~ def _load ( self ) : 
if "PYFUNCEBLE_AUTO_CONFIGURATION" not in PyFunceble . environ : 
~~~ response = input ( 
PyFunceble . Style . BRIGHT 
+ PyFunceble . Fore . RED 
+ PyFunceble . Fore . RESET 
+ self . path_to_config 
+ PyFunceble . Style . RESET_ALL 
if isinstance ( response , str ) : 
~~~ if response . lower ( ) == "y" : 
~~~ self . _merge_values ( ) 
self . _save ( ) 
PyFunceble . Style . BRIGHT + PyFunceble . Fore . GREEN + "Done!\\n" 
~~ elif response . lower ( ) == "n" : 
~~ ~~ def split_versions ( cls , version , return_non_digits = False ) : 
splited_version = version . split ( "." ) 
digits = [ x for x in splited_version if x . isdigit ( ) ] 
if not return_non_digits : 
~~~ return digits 
~~ non_digits = [ x for x in splited_version if not x . isdigit ( ) ] 
return ( digits , non_digits [ 0 ] ) 
~~ def check_versions ( cls , local , upstream ) : 
status = [ None , None , None ] 
for index , version_number in enumerate ( local ) : 
~~~ if int ( version_number ) < int ( upstream [ index ] ) : 
~~~ status [ index ] = True 
~~ elif int ( version_number ) > int ( upstream [ index ] ) : 
~~~ status [ index ] = False 
~~ ~~ if False in status : 
~~ if True in status : 
~~ def compare ( self ) : 
if self . upstream_data [ "force_update" ] [ "status" ] : 
~~~ for minimal in self . upstream_data [ "force_update" ] [ "minimal_version" ] : 
~~~ checked = self . check_versions ( 
self . local_splited , self . split_versions ( minimal ) 
~~~ if checked or checked is not False and not checked : 
~~~ message = ( 
message += ( 
+ PyFunceble . Fore . GREEN 
print ( message ) 
~~ ~~ elif checked or checked is not False and not checked : 
~~ ~~ ~~ for version in self . upstream_data [ "deprecated" ] : 
self . local_splited , self . split_versions ( version ) 
and checked 
or checked is not False 
and not checked 
~~ if checked or checked is not False and not checked : 
~~ ~~ status = self . check_versions ( 
self . local_splited , 
self . split_versions ( self . upstream_data [ "current_version" ] ) , 
if status is not None and not status and not PyFunceble . CONFIGURATION [ "quiet" ] : 
+ PyFunceble . Fore . CYAN 
+ PyFunceble . VERSION 
+ "\\n" 
+ self . upstream_data [ "current_version" ] 
~~ elif status : 
~~~ if not PyFunceble . CONFIGURATION [ "quiet" ] : 
+ PyFunceble . Fore . YELLOW 
"current_version" 
~~ ~~ return 
~~ def is_cloned ( cls ) : 
if not PyFunceble . path . isdir ( ".git" ) : 
~~ list_of_file = [ 
".coveragerc" , 
".coveralls.yml" , 
".gitignore" , 
".PyFunceble_production.yaml" , 
".travis.yml" , 
"CODE_OF_CONDUCT.md" , 
"CONTRIBUTING.md" , 
"dir_structure_production.json" , 
"MANIFEST.in" , 
"README.rst" , 
"requirements.txt" , 
"setup.py" , 
"version.yaml" , 
list_of_dir = [ "docs" , "PyFunceble" , "tests" ] 
for file in list_of_file : 
~~~ if not PyFunceble . path . isfile ( file ) : 
~~ ~~ for directory in list_of_dir : 
~~~ if not PyFunceble . path . isdir ( directory ) : 
~~ def _handle_non_existant_index ( cls ) : 
~~~ PyFunceble . INTERN [ "http_code" ] 
~~~ PyFunceble . INTERN [ "http_code" ] = "*" * 3 
~~~ PyFunceble . INTERN [ "referer" ] 
~~~ PyFunceble . INTERN [ "referer" ] = "Unknown" 
~~ ~~ def _analytic_host_file_directory ( self ) : 
output_dir = ( 
self . output_parent_dir 
+ PyFunceble . OUTPUTS [ "analytic" ] [ "directories" ] [ "parent" ] 
if self . domain_status . lower ( ) in PyFunceble . STATUS [ "list" ] [ "potentially_up" ] : 
~~~ output_dir += PyFunceble . OUTPUTS [ "analytic" ] [ "directories" ] [ 
"potentially_up" 
self . domain_status . lower ( ) in PyFunceble . STATUS [ "list" ] [ "potentially_down" ] 
"potentially_down" 
~~ elif self . domain_status . lower ( ) in PyFunceble . STATUS [ "list" ] [ "suspicious" ] : 
~~~ output_dir += PyFunceble . OUTPUTS [ "analytic" ] [ "directories" ] [ "suspicious" ] 
~~~ output_dir += PyFunceble . OUTPUTS [ "analytic" ] [ "directories" ] [ "up" ] 
~~ return output_dir 
if self . _do_not_produce_file ( ) : 
"file_to_test" in PyFunceble . INTERN 
and ( 
PyFunceble . CONFIGURATION [ "generate_hosts" ] 
or PyFunceble . CONFIGURATION [ "plain_list_domain" ] 
or PyFunceble . CONFIGURATION [ "generate_json" ] 
~~~ splited_destination = "" 
http_list = [ ] 
http_list . extend ( PyFunceble . STATUS [ "list" ] [ "potentially_up" ] ) 
http_list . extend ( PyFunceble . STATUS [ "list" ] [ "potentially_down" ] ) 
http_list . extend ( PyFunceble . STATUS [ "list" ] [ "http_active" ] ) 
http_list . extend ( PyFunceble . STATUS [ "list" ] [ "suspicious" ] ) 
output_hosts = ( 
+ PyFunceble . OUTPUTS [ "hosts" ] [ "directory" ] 
+ "%s" 
+ directory_separator 
+ PyFunceble . OUTPUTS [ "hosts" ] [ "filename" ] 
output_domains = ( 
+ PyFunceble . OUTPUTS [ "domains" ] [ "directory" ] 
+ PyFunceble . OUTPUTS [ "domains" ] [ "filename" ] 
output_json = ( 
+ PyFunceble . OUTPUTS [ "json" ] [ "directory" ] 
+ PyFunceble . OUTPUTS [ "json" ] [ "filename" ] 
if self . domain_status . lower ( ) in PyFunceble . STATUS [ "list" ] [ "up" ] : 
~~~ hosts_destination = output_hosts % PyFunceble . STATUS [ "official" ] [ "up" ] 
plain_destination = output_domains % PyFunceble . STATUS [ "official" ] [ "up" ] 
json_destination = output_json % PyFunceble . STATUS [ "official" ] [ "up" ] 
~~ elif self . domain_status . lower ( ) in PyFunceble . STATUS [ "list" ] [ "valid" ] : 
~~~ hosts_destination = ( 
output_hosts % PyFunceble . STATUS [ "official" ] [ "valid" ] 
plain_destination = ( 
output_domains % PyFunceble . STATUS [ "official" ] [ "valid" ] 
json_destination = output_json % PyFunceble . STATUS [ "official" ] [ "valid" ] 
~~ elif self . domain_status . lower ( ) in PyFunceble . STATUS [ "list" ] [ "down" ] : 
~~~ hosts_destination = output_hosts % PyFunceble . STATUS [ "official" ] [ "down" ] 
output_domains % PyFunceble . STATUS [ "official" ] [ "down" ] 
json_destination = output_json % PyFunceble . STATUS [ "official" ] [ "down" ] 
~~ elif self . domain_status . lower ( ) in PyFunceble . STATUS [ "list" ] [ "invalid" ] : 
output_hosts % PyFunceble . STATUS [ "official" ] [ "invalid" ] 
output_domains % PyFunceble . STATUS [ "official" ] [ "invalid" ] 
json_destination = ( 
output_json % PyFunceble . STATUS [ "official" ] [ "invalid" ] 
~~ elif self . domain_status . lower ( ) in http_list : 
~~~ output_dir = self . _analytic_host_file_directory ( ) 
if not output_dir . endswith ( directory_separator ) : 
~~~ output_dir += directory_separator 
~~ hosts_destination = output_dir + PyFunceble . OUTPUTS [ "hosts" ] [ "filename" ] 
output_dir + PyFunceble . OUTPUTS [ "domains" ] [ "filename" ] 
json_destination = output_dir + PyFunceble . OUTPUTS [ "json" ] [ "filename" ] 
splited_destination = output_dir + str ( PyFunceble . INTERN [ "http_code" ] ) 
~~ if PyFunceble . CONFIGURATION [ "generate_hosts" ] : 
~~~ Prints ( 
[ PyFunceble . CONFIGURATION [ "custom_ip" ] , self . tested ] , 
"FullHosts" , 
hosts_destination , 
) . data ( ) 
~~ if PyFunceble . CONFIGURATION [ "plain_list_domain" ] : 
~~~ Prints ( [ self . tested ] , "PlainDomain" , plain_destination ) . data ( ) 
~~ if PyFunceble . CONFIGURATION [ "split" ] and splited_destination : 
~~~ Prints ( [ self . tested ] , "PlainDomain" , splited_destination ) . data ( ) 
~~ if PyFunceble . CONFIGURATION [ "generate_json" ] : 
~~~ Prints ( [ self . tested ] , "JSON" , json_destination ) . data ( ) 
~~ ~~ ~~ def unified_file ( self ) : 
and PyFunceble . CONFIGURATION [ "unified" ] 
~~~ output = ( 
self . output_parent_dir + PyFunceble . OUTPUTS [ "default_files" ] [ "results" ] 
~~~ if PyFunceble . HTTP_CODE [ "active" ] : 
~~~ to_print = [ 
self . tested , 
self . domain_status , 
PyFunceble . INTERN [ "http_code" ] , 
~~~ to_print = [ self . tested , self . domain_status , self . source ] 
~~ Prints ( to_print , "Less" , output , True ) . data ( ) 
self . source , 
PyFunceble . CURRENT_TIME , 
Prints ( to_print , "Generic_File" , output , True ) . data ( ) 
~~ ~~ ~~ def analytic_file ( self , new_status , old_status = None ) : 
if not old_status : 
~~~ old_status = self . domain_status 
~~ if "file_to_test" in PyFunceble . INTERN and PyFunceble . INTERN [ "file_to_test" ] : 
+ "%s%s" 
if new_status . lower ( ) in PyFunceble . STATUS [ "list" ] [ "up" ] : 
~~~ output = output % ( 
PyFunceble . OUTPUTS [ "analytic" ] [ "directories" ] [ "up" ] , 
PyFunceble . OUTPUTS [ "analytic" ] [ "filenames" ] [ "up" ] , 
Generate ( "HTTP_Active" ) . info_files ( ) 
~~ elif new_status . lower ( ) in PyFunceble . STATUS [ "list" ] [ "potentially_up" ] : 
PyFunceble . OUTPUTS [ "analytic" ] [ "directories" ] [ "potentially_up" ] , 
PyFunceble . OUTPUTS [ "analytic" ] [ "filenames" ] [ "potentially_up" ] , 
Generate ( "potentially_up" ) . info_files ( ) 
~~ elif new_status . lower ( ) in PyFunceble . STATUS [ "list" ] [ "suspicious" ] : 
PyFunceble . OUTPUTS [ "analytic" ] [ "directories" ] [ "suspicious" ] , 
PyFunceble . OUTPUTS [ "analytic" ] [ "filenames" ] [ "suspicious" ] , 
Generate ( "suspicious" ) . info_files ( ) 
PyFunceble . OUTPUTS [ "analytic" ] [ "directories" ] [ "potentially_down" ] , 
PyFunceble . OUTPUTS [ "analytic" ] [ "filenames" ] [ "potentially_down" ] , 
Generate ( "potentially_down" ) . info_files ( ) 
~~ Prints ( 
old_status , 
"HTTP" , 
output , 
True , 
if PyFunceble . INTERN [ "file_to_test" ] : 
+ PyFunceble . OUTPUTS [ "splited" ] [ "directory" ] 
+ self . domain_status 
[ self . tested , self . domain_status , self . source ] , "Less" , output , True 
~~ elif PyFunceble . CONFIGURATION [ "split" ] : 
~~~ if self . domain_status . lower ( ) in PyFunceble . STATUS [ "list" ] [ "up" ] : 
~~~ data_to_print = [ 
data_to_print , PyFunceble . STATUS [ "official" ] [ "up" ] , output , True 
~~~ data_to_print = [ self . tested , self . source , PyFunceble . CURRENT_TIME ] 
Prints ( 
data_to_print , 
PyFunceble . STATUS [ "official" ] [ "valid" ] , 
PyFunceble . INTERN [ "referer" ] , 
PyFunceble . STATUS [ "official" ] [ "down" ] , 
PyFunceble . STATUS [ "official" ] [ "invalid" ] , 
~~ ~~ ~~ ~~ def _prints_status_screen ( self ) : 
~~~ if PyFunceble . CONFIGURATION [ "less" ] : 
if not PyFunceble . HTTP_CODE [ "active" ] : 
~~~ to_print [ - 1 ] = self . source 
~~ Prints ( to_print , "Less" ) . data ( ) 
~~ Prints ( data_to_print , "Generic" ) . data ( ) 
if "file_to_test" in PyFunceble . INTERN : 
~~~ Generate ( self . domain_status , self . source , self . expiration_date ) . info_files ( ) 
Percentage ( self . domain_status ) . count ( ) 
self . _prints_status_screen ( ) 
not PyFunceble . CONFIGURATION [ "no_files" ] 
and PyFunceble . CONFIGURATION [ "split" ] 
~~~ self . _prints_status_file ( ) 
~~~ self . unified_file ( ) 
~~ ~~ ~~ def _do_not_produce_file ( self ) : 
Inactive ( ) . is_present ( ) 
and self . domain_status 
in [ 
and PyFunceble . INTERN [ "to_test" ] 
not in PyFunceble . INTERN [ "extracted_list_to_test" ] 
~~ def _extensions ( self , line ) : 
line = line . strip ( ) 
if not line . startswith ( "//" ) and "." in line : 
~~~ line = line . encode ( "idna" ) . decode ( "utf-8" ) 
if line . startswith ( "*." ) : 
~~~ line = line [ 2 : ] 
~~ extension = line . split ( "." ) [ - 1 ] 
if extension in self . public_suffix_db : 
~~~ self . public_suffix_db [ extension ] = List ( 
self . public_suffix_db [ extension ] + [ line ] 
~~~ self . public_suffix_db . update ( { extension : [ line ] } ) 
~~ ~~ ~~ def update ( self ) : 
~~ list ( map ( self . _extensions , self . _data ( ) . split ( "\\n" ) ) ) 
Dict ( self . public_suffix_db ) . to_json ( self . destination ) 
~~~ print ( PyFunceble . INTERN [ "done" ] ) 
~~ ~~ def load ( self ) : 
if not PyFunceble . INTERN [ "psl_db" ] : 
~~~ PyFunceble . INTERN [ "psl_db" ] = Dict ( ) . from_json ( 
File ( self . destination ) . read ( ) 
~~ ~~ def standard ( cls , element ) : 
Regex ( element , cls . regex_replace , replace_with = "@funilrys" ) 
. replace ( ) 
. replace ( "@funilrys" , "" ) 
~~ def hierarchical ( cls , element ) : 
to_sort = "" 
full_extension = "" 
element = element . lower ( ) 
url_base = Check ( ) . is_url_valid ( element , return_base = True ) 
if not isinstance ( url_base , str ) : 
~~~ if "." in element : 
~~~ extension_index = element . rindex ( "." ) + 1 
extension = element [ extension_index : ] 
if extension in PyFunceble . INTERN [ "psl_db" ] : 
~~~ for suffix in PyFunceble . INTERN [ "psl_db" ] [ extension ] : 
~~~ formatted_suffix = "." + suffix 
if element . endswith ( formatted_suffix ) : 
~~~ suffix_index = element . rindex ( formatted_suffix ) 
to_sort = element [ : suffix_index ] 
full_extension = suffix 
~~ ~~ ~~ if not full_extension : 
~~~ full_extension = element [ extension_index : ] 
to_sort = element [ : extension_index - 1 ] 
~~ full_extension += "." 
tros_ot = to_sort [ : : - 1 ] 
if "." in tros_ot : 
~~~ full_extension = ( 
tros_ot [ : tros_ot . index ( "." ) ] [ : : - 1 ] + "." + full_extension 
tros_ot = tros_ot [ tros_ot . index ( "." ) + 1 : ] 
reversion = full_extension + "." . join ( 
[ x [ : : - 1 ] for x in tros_ot . split ( "." ) ] 
Regex ( reversion , cls . regex_replace , replace_with = "@funilrys" ) 
~~ return ( 
Regex ( 
to_sort + full_extension , 
cls . regex_replace , 
replace_with = "@funilrys" , 
~~ protocol_position = element . rindex ( url_base ) 
protocol = element [ : protocol_position ] 
return protocol + cls . hierarchical ( url_base ) 
~~ def load ( self ) : 
if "iana_db" not in PyFunceble . INTERN or not PyFunceble . INTERN [ "iana_db" ] : 
~~~ PyFunceble . INTERN [ "iana_db" ] = self . iana_db 
~~ ~~ def _referer ( self , extension ) : 
iana_record = self . lookup . whois ( 
PyFunceble . CONFIGURATION [ "iana_whois_server" ] , "hello.%s" % extension 
if iana_record and "refer" in iana_record : 
~~~ regex_referer = r"(?s)refer\\:\\s+([a-zA-Z0-9._-]+)\\n" 
matched = Regex ( 
iana_record , regex_referer , return_data = True , group = 1 
if matched : 
~~~ return matched 
~~ ~~ if extension in self . manual_server : 
~~~ return self . manual_server [ extension ] 
~~ def _extensions ( self ) : 
upstream_lines = ( 
Download ( self . iana_url , return_data = True ) 
. text ( ) 
. split ( \ ) 
regex_valid_extension = r"(/domains/root/db/)(.*)(\\.html)" 
for block in upstream_lines : 
~~~ if "/domains/root/db/" in block : 
~~~ matched = Regex ( 
block , regex_valid_extension , return_data = True , rematch = True 
) . match ( ) [ 1 ] 
~~~ referer = self . _referer ( matched ) 
yield ( matched , referer ) 
~~ ~~ ~~ ~~ def update ( self ) : 
~~ for extension , referer in self . _extensions ( ) : 
~~~ if extension not in self . iana_db or self . iana_db [ extension ] != referer : 
~~~ self . iana_db [ extension ] = referer 
Dict ( self . iana_db ) . to_json ( self . destination ) 
~~ ~~ if not PyFunceble . CONFIGURATION [ "quiet" ] : 
if PyFunceble . CONFIGURATION [ "mining" ] : 
~~~ history = PyFunceble . requests . get ( 
) . history 
mined = { self . to_get_bare : [ ] } 
for element in history : 
~~~ element = element . url 
if PyFunceble . INTERN [ "to_test_type" ] == "url" : 
~~~ to_append = Check ( ) . is_url_valid ( element , return_base = False ) 
~~ elif PyFunceble . INTERN [ "to_test_type" ] == "domain" : 
~~~ to_append = Check ( ) . is_url_valid ( element , return_base = True ) 
~~ if to_append : 
~~~ if to_append . endswith ( ":80" ) : 
~~~ to_append = to_append [ : - 3 ] 
~~ if to_append != self . to_get_bare : 
~~~ mined [ self . to_get_bare ] . append ( to_append ) 
~~ ~~ ~~ if mined [ self . to_get_bare ] : 
~~~ return mined 
~~ def _retrieve ( self ) : 
~~~ if "mined" not in PyFunceble . INTERN : 
~~~ PyFunceble . INTERN [ "mined" ] = { } 
~~ if PyFunceble . path . isfile ( self . file ) : 
~~~ data = Dict ( ) . from_json ( File ( self . file ) . read ( ) ) 
for file_path in data : 
~~~ PyFunceble . INTERN [ "mined" ] [ file_path ] = { } 
for element in data [ file_path ] : 
~~~ if data [ file_path ] [ element ] : 
~~~ PyFunceble . INTERN [ "mined" ] [ file_path ] [ element ] = data [ 
file_path 
] [ element ] 
~~ ~~ PyFunceble . INTERN [ "mined" ] = { } 
~~ def _backup ( self ) : 
~~~ Dict ( PyFunceble . INTERN [ "mined" ] ) . to_json ( self . file ) 
~~ ~~ def _add ( self , to_add ) : 
~~~ if PyFunceble . INTERN [ "file_to_test" ] not in PyFunceble . INTERN [ "mined" ] : 
~~~ PyFunceble . INTERN [ "mined" ] [ PyFunceble . INTERN [ "file_to_test" ] ] = { } 
~~ for element in to_add : 
element 
in PyFunceble . INTERN [ "mined" ] [ PyFunceble . INTERN [ "file_to_test" ] ] 
~~~ PyFunceble . INTERN [ "mined" ] [ PyFunceble . INTERN [ "file_to_test" ] ] [ 
] . extend ( to_add [ element ] ) 
] = to_add [ element ] 
~~ PyFunceble . INTERN [ "mined" ] [ PyFunceble . INTERN [ "file_to_test" ] ] [ 
] = List ( 
PyFunceble . INTERN [ "mined" ] [ PyFunceble . INTERN [ "file_to_test" ] ] [ 
~~ self . _backup ( ) 
~~ ~~ def remove ( self ) : 
~~~ if PyFunceble . INTERN [ "file_to_test" ] in PyFunceble . INTERN [ "mined" ] : 
~~~ for element in PyFunceble . INTERN [ "mined" ] [ 
PyFunceble . INTERN [ "file_to_test" ] 
self . to_get_bare 
in PyFunceble . INTERN [ "mined" ] [ 
] . remove ( self . to_get_bare ) 
~~ ~~ self . _backup ( ) 
~~ ~~ ~~ def list_of_mined ( cls ) : 
~~ result = List ( result ) . format ( ) 
~~~ mined = self . mine ( ) 
if mined : 
~~~ self . _add ( mined ) 
self . _backup ( ) 
~~ ~~ ~~ def _get_content ( cls , file ) : 
if PyFunceble . path . isfile ( file ) : 
~~~ return Dict ( ) . from_json ( File ( file ) . read ( ) ) 
~~ return { } 
~~ def _write_content ( cls , content , file ) : 
if not PyFunceble . CONFIGURATION [ "no_files" ] : 
~~~ if not isinstance ( content , dict ) : 
~~~ content = { } 
~~ Dict ( content ) . to_json ( file ) 
~~ ~~ def whois ( self , record ) : 
if PyFunceble . CONFIGURATION [ "debug" ] and PyFunceble . CONFIGURATION [ "logs" ] : 
~~~ if PyFunceble . INTERN [ "referer" ] : 
~~~ referer = PyFunceble . INTERN [ "referer" ] 
~~~ referer = None 
~~ to_write = { 
self . current_time : { 
"domain" : PyFunceble . INTERN [ "to_test" ] , 
"record" : record , 
"referer" : referer , 
if self . output : 
~~~ output = self . output 
~~~ output = PyFunceble . OUTPUT_DIRECTORY 
output += PyFunceble . OUTPUTS [ "parent_directory" ] 
output += PyFunceble . OUTPUTS [ "logs" ] [ "directories" ] [ "parent" ] 
output += PyFunceble . OUTPUTS [ "logs" ] [ "filenames" ] [ "whois" ] 
~~ current_content = self . _get_content ( output ) 
current_content . update ( to_write ) 
self . _write_content ( current_content , output ) 
~~ ~~ def expiration_date ( self , extracted ) : 
if PyFunceble . CONFIGURATION [ "logs" ] : 
"expiration_date" : extracted , 
"whois_server" : referer , 
output += PyFunceble . OUTPUTS [ "logs" ] [ "filenames" ] [ "date_format" ] 
if PyFunceble . CONFIGURATION [ "share_logs" ] : 
~~~ PyFunceble . requests . post ( 
PyFunceble . LINKS [ "api_date_format" ] , 
data = to_write [ self . current_time ] , 
~~ ~~ ~~ def referer_not_found ( self , extension ) : 
~~~ to_write = { 
"extension" : extension , 
output += PyFunceble . OUTPUTS [ "logs" ] [ "filenames" ] [ "no_referer" ] 
PyFunceble . LINKS [ "api_no_referer" ] , data = to_write [ self . current_time ] 
~~ ~~ ~~ def _before_header ( self ) : 
and self . output 
and not PyFunceble . path . isfile ( self . output ) 
date_of_generation = ( 
authorized_templates = [ 
"Generic_File" , 
"Less" , 
if self . template in authorized_templates : 
~~~ header = ( 
self . _header_constructor ( self . currently_used_header , None ) [ 0 ] + "\\n" 
~~~ File ( self . output ) . write ( link + date_of_generation + header ) 
~~~ File ( self . output ) . write ( link + date_of_generation ) 
~~ ~~ ~~ def _header_constructor ( 
header_data = [ ] 
header_size = "" 
before_size = "%-" 
after_size = "s" 
if header_separator : 
~~~ header_separator_data = [ ] 
~~ length_data_to_print = len ( data_to_print ) - 1 
i = 0 
for data in data_to_print : 
~~~ size = data_to_print [ data ] 
header_data . append ( data ) 
header_size += before_size + str ( size ) + after_size 
if i < length_data_to_print : 
~~~ header_size += column_separator 
~~ if header_separator : 
~~~ header_separator_data . append ( header_separator * size ) 
~~ i += 1 
header_size % tuple ( header_data ) , 
header_size % tuple ( header_separator_data ) , 
~~ return [ header_size % tuple ( header_data ) ] 
~~ def header ( 
self , do_not_print = False 
not PyFunceble . CONFIGURATION [ "header_printed" ] 
or self . template == "Percentage" 
or do_not_print 
self . template . lower ( ) in PyFunceble . STATUS [ "list" ] [ "generic" ] 
or self . template == "Generic_File" 
~~~ to_print = self . headers [ "Generic" ] 
and PyFunceble . HTTP_CODE [ "active" ] 
~~ ~~ elif self . template . lower ( ) in PyFunceble . STATUS [ "list" ] [ "up" ] : 
~~~ to_print = self . headers [ PyFunceble . STATUS [ "official" ] [ "up" ] ] 
~~ elif self . template . lower ( ) in PyFunceble . STATUS [ "list" ] [ "valid" ] : 
~~~ to_print = self . headers [ PyFunceble . STATUS [ "official" ] [ "valid" ] ] 
~~ elif self . template . lower ( ) in PyFunceble . STATUS [ "list" ] [ "down" ] : 
~~~ to_print = self . headers [ PyFunceble . STATUS [ "official" ] [ "down" ] ] 
~~ elif self . template . lower ( ) in PyFunceble . STATUS [ "list" ] [ "invalid" ] : 
~~~ to_print = self . headers [ PyFunceble . STATUS [ "official" ] [ "invalid" ] ] 
self . template == "Less" 
or self . template == "HTTP" 
~~~ to_print = self . headers [ self . template ] 
if self . template == "Less" and not PyFunceble . HTTP_CODE [ "active" ] : 
~~~ to_print [ "Source" ] = 10 
~~ ~~ if not PyFunceble . HTTP_CODE [ "active" ] : 
~~ self . currently_used_header = to_print 
if not do_not_print : 
~~~ self . _before_header ( ) 
for formatted_template in self . _header_constructor ( to_print ) : 
~~~ if not self . only_on_file : 
~~~ print ( formatted_template ) 
~~ if not PyFunceble . CONFIGURATION [ "no_files" ] and self . output : 
~~~ File ( self . output ) . write ( formatted_template + "\\n" ) 
~~ ~~ ~~ ~~ ~~ def _data_constructor ( self , size ) : 
result = PyFunceble . OrderedDict ( ) 
if len ( self . data_to_print ) == len ( size ) : 
~~~ for i in range ( len ( self . data_to_print ) ) : 
~~~ result [ self . data_to_print [ i ] ] = size [ i ] 
~~ def _size_from_header ( cls , header ) : 
for data in header : 
~~~ result . append ( header [ data ] ) 
~~ def _colorify ( self , data ) : 
if self . template in [ "Generic" , "Less" ] : 
self . data_to_print [ 1 ] . lower ( ) in PyFunceble . STATUS [ "list" ] [ "up" ] 
or self . data_to_print [ 1 ] . lower ( ) in PyFunceble . STATUS [ "list" ] [ "valid" ] 
~~~ data = PyFunceble . Fore . BLACK + PyFunceble . Back . GREEN + data 
~~ elif self . data_to_print [ 1 ] . lower ( ) in PyFunceble . STATUS [ "list" ] [ "down" ] : 
~~~ data = PyFunceble . Fore . BLACK + PyFunceble . Back . RED + data 
~~~ data = PyFunceble . Fore . BLACK + PyFunceble . Back . CYAN + data 
~~~ if PyFunceble . path . isfile ( self . output ) : 
~~~ content = Dict ( ) . from_json ( File ( self . output ) . read ( ) ) 
if isinstance ( content , list ) : 
~~~ content . extend ( self . data_to_print ) 
content = List ( content ) . custom_format ( Sort . standard ) 
~~~ content = List ( content ) . custom_format ( Sort . hierarchical ) 
~~ Dict ( content ) . to_json ( self . output ) 
~~~ Dict ( self . data_to_print ) . to_json ( self . output ) 
if isinstance ( self . data_to_print , list ) : 
~~~ to_print = { } 
to_print_size = [ ] 
alone_cases = [ "Percentage" , "HTTP" ] 
without_header = [ "FullHosts" , "PlainDomain" ] 
if self . template . lower ( ) == "json" : 
~~~ if not PyFunceble . CONFIGURATION [ "no_files" ] and self . output : 
~~~ return self . _json_print ( ) 
~~ if self . template not in alone_cases and self . template not in without_header : 
~~~ self . header ( True ) 
to_print_size = self . _size_from_header ( self . currently_used_header ) 
~~ elif self . template in without_header : 
~~~ for data in self . data_to_print : 
~~~ to_print_size . append ( str ( len ( data ) ) ) 
~~~ to_print_size = self . _size_from_header ( self . headers [ self . template ] ) 
~~ to_print = self . _data_constructor ( to_print_size ) 
self . _before_header ( ) 
for data in self . _header_constructor ( to_print , False ) : 
~~~ if self . template . lower ( ) in PyFunceble . STATUS [ "list" ] [ 
"generic" 
] or self . template in [ "Less" , "Percentage" ] : 
~~~ colorified_data = self . _colorify ( data ) 
print ( colorified_data ) 
~~ ~~ if not PyFunceble . CONFIGURATION [ "no_files" ] and self . output : 
~~~ File ( self . output ) . write ( data + "\\n" ) 
self . _authorization ( ) 
and PyFunceble . CONFIGURATION [ "logs" ] 
and "file_to_test" in PyFunceble . INTERN 
~~~ self . file = ( 
PyFunceble . OUTPUT_DIRECTORY 
+ PyFunceble . OUTPUTS [ "parent_directory" ] 
+ PyFunceble . OUTPUTS [ "logs" ] [ "directories" ] [ "parent" ] 
+ PyFunceble . OUTPUTS [ "logs" ] [ "filenames" ] [ "execution_time" ] 
if PyFunceble . path . isfile ( self . file ) : 
~~~ content = Dict ( ) . from_json ( File ( self . file ) . read ( ) ) 
~~ if self . action == "start" : 
~~~ if "final_total" in content and content [ "final_total" ] : 
~~~ del content [ "final_total" ] 
~~ if "data" in content : 
~~~ content [ "data" ] . append ( [ PyFunceble . INTERN [ "start" ] ] ) 
~~~ content [ "data" ] = [ [ PyFunceble . INTERN [ "start" ] ] ] 
~~ ~~ elif self . action == "stop" : 
~~~ content [ "data" ] [ - 1 ] . append ( PyFunceble . INTERN [ "end" ] ) 
start = content [ "data" ] [ 0 ] [ 0 ] 
end = content [ "data" ] [ - 1 ] [ - 1 ] 
content [ "current_total" ] = self . format_execution_time ( start , end ) 
if last : 
~~~ content [ "final_total" ] = content [ "current_total" ] 
PyFunceble . Fore . MAGENTA 
+ PyFunceble . Style . BRIGHT 
+ content [ "final_total" ] 
~~~ Dict ( content ) . to_json ( self . file ) 
Dict ( content ) . to_json ( self . file ) 
~~ ~~ ~~ def _calculate ( cls , start = None , end = None ) : 
if start and end : 
~~~ time_difference = int ( end ) - int ( start ) 
~~~ time_difference = PyFunceble . INTERN [ "end" ] - PyFunceble . INTERN [ "start" ] 
~~ data = PyFunceble . OrderedDict ( ) 
data [ "days" ] = str ( time_difference // ( 24 * 60 * 60 ) ) . zfill ( 2 ) 
data [ "hours" ] = str ( ( time_difference // ( 60 * 60 ) ) % 24 ) . zfill ( 2 ) 
data [ "minutes" ] = str ( ( time_difference % 3600 ) // 60 ) . zfill ( 2 ) 
data [ "seconds" ] = str ( time_difference % 60 ) . zfill ( 2 ) 
~~ def format_execution_time ( self , start = None , end = None ) : 
return ":" . join ( list ( self . _calculate ( start , end ) . values ( ) ) ) 
~~ def file_to_delete ( cls ) : 
directory = PyFunceble . OUTPUT_DIRECTORY + PyFunceble . OUTPUTS [ "parent_directory" ] 
~~~ directory += PyFunceble . directory_separator 
~~ result = [ ] 
for root , _ , files in PyFunceble . walk ( directory ) : 
~~~ if file not in [ ".gitignore" , ".keep" ] : 
~~~ result . append ( root + file ) 
~~~ result . append ( 
root + PyFunceble . directory_separator + file 
directory = PyFunceble . CURRENT_DIRECTORY 
result . append ( 
directory 
directory + PyFunceble . CONFIGURATION [ "outputs" ] [ "default_files" ] [ "iana" ] 
+ PyFunceble . CONFIGURATION [ "outputs" ] [ "default_files" ] [ "inactive_db" ] 
directory + PyFunceble . CONFIGURATION [ "outputs" ] [ "default_files" ] [ "mining" ] 
directory + PyFunceble . CONFIGURATION [ "outputs" ] [ "default_files" ] [ "whois_db" ] 
~~ def almost_everything ( self , clean_all = False ) : 
to_delete = self . file_to_delete ( ) 
~~~ to_delete . extend ( self . databases_to_delete ( ) ) 
~~ for file in to_delete : 
~~~ File ( file ) . delete ( ) 
~~~ Load ( PyFunceble . CURRENT_DIRECTORY ) 
~~ ~~ def _hash_file ( self , algo ) : 
hash_data = getattr ( hashlib , algo ) ( ) 
with open ( self . path , "rb" ) as file : 
~~~ content = file . read ( ) 
hash_data . update ( content ) 
~~ return hash_data . hexdigest ( ) 
~~ def _hash_data ( self , algo ) : 
hash_data . update ( self . data ) 
return hash_data . hexdigest ( ) 
result = { } 
if self . algorithm in self . valid_algorithms : 
~~~ if self . algorithm == "all" : 
~~~ del self . valid_algorithms [ 0 ] 
for algo in self . valid_algorithms : 
~~~ if self . path and path . isfile ( self . path ) : 
~~~ result [ algo ] = self . _hash_file ( algo ) 
~~ elif self . data : 
~~~ result [ algo ] = self . _hash_data ( algo ) 
~~~ result [ self . algorithm ] = self . _hash_file ( self . algorithm ) 
~~~ result [ self . algorithm ] = self . _hash_data ( self . algorithm ) 
~~ if self . algorithm != "all" and self . only_hash : 
~~~ return result [ self . algorithm ] 
~~ def execute ( self ) : 
process = Popen ( self . command , stdout = PIPE , stderr = PIPE , shell = True ) 
( output , error ) = process . communicate ( ) 
~~~ return self . _decode_output ( error ) 
~~ return self . _decode_output ( output ) 
with Popen ( self . command , stdout = PIPE , shell = True ) as process : 
~~~ current_line = process . stdout . readline ( ) . rstrip ( ) 
if not current_line : 
~~ yield self . _decode_output ( current_line ) 
~~ ~~ ~~ def remove_key ( self , key_to_remove ) : 
if isinstance ( self . main_dictionnary , dict ) : 
~~~ if isinstance ( key_to_remove , list ) : 
~~~ for key in key_to_remove : 
~~~ del self . main_dictionnary [ key ] 
~~~ del self . main_dictionnary [ key_to_remove ] 
~~ ~~ return self . main_dictionnary 
~~ def rename_key ( self , key_to_rename , strict = True ) : 
if isinstance ( self . main_dictionnary , dict ) and isinstance ( key_to_rename , dict ) : 
~~~ for old , new in key_to_rename . items ( ) : 
~~~ if old in self . main_dictionnary : 
~~~ self . main_dictionnary [ new ] = self . main_dictionnary . pop ( old ) 
~~~ to_rename = { } 
for index in self . main_dictionnary : 
~~~ if old in index : 
~~~ to_rename . update ( { index : new [ : - 1 ] + index . split ( old ) [ - 1 ] } ) 
~~ ~~ self . main_dictionnary = Dict ( self . main_dictionnary ) . rename_key ( 
to_rename , True 
~~ def merge ( self , to_merge , strict = True ) : 
for element in to_merge : 
~~~ if element in self . main_dictionnary : 
~~~ if isinstance ( to_merge [ element ] , dict ) and isinstance ( 
self . main_dictionnary [ element ] , dict 
~~~ result [ element ] = Dict ( self . main_dictionnary [ element ] ) . merge ( 
to_merge [ element ] 
~~ elif isinstance ( to_merge [ element ] , list ) and isinstance ( 
self . main_dictionnary [ element ] , list 
~~~ result [ element ] = List ( self . main_dictionnary [ element ] ) . merge ( 
to_merge [ element ] , strict 
~~~ result . update ( { element : to_merge [ element ] } ) 
~~ ~~ for element in self . main_dictionnary : 
~~~ if element not in result : 
~~~ result [ element ] = self . main_dictionnary [ element ] 
~~ def to_json ( self , destination ) : 
~~~ with open ( destination , "w" ) as file : 
~~~ dump ( 
self . main_dictionnary , 
file , 
indent = 4 , 
sort_keys = True , 
~~~ with open ( destination , "w" , encoding = "utf-8" ) as file : 
~~ ~~ ~~ def to_yaml ( self , destination , flow_style = False ) : 
with open ( destination , "w" ) as file : 
~~~ dump_yaml ( 
encoding = "utf-8" , 
allow_unicode = True , 
default_flow_style = flow_style , 
~~ ~~ def fix_path ( self , splited_path = None ) : 
if not splited_path : 
~~~ split_path = [ ] 
if self . directory : 
~~~ if "/" in self . directory : 
~~~ split_path = self . directory . split ( "/" ) 
~~ elif "\\\\" in self . directory : 
~~~ split_path = self . directory . split ( "\\\\" ) 
~~~ split_path = [ self . directory ] 
~~ return self . fix_path ( splited_path = [ x for x in split_path if x ] ) 
~~ return self . directory 
~~ return directory_separator . join ( splited_path ) + directory_separator 
~~ def write ( self , data_to_write , overwrite = False ) : 
if overwrite or not path . isfile ( self . file ) : 
~~~ with open ( self . file , "w" , encoding = "utf-8" , newline = "\\n" ) as file : 
~~~ if data_to_write and isinstance ( data_to_write , str ) : 
~~~ file . write ( data_to_write ) 
~~~ with open ( self . file , "a" , encoding = "utf-8" , newline = "\\n" ) as file : 
~~ ~~ ~~ ~~ def read ( self ) : 
~~~ with open ( self . file , "r" , encoding = "utf-8" ) as file : 
~~~ funilrys = file . read ( ) 
~~~ with open ( self . file , "r" ) as file : 
~~ ~~ return funilrys 
~~ def format ( self ) : 
~~~ return sorted ( list ( set ( self . main_list ) ) , key = str . lower ) 
~~~ return self . main_list 
~~ ~~ def custom_format ( self , key_method , reverse = False ) : 
~~~ return sorted ( list ( set ( self . main_list ) ) , key = key_method , reverse = reverse ) 
~~ ~~ def merge ( self , to_merge , strict = True ) : 
if strict : 
~~~ for index , element in enumerate ( to_merge ) : 
~~~ if isinstance ( element , dict ) and isinstance ( 
self . main_list [ index ] , dict 
~~~ result . append ( Dict ( self . main_list [ index ] ) . merge ( element ) ) 
~~ elif isinstance ( element , list ) and isinstance ( 
self . main_list [ index ] , list 
~~~ result . append ( List ( self . main_list [ index ] ) . merge ( element ) ) 
~~~ result . append ( element ) 
~~~ result = self . main_list 
~~ ~~ ~~ return result 
~~ def not_matching_list ( self ) : 
pre_result = comp ( self . regex ) 
return [ x for x in self . data if not pre_result . search ( str ( x ) ) ] 
~~ def match ( self ) : 
to_match = comp ( self . regex ) 
~~~ pre_result = to_match . findall ( self . data ) 
~~~ pre_result = to_match . search ( self . data ) 
~~~ for data in pre_result : 
~~~ if isinstance ( data , tuple ) : 
~~~ result . extend ( list ( data ) ) 
~~~ result = pre_result . group ( 
) . strip ( ) 
~~ def replace ( self ) : 
~~~ return substrings ( 
self . regex , 
self . data , 
~~ return self . data 
~~ def text ( self ) : 
~~~ req = requests . get ( self . link , verify = self . verification ) 
if req . status_code == 200 : 
~~~ if self . return_data : 
~~~ return req . text 
~~ File ( self . destination ) . write ( req . text , overwrite = True ) 
~~ except requests . exceptions . ConnectionError : 
~~ ~~ def count ( self ) : 
if self . status : 
~~~ PyFunceble . INTERN [ "counter" ] [ "number" ] [ "tested" ] += 1 
self . status . lower ( ) in PyFunceble . STATUS [ "list" ] [ "up" ] 
or self . status . lower ( ) in PyFunceble . STATUS [ "list" ] [ "valid" ] 
~~~ PyFunceble . INTERN [ "counter" ] [ "number" ] [ "up" ] += 1 
~~ elif self . status . lower ( ) in PyFunceble . STATUS [ "list" ] [ "down" ] : 
~~~ PyFunceble . INTERN [ "counter" ] [ "number" ] [ "down" ] += 1 
~~~ PyFunceble . INTERN [ "counter" ] [ "number" ] [ "invalid" ] += 1 
~~ ~~ ~~ def _calculate ( cls ) : 
percentages = { 
"up" : PyFunceble . INTERN [ "counter" ] [ "number" ] [ "up" ] , 
"down" : PyFunceble . INTERN [ "counter" ] [ "number" ] [ "down" ] , 
"invalid" : PyFunceble . INTERN [ "counter" ] [ "number" ] [ "invalid" ] , 
for percentage in percentages : 
~~~ calculation = ( 
percentages [ percentage ] 
* 100 
// PyFunceble . INTERN [ "counter" ] [ "number" ] [ "tested" ] 
PyFunceble . INTERN [ "counter" ] [ "percentage" ] . update ( { percentage : calculation } ) 
~~ ~~ def log ( self ) : 
PyFunceble . CONFIGURATION [ "show_percentage" ] 
and PyFunceble . INTERN [ "counter" ] [ "number" ] [ "tested" ] > 0 
+ PyFunceble . OUTPUTS [ "logs" ] [ "directories" ] [ "percentage" ] 
+ PyFunceble . OUTPUTS [ "logs" ] [ "filenames" ] [ "percentage" ] 
File ( output ) . delete ( ) 
self . _calculate ( ) 
Prints ( None , "Percentage" , output ) . header ( ) 
lines_to_print = [ 
str ( PyFunceble . INTERN [ "counter" ] [ "percentage" ] [ "up" ] ) + "%" , 
PyFunceble . INTERN [ "counter" ] [ "number" ] [ "up" ] , 
str ( PyFunceble . INTERN [ "counter" ] [ "percentage" ] [ "down" ] ) + "%" , 
PyFunceble . INTERN [ "counter" ] [ "number" ] [ "down" ] , 
str ( PyFunceble . INTERN [ "counter" ] [ "percentage" ] [ "invalid" ] ) 
+ "%" , 
PyFunceble . INTERN [ "counter" ] [ "number" ] [ "invalid" ] , 
~~~ lines_to_print [ 0 ] [ 0 ] = PyFunceble . STATUS [ "official" ] [ "valid" ] 
del lines_to_print [ 1 ] 
~~ for to_print in lines_to_print : 
~~~ Prints ( to_print , "Percentage" , output ) . data ( ) 
~~ ~~ ~~ elif PyFunceble . INTERN [ "counter" ] [ "number" ] [ "tested" ] > 0 : 
~~~ self . _calculate ( ) 
~~ ~~ def is_url_valid ( self , url = None , return_base = False , return_formatted = False ) : 
initial_base = None 
if url : 
~~~ to_test = url 
~~ elif self . element : 
~~~ to_test = self . element 
~~~ to_test = PyFunceble . INTERN [ "to_test" ] 
~~ if to_test . startswith ( "http" ) : 
~~~ regex = r"(^(http:\\/\\/|https:\\/\\/)(.+?(?=\\/)|.+?$))" 
initial_base = base = Regex ( 
to_test , regex , return_data = True , rematch = True 
) . match ( ) [ 2 ] 
~~~ base = domain2idna ( base ) 
~~ domain_status = self . is_domain_valid ( base ) 
ip_status = self . is_ip_valid ( base ) 
if domain_status or ip_status : 
~~~ if PyFunceble . CONFIGURATION [ "idna_conversion" ] and return_formatted : 
~~~ return Regex ( 
to_test , 
initial_base , 
escape = True , 
return_data = True , 
replace_with = base , 
occurences = 1 , 
~~ if return_formatted : 
~~~ return to_test 
~~ if return_base : 
~~ ~~ except TypeError : 
~~ ~~ if return_formatted : 
~~ def is_domain_valid ( 
self , domain = None , subdomain_check = False 
~~~ to_test = domain 
~~~ last_point_index = to_test . rindex ( "." ) 
extension = to_test [ last_point_index + 1 : ] 
if not extension and to_test . endswith ( "." ) : 
~~~ extension = [ x for x in to_test . split ( "." ) if x ] [ - 1 ] 
~~ ~~ if not extension or extension not in PyFunceble . INTERN [ "iana_db" ] : 
Regex ( to_test , regex_valid_domains , return_data = False ) . match ( ) 
and not subdomain_check 
~~ if extension in PyFunceble . INTERN [ "psl_db" ] : 
~~~ suffix_index = to_test . rindex ( "." + suffix ) 
to_check = to_test [ : suffix_index ] 
if "." not in to_check and subdomain_check : 
~~ if "." in to_check and subdomain_check : 
~~ if "." in to_check : 
to_check , regex_valid_subdomains , return_data = False 
~~ ~~ ~~ to_check = to_test [ : last_point_index ] 
if "." in to_check and subdomain_check : 
~~ ~~ except ( ValueError , AttributeError ) : 
~~ def is_subdomain ( self , domain = None ) : 
~~ return self . is_domain_valid ( to_test , subdomain_check = True ) 
~~ def is_ip_valid ( self , ip_to_check = None ) : 
if ip_to_check : 
~~~ to_test = ip_to_check 
~~ return Regex ( to_test , regex_ipv4 , return_data = False ) . match ( ) 
~~ def is_ip_range ( self , ip_to_check = None ) : 
~~ if self . is_ip_valid ( to_test ) : 
return Regex ( to_test , regex_ipv4_range , return_data = False ) . match ( ) 
if PyFunceble . INTERN [ "to_test_type" ] == "domain" : 
~~~ if Check ( ) . is_domain_valid ( ) or Check ( ) . is_ip_valid ( ) : 
~~~ return SyntaxStatus ( PyFunceble . STATUS [ "official" ] [ "valid" ] ) . handle ( ) 
~~ ~~ elif PyFunceble . INTERN [ "to_test_type" ] == "url" : 
~~~ if Check ( ) . is_url_valid ( ) : 
~~ return SyntaxStatus ( PyFunceble . STATUS [ "official" ] [ "invalid" ] ) . handle ( ) 
if PyFunceble . CONFIGURATION [ "inactive_database" ] : 
~~~ historical_formating_error = ( 
PyFunceble . CURRENT_DIRECTORY + "inactive-db.json" 
if PyFunceble . path . isfile ( historical_formating_error ) : 
~~~ data = Dict ( ) . from_json ( File ( historical_formating_error ) . read ( ) ) 
data_to_parse = { } 
top_keys = data . keys ( ) 
for top_key in top_keys : 
~~~ low_keys = data [ top_key ] . keys ( ) 
data_to_parse [ top_key ] = { } 
for low_key in low_keys : 
~~~ if low_key . isdigit ( ) : 
~~~ data_to_parse [ top_key ] [ 
int ( low_key ) - ( self . one_day_in_seconds * 30 ) 
] = data [ top_key ] [ low_key ] 
int ( PyFunceble . time ( ) ) - ( self . one_day_in_seconds * 30 ) 
~~ ~~ ~~ if "inactive_db" in PyFunceble . INTERN : 
~~~ PyFunceble . INTERN [ "inactive_db" ] . update ( data_to_parse ) 
~~~ PyFunceble . INTERN [ "inactive_db" ] = data_to_parse 
~~ File ( historical_formating_error ) . delete ( ) 
~~ ~~ ~~ def _merge ( self ) : 
~~~ database_content = Dict ( ) . from_json ( File ( self . inactive_db_path ) . read ( ) ) 
database_top_keys = database_content . keys ( ) 
for database_top_key in database_top_keys : 
~~~ if database_top_key not in PyFunceble . INTERN [ "inactive_db" ] : 
~~~ PyFunceble . INTERN [ "inactive_db" ] [ 
database_top_key 
] = database_content [ database_top_key ] 
~~~ database_low_keys = database_content [ database_top_key ] . keys ( ) 
for database_low_key in database_low_keys : 
database_low_key 
not in PyFunceble . INTERN [ "inactive_db" ] [ database_top_key ] 
~~~ PyFunceble . INTERN [ "inactive_db" ] [ database_top_key ] [ 
] = database_content [ database_top_key ] [ database_low_key ] 
] . extend ( 
database_content [ database_top_key ] [ database_low_key ] 
PyFunceble . INTERN [ "inactive_db" ] [ database_top_key ] [ 
~~ ~~ ~~ ~~ ~~ ~~ def _retrieve ( self ) : 
~~~ self . _reformat_historical_formating_error ( ) 
if PyFunceble . path . isfile ( self . inactive_db_path ) : 
~~~ self . _merge ( ) 
~~ ~~ ~~ def _backup ( self ) : 
~~~ Dict ( PyFunceble . INTERN [ "inactive_db" ] ) . to_json ( self . inactive_db_path ) 
~~ ~~ def _timestamp ( self ) : 
"inactive_db" in PyFunceble . INTERN 
in PyFunceble . INTERN [ "inactive_db" ] 
and PyFunceble . INTERN [ "inactive_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] 
~~~ database_keys = [ 
for x in PyFunceble . INTERN [ "inactive_db" ] [ 
] . keys ( ) 
if x . isdigit ( ) 
if database_keys : 
~~~ recent_date = max ( database_keys ) 
~~~ return int ( PyFunceble . time ( ) ) 
~~ if int ( PyFunceble . time ( ) ) > int ( recent_date ) + self . one_day_in_seconds : 
~~ if int ( PyFunceble . time ( ) ) < int ( recent_date ) + self . days_in_seconds : 
~~~ return int ( recent_date ) 
~~ ~~ ~~ return int ( PyFunceble . time ( ) ) 
~~ def add ( self ) : 
~~~ timestamp = str ( self . _timestamp ( ) ) 
timestamp 
in PyFunceble . INTERN [ "inactive_db" ] [ 
PyFunceble . INTERN [ "to_test" ] 
not in PyFunceble . INTERN [ "inactive_db" ] [ 
] [ timestamp ] 
] [ timestamp ] . append ( PyFunceble . INTERN [ "to_test" ] ) 
] . update ( { timestamp : [ PyFunceble . INTERN [ "to_test" ] ] } ) 
] [ "to_test" ] 
~~~ PyFunceble . INTERN [ "inactive_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] [ 
] . remove ( PyFunceble . INTERN [ "to_test" ] ) 
~~~ PyFunceble . INTERN [ "inactive_db" ] = { 
PyFunceble . INTERN [ "file_to_test" ] : { 
timestamp : [ PyFunceble . INTERN [ "to_test" ] ] 
~~~ if PyFunceble . INTERN [ "file_to_test" ] in PyFunceble . INTERN [ "inactive_db" ] : 
~~~ for data in PyFunceble . INTERN [ "inactive_db" ] [ 
] [ data ] 
] [ data ] . remove ( PyFunceble . INTERN [ "to_test" ] ) 
~~ ~~ ~~ self . _backup ( ) 
~~ ~~ def content ( cls ) : 
PyFunceble . CONFIGURATION [ "inactive_database" ] 
and PyFunceble . INTERN [ "inactive_db" ] 
~~~ for key in PyFunceble . INTERN [ "inactive_db" ] [ 
~~~ if key == "to_test" : 
~~ result . extend ( 
PyFunceble . INTERN [ "inactive_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] [ 
key 
~~ def is_present ( cls ) : 
~~~ if PyFunceble . INTERN [ "to_test" ] in PyFunceble . INTERN [ 
"flatten_inactive_db" 
] or ( 
PyFunceble . INTERN [ "file_to_test" ] in PyFunceble . INTERN [ "inactive_db" ] 
and "to_test" 
in PyFunceble . INTERN [ "inactive_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] 
in PyFunceble . INTERN [ "inactive_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] [ 
if self . _authorization ( ) and "whois_db" not in PyFunceble . INTERN : 
~~~ if PyFunceble . path . isfile ( self . whois_db_path ) : 
~~~ PyFunceble . INTERN [ "whois_db" ] = Dict ( ) . from_json ( 
File ( self . whois_db_path ) . read ( ) 
~~~ PyFunceble . INTERN [ "whois_db" ] = { } 
if self . _authorization ( ) : 
~~~ Dict ( PyFunceble . INTERN [ "whois_db" ] ) . to_json ( self . whois_db_path ) 
~~ ~~ def is_in_database ( self ) : 
and PyFunceble . INTERN [ "file_to_test" ] in PyFunceble . INTERN [ "whois_db" ] 
in PyFunceble . INTERN [ "whois_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] 
~~ def is_time_older ( self ) : 
and self . is_in_database ( ) 
and int ( 
PyFunceble . INTERN [ "whois_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] [ 
] [ "epoch" ] 
< int ( PyFunceble . time ( ) ) 
~~ def get_expiration_date ( self ) : 
if self . _authorization ( ) and self . is_in_database ( ) and not self . is_time_older ( ) : 
~~~ result = PyFunceble . INTERN [ "whois_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] [ 
] [ "expiration_date" ] 
if result : 
~~~ return result 
~~~ if self . epoch < int ( PyFunceble . time ( ) ) : 
~~~ state = "past" 
~~~ state = "future" 
~~ if self . is_in_database ( ) : 
str ( self . epoch ) 
!= PyFunceble . INTERN [ "whois_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] [ 
~~~ PyFunceble . INTERN [ "whois_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] [ 
] . update ( 
"epoch" : str ( self . epoch ) , 
"state" : state , 
"expiration_date" : self . expiration_date , 
~~ elif self . is_time_older ( ) : 
PyFunceble . INTERN [ "whois_db" ] [ 
] [ PyFunceble . INTERN [ "to_test" ] ] [ "state" ] 
!= "past" 
~~~ PyFunceble . INTERN [ "whois_db" ] [ 
] [ PyFunceble . INTERN [ "to_test" ] ] . update ( { "state" : "past" } ) 
~~ ~~ elif ( 
] [ "state" ] 
!= "future" 
] . update ( { "state" : "future" } ) 
not PyFunceble . INTERN [ "file_to_test" ] 
in PyFunceble . INTERN [ "whois_db" ] 
] = { } 
~~ PyFunceble . INTERN [ "whois_db" ] [ PyFunceble . INTERN [ "file_to_test" ] ] . update ( 
PyFunceble . INTERN [ "to_test" ] : { 
~~ ~~ def travis_permissions ( cls ) : 
if PyFunceble . CONFIGURATION [ "travis" ] : 
~~~ build_dir = PyFunceble . environ [ "TRAVIS_BUILD_DIR" ] 
commands = [ 
% ( build_dir + PyFunceble . directory_separator ) , 
for command in commands : 
~~~ Command ( command ) . execute ( ) 
~~ ~~ ~~ def _travis ( self ) : 
~~~ _ = PyFunceble . environ [ "TRAVIS_BUILD_DIR" ] 
time_autorisation = False 
~~~ time_autorisation = int ( PyFunceble . time ( ) ) >= int ( 
PyFunceble . INTERN [ "start" ] 
) + ( int ( PyFunceble . CONFIGURATION [ "travis_autosave_minutes" ] ) * 60 ) 
~~~ if self . last and not self . bypass : 
~~ ~~ if self . last or time_autorisation or self . bypass : 
~~~ Percentage ( ) . log ( ) 
self . travis_permissions ( ) 
command = \ 
if self . last or self . bypass : 
~~~ if PyFunceble . CONFIGURATION [ "command_before_end" ] : 
~~~ for line in Command ( 
PyFunceble . CONFIGURATION [ "command_before_end" ] 
) . run ( ) : 
~~~ sys_stdout . write ( "{}\\n" . format ( line ) ) 
~~ self . travis_permissions ( ) 
~~ message = ( 
PyFunceble . CONFIGURATION [ "travis_autosave_final_commit" ] 
Command ( command % message ) . execute ( ) 
~~~ if PyFunceble . CONFIGURATION [ "command" ] : 
PyFunceble . CONFIGURATION [ "command" ] 
~~ Command ( 
command % PyFunceble . CONFIGURATION [ "travis_autosave_commit" ] 
) . execute ( ) 
~~ print ( 
Command ( 
% PyFunceble . CONFIGURATION [ "travis_branch" ] 
exit ( 0 ) 
~~ ~~ ~~ def nslookup ( cls ) : 
~~~ if not Check ( ) . is_ip_valid ( ) : 
~~~ request = PyFunceble . socket . getaddrinfo ( 
PyFunceble . INTERN [ "to_test" ] , 
80 , 
PyFunceble . socket . IPPROTO_TCP , 
for sequence in request : 
~~~ PyFunceble . INTERN [ "current_test_data" ] [ "nslookup" ] . append ( 
sequence [ - 1 ] [ 0 ] 
~~~ request = PyFunceble . socket . gethostbyaddr ( 
PyFunceble . INTERN [ "current_test_data" ] [ "nslookup" ] [ 
"hostname" 
] = request [ 0 ] 
"aliases" 
] = request [ 1 ] 
PyFunceble . INTERN [ "current_test_data" ] [ "nslookup" ] [ "ips" ] = request [ 
2 
~~~ PyFunceble . socket . getaddrinfo ( 
~~~ PyFunceble . socket . gethostbyaddr ( PyFunceble . INTERN [ "to_test" ] ) 
~~ except ( OSError , PyFunceble . socket . herror , PyFunceble . socket . gaierror ) : 
if domain is None : 
~~~ domain = PyFunceble . INTERN [ "to_test" ] 
~~ if timeout is None : 
~~~ timeout = PyFunceble . CONFIGURATION [ "seconds_before_http_timeout" ] 
~~ if whois_server : 
~~~ req = PyFunceble . socket . socket ( 
PyFunceble . socket . AF_INET , PyFunceble . socket . SOCK_STREAM 
if timeout % 3 == 0 : 
~~~ req . settimeout ( timeout ) 
~~~ req . settimeout ( 3 ) 
~~~ req . connect ( ( whois_server , 43 ) ) 
~~ except PyFunceble . socket . error : 
~~ req . send ( ( domain + "\\r\\n" ) . encode ( ) ) 
response = b"" 
~~~ data = req . recv ( 4096 ) 
~~ except ( PyFunceble . socket . timeout , ConnectionResetError ) : 
~~~ req . close ( ) 
~~ response += data 
~~ ~~ req . close ( ) 
~~~ return response . decode ( ) 
~~~ return response . decode ( "utf-8" , "replace" ) 
if Check ( ) . is_url_valid ( ) or PyFunceble . CONFIGURATION [ "local" ] : 
~~~ PyFunceble . INTERN [ "current_test_data" ] [ "url_syntax_validation" ] = True 
~~ PyFunceble . INTERN . update ( { "http_code" : HTTPCode ( ) . get ( ) } ) 
active_list = [ ] 
active_list . extend ( PyFunceble . HTTP_CODE [ "list" ] [ "potentially_up" ] ) 
active_list . extend ( PyFunceble . HTTP_CODE [ "list" ] [ "up" ] ) 
inactive_list = [ ] 
inactive_list . extend ( PyFunceble . HTTP_CODE [ "list" ] [ "potentially_down" ] ) 
inactive_list . append ( "*" * 3 ) 
if PyFunceble . INTERN [ "http_code" ] in active_list : 
~~~ return URLStatus ( PyFunceble . STATUS [ "official" ] [ "up" ] ) . handle ( ) 
~~ if PyFunceble . INTERN [ "http_code" ] in inactive_list : 
~~~ return URLStatus ( PyFunceble . STATUS [ "official" ] [ "down" ] ) . handle ( ) 
~~ ~~ if "current_test_data" in PyFunceble . INTERN : 
~~~ PyFunceble . INTERN [ "current_test_data" ] [ "url_syntax_validation" ] = False 
~~ return URLStatus ( PyFunceble . STATUS [ "official" ] [ "invalid" ] ) . handle ( ) 
~~ def _get_version ( ) : 
to_match = comp ( r\ ) 
extracted = to_match . findall ( 
open ( "PyFunceble/__init__.py" , encoding = "utf-8" ) . read ( ) 
) [ 0 ] 
return "." . join ( [ x for x in extracted . split ( "." ) if x . isdigit ( ) ] ) 
if not PyFunceble . CONFIGURATION [ "local" ] : 
~~~ if self . domain_extension not in self . ignored_extension : 
if self . domain_extension in PyFunceble . INTERN [ "iana_db" ] : 
~~~ if not PyFunceble . CONFIGURATION [ "no_whois" ] : 
~~~ referer = PyFunceble . INTERN [ "iana_db" ] [ self . domain_extension ] 
if not referer : 
~~~ Logs ( ) . referer_not_found ( self . domain_extension ) 
~~ return referer 
~~ def standard_paths ( ) : 
for is_plat_spec in [ True , False ] : 
~~~ path = distutils . sysconfig . get_python_lib ( standard_lib = True , 
plat_specific = is_plat_spec ) 
for name in os . listdir ( path ) : 
~~~ yield name 
~~~ for name in os . listdir ( os . path . join ( path , 'lib-dynload' ) ) : 
~~ ~~ ~~ def standard_package_names ( ) : 
for name in standard_paths ( ) : 
~~~ if name . startswith ( '_' ) or '-' in name : 
~~ if '.' in name and name . rsplit ( '.' ) [ - 1 ] not in [ 'so' , 'py' , 'pyc' ] : 
~~ yield name . split ( '.' ) [ 0 ] 
~~ ~~ def unused_import_line_numbers ( messages ) : 
~~~ if isinstance ( message , pyflakes . messages . UnusedImport ) : 
~~~ yield message . lineno 
~~ ~~ ~~ def unused_import_module_name ( messages ) : 
pattern = r'\\'(.+?)\\'' 
~~~ module_name = re . search ( pattern , str ( message ) ) 
module_name = module_name . group ( ) [ 1 : - 1 ] 
if module_name : 
~~~ yield ( message . lineno , module_name ) 
~~ ~~ ~~ ~~ def star_import_used_line_numbers ( messages ) : 
~~~ if isinstance ( message , pyflakes . messages . ImportStarUsed ) : 
~~ ~~ ~~ def star_import_usage_undefined_name ( messages ) : 
~~~ if isinstance ( message , pyflakes . messages . ImportStarUsage ) : 
~~~ undefined_name = message . message_args [ 0 ] 
module_name = message . message_args [ 1 ] 
yield ( message . lineno , undefined_name , module_name ) 
~~ ~~ ~~ def unused_variable_line_numbers ( messages ) : 
~~~ if isinstance ( message , pyflakes . messages . UnusedVariable ) : 
~~ ~~ ~~ def duplicate_key_line_numbers ( messages , source ) : 
messages = [ 
message for message in messages 
if isinstance ( message , pyflakes . messages . MultiValueRepeatedKeyLiteral ) ] 
if messages : 
~~~ key_to_messages = create_key_to_messages_dict ( messages ) 
lines = source . split ( '\\n' ) 
for ( key , messages ) in key_to_messages . items ( ) : 
~~~ good = True 
~~~ line = lines [ message . lineno - 1 ] 
key = message . message_args [ 0 ] 
if not dict_entry_has_key ( line , key ) : 
~~~ good = False 
~~ ~~ if good : 
~~~ for message in messages : 
~~ ~~ ~~ ~~ ~~ def create_key_to_messages_dict ( messages ) : 
dictionary = collections . defaultdict ( lambda : [ ] ) 
~~~ dictionary [ message . message_args [ 0 ] ] . append ( message ) 
~~ return dictionary 
~~ def check ( source ) : 
if sys . version_info [ 0 ] == 2 and isinstance ( source , unicode ) : 
~~~ source = source . encode ( 'utf-8' ) 
~~ ~~ reporter = ListReporter ( ) 
~~~ pyflakes . api . check ( source , filename = '<string>' , reporter = reporter ) 
~~ except ( AttributeError , RecursionError , UnicodeDecodeError ) : 
~~ return reporter . messages 
~~ def extract_package_name ( line ) : 
assert '\\\\' not in line 
assert '(' not in line 
assert ')' not in line 
assert ';' not in line 
if line . lstrip ( ) . startswith ( ( 'import' , 'from' ) ) : 
~~~ word = line . split ( ) [ 1 ] 
~~ package = word . split ( '.' ) [ 0 ] 
return package 
~~ def multiline_import ( line , previous_line = '' ) : 
for symbol in '()' : 
~~~ if symbol in line : 
~~ ~~ if line . lstrip ( ) . startswith ( '>' ) : 
~~ return multiline_statement ( line , previous_line ) 
~~ def multiline_statement ( line , previous_line = '' ) : 
for symbol in '\\\\:;' : 
~~ ~~ sio = io . StringIO ( line ) 
~~~ list ( tokenize . generate_tokens ( sio . readline ) ) 
return previous_line . rstrip ( ) . endswith ( '\\\\' ) 
~~ except ( SyntaxError , tokenize . TokenError ) : 
~~ ~~ def filter_from_import ( line , unused_module ) : 
( indentation , imports ) = re . split ( pattern = r'\\bimport\\b' , 
string = line , maxsplit = 1 ) 
string = indentation ) . group ( 1 ) 
imports = re . split ( pattern = r',' , string = imports . strip ( ) ) 
imports = [ base_module + '.' + x . strip ( ) for x in imports ] 
filtered_imports = [ x . replace ( base_module + '.' , '' ) 
for x in imports if x not in unused_module ] 
if not filtered_imports : 
~~~ return get_indentation ( line ) + 'pass' + get_line_ending ( line ) 
indentation + 
get_line_ending ( line ) ) 
~~ def break_up_import ( line ) : 
assert '#' not in line 
assert not line . lstrip ( ) . startswith ( 'from' ) 
newline = get_line_ending ( line ) 
if not newline : 
~~~ return line 
~~ ( indentation , imports ) = re . split ( pattern = r'\\bimport\\b' , 
assert newline 
return '' . join ( [ indentation + i . strip ( ) + newline 
for i in sorted ( imports . split ( ',' ) ) ] ) 
~~ def filter_code ( source , additional_imports = None , 
expand_star_imports = False , 
remove_all_unused_imports = False , 
remove_duplicate_keys = False , 
remove_unused_variables = False , 
ignore_init_module_imports = False , 
imports = SAFE_IMPORTS 
if additional_imports : 
~~~ imports |= frozenset ( additional_imports ) 
~~ del additional_imports 
messages = check ( source ) 
if ignore_init_module_imports : 
~~~ marked_import_line_numbers = frozenset ( ) 
~~~ marked_import_line_numbers = frozenset ( 
unused_import_line_numbers ( messages ) ) 
~~ marked_unused_module = collections . defaultdict ( lambda : [ ] ) 
for line_number , module_name in unused_import_module_name ( messages ) : 
~~~ marked_unused_module [ line_number ] . append ( module_name ) 
~~ if expand_star_imports and not ( 
re . search ( r'\\b__all__\\b' , source ) or 
re . search ( r'\\bdel\\b' , source ) 
~~~ marked_star_import_line_numbers = frozenset ( 
star_import_used_line_numbers ( messages ) ) 
if len ( marked_star_import_line_numbers ) > 1 : 
~~~ marked_star_import_line_numbers = frozenset ( ) 
~~~ undefined_names = [ ] 
for line_number , undefined_name , _ in star_import_usage_undefined_name ( messages ) : 
~~~ undefined_names . append ( undefined_name ) 
~~ if not undefined_names : 
~~ if remove_unused_variables : 
~~~ marked_variable_line_numbers = frozenset ( 
unused_variable_line_numbers ( messages ) ) 
~~~ marked_variable_line_numbers = frozenset ( ) 
~~ if remove_duplicate_keys : 
~~~ marked_key_line_numbers = frozenset ( 
duplicate_key_line_numbers ( messages , source ) ) 
~~~ marked_key_line_numbers = frozenset ( ) 
~~ line_messages = get_messages_by_line ( messages ) 
sio = io . StringIO ( source ) 
previous_line = '' 
for line_number , line in enumerate ( sio . readlines ( ) , start = 1 ) : 
~~~ if '#' in line : 
~~ elif line_number in marked_import_line_numbers : 
~~~ yield filter_unused_import ( 
line , 
unused_module = marked_unused_module [ line_number ] , 
remove_all_unused_imports = remove_all_unused_imports , 
imports = imports , 
previous_line = previous_line ) 
~~ elif line_number in marked_variable_line_numbers : 
~~~ yield filter_unused_variable ( line ) 
~~ elif line_number in marked_key_line_numbers : 
~~~ yield filter_duplicate_key ( line , line_messages [ line_number ] , 
line_number , marked_key_line_numbers , 
source ) 
~~ elif line_number in marked_star_import_line_numbers : 
~~~ yield filter_star_import ( line , undefined_names ) 
~~ previous_line = line 
~~ ~~ def get_messages_by_line ( messages ) : 
line_messages = { } 
~~~ line_messages [ message . lineno ] = message 
~~ return line_messages 
~~ def filter_star_import ( line , marked_star_import_undefined_name ) : 
undefined_name = sorted ( set ( marked_star_import_undefined_name ) ) 
~~ def filter_unused_import ( line , unused_module , remove_all_unused_imports , 
imports , previous_line = '' ) : 
if multiline_import ( line , previous_line ) : 
~~ is_from_import = line . lstrip ( ) . startswith ( 'from' ) 
if ',' in line and not is_from_import : 
~~~ return break_up_import ( line ) 
~~ package = extract_package_name ( line ) 
if not remove_all_unused_imports and package not in imports : 
~~ if ',' in line : 
~~~ assert is_from_import 
return filter_from_import ( line , unused_module ) 
~~~ return ( get_indentation ( line ) + 
'pass' + 
~~ ~~ def filter_unused_variable ( line , previous_line = '' ) : 
if re . match ( EXCEPT_REGEX , line ) : 
~~ elif multiline_statement ( line , previous_line ) : 
~~ elif line . count ( '=' ) == 1 : 
~~~ split_line = line . split ( '=' ) 
assert len ( split_line ) == 2 
value = split_line [ 1 ] . lstrip ( ) 
if ',' in split_line [ 0 ] : 
~~ if is_literal_or_name ( value ) : 
~~~ value = 'pass' + get_line_ending ( line ) 
~~ return get_indentation ( line ) + value 
~~ ~~ def filter_duplicate_key ( line , message , line_number , marked_line_numbers , 
source , previous_line = '' ) : 
if marked_line_numbers and line_number == sorted ( marked_line_numbers ) [ 0 ] : 
~~ return line 
~~ def dict_entry_has_key ( line , key ) : 
if '#' in line : 
~~ result = re . match ( r'\\s*(.*)\\s*:\\s*(.*),\\s*$' , line ) 
~~~ candidate_key = ast . literal_eval ( result . group ( 1 ) ) 
~~ except ( SyntaxError , ValueError ) : 
~~ if multiline_statement ( result . group ( 2 ) ) : 
~~ return candidate_key == key 
~~ def is_literal_or_name ( value ) : 
~~~ ast . literal_eval ( value ) 
~~ if value . strip ( ) in [ 'dict()' , 'list()' , 'set()' ] : 
~~ return re . match ( r'^\\w+\\s*$' , value ) 
~~ def useless_pass_line_numbers ( source ) : 
previous_token_type = None 
last_pass_row = None 
last_pass_indentation = None 
for token in tokenize . generate_tokens ( sio . readline ) : 
~~~ token_type = token [ 0 ] 
start_row = token [ 2 ] [ 0 ] 
line = token [ 4 ] 
is_pass = ( token_type == tokenize . NAME and line . strip ( ) == 'pass' ) 
if ( start_row - 1 == last_pass_row and 
get_indentation ( line ) == last_pass_indentation and 
token_type in ATOMS and 
not is_pass ) : 
~~~ yield start_row - 1 
~~ if is_pass : 
~~~ last_pass_row = start_row 
last_pass_indentation = get_indentation ( line ) 
~~ if ( is_pass and 
previous_token_type != tokenize . INDENT and 
not previous_line . rstrip ( ) . endswith ( '\\\\' ) ) : 
~~~ yield start_row 
~~ previous_token_type = token_type 
previous_line = line 
~~ ~~ def filter_useless_pass ( source ) : 
~~~ marked_lines = frozenset ( useless_pass_line_numbers ( source ) ) 
~~~ marked_lines = frozenset ( ) 
~~ sio = io . StringIO ( source ) 
~~~ if line_number not in marked_lines : 
~~ ~~ ~~ def get_indentation ( line ) : 
if line . strip ( ) : 
~~~ non_whitespace_index = len ( line ) - len ( line . lstrip ( ) ) 
return line [ : non_whitespace_index ] 
~~ ~~ def get_line_ending ( line ) : 
non_whitespace_index = len ( line . rstrip ( ) ) - len ( line ) 
if not non_whitespace_index : 
~~~ return line [ non_whitespace_index : ] 
~~ ~~ def fix_code ( source , additional_imports = None , expand_star_imports = False , 
remove_all_unused_imports = False , remove_duplicate_keys = False , 
remove_unused_variables = False , ignore_init_module_imports = False ) : 
if not source : 
~~~ return source 
~~ if 'nonlocal' in source : 
~~~ remove_unused_variables = False 
~~ filtered_source = None 
~~~ filtered_source = '' . join ( 
filter_useless_pass ( '' . join ( 
filter_code ( 
additional_imports = additional_imports , 
expand_star_imports = expand_star_imports , 
remove_duplicate_keys = remove_duplicate_keys , 
remove_unused_variables = remove_unused_variables , 
ignore_init_module_imports = ignore_init_module_imports , 
) ) ) ) 
if filtered_source == source : 
~~ source = filtered_source 
~~ return filtered_source 
~~ def fix_file ( filename , args , standard_out ) : 
encoding = detect_encoding ( filename ) 
with open_with_encoding ( filename , encoding = encoding ) as input_file : 
~~~ source = input_file . read ( ) 
~~ original_source = source 
isInitFile = os . path . basename ( filename ) == '__init__.py' 
if args . ignore_init_module_imports and isInitFile : 
~~~ ignore_init_module_imports = True 
~~~ ignore_init_module_imports = False 
~~ filtered_source = fix_code ( 
additional_imports = args . imports . split ( ',' ) if args . imports else None , 
expand_star_imports = args . expand_star_imports , 
remove_all_unused_imports = args . remove_all_unused_imports , 
remove_duplicate_keys = args . remove_duplicate_keys , 
remove_unused_variables = args . remove_unused_variables , 
if original_source != filtered_source : 
~~~ if args . check : 
~~ if args . in_place : 
~~~ with open_with_encoding ( filename , mode = 'w' , 
encoding = encoding ) as output_file : 
~~~ output_file . write ( filtered_source ) 
~~~ diff = get_diff_text ( 
io . StringIO ( original_source ) . readlines ( ) , 
io . StringIO ( filtered_source ) . readlines ( ) , 
standard_out . write ( '' . join ( diff ) ) 
~~ ~~ ~~ def detect_encoding ( filename , limit_byte_check = - 1 ) : 
~~~ with open ( filename , 'rb' ) as input_file : 
~~~ encoding = _detect_encoding ( input_file . readline ) 
with open_with_encoding ( filename , encoding ) as input_file : 
~~~ input_file . read ( limit_byte_check ) 
~~ ~~ return encoding 
~~ except ( LookupError , SyntaxError , UnicodeDecodeError ) : 
~~~ return 'latin-1' 
~~ ~~ def _detect_encoding ( readline ) : 
~~~ from lib2to3 . pgen2 import tokenize as lib2to3_tokenize 
encoding = lib2to3_tokenize . detect_encoding ( readline ) [ 0 ] 
return encoding 
~~ ~~ def _split_comma_separated ( string ) : 
return set ( text . strip ( ) for text in string . split ( ',' ) if text . strip ( ) ) 
~~ def is_python_file ( filename ) : 
if filename . endswith ( '.py' ) : 
~~~ with open_with_encoding ( 
limit_byte_check = MAX_PYTHON_FILE_DETECTION_BYTES ) as f : 
~~~ text = f . read ( MAX_PYTHON_FILE_DETECTION_BYTES ) 
~~ first_line = text . splitlines ( ) [ 0 ] 
~~ ~~ except ( IOError , IndexError ) : 
~~ if not PYTHON_SHEBANG_REGEX . match ( first_line ) : 
~~ def is_exclude_file ( filename , exclude ) : 
base_name = os . path . basename ( filename ) 
if base_name . startswith ( '.' ) : 
~~ for pattern in exclude : 
~~~ if fnmatch . fnmatch ( base_name , pattern ) : 
~~ if fnmatch . fnmatch ( filename , pattern ) : 
~~ def match_file ( filename , exclude ) : 
if is_exclude_file ( filename , exclude ) : 
~~ if not os . path . isdir ( filename ) and not is_python_file ( filename ) : 
~~ def find_files ( filenames , recursive , exclude ) : 
while filenames : 
~~~ name = filenames . pop ( 0 ) 
if recursive and os . path . isdir ( name ) : 
~~~ for root , directories , children in os . walk ( name ) : 
~~~ filenames += [ os . path . join ( root , f ) for f in children 
if match_file ( os . path . join ( root , f ) , 
exclude ) ] 
directories [ : ] = [ d for d in directories 
if match_file ( os . path . join ( root , d ) , 
~~~ if not is_exclude_file ( name , exclude ) : 
~~ ~~ ~~ ~~ def _main ( argv , standard_out , standard_error ) : 
import argparse 
parser = argparse . ArgumentParser ( description = __doc__ , prog = 'autoflake' ) 
parser . add_argument ( '-c' , '--check' , action = 'store_true' , 
parser . add_argument ( '-i' , '--in-place' , action = 'store_true' , 
parser . add_argument ( '-r' , '--recursive' , action = 'store_true' , 
parser . add_argument ( '--exclude' , metavar = 'globs' , 
parser . add_argument ( '--imports' , 
parser . add_argument ( '--expand-star-imports' , action = 'store_true' , 
'file' ) 
parser . add_argument ( '--remove-all-unused-imports' , action = 'store_true' , 
parser . add_argument ( '--ignore-init-module-imports' , action = 'store_true' , 
'imports' ) 
parser . add_argument ( '--remove-duplicate-keys' , action = 'store_true' , 
parser . add_argument ( '--remove-unused-variables' , action = 'store_true' , 
parser . add_argument ( '--version' , action = 'version' , 
args = parser . parse_args ( argv [ 1 : ] ) 
if args . remove_all_unused_imports and args . imports : 
file = standard_error ) 
return 1 
~~ if args . exclude : 
~~~ args . exclude = _split_comma_separated ( args . exclude ) 
~~~ args . exclude = set ( [ ] ) 
~~ filenames = list ( set ( args . files ) ) 
failure = False 
for name in find_files ( filenames , args . recursive , args . exclude ) : 
~~~ fix_file ( name , args = args , standard_out = standard_out ) 
~~ except IOError as exception : 
~~~ print ( unicode ( exception ) , file = standard_error ) 
failure = True 
~~ ~~ return 1 if failure else 0 
pen = Pen ( ) 
pen . __dict__ = self . __dict__ . copy ( ) 
return pen 
~~ def lookup_color ( c ) : 
import sys 
import gi 
gi . require_version ( 'Gtk' , '3.0' ) 
gi . require_version ( 'PangoCairo' , '1.0' ) 
from gi . repository import Gdk 
~~~ color = Gdk . color_parse ( c ) 
~~~ s = 1.0 / 65535.0 
r = color . red * s 
g = color . green * s 
b = color . blue * s 
a = 1.0 
return r , g , b , a 
~~~ dummy , scheme , index = c . split ( '/' ) 
r , g , b = brewer_colors [ scheme ] [ int ( index ) ] 
~~ except ( ValueError , KeyError ) : 
~~~ s = 1.0 / 255.0 
r = r * s 
g = g * s 
b = b * s 
~~ def draw ( self , cr , highlight = False , bounding = None ) : 
if bounding is None or self . _intersects ( bounding ) : 
~~~ self . _draw ( cr , highlight , bounding ) 
~~ ~~ def _cubic_bernstein_extrema ( p0 , p1 , p2 , p3 ) : 
a = 3. * ( p3 - p0 + 3. * ( p1 - p2 ) ) 
b = 6. * ( p0 + p2 - 2. * p1 ) 
c = 3. * ( p1 - p0 ) 
if a == 0 : 
~~~ if b == 0 : 
~~ d = b * b - 4. * a * c 
if d < 0 : 
~~~ return ( ) 
~~ k = - 2. * a 
if d == 0 : 
~~~ return ( b / k , ) 
~~ r = math . sqrt ( d ) 
return ( ( b + r ) / k , ( b - r ) / k ) 
~~ def _cubic_bernstein ( p0 , p1 , p2 , p3 , t ) : 
u = 1 - t 
return p0 * ( u ** 3 ) + 3 * t * u * ( p1 * u + p2 * t ) + p3 * ( t ** 3 ) 
~~ def _build_choices ( self ) : 
tree_token = u\ % ( self . tree , self . template ) 
context_kwargs = { 'current_app' : 'admin' } 
context = template . Context ( context_kwargs ) if VERSION >= ( 1 , 8 ) else template . Context ( ** context_kwargs ) 
context . update ( { 'request' : object ( ) } ) 
choices_str = sitetree_tree ( 
Parser ( None ) , Token ( token_type = TOKEN_BLOCK , contents = tree_token ) 
) . render ( context ) 
tree_choices = [ ( ITEMS_FIELD_ROOT_ID , self . root_title ) ] 
for line in choices_str . splitlines ( ) : 
~~~ if line . strip ( ) : 
~~~ splitted = line . split ( ':::' ) 
tree_choices . append ( ( splitted [ 0 ] , mark_safe ( splitted [ 1 ] ) ) ) 
~~ ~~ return tree_choices 
~~ def options_getter ( command_options ) : 
def get_options ( option_func = None ) : 
~~~ from optparse import make_option 
from django . core . management . base import BaseCommand 
func = option_func or make_option 
options = tuple ( [ func ( * option . args , ** option . kwargs ) for option in command_options ] ) 
if option_func is None : 
~~~ if VERSION < ( 1 , 8 ) : 
~~~ result = BaseCommand . option_list + options 
~~~ result = options 
~~ return get_options 
~~ def get_sitetree ( ) : 
sitetree = getattr ( _THREAD_LOCAL , _THREAD_SITETREE , None ) 
if sitetree is None : 
~~~ sitetree = SiteTree ( ) 
setattr ( _THREAD_LOCAL , _THREAD_SITETREE , sitetree ) 
~~ return sitetree 
~~ def register_items_hook ( func ) : 
global _ITEMS_PROCESSOR 
global _ITEMS_PROCESSOR_ARGS_LEN 
_ITEMS_PROCESSOR = func 
if func : 
~~~ args_len = len ( getargspec ( func ) . args ) 
if args_len not in { 2 , 3 } : 
~~ _ITEMS_PROCESSOR_ARGS_LEN = args_len 
~~ ~~ def register_dynamic_trees ( trees , * args , ** kwargs ) : 
global _DYNAMIC_TREES 
if _IDX_ORPHAN_TREES not in _DYNAMIC_TREES : 
~~~ _DYNAMIC_TREES [ _IDX_ORPHAN_TREES ] = { } 
~~~ trees = [ trees ] 
trees . extend ( args ) 
~~ for tree in trees or [ ] : 
~~~ if tree is not None and tree [ 'sitetrees' ] is not None : 
~~~ if tree [ 'tree' ] is None : 
~~~ for st in tree [ 'sitetrees' ] : 
~~~ if st . alias not in _DYNAMIC_TREES [ _IDX_ORPHAN_TREES ] : 
~~~ _DYNAMIC_TREES [ _IDX_ORPHAN_TREES ] [ st . alias ] = [ ] 
~~ _DYNAMIC_TREES [ _IDX_ORPHAN_TREES ] [ st . alias ] . append ( st ) 
~~~ index = _IDX_TPL % ( tree [ 'tree' ] , tree [ 'parent_item' ] ) 
if index not in _DYNAMIC_TREES : 
~~~ _DYNAMIC_TREES [ index ] = [ ] 
~~ _DYNAMIC_TREES [ index ] . extend ( tree [ 'sitetrees' ] ) 
~~ ~~ ~~ reset_cache = kwargs . get ( 'reset_cache' , False ) 
if reset_cache : 
~~~ cache_ = get_sitetree ( ) . cache 
cache_ . empty ( ) 
cache_ . reset ( ) 
~~ ~~ def compose_dynamic_tree ( src , target_tree_alias = None , parent_tree_item_alias = None , include_trees = None ) : 
def result ( sitetrees = src ) : 
~~~ if include_trees is not None : 
~~~ sitetrees = [ tree for tree in sitetrees if tree . alias in include_trees ] 
'app' : src , 
'sitetrees' : sitetrees , 
'tree' : target_tree_alias , 
'parent_item' : parent_tree_item_alias } 
~~ if isinstance ( src , six . string_types ) : 
~~~ module = import_app_sitetree_module ( src ) 
return None if module is None else result ( getattr ( module , 'sitetrees' , None ) ) 
~~ except ImportError as e : 
~~~ if settings . DEBUG : 
~~ ~~ return result ( ) 
~~ def init ( self ) : 
cache . get ( 'sitetrees_reset' ) and self . empty ( init = False ) 
self . cache = cache . get ( 
'sitetrees' , { 'sitetrees' : { } , 'parents' : { } , 'items_by_ids' : { } , 'tree_aliases' : { } } ) 
~~ def empty ( self , ** kwargs ) : 
cache . delete ( 'sitetrees' ) 
cache . delete ( 'sitetrees_reset' ) 
kwargs . get ( 'init' , True ) and self . init ( ) 
~~ def get_entry ( self , entry_name , key ) : 
return self . cache [ entry_name ] . get ( key , False ) 
~~ def update_entry_value ( self , entry_name , key , value ) : 
if key not in self . cache [ entry_name ] : 
~~~ self . cache [ entry_name ] [ key ] = { } 
~~ self . cache [ entry_name ] [ key ] . update ( value ) 
~~ def set_entry ( self , entry_name , key , value ) : 
self . cache [ entry_name ] [ key ] = value 
~~ def init ( self , context ) : 
self . cache = Cache ( ) 
self . current_page_context = context 
self . current_request = context . get ( 'request' , None ) if context else None 
self . current_lang = get_language ( ) 
self . _current_app_is_admin = None 
self . _current_user_permissions = _UNSET 
self . _current_items = { } 
~~ def resolve_tree_i18n_alias ( self , alias ) : 
if alias not in _I18N_TREES : 
~~~ return alias 
~~ current_language_code = self . current_lang 
i18n_tree_alias = '%s_%s' % ( alias , current_language_code ) 
trees_count = self . cache . get_entry ( 'tree_aliases' , i18n_tree_alias ) 
if trees_count is False : 
~~~ trees_count = MODEL_TREE_CLASS . objects . filter ( alias = i18n_tree_alias ) . count ( ) 
self . cache . set_entry ( 'tree_aliases' , i18n_tree_alias , trees_count ) 
~~ if trees_count : 
~~~ alias = i18n_tree_alias 
~~ return alias 
~~ def attach_dynamic_tree_items ( tree_alias , src_tree_items ) : 
if not _DYNAMIC_TREES : 
~~~ return src_tree_items 
~~ trees = deepcopy ( _DYNAMIC_TREES ) 
if not src_tree_items : 
~~~ if _IDX_ORPHAN_TREES in trees and tree_alias in trees [ _IDX_ORPHAN_TREES ] : 
~~~ for tree in trees [ _IDX_ORPHAN_TREES ] [ tree_alias ] : 
~~~ items . extend ( tree . dynamic_items ) 
~~~ for static_item in list ( src_tree_items ) : 
~~~ items . append ( static_item ) 
if not static_item . alias : 
~~ idx = _IDX_TPL % ( tree_alias , static_item . alias ) 
if idx not in trees : 
~~ for tree in trees [ idx ] : 
~~~ tree . alias = tree_alias 
for dyn_item in tree . dynamic_items : 
~~~ if dyn_item . parent is None : 
~~~ dyn_item . parent = static_item 
~~ dyn_item . id = generate_id_for ( dyn_item ) 
items . append ( dyn_item ) 
~~ ~~ ~~ idx = _IDX_TPL % ( tree_alias , None ) 
if idx in _DYNAMIC_TREES : 
~~~ trees = deepcopy ( _DYNAMIC_TREES ) 
for tree in trees [ idx ] : 
items . extend ( tree . dynamic_items ) 
~~ ~~ ~~ return items 
~~ def current_app_is_admin ( self ) : 
is_admin = self . _current_app_is_admin 
if is_admin is None : 
~~~ context = self . current_page_context 
current_app = getattr ( 
getattr ( context . get ( 'request' , None ) , 'resolver_match' , None ) , 'app_name' , 
getattr ( context , 'current_app' , None ) ) 
~~~ current_app = context . get ( 'current_app' , '' ) 
~~ is_admin = current_app == ADMIN_APP_NAME 
self . _current_app_is_admin = is_admin 
~~ return is_admin 
~~ def get_sitetree ( self , alias ) : 
cache_ = self . cache 
get_cache_entry = cache_ . get_entry 
set_cache_entry = cache_ . set_entry 
caching_required = False 
if not self . current_app_is_admin ( ) : 
~~~ alias = self . resolve_tree_i18n_alias ( alias ) 
~~ sitetree = get_cache_entry ( 'sitetrees' , alias ) 
if not sitetree : 
~~~ if DYNAMIC_ONLY : 
~~~ sitetree = [ ] 
~~~ sitetree = ( 
MODEL_TREE_ITEM_CLASS . objects . 
select_related ( 'parent' , 'tree' ) . 
prefetch_related ( 'access_permissions__content_type' ) . 
filter ( tree__alias__exact = alias ) . 
order_by ( 'parent__sort_order' , 'sort_order' ) ) 
~~ sitetree = self . attach_dynamic_tree_items ( alias , sitetree ) 
set_cache_entry ( 'sitetrees' , alias , sitetree ) 
caching_required = True 
~~ parents = get_cache_entry ( 'parents' , alias ) 
if not parents : 
~~~ parents = defaultdict ( list ) 
for item in sitetree : 
~~~ parent = getattr ( item , 'parent' ) 
parents [ parent ] . append ( item ) 
~~ set_cache_entry ( 'parents' , alias , parents ) 
~~ if caching_required : 
~~~ cache_update = cache_ . update_entry_value 
~~~ cache_update ( 'items_by_ids' , alias , { item . id : item } ) 
~~ ~~ url = self . url 
calculate_item_depth = self . calculate_item_depth 
~~~ if caching_required : 
~~~ item . has_children = False 
if not hasattr ( item , 'depth' ) : 
~~~ item . depth = calculate_item_depth ( alias , item . id ) 
~~ item . depth_range = range ( item . depth ) 
if item . access_restricted : 
~~~ permissions_src = ( 
item . permissions if getattr ( item , 'is_dynamic' , False ) 
else item . access_permissions . all ( ) ) 
item . perms = set ( 
[ '%s.%s' % ( perm . content_type . app_label , perm . codename ) for perm in permissions_src ] ) 
~~ ~~ item . url_resolved = url ( item ) 
item . title_resolved = LazyTitle ( item . title ) if VARIABLE_TAG_START in item . title else item . title 
item . is_current = False 
item . in_current_branch = False 
~~ self . get_tree_current_item ( alias ) 
if caching_required : 
~~~ cache_ . save ( ) 
~~ return alias , sitetree 
~~ def calculate_item_depth ( self , tree_alias , item_id , depth = 0 ) : 
item = self . get_item_by_id ( tree_alias , item_id ) 
if hasattr ( item , 'depth' ) : 
~~~ depth = item . depth + depth 
~~~ if item . parent is not None : 
~~~ depth = self . calculate_item_depth ( tree_alias , item . parent . id , depth + 1 ) 
~~ ~~ return depth 
~~ def get_tree_current_item ( self , tree_alias ) : 
current_item = self . _current_items . get ( tree_alias , _UNSET ) 
if current_item is not _UNSET : 
~~~ if current_item is not None : 
~~ return current_item 
~~ current_item = None 
if self . current_app_is_admin ( ) : 
~~~ self . _current_items [ tree_alias ] = current_item 
~~ current_url = self . current_request . path 
if isinstance ( current_url , str ) : 
~~~ current_url = current_url . encode ( 'UTF-8' ) 
~~ if current_url : 
~~~ current_url = urlquote ( current_url ) 
~~ for url_item , url in self . _items_urls . items ( ) : 
~~~ if url != current_url : 
~~ url_item . is_current = True 
if url_item . tree . alias == tree_alias : 
~~~ current_item = url_item 
~~ ~~ if current_item is not None : 
~~ def url ( self , sitetree_item , context = None ) : 
context = context or self . current_page_context 
resolve_var = self . resolve_var 
if not isinstance ( sitetree_item , MODEL_TREE_ITEM_CLASS ) : 
~~~ sitetree_item = resolve_var ( sitetree_item , context ) 
~~ resolved_url = self . _items_urls . get ( sitetree_item ) 
if resolved_url is not None : 
~~~ return resolved_url 
~~ if sitetree_item . urlaspattern : 
~~~ url = sitetree_item . url 
view_path = url 
all_arguments = [ ] 
for view_argument in view_path [ 1 : ] : 
~~~ resolved = resolve_var ( view_argument ) 
all_arguments . append ( \ % resolved ) 
~~ view_path = view_path [ 0 ] . strip ( \ ) 
~~~ url_pattern = '%s' % sitetree_item . url 
url_tag ( 
Parser ( None ) , 
Token ( token_type = TOKEN_BLOCK , contents = url_token ) 
resolved_url = context [ 'item.url_resolved' ] or UNRESOLVED_ITEM_MARKER 
~~~ resolved_url = url_pattern 
~~ self . _items_urls [ sitetree_item ] = resolved_url 
return resolved_url 
~~ def init_tree ( self , tree_alias , context ) : 
request = context . get ( 'request' , None ) 
if request is None : 
~~~ raise SiteTreeError ( 
~~ if id ( request ) != id ( self . current_request ) : 
~~~ self . init ( context ) 
~~ tree_alias = self . resolve_var ( tree_alias ) 
tree_alias , sitetree_items = self . get_sitetree ( tree_alias ) 
if not sitetree_items : 
~~~ return None , None 
~~ return tree_alias , sitetree_items 
~~ def get_current_page_attr ( self , attr_name , tree_alias , context ) : 
tree_alias , sitetree_items = self . init_tree ( tree_alias , context ) 
current_item = self . get_tree_current_item ( tree_alias ) 
if current_item is None : 
~~~ if settings . DEBUG and RAISE_ITEMS_ERRORS_ON_DEBUG : 
~~ return getattr ( current_item , attr_name , '' ) 
~~ def get_ancestor_level ( self , current_item , depth = 1 ) : 
if current_item . parent is None : 
~~~ return current_item 
~~ if depth <= 1 : 
~~~ return current_item . parent 
~~ return self . get_ancestor_level ( current_item . parent , depth = depth - 1 ) 
~~ def menu ( self , tree_alias , tree_branches , context ) : 
~~ tree_branches = self . resolve_var ( tree_branches ) 
parent_isnull = False 
parent_ids = [ ] 
parent_aliases = [ ] 
self . tree_climber ( tree_alias , current_item ) 
for branch_id in tree_branches . split ( ',' ) : 
~~~ branch_id = branch_id . strip ( ) 
if branch_id == ALIAS_TRUNK : 
~~~ parent_isnull = True 
~~ elif branch_id == ALIAS_THIS_CHILDREN and current_item is not None : 
~~~ branch_id = current_item . id 
parent_ids . append ( branch_id ) 
~~ elif branch_id == ALIAS_THIS_ANCESTOR_CHILDREN and current_item is not None : 
~~~ branch_id = self . get_ancestor_item ( tree_alias , current_item ) . id 
~~ elif branch_id == ALIAS_THIS_SIBLINGS and current_item is not None and current_item . parent is not None : 
~~~ branch_id = current_item . parent . id 
~~ elif branch_id == ALIAS_THIS_PARENT_SIBLINGS and current_item is not None : 
~~~ branch_id = self . get_ancestor_level ( current_item , depth = 2 ) . id 
~~ elif branch_id . isdigit ( ) : 
~~~ parent_ids . append ( int ( branch_id ) ) 
~~~ parent_aliases . append ( branch_id ) 
~~ ~~ check_access = self . check_access 
menu_items = [ ] 
for item in sitetree_items : 
~~~ if not item . hidden and item . inmenu and check_access ( item , context ) : 
~~~ if item . parent is None : 
~~~ if parent_isnull : 
~~~ menu_items . append ( item ) 
~~~ if item . parent . id in parent_ids or item . parent . alias in parent_aliases : 
~~ ~~ ~~ ~~ menu_items = self . apply_hook ( menu_items , 'menu' ) 
self . update_has_children ( tree_alias , menu_items , 'menu' ) 
return menu_items 
~~ def apply_hook ( self , items , sender ) : 
processor = _ITEMS_PROCESSOR 
if processor is None : 
~~~ return items 
~~ if _ITEMS_PROCESSOR_ARGS_LEN == 2 : 
~~~ return processor ( tree_items = items , tree_sender = sender ) 
~~ return processor ( tree_items = items , tree_sender = sender , context = self . current_page_context ) 
~~ def check_access ( self , item , context ) : 
if hasattr ( self . current_request . user . is_authenticated , '__call__' ) : 
~~~ authenticated = self . current_request . user . is_authenticated ( ) 
~~~ authenticated = self . current_request . user . is_authenticated 
~~ if item . access_loggedin and not authenticated : 
~~ if item . access_guest and authenticated : 
~~ if item . access_restricted : 
~~~ user_perms = self . _current_user_permissions 
if user_perms is _UNSET : 
~~~ user_perms = set ( context [ 'user' ] . get_all_permissions ( ) ) 
self . _current_user_permissions = user_perms 
~~ if item . access_perm_type == MODEL_TREE_ITEM_CLASS . PERM_TYPE_ALL : 
~~~ if len ( item . perms ) != len ( item . perms . intersection ( user_perms ) ) : 
~~~ if not len ( item . perms . intersection ( user_perms ) ) : 
~~ def breadcrumbs ( self , tree_alias , context ) : 
~~ current_item = self . get_tree_current_item ( tree_alias ) 
breadcrumbs = [ ] 
if current_item is not None : 
~~~ context_ = self . current_page_context 
check_access = self . check_access 
get_item_by_id = self . get_item_by_id 
def climb ( base_item ) : 
if base_item . inbreadcrumbs and not base_item . hidden and check_access ( base_item , context_ ) : 
~~~ breadcrumbs . append ( base_item ) 
~~ if hasattr ( base_item , 'parent' ) and base_item . parent is not None : 
~~~ climb ( get_item_by_id ( tree_alias , base_item . parent . id ) ) 
~~ ~~ climb ( current_item ) 
breadcrumbs . reverse ( ) 
~~ items = self . apply_hook ( breadcrumbs , 'breadcrumbs' ) 
self . update_has_children ( tree_alias , items , 'breadcrumbs' ) 
~~ def tree ( self , tree_alias , context ) : 
~~ tree_items = self . filter_items ( self . get_children ( tree_alias , None ) , 'sitetree' ) 
tree_items = self . apply_hook ( tree_items , 'sitetree' ) 
self . update_has_children ( tree_alias , tree_items , 'sitetree' ) 
return tree_items 
~~ def children ( self , parent_item , navigation_type , use_template , context ) : 
parent_item = self . resolve_var ( parent_item , context ) 
tree_alias , tree_items = self . get_sitetree ( parent_item . tree . alias ) 
self . tree_climber ( tree_alias , self . get_tree_current_item ( tree_alias ) ) 
tree_items = self . get_children ( tree_alias , parent_item ) 
tree_items = self . filter_items ( tree_items , navigation_type ) 
tree_items = self . apply_hook ( tree_items , '%s.children' % navigation_type ) 
self . update_has_children ( tree_alias , tree_items , navigation_type ) 
my_template = get_template ( use_template ) 
context . push ( ) 
context [ 'sitetree_items' ] = tree_items 
rendered = my_template . render ( context . flatten ( ) if _CONTEXT_FLATTEN else context ) 
context . pop ( ) 
return rendered 
~~ def get_children ( self , tree_alias , item ) : 
~~~ tree_alias = self . resolve_tree_i18n_alias ( tree_alias ) 
~~ return self . cache . get_entry ( 'parents' , tree_alias ) [ item ] 
~~ def update_has_children ( self , tree_alias , tree_items , navigation_type ) : 
get_children = self . get_children 
filter_items = self . filter_items 
apply_hook = self . apply_hook 
for tree_item in tree_items : 
~~~ children = get_children ( tree_alias , tree_item ) 
children = filter_items ( children , navigation_type ) 
children = apply_hook ( children , '%s.has_children' % navigation_type ) 
tree_item . has_children = len ( children ) > 0 
~~ ~~ def filter_items ( self , items , navigation_type = None ) : 
~~ items_filtered = [ ] 
context = self . current_page_context 
~~~ if item . hidden : 
~~ if not check_access ( item , context ) : 
~~ items_filtered . append ( item ) 
~~ return items_filtered 
~~ def get_ancestor_item ( self , tree_alias , base_item ) : 
parent = None 
if hasattr ( base_item , 'parent' ) and base_item . parent is not None : 
~~~ parent = self . get_ancestor_item ( tree_alias , self . get_item_by_id ( tree_alias , base_item . parent . id ) ) 
~~ if parent is None : 
~~~ return base_item 
~~ return parent 
~~ def tree_climber ( self , tree_alias , base_item ) : 
if base_item is not None : 
~~~ base_item . in_current_branch = True 
~~~ self . tree_climber ( tree_alias , self . get_item_by_id ( tree_alias , base_item . parent . id ) ) 
~~ ~~ ~~ def resolve_var ( self , varname , context = None ) : 
if isinstance ( varname , FilterExpression ) : 
~~~ varname = varname . resolve ( context ) 
~~~ varname = varname . strip ( ) 
~~~ varname = Variable ( varname ) . resolve ( context ) 
~~ except VariableDoesNotExist : 
~~~ varname = varname 
~~ ~~ return varname 
~~ def sitetree_tree ( parser , token ) : 
tokens = token . split_contents ( ) 
use_template = detect_clause ( parser , 'template' , tokens ) 
tokens_num = len ( tokens ) 
if tokens_num in ( 3 , 5 ) : 
~~~ tree_alias = parser . compile_filter ( tokens [ 2 ] ) 
return sitetree_treeNode ( tree_alias , use_template ) 
~~~ raise template . TemplateSyntaxError ( 
\ % tokens [ 0 ] ) 
~~ ~~ def sitetree_children ( parser , token ) : 
clauses_in_places = ( 
tokens_num == 5 and tokens [ 1 ] == 'of' and tokens [ 3 ] == 'for' and tokens [ 4 ] in ( 'menu' , 'sitetree' ) 
if clauses_in_places and use_template is not None : 
~~~ tree_item = tokens [ 2 ] 
navigation_type = tokens [ 4 ] 
return sitetree_childrenNode ( tree_item , navigation_type , use_template ) 
~~ ~~ def sitetree_breadcrumbs ( parser , token ) : 
if tokens_num == 3 : 
return sitetree_breadcrumbsNode ( tree_alias , use_template ) 
~~ ~~ def sitetree_menu ( parser , token ) : 
if tokens_num == 5 and tokens [ 3 ] == 'include' : 
tree_branches = parser . compile_filter ( tokens [ 4 ] ) 
return sitetree_menuNode ( tree_alias , tree_branches , use_template ) 
~~ ~~ def render ( context , tree_items , use_template ) : 
if isinstance ( use_template , FilterExpression ) : 
~~~ use_template = use_template . resolve ( context ) 
~~ content = get_template ( use_template ) . render ( context . flatten ( ) if _CONTEXT_FLATTEN else context ) 
return content 
~~ def for_tag ( cls , parser , token , preposition , error_hint ) : 
if len ( tokens ) >= 3 and tokens [ 1 ] == preposition : 
~~~ as_var = cls . get_as_var ( tokens ) 
tree_alias = parser . compile_filter ( tokens [ 2 ] ) 
return cls ( tree_alias , as_var ) 
~~ raise template . TemplateSyntaxError ( 
~~ def get_model_url_name ( model_nfo , page , with_namespace = False ) : 
if with_namespace : 
~~~ prefix = 'admin:' 
~~ return ( '%s%s_%s' % ( prefix , '%s_%s' % model_nfo , page ) ) . lower ( ) 
~~ def _reregister_tree_admin ( ) : 
~~~ admin . site . unregister ( MODEL_TREE_CLASS ) 
~~ except NotRegistered : 
~~ admin . site . register ( MODEL_TREE_CLASS , _TREE_ADMIN ( ) ) 
~~ def redirects_handler ( * args , ** kwargs ) : 
path = args [ 0 ] . path 
shift = '../' 
if 'delete' in path : 
~~~ shift += '../' 
~~ elif 'history' in path : 
~~~ if 'item_id' not in kwargs : 
~~ ~~ return HttpResponseRedirect ( path + shift ) 
~~ def _redirect ( self , request , response ) : 
if '_addanother' in request . POST : 
~~~ return HttpResponseRedirect ( '../item_add/' ) 
~~ elif '_save' in request . POST : 
~~~ return HttpResponseRedirect ( '../' ) 
~~ elif '_continue' in request . POST : 
~~ return HttpResponseRedirect ( '' ) 
~~ def response_add ( self , request , obj , post_url_continue = None , ** kwargs ) : 
if post_url_continue is None : 
~~~ post_url_continue = '../item_%s/' % obj . pk 
~~ return self . _redirect ( request , super ( TreeItemAdmin , self ) . response_add ( request , obj , post_url_continue ) ) 
~~ def response_change ( self , request , obj , ** kwargs ) : 
return self . _redirect ( request , super ( TreeItemAdmin , self ) . response_change ( request , obj ) ) 
~~ def get_form ( self , request , obj = None , ** kwargs ) : 
if obj is not None and obj . parent is not None : 
~~~ self . previous_parent = obj . parent 
previous_parent_id = self . previous_parent . id 
~~~ previous_parent_id = None 
~~ my_choice_field = TreeItemChoiceField ( self . tree , initial = previous_parent_id ) 
form = super ( TreeItemAdmin , self ) . get_form ( request , obj , ** kwargs ) 
my_choice_field . label = form . base_fields [ 'parent' ] . label 
my_choice_field . help_text = form . base_fields [ 'parent' ] . help_text 
my_choice_field . widget = form . base_fields [ 'parent' ] . widget 
form . base_fields [ 'parent' ] = my_choice_field 
if not getattr ( self , 'known_url_names' , False ) : 
~~~ self . known_url_names = [ ] 
self . known_url_rules = [ ] 
resolver = get_resolver ( get_urlconf ( ) ) 
for ns , ( url_prefix , ns_resolver ) in resolver . namespace_dict . items ( ) : 
~~~ if ns != 'admin' : 
~~~ self . _stack_known_urls ( ns_resolver . reverse_dict , ns ) 
~~ ~~ self . _stack_known_urls ( resolver . reverse_dict ) 
self . known_url_rules = sorted ( self . known_url_rules ) 
~~ form . known_url_names_hint = _ ( 
form . known_url_names = self . known_url_names 
form . known_url_rules = self . known_url_rules 
return form 
~~ def get_tree ( self , request , tree_id , item_id = None ) : 
if tree_id is None : 
~~~ tree_id = self . get_object ( request , item_id ) . tree_id 
~~ self . tree = MODEL_TREE_CLASS . _default_manager . get ( pk = tree_id ) 
self . tree . verbose_name_plural = self . tree . _meta . verbose_name_plural 
self . tree . urls = _TREE_URLS 
return self . tree 
~~ def item_move ( self , request , tree_id , item_id , direction ) : 
current_item = MODEL_TREE_ITEM_CLASS . _default_manager . get ( pk = item_id ) 
if direction == 'up' : 
~~~ sort_order = 'sort_order' 
~~~ sort_order = '-sort_order' 
~~ siblings = MODEL_TREE_ITEM_CLASS . _default_manager . filter ( 
parent = current_item . parent , 
tree = current_item . tree 
) . order_by ( sort_order ) 
previous_item = None 
for item in siblings : 
~~~ if item != current_item : 
~~~ previous_item = item 
~~ ~~ if previous_item is not None : 
~~~ current_item_sort_order = current_item . sort_order 
previous_item_sort_order = previous_item . sort_order 
current_item . sort_order = previous_item_sort_order 
previous_item . sort_order = current_item_sort_order 
current_item . save ( ) 
previous_item . save ( ) 
~~ return HttpResponseRedirect ( '../../' ) 
~~ def save_model ( self , request , obj , form , change ) : 
if change : 
~~~ if obj . parent is not None and obj . parent . id == obj . id : 
~~~ obj . parent = self . previous_parent 
messages . warning ( 
request , _ ( "Item\ ) , '' , True ) 
~~ ~~ obj . tree = self . tree 
obj . save ( ) 
~~ def get_urls ( self ) : 
urls = super ( TreeAdmin , self ) . get_urls ( ) 
prefix_change = 'change/' if DJANGO_POST_19 else '' 
sitetree_urls = [ 
url ( r'^change/$' , redirects_handler , name = get_tree_item_url_name ( 'changelist' ) ) , 
url ( r'^((?P<tree_id>\\d+)/)?%sitem_add/$' % prefix_change , 
self . admin_site . admin_view ( self . tree_admin . item_add ) , name = get_tree_item_url_name ( 'add' ) ) , 
url ( r'^(?P<tree_id>\\d+)/%sitem_(?P<item_id>\\d+)/$' % prefix_change , 
self . admin_site . admin_view ( self . tree_admin . item_edit ) , name = get_tree_item_url_name ( 'change' ) ) , 
url ( r'^%sitem_(?P<item_id>\\d+)/$' % prefix_change , 
url ( r'^((?P<tree_id>\\d+)/)?%sitem_(?P<item_id>\\d+)/delete/$' % prefix_change , 
self . admin_site . admin_view ( self . tree_admin . item_delete ) , name = get_tree_item_url_name ( 'delete' ) ) , 
url ( r'^((?P<tree_id>\\d+)/)?%sitem_(?P<item_id>\\d+)/history/$' % prefix_change , 
self . admin_site . admin_view ( self . tree_admin . item_history ) , name = get_tree_item_url_name ( 'history' ) ) , 
url ( r'^(?P<tree_id>\\d+)/%sitem_(?P<item_id>\\d+)/move_(?P<direction>(up|down))/$' % prefix_change , 
self . admin_site . admin_view ( self . tree_admin . item_move ) , name = get_tree_item_url_name ( 'move' ) ) , 
if not DJANGO_POST_19 : 
~~~ sitetree_urls = patterns_func ( '' , * sitetree_urls ) 
~~ if SMUGGLER_INSTALLED : 
~~~ sitetree_urls += ( url ( r'^dump_all/$' , self . admin_site . admin_view ( self . dump_view ) , name = 'sitetree_dump' ) , ) 
~~ return sitetree_urls + urls 
~~ def dump_view ( cls , request ) : 
from smuggler . views import dump_to_response 
return dump_to_response ( request , [ MODEL_TREE , MODEL_TREE_ITEM ] , filename_prefix = 'sitetrees' ) 
~~ def tree ( alias , title = '' , items = None , ** kwargs ) : 
tree_obj = get_tree_model ( ) ( alias = alias , title = title , ** kwargs ) 
tree_obj . id = generate_id_for ( tree_obj ) 
tree_obj . is_dynamic = True 
if items is not None : 
~~~ tree_obj . dynamic_items = [ ] 
def traverse ( items ) : 
~~~ item . tree = tree_obj 
tree_obj . dynamic_items . append ( item ) 
if hasattr ( item , 'dynamic_children' ) : 
~~~ traverse ( item . dynamic_children ) 
~~ ~~ ~~ traverse ( items ) 
~~ return tree_obj 
~~ def item ( 
title , url , children = None , url_as_pattern = True , hint = '' , alias = '' , description = '' , 
in_menu = True , in_breadcrumbs = True , in_sitetree = True , 
access_loggedin = False , access_guest = False , 
access_by_perms = None , perms_mode_all = True , ** kwargs ) : 
item_obj = get_tree_item_model ( ) ( 
title = title , url = url , urlaspattern = url_as_pattern , 
hint = hint , alias = alias , description = description , inmenu = in_menu , 
insitetree = in_sitetree , inbreadcrumbs = in_breadcrumbs , 
access_loggedin = access_loggedin , access_guest = access_guest , 
item_obj . id = generate_id_for ( item_obj ) 
item_obj . is_dynamic = True 
item_obj . dynamic_children = [ ] 
cleaned_permissions = [ ] 
if access_by_perms : 
~~~ if not isinstance ( access_by_perms , list ) : 
~~~ access_by_perms = [ access_by_perms ] 
~~ for perm in access_by_perms : 
~~~ if isinstance ( perm , six . string_types ) : 
~~~ app , codename = perm . split ( '.' ) 
~~~ perm = Permission . objects . get ( codename = codename , content_type__app_label = app ) 
~~ except Permission . DoesNotExist : 
~~ ~~ elif not isinstance ( perm , ( int , Permission ) ) : 
~~ cleaned_permissions . append ( perm ) 
~~ ~~ item_obj . permissions = cleaned_permissions or [ ] 
item_obj . access_perm_type = item_obj . PERM_TYPE_ALL if perms_mode_all else item_obj . PERM_TYPE_ANY 
if item_obj . permissions : 
~~~ item_obj . access_restricted = True 
~~ if children is not None : 
~~~ for child in children : 
~~~ child . parent = item_obj 
item_obj . dynamic_children . append ( child ) 
~~ ~~ return item_obj 
~~ def import_app_sitetree_module ( app ) : 
module_name = settings . APP_MODULE_NAME 
module = import_module ( app ) 
~~~ sub_module = import_module ( '%s.%s' % ( app , module_name ) ) 
return sub_module 
~~~ if module_has_submodule ( module , module_name ) : 
~~ ~~ def import_project_sitetree_modules ( ) : 
from django . conf import settings as django_settings 
submodules = [ ] 
for app in django_settings . INSTALLED_APPS : 
~~~ module = import_app_sitetree_module ( app ) 
if module is not None : 
~~~ submodules . append ( module ) 
~~ ~~ return submodules 
~~ def get_app_n_model ( settings_entry_name ) : 
~~~ app_name , model_name = getattr ( settings , settings_entry_name ) . split ( '.' ) 
~~~ raise ImproperlyConfigured ( 
~~ return app_name , model_name 
~~ def get_model_class ( settings_entry_name ) : 
app_name , model_name = get_app_n_model ( settings_entry_name ) 
~~~ model = apps_get_model ( app_name , model_name ) 
~~ except ( LookupError , ValueError ) : 
~~~ model = None 
~~ if model is None : 
~~ async def asgi_send ( self , message : dict ) -> None : 
if message [ "type" ] == "http.response.start" and self . state == ASGIHTTPState . REQUEST : 
~~~ self . response = message 
~~ elif message [ "type" ] == "http.response.body" and self . state in { 
ASGIHTTPState . REQUEST , 
ASGIHTTPState . RESPONSE , 
} : 
~~~ if self . state == ASGIHTTPState . REQUEST : 
~~~ headers = build_and_validate_headers ( self . response [ "headers" ] ) 
headers . extend ( self . response_headers ( ) ) 
await self . asend ( 
h11 . Response ( status_code = int ( self . response [ "status" ] ) , headers = headers ) 
self . state = ASGIHTTPState . RESPONSE 
not suppress_body ( self . scope [ "method" ] , int ( self . response [ "status" ] ) ) 
and message . get ( "body" , b"" ) != b"" 
~~~ await self . asend ( h11 . Data ( data = bytes ( message [ "body" ] ) ) ) 
~~ if not message . get ( "more_body" , False ) : 
~~~ if self . state != ASGIHTTPState . CLOSED : 
~~~ await self . asend ( h11 . EndOfMessage ( ) ) 
await self . asgi_put ( { "type" : "http.disconnect" } ) 
self . state = ASGIHTTPState . CLOSED 
~~~ raise UnexpectedMessage ( self . state , message [ "type" ] ) 
~~ ~~ async def asgi_send ( self , message : dict ) -> None : 
if message [ "type" ] == "websocket.accept" and self . state == ASGIWebsocketState . HANDSHAKE : 
~~~ headers = build_and_validate_headers ( message . get ( "headers" , [ ] ) ) 
raise_if_subprotocol_present ( headers ) 
AcceptConnection ( 
extensions = [ PerMessageDeflate ( ) ] , 
extra_headers = headers , 
subprotocol = message . get ( "subprotocol" ) , 
self . state = ASGIWebsocketState . CONNECTED 
self . config . access_logger . access ( 
self . scope , { "status" : 101 , "headers" : [ ] } , time ( ) - self . start_time 
message [ "type" ] == "websocket.http.response.start" 
and self . state == ASGIWebsocketState . HANDSHAKE 
self . config . access_logger . access ( self . scope , self . response , time ( ) - self . start_time ) 
~~ elif message [ "type" ] == "websocket.http.response.body" and self . state in { 
ASGIWebsocketState . HANDSHAKE , 
ASGIWebsocketState . RESPONSE , 
~~~ await self . _asgi_send_rejection ( message ) 
~~ elif message [ "type" ] == "websocket.send" and self . state == ASGIWebsocketState . CONNECTED : 
~~~ data : Union [ bytes , str ] 
if message . get ( "bytes" ) is not None : 
~~~ await self . asend ( BytesMessage ( data = bytes ( message [ "bytes" ] ) ) ) 
~~ elif not isinstance ( message [ "text" ] , str ) : 
~~~ raise TypeError ( f"{message[\ ) 
~~~ await self . asend ( TextMessage ( data = message [ "text" ] ) ) 
~~ ~~ elif message [ "type" ] == "websocket.close" and self . state == ASGIWebsocketState . HANDSHAKE : 
~~~ await self . send_http_error ( 403 ) 
self . state = ASGIWebsocketState . HTTPCLOSED 
~~ elif message [ "type" ] == "websocket.close" : 
~~~ await self . asend ( CloseConnection ( code = int ( message [ "code" ] ) ) ) 
self . state = ASGIWebsocketState . CLOSED 
~~ ~~ async def serve ( app : ASGIFramework , config : Config ) -> None : 
if config . debug : 
~~ if config . workers != 1 : 
~~ if config . worker_class != "asyncio" : 
~~ await worker_serve ( app , config ) 
~~ async def serve ( 
app : ASGIFramework , 
config : Config , 
task_status : trio . _core . _run . _TaskStatus = trio . TASK_STATUS_IGNORED , 
) -> None : 
~~ await worker_serve ( app , config , task_status = task_status ) 
~~ def from_mapping ( 
cls : Type [ "Config" ] , mapping : Optional [ Mapping [ str , Any ] ] = None , ** kwargs : Any 
) -> "Config" : 
mappings : Dict [ str , Any ] = { } 
if mapping is not None : 
~~~ mappings . update ( mapping ) 
~~ mappings . update ( kwargs ) 
config = cls ( ) 
for key , value in mappings . items ( ) : 
~~~ setattr ( config , key , value ) 
~~ def from_pyfile ( cls : Type [ "Config" ] , filename : FilePath ) -> "Config" : 
file_path = os . fspath ( filename ) 
spec = importlib . util . spec_from_file_location ( "module.name" , file_path ) 
module = importlib . util . module_from_spec ( spec ) 
return cls . from_object ( module ) 
~~ def from_toml ( cls : Type [ "Config" ] , filename : FilePath ) -> "Config" : 
with open ( file_path ) as file_ : 
~~~ data = toml . load ( file_ ) 
~~ return cls . from_mapping ( data ) 
~~ def from_object ( cls : Type [ "Config" ] , instance : Union [ object , str ] ) -> "Config" : 
if isinstance ( instance , str ) : 
~~~ path , config = instance . rsplit ( "." , 1 ) 
~~~ path = instance 
instance = importlib . import_module ( instance ) 
~~~ module = importlib . import_module ( path ) 
instance = getattr ( module , config ) 
~~ ~~ mapping = { 
key : getattr ( instance , key ) 
for key in dir ( instance ) 
if not isinstance ( getattr ( instance , key ) , types . ModuleType ) 
return cls . from_mapping ( mapping ) 
~~ def emit_spans ( self ) : 
if self . firehose_handler : 
~~~ self . _emit_spans_with_span_sender ( 
ZipkinBatchSender ( self . firehose_handler , 
self . max_span_batch_size , 
self . encoder ) 
~~ if not self . zipkin_attrs . is_sampled : 
~~~ self . _get_tracer ( ) . clear ( ) 
~~ span_sender = ZipkinBatchSender ( self . transport_handler , 
self . _emit_spans_with_span_sender ( span_sender ) 
self . _get_tracer ( ) . clear ( ) 
~~ def create_attrs_for_span ( 
sample_rate = 100.0 , 
trace_id = None , 
span_id = None , 
use_128bit_trace_id = False , 
if trace_id is None : 
~~~ if use_128bit_trace_id : 
~~~ trace_id = generate_random_128bit_string ( ) 
~~~ trace_id = generate_random_64bit_string ( ) 
~~ ~~ if span_id is None : 
~~~ span_id = generate_random_64bit_string ( ) 
~~ if sample_rate == 0.0 : 
~~~ is_sampled = False 
~~~ is_sampled = ( random . random ( ) * 100 ) < sample_rate 
~~ return ZipkinAttrs ( 
trace_id = trace_id , 
span_id = span_id , 
parent_span_id = None , 
flags = '0' , 
is_sampled = is_sampled , 
~~ def create_http_headers_for_new_span ( context_stack = None , tracer = None ) : 
if tracer : 
~~~ zipkin_attrs = tracer . get_zipkin_attrs ( ) 
~~ elif context_stack : 
~~~ zipkin_attrs = context_stack . get ( ) 
~~~ zipkin_attrs = get_default_tracer ( ) . get_zipkin_attrs ( ) 
~~ if not zipkin_attrs : 
'X-B3-TraceId' : zipkin_attrs . trace_id , 
'X-B3-SpanId' : generate_random_64bit_string ( ) , 
'X-B3-ParentSpanId' : zipkin_attrs . span_id , 
'X-B3-Flags' : '0' , 
'X-B3-Sampled' : '1' if zipkin_attrs . is_sampled else '0' , 
~~ def _get_current_context ( self ) : 
if self . _is_local_root_span : 
~~~ if self . sample_rate is not None : 
~~~ if self . zipkin_attrs_override and not self . zipkin_attrs_override . is_sampled : 
~~~ return True , create_attrs_for_span ( 
sample_rate = self . sample_rate , 
trace_id = self . zipkin_attrs_override . trace_id , 
~~ elif not self . zipkin_attrs_override : 
use_128bit_trace_id = self . use_128bit_trace_id , 
~~ ~~ if self . firehose_handler and not self . zipkin_attrs_override : 
sample_rate = 0.0 , 
~~ return False , self . zipkin_attrs_override 
~~~ existing_zipkin_attrs = self . get_tracer ( ) . get_zipkin_attrs ( ) 
if existing_zipkin_attrs : 
~~~ return False , ZipkinAttrs ( 
trace_id = existing_zipkin_attrs . trace_id , 
span_id = generate_random_64bit_string ( ) , 
parent_span_id = existing_zipkin_attrs . span_id , 
flags = existing_zipkin_attrs . flags , 
is_sampled = existing_zipkin_attrs . is_sampled , 
~~ ~~ return False , None 
self . do_pop_attrs = False 
report_root_timestamp , self . zipkin_attrs = self . _get_current_context ( ) 
if not self . zipkin_attrs : 
~~~ return self 
~~ self . get_tracer ( ) . push_zipkin_attrs ( self . zipkin_attrs ) 
self . do_pop_attrs = True 
self . start_timestamp = time . time ( ) 
~~~ if not self . zipkin_attrs . is_sampled and not self . firehose_handler : 
~~ if self . get_tracer ( ) . is_transport_configured ( ) : 
~~ endpoint = create_endpoint ( self . port , self . service_name , self . host ) 
self . logging_context = ZipkinLoggingContext ( 
self . zipkin_attrs , 
endpoint , 
self . span_name , 
self . transport_handler , 
report_root_timestamp or self . report_root_timestamp_override , 
self . get_tracer , 
self . service_name , 
binary_annotations = self . binary_annotations , 
add_logging_annotation = self . add_logging_annotation , 
client_context = self . kind == Kind . CLIENT , 
max_span_batch_size = self . max_span_batch_size , 
firehose_handler = self . firehose_handler , 
encoding = self . encoding , 
self . logging_context . start ( ) 
self . get_tracer ( ) . set_transport_configured ( configured = True ) 
~~ def stop ( self , _exc_type = None , _exc_value = None , _exc_traceback = None ) : 
if self . do_pop_attrs : 
~~~ self . get_tracer ( ) . pop_zipkin_attrs ( ) 
~~ if not self . get_tracer ( ) . is_transport_configured ( ) : 
~~ if any ( ( _exc_type , _exc_value , _exc_traceback ) ) : 
self . update_binary_annotations ( { 
ERROR_KEY : error_msg , 
~~ if self . logging_context : 
~~~ self . logging_context . stop ( ) 
repr ( ex ) , 
log . error ( err_msg ) 
~~~ self . logging_context = None 
self . get_tracer ( ) . clear ( ) 
self . get_tracer ( ) . set_transport_configured ( configured = False ) 
~~ ~~ end_timestamp = time . time ( ) 
if self . duration : 
~~~ duration = self . duration 
~~~ duration = end_timestamp - self . start_timestamp 
self . get_tracer ( ) . add_span ( Span ( 
trace_id = self . zipkin_attrs . trace_id , 
name = self . span_name , 
parent_id = self . zipkin_attrs . parent_span_id , 
span_id = self . zipkin_attrs . span_id , 
kind = self . kind , 
timestamp = self . timestamp if self . timestamp else self . start_timestamp , 
duration = duration , 
annotations = self . annotations , 
local_endpoint = endpoint , 
remote_endpoint = self . remote_endpoint , 
tags = self . binary_annotations , 
~~ def update_binary_annotations ( self , extra_annotations ) : 
if not self . logging_context : 
~~~ self . binary_annotations . update ( extra_annotations ) 
~~~ self . logging_context . tags . update ( extra_annotations ) 
~~ ~~ def add_sa_binary_annotation ( 
port = 0 , 
service_name = 'unknown' , 
host = '127.0.0.1' , 
if self . kind != Kind . CLIENT : 
~~ remote_endpoint = create_endpoint ( 
service_name = service_name , 
~~~ if self . remote_endpoint is not None : 
~~ self . remote_endpoint = remote_endpoint 
~~~ if self . logging_context . remote_endpoint is not None : 
~~ self . logging_context . remote_endpoint = remote_endpoint 
~~ ~~ def override_span_name ( self , name ) : 
self . span_name = name 
if self . logging_context : 
~~~ self . logging_context . span_name = name 
~~ ~~ def create_endpoint ( port = None , service_name = None , host = None , use_defaults = True ) : 
if use_defaults : 
~~~ if port is None : 
~~~ port = 0 
~~ if service_name is None : 
~~~ service_name = 'unknown' 
~~ if host is None : 
~~~ host = socket . gethostbyname ( socket . gethostname ( ) ) 
~~~ host = '127.0.0.1' 
~~ ~~ ~~ ipv4 = None 
ipv6 = None 
if host : 
~~~ socket . inet_pton ( socket . AF_INET , host ) 
ipv4 = host 
~~ except socket . error : 
~~~ socket . inet_pton ( socket . AF_INET6 , host ) 
ipv6 = host 
~~ ~~ ~~ return Endpoint ( 
ipv4 = ipv4 , 
ipv6 = ipv6 , 
~~ def copy_endpoint_with_new_service_name ( endpoint , new_service_name ) : 
return Endpoint ( 
service_name = new_service_name , 
ipv4 = endpoint . ipv4 , 
ipv6 = endpoint . ipv6 , 
port = endpoint . port , 
~~ def build_v1_span ( self ) : 
full_annotations = OrderedDict ( [ 
( 'cs' , self . timestamp ) , 
( 'sr' , self . timestamp ) , 
( 'ss' , self . timestamp + self . duration ) , 
( 'cr' , self . timestamp + self . duration ) , 
if self . kind != Kind . LOCAL : 
~~~ for ann in _DROP_ANNOTATIONS_BY_KIND [ self . kind ] : 
~~~ del full_annotations [ ann ] 
~~ ~~ full_annotations . update ( self . annotations ) 
return _V1Span ( 
trace_id = self . trace_id , 
name = self . name , 
parent_id = self . parent_id , 
id = self . span_id , 
timestamp = self . timestamp if self . shared is False else None , 
duration = self . duration if self . shared is False else None , 
endpoint = self . local_endpoint , 
annotations = full_annotations , 
binary_annotations = self . tags , 
~~ def encode_pb_list ( pb_spans ) : 
pb_list = zipkin_pb2 . ListOfSpans ( ) 
pb_list . spans . extend ( pb_spans ) 
return pb_list . SerializeToString ( ) 
~~ def create_protobuf_span ( span ) : 
pb_kwargs = { } 
pb_kwargs [ 'trace_id' ] = _hex_to_bytes ( span . trace_id ) 
if span . parent_id : 
~~~ pb_kwargs [ 'parent_id' ] = _hex_to_bytes ( span . parent_id ) 
~~ pb_kwargs [ 'id' ] = _hex_to_bytes ( span . span_id ) 
pb_kind = _get_protobuf_kind ( span . kind ) 
if pb_kind : 
~~~ pb_kwargs [ 'kind' ] = pb_kind 
~~ if span . name : 
~~~ pb_kwargs [ 'name' ] = span . name 
~~ if span . timestamp : 
~~~ pb_kwargs [ 'timestamp' ] = int ( span . timestamp * 1000 * 1000 ) 
~~ if span . duration : 
~~~ pb_kwargs [ 'duration' ] = int ( span . duration * 1000 * 1000 ) 
~~ if span . local_endpoint : 
~~~ pb_kwargs [ 'local_endpoint' ] = _convert_endpoint ( span . local_endpoint ) 
~~ if span . remote_endpoint : 
~~~ pb_kwargs [ 'remote_endpoint' ] = _convert_endpoint ( span . remote_endpoint ) 
~~ if len ( span . annotations ) > 0 : 
~~~ pb_kwargs [ 'annotations' ] = _convert_annotations ( span . annotations ) 
~~ if len ( span . tags ) > 0 : 
~~~ pb_kwargs [ 'tags' ] = span . tags 
~~ if span . debug : 
~~~ pb_kwargs [ 'debug' ] = span . debug 
~~ if span . shared : 
~~~ pb_kwargs [ 'shared' ] = span . shared 
~~ return zipkin_pb2 . Span ( ** pb_kwargs ) 
~~ def _hex_to_bytes ( hex_id ) : 
if len ( hex_id ) <= 16 : 
~~~ int_id = unsigned_hex_to_signed_int ( hex_id ) 
return struct . pack ( '>q' , int_id ) 
~~~ high_id = unsigned_hex_to_signed_int ( hex_id [ : - 16 ] ) 
high_bin = struct . pack ( '>q' , high_id ) 
low_id = unsigned_hex_to_signed_int ( hex_id [ - 16 : ] ) 
low_bin = struct . pack ( '>q' , low_id ) 
return high_bin + low_bin 
~~ ~~ def _get_protobuf_kind ( kind ) : 
if kind == Kind . CLIENT : 
~~~ return zipkin_pb2 . Span . CLIENT 
~~ elif kind == Kind . SERVER : 
~~~ return zipkin_pb2 . Span . SERVER 
~~ elif kind == Kind . PRODUCER : 
~~~ return zipkin_pb2 . Span . PRODUCER 
~~ elif kind == Kind . CONSUMER : 
~~~ return zipkin_pb2 . Span . CONSUMER 
~~ def _convert_endpoint ( endpoint ) : 
pb_endpoint = zipkin_pb2 . Endpoint ( ) 
if endpoint . service_name : 
~~~ pb_endpoint . service_name = endpoint . service_name 
~~ if endpoint . port and endpoint . port != 0 : 
~~~ pb_endpoint . port = endpoint . port 
~~ if endpoint . ipv4 : 
~~~ pb_endpoint . ipv4 = socket . inet_pton ( socket . AF_INET , endpoint . ipv4 ) 
~~ if endpoint . ipv6 : 
~~~ pb_endpoint . ipv6 = socket . inet_pton ( socket . AF_INET6 , endpoint . ipv6 ) 
~~ return pb_endpoint 
~~ def _convert_annotations ( annotations ) : 
pb_annotations = [ ] 
for value , ts in annotations . items ( ) : 
~~~ pb_annotations . append ( zipkin_pb2 . Annotation ( 
timestamp = int ( ts * 1000 * 1000 ) , 
~~ return pb_annotations 
~~ def create_annotation ( timestamp , value , host ) : 
return zipkin_core . Annotation ( timestamp = timestamp , value = value , host = host ) 
~~ def create_binary_annotation ( key , value , annotation_type , host ) : 
return zipkin_core . BinaryAnnotation ( 
annotation_type = annotation_type , 
~~ def create_endpoint ( port = 0 , service_name = 'unknown' , ipv4 = None , ipv6 = None ) : 
ipv4_int = 0 
ipv6_binary = None 
if ipv4 : 
~~~ ipv4_int = struct . unpack ( '!i' , socket . inet_pton ( socket . AF_INET , ipv4 ) ) [ 0 ] 
~~ if ipv6 : 
~~~ ipv6_binary = socket . inet_pton ( socket . AF_INET6 , ipv6 ) 
~~ port = struct . unpack ( 'h' , struct . pack ( 'H' , port ) ) [ 0 ] 
return zipkin_core . Endpoint ( 
ipv4 = ipv4_int , 
ipv6 = ipv6_binary , 
~~ def copy_endpoint_with_new_service_name ( endpoint , service_name ) : 
~~ def annotation_list_builder ( annotations , host ) : 
create_annotation ( int ( timestamp * 1000000 ) , key , host ) 
for key , timestamp in annotations . items ( ) 
~~ def binary_annotation_list_builder ( binary_annotations , host ) : 
ann_type = zipkin_core . AnnotationType . STRING 
create_binary_annotation ( key , str ( value ) , ann_type , host ) 
for key , value in binary_annotations . items ( ) 
~~ def create_span ( 
span_id , 
parent_span_id , 
trace_id , 
span_name , 
annotations , 
binary_annotations , 
timestamp_s , 
duration_s , 
trace_id_length = len ( trace_id ) 
trace_id_high = None 
if trace_id_length > 16 : 
~~~ assert trace_id_length == 32 
trace_id , trace_id_high = trace_id [ 16 : ] , trace_id [ : 16 ] 
~~ if trace_id_high : 
~~~ trace_id_high = unsigned_hex_to_signed_int ( trace_id_high ) 
~~ span_dict = { 
'trace_id' : unsigned_hex_to_signed_int ( trace_id ) , 
'name' : span_name , 
'id' : unsigned_hex_to_signed_int ( span_id ) , 
'annotations' : annotations , 
'binary_annotations' : binary_annotations , 
'timestamp' : int ( timestamp_s * 1000000 ) if timestamp_s else None , 
'duration' : int ( duration_s * 1000000 ) if duration_s else None , 
'trace_id_high' : trace_id_high , 
if parent_span_id : 
~~~ span_dict [ 'parent_id' ] = unsigned_hex_to_signed_int ( parent_span_id ) 
~~ return zipkin_core . Span ( ** span_dict ) 
~~ def span_to_bytes ( thrift_span ) : 
transport = TMemoryBuffer ( ) 
protocol = TBinaryProtocol ( transport ) 
thrift_span . write ( protocol ) 
return bytes ( transport . getvalue ( ) ) 
write_list_begin ( transport , TType . STRUCT , len ( binary_thrift_obj_list ) ) 
for thrift_bin in binary_thrift_obj_list : 
~~~ transport . write ( thrift_bin ) 
~~ return bytes ( transport . getvalue ( ) ) 
~~ def detect_span_version_and_encoding ( message ) : 
if isinstance ( message , six . string_types ) : 
~~~ if six . PY2 : 
~~ ~~ if len ( message ) < 2 : 
~~ if six . byte2int ( message ) <= 16 : 
~~~ if six . byte2int ( message ) == 10 and six . byte2int ( message [ 1 : 2 ] ) != 0 : 
~~~ return Encoding . V2_PROTO3 
~~ return Encoding . V1_THRIFT 
~~ str_msg = message . decode ( 'utf-8' ) 
if str_msg [ 0 ] == '[' : 
~~~ span_list = json . loads ( str_msg ) 
if len ( span_list ) > 0 : 
~~~ for span in span_list : 
~~~ if any ( word in span for word in _V2_ATTRIBUTES ) : 
~~~ return Encoding . V2_JSON 
'binaryAnnotations' in span or 
'annotations' in span and 
'endpoint' in span [ 'annotations' ] 
~~~ return Encoding . V1_JSON 
~~ ~~ return Encoding . V2_JSON 
~~ def convert_spans ( spans , output_encoding , input_encoding = None ) : 
if not isinstance ( input_encoding , Encoding ) : 
~~~ input_encoding = detect_span_version_and_encoding ( message = spans ) 
~~ if input_encoding == output_encoding : 
~~~ return spans 
~~ decoder = get_decoder ( input_encoding ) 
encoder = get_encoder ( output_encoding ) 
decoded_spans = decoder . decode_spans ( spans ) 
output_spans = [ ] 
for span in decoded_spans : 
~~~ output_spans . append ( encoder . encode_span ( span ) ) 
~~ return encoder . encode_queue ( output_spans ) 
~~ def push_zipkin_attrs ( zipkin_attr ) : 
from py_zipkin . storage import ThreadLocalStack 
return ThreadLocalStack ( ) . push ( zipkin_attr ) 
~~ def generate_random_128bit_string ( ) : 
t = int ( time . time ( ) ) 
lower_96 = random . getrandbits ( 96 ) 
return '{:032x}' . format ( ( t << 96 ) | lower_96 ) 
~~ def get_encoder ( encoding ) : 
if encoding == Encoding . V1_THRIFT : 
~~~ return _V1ThriftEncoder ( ) 
~~ if encoding == Encoding . V1_JSON : 
~~~ return _V1JSONEncoder ( ) 
~~ if encoding == Encoding . V2_JSON : 
~~~ return _V2JSONEncoder ( ) 
~~ if encoding == Encoding . V2_PROTO3 : 
~~~ return _V2ProtobufEncoder ( ) 
~~ def fits ( self , current_count , current_size , max_size , new_span ) : 
return thrift . LIST_HEADER_SIZE + current_size + len ( new_span ) <= max_size 
~~ def encode_span ( self , v2_span ) : 
span = v2_span . build_v1_span ( ) 
thrift_endpoint = thrift . create_endpoint ( 
span . endpoint . port , 
span . endpoint . service_name , 
span . endpoint . ipv4 , 
span . endpoint . ipv6 , 
thrift_annotations = thrift . annotation_list_builder ( 
span . annotations , 
thrift_endpoint , 
thrift_binary_annotations = thrift . binary_annotation_list_builder ( 
span . binary_annotations , 
if v2_span . remote_endpoint : 
~~~ self . encode_remote_endpoint ( 
v2_span . remote_endpoint , 
v2_span . kind , 
thrift_binary_annotations , 
~~ thrift_span = thrift . create_span ( 
span . id , 
span . parent_id , 
span . trace_id , 
span . name , 
thrift_annotations , 
span . timestamp , 
span . duration , 
encoded_span = thrift . span_to_bytes ( thrift_span ) 
return encoded_span 
~~ def _create_json_endpoint ( self , endpoint , is_v1 ) : 
json_endpoint = { } 
~~~ json_endpoint [ 'serviceName' ] = endpoint . service_name 
~~ elif is_v1 : 
~~~ json_endpoint [ 'serviceName' ] = "" 
~~~ json_endpoint [ 'port' ] = endpoint . port 
~~ if endpoint . ipv4 is not None : 
~~~ json_endpoint [ 'ipv4' ] = endpoint . ipv4 
~~ if endpoint . ipv6 is not None : 
~~~ json_endpoint [ 'ipv6' ] = endpoint . ipv6 
~~ return json_endpoint 
json_span = { 
'traceId' : span . trace_id , 
'name' : span . name , 
'id' : span . id , 
'annotations' : [ ] , 
'binaryAnnotations' : [ ] , 
~~~ json_span [ 'parentId' ] = span . parent_id 
~~~ json_span [ 'timestamp' ] = int ( span . timestamp * 1000000 ) 
~~~ json_span [ 'duration' ] = int ( span . duration * 1000000 ) 
~~ v1_endpoint = self . _create_json_endpoint ( span . endpoint , True ) 
for key , timestamp in span . annotations . items ( ) : 
~~~ json_span [ 'annotations' ] . append ( { 
'endpoint' : v1_endpoint , 
'timestamp' : int ( timestamp * 1000000 ) , 
'value' : key , 
~~ for key , value in span . binary_annotations . items ( ) : 
~~~ json_span [ 'binaryAnnotations' ] . append ( { 
'key' : key , 
'value' : value , 
~~ if v2_span . remote_endpoint : 
json_span [ 'binaryAnnotations' ] , 
~~ encoded_span = json . dumps ( json_span ) 
~~ def encode_span ( self , span ) : 
'id' : span . span_id , 
if span . name : 
~~~ json_span [ 'name' ] = span . name 
~~ if span . parent_id : 
~~ if span . shared is True : 
~~~ json_span [ 'shared' ] = True 
~~ if span . kind and span . kind . value is not None : 
~~~ json_span [ 'kind' ] = span . kind . value 
~~~ json_span [ 'localEndpoint' ] = self . _create_json_endpoint ( 
span . local_endpoint , 
False , 
~~~ json_span [ 'remoteEndpoint' ] = self . _create_json_endpoint ( 
span . remote_endpoint , 
~~ if span . tags and len ( span . tags ) > 0 : 
~~~ json_span [ 'tags' ] = span . tags 
~~ if span . annotations : 
~~~ json_span [ 'annotations' ] = [ 
for key , timestamp in span . annotations . items ( ) 
return current_size + len ( new_span ) <= max_size 
if not protobuf . installed ( ) : 
~~~ raise ZipkinError ( 
~~ pb_span = protobuf . create_protobuf_span ( span ) 
return protobuf . encode_pb_list ( [ pb_span ] ) 
~~ def get_decoder ( encoding ) : 
~~~ return _V1ThriftDecoder ( ) 
~~ def decode_spans ( self , spans ) : 
decoded_spans = [ ] 
transport = TMemoryBuffer ( spans ) 
if six . byte2int ( spans ) == TType . STRUCT : 
~~~ _ , size = read_list_begin ( transport ) 
~~~ size = 1 
~~ for _ in range ( size ) : 
~~~ span = zipkin_core . Span ( ) 
span . read ( TBinaryProtocol ( transport ) ) 
decoded_spans . append ( self . _decode_thrift_span ( span ) ) 
~~ return decoded_spans 
~~ def _convert_from_thrift_endpoint ( self , thrift_endpoint ) : 
ipv4 = None 
port = struct . unpack ( 'H' , struct . pack ( 'h' , thrift_endpoint . port ) ) [ 0 ] 
if thrift_endpoint . ipv4 != 0 : 
~~~ ipv4 = socket . inet_ntop ( 
socket . AF_INET , 
struct . pack ( '!i' , thrift_endpoint . ipv4 ) , 
~~ if thrift_endpoint . ipv6 : 
~~~ ipv6 = socket . inet_ntop ( socket . AF_INET6 , thrift_endpoint . ipv6 ) 
~~ return Endpoint ( 
service_name = thrift_endpoint . service_name , 
~~ def _decode_thrift_annotations ( self , thrift_annotations ) : 
local_endpoint = None 
kind = Kind . LOCAL 
all_annotations = { } 
timestamp = None 
duration = None 
for thrift_annotation in thrift_annotations : 
~~~ all_annotations [ thrift_annotation . value ] = thrift_annotation . timestamp 
if thrift_annotation . host : 
~~~ local_endpoint = self . _convert_from_thrift_endpoint ( 
thrift_annotation . host , 
~~ ~~ if 'cs' in all_annotations and 'sr' not in all_annotations : 
~~~ kind = Kind . CLIENT 
timestamp = all_annotations [ 'cs' ] 
duration = all_annotations [ 'cr' ] - all_annotations [ 'cs' ] 
~~ elif 'cs' not in all_annotations and 'sr' in all_annotations : 
~~~ kind = Kind . SERVER 
timestamp = all_annotations [ 'sr' ] 
duration = all_annotations [ 'ss' ] - all_annotations [ 'sr' ] 
~~ annotations = { 
name : self . seconds ( ts ) for name , ts in all_annotations . items ( ) 
if name not in _DROP_ANNOTATIONS 
return annotations , local_endpoint , kind , timestamp , duration 
~~ def _convert_from_thrift_binary_annotations ( self , thrift_binary_annotations ) : 
tags = { } 
remote_endpoint = None 
for binary_annotation in thrift_binary_annotations : 
~~~ if binary_annotation . key == 'sa' : 
~~~ remote_endpoint = self . _convert_from_thrift_endpoint ( 
thrift_endpoint = binary_annotation . host , 
~~~ key = binary_annotation . key 
annotation_type = binary_annotation . annotation_type 
value = binary_annotation . value 
if annotation_type == zipkin_core . AnnotationType . BOOL : 
~~~ tags [ key ] = "true" if value == 1 else "false" 
~~ elif annotation_type == zipkin_core . AnnotationType . STRING : 
~~~ tags [ key ] = str ( value ) 
~~ if binary_annotation . host : 
~~ ~~ ~~ return tags , local_endpoint , remote_endpoint 
~~ def _decode_thrift_span ( self , thrift_span ) : 
parent_id = None 
annotations = { } 
if thrift_span . parent_id : 
~~~ parent_id = self . _convert_unsigned_long_to_lower_hex ( 
thrift_span . parent_id , 
~~ if thrift_span . annotations : 
~~~ annotations , local_endpoint , kind , timestamp , duration = self . _decode_thrift_annotations ( thrift_span . annotations ) 
~~ if thrift_span . binary_annotations : 
~~~ tags , local_endpoint , remote_endpoint = self . _convert_from_thrift_binary_annotations ( 
thrift_span . binary_annotations , 
~~ trace_id = self . _convert_trace_id_to_string ( 
thrift_span . trace_id , 
thrift_span . trace_id_high , 
return Span ( 
name = thrift_span . name , 
parent_id = parent_id , 
span_id = self . _convert_unsigned_long_to_lower_hex ( thrift_span . id ) , 
timestamp = self . seconds ( timestamp or thrift_span . timestamp ) , 
duration = self . seconds ( duration or thrift_span . duration ) , 
local_endpoint = local_endpoint , 
remote_endpoint = remote_endpoint , 
shared = ( kind == Kind . SERVER and thrift_span . timestamp is None ) , 
annotations = annotations , 
tags = tags , 
~~ def _convert_trace_id_to_string ( self , trace_id , trace_id_high = None ) : 
if trace_id_high is not None : 
~~~ result = bytearray ( 32 ) 
self . _write_hex_long ( result , 0 , trace_id_high ) 
self . _write_hex_long ( result , 16 , trace_id ) 
return result . decode ( "utf8" ) 
~~ result = bytearray ( 16 ) 
self . _write_hex_long ( result , 0 , trace_id ) 
~~ def _convert_unsigned_long_to_lower_hex ( self , value ) : 
result = bytearray ( 16 ) 
self . _write_hex_long ( result , 0 , value ) 
~~ def _write_hex_long ( self , data , pos , value ) : 
self . _write_hex_byte ( data , pos + 0 , ( value >> 56 ) & 0xff ) 
self . _write_hex_byte ( data , pos + 2 , ( value >> 48 ) & 0xff ) 
self . _write_hex_byte ( data , pos + 4 , ( value >> 40 ) & 0xff ) 
self . _write_hex_byte ( data , pos + 6 , ( value >> 32 ) & 0xff ) 
self . _write_hex_byte ( data , pos + 8 , ( value >> 24 ) & 0xff ) 
self . _write_hex_byte ( data , pos + 10 , ( value >> 16 ) & 0xff ) 
self . _write_hex_byte ( data , pos + 12 , ( value >> 8 ) & 0xff ) 
self . _write_hex_byte ( data , pos + 14 , ( value & 0xff ) ) 
~~ def date_fixup_pre_processor ( transactions , tag , tag_dict , * args ) : 
if tag_dict [ 'month' ] == '02' : 
~~~ year = int ( tag_dict [ 'year' ] , 10 ) 
_ , max_month_day = calendar . monthrange ( year , 2 ) 
if int ( tag_dict [ 'day' ] , 10 ) > max_month_day : 
~~~ tag_dict [ 'day' ] = str ( max_month_day ) 
~~ ~~ return tag_dict 
~~ def mBank_set_transaction_code ( transactions , tag , tag_dict , * args ) : 
tag_dict [ 'transaction_code' ] = int ( 
return tag_dict 
~~ def mBank_set_iph_id ( transactions , tag , tag_dict , * args ) : 
matches = iph_id_re . search ( tag_dict [ tag . slug ] ) 
~~~ tag_dict [ 'iph_id' ] = matches . groupdict ( ) [ 'iph_id' ] 
~~ return tag_dict 
~~ def mBank_set_tnr ( transactions , tag , tag_dict , * args ) : 
matches = tnr_re . search ( tag_dict [ tag . slug ] ) 
~~~ tag_dict [ 'tnr' ] = matches . groupdict ( ) [ 'tnr' ] 
~~ def parse ( self , data ) : 
data = '\\n' . join ( self . strip ( data . split ( '\\n' ) ) ) 
tag_re = re . compile ( 
r'^:\\n?(?P<full_tag>(?P<tag>[0-9]{2}|NS)(?P<sub_tag>[A-Z])?):' , 
re . MULTILINE ) 
matches = list ( tag_re . finditer ( data ) ) 
valid_matches = list ( self . sanatize_tag_id_matches ( matches ) ) 
for i , match in enumerate ( valid_matches ) : 
~~~ tag_id = self . normalize_tag_id ( match . group ( 'tag' ) ) 
tag = self . tags . get ( match . group ( 'full_tag' ) ) or self . tags [ tag_id ] 
if valid_matches [ i + 1 : ] : 
~~~ tag_data = data [ match . end ( ) : valid_matches [ i + 1 ] . start ( ) ] . strip ( ) 
~~~ tag_data = data [ match . end ( ) : ] . strip ( ) 
~~ tag_dict = tag . parse ( self , tag_data ) 
for processor in self . processors . get ( 'pre_%s' % tag . slug , [ ] ) : 
~~~ tag_dict = processor ( self , tag , tag_dict ) 
~~ result = tag ( self , tag_dict ) 
for processor in self . processors . get ( 'post_%s' % tag . slug , [ ] ) : 
~~~ result = processor ( self , tag , tag_dict , result ) 
~~ if isinstance ( tag , mt940 . tags . Statement ) : 
~~~ if not self . transactions : 
~~~ transaction = Transaction ( self ) 
self . transactions . append ( transaction ) 
~~ if transaction . data . get ( 'id' ) : 
~~~ transaction = Transaction ( self , result ) 
~~~ transaction . data . update ( result ) 
~~ ~~ elif issubclass ( tag . scope , Transaction ) and self . transactions : 
~~~ for k , v in _compat . iteritems ( result ) : 
~~~ if k in transaction . data and hasattr ( v , 'strip' ) : 
~~~ transaction . data [ k ] += '\\n%s' % v . strip ( ) 
~~~ transaction . data [ k ] = v 
~~~ self . data . update ( result ) 
~~ ~~ return self . transactions 
~~ def parse ( src , encoding = None ) : 
def safe_is_file ( filename ) : 
~~~ return os . path . isfile ( src ) 
~~~ data = src . read ( ) 
~~ elif safe_is_file ( src ) : 
~~~ with open ( src , 'rb' ) as fh : 
~~~ data = fh . read ( ) 
~~~ data = src 
~~~ exception = None 
encodings = [ encoding , 'utf-8' , 'cp852' , 'iso8859-15' , 'latin1' ] 
~~~ if not encoding : 
~~~ data = data . decode ( encoding ) 
~~ except UnicodeDecodeError as e : 
~~~ exception = e 
~~ except UnicodeEncodeError : 
~~ ~~ transactions = mt940 . models . Transactions ( ) 
transactions . parse ( data ) 
return transactions 
~~ def join_lines ( string , strip = Strip . BOTH ) : 
for line in string . splitlines ( ) : 
~~~ if strip & Strip . RIGHT : 
~~ if strip & Strip . LEFT : 
~~~ line = line . lstrip ( ) 
~~ lines . append ( line ) 
~~ return '' . join ( lines ) 
~~ async def json_or_text ( response ) : 
text = await response . text ( ) 
~~~ return json . loads ( text ) 
~~ async def limited ( until ) : 
duration = int ( round ( until - time . time ( ) ) ) 
mins = duration / 60 
log . warn ( fmt , duration , mins ) 
~~ async def request ( self , method , url , ** kwargs ) : 
rate_limiter = RateLimiter ( max_calls = 59 , period = 60 , callback = limited ) 
~~~ if not self . token : 
'User-Agent' : self . user_agent , 
'Content-Type' : 'application/json' 
if 'json' in kwargs : 
~~~ kwargs [ 'data' ] = to_json ( kwargs . pop ( 'json' ) ) 
~~ kwargs [ 'headers' ] = headers 
headers [ 'Authorization' ] = self . token 
for tries in range ( 5 ) : 
~~~ async with self . session . request ( method , url , ** kwargs ) as resp : 
url , kwargs . get ( 'data' ) , resp . status ) 
data = await json_or_text ( resp ) 
if 300 > resp . status >= 200 : 
retry_after = json . loads ( resp . headers . get ( 'Retry-After' ) ) 
mins = retry_after / 60 
log . warning ( fmt , retry_after , mins ) 
if is_global : 
~~~ self . _global_over . clear ( ) 
~~ await asyncio . sleep ( retry_after , loop = self . loop ) 
~~~ self . _global_over . set ( ) 
~~ if resp . status == 400 : 
~~~ raise HTTPException ( resp , data ) 
~~ elif resp . status == 401 : 
~~~ raise Unauthorized ( resp , data ) 
~~ elif resp . status == 403 : 
~~~ raise Forbidden ( resp , data ) 
~~ elif resp . status == 404 : 
~~~ raise NotFound ( resp , data ) 
~~ ~~ ~~ raise HTTPException ( resp , data ) 
~~ ~~ async def get_bot_info ( self , bot_id ) : 
resp = await self . request ( 'GET' , '{}/bots/{}' . format ( self . BASE , bot_id ) ) 
resp [ 'date' ] = datetime . strptime ( resp [ 'date' ] , '%Y-%m-%dT%H:%M:%S.%fZ' ) 
for k in resp : 
~~~ if resp [ k ] == '' : 
~~~ resp [ k ] = None 
~~ ~~ return resp 
~~ async def get_bots ( self , limit , offset ) : 
if limit > 500 : 
~~~ limit = 50 
~~ return await self . request ( 'GET' , '{}/bots?limit={}&offset={}' . format ( self . BASE , limit , offset ) ) 
~~ def guild_count ( self ) : 
~~~ return len ( self . bot . guilds ) 
~~~ return len ( self . bot . servers ) 
~~ ~~ async def post_guild_count ( 
shard_count : int = None , 
shard_no : int = None 
await self . http . post_guild_count ( self . bot_id , self . guild_count ( ) , shard_count , shard_no ) 
~~ async def get_guild_count ( self , bot_id : int = None ) : 
if bot_id is None : 
~~~ bot_id = self . bot_id 
~~ return await self . http . get_guild_count ( bot_id ) 
~~ async def get_bot_info ( self , bot_id : int = None ) : 
~~ return await self . http . get_bot_info ( bot_id ) 
~~ async def get_bots ( self , limit : int = 50 , offset : int = 0 ) : 
return await self . http . get_bots ( limit , offset ) 
~~ async def generate_widget_large ( 
bot_id : int = None , 
top : str = '2C2F33' , 
mid : str = '23272A' , 
user : str = 'FFFFFF' , 
cert : str = 'FFFFFF' , 
data : str = 'FFFFFF' , 
label : str = '99AAB5' , 
highlight : str = '2C2F33' 
url = 'https://discordbots.org/api/widget/{0}.png?topcolor={1}&middlecolor={2}&usernamecolor={3}&certifiedcolor={4}&datacolor={5}&labelcolor={6}&highlightcolor={7}' 
~~ url = url . format ( bot_id , top , mid , user , cert , data , label , highlight ) 
~~ async def get_widget_large ( self , bot_id : int = None ) : 
~~ url = 'https://discordbots.org/api/widget/{0}.png' . format ( bot_id ) 
~~ async def generate_widget_small ( 
avabg : str = '2C2F33' , 
lcol : str = '23272A' , 
rcol : str = '2C2F33' , 
ltxt : str = 'FFFFFF' , 
rtxt : str = 'FFFFFF' 
url = 'https://discordbots.org/api/widget/lib/{0}.png?avatarbg={1}&lefttextcolor={2}&righttextcolor={3}&leftcolor={4}&rightcolor={5}' 
~~ url = url . format ( bot_id , avabg , ltxt , rtxt , lcol , rcol ) 
~~ async def get_widget_small ( self , bot_id : int = None ) : 
~~ url = 'https://discordbots.org/api/widget/lib/{0}.png' . format ( bot_id ) 
~~ async def close ( self ) : 
if self . _is_closed : 
~~~ await self . http . close ( ) 
self . _is_closed = True 
~~ ~~ def read ( self ) : 
packet = self . packet 
with self . __read_lock : 
~~~ buffer = self . __buffer 
while len ( buffer ) < packet : 
~~~ buffer += self . _read_data ( ) 
~~ length = self . __unpack ( buffer [ : packet ] ) [ 0 ] + packet 
while len ( buffer ) < length : 
~~ term , self . __buffer = decode ( buffer [ packet : ] ) 
~~ return term 
~~ def write ( self , message ) : 
data = encode ( message , compressed = self . compressed ) 
data = self . __pack ( length ) + data 
with self . __write_lock : 
~~~ while data : 
~~~ n = os . write ( self . out_d , data ) 
~~ except OSError as why : 
~~~ if why . errno in ( errno . EPIPE , errno . EINVAL ) : 
~~~ raise EOFError ( ) 
~~ if not n : 
~~ data = data [ n : ] 
~~ ~~ return length + self . packet 
os . close ( self . in_d ) 
os . close ( self . out_d ) 
~~ def decode ( string ) : 
~~~ raise IncompleteData ( string ) 
~~ if string [ 0 ] != 131 : 
~~ if string [ 1 : 2 ] == b'P' : 
~~~ if len ( string ) < 16 : 
~~ d = decompressobj ( ) 
term_string = d . decompress ( string [ 6 : ] ) + d . flush ( ) 
uncompressed_size , = _int4_unpack ( string [ 2 : 6 ] ) 
if len ( term_string ) != uncompressed_size : 
~~ term , _tail = decode_term ( term_string ) 
return term , d . unused_data 
~~ return decode_term ( string [ 1 : ] ) 
~~ def encode ( term , compressed = False ) : 
encoded_term = encode_term ( term ) 
if compressed : 
~~~ if compressed is True : 
~~~ compressed = 6 
~~ elif compressed < 0 or compressed > 9 : 
~~ zlib_term = compress ( encoded_term , compressed ) 
ln = len ( encoded_term ) 
if len ( zlib_term ) + 5 <= ln : 
~~~ return b"\\x83P" + _int4_pack ( ln ) + zlib_term 
~~ ~~ return b"\\x83" + encoded_term 
~~ def addSourceAddr ( self , addr ) : 
~~~ self . _multiInSocket . setsockopt ( socket . IPPROTO_IP , socket . IP_ADD_MEMBERSHIP , self . _makeMreq ( addr ) ) 
~~ sock = self . _createMulticastOutSocket ( addr , self . _observer . ttl ) 
self . _multiOutUniInSockets [ addr ] = sock 
self . _poll . register ( sock , select . POLLIN ) 
~~ def _sendPendingMessages ( self ) : 
if len ( self . _queue ) == 0 : 
~~~ time . sleep ( 0.1 ) 
~~ msg = self . _queue . pop ( 0 ) 
if msg . canSend ( ) : 
~~~ self . _sendMsg ( msg ) 
msg . refresh ( ) 
if not ( msg . isFinished ( ) ) : 
~~~ self . _queue . append ( msg ) 
time . sleep ( 0.01 ) 
~~ ~~ def setRemoteServiceHelloCallback ( self , cb , types = None , scopes = None ) : 
self . _remoteServiceHelloCallback = cb 
self . _remoteServiceHelloCallbackTypesFilter = types 
self . _remoteServiceHelloCallbackScopesFilter = scopes 
self . clearRemoteServices ( ) 
self . clearLocalServices ( ) 
self . _stopThreads ( ) 
self . _serverStarted = False 
~~ def clearLocalServices ( self ) : 
for service in list ( self . _localServices . values ( ) ) : 
~~~ self . _sendBye ( service ) 
~~ self . _localServices . clear ( ) 
~~ def searchServices ( self , types = None , scopes = None , timeout = 3 ) : 
if not self . _serverStarted : 
~~ self . _sendProbe ( types , scopes ) 
return self . _filterServices ( list ( self . _remoteServices . values ( ) ) , types , scopes ) 
~~ def publishService ( self , types , scopes , xAddrs ) : 
~~ instanceId = _generateInstanceId ( ) 
service = Service ( types , scopes , xAddrs , self . uuid , instanceId ) 
self . _localServices [ self . uuid ] = service 
self . _sendHello ( service ) 
time . sleep ( 0.001 ) 
~~ def createSOAPMessage ( env ) : 
if env . getAction ( ) == ACTION_PROBE : 
~~~ return createProbeMessage ( env ) 
~~ if env . getAction ( ) == ACTION_PROBE_MATCH : 
~~~ return createProbeMatchMessage ( env ) 
~~ if env . getAction ( ) == ACTION_RESOLVE : 
~~~ return createResolveMessage ( env ) 
~~ if env . getAction ( ) == ACTION_RESOLVE_MATCH : 
~~~ return createResolveMatchMessage ( env ) 
~~ if env . getAction ( ) == ACTION_HELLO : 
~~~ return createHelloMessage ( env ) 
~~ if env . getAction ( ) == ACTION_BYE : 
~~~ return createByeMessage ( env ) 
~~ ~~ def parseSOAPMessage ( data , ipAddr ) : 
~~~ dom = minidom . parseString ( data ) 
#print(\ 
~~ if dom . getElementsByTagNameNS ( NS_S , "Fault" ) : 
~~ soapAction = dom . getElementsByTagNameNS ( NS_A , "Action" ) [ 0 ] . firstChild . data . strip ( ) 
if soapAction == ACTION_PROBE : 
~~~ return parseProbeMessage ( dom ) 
~~ elif soapAction == ACTION_PROBE_MATCH : 
~~~ return parseProbeMatchMessage ( dom ) 
~~ elif soapAction == ACTION_RESOLVE : 
~~~ return parseResolveMessage ( dom ) 
~~ elif soapAction == ACTION_RESOLVE_MATCH : 
~~~ return parseResolveMatchMessage ( dom ) 
~~ elif soapAction == ACTION_BYE : 
~~~ return parseByeMessage ( dom ) 
~~ elif soapAction == ACTION_HELLO : 
~~~ return parseHelloMessage ( dom ) 
~~ ~~ def discover ( scope , loglevel , capture ) : 
if loglevel : 
~~~ level = getattr ( logging , loglevel , None ) 
if not level : 
~~ logger . setLevel ( level ) 
~~ run ( scope = scope , capture = capture ) 
~~ def get_tagged_item_manager ( self ) : 
rel_name = self . through . _meta . get_field ( 'content_object' ) . remote_field . get_accessor_name ( ) 
return getattr ( self . instance , rel_name ) 
~~ def get_serializable_data_for_fields ( model ) : 
pk_field = model . _meta . pk 
while pk_field . remote_field and pk_field . remote_field . parent_link : 
~~~ pk_field = pk_field . remote_field . model . _meta . pk 
~~ obj = { 'pk' : get_field_value ( pk_field , model ) } 
for field in model . _meta . fields : 
~~~ if field . serialize : 
~~~ obj [ field . name ] = get_field_value ( field , model ) 
~~ def get_all_child_relations ( model ) : 
field for field in model . _meta . get_fields ( ) 
if isinstance ( field . remote_field , ParentalKey ) 
~~ def get_all_child_m2m_relations ( model ) : 
if isinstance ( field , ParentalManyToManyField ) 
~~ def save ( self , ** kwargs ) : 
child_relation_names = [ rel . get_accessor_name ( ) for rel in get_all_child_relations ( self ) ] 
child_m2m_field_names = [ field . name for field in get_all_child_m2m_relations ( self ) ] 
update_fields = kwargs . pop ( 'update_fields' , None ) 
if update_fields is None : 
~~~ real_update_fields = None 
relations_to_commit = child_relation_names 
m2m_fields_to_commit = child_m2m_field_names 
~~~ real_update_fields = [ ] 
relations_to_commit = [ ] 
m2m_fields_to_commit = [ ] 
for field in update_fields : 
~~~ if field in child_relation_names : 
~~~ relations_to_commit . append ( field ) 
~~ elif field in child_m2m_field_names : 
~~~ m2m_fields_to_commit . append ( field ) 
~~~ real_update_fields . append ( field ) 
~~ ~~ ~~ super ( ClusterableModel , self ) . save ( update_fields = real_update_fields , ** kwargs ) 
for relation in relations_to_commit : 
~~~ getattr ( self , relation ) . commit ( ) 
~~ for field in m2m_fields_to_commit : 
~~~ getattr ( self , field ) . commit ( ) 
~~ ~~ def from_serializable_data ( cls , data , check_fks = True , strict_fks = False ) : 
obj = model_from_serializable_data ( cls , data , check_fks = check_fks , strict_fks = strict_fks ) 
if obj is None : 
~~ child_relations = get_all_child_relations ( cls ) 
for rel in child_relations : 
~~~ rel_name = rel . get_accessor_name ( ) 
~~~ child_data_list = data [ rel_name ] 
~~ related_model = rel . related_model 
if hasattr ( related_model , 'from_serializable_data' ) : 
~~~ children = [ 
related_model . from_serializable_data ( child_data , check_fks = check_fks , strict_fks = True ) 
for child_data in child_data_list 
model_from_serializable_data ( related_model , child_data , check_fks = check_fks , strict_fks = True ) 
~~ children = filter ( lambda child : child is not None , children ) 
setattr ( obj , rel_name , children ) 
~~ def validate_unique ( self ) : 
all_unique_checks = set ( ) 
all_date_checks = set ( ) 
forms_to_delete = self . deleted_forms 
valid_forms = [ form for form in self . forms if form . is_valid ( ) and form not in forms_to_delete ] 
for form in valid_forms : 
~~~ unique_checks , date_checks = form . instance . _get_unique_checks ( ) 
all_unique_checks . update ( unique_checks ) 
all_date_checks . update ( date_checks ) 
~~ errors = [ ] 
for uclass , unique_check in all_unique_checks : 
~~~ seen_data = set ( ) 
~~~ row_data = ( 
field if field in self . unique_fields else form . cleaned_data [ field ] 
for field in unique_check if field in form . cleaned_data 
row_data = tuple ( d . _get_pk_val ( ) if hasattr ( d , '_get_pk_val' ) else d 
for d in row_data ) 
if row_data and None not in row_data : 
~~~ if row_data in seen_data : 
~~~ errors . append ( self . get_unique_error_message ( unique_check ) ) 
form . _errors [ NON_FIELD_ERRORS ] = self . error_class ( [ self . get_form_error ( ) ] ) 
for field in unique_check : 
~~~ if field in form . cleaned_data : 
~~~ del form . cleaned_data [ field ] 
~~ ~~ ~~ seen_data . add ( row_data ) 
~~ ~~ ~~ if errors : 
~~ ~~ def has_changed ( self ) : 
if self . formsets : 
~~~ for formset in self . formsets . values ( ) : 
~~~ for form in formset . forms : 
~~~ if form . has_changed ( ) : 
~~ ~~ ~~ ~~ return bool ( self . changed_data ) 
~~ def create_deferring_foreign_related_manager ( related , original_manager_cls ) : 
relation_name = related . get_accessor_name ( ) 
rel_field = related . field 
rel_model = related . related_model 
superclass = rel_model . _default_manager . __class__ 
class DeferringRelatedManager ( superclass ) : 
~~~ def __init__ ( self , instance ) : 
~~~ super ( DeferringRelatedManager , self ) . __init__ ( ) 
self . model = rel_model 
self . instance = instance 
~~ def _get_cluster_related_objects ( self ) : 
~~~ return self . instance . _cluster_related_objects 
~~~ cluster_related_objects = { } 
self . instance . _cluster_related_objects = cluster_related_objects 
return cluster_related_objects 
~~ ~~ def get_live_query_set ( self ) : 
~~~ return self . get_live_queryset ( ) 
~~ def get_live_queryset ( self ) : 
return original_manager_cls ( self . instance ) . get_queryset ( ) 
~~ def get_queryset ( self ) : 
~~~ results = self . instance . _cluster_related_objects [ relation_name ] 
~~ except ( AttributeError , KeyError ) : 
~~ return FakeQuerySet ( related . related_model , results ) 
~~~ return queryset . _next_is_sticky ( ) 
~~ def get_prefetch_queryset ( self , instances , queryset = None ) : 
~~~ if queryset is None : 
~~~ db = self . _db or router . db_for_read ( self . model , instance = instances [ 0 ] ) 
queryset = super ( DeferringRelatedManager , self ) . get_queryset ( ) . using ( db ) 
~~ rel_obj_attr = rel_field . get_local_related_value 
instance_attr = rel_field . get_foreign_related_value 
instances_dict = dict ( ( instance_attr ( inst ) , inst ) for inst in instances ) 
query = { '%s__in' % rel_field . name : instances } 
qs = queryset . filter ( ** query ) 
for rel_obj in qs : 
~~~ instance = instances_dict [ rel_obj_attr ( rel_obj ) ] 
setattr ( rel_obj , rel_field . name , instance ) 
~~ cache_name = rel_field . related_query_name ( ) 
return qs , rel_obj_attr , instance_attr , False , cache_name , False 
~~ def get_object_list ( self ) : 
cluster_related_objects = self . _get_cluster_related_objects ( ) 
~~~ object_list = cluster_related_objects [ relation_name ] 
~~~ object_list = list ( self . get_live_queryset ( ) ) 
cluster_related_objects [ relation_name ] = object_list 
~~ return object_list 
~~ def add ( self , * new_items ) : 
items = self . get_object_list ( ) 
for target in new_items : 
~~~ item_matched = False 
for i , item in enumerate ( items ) : 
~~~ if item == target : 
~~~ items [ i ] = target 
item_matched = True 
~~ ~~ if not item_matched : 
~~~ items . append ( target ) 
~~ setattr ( target , related . field . name , self . instance ) 
~~ if rel_model . _meta . ordering and len ( items ) > 1 : 
~~~ sort_by_fields ( items , rel_model . _meta . ordering ) 
~~ ~~ def remove ( self , * items_to_remove ) : 
items [ : ] = [ item for item in items if item not in items_to_remove ] 
~~ def create ( self , ** kwargs ) : 
~~~ items = self . get_object_list ( ) 
new_item = related . related_model ( ** kwargs ) 
items . append ( new_item ) 
return new_item 
~~ def clear ( self ) : 
self . set ( [ ] ) 
~~ def set ( self , objs , bulk = True , clear = False ) : 
~~~ objs = list ( objs ) 
for obj in objs : 
~~~ setattr ( obj , related . field . name , self . instance ) 
~~ if rel_model . _meta . ordering and len ( objs ) > 1 : 
~~~ sort_by_fields ( objs , rel_model . _meta . ordering ) 
~~ cluster_related_objects [ relation_name ] = objs 
~~ def commit ( self ) : 
if self . instance . pk is None : 
~~~ final_items = self . instance . _cluster_related_objects [ relation_name ] 
~~ original_manager = original_manager_cls ( self . instance ) 
live_items = list ( original_manager . get_queryset ( ) ) 
for item in live_items : 
~~~ if item not in final_items : 
~~~ item . delete ( ) 
~~ ~~ for item in final_items : 
~~~ original_manager . add ( item , bulk = False ) 
~~ del self . instance . _cluster_related_objects [ relation_name ] 
~~ ~~ return DeferringRelatedManager 
~~ def sort_by_fields ( items , fields ) : 
for key in reversed ( fields ) : 
~~~ reverse = False 
if key [ 0 ] == '-' : 
~~~ reverse = True 
key = key [ 1 : ] 
~~ items . sort ( key = lambda x : ( getattr ( x , key ) is not None , getattr ( x , key ) ) , reverse = reverse ) 
~~ ~~ def modular_squareroot_in_FQ2 ( value : FQ2 ) -> FQ2 : 
candidate_squareroot = value ** ( ( FQ2_order + 8 ) // 16 ) 
check = candidate_squareroot ** 2 / value 
if check in eighth_roots_of_unity [ : : 2 ] : 
~~~ x1 = candidate_squareroot / eighth_roots_of_unity [ eighth_roots_of_unity . index ( check ) // 2 ] 
x2 = - x1 
x1_re , x1_im = x1 . coeffs 
x2_re , x2_im = x2 . coeffs 
return x1 if ( x1_im > x2_im or ( x1_im == x2_im and x1_re > x2_re ) ) else x2 
~~ def compress_G1 ( pt : G1Uncompressed ) -> G1Compressed : 
if is_inf ( pt ) : 
~~~ return G1Compressed ( POW_2_383 + POW_2_382 ) 
~~~ x , y = normalize ( pt ) 
a_flag = ( y . n * 2 ) // q 
return G1Compressed ( x . n + a_flag * POW_2_381 + POW_2_383 ) 
~~ ~~ def decompress_G1 ( z : G1Compressed ) -> G1Uncompressed : 
b_flag = ( z % POW_2_383 ) // POW_2_382 
if b_flag == 1 : 
~~~ return Z1 
~~ x = z % POW_2_381 
y = pow ( ( x ** 3 + b . n ) % q , ( q + 1 ) // 4 , q ) 
if pow ( y , 2 , q ) != ( x ** 3 + b . n ) % q : 
~~ a_flag = ( z % POW_2_382 ) // POW_2_381 
if ( y * 2 ) // q != a_flag : 
~~~ y = q - y 
~~ return ( FQ ( x ) , FQ ( y ) , FQ ( 1 ) ) 
~~ def compress_G2 ( pt : G2Uncompressed ) -> G2Compressed : 
if not is_on_curve ( pt , b2 ) : 
~~ if is_inf ( pt ) : 
~~~ return G2Compressed ( ( POW_2_383 + POW_2_382 , 0 ) ) 
~~ x , y = normalize ( pt ) 
x_re , x_im = x . coeffs 
y_re , y_im = y . coeffs 
a_flag1 = ( y_im * 2 ) // q if y_im > 0 else ( y_re * 2 ) // q 
z1 = x_im + a_flag1 * POW_2_381 + POW_2_383 
z2 = x_re 
return G2Compressed ( ( z1 , z2 ) ) 
~~ def decompress_G2 ( p : G2Compressed ) -> G2Uncompressed : 
z1 , z2 = p 
b_flag1 = ( z1 % POW_2_383 ) // POW_2_382 
if b_flag1 == 1 : 
~~~ return Z2 
~~ x1 = z1 % POW_2_381 
x2 = z2 
x = FQ2 ( [ x2 , x1 ] ) 
y = modular_squareroot_in_FQ2 ( x ** 3 + b2 ) 
~~ a_flag1 = ( z1 % POW_2_382 ) // POW_2_381 
if ( y_im > 0 and ( y_im * 2 ) // q != a_flag1 ) or ( y_im == 0 and ( y_re * 2 ) // q != a_flag1 ) : 
~~~ y = FQ2 ( ( y * - 1 ) . coeffs ) 
~~ if not is_on_curve ( ( x , y , FQ2 ( [ 1 , 0 ] ) ) , b2 ) : 
~~ return ( x , y , FQ2 ( [ 1 , 0 ] ) ) 
~~ def prime_field_inv ( a : int , n : int ) -> int : 
~~ lm , hm = 1 , 0 
low , high = a % n , n 
while low > 1 : 
~~~ r = high // low 
nm , new = hm - lm * r , high - low * r 
lm , low , hm , high = nm , new , lm , low 
~~ return lm % n 
~~ def getEvents ( self ) : 
events = [ ] 
for json in self . conn . endpoints [ "self" ] . getEvents ( ) : 
~~~ events . append ( SkypeEvent . fromRaw ( self , json ) ) 
~~ return events 
~~ def setPresence ( self , status = SkypeUtils . Status . Online ) : 
self . conn ( "PUT" , "{0}/users/ME/presenceDocs/messagingService" . format ( self . conn . msgsHost ) , 
auth = SkypeConnection . Auth . RegToken , json = { "status" : status . label } ) 
~~ def setMood ( self , mood ) : 
self . conn ( "POST" , "{0}/users/{1}/profile/partial" . format ( SkypeConnection . API_USER , self . userId ) , 
auth = SkypeConnection . Auth . SkypeToken , json = { "payload" : { "mood" : mood or "" } } ) 
self . user . mood = SkypeUser . Mood ( plain = mood ) if mood else None 
~~ def setAvatar ( self , image ) : 
self . conn ( "PUT" , "{0}/users/{1}/profile/avatar" . format ( SkypeConnection . API_USER , self . userId ) , 
auth = SkypeConnection . Auth . SkypeToken , data = image . read ( ) ) 
~~ def getUrlMeta ( self , url ) : 
return self . conn ( "GET" , SkypeConnection . API_URL , params = { "url" : url } , 
auth = SkypeConnection . Auth . Authorize ) . json ( ) 
~~ def cycle ( self ) : 
~~~ events = self . getEvents ( ) 
~~ except requests . ConnectionError : 
~~ for event in events : 
~~~ self . onEvent ( event ) 
if self . autoAck : 
~~~ event . ack ( ) 
~~ ~~ ~~ def syncFlags ( self ) : 
self . flags = set ( self . skype . conn ( "GET" , SkypeConnection . API_FLAGS , 
auth = SkypeConnection . Auth . SkypeToken ) . json ( ) ) 
~~ def contact ( self , id ) : 
~~~ json = self . skype . conn ( "POST" , "{0}/users/batch/profiles" . format ( SkypeConnection . API_USER ) , 
json = { "usernames" : [ id ] } , auth = SkypeConnection . Auth . SkypeToken ) . json ( ) 
contact = SkypeContact . fromRaw ( self . skype , json [ 0 ] ) 
if contact . id not in self . contactIds : 
~~~ self . contactIds . append ( contact . id ) 
~~ return self . merge ( contact ) 
~~ except SkypeApiException as e : 
~~~ if len ( e . args ) >= 2 and getattr ( e . args [ 1 ] , "status_code" , None ) == 403 : 
~~ ~~ def user ( self , id ) : 
json = self . skype . conn ( "POST" , "{0}/batch/profiles" . format ( SkypeConnection . API_PROFILE ) , 
auth = SkypeConnection . Auth . SkypeToken , json = { "usernames" : [ id ] } ) . json ( ) 
if json and "status" not in json [ 0 ] : 
~~~ return self . merge ( SkypeUser . fromRaw ( self . skype , json [ 0 ] ) ) 
~~ ~~ def bots ( self ) : 
json = self . skype . conn ( "GET" , "{0}/agents" . format ( SkypeConnection . API_BOT ) , 
auth = SkypeConnection . Auth . SkypeToken ) . json ( ) . get ( "agentDescriptions" , [ ] ) 
return [ self . merge ( SkypeBotUser . fromRaw ( self . skype , raw ) ) for raw in json ] 
~~ def bot ( self , id ) : 
json = self . skype . conn ( "GET" , "{0}/agents" . format ( SkypeConnection . API_BOT ) , params = { "agentId" : id } , 
return self . merge ( SkypeBotUser . fromRaw ( self . skype , json [ 0 ] ) ) if json else None 
~~ def search ( self , query ) : 
results = self . skype . conn ( "GET" , SkypeConnection . API_DIRECTORY , 
auth = SkypeConnection . Auth . SkypeToken , 
params = { "searchstring" : query , "requestId" : "0" } ) . json ( ) . get ( "results" , [ ] ) 
return [ SkypeUser . fromRaw ( self . skype , json . get ( "nodeProfileData" , { } ) ) for json in results ] 
~~ def requests ( self ) : 
requests = [ ] 
for json in self . skype . conn ( "GET" , "{0}/users/{1}/invites" 
. format ( SkypeConnection . API_CONTACTS , self . skype . userId ) , 
auth = SkypeConnection . Auth . SkypeToken ) . json ( ) . get ( "invite_list" , [ ] ) : 
~~~ for invite in json . get ( "invites" , [ ] ) : 
~~~ invite [ "userId" ] = SkypeUtils . noPrefix ( json . get ( "mri" ) ) 
requests . append ( SkypeRequest . fromRaw ( self . skype , invite ) ) 
~~ ~~ return requests 
~~ def fromRaw ( cls , skype = None , raw = { } ) : 
return cls ( skype , raw , ** cls . rawToFields ( raw ) ) 
~~ def merge ( self , other ) : 
for attr in self . attrs : 
~~~ if not getattr ( other , attr , None ) is None : 
~~~ setattr ( self , attr , getattr ( other , attr ) ) 
~~ ~~ if other . raw : 
~~~ if not self . raw : 
~~~ self . raw = { } 
~~ self . raw . update ( other . raw ) 
~~ ~~ def merge ( self , obj ) : 
if obj . id in self . cache : 
~~~ self . cache [ obj . id ] . merge ( obj ) 
~~~ self . cache [ obj . id ] = obj 
~~ return self . cache [ obj . id ] 
~~ def handle ( * codes , ** kwargs ) : 
regToken = kwargs . get ( "regToken" , False ) 
subscribe = kwargs . get ( "subscribe" ) 
def decorator ( fn ) : 
~~~ @ functools . wraps ( fn ) 
def wrapper ( self , * args , ** kwargs ) : 
~~~ return fn ( self , * args , ** kwargs ) 
~~~ if isinstance ( e . args [ 1 ] , requests . Response ) and e . args [ 1 ] . status_code in codes : 
~~~ conn = self if isinstance ( self , SkypeConnection ) else self . conn 
if regToken : 
~~~ conn . getRegToken ( ) 
~~ if subscribe : 
~~~ conn . endpoints [ subscribe ] . subscribe ( ) 
~~ return fn ( self , * args , ** kwargs ) 
~~ ~~ return wrapper 
~~ def externalCall ( cls , method , url , codes = ( 200 , 201 , 204 , 207 ) , ** kwargs ) : 
if os . getenv ( "SKPY_DEBUG_HTTP" ) : 
print ( pformat ( kwargs ) ) 
~~ resp = cls . extSess . request ( method , url , ** kwargs ) 
print ( pformat ( dict ( resp . headers ) ) ) 
~~~ print ( pformat ( resp . json ( ) ) ) 
~~~ print ( resp . text ) 
~~ ~~ if resp . status_code not in codes : 
~~ return resp 
~~ def syncStateCall ( self , method , url , params = { } , ** kwargs ) : 
~~~ states = self . syncStates [ ( method , url ) ] 
~~~ states = self . syncStates [ ( method , url ) ] = [ ] 
~~ if states : 
~~~ url = states [ - 1 ] 
~~ resp = self ( method , url , params = params , ** kwargs ) 
~~~ json = resp . json ( ) 
~~~ state = json . get ( "_metadata" , { } ) . get ( "syncState" ) 
if state : 
~~~ states . append ( state ) 
~~ def setUserPwd ( self , user , pwd ) : 
def getSkypeToken ( self ) : 
~~~ self . liveLogin ( user , pwd ) 
~~ self . getSkypeToken = MethodType ( getSkypeToken , self ) 
~~ def readToken ( self ) : 
if not self . tokenFile : 
~~~ with open ( self . tokenFile , "r" ) as f : 
~~~ lines = f . read ( ) . splitlines ( ) 
~~ ~~ except OSError : 
~~~ user , skypeToken , skypeExpiry , regToken , regExpiry , msgsHost = lines 
skypeExpiry = datetime . fromtimestamp ( int ( skypeExpiry ) ) 
regExpiry = datetime . fromtimestamp ( int ( regExpiry ) ) 
~~ if datetime . now ( ) >= skypeExpiry : 
~~ self . userId = user 
self . tokens [ "skype" ] = skypeToken 
self . tokenExpiry [ "skype" ] = skypeExpiry 
if datetime . now ( ) < regExpiry : 
~~~ self . tokens [ "reg" ] = regToken 
self . tokenExpiry [ "reg" ] = regExpiry 
self . msgsHost = msgsHost 
~~~ self . getRegToken ( ) 
~~ ~~ def writeToken ( self ) : 
with os . fdopen ( os . open ( self . tokenFile , os . O_WRONLY | os . O_CREAT , 0o600 ) , "w" ) as f : 
~~~ f . truncate ( ) 
f . write ( self . userId + "\\n" ) 
f . write ( self . tokens [ "skype" ] + "\\n" ) 
f . write ( str ( int ( time . mktime ( self . tokenExpiry [ "skype" ] . timetuple ( ) ) ) ) + "\\n" ) 
f . write ( self . tokens [ "reg" ] + "\\n" ) 
f . write ( str ( int ( time . mktime ( self . tokenExpiry [ "reg" ] . timetuple ( ) ) ) ) + "\\n" ) 
f . write ( self . msgsHost + "\\n" ) 
~~ ~~ def verifyToken ( self , auth ) : 
if auth in ( self . Auth . SkypeToken , self . Auth . Authorize ) : 
~~~ if "skype" not in self . tokenExpiry or datetime . now ( ) >= self . tokenExpiry [ "skype" ] : 
~~~ if not hasattr ( self , "getSkypeToken" ) : 
~~ self . getSkypeToken ( ) 
~~ ~~ elif auth == self . Auth . RegToken : 
~~~ if "reg" not in self . tokenExpiry or datetime . now ( ) >= self . tokenExpiry [ "reg" ] : 
~~ ~~ ~~ def liveLogin ( self , user , pwd ) : 
self . tokens [ "skype" ] , self . tokenExpiry [ "skype" ] = SkypeLiveAuthProvider ( self ) . auth ( user , pwd ) 
self . getUserId ( ) 
self . getRegToken ( ) 
~~ def guestLogin ( self , url , name ) : 
self . tokens [ "skype" ] , self . tokenExpiry [ "skype" ] = SkypeGuestAuthProvider ( self ) . auth ( url , name ) 
~~ def refreshSkypeToken ( self ) : 
self . tokens [ "skype" ] , self . tokenExpiry [ "skype" ] = SkypeRefreshAuthProvider ( self ) . auth ( self . tokens [ "skype" ] ) 
~~ def getUserId ( self ) : 
self . userId = self ( "GET" , "{0}/users/self/profile" . format ( self . API_USER ) , 
auth = self . Auth . SkypeToken ) . json ( ) . get ( "username" ) 
~~ def getRegToken ( self ) : 
self . verifyToken ( self . Auth . SkypeToken ) 
token , expiry , msgsHost , endpoint = SkypeRegistrationTokenProvider ( self ) . auth ( self . tokens [ "skype" ] ) 
self . tokens [ "reg" ] = token 
self . tokenExpiry [ "reg" ] = expiry 
if endpoint : 
~~~ endpoint . config ( ) 
self . endpoints [ "main" ] = endpoint 
~~ self . syncEndpoints ( ) 
if self . tokenFile : 
~~~ self . writeToken ( ) 
~~ ~~ def syncEndpoints ( self ) : 
self . endpoints [ "all" ] = [ ] 
for json in self ( "GET" , "{0}/users/ME/presenceDocs/messagingService" . format ( self . msgsHost ) , 
params = { "view" : "expanded" } , auth = self . Auth . RegToken ) . json ( ) . get ( "endpointPresenceDocs" , [ ] ) : 
~~~ id = json . get ( "link" , "" ) . split ( "/" ) [ 7 ] 
self . endpoints [ "all" ] . append ( SkypeEndpoint ( self , id ) ) 
~~ ~~ def auth ( self , user , pwd ) : 
pwdHash = base64 . b64encode ( hashlib . md5 ( ( user + "\\nskyper\\n" + pwd ) . encode ( "utf-8" ) ) . digest ( ) ) . decode ( "utf-8" ) 
json = self . conn ( "POST" , "{0}/login/skypetoken" . format ( SkypeConnection . API_USER ) , 
json = { "username" : user , "passwordHash" : pwdHash , "scopes" : "client" } ) . json ( ) 
if "skypetoken" not in json : 
~~~ raise SkypeAuthException ( "Couldn\ ) 
~~ expiry = None 
if "expiresIn" in json : 
~~~ expiry = datetime . fromtimestamp ( int ( time . time ( ) ) + int ( json [ "expiresIn" ] ) ) 
~~ return json [ "skypetoken" ] , expiry 
~~ def checkUser ( self , user ) : 
return not self . conn ( "POST" , "{0}/GetCredentialType.srf" . format ( SkypeConnection . API_MSACC ) , 
json = { "username" : user } ) . json ( ) . get ( "IfExistsResult" ) 
~~ def auth ( self , user , pwd ) : 
params = self . getParams ( ) 
t = self . sendCreds ( user , pwd , params ) 
return self . getToken ( t ) 
~~ def auth ( self , url , name ) : 
urlId = url . split ( "/" ) [ - 1 ] 
cookies = self . conn ( "GET" , "{0}/{1}" . format ( SkypeConnection . API_JOIN , urlId ) , 
headers = { "User-Agent" : agent } ) . cookies 
ids = self . conn ( "POST" , "{0}/api/v2/conversation/" . format ( SkypeConnection . API_JOIN ) , 
json = { "shortId" : urlId , "type" : "wl" } ) . json ( ) 
token = self . conn ( "POST" , "{0}/api/v1/users/guests" . format ( SkypeConnection . API_JOIN ) , 
headers = { "csrf_token" : cookies . get ( "csrf_token" ) , 
"X-Skype-Request-Id" : cookies . get ( "launcher_session_id" ) } , 
json = { "flowId" : cookies . get ( "launcher_session_id" ) , 
"shortId" : urlId , 
"longId" : ids . get ( "Long" ) , 
"threadId" : ids . get ( "Resource" ) , 
"name" : name } ) . json ( ) . get ( "skypetoken" ) 
expiry = datetime . now ( ) + timedelta ( days = 1 ) 
return token , expiry 
~~ def auth ( self , token ) : 
t = self . sendToken ( token ) 
~~ def auth ( self , skypeToken ) : 
token = expiry = endpoint = None 
msgsHost = SkypeConnection . API_MSGSHOST 
while not token : 
~~~ secs = int ( time . time ( ) ) 
hash = self . getMac256Hash ( str ( secs ) ) 
"Authentication" : "skypetoken=" + skypeToken , "BehaviorOverride" : "redirectAs404" } 
endpointResp = self . conn ( "POST" , "{0}/users/ME/endpoints" . format ( msgsHost ) , codes = ( 200 , 201 , 404 ) , 
headers = headers , json = { "endpointFeatures" : "Agent" } ) 
regTokenHead = endpointResp . headers . get ( "Set-RegistrationToken" ) 
locHead = endpointResp . headers . get ( "Location" ) 
if locHead : 
~~~ locParts = re . search ( r"(https://[^/]+/v1)/users/ME/endpoints(/(%7B[a-z0-9\\-]+%7D))?" , locHead ) . groups ( ) 
if locParts [ 2 ] : 
~~~ endpoint = SkypeEndpoint ( self . conn , locParts [ 2 ] . replace ( "%7B" , "{" ) . replace ( "%7D" , "}" ) ) 
~~ if not locParts [ 0 ] == msgsHost : 
~~~ msgsHost = locHead . rsplit ( "/" , 4 if locParts [ 2 ] else 3 ) [ 0 ] 
~~ ~~ if regTokenHead : 
~~~ token = re . search ( r"(registrationToken=[a-z0-9\\+/=]+)" , regTokenHead , re . I ) . group ( 1 ) 
regExpiry = re . search ( r"expires=(\\d+)" , regTokenHead ) . group ( 1 ) 
expiry = datetime . fromtimestamp ( int ( regExpiry ) ) 
regEndMatch = re . search ( r"endpointId=({[a-z0-9\\-]+})" , regTokenHead ) 
if regEndMatch : 
~~~ endpoint = SkypeEndpoint ( self . conn , regEndMatch . group ( 1 ) ) 
~~ ~~ if not endpoint and endpointResp . status_code == 200 and endpointResp . json ( ) : 
~~~ endpoint = SkypeEndpoint ( self . conn , endpointResp . json ( ) [ 0 ] [ "id" ] ) 
~~ ~~ return token , expiry , msgsHost , endpoint 
~~ def getMac256Hash ( challenge , appId = "msmsgs@msnmsgr.com" , key = "Q1P7W2E4J9R8U3S5" ) : 
clearText = challenge + appId 
clearText += "0" * ( 8 - len ( clearText ) % 8 ) 
def int32ToHexString ( n ) : 
~~~ hexChars = "0123456789abcdef" 
hexString = "" 
for i in range ( 4 ) : 
~~~ hexString += hexChars [ ( n >> ( i * 8 + 4 ) ) & 15 ] 
hexString += hexChars [ ( n >> ( i * 8 ) ) & 15 ] 
~~ return hexString 
~~ def int64Xor ( a , b ) : 
~~~ sA = "{0:b}" . format ( a ) 
sB = "{0:b}" . format ( b ) 
sC = "" 
sD = "" 
diff = abs ( len ( sA ) - len ( sB ) ) 
for i in range ( diff ) : 
~~~ sD += "0" 
~~ if len ( sA ) < len ( sB ) : 
~~~ sD += sA 
sA = sD 
~~ elif len ( sB ) < len ( sA ) : 
~~~ sD += sB 
sB = sD 
~~ for i in range ( len ( sA ) ) : 
~~~ sC += "0" if sA [ i ] == sB [ i ] else "1" 
~~ return int ( sC , 2 ) 
~~ def cS64 ( pdwData , pInHash ) : 
~~~ MODULUS = 2147483647 
CS64_a = pInHash [ 0 ] & MODULUS 
CS64_b = pInHash [ 1 ] & MODULUS 
CS64_c = pInHash [ 2 ] & MODULUS 
CS64_d = pInHash [ 3 ] & MODULUS 
CS64_e = 242854337 
qwDatum = 0 
qwMAC = 0 
qwSum = 0 
for i in range ( len ( pdwData ) // 2 ) : 
~~~ qwDatum = int ( pdwData [ pos ] ) 
pos += 1 
qwDatum *= CS64_e 
qwDatum = qwDatum % MODULUS 
qwMAC += qwDatum 
qwMAC *= CS64_a 
qwMAC += CS64_b 
qwMAC = qwMAC % MODULUS 
qwSum += qwMAC 
qwMAC += int ( pdwData [ pos ] ) 
qwMAC *= CS64_c 
qwMAC += CS64_d 
~~ qwMAC += CS64_b 
qwSum += CS64_d 
qwSum = qwSum % MODULUS 
return [ qwMAC , qwSum ] 
~~ cchClearText = len ( clearText ) // 4 
pClearText = [ ] 
for i in range ( cchClearText ) : 
~~~ pClearText = pClearText [ : i ] + [ 0 ] + pClearText [ i : ] 
for pos in range ( 4 ) : 
~~~ pClearText [ i ] += ord ( clearText [ 4 * i + pos ] ) * ( 256 ** pos ) 
~~ ~~ sha256Hash = [ 0 , 0 , 0 , 0 ] 
hash = hashlib . sha256 ( ( challenge + key ) . encode ( "utf-8" ) ) . hexdigest ( ) . upper ( ) 
for i in range ( len ( sha256Hash ) ) : 
~~~ sha256Hash [ i ] = 0 
~~~ dpos = 8 * i + pos * 2 
sha256Hash [ i ] += int ( hash [ dpos : dpos + 2 ] , 16 ) * ( 256 ** pos ) 
~~ ~~ macHash = cS64 ( pClearText , sha256Hash ) 
macParts = [ macHash [ 0 ] , macHash [ 1 ] , macHash [ 0 ] , macHash [ 1 ] ] 
return "" . join ( map ( int32ToHexString , map ( int64Xor , sha256Hash , macParts ) ) ) 
~~ def config ( self , name = "skype" ) : 
self . conn ( "PUT" , "{0}/users/ME/endpoints/{1}/presenceDocs/messagingService" 
. format ( self . conn . msgsHost , self . id ) , 
auth = SkypeConnection . Auth . RegToken , 
json = { "id" : "messagingService" , 
"type" : "EndpointPresenceDoc" , 
"selfLink" : "uri" , 
"privateInfo" : { "epname" : name } , 
"publicInfo" : { "capabilities" : "" , 
"type" : 1 , 
"skypeNameVersion" : "skype.com" , 
"nodeInfo" : "xx" , 
"version" : "908/1.30.0.128" } } ) 
~~ def ping ( self , timeout = 12 ) : 
self . conn ( "POST" , "{0}/users/ME/endpoints/{1}/active" . format ( self . conn . msgsHost , self . id ) , 
auth = SkypeConnection . Auth . RegToken , json = { "timeout" : timeout } ) 
~~ def subscribe ( self ) : 
self . conn ( "POST" , "{0}/users/ME/endpoints/{1}/subscriptions" . format ( self . conn . msgsHost , self . id ) , 
json = { "interestedResources" : [ "/v1/threads/ALL" , 
"/v1/users/ME/contacts/ALL" , 
"/v1/users/ME/conversations/ALL/messages" , 
"/v1/users/ME/conversations/ALL/properties" ] , 
"template" : "raw" , 
"channelType" : "httpLongPoll" } ) 
self . subscribed = True 
~~ def recent ( self ) : 
url = "{0}/users/ME/conversations" . format ( self . skype . conn . msgsHost ) 
params = { "startTime" : 0 , 
"view" : "msnp24Equivalent" , 
"targetType" : "Passport|Skype|Lync|Thread" } 
resp = self . skype . conn . syncStateCall ( "GET" , url , params , auth = SkypeConnection . Auth . RegToken ) . json ( ) 
chats = { } 
for json in resp . get ( "conversations" , [ ] ) : 
~~~ cls = SkypeSingleChat 
if "threadProperties" in json : 
~~~ info = self . skype . conn ( "GET" , "{0}/threads/{1}" . format ( self . skype . conn . msgsHost , json . get ( "id" ) ) , 
params = { "view" : "msnp24Equivalent" } ) . json ( ) 
json . update ( info ) 
cls = SkypeGroupChat 
~~ chats [ json . get ( "id" ) ] = self . merge ( cls . fromRaw ( self . skype , json ) ) 
~~ return chats 
~~ def chat ( self , id ) : 
json = self . skype . conn ( "GET" , "{0}/users/ME/conversations/{1}" . format ( self . skype . conn . msgsHost , id ) , 
auth = SkypeConnection . Auth . RegToken , params = { "view" : "msnp24Equivalent" } ) . json ( ) 
cls = SkypeSingleChat 
~~ return self . merge ( cls . fromRaw ( self . skype , json ) ) 
~~ def create ( self , members = ( ) , admins = ( ) ) : 
memberObjs = [ { "id" : "8:{0}" . format ( self . skype . userId ) , "role" : "Admin" } ] 
for id in members : 
~~~ if id == self . skype . userId : 
~~ memberObjs . append ( { "id" : "8:{0}" . format ( id ) , "role" : "Admin" if id in admins else "User" } ) 
~~ resp = self . skype . conn ( "POST" , "{0}/threads" . format ( self . skype . conn . msgsHost ) , 
auth = SkypeConnection . Auth . RegToken , json = { "members" : memberObjs } ) 
return self . chat ( resp . headers [ "Location" ] . rsplit ( "/" , 1 ) [ 1 ] ) 
~~ def urlToIds ( url ) : 
convUrl = "https://join.skype.com/api/v2/conversation/" 
json = SkypeConnection . externalCall ( "POST" , convUrl , json = { "shortId" : urlId , "type" : "wl" } ) . json ( ) 
return { "id" : json . get ( "Resource" ) , 
"long" : json . get ( "Id" ) , 
"blob" : json . get ( "ChatBlob" ) } 
~~ def userToId ( url ) : 
match = re . search ( r"users(/ME/contacts)?/[0-9]+:([^/]+)" , url ) 
return match . group ( 2 ) if match else None 
~~ def chatToId ( url ) : 
match = re . search ( r"conversations/([0-9]+:[^/]+)" , url ) 
return match . group ( 1 ) if match else None 
~~ def initAttrs ( cls ) : 
def __init__ ( self , skype = None , raw = None , * args , ** kwargs ) : 
~~~ super ( cls , self ) . __init__ ( skype , raw ) 
for i in range ( len ( args ) ) : 
~~~ kwargs [ cls . attrs [ i ] ] = args [ i ] 
~~ unknown = set ( kwargs ) - set ( cls . attrs ) 
if unknown : 
~~ for k in cls . attrs : 
~~~ setattr ( self , k , kwargs . get ( k , cls . defaults . get ( k ) ) ) 
~~ ~~ setattr ( cls , "__init__" , __init__ ) 
return cls 
~~ def convertIds ( * types , ** kwargs ) : 
user = kwargs . get ( "user" , ( ) ) 
users = kwargs . get ( "users" , ( ) ) 
chat = kwargs . get ( "chat" , ( ) ) 
def userObj ( self , field ) : 
~~~ return self . skype . contacts [ getattr ( self , field ) ] 
~~ def userObjs ( self , field ) : 
~~~ return ( self . skype . contacts [ id ] for id in getattr ( self , field ) ) 
~~ def chatObj ( self , field ) : 
~~~ return self . skype . chats [ getattr ( self , field ) ] 
~~ def attach ( cls , fn , field , idField ) : 
setattr ( cls , field , property ( functools . wraps ( fn ) ( functools . partial ( fn , field = idField ) ) ) ) 
~~ def wrapper ( cls ) : 
~~~ for type in types : 
~~~ if type == "user" : 
~~~ attach ( cls , userObj , "user" , "userId" ) 
~~ elif type == "users" : 
~~~ attach ( cls , userObjs , "users" , "userIds" ) 
~~ elif type == "chat" : 
~~~ attach ( cls , chatObj , "chat" , "chatId" ) 
~~ ~~ for field in user : 
~~~ attach ( cls , userObj , field , "{0}Id" . format ( field ) ) 
~~ for field in users : 
~~~ attach ( cls , userObjs , "{0}s" . format ( field ) , "{0}Ids" . format ( field ) ) 
~~ for field in chat : 
~~~ attach ( cls , chatObj , field , "{0}Id" . format ( field ) ) 
~~ return cls 
~~ def truthyAttrs ( cls ) : 
def __bool__ ( self ) : 
~~~ return bool ( any ( getattr ( self , attr ) for attr in self . attrs ) ) 
~~ cls . __bool__ = cls . __nonzero__ = __bool__ 
~~ def cacheResult ( fn ) : 
cache = { } 
@ functools . wraps ( fn ) 
def wrapper ( * args , ** kwargs ) : 
~~~ key = args + tuple ( kwargs . items ( ) ) 
~~~ if key in cache : 
~~~ return cache [ key ] 
~~~ return fn ( * args , ** kwargs ) 
~~ cache [ key ] = fn ( * args , ** kwargs ) 
return cache [ key ] 
~~ wrapper . cache = cache 
return wrapper 
~~ def exhaust ( fn , transform = None , * args , ** kwargs ) : 
~~~ iterRes = fn ( * args , ** kwargs ) 
if iterRes : 
~~~ for item in transform ( iterRes ) if transform else iterRes : 
~~ ~~ ~~ def write_to ( self , out ) : 
out . write ( bytes ( self . header ) ) 
out . write ( self . record_data ) 
~~ def read_from ( cls , data_stream ) : 
raw_vlr = cls ( ) 
header = RawVLRHeader . from_stream ( data_stream ) 
raw_vlr . header = header 
raw_vlr . record_data = data_stream . read ( header . record_length_after_header ) 
return raw_vlr 
~~ def parse_geo_tiff_keys_from_vlrs ( vlr_list : vlrlist . VLRList ) -> List [ GeoTiffKey ] : 
geo_key_dir = vlr_list . get_by_id ( 
GeoKeyDirectoryVlr . official_user_id ( ) , GeoKeyDirectoryVlr . official_record_ids ( ) 
geo_doubles = vlr_list . get_by_id ( 
GeoDoubleParamsVlr . official_user_id ( ) , GeoDoubleParamsVlr . official_record_ids ( ) 
geo_ascii = vlr_list . get_by_id ( 
GeoAsciiParamsVlr . official_user_id ( ) , GeoAsciiParamsVlr . official_record_ids ( ) 
return parse_geo_tiff ( geo_key_dir , geo_doubles , geo_ascii ) 
~~ def parse_geo_tiff ( 
key_dir_vlr : GeoKeyDirectoryVlr , 
double_vlr : GeoDoubleParamsVlr , 
ascii_vlr : GeoAsciiParamsVlr , 
) -> List [ GeoTiffKey ] : 
geotiff_keys = [ ] 
for k in key_dir_vlr . geo_keys : 
~~~ if k . tiff_tag_location == 0 : 
~~~ value = k . value_offset 
~~ elif k . tiff_tag_location == 34736 : 
~~~ value = double_vlr . doubles [ k . value_offset ] 
~~ elif k . tiff_tag_location == 34737 : 
~~~ value = ascii_vlr . strings [ k . value_offset ] [ k . count : ] 
~~~ value = ascii_vlr . strings [ 0 ] [ k . value_offset : k . value_offset + k . count ] 
k . tiff_tag_location 
~~ geotiff_keys . append ( GeoTiffKey ( k . id , value ) ) 
~~ return geotiff_keys 
~~ def get_signedness_for_extra_dim ( type_index ) : 
~~~ t = _extra_dims_style_2 [ type_index ] 
if "uint" in t : 
~~~ return DimensionSignedness . UNSIGNED 
~~ elif "int" in t : 
~~~ return DimensionSignedness . SIGNED 
~~~ return DimensionSignedness . FLOATING 
~~ ~~ except IndexError : 
~~~ raise errors . UnknownExtraType ( type_index ) 
~~ ~~ def get_id_for_extra_dim_type ( type_str ) : 
~~~ return _type_to_extra_dim_id_style_1 [ type_str ] 
~~~ return _type_to_extra_dim_id_style_2 [ type_str ] 
~~~ raise errors . UnknownExtraType ( type_str ) 
~~ ~~ ~~ def from_point_record ( cls , other_point_record , new_point_format ) : 
array = np . zeros_like ( other_point_record . array , dtype = new_point_format . dtype ) 
new_record = cls ( array , new_point_format ) 
new_record . copy_fields_from ( other_point_record ) 
~~ def copy_fields_from ( self , other_record ) : 
for dim_name in self . dimensions_names : 
~~~ self [ dim_name ] = other_record [ dim_name ] 
~~ ~~ ~~ def _append_zeros_if_too_small ( self , value ) : 
size_diff = len ( value ) - len ( self . array ) 
if size_diff : 
~~~ self . array = np . append ( 
self . array , np . zeros ( size_diff , dtype = self . array . dtype ) 
~~ ~~ def all_dimensions_names ( self ) : 
return frozenset ( self . array . dtype . names + tuple ( self . sub_fields_dict . keys ( ) ) ) 
~~ def zeros ( cls , point_format , point_count ) : 
data = np . zeros ( point_count , point_format . dtype ) 
return cls ( data , point_format ) 
~~ def from_stream ( cls , stream , point_format , count ) : 
points_dtype = point_format . dtype 
point_data_buffer = bytearray ( stream . read ( count * points_dtype . itemsize ) ) 
~~~ data = np . frombuffer ( point_data_buffer , dtype = points_dtype , count = count ) 
~~~ expected_bytes_len = count * points_dtype . itemsize 
if len ( point_data_buffer ) % points_dtype . itemsize != 0 : 
~~~ missing_bytes_len = expected_bytes_len - len ( point_data_buffer ) 
raise_not_enough_bytes_error ( 
expected_bytes_len , 
missing_bytes_len , 
len ( point_data_buffer ) , 
points_dtype , 
~~~ actual_count = len ( point_data_buffer ) // points_dtype . itemsize 
logger . critical ( 
count , actual_count , count - actual_count 
data = np . frombuffer ( 
point_data_buffer , dtype = points_dtype , count = actual_count 
~~ ~~ return cls ( data , point_format ) 
~~ def from_compressed_buffer ( cls , compressed_buffer , point_format , count , laszip_vlr ) : 
point_dtype = point_format . dtype 
uncompressed = decompress_buffer ( 
compressed_buffer , point_dtype , count , laszip_vlr 
return cls ( uncompressed , point_format ) 
~~ def x ( self ) : 
return scale_dimension ( self . X , self . header . x_scale , self . header . x_offset ) 
~~ def y ( self ) : 
return scale_dimension ( self . Y , self . header . y_scale , self . header . y_offset ) 
~~ def z ( self ) : 
return scale_dimension ( self . Z , self . header . z_scale , self . header . z_offset ) 
~~ def points ( self , value ) : 
if value . dtype != self . points . dtype : 
~~ new_point_record = record . PackedPointRecord ( value , self . points_data . point_format ) 
dims . raise_if_version_not_compatible_with_fmt ( 
new_point_record . point_format . id , self . header . version 
self . points_data = new_point_record 
self . update_header ( ) 
~~ def add_extra_dim ( self , name , type , description = "" ) : 
type_id = extradims . get_id_for_extra_dim_type ( type ) 
extra_byte = ExtraBytesStruct ( 
data_type = type_id , name = name . encode ( ) , description = description . encode ( ) 
~~~ extra_bytes_vlr = self . vlrs . get ( "ExtraBytesVlr" ) [ 0 ] 
~~~ extra_bytes_vlr = ExtraBytesVlr ( ) 
self . vlrs . append ( extra_bytes_vlr ) 
~~~ extra_bytes_vlr . extra_bytes_structs . append ( extra_byte ) 
self . points_data . add_extra_dims ( [ ( name , type ) ] ) 
~~ ~~ def write_to ( self , out_stream , do_compress = False ) : 
self . vlrs . get ( "ExtraBytesVlr" ) 
and not self . points_data . extra_dimensions_names 
self . vlrs . extract ( "ExtraBytesVlr" ) 
~~ if do_compress : 
~~~ laz_vrl = create_laz_vlr ( self . points_data ) 
self . vlrs . append ( known . LasZipVlr ( laz_vrl . data ( ) ) ) 
raw_vlrs = vlrlist . RawVLRList . from_list ( self . vlrs ) 
self . header . offset_to_point_data = ( 
self . header . size + raw_vlrs . total_size_in_bytes ( ) 
self . header . point_format_id = uncompressed_id_to_compressed ( 
self . header . point_format_id 
self . header . number_of_vlr = len ( raw_vlrs ) 
points_bytes = compress_buffer ( 
np . frombuffer ( self . points_data . array , np . uint8 ) , 
laz_vrl . schema , 
self . header . offset_to_point_data , 
) . tobytes ( ) 
~~~ raw_vlrs = vlrlist . RawVLRList . from_list ( self . vlrs ) 
points_bytes = self . points_data . raw_bytes ( ) 
~~ self . header . write_to ( out_stream ) 
self . _raise_if_not_expected_pos ( out_stream , self . header . size ) 
raw_vlrs . write_to ( out_stream ) 
self . _raise_if_not_expected_pos ( out_stream , self . header . offset_to_point_data ) 
out_stream . write ( points_bytes ) 
~~ def write_to_file ( self , filename , do_compress = None ) : 
is_ext_laz = filename . split ( "." ) [ - 1 ] == "laz" 
if is_ext_laz and do_compress is None : 
~~~ do_compress = True 
~~ with open ( filename , mode = "wb" ) as out : 
~~~ self . write_to ( out , do_compress = do_compress ) 
~~ ~~ def write ( self , destination , do_compress = None ) : 
if isinstance ( destination , str ) : 
~~~ self . write_to_file ( destination ) 
~~~ if do_compress is None : 
~~~ do_compress = False 
~~ self . write_to ( destination , do_compress = do_compress ) 
~~ ~~ def _build_point_formats_dtypes ( point_format_dimensions , dimensions_dict ) : 
fmt_id : _point_format_to_dtype ( point_fmt , dimensions_dict ) 
for fmt_id , point_fmt in point_format_dimensions . items ( ) 
~~ def _build_unpacked_point_formats_dtypes ( 
point_formats_dimensions , composed_fields_dict , dimensions_dict 
unpacked_dtypes = { } 
for fmt_id , dim_names in point_formats_dimensions . items ( ) : 
~~~ composed_dims , dtype = composed_fields_dict [ fmt_id ] , [ ] 
for dim_name in dim_names : 
~~~ if dim_name in composed_dims : 
~~~ dtype . extend ( ( f . name , f . type ) for f in composed_dims [ dim_name ] ) 
~~~ dtype . append ( dimensions_dict [ dim_name ] ) 
~~ ~~ unpacked_dtypes [ fmt_id ] = np . dtype ( dtype ) 
~~ return unpacked_dtypes 
~~ def np_dtype_to_point_format ( dtype , unpacked = False ) : 
all_dtypes = ( 
ALL_POINT_FORMATS_DTYPE if not unpacked else UNPACKED_POINT_FORMATS_DTYPES 
for format_id , fmt_dtype in all_dtypes . items ( ) : 
~~~ if fmt_dtype == dtype : 
~~~ return format_id 
~~~ raise errors . IncompatibleDataFormat ( 
dtype 
~~ ~~ def min_file_version_for_point_format ( point_format_id ) : 
for version , point_formats in sorted ( VERSION_TO_POINT_FMT . items ( ) ) : 
~~~ if point_format_id in point_formats : 
~~~ return version 
~~~ raise errors . PointFormatNotSupported ( point_format_id ) 
~~ ~~ def is_point_fmt_compatible_with_version ( point_format_id , file_version ) : 
~~~ return point_format_id in VERSION_TO_POINT_FMT [ str ( file_version ) ] 
~~~ raise errors . FileVersionNotSupported ( file_version ) 
~~ ~~ def get_by_id ( self , user_id = "" , record_ids = ( None , ) ) : 
if user_id != "" and record_ids != ( None , ) : 
vlr 
for vlr in self . vlrs 
if vlr . user_id == user_id and vlr . record_id in record_ids 
if vlr . user_id == user_id or vlr . record_id in record_ids 
~~ ~~ def get ( self , vlr_type ) : 
return [ v for v in self . vlrs if v . __class__ . __name__ == vlr_type ] 
~~ def extract ( self , vlr_type ) : 
kept_vlrs , extracted_vlrs = [ ] , [ ] 
for vlr in self . vlrs : 
~~~ if vlr . __class__ . __name__ == vlr_type : 
~~~ extracted_vlrs . append ( vlr ) 
~~~ kept_vlrs . append ( vlr ) 
~~ ~~ self . vlrs = kept_vlrs 
return extracted_vlrs 
~~ def read_from ( cls , data_stream , num_to_read ) : 
vlrlist = cls ( ) 
for _ in range ( num_to_read ) : 
~~~ raw = RawVLR . read_from ( data_stream ) 
~~~ vlrlist . append ( vlr_factory ( raw ) ) 
~~ ~~ return vlrlist 
~~ def files_have_same_point_format_id ( las_files ) : 
point_format_found = { las . header . point_format_id for las in las_files } 
return len ( point_format_found ) == 1 
~~ def files_have_same_dtype ( las_files ) : 
dtypes = { las . points . dtype for las in las_files } 
return len ( dtypes ) == 1 
~~ def _raise_if_wrong_file_signature ( stream ) : 
file_sig = stream . read ( len ( headers . LAS_FILE_SIGNATURE ) ) 
if file_sig != headers . LAS_FILE_SIGNATURE : 
~~~ raise errors . PylasError ( 
~~ ~~ def read_header ( self ) : 
self . stream . seek ( self . start_pos ) 
return headers . HeaderFactory ( ) . read_from_stream ( self . stream ) 
~~ def read_vlrs ( self ) : 
self . stream . seek ( self . start_pos + self . header . size ) 
return VLRList . read_from ( self . stream , num_to_read = self . header . number_of_vlr ) 
vlrs = self . read_vlrs ( ) 
self . _warn_if_not_at_expected_pos ( 
self . stream . seek ( self . start_pos + self . header . offset_to_point_data ) 
~~~ points = self . _read_points ( vlrs ) 
~~ except ( RuntimeError , errors . LazPerfNotFound ) as e : 
self . __init__ ( io . BytesIO ( laszip_decompress ( self . stream ) ) ) 
return self . read ( ) 
~~ if points . point_format . has_waveform_packet : 
~~~ self . stream . seek ( 
self . start_pos + self . header . start_of_waveform_data_packet_record 
if self . header . global_encoding . are_waveform_flag_equal ( ) : 
"set" 
if self . header . global_encoding . waveform_internal 
else "unset" 
~~ if self . header . global_encoding . waveform_internal : 
~~~ _ , _ = self . _read_internal_waveform_packet ( ) 
~~ elif self . header . global_encoding . waveform_external : 
~~ ~~ if self . header . version >= "1.4" : 
~~~ evlrs = self . read_evlrs ( ) 
return las14 . LasData ( 
header = self . header , vlrs = vlrs , points = points , evlrs = evlrs 
~~ return las12 . LasData ( header = self . header , vlrs = vlrs , points = points ) 
~~ def _read_points ( self , vlrs ) : 
~~~ extra_dims = vlrs . get ( "ExtraBytesVlr" ) [ 0 ] . type_of_extra_dims ( ) 
~~~ extra_dims = None 
~~ point_format = PointFormat ( self . header . point_format_id , extra_dims = extra_dims ) 
if self . header . are_points_compressed : 
~~~ laszip_vlr = vlrs . pop ( vlrs . index ( "LasZipVlr" ) ) 
points = self . _read_compressed_points_data ( laszip_vlr , point_format ) 
~~~ points = record . PackedPointRecord . from_stream ( 
self . stream , point_format , self . header . point_count 
~~ return points 
~~ def _read_compressed_points_data ( self , laszip_vlr , point_format ) : 
offset_to_chunk_table = struct . unpack ( "<q" , self . stream . read ( 8 ) ) [ 0 ] 
size_of_point_data = offset_to_chunk_table - self . stream . tell ( ) 
if offset_to_chunk_table <= 0 : 
offset_to_chunk_table 
~~ points = record . PackedPointRecord . from_compressed_buffer ( 
self . stream . read ( size_of_point_data ) , 
point_format , 
self . header . point_count , 
laszip_vlr , 
return points 
~~ def _read_internal_waveform_packet ( self ) : 
b = bytearray ( self . stream . read ( rawvlr . VLR_HEADER_SIZE ) ) 
waveform_header = rawvlr . RawVLRHeader . from_buffer ( b ) 
waveform_record = self . stream . read ( ) 
return waveform_header , waveform_record 
~~ def read_evlrs ( self ) : 
self . stream . seek ( self . start_pos + self . header . start_of_first_evlr ) 
return evlrs . EVLRList . read_from ( self . stream , self . header . number_of_evlr ) 
~~ def _warn_if_not_at_expected_pos ( self , expected_pos , end_of , start_of ) : 
diff = expected_pos - self . stream . tell ( ) 
if diff != 0 : 
~~ ~~ def vlr_factory ( raw_vlr ) : 
user_id = raw_vlr . header . user_id . rstrip ( NULL_BYTE ) . decode ( ) 
known_vlrs = BaseKnownVLR . __subclasses__ ( ) 
for known_vlr in known_vlrs : 
known_vlr . official_user_id ( ) == user_id 
and raw_vlr . header . record_id in known_vlr . official_record_ids ( ) 
~~~ return known_vlr . from_raw ( raw_vlr ) 
~~~ return VLR . from_raw ( raw_vlr ) 
~~ ~~ def open_las ( source , closefd = True ) : 
if isinstance ( source , str ) : 
~~~ stream = open ( source , mode = "rb" ) 
if not closefd : 
~~ ~~ elif isinstance ( source , bytes ) : 
~~~ stream = io . BytesIO ( source ) 
~~~ stream = source 
~~ return LasReader ( stream , closefd = closefd ) 
~~ def read_las ( source , closefd = True ) : 
with open_las ( source , closefd = closefd ) as reader : 
~~~ return reader . read ( ) 
~~ ~~ def create_from_header ( header ) : 
header = copy . copy ( header ) 
header . point_count = 0 
points = record . PackedPointRecord . empty ( PointFormat ( header . point_format_id ) ) 
if header . version >= "1.4" : 
~~~ return las14 . LasData ( header = header , points = points ) 
~~ return las12 . LasData ( header = header , points = points ) 
~~ def create_las ( * , point_format_id = 0 , file_version = None ) : 
if file_version is not None : 
~~~ dims . raise_if_version_not_compatible_with_fmt ( point_format_id , file_version ) 
~~~ file_version = dims . min_file_version_for_point_format ( point_format_id ) 
~~ header = headers . HeaderFactory . new ( file_version ) 
header . point_format_id = point_format_id 
if file_version >= "1.4" : 
~~~ return las14 . LasData ( header = header ) 
~~ return las12 . LasData ( header = header ) 
~~ def convert ( source_las , * , point_format_id = None , file_version = None ) : 
if point_format_id is None : 
~~~ point_format_id = source_las . points_data . point_format . id 
~~ if file_version is None : 
~~~ file_version = max ( 
source_las . header . version , 
dims . min_file_version_for_point_format ( point_format_id ) , 
~~~ file_version = str ( file_version ) 
dims . raise_if_version_not_compatible_with_fmt ( point_format_id , file_version ) 
~~ header = headers . HeaderFactory . convert_header ( source_las . header , file_version ) 
point_format = PointFormat ( 
point_format_id , source_las . points_data . point_format . extra_dims 
points = record . PackedPointRecord . from_point_record ( 
source_las . points_data , point_format 
~~~ evlrs = source_las . evlrs 
~~~ evlrs = [ ] 
~~ if file_version >= "1.4" : 
~~~ las = las14 . LasData ( 
header = header , vlrs = source_las . vlrs , points = points , evlrs = evlrs 
~~~ if evlrs : 
len ( evlrs ) , file_version 
~~ las = las12 . LasData ( header = header , vlrs = source_las . vlrs , points = points ) 
~~ return las 
~~ def merge_las ( * las_files ) : 
if len ( las_files ) == 1 : 
~~~ las_files = las_files [ 0 ] 
~~ if not las_files : 
~~ if not utils . files_have_same_dtype ( las_files ) : 
~~ header = las_files [ 0 ] . header 
num_pts_merged = sum ( len ( las . points ) for las in las_files ) 
merged = create_from_header ( header ) 
for dim_name , dim_type in las_files [ 0 ] . points_data . point_format . extra_dims : 
~~~ merged . add_extra_dim ( dim_name , dim_type ) 
~~ merged . points = np . zeros ( num_pts_merged , merged . points . dtype ) 
merged_x = np . zeros ( num_pts_merged , np . float64 ) 
merged_y = np . zeros ( num_pts_merged , np . float64 ) 
merged_z = np . zeros ( num_pts_merged , np . float64 ) 
for i , las in enumerate ( las_files , start = 1 ) : 
~~~ slc = slice ( offset , offset + len ( las . points ) ) 
merged . points [ slc ] = las . points 
merged_x [ slc ] = las . x 
merged_y [ slc ] = las . y 
merged_z [ slc ] = las . z 
merged [ 'point_source_id' ] [ slc ] = i 
offset += len ( las . points ) 
~~ merged . x = merged_x 
merged . y = merged_y 
merged . z = merged_z 
return merged 
~~ def write_then_read_again ( las , do_compress = False ) : 
out = io . BytesIO ( ) 
las . write ( out , do_compress = do_compress ) 
out . seek ( 0 ) 
return read_las ( out ) 
~~ def date ( self ) : 
~~~ return datetime . date ( self . creation_year , 1 , 1 ) + datetime . timedelta ( 
self . creation_day_of_year - 1 
~~ ~~ def date ( self , date ) : 
self . creation_year = date . year 
self . creation_day_of_year = date . timetuple ( ) . tm_yday 
~~ def mins ( self ) : 
return np . array ( [ self . x_min , self . y_min , self . z_min ] ) 
~~ def mins ( self , value ) : 
self . x_min , self . y_min , self . z_min = value 
~~ def maxs ( self ) : 
return np . array ( [ self . x_max , self . y_max , self . z_max ] ) 
~~ def maxs ( self , value ) : 
self . x_max , self . y_max , self . z_max = value 
~~ def scales ( self ) : 
return np . array ( [ self . x_scale , self . y_scale , self . z_scale ] ) 
~~ def offsets ( self ) : 
return np . array ( [ self . x_offset , self . y_offset , self . z_offset ] ) 
~~ def header_class_for_version ( cls , version ) : 
~~~ return cls . _version_to_header [ str ( version ) ] 
~~~ raise errors . FileVersionNotSupported ( version ) 
~~ ~~ def peek_file_version ( cls , stream ) : 
old_pos = stream . tell ( ) 
stream . seek ( cls . _offset_to_major_version ) 
major = int . from_bytes ( stream . read ( ctypes . sizeof ( ctypes . c_uint8 ) ) , "little" ) 
minor = int . from_bytes ( stream . read ( ctypes . sizeof ( ctypes . c_uint8 ) ) , "little" ) 
stream . seek ( old_pos ) 
return "{}.{}" . format ( major , minor ) 
~~ def convert_header ( cls , old_header , new_version ) : 
new_header_class = cls . header_class_for_version ( new_version ) 
b = bytearray ( old_header ) 
b += b"\\x00" * ( ctypes . sizeof ( new_header_class ) - len ( b ) ) 
new_header = new_header_class . from_buffer ( b ) 
new_header . version = str ( new_version ) 
return new_header 
~~ def unpack ( source_array , mask , dtype = np . uint8 ) : 
lsb = least_significant_bit ( mask ) 
return ( ( source_array & mask ) >> lsb ) . astype ( dtype ) 
~~ def pack ( array , sub_field_array , mask , inplace = False ) : 
max_value = int ( mask >> lsb ) 
if sub_field_array . max ( ) > max_value : 
~~~ raise OverflowError ( 
sub_field_array . max ( ) , max_value 
~~ if inplace : 
~~~ array [ : ] = array & ~ mask 
array [ : ] = array | ( ( sub_field_array << lsb ) & mask ) . astype ( array . dtype ) 
~~~ array = array & ~ mask 
return array | ( ( sub_field_array << lsb ) & mask ) . astype ( array . dtype ) 
~~ ~~ def lost_dimensions ( point_fmt_in , point_fmt_out ) : 
unpacked_dims_in = PointFormat ( point_fmt_in ) . dtype 
unpacked_dims_out = PointFormat ( point_fmt_out ) . dtype 
out_dims = unpacked_dims_out . fields 
completely_lost = [ ] 
for dim_name in unpacked_dims_in . names : 
~~~ if dim_name not in out_dims : 
~~~ completely_lost . append ( dim_name ) 
~~ ~~ return completely_lost 
~~ def dtype ( self ) : 
dtype = self . _access_dict ( dims . ALL_POINT_FORMATS_DTYPE , self . id ) 
dtype = self . _dtype_add_extra_dims ( dtype ) 
return dtype 
~~ def unpacked_dtype ( self ) : 
dtype = self . _access_dict ( dims . UNPACKED_POINT_FORMATS_DTYPES , self . id ) 
~~ def sub_fields ( self ) : 
sub_fields_dict = { } 
for composed_dim_name , sub_fields in self . composed_fields . items ( ) : 
~~~ for sub_field in sub_fields : 
~~~ sub_fields_dict [ sub_field . name ] = ( composed_dim_name , sub_field ) 
~~ ~~ return sub_fields_dict 
~~ def num_extra_bytes ( self ) : 
return sum ( np . dtype ( extra_dim [ 1 ] ) . itemsize for extra_dim in self . extra_dims ) 
~~ def has_waveform_packet ( self ) : 
dimensions = set ( self . dimension_names ) 
return all ( name in dimensions for name in dims . WAVEFORM_FIELDS_NAMES ) 
~~ def main ( port , ip , command , loglevel ) : 
numeric_level = getattr ( logging , loglevel . upper ( ) , None ) 
if not isinstance ( numeric_level , int ) : 
~~ logging . basicConfig ( level = numeric_level ) 
if command == "demo" : 
~~~ demo ( ip , port ) 
~~ ~~ def checksum ( command ) : 
crc = 0x147A 
for b in command : 
~~~ crc = ( ( crc << 1 ) & 0xFFFF ) | ( crc & 0x8000 ) >> 15 
crc = crc ^ 0xFFFF 
crc = ( crc + ( crc >> 8 ) + b ) & 0xFFFF 
~~ return crc 
~~ def print_hex ( data ) : 
hex_msg = "" 
for c in data : 
~~~ hex_msg += "\\\\x" + format ( c , "02x" ) 
~~ _LOGGER . debug ( hex_msg ) 
~~ def verify_and_strip ( resp ) : 
if resp [ 0 : 2 ] != b'\\xFE\\xFE' : 
print_hex ( resp ) 
~~ if resp [ - 2 : ] != b'\\xFE\\x0D' : 
~~ output = resp [ 2 : - 2 ] . replace ( b'\\xFE\\xF0' , b'\\xFE' ) 
c = checksum ( bytearray ( output [ 0 : - 2 ] ) ) 
if ( 256 * output [ - 2 : - 1 ] [ 0 ] + output [ - 1 : ] [ 0 ] ) != c : 
( 256 * output [ - 2 : - 1 ] [ 0 ] + output [ - 1 : ] [ 0 ] ) , c ) ) 
~~ return output [ 0 : - 2 ] 
~~ def list_set_bits ( r , expected_length ) : 
set_bit_numbers = [ ] 
bit_index = 0x1 
assert ( len ( r ) == expected_length + 1 ) 
for b in r [ 1 : ] : 
~~~ for i in range ( 8 ) : 
~~~ if ( ( b >> i ) & 1 ) == 1 : 
~~~ set_bit_numbers . append ( bit_index ) 
~~ bit_index += 1 
~~ ~~ return set_bit_numbers 
~~ def generate_query ( command ) : 
data = bytearray ( command ) 
c = checksum ( data ) 
data . append ( c >> 8 ) 
data . append ( c & 0xFF ) 
data . replace ( b'\\xFE' , b'\\xFE\\xF0' ) 
data = bytearray . fromhex ( "FEFE" ) + data + bytearray . fromhex ( "FE0D" ) 
~~ def demo ( host , port ) : 
stl = AsyncSatel ( host , 
port , 
loop , 
[ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 
20 , 21 , 22 , 23 , 25 , 26 , 27 , 28 , 29 , 30 ] , 
[ 8 , 9 , 10 ] 
loop . run_until_complete ( stl . connect ( ) ) 
loop . create_task ( stl . arm ( "3333" , 1 ) ) 
loop . create_task ( stl . disarm ( "3333" ) ) 
loop . create_task ( stl . keep_alive ( ) ) 
loop . create_task ( stl . monitor_status ( ) ) 
loop . close ( ) 
~~ async def connect ( self ) : 
_LOGGER . debug ( "Connecting..." ) 
~~~ self . _reader , self . _writer = await asyncio . open_connection ( 
self . _host , self . _port , loop = self . _loop ) 
self . _writer = None 
self . _reader = None 
~~ async def start_monitoring ( self ) : 
data = generate_query ( 
b'\\x7F\\x01\\xDC\\x99\\x80\\x00\\x04\\x00\\x00\\x00\\x00\\x00\\x00' ) 
await self . _send_data ( data ) 
resp = await self . _read_data ( ) 
if resp is None : 
~~ if resp [ 1 : 2 ] != b'\\xFF' : 
~~ ~~ def _output_changed ( self , msg ) : 
status = { "outputs" : { } } 
output_states = list_set_bits ( msg , 32 ) 
self . violated_outputs = output_states 
output_states , self . _monitored_outputs ) 
for output in self . _monitored_outputs : 
~~~ status [ "outputs" ] [ output ] = 1 if output in output_states else 0 
if self . _output_changed_callback : 
~~~ self . _output_changed_callback ( status ) 
~~ return status 
~~ async def arm ( self , code , partition_list , mode = 0 ) : 
while len ( code ) < 16 : 
~~~ code += 'F' 
~~ code_bytes = bytearray . fromhex ( code ) 
mode_command = 0x80 + mode 
data = generate_query ( mode_command . to_bytes ( 1 , 'big' ) 
+ code_bytes 
+ partition_bytes ( partition_list ) ) 
~~ async def disarm ( self , code , partition_list ) : 
data = generate_query ( b'\\x84' + code_bytes 
~~ async def clear_alarm ( self , code , partition_list ) : 
data = generate_query ( b'\\x85' + code_bytes 
~~ async def set_output ( self , code , output_id , state ) : 
mode_command = 0x88 if state else 0x89 
data = generate_query ( mode_command . to_bytes ( 1 , 'big' ) + 
code_bytes + 
output_bytes ( output_id ) ) 
~~ async def keep_alive ( self ) : 
~~~ await asyncio . sleep ( self . _keep_alive_timeout ) 
if self . closed : 
~~ data = generate_query ( b'\\xEE\\x01\\x01' ) 
~~ ~~ async def monitor_status ( self , alarm_status_callback = None , 
zone_changed_callback = None , 
output_changed_callback = None ) : 
self . _alarm_status_callback = alarm_status_callback 
self . _zone_changed_callback = zone_changed_callback 
self . _output_changed_callback = output_changed_callback 
while not self . closed : 
while not self . connected : 
await self . connect ( ) 
if not self . connected : 
await asyncio . sleep ( self . _reconnection_timeout ) 
~~ ~~ await self . start_monitoring ( ) 
~~~ await self . _update_status ( ) 
_LOGGER . debug ( "Closing..." ) 
self . closed = True 
if self . connected : 
~~~ self . _writer . close ( ) 
~~ ~~ def get_author_string ( self , links = False ) : 
~~~ saved_args = locals ( ) 
saved_args = saved_args [ 'links' ] 
def format_author ( author ) : 
~~~ if links and author . person . slug : 
~~~ return \ % ( author . person . slug , author . person . full_name ) 
~~ return author . person . full_name 
~~ if links == True or links == False : 
~~~ authors = map ( format_author , self . authors . all ( ) ) 
~~~ authors = map ( format_author , saved_args ) 
~~ if not authors : 
~~ elif len ( authors ) == 1 : 
~~~ return authors [ 0 ] 
~~ def get_author_type_string ( self ) : 
authorTypeString = '' 
aStringA = '' 
aStringB = '' 
aStringC = '' 
aStringD = '' 
authors = dict ( ( k , list ( v ) ) for k , v in groupby ( self . authors . all ( ) , lambda a : a . type ) ) 
for author in authors : 
~~~ if author == 'author' : 
~~ if author == 'photographer' : 
~~ if author == 'illustrator' : 
~~ if author == 'videographer' : 
~~ ~~ if aStringA != '' : 
~~~ authorTypeString += aStringA 
~~ if aStringB != '' : 
~~ if aStringC != '' : 
~~ if aStringD != '' : 
~~ return authorTypeString 
~~ def sanitize_block ( self , block ) : 
embed_type = block . get ( 'type' , None ) 
data = block . get ( 'data' , { } ) 
serializer = self . serializers . get ( embed_type , None ) 
if serializer is None : 
~~~ return block 
~~ block [ 'data' ] = serializer . to_internal_value ( data ) 
return block 
~~ def queue_instance ( self , embed_type , data ) : 
~~ instance_id = serializer . get_id ( data ) 
if embed_type not in self . ids : 
~~~ self . ids [ embed_type ] = [ ] 
~~ self . ids [ embed_type ] . append ( instance_id ) 
~~ def load_instances ( self , embed_type , ids ) : 
~~ self . instances [ embed_type ] = serializer . fetch ( ids ) 
~~ def insert_instance ( self , block ) : 
~~~ instance_id = serializer . get_id ( data ) 
instance = self . instances [ embed_type ] [ instance_id ] 
data [ embed_type ] = serializer . serialize ( instance ) 
~~~ data [ embed_type ] = None 
~~ block [ 'data' ] = data 
~~ def load_data ( self ) : 
for embed_type in self . ids . keys ( ) : 
~~~ self . load_instances ( embed_type , self . ids [ embed_type ] ) 
~~ ~~ def validate ( self , data ) : 
from dispatch . theme import ThemeManager 
errors = { } 
if data . get ( 'widget' ) is not None : 
~~~ widget = ThemeManager . Widgets . get ( data [ 'widget' ] ) 
~~ except WidgetNotFound as e : 
~~~ errors [ 'widget' ] = str ( e ) 
~~~ for field in widget . fields : 
~~~ field_data = data [ 'data' ] . get ( field . name ) 
if field_data is not None : 
~~~ field . validate ( field_data ) 
~~ except InvalidField as e : 
~~~ errors [ field . name ] = str ( e ) 
~~ ~~ elif field . required : 
~~ ~~ ~~ ~~ if errors : 
~~ def admin ( request ) : 
context = { 
'api_url' : settings . API_URL , 
'app_js_bundle' : 'manager-%s.js' % dispatch . __version__ , 
'app_css_bundle' : 'manager-%s.css' % dispatch . __version__ 
return render_to_response ( 'manager/index.html' , context ) 
for field in self . fields : 
~~~ result [ field . name ] = field . to_json ( self . data . get ( field . name ) ) 
~~ def hide_authenticated_fields ( self ) : 
authenticated_fields = getattr ( self . Meta , 'authenticated_fields' , [ ] ) 
if not self . is_authenticated ( ) : 
~~~ for field in authenticated_fields : 
~~~ self . fields . pop ( field ) 
~~ ~~ ~~ def exclude_fields ( self ) : 
request = self . context . get ( 'request' ) 
if request : 
~~~ exclude = request . query_params . get ( 'exclude' , None ) 
if exclude is None : return 
excluded_fields = exclude . split ( ',' ) 
for field in excluded_fields : 
~~ ~~ ~~ def get ( self , * args , ** kwargs ) : 
if 'pk' in kwargs : 
~~~ kwargs [ 'parent' ] = kwargs [ 'pk' ] 
kwargs [ 'head' ] = True 
del kwargs [ 'pk' ] 
if 'request' in kwargs : 
~~~ request = kwargs [ 'request' ] 
version = request . GET . get ( 'version' , None ) 
preview_id = request . GET . get ( 'preview_id' , None ) 
if ( version is not None ) and ( preview_id is not None ) : 
~~~ kwargs [ 'revision_id' ] = version 
kwargs [ 'preview_id' ] = preview_id 
del kwargs [ 'is_published' ] 
~~ del kwargs [ 'request' ] 
~~ return super ( PublishableManager , self ) . get ( * args , ** kwargs ) 
queryset = self . get_publishable_queryset ( ) 
queryset = queryset . select_related ( 'featured_image' , 'featured_video' , 'topic' , 'section' , 'subsection' ) . prefetch_related ( 
'tags' , 
'featured_image__image__authors' , 
'authors' 
queryset = queryset . order_by ( '-updated_at' ) 
q = self . request . query_params . get ( 'q' , None ) 
section = self . request . query_params . get ( 'section' , None ) 
tags = self . request . query_params . getlist ( 'tags' , None ) 
author = self . request . query_params . get ( 'author' , None ) 
if q is not None : 
~~~ queryset = queryset . filter ( headline__icontains = q ) 
~~ if section is not None : 
~~~ queryset = queryset . filter ( section_id = section ) 
~~ if tags is not None : 
~~~ for tag in tags : 
~~~ queryset = queryset . filter ( tags__id = tag ) 
~~ ~~ if author is not None : 
~~~ queryset = queryset . filter ( authors__person_id = author ) 
~~ return queryset 
q = self . request . query_params . get ( 'q' ) 
if q : 
~~~ queryset = queryset . filter ( title__icontains = q ) 
~~ def get_attribute ( self , instance ) : 
attr = super ( NullBooleanField , self ) . get_attribute ( instance ) 
return True if attr else False 
~~ def validate_widget ( widget ) : 
if not has_valid_id ( widget ) : 
~~ if not has_valid_name ( widget ) : 
~~ if not has_valid_template ( widget ) : 
~~ if not hasattr ( widget , 'zones' ) or not widget . zones : 
~~ ~~ def validate_zone ( zone ) : 
if not has_valid_id ( zone ) : 
~~ if not has_valid_name ( zone ) : 
~~ ~~ def is_valid_uuid ( id ) : 
if not isinstance ( id , basestring ) : 
~~~ val = UUID ( id , version = 4 ) 
~~ def get_permissions ( self ) : 
permissions = '' 
if self . groups . filter ( name = 'Admin' ) . exists ( ) or self . is_superuser : 
~~~ permissions = 'admin' 
~~ return permissions 
~~ def modify_permissions ( self , permissions ) : 
group = Group . objects . get ( name = 'Admin' ) 
if permissions == 'admin' : 
~~~ self . groups . add ( group ) 
~~~ self . groups . remove ( group ) 
~~ ~~ def AuthorValidator ( data ) : 
if not isinstance ( data , list ) : 
~~ for author in data : 
~~~ if 'person' not in author : 
~~ if 'type' in author and not isinstance ( author [ 'type' ] , basestring ) : 
~~ ~~ ~~ def save ( self , validated_data ) : 
( zone , created ) = ZoneModel . objects . get_or_create ( zone_id = self . id ) 
zone . widget_id = validated_data [ 'widget' ] 
zone . data = validated_data [ 'data' ] 
for key in list ( zone . data . keys ( ) ) : 
~~~ if isinstance ( zone . data [ key ] , dict ) and ( 'id' in zone . data [ key ] . keys ( ) ) and ( 'data' in zone . data [ key ] . keys ( ) ) : 
~~~ zone . data [ key ] [ 'data' ] = self . before_save ( zone . data [ key ] [ 'id' ] , zone . data [ key ] [ 'data' ] ) 
~~ ~~ zone . data = self . before_save ( zone . widget_id , zone . data ) 
return zone . save ( ) 
~~ def get_data ( self ) : 
~~~ result [ field . name ] = self . data . get ( field . name ) 
~~ def prepare_data ( self ) : 
~~~ data = self . data . get ( field . name ) 
result [ field . name ] = field . prepare_data ( data ) 
~~ def render ( self , data = None , add_context = None ) : 
template = loader . get_template ( self . template ) 
~~~ data = self . context ( self . prepare_data ( ) ) 
~~ if add_context is not None : 
~~~ for key , value in add_context . iteritems ( ) : 
~~~ if key in self . accepted_keywords : 
~~~ data [ key ] = value 
~~ ~~ ~~ return template . render ( data ) 
~~ def content_to_html ( content , article_id ) : 
def render_node ( html , node , index ) : 
if node [ 'type' ] == 'paragraph' : 
~~~ return html + '<p>%s</p>' % node [ 'data' ] 
~~~ if node [ 'type' ] == 'ad' : 
~~~ id = 'div-gpt-ad-1443288719995-' + str ( 10 + index ) + '-' + str ( article_id ) 
dfp_type = 'Intra_Article_' + str ( index + 1 ) 
size = 'banner' 
if node [ 'data' ] == 'mobile' : 
~~~ size = 'box' 
~~ newString = \ + id + \ + size + \ + dfp_type + \ 
return html + \ % newString 
~~~ if node [ 'type' ] == 'poll' : 
~~~ node [ 'type' ] = 'widget' 
node [ 'data' ] [ 'data' ] = node [ 'data' ] 
~~ return html + embeds . render ( node [ 'type' ] , node [ 'data' ] ) 
~~ except EmbedException : 
~~~ return html 
~~ ~~ ~~ html = '' 
index = 0 
for node in content : 
~~~ html = render_node ( html , node , index ) 
if ( node [ 'type' ] == 'ad' ) : 
~~ ~~ return mark_safe ( html ) 
~~ def content_to_json ( content ) : 
def render_node ( node ) : 
'type' : node [ 'type' ] , 
'data' : embeds . to_json ( node [ 'type' ] , node [ 'data' ] ) 
~~ ~~ return map ( render_node , content ) 
~~ def get_settings ( cls , show_hidden = False ) : 
settings = Integration . objects . get_settings ( cls . ID ) 
if not show_hidden : 
~~~ for field in cls . HIDDEN_FIELDS : 
~~~ settings . pop ( field , None ) 
~~ ~~ return settings 
~~ def callback ( cls , user , query ) : 
settings = cls . get_settings ( show_hidden = True ) 
fb = Facebook ( ) 
'client_id' : settings [ 'client_id' ] , 
'client_secret' : settings [ 'client_secret' ] , 
'code' : query [ 'code' ] , 
'redirect_uri' : cls . REDIRECT_URI 
~~~ fb . get_access_token ( payload ) 
pages = fb . list_pages ( 'me' ) 
~~ except FacebookAPIError , e : 
~~~ raise IntegrationCallbackError ( e . message ) 
'pages' : pages 
~~ def get_settings ( self , integration_id ) : 
~~~ integration = self . get ( integration_id = integration_id ) 
return json . loads ( integration . settings ) 
~~ except ( self . model . DoesNotExist , ValueError ) : 
~~ ~~ def update_settings ( self , integration_id , settings ) : 
( integration , created ) = self . get_or_create ( integration_id = integration_id ) 
~~~ current_settings = json . loads ( integration . settings ) 
~~~ current_settings = { } 
~~ current_settings . update ( settings ) 
integration . settings = json . dumps ( current_settings ) 
integration . save ( ) 
~~ def signup ( request , uuid = None ) : 
invite = get_object_or_404 ( Invite . objects . all ( ) , id = uuid ) 
if invite . expiration_date < timezone . now ( ) : 
~~~ invite . delete ( ) 
~~ if request . method == 'POST' : 
~~~ form = SignUpForm ( request . POST ) 
if form . is_valid ( ) : 
~~~ user = form . save ( commit = False ) 
user . email = invite . email 
user . person = invite . person 
user . save ( ) 
if invite . permissions == 'admin' : 
~~~ group = Group . objects . get ( name = 'Admin' ) 
user . groups . add ( group ) 
~~ invite . delete ( ) 
return redirect ( 'dispatch-admin' ) 
request , 
'registration/signup.html' , 
'form' : form , 
'email' : invite . email 
~~~ form = SignUpForm ( ) 
~~ def maptag ( tagname , contents ) : 
return u'' . join ( tag ( tagname , item ) for item in contents ) 
~~ def zone ( zone_id , ** kwargs ) : 
~~~ zone = ThemeManager . Zones . get ( zone_id ) 
~~ except ZoneNotFound : 
~~~ return zone . widget . render ( add_context = kwargs ) 
~~ except ( WidgetNotFound , AttributeError ) : 
~~ def save ( self , revision = True , * args , ** kwargs ) : 
if revision : 
~~~ self . head = True 
self . revision_id += 1 
previous_revision = self . get_previous_revision ( ) 
if not self . is_parent ( ) : 
~~~ type ( self ) . objects . filter ( parent = self . parent , head = True ) . update ( head = None ) 
self . pk = None 
self . id = None 
self . is_published = None 
~~ ~~ if not self . created_at : 
~~~ self . created_at = timezone . now ( ) 
self . updated_at = timezone . now ( ) 
~~ if revision : 
~~~ self . updated_at = timezone . now ( ) 
~~ super ( Publishable , self ) . save ( * args , ** kwargs ) 
if not self . parent : 
~~~ self . parent = self 
super ( Publishable , self ) . save ( update_fields = [ 'parent' ] ) 
~~~ type ( self ) . objects . filter ( parent = self . parent ) . update ( latest_version = self . revision_id ) 
self . latest_version = self . revision_id 
~~ def save_featured_image ( self , data ) : 
attachment = self . featured_image 
if data is None : 
~~~ if attachment : 
~~~ attachment . delete ( ) 
~~ self . featured_image = None 
~~ if data [ 'image_id' ] is None : 
~~ if not attachment : 
~~~ attachment = ImageAttachment ( ) 
~~ attachment . image_id = data . get ( 'image_id' , attachment . image_id ) 
attachment . caption = data . get ( 'caption' , None ) 
attachment . credit = data . get ( 'credit' , None ) 
instance_type = str ( type ( self ) ) . lower ( ) 
setattr ( attachment , instance_type , self ) 
attachment . save ( ) 
self . featured_image = attachment 
~~ def save_subsection ( self , subsection_id ) : 
Article . objects . filter ( parent_id = self . parent . id ) . update ( subsection_id = subsection_id ) 
~~ def get_extension ( self ) : 
ext = os . path . splitext ( self . img . name ) [ 1 ] 
if ext : 
~~~ return ext [ 1 : ] 
~~ return ext 
~~ def get_medium_url ( self ) : 
if self . is_gif ( ) : 
~~~ return self . get_absolute_url ( ) 
~~ return '%s%s-%s.jpg' % ( settings . MEDIA_URL , self . get_name ( ) , 'medium' ) 
is_new = self . pk is None 
if is_new : 
~~~ self . img . name = self . img . name . lower ( ) 
~~ super ( Image , self ) . save ( ** kwargs ) 
if is_new and self . img : 
~~~ data = self . img . read ( ) 
~~ image = Img . open ( StringIO . StringIO ( data ) ) 
self . width , self . height = image . size 
super ( Image , self ) . save ( ) 
name = self . get_name ( ) 
ext = self . get_extension ( ) 
for size in self . SIZES . keys ( ) : 
~~~ self . save_thumbnail ( image , self . SIZES [ size ] , name , size , ext ) 
~~ ~~ ~~ def save_thumbnail ( self , image , size , name , label , file_type ) : 
width , height = size 
( imw , imh ) = image . size 
if ( imw > width ) or ( imh > height ) : 
~~~ image . thumbnail ( size , Img . ANTIALIAS ) 
~~ name = "%s-%s.jpg" % ( name , label ) 
if file_type in self . JPG_FORMATS : 
~~~ file_type = 'JPEG' 
~~ image_io = StringIO . StringIO ( ) 
image . save ( image_io , format = file_type , quality = 75 ) 
thumb_file = InMemoryUploadedFile ( image_io , None , name , 'image/jpeg' , image_io . len , None ) 
default_storage . save ( name , thumb_file ) 
~~ def get_bandwith_limited_stream ( self , fileobj , transfer_coordinator , 
enabled = True ) : 
stream = BandwidthLimitedStream ( 
fileobj , self . _leaky_bucket , transfer_coordinator , 
self . _time_utils ) 
if not enabled : 
~~~ stream . disable_bandwidth_limiting ( ) 
~~ return stream 
~~ def read ( self , amount ) : 
if not self . _bandwidth_limiting_enabled : 
~~~ return self . _fileobj . read ( amount ) 
~~ self . _bytes_seen += amount 
if self . _bytes_seen < self . _bytes_threshold : 
~~ self . _consume_through_leaky_bucket ( ) 
return self . _fileobj . read ( amount ) 
~~ def consume ( self , amt , request_token ) : 
with self . _lock : 
~~~ time_now = self . _time_utils . time ( ) 
if self . _consumption_scheduler . is_scheduled ( request_token ) : 
~~~ return self . _release_requested_amt_for_scheduled_request ( 
amt , request_token , time_now ) 
~~ elif self . _projected_to_exceed_max_rate ( amt , time_now ) : 
~~~ self . _raise_request_exceeded_exception ( 
~~~ return self . _release_requested_amt ( amt , time_now ) 
~~ ~~ ~~ def schedule_consumption ( self , amt , token , time_to_consume ) : 
self . _total_wait += time_to_consume 
self . _tokens_to_scheduled_consumption [ token ] = { 
'wait_duration' : self . _total_wait , 
'time_to_consume' : time_to_consume , 
return self . _total_wait 
~~ def process_scheduled_consumption ( self , token ) : 
scheduled_retry = self . _tokens_to_scheduled_consumption . pop ( token ) 
self . _total_wait = max ( 
self . _total_wait - scheduled_retry [ 'time_to_consume' ] , 0 ) 
~~ def get_projected_rate ( self , amt , time_at_consumption ) : 
if self . _last_time is None : 
~~ return self . _calculate_exponential_moving_average_rate ( 
amt , time_at_consumption ) 
~~ def record_consumption_rate ( self , amt , time_at_consumption ) : 
~~~ self . _last_time = time_at_consumption 
self . _current_rate = 0.0 
~~ self . _current_rate = self . _calculate_exponential_moving_average_rate ( 
self . _last_time = time_at_consumption 
~~ def download_file ( self , bucket , key , filename , extra_args = None , 
expected_size = None ) : 
self . _start_if_needed ( ) 
if extra_args is None : 
~~~ extra_args = { } 
~~ self . _validate_all_known_args ( extra_args ) 
transfer_id = self . _transfer_monitor . notify_new_transfer ( ) 
download_file_request = DownloadFileRequest ( 
transfer_id = transfer_id , bucket = bucket , key = key , 
filename = filename , extra_args = extra_args , 
expected_size = expected_size , 
self . _download_request_queue . put ( download_file_request ) 
call_args = CallArgs ( 
bucket = bucket , key = key , filename = filename , extra_args = extra_args , 
expected_size = expected_size ) 
future = self . _get_transfer_future ( transfer_id , call_args ) 
return future 
~~ def poll_for_result ( self , transfer_id ) : 
self . _transfer_states [ transfer_id ] . wait_till_done ( ) 
exception = self . _transfer_states [ transfer_id ] . exception 
if exception : 
~~ def calculate_range_parameter ( part_size , part_index , num_parts , 
total_size = None ) : 
start_range = part_index * part_size 
if part_index == num_parts - 1 : 
~~~ end_range = '' 
if total_size is not None : 
~~~ end_range = str ( total_size - 1 ) 
~~~ end_range = start_range + part_size - 1 
~~ range_param = 'bytes=%s-%s' % ( start_range , end_range ) 
return range_param 
~~ def get_callbacks ( transfer_future , callback_type ) : 
callbacks = [ ] 
for subscriber in transfer_future . meta . call_args . subscribers : 
~~~ callback_name = 'on_' + callback_type 
if hasattr ( subscriber , callback_name ) : 
~~~ callbacks . append ( 
getattr ( subscriber , callback_name ) , 
future = transfer_future 
~~ ~~ return callbacks 
~~ def get_filtered_dict ( original_dict , whitelisted_keys ) : 
filtered_dict = { } 
for key , value in original_dict . items ( ) : 
~~~ if key in whitelisted_keys : 
~~~ filtered_dict [ key ] = value 
~~ ~~ return filtered_dict 
~~ def decrement ( self ) : 
~~~ if self . _count == 0 : 
~~ self . _count -= 1 
if self . _is_finalized and self . _count == 0 : 
~~~ self . _callback ( ) 
~~ ~~ ~~ def finalize ( self ) : 
~~~ self . _is_finalized = True 
if self . _count == 0 : 
~~ ~~ ~~ def is_special_file ( cls , filename ) : 
if not os . path . exists ( filename ) : 
~~ mode = os . stat ( filename ) . st_mode 
if stat . S_ISCHR ( mode ) : 
~~ if stat . S_ISBLK ( mode ) : 
~~ if stat . S_ISFIFO ( mode ) : 
~~ if stat . S_ISSOCK ( mode ) : 
~~ def from_filename ( cls , filename , start_byte , chunk_size , callbacks = None , 
enable_callbacks = True ) : 
f . seek ( start_byte ) 
file_size = os . fstat ( f . fileno ( ) ) . st_size 
return cls ( f , chunk_size , file_size , callbacks , enable_callbacks ) 
~~ def acquire ( self , tag , blocking = True ) : 
if not self . _semaphore . acquire ( blocking ) : 
~~ ~~ def release ( self , tag , acquire_token ) : 
self . _semaphore . release ( ) 
~~ def adjust_chunksize ( self , current_chunksize , file_size = None ) : 
chunksize = current_chunksize 
if file_size is not None : 
~~~ chunksize = self . _adjust_for_max_parts ( chunksize , file_size ) 
~~ return self . _adjust_for_chunksize_limits ( chunksize ) 
~~ def queue_file_io_task ( self , fileobj , data , offset ) : 
self . _transfer_coordinator . submit ( 
self . _io_executor , 
self . get_io_write_task ( fileobj , data , offset ) 
~~ def get_io_write_task ( self , fileobj , data , offset ) : 
return IOWriteTask ( 
self . _transfer_coordinator , 
main_kwargs = { 
'fileobj' : fileobj , 
'offset' : offset , 
~~ def _get_download_output_manager_cls ( self , transfer_future , osutil ) : 
download_manager_resolver_chain = [ 
DownloadSpecialFilenameOutputManager , 
DownloadFilenameOutputManager , 
DownloadSeekableOutputManager , 
DownloadNonSeekableOutputManager , 
fileobj = transfer_future . meta . call_args . fileobj 
for download_manager_cls in download_manager_resolver_chain : 
~~~ if download_manager_cls . is_compatible ( fileobj , osutil ) : 
~~~ return download_manager_cls 
~~ ~~ raise RuntimeError ( 
fileobj , type ( fileobj ) ) ) 
~~ def _submit ( self , client , config , osutil , request_executor , io_executor , 
transfer_future , bandwidth_limiter = None ) : 
if transfer_future . meta . size is None : 
~~~ response = client . head_object ( 
Bucket = transfer_future . meta . call_args . bucket , 
Key = transfer_future . meta . call_args . key , 
** transfer_future . meta . call_args . extra_args 
transfer_future . meta . provide_transfer_size ( 
response [ 'ContentLength' ] ) 
~~ download_output_manager = self . _get_download_output_manager_cls ( 
transfer_future , osutil ) ( osutil , self . _transfer_coordinator , 
io_executor ) 
if transfer_future . meta . size < config . multipart_threshold : 
~~~ self . _submit_download_request ( 
client , config , osutil , request_executor , io_executor , 
download_output_manager , transfer_future , bandwidth_limiter ) 
~~~ self . _submit_ranged_download_request ( 
~~ ~~ def _main ( self , client , bucket , key , fileobj , extra_args , callbacks , 
max_attempts , download_output_manager , io_chunksize , 
start_index = 0 , bandwidth_limiter = None ) : 
last_exception = None 
for i in range ( max_attempts ) : 
~~~ response = client . get_object ( 
Bucket = bucket , Key = key , ** extra_args ) 
streaming_body = StreamReaderProgress ( 
response [ 'Body' ] , callbacks ) 
if bandwidth_limiter : 
~~~ streaming_body = bandwidth_limiter . get_bandwith_limited_stream ( 
streaming_body , self . _transfer_coordinator ) 
~~ current_index = start_index 
chunks = DownloadChunkIterator ( streaming_body , io_chunksize ) 
~~~ if not self . _transfer_coordinator . done ( ) : 
~~~ self . _handle_io ( 
download_output_manager , fileobj , chunk , 
current_index 
current_index += len ( chunk ) 
~~ except S3_RETRYABLE_DOWNLOAD_ERRORS as e : 
max_attempts , exc_info = True ) 
last_exception = e 
invoke_progress_callbacks ( 
callbacks , start_index - current_index ) 
~~ ~~ raise RetriesExceededError ( last_exception ) 
~~ def _main ( self , fileobj , data , offset ) : 
fileobj . seek ( offset ) 
fileobj . write ( data ) 
~~ def request_writes ( self , offset , data ) : 
if offset < self . _next_offset : 
~~ writes = [ ] 
if offset in self . _pending_offsets : 
~~ heapq . heappush ( self . _writes , ( offset , data ) ) 
self . _pending_offsets . add ( offset ) 
while self . _writes and self . _writes [ 0 ] [ 0 ] == self . _next_offset : 
~~~ next_write = heapq . heappop ( self . _writes ) 
writes . append ( { 'offset' : next_write [ 0 ] , 'data' : next_write [ 1 ] } ) 
self . _pending_offsets . remove ( next_write [ 0 ] ) 
self . _next_offset += len ( next_write [ 1 ] ) 
~~ return writes 
~~ def seekable ( fileobj ) : 
if hasattr ( fileobj , 'seekable' ) : 
~~~ return fileobj . seekable ( ) 
~~ elif hasattr ( fileobj , 'seek' ) and hasattr ( fileobj , 'tell' ) : 
~~~ fileobj . seek ( 0 , 1 ) 
~~ except ( OSError , IOError ) : 
~~ def upload ( self , fileobj , bucket , key , extra_args = None , subscribers = None ) : 
~~ if subscribers is None : 
~~~ subscribers = [ ] 
~~ self . _validate_all_known_args ( extra_args , self . ALLOWED_UPLOAD_ARGS ) 
fileobj = fileobj , bucket = bucket , key = key , extra_args = extra_args , 
subscribers = subscribers 
extra_main_kwargs = { } 
if self . _bandwidth_limiter : 
~~~ extra_main_kwargs [ 'bandwidth_limiter' ] = self . _bandwidth_limiter 
~~ return self . _submit_transfer ( 
call_args , UploadSubmissionTask , extra_main_kwargs ) 
~~ def download ( self , bucket , key , fileobj , extra_args = None , 
subscribers = None ) : 
~~ self . _validate_all_known_args ( extra_args , self . ALLOWED_DOWNLOAD_ARGS ) 
bucket = bucket , key = key , fileobj = fileobj , extra_args = extra_args , 
extra_main_kwargs = { 'io_executor' : self . _io_executor } 
call_args , DownloadSubmissionTask , extra_main_kwargs ) 
~~ def copy ( self , copy_source , bucket , key , extra_args = None , 
subscribers = None , source_client = None ) : 
~~ if source_client is None : 
~~~ source_client = self . _client 
~~ self . _validate_all_known_args ( extra_args , self . ALLOWED_COPY_ARGS ) 
copy_source = copy_source , bucket = bucket , key = key , 
extra_args = extra_args , subscribers = subscribers , 
source_client = source_client 
return self . _submit_transfer ( call_args , CopySubmissionTask ) 
~~ def delete ( self , bucket , key , extra_args = None , subscribers = None ) : 
~~ self . _validate_all_known_args ( extra_args , self . ALLOWED_DELETE_ARGS ) 
bucket = bucket , key = key , extra_args = extra_args , 
return self . _submit_transfer ( call_args , DeleteSubmissionTask ) 
~~ def shutdown ( self , cancel = False , cancel_msg = '' ) : 
self . _shutdown ( cancel , cancel , cancel_msg ) 
~~ def cancel ( self , msg = '' , exc_type = CancelledError ) : 
for transfer_coordinator in self . tracked_transfer_coordinators : 
~~~ transfer_coordinator . cancel ( msg , exc_type ) 
~~ ~~ def wait ( self ) : 
~~~ transfer_coordinator = None 
~~~ transfer_coordinator . result ( ) 
~~ ~~ except KeyboardInterrupt : 
if transfer_coordinator : 
transfer_coordinator ) 
~~ ~~ def _read ( self , fileobj , amount , truncate = True ) : 
if len ( self . _initial_data ) == 0 : 
~~~ return fileobj . read ( amount ) 
~~ if amount <= len ( self . _initial_data ) : 
~~~ data = self . _initial_data [ : amount ] 
if truncate : 
~~~ self . _initial_data = self . _initial_data [ amount : ] 
~~ amount_to_read = amount - len ( self . _initial_data ) 
data = self . _initial_data + fileobj . read ( amount_to_read ) 
~~~ self . _initial_data = b'' 
~~ def _wrap_data ( self , data , callbacks , close_callbacks ) : 
fileobj = self . _wrap_fileobj ( six . BytesIO ( data ) ) 
return self . _osutil . open_file_chunk_reader_from_fileobj ( 
fileobj = fileobj , chunk_size = len ( data ) , full_file_size = len ( data ) , 
callbacks = callbacks , close_callbacks = close_callbacks ) 
~~ def _get_upload_input_manager_cls ( self , transfer_future ) : 
upload_manager_resolver_chain = [ 
UploadFilenameInputManager , 
UploadSeekableInputManager , 
UploadNonSeekableInputManager 
for upload_manager_cls in upload_manager_resolver_chain : 
~~~ if upload_manager_cls . is_compatible ( fileobj ) : 
~~~ return upload_manager_cls 
~~ def _submit ( self , client , config , osutil , request_executor , 
upload_input_manager = self . _get_upload_input_manager_cls ( 
transfer_future ) ( 
osutil , self . _transfer_coordinator , bandwidth_limiter ) 
~~~ upload_input_manager . provide_transfer_size ( transfer_future ) 
~~ if not upload_input_manager . requires_multipart_upload ( 
transfer_future , config ) : 
~~~ self . _submit_upload_request ( 
client , config , osutil , request_executor , transfer_future , 
upload_input_manager ) 
~~~ self . _submit_multipart_request ( 
~~ ~~ def _main ( self , client , fileobj , bucket , key , extra_args ) : 
with fileobj as body : 
~~~ client . put_object ( Bucket = bucket , Key = key , Body = body , ** extra_args ) 
~~ ~~ def _main ( self , client , fileobj , bucket , key , upload_id , part_number , 
extra_args ) : 
~~~ response = client . upload_part ( 
Bucket = bucket , Key = key , 
UploadId = upload_id , PartNumber = part_number , 
Body = body , ** extra_args ) 
~~ etag = response [ 'ETag' ] 
return { 'ETag' : etag , 'PartNumber' : part_number } 
~~ def set_exception ( self , exception ) : 
if not self . done ( ) : 
~~~ raise TransferNotDoneError ( 
'complete.' ) 
~~ self . _coordinator . set_exception ( exception , override = True ) 
~~ def set_result ( self , result ) : 
~~~ self . _exception = None 
self . _result = result 
self . _status = 'success' 
~~ ~~ def set_exception ( self , exception , override = False ) : 
~~~ if not self . done ( ) or override : 
~~~ self . _exception = exception 
self . _status = 'failed' 
~~ ~~ ~~ def result ( self ) : 
self . _done_event . wait ( MAXINT ) 
if self . _exception : 
~~~ raise self . _exception 
~~ return self . _result 
~~~ if not self . done ( ) : 
~~~ should_announce_done = False 
self . _exception = exc_type ( msg ) 
if self . _status == 'not-started' : 
~~~ should_announce_done = True 
~~ self . _status = 'cancelled' 
if should_announce_done : 
~~~ self . announce_done ( ) 
~~ ~~ ~~ ~~ def submit ( self , executor , task , tag = None ) : 
task , executor , self . transfer_id ) 
future = executor . submit ( task , tag = tag ) 
self . add_associated_future ( future ) 
future . add_done_callback ( 
FunctionContainer ( self . remove_associated_future , future ) ) 
~~ def add_done_callback ( self , function , * args , ** kwargs ) : 
with self . _done_callbacks_lock : 
~~~ self . _done_callbacks . append ( 
FunctionContainer ( function , * args , ** kwargs ) 
~~ ~~ def add_failure_cleanup ( self , function , * args , ** kwargs ) : 
with self . _failure_cleanups_lock : 
~~~ self . _failure_cleanups . append ( 
FunctionContainer ( function , * args , ** kwargs ) ) 
~~ ~~ def announce_done ( self ) : 
if self . status != 'success' : 
~~~ self . _run_failure_cleanups ( ) 
~~ self . _done_event . set ( ) 
self . _run_done_callbacks ( ) 
~~ def submit ( self , task , tag = None , block = True ) : 
semaphore = self . _semaphore 
if tag : 
~~~ semaphore = self . _tag_semaphores [ tag ] 
~~ acquire_token = semaphore . acquire ( task . transfer_id , block ) 
release_callback = FunctionContainer ( 
semaphore . release , task . transfer_id , acquire_token ) 
future = ExecutorFuture ( self . _executor . submit ( task ) ) 
future . add_done_callback ( release_callback ) 
~~ def add_done_callback ( self , fn ) : 
def done_callback ( future_passed_to_callback ) : 
~~~ return fn ( ) 
~~ self . _future . add_done_callback ( done_callback ) 
~~ def _submit ( self , client , request_executor , transfer_future , ** kwargs ) : 
call_args = transfer_future . meta . call_args 
request_executor , 
DeleteObjectTask ( 
transfer_coordinator = self . _transfer_coordinator , 
'client' : client , 
'bucket' : call_args . bucket , 
'key' : call_args . key , 
'extra_args' : call_args . extra_args , 
is_final = True 
transfer_future ) : 
~~~ call_args = transfer_future . meta . call_args 
head_object_request = self . _get_head_object_request_from_copy_source ( 
call_args . copy_source ) 
extra_args = call_args . extra_args 
for param , value in extra_args . items ( ) : 
~~~ if param in self . EXTRA_ARGS_TO_HEAD_ARGS_MAPPING : 
~~~ head_object_request [ 
self . EXTRA_ARGS_TO_HEAD_ARGS_MAPPING [ param ] ] = value 
~~ ~~ response = call_args . source_client . head_object ( 
** head_object_request ) 
~~ if transfer_future . meta . size < config . multipart_threshold : 
~~~ self . _submit_copy_request ( 
client , config , osutil , request_executor , transfer_future ) 
~~ ~~ def _main ( self , client , copy_source , bucket , key , extra_args , callbacks , 
size ) : 
client . copy_object ( 
CopySource = copy_source , Bucket = bucket , Key = key , ** extra_args ) 
for callback in callbacks : 
~~~ callback ( bytes_transferred = size ) 
~~ ~~ def _main ( self , client , copy_source , bucket , key , upload_id , part_number , 
extra_args , callbacks , size ) : 
response = client . upload_part_copy ( 
CopySource = copy_source , Bucket = bucket , Key = key , 
UploadId = upload_id , PartNumber = part_number , ** extra_args ) 
~~ etag = response [ 'CopyPartResult' ] [ 'ETag' ] 
~~ def from_filename ( cls , filename , start_byte , chunk_size , callback = None , 
enable_callback = True ) : 
return cls ( f , start_byte , chunk_size , file_size , callback , 
enable_callback ) 
~~ def upload_file ( self , filename , bucket , key , 
callback = None , extra_args = None ) : 
events = self . _client . meta . events 
events . register_first ( 'request-created.s3' , 
disable_upload_callbacks , 
unique_id = 's3upload-callback-disable' ) 
events . register_last ( 'request-created.s3' , 
enable_upload_callbacks , 
unique_id = 's3upload-callback-enable' ) 
if self . _osutil . get_file_size ( filename ) >= self . _config . multipart_threshold : 
~~~ self . _multipart_upload ( filename , bucket , key , callback , extra_args ) 
~~~ self . _put_object ( filename , bucket , key , callback , extra_args ) 
~~ ~~ def download_file ( self , bucket , key , filename , extra_args = None , 
callback = None ) : 
object_size = self . _object_size ( bucket , key , extra_args ) 
temp_filename = filename + os . extsep + random_file_extension ( ) 
~~~ self . _download_file ( bucket , key , temp_filename , object_size , 
extra_args , callback ) 
self . _osutil . remove_file ( temp_filename ) 
~~~ self . _osutil . rename_file ( temp_filename , filename ) 
~~ ~~ def _main ( self , transfer_future , ** kwargs ) : 
~~~ self . _transfer_coordinator . set_status_to_queued ( ) 
on_queued_callbacks = get_callbacks ( transfer_future , 'queued' ) 
for on_queued_callback in on_queued_callbacks : 
~~~ on_queued_callback ( ) 
~~ self . _transfer_coordinator . set_status_to_running ( ) 
self . _submit ( transfer_future = transfer_future , ** kwargs ) 
~~~ self . _log_and_set_exception ( e ) 
self . _wait_for_all_submitted_futures_to_complete ( ) 
self . _transfer_coordinator . announce_done ( ) 
~~ ~~ def _main ( self , client , bucket , key , extra_args ) : 
response = client . create_multipart_upload ( 
upload_id = response [ 'UploadId' ] 
self . _transfer_coordinator . add_failure_cleanup ( 
client . abort_multipart_upload , Bucket = bucket , Key = key , 
UploadId = upload_id 
return upload_id 
~~ def _main ( self , client , bucket , key , upload_id , parts , extra_args ) : 
client . complete_multipart_upload ( 
Bucket = bucket , Key = key , UploadId = upload_id , 
MultipartUpload = { 'Parts' : parts } , 
** extra_args ) 
~~ def parse ( file_path , content = None ) : 
~~~ if content is None : 
~~~ with open ( file_path ) as f : 
~~~ content = f . read ( ) 
~~ ~~ py_tree = _parser . parse ( 
content , path = file_path , error_recovery = False ) 
return ParsoPythonFile ( file_path , py_tree ) 
~~ except parso . parser . ParserSyntaxError as ex : 
ex . error_leaf . line , ex . error_leaf . get_code ( ) ) 
~~ ~~ def _iter_step_func_decorators ( self ) : 
func_defs = [ func for func in self . py_tree . iter_funcdefs ( ) ] + [ func for cls in self . py_tree . iter_classdefs ( ) for func in cls . iter_funcdefs ( ) ] 
for func in func_defs : 
~~~ for decorator in func . get_decorators ( ) : 
~~~ if decorator . children [ 1 ] . value == 'step' : 
~~~ yield func , decorator 
~~ ~~ ~~ ~~ def _step_decorator_args ( self , decorator ) : 
args = decorator . children [ 3 : - 2 ] 
step = None 
if len ( args ) == 1 : 
~~~ step = ast . literal_eval ( args [ 0 ] . get_code ( ) ) 
~~ except ( ValueError , SyntaxError ) : 
~~ if isinstance ( step , six . string_types + ( list , ) ) : 
~~~ return step 
self . file_path , decorator . start_pos [ 0 ] ) 
~~ ~~ def iter_steps ( self ) : 
for func , decorator in self . _iter_step_func_decorators ( ) : 
~~~ step = self . _step_decorator_args ( decorator ) 
if step : 
~~~ span = self . _span_from_pos ( decorator . start_pos , func . end_pos ) 
yield step , func . name . value , span 
~~ ~~ ~~ def _find_step_node ( self , step_text ) : 
arg_node = decorator . children [ 3 ] 
if step == step_text : 
~~~ return arg_node , func 
~~ elif isinstance ( step , list ) and step_text in step : 
~~~ idx = step . index ( step_text ) 
step_node = arg_node . children [ 1 ] . children [ idx * 2 ] 
return step_node , func 
~~ ~~ return None , None 
~~ def refactor_step ( self , old_text , new_text , move_param_from_idx ) : 
diffs = [ ] 
step , func = self . _find_step_node ( old_text ) 
if step is None : 
~~~ return diffs 
~~ step_diff = self . _refactor_step_text ( step , old_text , new_text ) 
diffs . append ( step_diff ) 
params_list_node = func . children [ 2 ] 
moved_params = self . _move_param_nodes ( 
params_list_node . children , move_param_from_idx ) 
if params_list_node . children is not moved_params : 
~~~ params_span = self . _span_from_pos ( 
params_list_node . children [ 0 ] . end_pos , 
params_list_node . children [ - 1 ] . start_pos ) 
params_list_node . children = moved_params 
param_code = '' . join ( p . get_code ( ) for p in moved_params [ 1 : - 1 ] ) 
diffs . append ( ( params_span , param_code ) ) 
~~ return diffs 
~~ ~~ py_tree = RedBaron ( content ) 
return RedbaronPythonFile ( file_path , py_tree ) 
~~~ msg = str ( ex ) 
marker_pos = msg . find ( marker ) 
if marker_pos > 0 : 
~~~ msg = msg [ : marker_pos + len ( marker ) ] 
for node in self . py_tree . find_all ( 'def' ) : 
~~~ for decorator in node . decorators : 
~~~ if decorator . name . value == 'step' : 
~~~ yield node , decorator 
args = decorator . call . value 
~~~ step = args [ 0 ] . value . to_python ( ) 
self . file_path ) 
~~~ yield step , func . name , self . _span_for_node ( func , True ) 
arg_node = decorator . call . value [ 0 ] . value 
~~~ step_node = arg_node [ step . index ( step_text ) ] 
moved_params = self . _move_params ( func . arguments , move_param_from_idx ) 
if func . arguments is not moved_params : 
~~~ params_span = self . _span_for_node ( func . arguments , False ) 
func . arguments = moved_params 
diffs . append ( ( params_span , func . arguments . dumps ( ) ) ) 
~~ def select_python_parser ( parser = None ) : 
if parser == 'redbaron' or os . environ . get ( 'GETGAUGE_USE_0_3_3_PARSER' ) : 
~~~ PythonFile . Class = RedbaronPythonFile 
~~~ PythonFile . Class = ParsoPythonFile 
~~ ~~ def list ( self , teamId , max = None , ** request_parameters ) : 
check_type ( teamId , basestring , may_be_none = False ) 
check_type ( max , int ) 
params = dict_from_items_with_values ( 
request_parameters , 
teamId = teamId , 
max = max , 
items = self . _session . get_items ( API_ENDPOINT , params = params ) 
~~~ yield self . _object_factory ( OBJECT_TYPE , item ) 
~~ ~~ def create ( self , teamId , personId = None , personEmail = None , 
isModerator = False , ** request_parameters ) : 
check_type ( personId , basestring ) 
check_type ( personEmail , basestring ) 
check_type ( isModerator , bool ) 
post_data = dict_from_items_with_values ( 
personId = personId , 
personEmail = personEmail , 
isModerator = isModerator , 
json_data = self . _session . post ( API_ENDPOINT , json = post_data ) 
return self . _object_factory ( OBJECT_TYPE , json_data ) 
~~ def update ( self , membershipId , isModerator = None , ** request_parameters ) : 
check_type ( membershipId , basestring , may_be_none = False ) 
put_data = dict_from_items_with_values ( 
json_data = self . _session . put ( API_ENDPOINT + '/' + membershipId , 
json = put_data ) 
~~ def delete ( self , membershipId ) : 
self . _session . delete ( API_ENDPOINT + '/' + membershipId ) 
~~ def get_catfact ( ) : 
response = requests . get ( CAT_FACTS_URL , verify = False ) 
json_data = response . json ( ) 
return json_data [ 'fact' ] 
~~ def POST ( self ) : 
json_data = web . data ( ) 
print ( json_data , "\\n" ) 
webhook_obj = Webhook ( json_data ) 
room = api . rooms . get ( webhook_obj . data . roomId ) 
message = api . messages . get ( webhook_obj . data . id ) 
person = api . people . get ( message . personId ) 
me = api . people . me ( ) 
if message . personId == me . id : 
~~~ return 'OK' 
~~~ if "/CAT" in message . text : 
cat_fact = get_catfact ( ) 
api . messages . create ( room . id , text = cat_fact ) 
~~ ~~ return 'OK' 
~~ def list ( self , roomId = None , personId = None , personEmail = None , max = None , 
** request_parameters ) : 
check_type ( roomId , basestring ) 
roomId = roomId , 
~~ ~~ def delete ( self , membershipId ) : 
check_type ( membershipId , basestring ) 
~~ def to_unicode ( string ) : 
assert isinstance ( string , basestring ) 
if sys . version_info [ 0 ] >= 3 : 
~~~ if isinstance ( string , bytes ) : 
~~~ return string . decode ( 'utf-8' ) 
~~~ if isinstance ( string , str ) : 
~~ ~~ ~~ def to_bytes ( string ) : 
~~~ return string . encode ( 'utf-8' ) 
~~~ if isinstance ( string , unicode ) : 
~~ ~~ ~~ def validate_base_url ( base_url ) : 
parsed_url = urllib . parse . urlparse ( base_url ) 
if parsed_url . scheme and parsed_url . netloc : 
~~~ return parsed_url . geturl ( ) 
raise ValueError ( error_message ) 
~~ ~~ def is_web_url ( string ) : 
parsed_url = urllib . parse . urlparse ( string ) 
parsed_url . scheme . lower ( ) == 'http' 
or parsed_url . scheme . lower ( ) == 'https' 
and parsed_url . netloc 
~~ def open_local_file ( file_path ) : 
assert isinstance ( file_path , basestring ) 
assert is_local_file ( file_path ) 
file_name = os . path . basename ( file_path ) 
file_object = open ( file_path , 'rb' ) 
content_type = mimetypes . guess_type ( file_name ) [ 0 ] or 'text/plain' 
return EncodableFile ( file_name = file_name , 
file_object = file_object , 
content_type = content_type ) 
~~ def check_type ( o , acceptable_types , may_be_none = True ) : 
if not isinstance ( acceptable_types , tuple ) : 
~~~ acceptable_types = ( acceptable_types , ) 
~~ if may_be_none and o is None : 
~~ elif isinstance ( o , acceptable_types ) : 
~~~ error_message = ( 
"{o_type}." . format ( 
o = o , 
o_type = repr ( type ( o ) . __name__ ) 
raise TypeError ( error_message ) 
~~ ~~ def dict_from_items_with_values ( * dictionaries , ** items ) : 
dict_list = list ( dictionaries ) 
dict_list . append ( items ) 
for d in dict_list : 
~~~ for key , value in d . items ( ) : 
~~~ if value is not None : 
~~~ result [ key ] = value 
~~ def check_response_code ( response , expected_response_code ) : 
if response . status_code == expected_response_code : 
~~ elif response . status_code == RATE_LIMIT_RESPONSE_CODE : 
~~~ raise RateLimitError ( response ) 
~~~ raise ApiError ( response ) 
~~ ~~ def json_dict ( json_data ) : 
if isinstance ( json_data , dict ) : 
~~~ return json_data 
~~ elif isinstance ( json_data , basestring ) : 
~~~ return json . loads ( json_data , object_hook = OrderedDict ) 
"\ 
~~ ~~ def strptime ( cls , date_string , format = WEBEX_TEAMS_DATETIME_FORMAT ) : 
return super ( WebexTeamsDateTime , cls ) . strptime ( 
date_string , format 
) . replace ( tzinfo = ZuluTimeZone ( ) ) 
~~ def list ( self , teamId = None , type = None , sortBy = None , max = None , 
check_type ( teamId , basestring ) 
check_type ( type , basestring ) 
check_type ( sortBy , basestring ) 
type = type , 
sortBy = sortBy , 
~~ ~~ def create ( self , title , teamId = None , ** request_parameters ) : 
check_type ( title , basestring ) 
title = title , 
~~ def update ( self , roomId , title = None , ** request_parameters ) : 
check_type ( roomId , basestring , may_be_none = False ) 
json_data = self . _session . put ( API_ENDPOINT + '/' + roomId , 
~~ def delete ( self , roomId ) : 
self . _session . delete ( API_ENDPOINT + '/' + roomId ) 
~~ def list ( self , orgId = None , ** request_parameters ) : 
check_type ( orgId , basestring ) 
orgId = orgId , 
~~ ~~ def created ( self ) : 
created = self . _json_data . get ( 'created' ) 
if created : 
~~~ return WebexTeamsDateTime . strptime ( created ) 
~~ ~~ def generator_container ( generator_function ) : 
@ functools . wraps ( generator_function ) 
def generator_container_wrapper ( * args , ** kwargs ) : 
return GeneratorContainer ( generator_function , * args , ** kwargs ) 
~~ return generator_container_wrapper 
~~ def webex_teams_webhook_events ( ) : 
if request . method == 'GET' : 
~~ elif request . method == 'POST' : 
json_data = request . json 
print ( "\\n" ) 
print ( json_data ) 
~~ return 'OK' 
~~ ~~ ~~ def _get_access_token ( ) : 
access_token = os . environ . get ( ACCESS_TOKEN_ENVIRONMENT_VARIABLE ) 
~~~ return access_token 
~~~ for access_token_variable in LEGACY_ACCESS_TOKEN_ENVIRONMENT_VARIABLES : 
~~~ access_token = os . environ . get ( access_token_variable ) 
~~~ env_var_deprecation_warning = PendingDeprecationWarning ( 
"variable." . format ( 
legacy = access_token , 
new = ACCESS_TOKEN_ENVIRONMENT_VARIABLE , 
warnings . warn ( env_var_deprecation_warning ) 
return access_token 
~~ ~~ ~~ ~~ def create ( self , name , targetUrl , resource , event , 
filter = None , secret = None , ** request_parameters ) : 
check_type ( name , basestring , may_be_none = False ) 
check_type ( targetUrl , basestring , may_be_none = False ) 
check_type ( resource , basestring , may_be_none = False ) 
check_type ( event , basestring , may_be_none = False ) 
check_type ( filter , basestring ) 
check_type ( secret , basestring ) 
targetUrl = targetUrl , 
resource = resource , 
event = event , 
filter = filter , 
secret = secret , 
~~ def update ( self , webhookId , name = None , targetUrl = None , 
check_type ( webhookId , basestring , may_be_none = False ) 
check_type ( name , basestring ) 
check_type ( targetUrl , basestring ) 
json_data = self . _session . put ( API_ENDPOINT + '/' + webhookId , 
~~ def delete ( self , webhookId ) : 
self . _session . delete ( API_ENDPOINT + '/' + webhookId ) 
~~ def _fix_next_url ( next_url ) : 
next_url = str ( next_url ) 
parsed_url = urllib . parse . urlparse ( next_url ) 
if not parsed_url . scheme or not parsed_url . netloc or not parsed_url . path : 
~~ if parsed_url . query : 
~~~ query_list = parsed_url . query . split ( '&' ) 
if 'max=null' in query_list : 
~~~ query_list . remove ( 'max=null' ) 
~~ new_query = '&' . join ( query_list ) 
parsed_url = list ( parsed_url ) 
parsed_url [ 4 ] = new_query 
~~ return urllib . parse . urlunparse ( parsed_url ) 
~~ def single_request_timeout ( self , value ) : 
check_type ( value , int ) 
assert value is None or value > 0 
self . _single_request_timeout = value 
~~ def wait_on_rate_limit ( self , value ) : 
check_type ( value , bool , may_be_none = False ) 
self . _wait_on_rate_limit = value 
~~ def update_headers ( self , headers ) : 
check_type ( headers , dict , may_be_none = False ) 
self . _req_session . headers . update ( headers ) 
~~ def abs_url ( self , url ) : 
parsed_url = urllib . parse . urlparse ( url ) 
if not parsed_url . scheme and not parsed_url . netloc : 
~~~ return urllib . parse . urljoin ( str ( self . base_url ) , str ( url ) ) 
~~ ~~ def request ( self , method , url , erc , ** kwargs ) : 
abs_url = self . abs_url ( url ) 
kwargs . setdefault ( 'timeout' , self . single_request_timeout ) 
~~~ response = self . _req_session . request ( method , abs_url , ** kwargs ) 
~~~ check_response_code ( response , erc ) 
~~ except RateLimitError as e : 
~~~ if self . wait_on_rate_limit : 
~~~ warnings . warn ( RateLimitWarning ( response ) ) 
time . sleep ( e . retry_after ) 
~~ ~~ ~~ def get ( self , url , params = None , ** kwargs ) : 
check_type ( url , basestring , may_be_none = False ) 
check_type ( params , dict ) 
erc = kwargs . pop ( 'erc' , EXPECTED_RESPONSE_CODE [ 'GET' ] ) 
response = self . request ( 'GET' , url , erc , params = params , ** kwargs ) 
return extract_and_parse_json ( response ) 
~~ def get_pages ( self , url , params = None , ** kwargs ) : 
~~~ yield extract_and_parse_json ( response ) 
if response . links . get ( 'next' ) : 
~~~ next_url = response . links . get ( 'next' ) . get ( 'url' ) 
next_url = _fix_next_url ( next_url ) 
response = self . request ( 'GET' , next_url , erc , ** kwargs ) 
~~ ~~ ~~ def get_items ( self , url , params = None , ** kwargs ) : 
pages = self . get_pages ( url , params = params , ** kwargs ) 
for json_page in pages : 
~~~ assert isinstance ( json_page , dict ) 
items = json_page . get ( 'items' ) 
if items is None : 
~~~ error_message = "\ "{!r}" . format ( json_page ) 
raise MalformedResponse ( error_message ) 
~~ ~~ ~~ ~~ def put ( self , url , json = None , data = None , ** kwargs ) : 
erc = kwargs . pop ( 'erc' , EXPECTED_RESPONSE_CODE [ 'PUT' ] ) 
response = self . request ( 'PUT' , url , erc , json = json , data = data , 
~~ def delete ( self , url , ** kwargs ) : 
erc = kwargs . pop ( 'erc' , EXPECTED_RESPONSE_CODE [ 'DELETE' ] ) 
self . request ( 'DELETE' , url , erc , ** kwargs ) 
~~ def create ( self , subject , displayName , issuerToken , expiration , secret ) : 
check_type ( subject , basestring ) 
check_type ( displayName , basestring ) 
check_type ( issuerToken , basestring ) 
check_type ( expiration , basestring ) 
"sub" : subject , 
"name" : displayName , 
"iss" : issuerToken , 
"exp" : expiration 
key = base64 . b64decode ( secret ) 
jwt_token = jwt . encode ( payload , key , algorithm = 'HS256' ) 
url = self . _session . base_url + API_ENDPOINT + "/" + "login" 
response = requests . post ( url , headers = headers ) 
check_response_code ( response , EXPECTED_RESPONSE_CODE [ 'GET' ] ) 
return self . _object_factory ( OBJECT_TYPE , response . json ( ) ) 
~~ def list ( self , roomId , mentionedPeople = None , before = None , 
beforeMessage = None , max = None , ** request_parameters ) : 
check_type ( mentionedPeople , basestring ) 
check_type ( before , basestring ) 
check_type ( beforeMessage , basestring ) 
mentionedPeople = mentionedPeople , 
before = before , 
beforeMessage = beforeMessage , 
~~ ~~ def create ( self , roomId = None , toPersonId = None , toPersonEmail = None , 
text = None , markdown = None , files = None , ** request_parameters ) : 
check_type ( toPersonId , basestring ) 
check_type ( toPersonEmail , basestring ) 
check_type ( text , basestring ) 
check_type ( markdown , basestring ) 
check_type ( files , list ) 
if files : 
~~~ if len ( files ) != 1 : 
"message." ) 
~~ check_type ( files [ 0 ] , basestring ) 
~~ post_data = dict_from_items_with_values ( 
toPersonId = toPersonId , 
toPersonEmail = toPersonEmail , 
text = text , 
markdown = markdown , 
files = files , 
if not files or is_web_url ( files [ 0 ] ) : 
~~~ json_data = self . _session . post ( API_ENDPOINT , json = post_data ) 
~~ elif is_local_file ( files [ 0 ] ) : 
~~~ post_data [ 'files' ] = open_local_file ( files [ 0 ] ) 
multipart_data = MultipartEncoder ( post_data ) 
headers = { 'Content-type' : multipart_data . content_type } 
json_data = self . _session . post ( API_ENDPOINT , 
data = multipart_data ) 
~~~ post_data [ 'files' ] . file_object . close ( ) 
~~ return self . _object_factory ( OBJECT_TYPE , json_data ) 
~~ def delete ( self , messageId ) : 
check_type ( messageId , basestring , may_be_none = False ) 
self . _session . delete ( API_ENDPOINT + '/' + messageId ) 
~~ def list ( self , email = None , displayName = None , id = None , orgId = None , max = None , 
check_type ( id , basestring ) 
check_type ( email , basestring ) 
id = id , 
email = email , 
displayName = displayName , 
~~ ~~ def create ( self , emails , displayName = None , firstName = None , lastName = None , 
avatar = None , orgId = None , roles = None , licenses = None , 
check_type ( emails , list , may_be_none = False ) 
check_type ( firstName , basestring ) 
check_type ( lastName , basestring ) 
check_type ( avatar , basestring ) 
check_type ( roles , list ) 
check_type ( licenses , list ) 
emails = emails , 
firstName = firstName , 
lastName = lastName , 
avatar = avatar , 
roles = roles , 
licenses = licenses , 
~~ def get ( self , personId ) : 
check_type ( personId , basestring , may_be_none = False ) 
json_data = self . _session . get ( API_ENDPOINT + '/' + personId ) 
~~ def update ( self , personId , emails = None , displayName = None , firstName = None , 
lastName = None , avatar = None , orgId = None , roles = None , 
licenses = None , ** request_parameters ) : 
check_type ( emails , list ) 
json_data = self . _session . put ( API_ENDPOINT + '/' + personId , 
~~ def delete ( self , personId ) : 
self . _session . delete ( API_ENDPOINT + '/' + personId ) 
~~ def me ( self ) : 
json_data = self . _session . get ( API_ENDPOINT + '/me' ) 
~~ def list ( self , ** request_parameters ) : 
items = self . _session . get_items ( 
API_ENDPOINT , 
params = request_parameters 
~~ ~~ def list ( self , max = None , ** request_parameters ) : 
~~ ~~ def create ( self , name , ** request_parameters ) : 
~~ def update ( self , teamId , name = None , ** request_parameters ) : 
json_data = self . _session . put ( API_ENDPOINT + '/' + teamId , 
~~ def delete ( self , teamId ) : 
self . _session . delete ( API_ENDPOINT + '/' + teamId ) 
~~ def list ( self , resource = None , type = None , actorId = None , _from = None , to = None , 
max = None , ** request_parameters ) : 
check_type ( resource , basestring ) 
check_type ( actorId , basestring ) 
check_type ( _from , basestring ) 
check_type ( to , basestring ) 
actorId = actorId , 
_from = _from , 
to = to , 
if _from : 
~~~ params [ "from" ] = params . pop ( "_from" ) 
~~ items = self . _session . get_items ( API_ENDPOINT , params = params ) 
~~ ~~ def _serialize ( cls , data ) : 
if hasattr ( data , "__hash__" ) and callable ( data . __hash__ ) : 
~~ elif isinstance ( data , list ) : 
~~~ return tuple ( ( cls . _serialize ( item ) for item in data ) ) 
~~ elif isinstance ( data , dict ) : 
~~~ key_value_tuples = [ 
( key , cls . _serialize ( value ) ) 
for key , value in data . items ( ) 
key_value_tuples . sort ( ) 
return tuple ( key_value_tuples ) 
~~ ~~ def get ( self , client_id , client_secret , code , redirect_uri ) : 
check_type ( client_id , basestring , may_be_none = False ) 
check_type ( client_secret , basestring , may_be_none = False ) 
check_type ( code , basestring , may_be_none = False ) 
check_type ( redirect_uri , basestring , may_be_none = False ) 
grant_type = "authorization_code" , 
client_secret = client_secret , 
code = code , 
redirect_uri = redirect_uri , 
response = requests . post ( self . _endpoint_url , data = post_data , 
** self . _request_kwargs ) 
check_response_code ( response , EXPECTED_RESPONSE_CODE [ 'POST' ] ) 
json_data = extract_and_parse_json ( response ) 
~~ def lastActivity ( self ) : 
last_activity = self . _json_data . get ( 'lastActivity' ) 
if last_activity : 
~~~ return WebexTeamsDateTime . strptime ( last_activity ) 
~~ ~~ def post_events_service ( request ) : 
log . info ( "\\n" ) 
log . info ( json_data ) 
~~~ return { 'Message' : 'OK' } 
catfact = get_catfact ( ) 
api . messages . create ( room . id , text = catfact ) 
~~ return { 'Message' : 'OK' } 
~~ ~~ def get_ngrok_public_url ( ) : 
~~~ response = requests . get ( url = NGROK_CLIENT_API_BASE_URL + "/tunnels" , 
headers = { 'content-type' : 'application/json' } ) 
~~ except requests . exceptions . RequestException : 
~~~ for tunnel in response . json ( ) [ "tunnels" ] : 
~~~ if tunnel . get ( "public_url" , "" ) . startswith ( "http://" ) : 
return tunnel [ "public_url" ] 
~~ ~~ ~~ ~~ def delete_webhooks_with_name ( api , name ) : 
for webhook in api . webhooks . list ( ) : 
~~~ if webhook . name == name : 
api . webhooks . delete ( webhook . id ) 
~~ ~~ ~~ def create_ngrok_webhook ( api , ngrok_public_url ) : 
webhook = api . webhooks . create ( 
name = WEBHOOK_NAME , 
targetUrl = urljoin ( ngrok_public_url , WEBHOOK_URL_SUFFIX ) , 
resource = WEBHOOK_RESOURCE , 
event = WEBHOOK_EVENT , 
print ( webhook ) 
return webhook 
api = WebexTeamsAPI ( ) 
delete_webhooks_with_name ( api , name = WEBHOOK_NAME ) 
public_url = get_ngrok_public_url ( ) 
if public_url is not None : 
~~~ create_ngrok_webhook ( api , public_url ) 
data = [ 0 ] * 6 
for i in range ( 6 ) : 
~~~ data [ i ] = random . uniform ( - 2048 , 2048 ) 
~~ accel = data [ : 3 ] 
mag = data [ 3 : ] 
return ( accel , mag ) 
~~ def get_version ( file , name = '__version__' ) : 
path = os . path . realpath ( file ) 
version_ns = { } 
with io . open ( path , encoding = "utf8" ) as f : 
~~~ exec ( f . read ( ) , { } , version_ns ) 
~~ return version_ns [ name ] 
~~ def ensure_python ( specs ) : 
if not isinstance ( specs , ( list , tuple ) ) : 
~~~ specs = [ specs ] 
~~ v = sys . version_info 
part = '%s.%s' % ( v . major , v . minor ) 
for spec in specs : 
~~~ if part == spec : 
~~~ if eval ( part + spec ) : 
~~ ~~ except SyntaxError : 
~~ def find_packages ( top = HERE ) : 
packages = [ ] 
for d , dirs , _ in os . walk ( top , followlinks = True ) : 
~~~ if os . path . exists ( pjoin ( d , '__init__.py' ) ) : 
~~~ packages . append ( os . path . relpath ( d , top ) . replace ( os . path . sep , '.' ) ) 
~~ elif d != top : 
~~~ dirs [ : ] = [ ] 
~~ ~~ return packages 
~~ def create_cmdclass ( prerelease_cmd = None , package_data_spec = None , 
data_files_spec = None ) : 
wrapped = [ prerelease_cmd ] if prerelease_cmd else [ ] 
if package_data_spec or data_files_spec : 
~~~ wrapped . append ( 'handle_files' ) 
~~ wrapper = functools . partial ( _wrap_command , wrapped ) 
handle_files = _get_file_handler ( package_data_spec , data_files_spec ) 
if 'bdist_egg' in sys . argv : 
~~~ egg = wrapper ( bdist_egg , strict = True ) 
~~~ egg = bdist_egg_disabled 
~~ cmdclass = dict ( 
build_py = wrapper ( build_py , strict = is_repo ) , 
bdist_egg = egg , 
sdist = wrapper ( sdist , strict = True ) , 
handle_files = handle_files , 
if bdist_wheel : 
~~~ cmdclass [ 'bdist_wheel' ] = wrapper ( bdist_wheel , strict = True ) 
~~ cmdclass [ 'develop' ] = wrapper ( develop , strict = True ) 
return cmdclass 
~~ def command_for_func ( func ) : 
class FuncCommand ( BaseCommand ) : 
~~~ def run ( self ) : 
update_package_data ( self . distribution ) 
~~ ~~ return FuncCommand 
~~ def run ( cmd , ** kwargs ) : 
kwargs . setdefault ( 'cwd' , HERE ) 
kwargs . setdefault ( 'shell' , os . name == 'nt' ) 
if not isinstance ( cmd , ( list , tuple ) ) and os . name != 'nt' : 
~~~ cmd = shlex . split ( cmd ) 
~~ cmd [ 0 ] = which ( cmd [ 0 ] ) 
return subprocess . check_call ( cmd , ** kwargs ) 
~~ def recursive_mtime ( path , newest = True ) : 
if os . path . isfile ( path ) : 
~~~ return mtime ( path ) 
~~ current_extreme = None 
for dirname , dirnames , filenames in os . walk ( path , topdown = False ) : 
~~~ mt = mtime ( pjoin ( dirname , filename ) ) 
~~~ if mt >= ( current_extreme or mt ) : 
~~~ current_extreme = mt 
~~ ~~ elif mt <= ( current_extreme or mt ) : 
~~ ~~ ~~ return current_extreme 
~~ def ensure_targets ( targets ) : 
class TargetsCheck ( BaseCommand ) : 
~~~ if skip_npm : 
~~ missing = [ t for t in targets if not os . path . exists ( t ) ] 
if missing : 
~~ ~~ ~~ return TargetsCheck 
~~ def _wrap_command ( cmds , cls , strict = True ) : 
class WrappedCommand ( cls ) : 
~~~ if not getattr ( self , 'uninstall' , None ) : 
~~~ [ self . run_command ( cmd ) for cmd in cmds ] 
~~ ~~ ~~ update_package_data ( self . distribution ) 
result = cls . run ( self ) 
~~ ~~ return WrappedCommand 
~~ def _get_file_handler ( package_data_spec , data_files_spec ) : 
class FileHandler ( BaseCommand ) : 
~~~ package_data = self . distribution . package_data 
package_spec = package_data_spec or dict ( ) 
for ( key , patterns ) in package_spec . items ( ) : 
~~~ package_data [ key ] = _get_package_data ( key , patterns ) 
~~ self . distribution . data_files = _get_data_files ( 
data_files_spec , self . distribution . data_files 
~~ ~~ return FileHandler 
~~ def _get_data_files ( data_specs , existing ) : 
file_data = defaultdict ( list ) 
for ( path , files ) in existing or [ ] : 
~~~ file_data [ path ] = files 
~~ for ( path , dname , pattern ) in data_specs or [ ] : 
~~~ dname = dname . replace ( os . sep , '/' ) 
offset = len ( dname ) + 1 
files = _get_files ( pjoin ( dname , pattern ) ) 
for fname in files : 
~~~ root = os . path . dirname ( fname ) 
full_path = '/' . join ( [ path , root [ offset : ] ] ) 
if full_path . endswith ( '/' ) : 
~~~ full_path = full_path [ : - 1 ] 
~~ file_data [ full_path ] . append ( fname ) 
~~ ~~ data_files = [ ] 
for ( path , files ) in file_data . items ( ) : 
~~~ data_files . append ( ( path , files ) ) 
~~ return data_files 
~~ def _get_package_data ( root , file_patterns = None ) : 
if file_patterns is None : 
~~~ file_patterns = [ '*' ] 
~~ return _get_files ( file_patterns , pjoin ( HERE , root ) ) 
~~ def _compile_pattern ( pat , ignore_case = True ) : 
if isinstance ( pat , bytes ) : 
~~~ pat_str = pat . decode ( 'ISO-8859-1' ) 
res_str = _translate_glob ( pat_str ) 
res = res_str . encode ( 'ISO-8859-1' ) 
~~~ res = _translate_glob ( pat ) 
~~ flags = re . IGNORECASE if ignore_case else 0 
return re . compile ( res , flags = flags ) . match 
~~ def _iexplode_path ( path ) : 
( head , tail ) = os . path . split ( path ) 
if not head or ( not tail and head == path ) : 
~~~ if head : 
~~~ yield head 
~~ if tail or not head : 
~~~ yield tail 
~~ for p in _iexplode_path ( head ) : 
~~~ yield p 
~~ yield tail 
~~ def _translate_glob ( pat ) : 
translated_parts = [ ] 
for part in _iexplode_path ( pat ) : 
~~~ translated_parts . append ( _translate_glob_part ( part ) ) 
~~ os_sep_class = '[%s]' % re . escape ( SEPARATORS ) 
res = _join_translated ( translated_parts , os_sep_class ) 
return '{res}\\\\Z(?ms)' . format ( res = res ) 
~~ def _join_translated ( translated_parts , os_sep_class ) : 
res = '' 
for part in translated_parts [ : - 1 ] : 
~~~ if part == '.*' : 
~~~ res += part 
~~~ res += part + os_sep_class 
~~ ~~ if translated_parts [ - 1 ] == '.*' : 
~~~ res += '.+' 
res += '({os_sep_class}?.*)?' . format ( os_sep_class = os_sep_class ) 
~~~ res += translated_parts [ - 1 ] 
~~ def _translate_glob_part ( pat ) : 
if pat == '**' : 
~~~ return '.*' 
~~ i , n = 0 , len ( pat ) 
while i < n : 
~~~ c = pat [ i ] 
i = i + 1 
if c == '*' : 
~~~ res . append ( '[^%s]*' % SEPARATORS ) 
~~ elif c == '?' : 
~~~ res . append ( '[^%s]?' % SEPARATORS ) 
~~ elif c == '[' : 
~~~ j = i 
if j < n and pat [ j ] == '!' : 
~~~ j = j + 1 
~~ if j < n and pat [ j ] == ']' : 
~~ while j < n and pat [ j ] != ']' : 
~~ if j >= n : 
~~~ res . append ( '\\\\[' ) 
~~~ stuff = pat [ i : j ] . replace ( '\\\\' , '\\\\\\\\' ) 
i = j + 1 
if stuff [ 0 ] == '!' : 
~~~ stuff = '^' + stuff [ 1 : ] 
~~ elif stuff [ 0 ] == '^' : 
~~~ stuff = '\\\\' + stuff 
~~ res . append ( '[%s]' % stuff ) 
~~~ res . append ( re . escape ( c ) ) 
~~ ~~ return '' . join ( res ) 
~~ def parse_fntdata ( _data , _config , _extra_data_receiver = None ) : 
frame_data_list = [ ] 
raw_frames_data = { } 
for index in xrange ( 0 , parse_char_count [ "count" ] ) : 
frame_data = { } 
frame_data [ "name" ] = "{prefix}_{id}.png" . format ( prefix = _config [ "prefix" ] , id = parse_frame [ "id" ] , letter = parse_frame [ "letter" ] ) 
frame_data [ "source_size" ] = ( parse_frame [ "width" ] , parse_frame [ "height" ] ) 
frame_data [ "rotated" ] = False 
frame_data [ "src_rect" ] = ( parse_frame [ "x" ] , parse_frame [ "y" ] , parse_frame [ "x" ] + parse_frame [ "width" ] , parse_frame [ "y" ] + parse_frame [ "height" ] ) 
frame_data [ "offset" ] = ( 0 , 0 ) 
if parse_frame [ "width" ] <= 0 or parse_frame [ "height" ] <= 0 : 
~~ frame_data_list . append ( frame_data ) 
parse_frame_named_data = parse_frame . named . copy ( ) 
parse_frame_named_data [ "texture" ] = frame_data [ "name" ] 
raw_frames_data [ parse_frame [ "id" ] ] = parse_frame_named_data 
~~ data [ "texture" ] = parse_page_info [ "file" ] 
data [ "frames" ] = frame_data_list 
if _extra_data_receiver != None : 
~~~ _extra_data_receiver [ "common" ] = parse_common_info . named 
_extra_data_receiver [ "frames" ] = raw_frames_data 
~~ def _pvr_head ( _data ) : 
"sig" : _data [ : 4 ] , 
"compression_type" : struct . unpack ( "H" , _data [ 4 : 6 ] ) [ 0 ] , 
"version" : struct . unpack ( "H" , _data [ 6 : 8 ] ) [ 0 ] , 
"reserved" : struct . unpack ( "I" , _data [ 8 : 12 ] ) [ 0 ] , 
"len" : struct . unpack ( "I" , _data [ 12 : 16 ] ) [ 0 ] , 
~~ def disconnect ( self ) : 
self . reconnect_required . clear ( ) 
self . disconnect_called . set ( ) 
if self . socket : 
~~~ self . socket . close ( ) 
~~ self . join ( timeout = 1 ) 
~~ def reconnect ( self ) : 
self . connected . clear ( ) 
self . reconnect_required . set ( ) 
~~ ~~ def _connect ( self ) : 
self . socket = websocket . WebSocketApp ( 
self . url , 
on_open = self . _on_open , 
on_message = self . _on_message , 
on_error = self . _on_error , 
on_close = self . _on_close 
if 'ca_certs' not in self . sslopt . keys ( ) : 
~~~ ssl_defaults = ssl . get_default_verify_paths ( ) 
self . sslopt [ 'ca_certs' ] = ssl_defaults . cafile 
self . socket . run_forever ( sslopt = self . sslopt , 
http_proxy_host = self . http_proxy_host , 
http_proxy_port = self . http_proxy_port , 
http_proxy_auth = self . http_proxy_auth , 
http_no_proxy = self . http_no_proxy ) 
self . _stop_timers ( ) 
while self . reconnect_required . is_set ( ) : 
~~~ if not self . disconnect_called . is_set ( ) : 
% self . reconnect_interval ) 
self . state = "unavailable" 
time . sleep ( self . reconnect_interval ) 
self . socket . keep_running = True 
self . socket . sock = None 
~~ ~~ ~~ def _on_message ( self , ws , message ) : 
raw , received_at = message , time . time ( ) 
raw , received_at ) 
~~~ data = json . loads ( raw ) 
~~ except json . JSONDecodeError : 
~~~ self . _system_handler ( data , received_at ) 
~~~ if data [ 1 ] == 'hb' : 
~~~ self . _heartbeat_handler ( ) 
~~~ self . _data_handler ( data , received_at ) 
~~ ~~ self . _start_timers ( ) 
~~ def _stop_timers ( self ) : 
if self . ping_timer : 
~~~ self . ping_timer . cancel ( ) 
~~ if self . connection_timer : 
~~~ self . connection_timer . cancel ( ) 
~~ if self . pong_timer : 
~~~ self . pong_timer . cancel ( ) 
~~ def send_ping ( self ) : 
self . socket . send ( json . dumps ( { 'event' : 'ping' } ) ) 
self . pong_timer = Timer ( self . pong_timeout , self . _check_pong ) 
self . pong_timer . start ( ) 
~~ def _check_pong ( self ) : 
self . pong_timer . cancel ( ) 
if self . pong_received : 
self . pong_received = False 
self . reconnect ( ) 
~~ ~~ def send ( self , api_key = None , secret = None , list_data = None , auth = False , ** kwargs ) : 
~~~ nonce = str ( int ( time . time ( ) * 10000000 ) ) 
auth_string = 'AUTH' + nonce 
auth_sig = hmac . new ( secret . encode ( ) , auth_string . encode ( ) , 
hashlib . sha384 ) . hexdigest ( ) 
payload = { 'event' : 'auth' , 'apiKey' : api_key , 'authSig' : auth_sig , 
'authPayload' : auth_string , 'authNonce' : nonce } 
payload = json . dumps ( payload ) 
~~ elif list_data : 
~~~ payload = json . dumps ( list_data ) 
~~~ payload = json . dumps ( kwargs ) 
~~~ self . socket . send ( payload ) 
~~ except websocket . WebSocketConnectionClosedException : 
~~ ~~ def pass_to_client ( self , event , data , * args ) : 
self . q . put ( ( event , data , * args ) ) 
~~ def _unpause ( self ) : 
self . paused . clear ( ) 
self . _resubscribe ( soft = True ) 
~~ def _system_handler ( self , data , ts ) : 
event = data . pop ( 'event' ) 
if event == 'pong' : 
data ) 
self . _pong_handler ( ) 
~~ elif event == 'info' : 
self . _info_handler ( data ) 
~~ elif event == 'error' : 
self . _error_handler ( data ) 
~~ elif event in ( 'subscribed' , 'unsubscribed' , 'conf' , 'auth' , 'unauth' ) : 
"_response_handler.." , data ) 
self . _response_handler ( event , data , ts ) 
~~ ~~ def _response_handler ( self , event , data , ts ) : 
self . pass_to_client ( event , data , ts ) 
~~ def _info_handler ( self , data ) : 
def raise_exception ( ) : 
~~ if 'code' not in data and 'version' in data : 
codes = { 20051 : self . reconnect , 20060 : self . _pause , 
20061 : self . _unpause } 
if 'version' in data : 
~~~ self . log . info ( info_message [ data [ 'code' ] ] ) 
codes [ data [ 'code' ] ] ( ) 
~~~ self . log . exception ( e ) 
~~ ~~ def _error_handler ( self , data ) : 
~~~ self . log . error ( errors [ data [ 'code' ] ] ) 
"Reconnecting.." , data ) 
~~ ~~ def _data_handler ( self , data , ts ) : 
self . pass_to_client ( 'data' , data , ts ) 
~~ def _resubscribe ( self , soft = False ) : 
if self . bitfinex_config : 
~~~ self . send ( ** self . bitfinex_config ) 
~~ q_list = [ ] 
~~~ identifier , q = self . channel_configs . popitem ( last = True if soft else False ) 
~~ q_list . append ( ( identifier , q . copy ( ) ) ) 
if identifier == 'auth' : 
~~~ self . send ( ** q , auth = True ) 
~~ if soft : 
~~~ q [ 'event' ] = 'unsubscribe' 
~~ self . send ( ** q ) 
~~~ for identifier , q in reversed ( q_list ) : 
~~~ self . channel_configs [ identifier ] = q 
self . send ( ** q ) 
~~~ for identifier , q in q_list : 
~~ ~~ ~~ def join ( self , timeout = None ) : 
self . _stopped . set ( ) 
super ( QueueProcessor , self ) . join ( timeout = timeout ) 
while not self . _stopped . is_set ( ) : 
~~~ message = self . q . get ( timeout = 0.1 ) 
~~ dtype , data , ts = message 
if dtype in ( 'subscribed' , 'unsubscribed' , 'conf' , 'auth' , 'unauth' ) : 
~~~ self . _response_handlers [ dtype ] ( dtype , data , ts ) 
~~ ~~ elif dtype == 'data' : 
~~~ channel_id = data [ 0 ] 
if channel_id != 0 : 
~~~ channel_type , * _ = self . channel_directory [ channel_id ] 
self . _data_handlers [ channel_type ] ( channel_type , data , ts ) 
self . update_timestamps ( channel_id , ts ) 
~~~ self . _handle_account ( data = data , ts = ts ) 
message ) 
~~ ~~ ~~ def _handle_subscribed ( self , dtype , data , ts , ) : 
channel_name = data . pop ( 'channel' ) 
channel_id = data . pop ( 'chanId' ) 
config = data 
if 'pair' in config : 
~~~ symbol = config [ 'pair' ] 
if symbol . startswith ( 't' ) : 
~~~ symbol = symbol [ 1 : ] 
~~ ~~ elif 'symbol' in config : 
~~~ symbol = config [ 'symbol' ] 
~~ ~~ elif 'key' in config : 
~~~ symbol = None 
~~ if 'prec' in config and config [ 'prec' ] . startswith ( 'R' ) : 
~~~ channel_name = 'raw_' + channel_name 
~~ self . channel_handlers [ channel_id ] = self . _data_handlers [ channel_name ] 
if 'key' in config : 
~~~ identifier = ( channel_name , symbol , config [ 'key' ] . split ( ':' ) [ 1 ] ) 
~~~ identifier = ( channel_name , symbol ) 
~~ self . channel_handlers [ channel_id ] = identifier 
self . channel_directory [ identifier ] = channel_id 
self . channel_directory [ channel_id ] = identifier 
~~ def _handle_unsubscribed ( self , dtype , data , ts ) : 
chan_identifier = self . channel_directory . pop ( channel_id ) 
self . channel_directory . pop ( chan_identifier ) 
self . channel_handlers . pop ( channel_id ) 
self . last_update . pop ( channel_id ) 
~~ def _handle_auth ( self , dtype , data , ts ) : 
if dtype == 'unauth' : 
~~ channel_id = data . pop ( 'chanId' ) 
user_id = data . pop ( 'userId' ) 
identifier = ( 'auth' , user_id ) 
self . channel_handlers [ identifier ] = channel_id 
~~ def _handle_conf ( self , dtype , data , ts ) : 
~~ def update_timestamps ( self , chan_id , ts ) : 
~~~ self . last_update [ chan_id ] = ts 
self . channel_directory [ chan_id ] ) 
~~ ~~ def _handle_account ( self , data , ts ) : 
chan_id , channel_short_name , * data = data 
entry = ( channel_short_name , data , ts ) 
self . account . put ( entry ) 
~~ def _handle_ticker ( self , dtype , data , ts ) : 
channel_id , * data = data 
channel_identifier = self . channel_directory [ channel_id ] 
entry = ( data , ts ) 
self . tickers [ channel_identifier ] . put ( entry ) 
~~ def _handle_book ( self , dtype , data , ts ) : 
self . books [ channel_identifier ] . put ( entry ) 
~~ def _handle_raw_book ( self , dtype , data , ts ) : 
self . raw_books [ channel_identifier ] . put ( entry ) 
~~ def _handle_trades ( self , dtype , data , ts ) : 
self . trades [ channel_identifier ] . put ( entry ) 
~~ def _handle_candles ( self , dtype , data , ts ) : 
self . candles [ channel_identifier ] . put ( entry ) 
~~ def reset ( self ) : 
self . conn . reconnect ( ) 
while not self . conn . connected . is_set ( ) : 
~~ for key in self . channel_configs : 
~~~ self . conn . send ( ** self . channel_configs [ key ] ) 
~~ ~~ def candles ( self , pair , timeframe = None ) : 
timeframe = '1m' if not timeframe else timeframe 
key = ( 'candles' , pair , timeframe ) 
return self . queue_processor . candles [ key ] 
~~ def config ( self , decimals_as_strings = True , ts_as_dates = False , 
sequencing = False , ts = False , ** kwargs ) : 
flags = 0 
if decimals_as_strings : 
~~~ flags += 8 
~~ if ts_as_dates : 
~~~ flags += 32 
~~ if ts : 
~~~ flags += 32768 
~~ if sequencing : 
~~~ flags += 65536 
~~ q = { 'event' : 'conf' , 'flags' : flags } 
q . update ( kwargs ) 
self . conn . bitfinex_config = q 
self . conn . send ( ** q ) 
~~ def subscribe_to_ticker ( self , pair , ** kwargs ) : 
identifier = ( 'ticker' , pair ) 
self . _subscribe ( 'ticker' , identifier , symbol = pair , ** kwargs ) 
~~ def unsubscribe_from_ticker ( self , pair , ** kwargs ) : 
self . _unsubscribe ( 'ticker' , identifier , symbol = pair , ** kwargs ) 
~~ def subscribe_to_order_book ( self , pair , ** kwargs ) : 
identifier = ( 'book' , pair ) 
self . _subscribe ( 'book' , identifier , symbol = pair , ** kwargs ) 
~~ def unsubscribe_from_order_book ( self , pair , ** kwargs ) : 
self . _unsubscribe ( 'book' , identifier , symbol = pair , ** kwargs ) 
~~ def subscribe_to_raw_order_book ( self , pair , prec = None , ** kwargs ) : 
identifier = ( 'raw_book' , pair ) 
prec = 'R0' if prec is None else prec 
self . _subscribe ( 'book' , identifier , pair = pair , prec = prec , ** kwargs ) 
~~ def unsubscribe_from_raw_order_book ( self , pair , prec = None , ** kwargs ) : 
self . _unsubscribe ( 'book' , identifier , pair = pair , prec = prec , ** kwargs ) 
~~ def subscribe_to_trades ( self , pair , ** kwargs ) : 
identifier = ( 'trades' , pair ) 
self . _subscribe ( 'trades' , identifier , symbol = pair , ** kwargs ) 
~~ def unsubscribe_from_trades ( self , pair , ** kwargs ) : 
self . _unsubscribe ( 'trades' , identifier , symbol = pair , ** kwargs ) 
~~ def subscribe_to_candles ( self , pair , timeframe = None , ** kwargs ) : 
valid_tfs = [ '1m' , '5m' , '15m' , '30m' , '1h' , '3h' , '6h' , '12h' , '1D' , 
'7D' , '14D' , '1M' ] 
if timeframe : 
~~~ if timeframe not in valid_tfs : 
~~~ timeframe = '1m' 
~~ identifier = ( 'candles' , pair , timeframe ) 
pair = 't' + pair if not pair . startswith ( 't' ) else pair 
key = 'trade:' + timeframe + ':' + pair 
self . _subscribe ( 'candles' , identifier , key = key , ** kwargs ) 
~~ def unsubscribe_from_candles ( self , pair , timeframe = None , ** kwargs ) : 
self . _unsubscribe ( 'candles' , identifier , key = key , ** kwargs ) 
~~ def authenticate ( self ) : 
if not self . key and not self . secret : 
~~ self . channel_configs [ 'auth' ] = { 'api_key' : self . key , 'secret' : self . secret } 
self . conn . send ( api_key = self . key , secret = self . secret , auth = True ) 
~~ def cancel_order ( self , multi = False , ** order_identifiers ) : 
if multi : 
~~~ self . _send_auth_command ( 'oc_multi' , order_identifiers ) 
~~~ self . _send_auth_command ( 'oc' , order_identifiers ) 
if isinstance ( self . application , str ) : 
~~~ return util . import_app ( self . application ) 
~~~ return self . application 
~~ ~~ def init_app ( self , app ) : 
if not hasattr ( app , 'extensions' ) : 
~~~ app . extensions = { } 
~~ if 'common' in app . extensions : 
~~ app . extensions [ 'common' ] = self 
if 'COMMON_FILESERVER_DISABLED' not in app . config : 
~~~ with app . test_request_context ( ) : 
~~~ app . wsgi_app = WhiteNoise ( app . wsgi_app , root = url_for ( 'static' , filename = '' ) [ 1 : ] ) 
~~ ~~ self . cache = Cache ( app , config = { 'CACHE_TYPE' : app . config . get ( "COMMON_CACHE_TYPE" , 'simple' ) } ) 
@ app . before_request 
def before_request_callback ( ) : 
~~~ request . start_time = maya . now ( ) 
~~ @ app . after_request 
def after_request_callback ( response ) : 
~~~ if 'COMMON_POWERED_BY_DISABLED' not in current_app . config : 
~~~ response . headers [ 'X-Powered-By' ] = 'Flask' 
~~ if 'COMMON_PROCESSED_TIME_DISABLED' not in current_app . config : 
~~~ response . headers [ 'X-Processed-Time' ] = maya . now ( ) . epoch - request . start_time . epoch 
~~ @ app . route ( '/favicon.ico' ) 
def favicon ( ) : 
~~~ return redirect ( url_for ( 'static' , filename = 'favicon.ico' ) , code = 301 ) 
~~ ~~ def serve ( self , workers = None , ** kwargs ) : 
if self . app . debug : 
self . app . run ( ) 
server = GunicornServer ( 
self . app , workers = workers or number_of_gunicorn_workers ( ) , 
worker_class = 'egg:meinheld#gunicorn_worker' , ** kwargs ) 
server . run ( ) 
~~ ~~ def get_primary_keys ( model ) : 
mapper = model . __mapper__ 
return [ mapper . get_property_by_column ( column ) for column in mapper . primary_key ] 
~~ def _deserialize ( self , value , * args , ** kwargs ) : 
if not isinstance ( value , dict ) : 
~~~ if len ( self . related_keys ) != 1 : 
~~~ self . fail ( 
"invalid" , 
keys = [ prop . key for prop in self . related_keys ] , 
~~ value = { self . related_keys [ 0 ] . key : value } 
~~ if self . transient : 
~~~ return self . related_model ( ** value ) 
~~~ result = self . _get_existing_instance ( 
self . session . query ( self . related_model ) , value 
~~ except NoResultFound : 
~~ def _get_existing_instance ( self , query , value ) : 
if self . columns : 
~~~ result = query . filter_by ( 
** { prop . key : value . get ( prop . key ) for prop in self . related_keys } 
) . one ( ) 
~~~ result = query . get ( [ value . get ( prop . key ) for prop in self . related_keys ] ) 
if result is None : 
~~~ raise NoResultFound 
~~ def _add_column_kwargs ( self , kwargs , column ) : 
if column . nullable : 
~~~ kwargs [ "allow_none" ] = True 
~~ kwargs [ "required" ] = not column . nullable and not _has_default ( column ) 
if hasattr ( column . type , "enums" ) : 
~~~ kwargs [ "validate" ] . append ( validate . OneOf ( choices = column . type . enums ) ) 
~~ if hasattr ( column . type , "length" ) : 
~~~ python_type = column . type . python_type 
~~ except ( AttributeError , NotImplementedError ) : 
~~~ python_type = None 
~~ if not python_type or not issubclass ( python_type , uuid . UUID ) : 
~~~ kwargs [ "validate" ] . append ( validate . Length ( max = column . type . length ) ) 
~~ ~~ if hasattr ( column . type , "scale" ) : 
~~~ kwargs [ "places" ] = getattr ( column . type , "scale" , None ) 
~~ ~~ def _add_relationship_kwargs ( self , kwargs , prop ) : 
nullable = True 
for pair in prop . local_remote_pairs : 
~~~ if not pair [ 0 ] . nullable : 
~~~ if prop . uselist is True : 
~~~ nullable = False 
~~ ~~ kwargs . update ( { "allow_none" : nullable , "required" : not nullable } ) 
~~ def get_declared_fields ( mcs , klass , cls_fields , inherited_fields , dict_cls ) : 
opts = klass . opts 
Converter = opts . model_converter 
converter = Converter ( schema_cls = klass ) 
declared_fields = super ( SchemaMeta , mcs ) . get_declared_fields ( 
klass , cls_fields , inherited_fields , dict_cls 
fields = mcs . get_fields ( converter , opts , declared_fields , dict_cls ) 
fields . update ( declared_fields ) 
return fields 
~~ def get_instance ( self , data ) : 
if self . transient : 
~~ props = get_primary_keys ( self . opts . model ) 
filters = { prop . key : data . get ( prop . key ) for prop in props } 
if None not in filters . values ( ) : 
~~~ return self . session . query ( self . opts . model ) . filter_by ( ** filters ) . first ( ) 
~~ def make_instance ( self , data ) : 
instance = self . instance or self . get_instance ( data ) 
if instance is not None : 
~~~ for key , value in iteritems ( data ) : 
~~~ setattr ( instance , key , value ) 
~~ return instance 
~~ kwargs , association_attrs = self . _split_model_kwargs_association ( data ) 
instance = self . opts . model ( ** kwargs ) 
for attr , value in iteritems ( association_attrs ) : 
~~~ setattr ( instance , attr , value ) 
~~ def load ( self , data , session = None , instance = None , transient = False , * args , ** kwargs ) : 
self . _session = session or self . _session 
self . _transient = transient or self . _transient 
if not ( self . transient or self . session ) : 
~~ self . instance = instance or self . instance 
~~~ return super ( ModelSchema , self ) . load ( data , * args , ** kwargs ) 
~~~ self . instance = None 
~~ ~~ def _split_model_kwargs_association ( self , data ) : 
association_attrs = { 
key : value 
for key , value in iteritems ( data ) 
if hasattr ( getattr ( self . opts . model , key , None ) , "remote_attr" ) 
if ( hasattr ( self . opts . model , key ) and key not in association_attrs ) 
return kwargs , association_attrs 
~~ def on_epoch_end ( self ) -> None : 
self . indexes = np . arange ( self . nrows ) 
if self . shuffle : 
~~~ np . random . shuffle ( self . indexes ) 
~~ ~~ def textacy_cleaner ( text : str ) -> str : 
return preprocess_text ( text , 
fix_unicode = True , 
lowercase = True , 
transliterate = True , 
no_urls = True , 
no_emails = True , 
no_phone_numbers = True , 
no_numbers = True , 
no_currency_symbols = True , 
no_punct = True , 
no_contractions = False , 
no_accents = True ) 
~~ def apply_parallel ( func : Callable , 
data : List [ Any ] , 
cpu_cores : int = None ) -> List [ Any ] : 
if not cpu_cores : 
~~~ cpu_cores = cpu_count ( ) 
~~~ chunk_size = ceil ( len ( data ) / cpu_cores ) 
pool = Pool ( cpu_cores ) 
transformed_data = pool . map ( func , chunked ( data , chunk_size ) , chunksize = 1 ) 
~~~ pool . close ( ) 
return transformed_data 
~~ ~~ def process_text_constructor ( cleaner : Callable , 
tokenizer : Callable , 
append_indicators : bool , 
start_tok : str , 
end_tok : str ) : 
def process_text ( text ) : 
~~~ if append_indicators : 
~~~ return [ [ start_tok ] + tokenizer ( cleaner ( doc ) ) + [ end_tok ] for doc in text ] 
~~ return [ tokenizer ( cleaner ( doc ) ) for doc in text ] 
~~ return process_text 
~~ def process_text ( self , text : List [ str ] ) -> List [ List [ str ] ] : 
process_text = process_text_constructor ( cleaner = self . cleaner , 
tokenizer = self . tokenizer , 
append_indicators = self . append_indicators , 
start_tok = self . start_tok , 
end_tok = self . end_tok ) 
return process_text ( text ) 
~~ def parallel_process_text ( self , data : List [ str ] ) -> List [ List [ str ] ] : 
n_cores = self . num_cores 
return flattenlist ( apply_parallel ( process_text , data , n_cores ) ) 
~~ def generate_doc_length_stats ( self ) : 
heuristic = self . heuristic_pct 
histdf = ( pd . DataFrame ( [ ( a , b ) for a , b in self . document_length_histogram . items ( ) ] , 
columns = [ 'bin' , 'doc_count' ] ) 
. sort_values ( by = 'bin' ) ) 
histdf [ 'cumsum_pct' ] = histdf . doc_count . cumsum ( ) / histdf . doc_count . sum ( ) 
self . document_length_stats = histdf 
self . padding_maxlen = self . doc_length_huerestic 
~~ def fit ( self , 
data : List [ str ] , 
return_tokenized_data : bool = False ) -> Union [ None , List [ List [ str ] ] ] : 
self . __clear_data ( ) 
now = get_time ( ) 
tokenized_data = self . parallel_process_text ( data ) 
if not self . padding_maxlen : 
~~~ length_counts = map ( count_len , tokenized_data ) 
self . document_length_histogram = Counter ( length_counts ) 
self . generate_doc_length_stats ( ) 
self . indexer = custom_Indexer ( num_words = self . keep_n ) 
self . indexer . fit_on_tokenized_texts ( tokenized_data ) 
self . token2id = self . indexer . word_index 
self . id2token = { v : k for k , v in self . token2id . items ( ) } 
self . n_tokens = max ( self . indexer . word_index . values ( ) ) 
if return_tokenized_data : 
~~~ return tokenized_data 
~~ ~~ def token_count_pandas ( self ) : 
freq_df = pd . DataFrame . from_dict ( self . indexer . word_counts , orient = 'index' ) 
freq_df . columns = [ 'count' ] 
return freq_df . sort_values ( 'count' , ascending = False ) 
~~ def fit_transform ( self , 
data : List [ str ] ) -> List [ List [ int ] ] : 
tokenized_data = self . fit ( data , return_tokenized_data = True ) 
indexed_data = self . indexer . tokenized_texts_to_sequences ( tokenized_data ) 
final_data = self . pad ( indexed_data ) 
return final_data 
~~ def transform ( self , data : List [ str ] ) -> List [ List [ int ] ] : 
tokenized_data = self . process_text ( data ) 
return self . pad ( indexed_data ) 
~~ def transform_parallel ( self , data : List [ str ] ) -> List [ List [ int ] ] : 
~~ def pad ( self , docs : List [ List [ int ] ] ) -> List [ List [ int ] ] : 
return pad_sequences ( docs , 
maxlen = self . padding_maxlen , 
dtype = self . padding_dtype , 
padding = self . padding , 
truncating = self . truncating , 
value = self . padding_value ) 
~~ def tokenized_texts_to_sequences ( self , tok_texts ) : 
for vect in self . tokenized_texts_to_sequences_generator ( tok_texts ) : 
~~~ res . append ( vect ) 
~~ def tokenized_texts_to_sequences_generator ( self , tok_texts ) : 
for seq in tok_texts : 
~~~ vect = [ ] 
for w in seq : 
~~~ i = self . word_index . get ( w , 1 ) 
vect . append ( i ) 
~~ yield vect 
~~ ~~ def write_temp_file ( text = "" ) : 
with NamedTemporaryFile ( mode = 'w+t' , suffix = '.yml' , delete = False ) as tempfile : 
~~~ tempfile . write ( text ) 
return tempfile . name 
~~ ~~ def get_contact_list_by_user_selection ( address_books , search , strict_search ) : 
return get_contacts ( 
address_books , search , "name" if strict_search else "all" , 
config . reverse ( ) , config . group_by_addressbook ( ) , config . sort ) 
~~ def get_contacts ( address_books , query , method = "all" , reverse = False , 
group = False , sort = "first_name" ) : 
contacts = [ ] 
for address_book in address_books : 
~~~ contacts . extend ( address_book . search ( query , method = method ) ) 
~~ if group : 
~~~ if sort == "first_name" : 
~~~ return sorted ( contacts , reverse = reverse , key = lambda x : ( 
unidecode ( x . address_book . name ) . lower ( ) , 
unidecode ( x . get_first_name_last_name ( ) ) . lower ( ) ) ) 
~~ elif sort == "last_name" : 
unidecode ( x . get_last_name_first_name ( ) ) . lower ( ) ) ) 
~~~ raise ValueError ( \ 
'{}.' . format ( sort ) ) 
~~~ return sorted ( contacts , reverse = reverse , key = lambda x : 
unidecode ( x . get_first_name_last_name ( ) ) . lower ( ) ) 
unidecode ( x . get_last_name_first_name ( ) ) . lower ( ) ) 
~~ ~~ ~~ def merge_args_into_config ( args , config ) : 
if "display" in args and args . display : 
~~~ config . set_display_by_name ( args . display ) 
~~ if "group_by_addressbook" in args and args . group_by_addressbook : 
~~~ config . set_group_by_addressbook ( True ) 
~~ if "reverse" in args and args . reverse : 
~~~ config . set_reverse ( True ) 
~~ if "sort" in args and args . sort : 
~~~ config . sort = args . sort 
~~ if "vcard_version" in args and args . vcard_version : 
~~~ config . set_preferred_vcard_version ( args . vcard_version ) 
~~ if "search_in_source_files" in args and args . search_in_source_files : 
~~~ config . set_search_in_source_files ( True ) 
~~ if "skip_unparsable" in args and args . skip_unparsable : 
~~~ config . set_skip_unparsable ( True ) 
~~ if "addressbook" in args and not args . addressbook : 
~~~ args . addressbook = [ abook . name for abook in config . abooks ] 
~~ if "target_addressbook" in args and not args . target_addressbook : 
~~~ args . target_addressbook = [ abook . name for abook in config . abooks ] 
~~ ~~ def load_address_books ( names , config , search_queries ) : 
all_names = { str ( book ) for book in config . abooks } 
if not names : 
~~~ names = all_names 
~~ elif not all_names . issuperset ( names ) : 
~~~ sys . exit ( \ 
\ . join ( set ( names ) - all_names ) , 
~~ for name in names : 
~~~ address_book = config . abook . get_abook ( name ) 
address_book . load ( search_queries [ address_book . name ] , 
search_in_source_files = config . search_in_source_files ( ) ) 
yield address_book 
~~ ~~ def prepare_search_queries ( args ) : 
source_queries = [ ] 
target_queries = [ ] 
if "source_search_terms" in args and args . source_search_terms : 
~~~ escaped_term = ".*" . join ( re . escape ( x ) 
for x in args . source_search_terms ) 
source_queries . append ( escaped_term ) 
args . source_search_terms = escaped_term 
~~ if "search_terms" in args and args . search_terms : 
~~~ escaped_term = ".*" . join ( re . escape ( x ) for x in args . search_terms ) 
args . search_terms = escaped_term 
~~ if "target_contact" in args and args . target_contact : 
~~~ escaped_term = re . escape ( args . target_contact ) 
target_queries . append ( escaped_term ) 
args . target_contact = escaped_term 
~~ if "uid" in args and args . uid : 
~~~ source_queries . append ( args . uid ) 
~~ if "target_uid" in args and args . target_uid : 
~~~ target_queries . append ( args . target_uid ) 
~~ source_queries = "^.*(%s).*$" % ')|(' . join ( source_queries ) if source_queries else None 
target_queries = "^.*(%s).*$" % ')|(' . join ( target_queries ) if target_queries else None 
queries = { abook . name : [ ] for abook in config . abook . _abooks } 
for name in queries : 
~~~ if "addressbook" in args and name in args . addressbook : 
~~~ queries [ name ] . append ( source_queries ) 
~~ if "target_addressbook" in args and name in args . target_addressbook : 
~~~ queries [ name ] . append ( target_queries ) 
~~ if None in queries [ name ] : 
~~~ queries [ name ] = None 
~~~ queries [ name ] = "({})" . format ( ')|(' . join ( queries [ name ] ) ) 
return queries 
~~ def generate_contact_list ( config , args ) : 
vcard_list = [ ] 
if "uid" in args and args . uid : 
~~~ logging . debug ( "args.uid=%s" , args . uid ) 
args . search_terms = ".*" 
vcard_list = get_contacts ( args . addressbook , args . uid , method = "uid" ) 
if not vcard_list : 
~~ elif len ( vcard_list ) != 1 : 
for vcard in vcard_list : 
~~~ if "source_search_terms" in args : 
~~~ if args . source_search_terms : 
~~~ args . search_terms = args . source_search_terms 
~~~ args . search_terms = ".*" 
~~ ~~ elif "search_terms" in args : 
~~~ if args . search_terms : 
~~~ args . search_terms = args . search_terms 
~~ logging . debug ( "args.search_terms=%s" , args . search_terms ) 
vcard_list = get_contact_list_by_user_selection ( 
args . addressbook , args . search_terms , 
args . strict_search if "strict_search" in args else False ) 
~~ return vcard_list 
~~ def new_subcommand ( selected_address_books , input_from_stdin_or_file , 
open_editor ) : 
selected_address_book = choose_address_book_from_list ( 
if selected_address_book is None : 
~~ if input_from_stdin_or_file : 
~~~ new_contact = CarddavObject . from_user_input ( 
selected_address_book , input_from_stdin_or_file , 
config . get_supported_private_objects ( ) , 
config . get_preferred_vcard_version ( ) , 
config . localize_dates ( ) ) 
~~ except ValueError as err : 
~~~ print ( err ) 
~~~ new_contact . write_to_file ( ) 
~~ if open_editor : 
~~~ modify_existing_contact ( new_contact ) 
~~~ create_new_contact ( selected_address_book ) 
~~ ~~ def add_email_subcommand ( input_from_stdin_or_file , selected_address_books ) : 
message = message_from_string ( input_from_stdin_or_file , policy = SMTP_POLICY ) 
if not message [ 'From' ] or not message [ 'From' ] . addresses : 
~~ email_address = message [ 'From' ] . addresses [ 0 ] . addr_spec 
name = message [ 'From' ] . addresses [ 0 ] . display_name 
~~~ name = input ( "Contact\ ) 
~~ selected_vcard = choose_vcard_from_list ( 
get_contact_list_by_user_selection ( selected_address_books , name , True ) ) 
if selected_vcard is None : 
if input_string . lower ( ) in [ "" , "n" , "q" ] : 
~~~ print ( "Canceled" ) 
~~ if input_string . lower ( ) == "y" : 
~~ ~~ selected_address_book = choose_address_book_from_list ( 
if not first_name and not last_name and not organisation : 
~~ ~~ selected_vcard = CarddavObject . from_user_input ( 
selected_address_book , 
first_name , last_name , organisation ) , 
~~ for type , email_list in sorted ( 
selected_vcard . get_email_addresses ( ) . items ( ) , 
key = lambda k : k [ 0 ] . lower ( ) ) : 
~~~ for email in email_list : 
~~~ if email == email_address : 
( selected_vcard , email_address ) ) 
~~ ~~ ~~ while True : 
~~~ input_string = input ( 
% ( email_address , selected_vcard ) ) 
( email_address , selected_vcard ) ) 
~~~ selected_vcard . add_email_address ( label , email_address ) 
~~ ~~ selected_vcard . write_to_file ( overwrite = True ) 
print ( "Done.\\n\\n%s" % selected_vcard . print_vcard ( ) ) 
~~ def birthdays_subcommand ( vcard_list , parsable ) : 
vcard_list = [ 
vcard for vcard in vcard_list if vcard . get_birthday ( ) is not None ] 
vcard_list . sort ( 
key = lambda x : ( x . get_birthday ( ) . month , x . get_birthday ( ) . day ) 
if isinstance ( x . get_birthday ( ) , datetime . datetime ) 
else ( 0 , 0 , x . get_birthday ( ) ) ) 
birthday_list = [ ] 
~~~ date = vcard . get_birthday ( ) 
if parsable : 
~~~ if config . display_by_name ( ) == "first_name" : 
~~~ birthday_list . append ( "%04d.%02d.%02d\\t%s" 
% ( date . year , date . month , date . day , 
vcard . get_first_name_last_name ( ) ) ) 
vcard . get_last_name_first_name ( ) ) ) 
~~~ birthday_list . append ( "%s\\t%s" 
% ( vcard . get_first_name_last_name ( ) , 
vcard . get_formatted_birthday ( ) ) ) 
% ( vcard . get_last_name_first_name ( ) , 
~~ ~~ ~~ if birthday_list : 
~~~ if parsable : 
~~~ print ( '\\n' . join ( birthday_list ) ) 
~~~ list_birthdays ( birthday_list ) 
~~~ if not parsable : 
~~ ~~ def phone_subcommand ( search_terms , vcard_list , parsable ) : 
all_phone_numbers_list = [ ] 
matching_phone_number_list = [ ] 
~~~ for type , number_list in sorted ( vcard . get_phone_numbers ( ) . items ( ) , 
~~~ for number in sorted ( number_list ) : 
~~~ name = vcard . get_first_name_last_name ( ) 
~~~ name = vcard . get_last_name_first_name ( ) 
~~ line_formatted = "\\t" . join ( [ name , type , number ] ) 
line_parsable = "\\t" . join ( [ number , name , type ] ) 
~~~ phone_number_line = line_parsable 
~~~ phone_number_line = line_formatted 
~~ if re . search ( search_terms , 
"%s\\n%s" % ( line_formatted , line_parsable ) , 
re . IGNORECASE | re . DOTALL ) : 
~~~ matching_phone_number_list . append ( phone_number_line ) 
~~ elif len ( re . sub ( "\\D" , "" , search_terms ) ) >= 3 : 
~~~ if re . search ( re . sub ( "\\D" , "" , search_terms ) , 
re . sub ( "\\D" , "" , number ) , re . IGNORECASE ) : 
~~ ~~ all_phone_numbers_list . append ( phone_number_line ) 
~~ ~~ ~~ if matching_phone_number_list : 
~~~ print ( '\\n' . join ( matching_phone_number_list ) ) 
~~~ list_phone_numbers ( matching_phone_number_list ) 
~~ ~~ elif all_phone_numbers_list : 
~~~ print ( '\\n' . join ( all_phone_numbers_list ) ) 
~~~ list_phone_numbers ( all_phone_numbers_list ) 
~~ ~~ def post_address_subcommand ( search_terms , vcard_list , parsable ) : 
all_post_address_list = [ ] 
matching_post_address_list = [ ] 
~~ post_address_line_list = [ ] 
~~~ for type , post_address_list in sorted ( vcard . get_post_addresses ( ) . items ( ) , 
~~~ for post_address in post_address_list : 
~~~ post_address_line_list . append ( 
"\\t" . join ( [ str ( post_address ) , name , type ] ) ) 
~~~ for type , post_address_list in sorted ( vcard . get_formatted_post_addresses ( ) . items ( ) , 
~~~ for post_address in sorted ( post_address_list ) : 
"\\t" . join ( [ name , type , post_address ] ) ) 
~~ ~~ ~~ for post_address_line in post_address_line_list : 
~~~ if re . search ( search_terms , 
"%s\\n%s" % ( post_address_line , post_address_line ) , 
~~~ matching_post_address_list . append ( post_address_line ) 
~~ all_post_address_list . append ( post_address_line ) 
~~ ~~ if matching_post_address_list : 
~~~ print ( '\\n' . join ( matching_post_address_list ) ) 
~~~ list_post_addresses ( matching_post_address_list ) 
~~ ~~ elif all_post_address_list : 
~~~ print ( '\\n' . join ( all_post_address_list ) ) 
~~~ list_post_addresses ( all_post_address_list ) 
~~ ~~ def email_subcommand ( search_terms , vcard_list , parsable , remove_first_line ) : 
matching_email_address_list = [ ] 
all_email_address_list = [ ] 
~~~ for type , email_list in sorted ( vcard . get_email_addresses ( ) . items ( ) , 
~~~ for email in sorted ( email_list ) : 
~~ line_formatted = "\\t" . join ( [ name , type , email ] ) 
line_parsable = "\\t" . join ( [ email , name , type ] ) 
~~~ email_address_line = line_parsable 
~~~ email_address_line = line_formatted 
~~~ matching_email_address_list . append ( email_address_line ) 
~~ all_email_address_list . append ( email_address_line ) 
~~ ~~ ~~ if matching_email_address_list : 
~~~ if not remove_first_line : 
~~ print ( '\\n' . join ( matching_email_address_list ) ) 
~~~ list_email_addresses ( matching_email_address_list ) 
~~ ~~ elif all_email_address_list : 
~~ print ( '\\n' . join ( all_email_address_list ) ) 
~~~ list_email_addresses ( all_email_address_list ) 
~~ elif not remove_first_line : 
~~ ~~ def list_subcommand ( vcard_list , parsable ) : 
~~ elif parsable : 
~~~ contact_line_list = [ ] 
~~ contact_line_list . append ( '\\t' . join ( [ vcard . get_uid ( ) , name , 
vcard . address_book . name ] ) ) 
~~ print ( '\\n' . join ( contact_line_list ) ) 
~~~ list_contacts ( vcard_list ) 
~~ ~~ def modify_subcommand ( selected_vcard , input_from_stdin_or_file , open_editor ) : 
if selected_vcard . get_version ( ) not in config . supported_vcard_versions : 
% ( selected_vcard . get_version ( ) , 
config . get_preferred_vcard_version ( ) ) ) 
~~ ~~ ~~ if input_from_stdin_or_file : 
~~~ new_contact = CarddavObject . from_existing_contact_with_new_user_input ( 
selected_vcard , input_from_stdin_or_file , 
~~ if selected_vcard == new_contact : 
~~~ print ( "Modification\\n\\n%s\\n" % new_contact . print_vcard ( ) ) 
~~~ new_contact . write_to_file ( overwrite = True ) 
if open_editor : 
~~~ print ( "Done" ) 
~~~ modify_existing_contact ( selected_vcard ) 
~~ ~~ def remove_subcommand ( selected_vcard , force ) : 
if not force : 
~~ ~~ ~~ selected_vcard . delete_vcard_file ( ) 
~~ def source_subcommand ( selected_vcard , editor ) : 
child = subprocess . Popen ( [ editor , selected_vcard . filename ] ) 
child . communicate ( ) 
~~ def merge_subcommand ( vcard_list , selected_address_books , search_terms , 
target_uid ) : 
if target_uid != "" and search_terms != "" : 
"merge." ) 
~~ if target_uid != "" : 
~~~ target_vcards = get_contacts ( selected_address_books , target_uid , 
method = "uid" ) 
if len ( target_vcards ) != 1 : 
~~~ if not target_vcards : 
for vcard in target_vcards : 
~~ ~~ sys . exit ( 1 ) 
~~~ target_vcards = get_contact_list_by_user_selection ( 
selected_address_books , search_terms , False ) 
vcard_list ) 
if source_vcard is None : 
% ( source_vcard , source_vcard . address_book ) ) 
target_vcards ) 
if target_vcard is None : 
% ( target_vcard , target_vcard . address_book ) ) 
~~ if source_vcard == target_vcard : 
~~~ merge_existing_contacts ( source_vcard , target_vcard , True ) 
~~ ~~ def copy_or_move_subcommand ( action , vcard_list , target_address_book_list ) : 
source_vcard = choose_vcard_from_list ( 
% ( action . title ( ) , source_vcard , source_vcard . address_book ) ) 
~~ if len ( target_address_book_list ) == 1 and target_address_book_list [ 0 ] == source_vcard . address_book : 
% ( target_address_book_list [ 0 ] , source_vcard ) ) 
~~~ available_address_books = [ abook for abook in target_address_book_list 
if abook != source_vcard . address_book ] 
selected_target_address_book = choose_address_book_from_list ( 
if selected_target_address_book is None : 
~~ ~~ target_vcard = choose_vcard_from_list ( 
get_contact_list_by_user_selection ( [ selected_target_address_book ] , 
source_vcard . get_full_name ( ) , True ) ) 
~~~ copy_contact ( source_vcard , selected_target_address_book , 
action == "move" ) 
~~~ if source_vcard == target_vcard : 
if action == "move" : 
~~~ copy_contact ( source_vcard , selected_target_address_book , True ) 
"Source\\n\\n%s\\n\\nTarget\\n\\n%s\\n\\n" 
target_vcard . address_book , source_vcard , 
source_vcard . print_vcard ( ) , target_vcard . print_vcard ( ) , 
"Move" if action == "move" else "Copy" ) ) 
if input_string . lower ( ) == "a" : 
~~ if input_string . lower ( ) == "o" : 
target_vcard . delete_vcard_file ( ) 
~~ if input_string . lower ( ) == "m" : 
~~~ merge_existing_contacts ( source_vcard , target_vcard , 
~~ if input_string . lower ( ) in [ "" , "q" ] : 
~~ ~~ ~~ ~~ ~~ def parse_args ( argv ) : 
base = argparse . ArgumentParser ( 
formatter_class = argparse . RawTextHelpFormatter , add_help = False ) 
base . add_argument ( "--debug" , action = "store_true" , 
base . add_argument ( "--skip-unparsable" , action = "store_true" , 
base . add_argument ( "-v" , "--version" , action = "version" , 
first_parser = argparse . ArgumentParser ( parents = [ base ] ) 
first_parser . add_argument ( 'remainder' , nargs = argparse . REMAINDER ) 
parser = argparse . ArgumentParser ( parents = [ base ] ) 
default_addressbook_parser = argparse . ArgumentParser ( add_help = False ) 
default_addressbook_parser . add_argument ( 
"-a" , "--addressbook" , default = [ ] , 
type = lambda x : [ y . strip ( ) for y in x . split ( "," ) ] , 
new_addressbook_parser = argparse . ArgumentParser ( add_help = False ) 
new_addressbook_parser . add_argument ( 
copy_move_addressbook_parser = argparse . ArgumentParser ( add_help = False ) 
copy_move_addressbook_parser . add_argument ( 
"-A" , "--target-addressbook" , default = [ ] , 
merge_addressbook_parser = argparse . ArgumentParser ( add_help = False ) 
merge_addressbook_parser . add_argument ( 
email_header_input_file_parser = argparse . ArgumentParser ( add_help = False ) 
email_header_input_file_parser . add_argument ( 
"-i" , "--input-file" , default = "-" , 
template_input_file_parser = argparse . ArgumentParser ( add_help = False ) 
template_input_file_parser . add_argument ( 
sort_parser = argparse . ArgumentParser ( add_help = False ) 
sort_parser . add_argument ( 
"-d" , "--display" , choices = ( "first_name" , "last_name" ) , 
"-g" , "--group-by-addressbook" , action = "store_true" , 
"-r" , "--reverse" , action = "store_true" , 
"-s" , "--sort" , choices = ( "first_name" , "last_name" ) , 
default_search_parser = argparse . ArgumentParser ( add_help = False ) 
default_search_parser . add_argument ( 
"-f" , "--search-in-source-files" , action = "store_true" , 
"-e" , "--strict-search" , action = "store_true" , 
merge_search_parser = argparse . ArgumentParser ( add_help = False ) 
merge_search_parser . add_argument ( 
"-t" , "--target-contact" , "--target" , default = "" , 
"source_search_terms" , nargs = "*" , metavar = "source" , 
subparsers = parser . add_subparsers ( dest = "action" ) 
list_parser = subparsers . add_parser ( 
"list" , 
aliases = Actions . get_aliases ( "list" ) , 
parents = [ default_addressbook_parser , default_search_parser , 
sort_parser ] , 
list_parser . add_argument ( 
"-p" , "--parsable" , action = "store_true" , 
subparsers . add_parser ( 
"details" , 
aliases = Actions . get_aliases ( "details" ) , 
export_parser = subparsers . add_parser ( 
"export" , 
aliases = Actions . get_aliases ( "export" ) , 
export_parser . add_argument ( 
"--empty-contact-template" , action = "store_true" , 
"-o" , "--output-file" , default = sys . stdout , 
type = argparse . FileType ( "w" ) , 
birthdays_parser = subparsers . add_parser ( 
"birthdays" , 
aliases = Actions . get_aliases ( "birthdays" ) , 
parents = [ default_addressbook_parser , default_search_parser ] , 
birthdays_parser . add_argument ( 
email_parser = subparsers . add_parser ( 
"email" , 
aliases = Actions . get_aliases ( "email" ) , 
email_parser . add_argument ( 
"--remove-first-line" , action = "store_true" , 
phone_parser = subparsers . add_parser ( 
"phone" , 
aliases = Actions . get_aliases ( "phone" ) , 
phone_parser . add_argument ( 
post_address_parser = subparsers . add_parser ( 
"postaddress" , 
aliases = Actions . get_aliases ( "postaddress" ) , 
post_address_parser . add_argument ( 
"source" , 
aliases = Actions . get_aliases ( "source" ) , 
new_parser = subparsers . add_parser ( 
"new" , 
aliases = Actions . get_aliases ( "new" ) , 
parents = [ new_addressbook_parser , template_input_file_parser ] , 
new_parser . add_argument ( 
"--vcard-version" , choices = ( "3.0" , "4.0" ) , 
add_email_parser = subparsers . add_parser ( 
"add-email" , 
aliases = Actions . get_aliases ( "add-email" ) , 
parents = [ default_addressbook_parser , email_header_input_file_parser , 
default_search_parser , sort_parser ] , 
add_email_parser . add_argument ( 
"merge" , 
aliases = Actions . get_aliases ( "merge" ) , 
parents = [ merge_addressbook_parser , merge_search_parser , sort_parser ] , 
"modify" , 
aliases = Actions . get_aliases ( "modify" ) , 
parents = [ default_addressbook_parser , template_input_file_parser , 
"copy" , 
aliases = Actions . get_aliases ( "copy" ) , 
parents = [ copy_move_addressbook_parser , default_search_parser , 
"move" , 
aliases = Actions . get_aliases ( "move" ) , 
remove_parser = subparsers . add_parser ( 
"remove" , 
aliases = Actions . get_aliases ( "remove" ) , 
remove_parser . add_argument ( 
"--force" , action = "store_true" , 
"addressbooks" , 
aliases = Actions . get_aliases ( "addressbooks" ) , 
"filename" , 
aliases = Actions . get_aliases ( "filename" ) , 
first_parser . print_help = parser . print_help 
args = first_parser . parse_args ( argv ) 
remainder = args . remainder 
if "debug" in args and args . debug : 
~~ global config 
config = Config ( args . config ) 
if ( "debug" in args and args . debug ) or config . debug : 
logging . debug ( "remainder=%s" , remainder ) 
if not remainder or remainder [ 0 ] not in Actions . get_all ( ) : 
~~~ remainder . insert ( 0 , config . default_action ) 
~~ skip = args . skip_unparsable 
args = parser . parse_args ( remainder ) 
args . skip_unparsable = skip 
if "uid" in args and args . uid and ( 
( "search_terms" in args and args . search_terms ) or 
( "source_search_terms" in args and args . source_search_terms ) ) : 
~~ return args 
~~ def get_action ( cls , alias ) : 
for action , alias_list in cls . action_map . items ( ) : 
~~~ if alias in alias_list : 
~~ def _convert_boolean_config_value ( config , name , default = True ) : 
if name not in config : 
~~~ config [ name ] = default 
~~ elif config [ name ] == "yes" : 
~~~ config [ name ] = True 
~~ elif config [ name ] == "no" : 
~~~ config [ name ] = False 
~~ ~~ def new_contact ( cls , address_book , supported_private_objects , version , 
localize_dates ) : 
return cls ( address_book , None , supported_private_objects , version , 
localize_dates ) 
~~ def from_file ( cls , address_book , filename , supported_private_objects , 
return cls ( address_book , filename , supported_private_objects , None , 
~~ def from_user_input ( cls , address_book , user_input , 
supported_private_objects , version , localize_dates ) : 
contact = cls ( address_book , None , supported_private_objects , version , 
contact . _process_user_input ( user_input ) 
return contact 
~~ def from_existing_contact_with_new_user_input ( cls , contact , user_input , 
contact = cls ( contact . address_book , contact . filename , 
contact . supported_private_objects , None , localize_dates ) 
~~ def _get_names_part ( self , part ) : 
~~~ the_list = getattr ( self . vcard . n . value , part ) 
~~~ if not '' . join ( the_list ) : 
~~ ~~ return the_list if isinstance ( the_list , list ) else [ the_list ] 
~~ def get_first_name_last_name ( self ) : 
if self . _get_first_names ( ) : 
~~~ names += self . _get_first_names ( ) 
~~ if self . _get_additional_names ( ) : 
~~~ names += self . _get_additional_names ( ) 
~~ if self . _get_last_names ( ) : 
~~~ names += self . _get_last_names ( ) 
~~ if names : 
~~~ return self . get_full_name ( ) 
~~ ~~ def get_last_name_first_name ( self ) : 
last_names = [ ] 
if self . _get_last_names ( ) : 
~~~ last_names += self . _get_last_names ( ) 
~~ first_and_additional_names = [ ] 
~~~ first_and_additional_names += self . _get_first_names ( ) 
~~~ first_and_additional_names += self . _get_additional_names ( ) 
~~ if last_names and first_and_additional_names : 
~~ elif last_names : 
~~ elif first_and_additional_names : 
~~ ~~ def _get_organisations ( self ) : 
organisations = [ ] 
for child in self . vcard . getChildren ( ) : 
~~~ if child . name == "ORG" : 
~~~ organisations . append ( child . value ) 
~~ ~~ return sorted ( organisations ) 
~~ def _get_titles ( self ) : 
titles = [ ] 
~~~ if child . name == "TITLE" : 
~~~ titles . append ( child . value ) 
~~ ~~ return sorted ( titles ) 
~~ def _get_roles ( self ) : 
roles = [ ] 
~~~ if child . name == "ROLE" : 
~~~ roles . append ( child . value ) 
~~ ~~ return sorted ( roles ) 
~~ def get_phone_numbers ( self ) : 
phone_dict = { } 
~~~ if child . name == "TEL" : 
~~~ type = helpers . list_to_string ( 
if type not in phone_dict : 
~~~ phone_dict [ type ] = [ ] 
~~ if child . value . lower ( ) . startswith ( "tel:" ) : 
~~~ phone_dict [ type ] . append ( child . value [ 4 : ] ) 
~~~ phone_dict [ type ] . append ( child . value ) 
~~ ~~ ~~ for number_list in phone_dict . values ( ) : 
~~~ number_list . sort ( ) 
~~ return phone_dict 
~~ def get_email_addresses ( self ) : 
email_dict = { } 
~~~ if child . name == "EMAIL" : 
if type not in email_dict : 
~~~ email_dict [ type ] = [ ] 
~~ email_dict [ type ] . append ( child . value ) 
~~ ~~ for email_list in email_dict . values ( ) : 
~~~ email_list . sort ( ) 
~~ return email_dict 
~~ def get_post_addresses ( self ) : 
post_adr_dict = { } 
~~~ if child . name == "ADR" : 
if type not in post_adr_dict : 
~~~ post_adr_dict [ type ] = [ ] 
~~ post_adr_dict [ type ] . append ( 
"box" : child . value . box , 
"extended" : child . value . extended , 
"street" : child . value . street , 
"code" : child . value . code , 
"city" : child . value . city , 
"region" : child . value . region , 
"country" : child . value . country 
~~ ~~ for post_adr_list in post_adr_dict . values ( ) : 
~~~ post_adr_list . sort ( key = lambda x : ( 
~~ return post_adr_dict 
~~ def _get_categories ( self ) : 
category_list = [ ] 
~~~ if child . name == "CATEGORIES" : 
~~~ value = child . value 
category_list . append ( 
value if isinstance ( value , list ) else [ value ] ) 
~~ ~~ if len ( category_list ) == 1 : 
~~~ return category_list [ 0 ] 
~~ return sorted ( category_list ) 
~~ def _add_category ( self , categories ) : 
categories_obj = self . vcard . add ( 'categories' ) 
categories_obj . value = helpers . convert_to_vcard ( 
"category" , categories , ObjectType . list_with_strings ) 
~~ def get_nicknames ( self ) : 
nicknames = [ ] 
~~~ if child . name == "NICKNAME" : 
~~~ nicknames . append ( child . value ) 
~~ ~~ return sorted ( nicknames ) 
~~ def _get_notes ( self ) : 
~~~ if child . name == "NOTE" : 
~~~ notes . append ( child . value ) 
~~ ~~ return sorted ( notes ) 
~~ def _get_private_objects ( self ) : 
private_objects = { } 
~~~ if child . name . lower ( ) . startswith ( "x-" ) : 
~~~ key_index = [ 
x . lower ( ) for x in self . supported_private_objects 
] . index ( child . name [ 2 : ] . lower ( ) ) 
~~~ key = self . supported_private_objects [ key_index ] 
if key not in private_objects : 
~~~ private_objects [ key ] = [ ] 
~~ private_objects [ key ] . append ( child . value ) 
~~ ~~ ~~ for value in private_objects . values ( ) : 
~~~ value . sort ( ) 
~~ return private_objects 
~~ def _get_webpages ( self ) : 
urls = [ ] 
~~~ if child . name == "URL" : 
~~~ urls . append ( child . value ) 
~~ ~~ return sorted ( urls ) 
~~ def get_anniversary ( self ) : 
~~~ if self . vcard . anniversary . params . get ( "VALUE" ) [ 0 ] == "text" : 
~~~ return self . vcard . anniversary . value 
~~ ~~ except ( AttributeError , IndexError , TypeError ) : 
~~~ return helpers . string_to_date ( self . vcard . anniversary . value ) 
~~ except ( AttributeError , ValueError ) : 
~~~ return helpers . string_to_date ( self . vcard . x_anniversary . value ) 
~~ def get_birthday ( self ) : 
~~~ if self . vcard . bday . params . get ( "VALUE" ) [ 0 ] == "text" : 
~~~ return self . vcard . bday . value 
~~~ return helpers . string_to_date ( self . vcard . bday . value ) 
~~ def _get_types_for_vcard_object ( self , object , default_type ) : 
type_list = [ ] 
if object . group : 
~~~ for label in self . vcard . getChildren ( ) : 
~~~ if label . name == "X-ABLABEL" and label . group == object . group : 
~~~ custom_type = label . value . strip ( ) 
if custom_type : 
~~~ type_list . append ( custom_type ) 
~~ ~~ ~~ ~~ standard_types = object . params . get ( "TYPE" ) 
if standard_types is not None : 
~~~ if not isinstance ( standard_types , list ) : 
~~~ standard_types = [ standard_types ] 
~~ for type in standard_types : 
~~~ type = type . strip ( ) 
if type and type . lower ( ) != "pref" : 
~~~ if not type . lower ( ) . startswith ( "x-" ) : 
~~~ type_list . append ( type ) 
~~ elif type [ 2 : ] . lower ( ) not in [ x . lower ( ) for x in type_list ] : 
~~~ type_list . append ( type [ 2 : ] ) 
~~ ~~ ~~ ~~ try : 
~~~ type_list . append ( "pref=%d" % int ( object . params . get ( "PREF" ) [ 0 ] ) ) 
~~ except ( IndexError , TypeError , ValueError ) : 
~~~ for x in object . params . get ( "TYPE" ) : 
~~~ if x . lower ( ) == "pref" and "pref" not in type_list : 
~~~ type_list . append ( "pref" ) 
~~ ~~ if type_list : 
~~~ return type_list 
~~ return [ default_type ] 
~~ def _parse_type_value ( types , value , supported_types ) : 
custom_types = [ ] 
standard_types = [ ] 
pref = 0 
for type in types : 
if type : 
~~~ if type . lower ( ) in supported_types : 
~~~ standard_types . append ( type ) 
~~ elif type . lower ( ) == "pref" : 
~~~ pref += 1 
~~ elif re . match ( r"^pref=\\d{1,2}$" , type . lower ( ) ) : 
~~~ pref += int ( type . split ( "=" ) [ 1 ] ) 
~~~ if type . lower ( ) . startswith ( "x-" ) : 
~~~ custom_types . append ( type [ 2 : ] ) 
standard_types . append ( type ) 
~~~ custom_types . append ( type ) 
standard_types . append ( "X-{}" . format ( type ) ) 
~~ ~~ ~~ ~~ return ( standard_types , custom_types , pref ) 
~~ def list_to_string ( input , delimiter ) : 
if isinstance ( input , list ) : 
~~~ return delimiter . join ( 
list_to_string ( item , delimiter ) for item in input ) 
~~ return input 
~~ def string_to_date ( input ) : 
for format_string in ( "--%m%d" , "--%m-%d" , "%Y%m%d" , "%Y-%m-%d" , 
"%Y%m%dT%H%M%S" , "%Y-%m-%dT%H:%M:%S" , 
"%Y%m%dT%H%M%SZ" , "%Y-%m-%dT%H:%M:%SZ" ) : 
~~~ return datetime . strptime ( input , format_string ) 
~~ ~~ for format_string in ( "%Y%m%dT%H%M%S%z" , "%Y-%m-%dT%H:%M:%S%z" ) : 
~~~ return datetime . strptime ( '' . join ( input . rsplit ( ":" , 1 ) ) , 
format_string ) 
~~ ~~ raise ValueError 
~~ def convert_to_yaml ( 
name , value , indentation , indexOfColon , show_multi_line_character ) : 
strings = [ ] 
if isinstance ( value , list ) : 
~~~ if len ( value ) == 1 and isinstance ( value [ 0 ] , str ) : 
~~~ value = value [ 0 ] 
~~ elif len ( value ) == 1 and isinstance ( value [ 0 ] , list ) and len ( value [ 0 ] ) == 1 and isinstance ( value [ 0 ] [ 0 ] , str ) : 
~~~ value = value [ 0 ] [ 0 ] 
~~ ~~ if isinstance ( value , str ) : 
indent_multiline_string ( value , indentation + 4 , 
show_multi_line_character ) ) ) 
~~ elif isinstance ( value , list ) : 
for outer in value : 
~~~ if isinstance ( outer , list ) and len ( outer ) == 1 and isinstance ( outer [ 0 ] , str ) : 
~~~ outer = outer [ 0 ] 
~~ if isinstance ( outer , str ) : 
outer , indentation + 8 , show_multi_line_character ) ) ) 
~~ elif isinstance ( outer , list ) : 
for inner in outer : 
~~~ if isinstance ( inner , str ) : 
inner , indentation + 12 , 
~~ ~~ ~~ ~~ ~~ return strings 
~~ def convert_to_vcard ( name , value , allowed_object_type ) : 
if isinstance ( value , str ) : 
~~~ if allowed_object_type == ObjectType . list_with_strings : 
~~~ return value . strip ( ) 
~~ ~~ elif isinstance ( value , list ) : 
~~~ if allowed_object_type == ObjectType . string : 
~~~ for entry in value : 
~~~ if not isinstance ( entry , str ) : 
~~ ~~ return [ x . strip ( ) for x in value if x ] 
~~ elif allowed_object_type == ObjectType . list_with_strings : 
~~ ~~ ~~ def _compare_uids ( uid1 , uid2 ) : 
sum = 0 
for char1 , char2 in zip ( uid1 , uid2 ) : 
~~~ if char1 == char2 : 
~~~ sum += 1 
~~ ~~ return sum 
~~ def _search_all ( self , query ) : 
regexp = re . compile ( query , re . IGNORECASE | re . DOTALL ) 
for contact in self . contacts . values ( ) : 
~~~ contact_details = contact . print_vcard ( ) 
if regexp . search ( contact_details ) is not None : 
~~~ yield contact 
~~~ clean_contact_details = re . sub ( "[^a-zA-Z0-9\\n]" , "" , 
contact_details ) 
if regexp . search ( clean_contact_details ) is not None and len ( re . sub ( "\\D" , "" , query ) ) >= 3 : 
~~ ~~ ~~ ~~ def _search_names ( self , query ) : 
~~~ if regexp . search ( contact . get_full_name ( ) ) is not None : 
~~ ~~ ~~ def _search_uid ( self , query ) : 
~~~ yield self . contacts [ query ] 
~~~ for uid in self . contacts : 
~~~ if uid . startswith ( query ) : 
~~~ yield self . contacts [ uid ] 
~~ ~~ ~~ ~~ def search ( self , query , method = "all" ) : 
if not self . _loaded : 
~~~ self . load ( query ) 
~~ if method == "all" : 
~~~ search_function = self . _search_all 
~~ elif method == "name" : 
~~~ search_function = self . _search_names 
~~ elif method == "uid" : 
~~~ search_function = self . _search_uid 
~~ return list ( search_function ( query ) ) 
~~ def get_short_uid_dict ( self , query = None ) : 
if self . _short_uids is None : 
~~~ if not self . _loaded : 
~~ if not self . contacts : 
~~~ self . _short_uids = { } 
~~ elif len ( self . contacts ) == 1 : 
~~~ self . _short_uids = { uid [ 0 : 1 ] : contact 
for uid , contact in self . contacts . items ( ) } 
sorted_uids = sorted ( self . contacts ) 
item0 , item1 = sorted_uids [ : 2 ] 
same1 = self . _compare_uids ( item0 , item1 ) 
self . _short_uids [ item0 [ : same1 + 1 ] ] = self . contacts [ item0 ] 
for item_new in sorted_uids [ 2 : ] : 
~~~ item0 , item1 = item1 , item_new 
same0 , same1 = same1 , self . _compare_uids ( item0 , item1 ) 
same = max ( same0 , same1 ) 
self . _short_uids [ item0 [ : same + 1 ] ] = self . contacts [ item0 ] 
~~ self . _short_uids [ item1 [ : same1 + 1 ] ] = self . contacts [ item1 ] 
~~ ~~ return self . _short_uids 
~~ def get_short_uid ( self , uid ) : 
if uid : 
~~~ short_uids = self . get_short_uid_dict ( ) 
for length_of_uid in range ( len ( uid ) , 0 , - 1 ) : 
~~~ if short_uids . get ( uid [ : length_of_uid ] ) is not None : 
~~~ return uid [ : length_of_uid ] 
~~ def _find_vcard_files ( self , search = None , search_in_source_files = False ) : 
files = glob . glob ( os . path . join ( self . path , "*.vcf" ) ) 
if search and search_in_source_files : 
~~~ with open ( filename , "r" ) as filehandle : 
~~~ if re . search ( search , filehandle . read ( ) , 
~~~ yield filename 
~~~ yield from files 
~~ ~~ def load ( self , query = None , search_in_source_files = False ) : 
if self . _loaded : 
errors = 0 
for filename in self . _find_vcard_files ( 
search = query , search_in_source_files = search_in_source_files ) : 
~~~ card = CarddavObject . from_file ( self , filename , 
self . _private_objects , 
self . _localize_dates ) 
~~ except ( IOError , vobject . base . ParseError ) as err : 
~~~ verb = "open" if isinstance ( err , IOError ) else "parse" 
filename , err ) 
if self . _skip : 
~~~ errors += 1 
~~~ logging . error ( 
sys . exit ( 2 ) 
~~~ uid = card . get_uid ( ) 
if not uid : 
self . name ) 
~~ elif uid in self . contacts : 
~~~ logging . warning ( 
self . contacts [ uid ] , self . name ) 
~~~ self . contacts [ uid ] = card 
~~ ~~ ~~ self . _loaded = True 
if errors : 
errors , len ( self . contacts ) + errors , self ) 
len ( self . contacts ) , self . name ) 
~~ def get_abook ( self , name ) : 
for abook in self . _abooks : 
~~~ if abook . name == name : 
~~~ return abook 
~~ ~~ ~~ def get_table ( self , arch , pattern , colored = False , verbose = False ) : 
rawtable = self . search ( arch , pattern ) 
if len ( rawtable ) == 0 : 
~~ used_hd = self . __fetch_used_headers ( rawtable , verbose ) 
table = [ self . __make_colored_row ( used_hd , 'yellow,bold' , upper = True ) if colored else used_hd ] 
for command in rawtable : 
~~~ cur_tb_field = [ ] 
for hd in used_hd : 
~~~ value = command [ hd ] 
cur_tb_field . append ( self . __make_colored_field ( value , hd , verbose = verbose ) ) 
~~ table . append ( cur_tb_field ) 
~~ return DoubleTable ( table ) 
~~ def avail_archs ( self ) : 
ARM32 : ( KS_ARCH_ARM , KS_MODE_ARM ) , 
ARM64 : ( KS_ARCH_ARM64 , KS_MODE_LITTLE_ENDIAN ) , 
ARM_TB : ( KS_ARCH_ARM , KS_MODE_THUMB ) , 
HEXAGON : ( KS_ARCH_HEXAGON , KS_MODE_BIG_ENDIAN ) , 
MIPS32 : ( KS_ARCH_MIPS , KS_MODE_MIPS32 ) , 
MIPS64 : ( KS_ARCH_MIPS , KS_MODE_MIPS64 ) , 
PPC32 : ( KS_ARCH_PPC , KS_MODE_PPC32 ) , 
PPC64 : ( KS_ARCH_PPC , KS_MODE_PPC64 ) , 
SPARC32 : ( KS_ARCH_SPARC , KS_MODE_SPARC32 ) , 
SPARC64 : ( KS_ARCH_SPARC , KS_MODE_SPARC64 ) , 
SYSTEMZ : ( KS_ARCH_SYSTEMZ , KS_MODE_BIG_ENDIAN ) , 
X86_16 : ( KS_ARCH_X86 , KS_MODE_16 ) , 
X86_32 : ( KS_ARCH_X86 , KS_MODE_32 ) , 
X86_64 : ( KS_ARCH_X86 , KS_MODE_64 ) , 
ARM32 : ( CS_ARCH_ARM , CS_MODE_ARM ) , 
ARM64 : ( CS_ARCH_ARM64 , CS_MODE_LITTLE_ENDIAN ) , 
ARM_TB : ( CS_ARCH_ARM , CS_MODE_THUMB ) , 
MIPS32 : ( CS_ARCH_MIPS , CS_MODE_MIPS32 ) , 
MIPS64 : ( CS_ARCH_MIPS , CS_MODE_MIPS64 ) , 
SPARC32 : ( CS_ARCH_SPARC , CS_MODE_BIG_ENDIAN ) , 
SPARC64 : ( CS_ARCH_SPARC , CS_MODE_V9 ) , 
SYSTEMZ : ( CS_ARCH_SYSZ , CS_MODE_BIG_ENDIAN ) , 
X86_16 : ( CS_ARCH_X86 , CS_MODE_16 ) , 
X86_32 : ( CS_ARCH_X86 , CS_MODE_32 ) , 
X86_64 : ( CS_ARCH_X86 , CS_MODE_64 ) , 
~~ def dump ( obj , file , reducers = None , protocol = None ) : 
global _LokyPickler 
_LokyPickler ( file , reducers = reducers , protocol = protocol ) . dump ( obj ) 
~~ def register ( cls , type , reduce_func ) : 
if sys . version_info < ( 3 , ) : 
~~~ def dispatcher ( cls , obj ) : 
~~~ reduced = reduce_func ( obj ) 
cls . save_reduce ( obj = obj , * reduced ) 
~~ cls . dispatch_table [ type ] = dispatcher 
~~~ cls . dispatch_table [ type ] = reduce_func 
~~ ~~ def _sem_open ( name , value = None ) : 
~~~ handle = pthread . sem_open ( ctypes . c_char_p ( name ) , 0 ) 
~~~ handle = pthread . sem_open ( ctypes . c_char_p ( name ) , SEM_OFLAG , SEM_PERM , 
ctypes . c_int ( value ) ) 
~~ if handle == SEM_FAILURE : 
~~~ e = ctypes . get_errno ( ) 
if e == errno . EEXIST : 
~~ elif e == errno . ENOENT : 
~~ elif e == errno . ENOSYS : 
'system' ) 
~~~ raiseFromErrno ( ) 
~~ ~~ return handle 
~~ def cpu_count ( ) : 
~~~ cpu_count_mp = mp . cpu_count ( ) 
~~ except NotImplementedError : 
~~~ cpu_count_mp = 1 
~~ cpu_count_affinity = cpu_count_mp 
if hasattr ( os , 'sched_getaffinity' ) : 
~~~ cpu_count_affinity = len ( os . sched_getaffinity ( 0 ) ) 
~~ ~~ cpu_count_cfs = cpu_count_mp 
cfs_quota_fname = "/sys/fs/cgroup/cpu/cpu.cfs_quota_us" 
cfs_period_fname = "/sys/fs/cgroup/cpu/cpu.cfs_period_us" 
if os . path . exists ( cfs_quota_fname ) and os . path . exists ( cfs_period_fname ) : 
~~~ with open ( cfs_quota_fname , 'r' ) as fh : 
~~~ cfs_quota_us = int ( fh . read ( ) ) 
~~ with open ( cfs_period_fname , 'r' ) as fh : 
~~~ cfs_period_us = int ( fh . read ( ) ) 
~~ if cfs_quota_us > 0 and cfs_period_us > 0 : 
~~~ cpu_count_cfs = int ( math . ceil ( cfs_quota_us / cfs_period_us ) ) 
~~ ~~ cpu_count_loky = int ( os . environ . get ( 'LOKY_MAX_CPU_COUNT' , cpu_count_mp ) ) 
aggregate_cpu_count = min ( cpu_count_mp , cpu_count_affinity , cpu_count_cfs , 
cpu_count_loky ) 
return max ( aggregate_cpu_count , 1 ) 
~~ def Queue ( self , maxsize = 0 , reducers = None ) : 
from . queues import Queue 
return Queue ( maxsize , reducers = reducers , 
ctx = self . get_context ( ) ) 
~~ def SimpleQueue ( self , reducers = None ) : 
from . queues import SimpleQueue 
return SimpleQueue ( reducers = reducers , ctx = self . get_context ( ) ) 
~~ def _get_chunks ( chunksize , * iterables ) : 
if sys . version_info < ( 3 , 3 ) : 
~~~ it = itertools . izip ( * iterables ) 
~~~ it = zip ( * iterables ) 
~~~ chunk = tuple ( itertools . islice ( it , chunksize ) ) 
if not chunk : 
~~ yield chunk 
~~ ~~ def _sendback_result ( result_queue , work_id , result = None , exception = None ) : 
~~~ result_queue . put ( _ResultItem ( work_id , result = result , 
exception = exception ) ) 
~~~ exc = _ExceptionWithTraceback ( e ) 
result_queue . put ( _ResultItem ( work_id , exception = exc ) ) 
~~ ~~ def _process_worker ( call_queue , result_queue , initializer , initargs , 
processes_management_lock , timeout , worker_exit_lock , 
current_depth ) : 
if initializer is not None : 
~~~ initializer ( * initargs ) 
~~ ~~ global _CURRENT_DEPTH 
_CURRENT_DEPTH = current_depth 
_process_reference_size = None 
_last_memory_leak_check = None 
pid = os . getpid ( ) 
~~~ call_item = call_queue . get ( block = True , timeout = timeout ) 
if call_item is None : 
~~ ~~ except queue . Empty : 
% timeout ) 
if processes_management_lock . acquire ( block = False ) : 
~~~ processes_management_lock . release ( ) 
call_item = None 
~~ ~~ except BaseException as e : 
~~~ previous_tb = traceback . format_exc ( ) 
~~~ result_queue . put ( _RemoteTraceback ( previous_tb ) ) 
~~~ print ( previous_tb ) 
~~ if call_item is None : 
~~~ result_queue . put ( pid ) 
with worker_exit_lock : 
~~~ r = call_item ( ) 
result_queue . put ( _ResultItem ( call_item . work_id , exception = exc ) ) 
~~~ _sendback_result ( result_queue , call_item . work_id , result = r ) 
del r 
~~ del call_item 
if _USE_PSUTIL : 
~~~ if _process_reference_size is None : 
~~~ _process_reference_size = _get_memory_usage ( pid , force_gc = True ) 
_last_memory_leak_check = time ( ) 
~~ if time ( ) - _last_memory_leak_check > _MEMORY_LEAK_CHECK_DELAY : 
~~~ mem_usage = _get_memory_usage ( pid ) 
if mem_usage - _process_reference_size < _MAX_MEMORY_LEAK_SIZE : 
~~ mem_usage = _get_memory_usage ( pid , force_gc = True ) 
result_queue . put ( pid ) 
~~~ if ( ( _last_memory_leak_check is None ) or 
( time ( ) - _last_memory_leak_check > 
_MEMORY_LEAK_CHECK_DELAY ) ) : 
~~~ gc . collect ( ) 
~~ ~~ ~~ ~~ def _add_call_item_to_queue ( pending_work_items , 
running_work_items , 
work_ids , 
call_queue ) : 
~~~ if call_queue . full ( ) : 
~~~ work_id = work_ids . get ( block = False ) 
~~ except queue . Empty : 
~~~ work_item = pending_work_items [ work_id ] 
if work_item . future . set_running_or_notify_cancel ( ) : 
~~~ running_work_items += [ work_id ] 
call_queue . put ( _CallItem ( work_id , 
work_item . fn , 
work_item . args , 
work_item . kwargs ) , 
block = True ) 
~~~ del pending_work_items [ work_id ] 
~~ ~~ ~~ ~~ def _queue_management_worker ( executor_reference , 
executor_flags , 
processes , 
pending_work_items , 
work_ids_queue , 
call_queue , 
result_queue , 
thread_wakeup , 
processes_management_lock ) : 
executor = None 
def is_shutting_down ( ) : 
~~~ return ( _global_shutdown or 
( ( executor is None or executor_flags . shutdown ) 
and not executor_flags . broken ) ) 
~~ def shutdown_all_workers ( ) : 
executor_flags . flag_as_shutting_down ( ) 
with processes_management_lock : 
~~~ n_children_alive = 0 
for p in list ( processes . values ( ) ) : 
~~~ p . _worker_exit_lock . release ( ) 
n_children_alive += 1 
~~ ~~ n_children_to_stop = n_children_alive 
n_sentinels_sent = 0 
while n_sentinels_sent < n_children_to_stop and n_children_alive > 0 : 
~~~ for i in range ( n_children_to_stop - n_sentinels_sent ) : 
~~~ call_queue . put_nowait ( None ) 
n_sentinels_sent += 1 
~~ ~~ with processes_management_lock : 
~~~ n_children_alive = sum ( 
p . is_alive ( ) for p in list ( processes . values ( ) ) 
call_queue . close ( ) 
while processes : 
~~~ _ , p = processes . popitem ( ) 
p . join ( ) 
~~ result_reader = result_queue . _reader 
wakeup_reader = thread_wakeup . _reader 
readers = [ result_reader , wakeup_reader ] 
~~~ _add_call_item_to_queue ( pending_work_items , 
call_queue ) 
worker_sentinels = [ p . sentinel for p in list ( processes . values ( ) ) ] 
ready = wait ( readers + worker_sentinels ) 
TerminatedWorkerError ) 
if result_reader in ready : 
~~~ result_item = result_reader . recv ( ) 
broken = None 
if isinstance ( result_item , _RemoteTraceback ) : 
BrokenProcessPool ) 
~~~ tb = getattr ( e , "__traceback__" , None ) 
if tb is None : 
~~~ _ , _ , tb = sys . exc_info ( ) 
traceback . format_exception ( type ( e ) , e , tb ) , 
~~ ~~ elif wakeup_reader in ready : 
~~~ broken = None 
result_item = None 
~~ thread_wakeup . clear ( ) 
if broken is not None : 
~~~ msg , cause_tb , exc_type = broken 
if ( issubclass ( exc_type , TerminatedWorkerError ) and 
( sys . platform != "win32" ) ) : 
get_exitcodes_terminated_worker ( processes ) ) 
~~ bpe = exc_type ( msg ) 
if cause_tb is not None : 
~~~ bpe = set_cause ( bpe , _RemoteTraceback ( 
"\\n\ . format ( '' . join ( cause_tb ) ) ) ) 
~~ executor_flags . flag_as_broken ( bpe ) 
for work_id , work_item in pending_work_items . items ( ) : 
~~~ work_item . future . set_exception ( bpe ) 
del work_item 
~~ pending_work_items . clear ( ) 
~~~ recursive_terminate ( p ) 
~~ ~~ shutdown_all_workers ( ) 
~~ if isinstance ( result_item , int ) : 
~~~ with processes_management_lock : 
~~~ p = processes . pop ( result_item , None ) 
~~ if p is not None : 
del p 
~~ n_pending = len ( pending_work_items ) 
n_running = len ( running_work_items ) 
if ( n_pending - n_running > 0 or n_running > len ( processes ) ) : 
~~~ executor = executor_reference ( ) 
if ( executor is not None 
and len ( processes ) < executor . _max_workers ) : 
executor . _adjust_process_count ( ) 
~~ ~~ ~~ elif result_item is not None : 
~~~ work_item = pending_work_items . pop ( result_item . work_id , None ) 
if work_item is not None : 
~~~ if result_item . exception : 
~~~ work_item . future . set_exception ( result_item . exception ) 
~~~ work_item . future . set_result ( result_item . result ) 
~~ del work_item 
running_work_items . remove ( result_item . work_id ) 
~~ del result_item 
~~ executor = executor_reference ( ) 
if is_shutting_down ( ) : 
~~~ with executor_flags . shutdown_lock : 
~~~ executor_flags . shutdown = True 
~~ if executor_flags . kill_workers : 
~~~ while pending_work_items : 
~~~ _ , work_item = pending_work_items . popitem ( ) 
work_item . future . set_exception ( ShutdownExecutorError ( 
"complete." ) ) 
~~ while processes : 
recursive_terminate ( p ) 
~~ shutdown_all_workers ( ) 
~~ if not pending_work_items : 
~~~ shutdown_all_workers ( ) 
~~ ~~ elif executor_flags . broken : 
~~ executor = None 
~~ ~~ def _ensure_executor_running ( self ) : 
with self . _processes_management_lock : 
~~~ if len ( self . _processes ) != self . _max_workers : 
~~~ self . _adjust_process_count ( ) 
~~ self . _start_queue_management_thread ( ) 
~~ ~~ def map ( self , fn , * iterables , ** kwargs ) : 
timeout = kwargs . get ( 'timeout' , None ) 
chunksize = kwargs . get ( 'chunksize' , 1 ) 
if chunksize < 1 : 
~~ results = super ( ProcessPoolExecutor , self ) . map ( 
partial ( _process_chunk , fn ) , _get_chunks ( chunksize , * iterables ) , 
return _chain_from_iterable_of_lists ( results ) 
~~ def wrap_non_picklable_objects ( obj , keep_wrapper = True ) : 
if not cloudpickle : 
~~ if inspect . isclass ( obj ) : 
~~~ class CloudpickledClassWrapper ( CloudpickledObjectWrapper ) : 
~~~ def __init__ ( self , * args , ** kwargs ) : 
~~~ self . _obj = obj ( * args , ** kwargs ) 
self . _keep_wrapper = keep_wrapper 
~~ ~~ CloudpickledClassWrapper . __name__ = obj . __name__ 
return CloudpickledClassWrapper 
~~ return _wrap_non_picklable_objects ( obj , keep_wrapper = keep_wrapper ) 
~~ def start ( self , initializer = None , initargs = ( ) ) : 
assert self . _state . value == State . INITIAL 
if ( initializer is not None 
and not hasattr ( initializer , '__call__' ) ) : 
~~ reader , writer = mp . Pipe ( duplex = False ) 
self . _process = Process ( 
target = type ( self ) . _run_server , 
args = ( self . _registry , self . _address , bytes ( self . _authkey ) , 
self . _serializer , writer , initializer , initargs ) , 
ident = ':' . join ( str ( i ) for i in self . _process . _identity ) 
self . _process . name = type ( self ) . __name__ + '-' + ident 
self . _process . start ( ) 
writer . close ( ) 
self . _address = reader . recv ( ) 
reader . close ( ) 
self . _state . value = State . STARTED 
self . shutdown = mp . util . Finalize ( 
self , type ( self ) . _finalize_manager , 
args = ( self . _process , self . _address , self . _authkey , 
self . _state , self . _Client ) , 
exitpriority = 0 
~~ def DupFd ( fd ) : 
popen_obj = get_spawning_popen ( ) 
if popen_obj is not None : 
~~~ return popen_obj . DupFd ( popen_obj . duplicate_for_child ( fd ) ) 
~~ elif HAVE_SEND_HANDLE and sys . version_info [ : 2 ] > ( 3 , 3 ) : 
~~~ from multiprocessing import resource_sharer 
return resource_sharer . DupFd ( fd ) 
~~ ~~ def get_reusable_executor ( max_workers = None , context = None , timeout = 10 , 
kill_workers = False , reuse = "auto" , 
job_reducers = None , result_reducers = None , 
initializer = None , initargs = ( ) ) : 
with _executor_lock : 
~~~ global _executor , _executor_kwargs 
executor = _executor 
if max_workers is None : 
~~~ if reuse is True and executor is not None : 
~~~ max_workers = executor . _max_workers 
~~~ max_workers = cpu_count ( ) 
~~ ~~ elif max_workers <= 0 : 
. format ( max_workers ) ) 
~~ if isinstance ( context , STRING_TYPE ) : 
~~~ context = get_context ( context ) 
~~ if context is not None and context . get_start_method ( ) == "fork" : 
"context" ) 
~~ kwargs = dict ( context = context , timeout = timeout , 
job_reducers = job_reducers , 
result_reducers = result_reducers , 
initializer = initializer , initargs = initargs ) 
if executor is None : 
executor_id = _get_next_executor_id ( ) 
_executor_kwargs = kwargs 
_executor = executor = _ReusablePoolExecutor ( 
_executor_lock , max_workers = max_workers , 
executor_id = executor_id , ** kwargs ) 
~~~ if reuse == 'auto' : 
~~~ reuse = kwargs == _executor_kwargs 
~~ if ( executor . _flags . broken or executor . _flags . shutdown 
or not reuse ) : 
~~~ if executor . _flags . broken : 
~~~ reason = "broken" 
~~ elif executor . _flags . shutdown : 
~~~ reason = "shutdown" 
~~ mp . util . debug ( 
. format ( max_workers , reason ) ) 
executor . shutdown ( wait = True , kill_workers = kill_workers ) 
_executor = executor = _executor_kwargs = None 
return get_reusable_executor ( max_workers = max_workers , 
. format ( executor . _max_workers ) ) 
executor . _resize ( max_workers ) 
~~ ~~ ~~ return executor 
~~ def _wait_job_completion ( self ) : 
if len ( self . _pending_work_items ) > 0 : 
UserWarning ) 
~~ while len ( self . _pending_work_items ) > 0 : 
~~~ time . sleep ( 1e-3 ) 
~~ ~~ def get_preparation_data ( name , init_main_module = True ) : 
_check_not_importing_main ( ) 
d = dict ( 
log_to_stderr = util . _log_to_stderr , 
authkey = bytes ( process . current_process ( ) . authkey ) , 
if util . _logger is not None : 
~~~ d [ 'log_level' ] = util . _logger . getEffectiveLevel ( ) 
if len ( util . _logger . handlers ) > 0 : 
~~~ h = util . _logger . handlers [ 0 ] 
d [ 'log_fmt' ] = h . formatter . _fmt 
~~ ~~ sys_path = [ p for p in sys . path ] 
~~~ i = sys_path . index ( '' ) 
~~~ sys_path [ i ] = process . ORIGINAL_DIR 
~~ d . update ( 
sys_path = sys_path , 
sys_argv = sys . argv , 
orig_dir = process . ORIGINAL_DIR , 
dir = os . getcwd ( ) 
if sys . platform != "win32" : 
~~~ from . import semaphore_tracker 
semaphore_tracker . ensure_running ( ) 
d [ 'tracker_pid' ] = semaphore_tracker . _semaphore_tracker . _pid 
~~ if init_main_module : 
~~~ main_module = sys . modules [ '__main__' ] 
~~~ main_mod_name = getattr ( main_module . __spec__ , "name" , None ) 
~~~ main_mod_name = None 
~~ if main_mod_name is not None : 
~~~ d [ 'init_main_from_name' ] = main_mod_name 
~~ elif sys . platform != 'win32' or ( not WINEXE and not WINSERVICE ) : 
~~~ main_path = getattr ( main_module , '__file__' , None ) 
if main_path is not None : 
~~~ if ( not os . path . isabs ( main_path ) and 
process . ORIGINAL_DIR is not None ) : 
~~~ main_path = os . path . join ( process . ORIGINAL_DIR , main_path ) 
~~ d [ 'init_main_from_path' ] = os . path . normpath ( main_path ) 
d [ 'main_path' ] = d [ 'init_main_from_path' ] 
~~ ~~ ~~ return d 
~~ def prepare ( data ) : 
if 'name' in data : 
~~~ process . current_process ( ) . name = data [ 'name' ] 
~~ if 'authkey' in data : 
~~~ process . current_process ( ) . authkey = data [ 'authkey' ] 
~~ if 'log_to_stderr' in data and data [ 'log_to_stderr' ] : 
~~~ util . log_to_stderr ( ) 
~~ if 'log_level' in data : 
~~~ util . get_logger ( ) . setLevel ( data [ 'log_level' ] ) 
~~ if 'log_fmt' in data : 
~~~ import logging 
util . get_logger ( ) . handlers [ 0 ] . setFormatter ( 
logging . Formatter ( data [ 'log_fmt' ] ) 
~~ if 'sys_path' in data : 
~~~ sys . path = data [ 'sys_path' ] 
~~ if 'sys_argv' in data : 
~~~ sys . argv = data [ 'sys_argv' ] 
~~ if 'dir' in data : 
~~~ os . chdir ( data [ 'dir' ] ) 
~~ if 'orig_dir' in data : 
~~~ process . ORIGINAL_DIR = data [ 'orig_dir' ] 
~~ if 'tracker_pid' in data : 
semaphore_tracker . _semaphore_tracker . _pid = data [ "tracker_pid" ] 
~~ if 'init_main_from_name' in data : 
~~~ _fixup_main_from_name ( data [ 'init_main_from_name' ] ) 
~~ elif 'init_main_from_path' in data : 
~~~ _fixup_main_from_path ( data [ 'init_main_from_path' ] ) 
~~ ~~ def wait ( object_list , timeout = None ) : 
~~~ if timeout <= 0 : 
~~~ return _poll ( object_list , 0 ) 
~~~ deadline = monotonic ( ) + timeout 
~~~ return _poll ( object_list , timeout ) 
~~~ if e . errno != errno . EINTR : 
~~ ~~ if timeout is not None : 
~~~ timeout = deadline - monotonic ( ) 
keep_fds = set ( keep_fds ) . union ( [ 1 , 2 ] ) 
~~~ open_fds = set ( int ( fd ) for fd in os . listdir ( '/proc/self/fd' ) ) 
~~~ import resource 
max_nfds = resource . getrlimit ( resource . RLIMIT_NOFILE ) [ 0 ] 
open_fds = set ( fd for fd in range ( 3 , max_nfds ) ) 
open_fds . add ( 0 ) 
~~ for i in open_fds - keep_fds : 
~~~ os . close ( i ) 
~~ ~~ ~~ def _recursive_terminate_without_psutil ( process ) : 
~~~ _recursive_terminate ( process . pid ) 
process . terminate ( ) 
~~ process . join ( ) 
~~ def _recursive_terminate ( pid ) : 
if sys . platform == "win32" : 
~~~ subprocess . check_output ( 
[ "taskkill" , "/F" , "/T" , "/PID" , str ( pid ) ] , 
stderr = None ) 
~~ except subprocess . CalledProcessError as e : 
~~~ if e . returncode not in [ 1 , 128 , 255 ] : 
~~ elif e . returncode == 1 : 
~~~ os . kill ( pid , signal . SIGTERM ) 
~~~ children_pids = subprocess . check_output ( 
[ "pgrep" , "-P" , str ( pid ) ] , 
stderr = None 
~~~ if e . returncode == 1 : 
~~~ children_pids = b'' 
~~ ~~ children_pids = children_pids . decode ( ) . split ( '\\n' ) [ : - 1 ] 
for cpid in children_pids : 
~~~ cpid = int ( cpid ) 
_recursive_terminate ( cpid ) 
~~ ~~ ~~ ~~ def get_exitcodes_terminated_worker ( processes ) : 
patience = 5 
exitcodes = [ p . exitcode for p in list ( processes . values ( ) ) 
if p . exitcode is not None ] 
while len ( exitcodes ) == 0 and patience > 0 : 
~~~ patience -= 1 
time . sleep ( .05 ) 
~~ return _format_exitcodes ( exitcodes ) 
~~ def _format_exitcodes ( exitcodes ) : 
str_exitcodes = [ "{}({})" . format ( _get_exitcode_name ( e ) , e ) 
for e in exitcodes if e is not None ] 
~~ def main ( fd , verbose = 0 ) : 
signal . signal ( signal . SIGINT , signal . SIG_IGN ) 
signal . signal ( signal . SIGTERM , signal . SIG_IGN ) 
if _HAVE_SIGMASK : 
~~~ signal . pthread_sigmask ( signal . SIG_UNBLOCK , _IGNORED_SIGNALS ) 
~~ for f in ( sys . stdin , sys . stdout ) : 
sys . stderr . flush ( ) 
~~ cache = set ( ) 
~~~ with os . fdopen ( fd , 'rb' ) as f : 
~~~ cmd , name = line . strip ( ) . split ( b':' ) 
if cmd == b'REGISTER' : 
~~~ name = name . decode ( 'ascii' ) 
cache . add ( name ) 
~~ ~~ elif cmd == b'UNREGISTER' : 
cache . remove ( name ) 
. format ( name , len ( cache ) ) ) 
~~ ~~ elif cmd == b'PROBE' : 
~~~ sys . excepthook ( * sys . exc_info ( ) ) 
~~~ if cache : 
len ( cache ) ) 
~~ ~~ for name in cache : 
~~~ sem_unlink ( name ) 
~~ ~~ def ensure_running ( self ) : 
~~~ if self . _fd is not None : 
~~~ if self . _check_alive ( ) : 
~~ os . close ( self . _fd ) 
~~~ os . waitpid ( self . _pid , 0 ) 
~~ self . _fd = None 
self . _pid = None 
~~ fds_to_pass = [ ] 
~~~ fds_to_pass . append ( sys . stderr . fileno ( ) ) 
~~ r , w = os . pipe ( ) 
main . __module__ , r , VERBOSE ) 
~~~ fds_to_pass . append ( r ) 
exe = spawn . get_executable ( ) 
args = [ exe ] + util . _args_from_interpreter_flags ( ) 
if sys . version_info [ : 2 ] <= ( 3 , 3 ) : 
for i in range ( 1 , len ( args ) ) : 
~~~ args [ i ] = re . sub ( "-R+" , "-R" , args [ i ] ) 
~~ ~~ args += [ '-c' , cmd ] 
~~~ if _HAVE_SIGMASK : 
~~~ signal . pthread_sigmask ( signal . SIG_BLOCK , 
_IGNORED_SIGNALS ) 
~~ pid = spawnv_passfds ( exe , args , fds_to_pass ) 
~~~ signal . pthread_sigmask ( signal . SIG_UNBLOCK , 
~~ ~~ ~~ except BaseException : 
~~~ os . close ( w ) 
~~~ self . _fd = w 
self . _pid = pid 
~~~ os . close ( r ) 
~~ ~~ ~~ def login_as ( user , request , store_original_user = True ) : 
original_user_pk = request . user . pk 
if not hasattr ( user , "backend" ) : 
~~~ for backend in django_settings . AUTHENTICATION_BACKENDS : 
~~~ if not hasattr ( load_backend ( backend ) , "get_user" ) : 
~~ if user == load_backend ( backend ) . get_user ( user . pk ) : 
~~~ user . backend = backend 
~~ ~~ if original_user_pk : 
LogEntry . objects . log_action ( 
user_id = original_user_pk , 
content_type_id = ContentType . objects . get_for_model ( user ) . pk , 
object_id = user . pk , 
object_repr = str ( user ) , 
change_message = change_message , 
action_flag = CHANGE , 
~~ if not hasattr ( user , "backend" ) : 
~~ if la_settings . UPDATE_LAST_LOGIN : 
~~~ login ( request , user ) 
~~~ with no_update_last_login ( ) : 
~~ ~~ if store_original_user : 
~~~ messages . warning ( 
la_settings . MESSAGE_LOGIN_SWITCH . format ( username = user . __dict__ [ username_field ] ) , 
extra_tags = la_settings . MESSAGE_EXTRA_TAGS , 
request . session [ la_settings . USER_SESSION_FLAG ] = signer . sign ( original_user_pk ) 
~~ ~~ def restore_original_login ( request ) : 
original_session = request . session . get ( la_settings . USER_SESSION_FLAG ) 
logout ( request ) 
if not original_session : 
~~~ original_user_pk = signer . unsign ( 
original_session , max_age = timedelta ( days = la_settings . USER_SESSION_DAYS_TIMESTAMP ) . total_seconds ( ) 
user = get_user_model ( ) . objects . get ( pk = original_user_pk ) 
messages . info ( 
la_settings . MESSAGE_LOGIN_REVERT . format ( username = user . __dict__ [ username_field ] ) , 
login_as ( user , request , store_original_user = False ) 
if la_settings . USER_SESSION_FLAG in request . session : 
~~~ del request . session [ la_settings . USER_SESSION_FLAG ] 
~~ ~~ except SignatureExpired : 
~~ ~~ def _load_module ( path ) : 
i = path . rfind ( "." ) 
module , attr = path [ : i ] , path [ i + 1 : ] 
~~~ mod = import_module ( module ) 
~~~ can_login_as = getattr ( mod , attr ) 
~~ return can_login_as 
~~ def login ( self ) : 
if self . _session is None : 
~~~ self . _session = requests . session ( ) 
self . _session . headers . update ( { 'User-agent' : str ( UserAgent ( ) . random ) } ) 
~~ return self . _post_login_page ( ) 
~~ def _post_login_page ( self ) : 
'IDToken1' : self . username , 
'IDToken2' : self . password , 
'SunQueryParamsString' : base64 . b64encode ( b'realm=particuliers' ) , 
'encoded' : 'true' , 
'gx_charset' : 'UTF-8' 
~~~ self . _session . post ( LOGIN_URL , 
data = data , 
allow_redirects = False , 
timeout = self . _timeout ) 
~~ if 'iPlanetDirectoryPro' not in self . _session . cookies : 
~~ def _get_data ( self , p_p_resource_id , start_date = None , end_date = None ) : 
'_' + REQ_PART + '_dateDebut' : start_date , 
'_' + REQ_PART + '_dateFin' : end_date 
'p_p_id' : REQ_PART , 
'p_p_lifecycle' : 2 , 
'p_p_state' : 'normal' , 
'p_p_mode' : 'view' , 
'p_p_resource_id' : p_p_resource_id , 
'p_p_cacheability' : 'cacheLevelPage' , 
'p_p_col_id' : 'column-1' , 
'p_p_col_pos' : 1 , 
'p_p_col_count' : 3 
~~~ raw_res = self . _session . post ( DATA_URL , 
if 300 <= raw_res . status_code < 400 : 
~~ if raw_res . text is "" : 
~~ if 302 == raw_res . status_code and "/messages/maintenance.html" in raw_res . text : 
~~~ json_output = raw_res . json ( ) 
~~ except ( OSError , json . decoder . JSONDecodeError , simplejson . errors . JSONDecodeError ) as e : 
~~ if json_output . get ( 'etat' ) . get ( 'valeur' ) == 'erreur' : 
~~ return json_output . get ( 'graphe' ) 
~~ def fetch_data ( self ) : 
for t in [ HOURLY , DAILY , MONTHLY , YEARLY ] : 
~~~ self . _data [ t ] = self . get_data_per_period ( t ) 
parser = argparse . ArgumentParser ( ) 
parser . add_argument ( '-u' , '--username' , 
parser . add_argument ( '-p' , '--password' , 
required = True , help = 'Password' ) 
client = LinkyClient ( args . username , args . password ) 
~~~ client . login ( ) 
client . fetch_data ( ) 
~~ except BaseException as exp : 
~~~ print ( exp ) 
~~~ client . close_session ( ) 
~~ print ( json . dumps ( client . get_data ( ) , indent = 2 ) ) 
if self . __class__ . view : 
~~ with enaml . imports ( ) : 
~~~ View = pydoc . locate ( self . page . view ) 
self . __class__ . view = View ( 
site = self . site , 
page = self . page , 
request = self . request , 
~~ def initialize ( self ) : 
~~~ self . view . handler = self 
self . view . request = self . request 
~~~ from views . index import View 
~~ self . __class__ . view = View ( 
company = current_company , 
handler = self , 
~~ def get ( self , * args , ** kwargs ) : 
if self . is_websocket ( ) : 
~~~ return super ( DemoHandler , self ) . get ( * args , ** kwargs ) 
~~~ self . write ( self . view . render ( ) ) 
~~ ~~ def on_message ( self , message ) : 
change = tornado . escape . json_decode ( message ) 
ref = change . get ( 'ref' ) 
if not ref : 
~~ node = self . view . xpath ( \ . format ( ref ) , first = True ) 
if node is None : 
~~ if change . get ( 'type' ) and change . get ( 'name' ) : 
~~~ if change [ 'type' ] == 'event' : 
~~~ trigger = getattr ( node , change [ 'name' ] ) 
trigger ( ) 
~~ if change [ 'type' ] == 'update' : 
~~~ setattr ( node , change [ 'name' ] , change [ 'value' ] ) 
~~ ~~ ~~ def _update_menus ( self , change ) : 
menus = { } 
links = [ p . link for p in self . pages if p . link ] + self . links 
for link in links : 
~~~ for menu in link . menus : 
~~~ if menu not in menus : 
~~~ menus [ menu ] = [ ] 
~~ menus [ menu ] . append ( link ) 
~~ ~~ for name , menu in menus . items ( ) : 
~~~ k = '{}_menu' . format ( name ) 
if hasattr ( self , k ) : 
~~~ setattr ( self , k , menu ) 
~~ ~~ ~~ def _default_handlers ( self ) : 
static_path = os . path . abspath ( os . path . join ( os . path . dirname ( __file__ ) , "static" ) ) 
( r"/static/(.*)" , cyclone . web . StaticFileHandler , { "path" : static_path } ) , 
for p in self . pages : 
~~~ handler = p . handler 
handler . site = self 
handler . page = p 
urls . append ( ( p . link . url , handler ) ) 
~~ return urls 
change = json . loads ( message ) 
~~ nodes = self . viewer . xpath ( '//*[@ref=$ref]' , ref = ref ) 
if not nodes : 
~~ node = nodes [ 0 ] 
if change . get ( 'type' ) and change . get ( 'name' ) : 
~~ elif change [ 'type' ] == 'update' : 
~~ ~~ def on_dom_modified ( self , change ) : 
self . write_message ( json . dumps ( change [ 'value' ] ) ) 
~~ def create_widget ( self ) : 
self . widget = SubElement ( self . parent_widget ( ) , self . declaration . tag ) 
~~ def init_widget ( self ) : 
widget = self . widget 
d = self . declaration 
ref = d . ref 
CACHE [ ref ] = atomref ( self ) 
widget . set ( 'ref' , ref ) 
if d . text : 
~~~ self . set_text ( d . text ) 
~~ if d . tail : 
~~~ self . set_tail ( d . tail ) 
~~ if d . style : 
~~~ self . set_style ( d . style ) 
~~ if d . cls : 
~~~ self . set_cls ( d . cls ) 
~~ if d . attrs : 
~~~ self . set_attrs ( d . attrs ) 
~~ if d . id : 
~~~ widget . set ( 'id' , d . id ) 
~~ if d . draggable : 
~~~ self . set_draggable ( d . draggable ) 
~~ for name , member in d . members ( ) . items ( ) : 
~~~ if not member . metadata : 
~~ meta = member . metadata 
if not ( meta . get ( 'd_member' ) and meta . get ( 'd_final' ) ) : 
~~ elif not meta . get ( 'attr' , True ) : 
~~ elif isinstance ( member , Event ) : 
~~ value = getattr ( d , name ) 
if value : 
~~~ self . set_attribute ( name , value ) 
~~ ~~ ~~ def destroy ( self ) : 
if widget is not None : 
~~~ parent = widget . getparent ( ) 
if parent is not None : 
~~~ parent . remove ( widget ) 
~~ del self . widget 
~~~ del CACHE [ d . ref ] 
~~ ~~ super ( WebComponent , self ) . destroy ( ) 
~~ def child_added ( self , child ) : 
super ( WebComponent , self ) . child_added ( child ) 
if child . widget is not None : 
~~~ for i , c in enumerate ( self . children ( ) ) : 
~~~ if c == child : 
~~~ self . widget . insert ( i , child . widget ) 
~~ ~~ ~~ ~~ def child_removed ( self , child ) : 
super ( WebComponent , self ) . child_removed ( child ) 
~~~ del self . widget [ i ] 
~~ ~~ ~~ ~~ def find ( self , query , ** kwargs ) : 
nodes = self . widget . xpath ( query , ** kwargs ) 
~~ matches = [ ] 
for node in nodes : 
~~~ aref = CACHE . get ( node . attrib . get ( 'ref' ) ) 
obj = aref ( ) if aref else None 
~~ matches . append ( obj ) 
~~ return matches 
~~ def child_widgets ( self ) : 
for child in self . children ( ) : 
~~~ w = child . widget 
if w is not None : 
~~~ yield w 
~~ ~~ ~~ def set_attribute ( self , name , value ) : 
if value is True : 
~~~ self . widget . set ( name , name ) 
~~ elif value is False : 
~~~ del self . widget . attrib [ name ] 
~~~ self . widget . set ( name , str ( value ) ) 
~~ ~~ def _update_proxy ( self , change ) : 
if change [ 'type' ] == 'update' and self . proxy_is_active : 
~~~ handler = getattr ( self . proxy , 'set_' + change [ 'name' ] , None ) 
if handler is not None : 
~~~ handler ( change [ 'value' ] ) 
~~~ self . proxy . set_attribute ( change [ 'name' ] , change [ 'value' ] ) 
~~ self . _notify_modified ( change ) 
~~ ~~ def _notify_modified ( self , change ) : 
root = self . root_object ( ) 
if isinstance ( root , Html ) : 
~~~ name = change [ 'name' ] 
change = { 
'ref' : self . ref , 
'type' : change [ 'type' ] , 
'name' : change [ 'name' ] , 
'value' : change [ 'value' ] 
root . modified ( change ) 
~~ ~~ def xpath ( self , query , ** kwargs ) : 
nodes = self . proxy . find ( query , ** kwargs ) 
return [ n . declaration for n in nodes ] 
~~ def prepare ( self , ** kwargs ) : 
~~~ setattr ( self , k , v ) 
~~ if not self . is_initialized : 
~~~ self . initialize ( ) 
~~ if not self . proxy_is_active : 
~~~ self . activate_proxy ( ) 
~~ ~~ def init_widget ( self ) : 
if d . source : 
~~~ self . set_source ( d . source ) 
~~~ super ( RawComponent , self ) . init_widget ( ) 
~~ ~~ def set_source ( self , source ) : 
self . widget . clear ( ) 
html = etree . HTML ( source ) 
self . widget . extend ( html [ 0 ] ) 
super ( RawComponent , self ) . init_widget ( ) 
~~ def _observe_mode ( self , change ) : 
block = self . block 
if block and self . is_initialized and change [ 'type' ] == 'update' : 
~~~ if change [ 'oldvalue' ] == 'replace' : 
~~ for c in self . children : 
~~~ block . children . remove ( c ) 
c . set_parent ( None ) 
~~ self . refresh_items ( ) 
~~ ~~ def _observe_block ( self , change ) : 
if self . is_initialized and change [ 'type' ] == 'update' : 
~~~ old_block = change [ 'oldvalue' ] 
for c in self . children : 
~~~ old_block . children . remove ( c ) 
~~ ~~ def _observe__children ( self , change ) : 
if not self . is_initialized or change [ 'type' ] != 'update' : 
~~ block = self . block 
new_children = change [ 'value' ] 
old_children = change [ 'oldvalue' ] 
for c in old_children : 
~~~ if c not in new_children and not c . is_destroyed : 
~~~ c . destroy ( ) 
~~~ c . set_parent ( None ) 
~~ ~~ if block : 
~~~ before = None 
if self . mode == 'replace' : 
~~~ block . children = [ ] 
~~ if self . mode == 'prepend' and block . children : 
~~~ before = block . children [ 0 ] 
~~ block . insert_children ( before , new_children ) 
~~~ self . parent . insert_children ( self , new_children ) 
~~ ~~ def add_aggregation_columns ( 
df , * , 
group_cols : Union [ str , List [ str ] ] , 
aggregations : Dict [ str , Agg ] 
group = df . groupby ( group_cols ) 
for new_col , aggs in aggregations . items ( ) : 
~~~ assert len ( aggs ) == 1 
( col , agg ) , * _ = aggs . items ( ) 
df [ new_col ] = group [ col ] . transform ( agg ) 
~~ def top ( 
df , 
value : str , 
limit : int , 
order : str = 'asc' , 
group : Union [ str , List [ str ] ] = None 
ascending = order != 'desc' 
limit = int ( limit ) 
filter_func = 'nlargest' if ( limit > 0 ) ^ ascending else 'nsmallest' 
def _top ( df ) : 
~~~ return getattr ( df , filter_func ) ( abs ( limit ) , value ) . sort_values ( by = value , 
ascending = ascending ) 
~~ if group is None : 
~~~ df = _top ( df ) 
~~~ df = df . groupby ( group ) . apply ( _top ) 
~~ def top_group ( 
aggregate_by : List [ str ] , 
function : str = 'sum' , 
aggregate_by = aggregate_by or [ ] 
group_top = group or [ ] 
df2 = df . groupby ( group_top + aggregate_by ) . agg ( function ) . reset_index ( ) 
df2 = top ( df2 , group = group , value = value , limit = limit , order = order ) . reset_index ( drop = True ) 
df2 = df2 [ group_top + aggregate_by ] 
df = df2 . merge ( df , on = group_top + aggregate_by ) 
~~ def convert_str_to_datetime ( df , * , column : str , format : str ) : 
df [ column ] = pd . to_datetime ( df [ column ] , format = format ) 
~~ def convert_datetime_to_str ( df , * , column : str , format : str , new_column : str = None ) : 
new_column = new_column or column 
df [ new_column ] = df [ column ] . dt . strftime ( format ) 
~~ def change_date_format ( 
column : str , 
output_format : str , 
input_format : str = None , 
new_column : str = None , 
new_time_zone = None 
df [ new_column ] = ( pd . to_datetime ( df [ column ] , format = input_format , utc = True ) 
. dt . tz_convert ( new_time_zone ) 
. dt . strftime ( output_format ) ) 
~~ def cast ( df , column : str , type : str , new_column = None ) : 
df [ new_column ] = df [ column ] . astype ( type ) 
~~ def compute_evolution_by_frequency ( 
id_cols : List [ str ] , 
date_col : Union [ str , Dict [ str , str ] ] , 
value_col : str , 
freq = 1 , 
method : str = 'abs' , 
format : str = 'column' , 
offseted_suffix : str = '_offseted' , 
evolution_col_name : str = 'evolution_computed' , 
missing_date_as_zero : bool = False , 
raise_duplicate_error : bool = True 
if missing_date_as_zero : 
~~~ how = 'outer' 
fillna = 0 
~~~ how = 'left' 
fillna = None 
~~ return __compute_evolution ( 
df = df , 
id_cols = id_cols , 
value_col = value_col , 
date_col = date_col , 
freq = freq , 
format = format , 
offseted_suffix = offseted_suffix , 
evolution_col_name = evolution_col_name , 
how = how , 
fillna = fillna , 
raise_duplicate_error = raise_duplicate_error 
~~ def compute_evolution_by_criteria ( 
compare_to : str , 
return __compute_evolution ( ** locals ( ) ) 
~~ def __compute_evolution ( 
id_cols , 
value_col , 
date_col = None , 
compare_to = None , 
method = 'abs' , 
format = 'column' , 
offseted_suffix = '_offseted' , 
evolution_col_name = 'evolution_computed' , 
how = 'left' , 
fillna = None , 
raise_duplicate_error = True 
if date_col is not None : 
~~~ is_date_to_format = isinstance ( date_col , dict ) or ( df [ date_col ] . dtype == np . object ) 
if is_date_to_format : 
~~~ if isinstance ( date_col , dict ) : 
~~~ date_format = date_col . get ( 'format' , None ) 
date_col = date_col [ 'selector' ] 
~~~ date_format = None 
~~ df [ '_' + date_col + '_copy_' ] = pd . to_datetime ( df [ date_col ] , format = date_format ) 
date_col = '_' + date_col + '_copy_' 
~~ is_freq_dict = isinstance ( freq , dict ) 
if is_freq_dict : 
~~~ freq = pd . DateOffset ( ** { k : int ( v ) for k , v in freq . items ( ) } ) 
~~ check_params_columns_duplicate ( id_cols + [ value_col , date_col ] ) 
group_cols = id_cols + [ date_col ] 
df_offseted = df [ group_cols + [ value_col ] ] . copy ( ) 
df_offseted [ date_col ] += freq 
df_with_offseted_values = apply_merge ( 
df , df_offseted , group_cols , how , offseted_suffix , 
raise_duplicate_error 
~~~ del df_with_offseted_values [ date_col ] 
~~ ~~ elif compare_to is not None : 
~~~ check_params_columns_duplicate ( id_cols + [ value_col ] ) 
group_cols = id_cols 
df_offseted = df . query ( compare_to ) . copy ( ) 
df_offseted = df_offseted [ group_cols + [ value_col ] ] 
~~ apply_fillna ( df_with_offseted_values , value_col , offseted_suffix , fillna ) 
apply_method ( df_with_offseted_values , evolution_col_name , value_col , offseted_suffix , method ) 
return apply_format ( df_with_offseted_values , evolution_col_name , format ) 
~~ def rank ( 
value_cols : Union [ str , List [ str ] ] , 
group_cols : List [ str ] = None , 
rank_cols_names : List [ str ] = None , 
method = 'min' , 
ascending : bool = True 
value_cols = [ value_cols ] if not isinstance ( value_cols , list ) else value_cols 
for col in value_cols : 
~~~ if not np . issubdtype ( df [ col ] . dtype , np . number ) : 
~~ ~~ if rank_cols_names is None : 
~~~ rank_cols_names = [ x + '_rank' for x in value_cols ] 
~~ if group_cols is None : 
~~~ df [ rank_cols_names ] = df [ value_cols ] . rank ( method = method , ascending = ascending ) 
~~~ df [ rank_cols_names ] = ( df . groupby ( group_cols ) [ value_cols ] 
. rank ( method = method , ascending = ascending ) ) 
~~ if method != 'average' : 
~~~ df [ rank_cols_names ] = df [ rank_cols_names ] . astype ( 'int' ) 
~~ def waterfall ( 
date : str , 
start : Dict [ str , str ] , 
end : Dict [ str , str ] , 
upperGroup : Dict [ str , str ] , 
insideGroup : Dict [ str , str ] = None , 
filters : List [ str ] = None 
if len ( df ) == 0 : 
~~~ return df 
~~ if filters is not None : 
~~~ if isinstance ( filters , str ) : 
~~~ filters = [ filters ] 
~~ def sub_waterfall ( df ) : 
~~~ wa_df = waterfall ( df , date , value , start , end , upperGroup , insideGroup ) 
for filters_col in filters : 
~~~ wa_df [ filters_col ] = df [ filters_col ] . values [ 0 ] 
~~ return wa_df 
~~ list_of_sub_df = [ df [ ( df [ filters ] . values == i ) . all ( axis = 1 ) ] 
for i in df [ filters ] . drop_duplicates ( ) . values ] 
return pd . concat ( [ sub_waterfall ( df ) for df in list_of_sub_df ] , sort = False ) 
~~ groups = { 
'upperGroup' : { 
'type' : 'parent' , 
'id' : 'upperGroup' , 
'order' : { 
'by' : [ 'upperGroup_order' , 'groups' ] , 
'ascending' : [ True , True ] 
'obj' : upperGroup 
if insideGroup is not None : 
~~~ groups [ 'insideGroup' ] = { 
'type' : 'child' , 
'id' : 'insideGroup' , 
'by' : [ 'type' , 'insideGroup_order' , 'label' ] , 
'ascending' : [ False , True , True ] 
'obj' : insideGroup 
~~ df = _compute_rename ( df , date , value , groups ) 
agg_conf = { 'value' : sum } 
agg_conf . update ( { f'{col}_label' : 'first' for col in groups . keys ( ) } ) 
agg_conf . update ( { f'{col}_order' : 'first' for col in groups . keys ( ) } ) 
df = df . groupby ( list ( groups . keys ( ) ) + [ 'date' ] ) . agg ( agg_conf ) . reset_index ( ) 
df_start , df_end = _compute_start_end ( df , start , end ) 
df = _compute_value_diff ( df , start , end , groups ) 
middle = _compute_upper_group ( df ) 
~~~ middle = pd . concat ( [ middle , _compute_inside_group ( df ) ] ) 
~~ ret = _compute_order ( df_start , df_end , middle , groups ) 
~~ def _compute_start_end ( df , start , end ) : 
time_dict = { 'start' : start , 'end' : end } 
totals = df . groupby ( 'date' ) . agg ( { 'value' : sum } ) . reset_index ( ) 
for time_name , time in time_dict . items ( ) : 
~~~ if not totals [ totals [ 'date' ] == time [ 'id' ] ] . empty : 
~~~ value = totals . loc [ 
totals [ 'date' ] == time [ 'id' ] , 'value' 
] . values [ 0 ] 
~~~ value = 0 
~~ result [ time_name ] = pd . DataFrame ( [ { 
'label' : time [ 'label' ] , 
'groups' : time [ 'label' ] 
} ] ) 
~~ return result [ 'start' ] , result [ 'end' ] 
~~ def _compute_value_diff ( df , start , end , groups ) : 
start_values = df [ df [ 'date' ] == start [ 'id' ] ] . copy ( ) 
end_values = df [ df [ 'date' ] == end [ 'id' ] ] . copy ( ) 
merge_on = [ ] 
for key , group in groups . items ( ) : 
~~~ merge_on = merge_on + [ key , f'{key}_label' , f'{key}_order' ] 
~~ df = start_values . merge ( end_values , 
on = merge_on , 
how = 'outer' , 
suffixes = ( '_start' , '_end' ) , ) 
df [ [ 'value_start' , 'value_end' ] ] = df [ [ 'value_start' , 'value_end' ] ] . fillna ( 0 ) 
df [ 'value' ] = df [ 'value_end' ] - df [ 'value_start' ] 
df . drop ( [ 'date_start' , 'date_end' , 'value_end' ] , axis = 1 , inplace = True ) 
df . rename ( columns = { 'upperGroup' : 'groups' } , inplace = True ) 
~~ def _compute_inside_group ( df ) : 
inside_group = df . copy ( ) 
inside_group [ 'type' ] = 'child' 
inside_group [ 'variation' ] = inside_group [ 'value' ] / inside_group [ 
'value_start' ] 
inside_group . drop ( [ 'upperGroup_label' , 'insideGroup' , 'value_start' ] , 
axis = 1 , inplace = True ) 
inside_group . rename ( columns = { 'insideGroup_label' : 'label' } , 
inplace = True ) 
return inside_group 
~~ def _compute_upper_group ( df ) : 
upper_group = df . groupby ( [ 'groups' ] ) . agg ( { 
'value' : sum , 
'value_start' : sum , 
'upperGroup_label' : 'first' , 
'upperGroup_order' : 'first' 
} ) . reset_index ( ) 
upper_group [ 'type' ] = 'parent' 
upper_group [ 'variation' ] = upper_group [ 'value' ] / upper_group [ 
upper_group . drop ( [ 'value_start' ] , axis = 1 , inplace = True ) 
upper_group . rename ( columns = { 'upperGroup_label' : 'label' } , inplace = True ) 
return upper_group 
~~ def _basic_math_operation ( df , new_column , column_1 , column_2 , op ) : 
if not isinstance ( column_1 , ( str , int , float ) ) : 
~~ if not isinstance ( column_2 , ( str , int , float ) ) : 
~~ if isinstance ( column_1 , str ) : 
~~~ column_1 = df [ column_1 ] 
~~ if isinstance ( column_2 , str ) : 
~~~ column_2 = df [ column_2 ] 
~~ operator = getattr ( _operator , op ) 
df [ new_column ] = operator ( column_1 , column_2 ) 
~~ def add ( df , new_column , column_1 , column_2 ) : 
return _basic_math_operation ( df , new_column , column_1 , column_2 , op = 'add' ) 
~~ def subtract ( df , new_column , column_1 , column_2 ) : 
return _basic_math_operation ( df , new_column , column_1 , column_2 , op = 'sub' ) 
~~ def multiply ( df , new_column , column_1 , column_2 ) : 
return _basic_math_operation ( df , new_column , column_1 , column_2 , op = 'mul' ) 
~~ def divide ( df , new_column , column_1 , column_2 ) : 
return _basic_math_operation ( df , new_column , column_1 , column_2 , op = 'truediv' ) 
~~ def formula ( df , * , new_column : str , formula : str ) : 
tokens = _parse_formula ( formula ) 
expression_splitted = [ ] 
~~~ if not t . quoted and ( t in MATH_CHARACTERS or is_float ( t ) ) : 
~~~ expression_splitted . append ( t ) 
~~ elif t in df . columns : 
~~~ expression_splitted . append ( f\ ) 
~~~ raise FormulaError ( f\ ) 
~~ ~~ expression = '' . join ( expression_splitted ) 
df [ new_column ] = eval ( expression ) 
~~ def round_values ( df , * , column : str , decimals : int , new_column : str = None ) : 
df [ new_column ] = df [ column ] . round ( decimals ) 
~~ def absolute_values ( df , * , column : str , new_column : str = None ) : 
df [ new_column ] = abs ( df [ column ] ) 
~~ def pivot ( df , index : List [ str ] , column : str , value : str , agg_function : str = 'mean' ) : 
if df . dtypes [ value ] . type == np . object_ : 
~~~ df = pd . pivot_table ( df , index = index , 
columns = column , 
values = value , 
aggfunc = agg_function ) 
~~ df = df . reset_index ( ) 
~~ def pivot_by_group ( 
variable , 
value , 
new_columns , 
groups , 
id_cols = None 
if id_cols is None : 
~~~ index = [ variable ] 
~~~ index = [ variable ] + id_cols 
~~ param = pd . DataFrame ( groups , index = new_columns ) 
temporary_colum = 'tmp' 
df [ temporary_colum ] = df [ variable ] 
for column in param . columns : 
~~~ df . loc [ df [ variable ] . isin ( param [ column ] ) , variable ] = column 
~~ param = param . T 
~~~ df . loc [ 
df [ temporary_colum ] . isin ( param [ column ] ) , temporary_colum ] = column 
~~ df = pivot ( df , index , temporary_colum , value ) 
~~ def groupby ( df , * , group_cols : Union [ str , List [ str ] ] , 
aggregations : Dict [ str , Union [ str , List [ str ] ] ] ) : 
df = df . groupby ( group_cols , as_index = False ) . agg ( aggregations ) 
if df . columns . nlevels == 2 : 
~~~ level_0 = df . columns . get_level_values ( 0 ) 
level_1 = df . columns . get_level_values ( 1 ) 
new_columns = [ ( f'{x}_{y}' if x else y ) for ( x , y ) 
in zip ( level_1 , level_0 ) ] 
df . columns = new_columns 
~~ def cumsum ( df , new_column : str , column : str , index : list , date_column : str , date_format : str ) : 
date_temp = '__date_temp__' 
if isinstance ( index , str ) : 
~~~ index = [ index ] 
~~ levels = list ( range ( 0 , len ( index ) ) ) 
df [ date_temp ] = pd . to_datetime ( df [ date_column ] , format = date_format ) 
reference_cols = [ date_temp , date_column ] 
df = df . groupby ( index + reference_cols ) . sum ( ) 
df [ new_column ] = df . groupby ( level = levels ) [ column ] . cumsum ( ) 
df . reset_index ( inplace = True ) 
del df [ date_temp ] 
~~ def add_missing_row ( 
df : pd . DataFrame , 
reference_col : str , 
complete_index : Union [ Dict [ str , str ] , List [ str ] ] = None , 
method : str = None , 
cols_to_keep : List [ str ] = None 
) -> pd . DataFrame : 
if cols_to_keep is None : 
~~~ cols_for_index = [ reference_col ] 
~~~ cols_for_index = [ reference_col ] + cols_to_keep 
~~ check_params_columns_duplicate ( id_cols + cols_for_index ) 
if method == 'between' or method == 'between_and_after' : 
~~~ df [ 'start' ] = df . groupby ( id_cols ) [ reference_col ] . transform ( min ) 
id_cols += [ 'start' ] 
~~ if method == 'between' or method == 'between_and_before' : 
~~~ df [ 'end' ] = df . groupby ( id_cols ) [ reference_col ] . transform ( max ) 
id_cols += [ 'end' ] 
~~ names = id_cols + cols_for_index 
new_df = df . set_index ( names ) 
index_values = df . groupby ( id_cols ) . sum ( ) . index . values 
if complete_index is None : 
~~~ complete_index = df . groupby ( cols_for_index ) . sum ( ) . index . values 
~~ elif isinstance ( complete_index , dict ) : 
~~~ if complete_index [ 'type' ] == 'date' : 
~~~ freq = complete_index [ 'freq' ] 
date_format = complete_index [ 'format' ] 
start = complete_index [ 'start' ] 
end = complete_index [ 'end' ] 
if isinstance ( freq , dict ) : 
~~ complete_index = pd . date_range ( start = start , end = end , freq = freq ) 
complete_index = complete_index . strftime ( date_format ) 
~~ ~~ if not isinstance ( index_values [ 0 ] , tuple ) : 
~~~ index_values = [ ( x , ) for x in index_values ] 
~~ if not isinstance ( complete_index [ 0 ] , tuple ) : 
~~~ complete_index = [ ( x , ) for x in complete_index ] 
~~ new_tuples_index = [ x + y for x in index_values for y in complete_index ] 
new_index = pd . MultiIndex . from_tuples ( new_tuples_index , names = names ) 
new_df = new_df . reindex ( new_index ) . reset_index ( ) 
~~~ new_df = new_df [ new_df [ reference_col ] >= new_df [ 'start' ] ] 
del new_df [ 'start' ] 
~~~ new_df = new_df [ new_df [ reference_col ] <= new_df [ 'end' ] ] 
del new_df [ 'end' ] 
~~ return new_df 
~~ def extract_zip ( zip_file_path ) : 
dfs = { } 
with zipfile . ZipFile ( zip_file_path , mode = 'r' ) as z_file : 
~~~ names = z_file . namelist ( ) 
~~~ content = z_file . read ( name ) 
_ , tmp_file_path = tempfile . mkstemp ( ) 
~~~ with open ( tmp_file_path , 'wb' ) as tmp_file : 
~~~ tmp_file . write ( content ) 
~~ dfs [ name ] = joblib . load ( tmp_file_path ) 
~~~ shutil . rmtree ( tmp_file_path , ignore_errors = True ) 
~~ ~~ ~~ return dfs 
~~ def extract ( data ) : 
~~~ tmp_file . write ( data ) 
~~ if zipfile . is_zipfile ( tmp_file_path ) : 
~~~ return extract_zip ( tmp_file_path ) 
~~ ~~ def read_from_cache ( self , domains = None ) : 
if domains is not None and isinstance ( domains , list ) : 
~~~ dfs = { domain : self . read_entry ( domain ) for domain in domains } 
~~~ dfs = { name : self . read_entry ( name ) 
for name in os . listdir ( self . EXTRACTION_CACHE_PATH ) } 
~~ return dfs 
~~ def read_entry ( self , file_name ) : 
file_path = os . path . join ( self . EXTRACTION_CACHE_PATH , file_name ) 
return joblib . load ( file_path ) 
~~ def write ( self , dfs ) : 
if not os . path . exists ( self . EXTRACTION_CACHE_PATH ) : 
~~~ os . makedirs ( self . EXTRACTION_CACHE_PATH ) 
~~ for name , df in dfs . items ( ) : 
~~~ file_path = os . path . join ( self . EXTRACTION_CACHE_PATH , name ) 
joblib . dump ( df , filename = file_path ) 
~~ ~~ def clean_dataframe ( df , is_slugify = True , threshold = 50 , rename_cols = None ) : 
if is_slugify : 
~~~ df = df . rename ( columns = slugify ) 
~~ df = df . dropna ( axis = 1 , how = 'all' ) 
for column in get_category_cols ( df , threshold = threshold ) : 
~~~ df [ column ] = df [ column ] . astype ( 'category' ) 
~~ for column in get_int_cols ( df ) : 
~~~ df [ column ] = df [ column ] . astype ( int ) 
~~ if rename_cols is not None : 
~~~ df = df . rename ( columns = rename_cols ) 
~~ def compute_ffill_by_group ( 
reference_cols : List [ str ] , 
value_col : str 
check_params_columns_duplicate ( id_cols + reference_cols + [ value_col ] ) 
df = df . sort_values ( by = id_cols + reference_cols ) 
df = df . set_index ( id_cols ) 
df [ 'fill' ] = 1 - df [ value_col ] . isnull ( ) . astype ( int ) 
df [ 'fill' ] = df . groupby ( 
level = list ( range ( 0 , len ( id_cols ) - 1 ) ) 
) [ 'fill' ] . cumsum ( ) 
df [ value_col ] = df [ value_col ] . ffill ( ) 
df . loc [ df [ 'fill' ] == 0 , value_col ] = None 
del df [ 'fill' ] 
return df . reset_index ( ) 
~~ def fake_data_generator ( conf : List [ dict ] ) -> pd . DataFrame : 
label_confs = [ x for x in conf if x [ 'type' ] == 'label' ] 
label_names = [ x [ 'name' ] for x in label_confs ] 
label_values = [ x [ 'values' ] for x in label_confs ] 
df = pd . DataFrame ( list ( product ( * label_values ) ) , columns = label_names ) 
number_confs = [ x for x in conf if x [ 'type' ] == 'number' ] 
for num_conf in number_confs : 
~~~ num_column = np . random . uniform ( low = num_conf [ 'min' ] , high = num_conf [ 'max' ] , size = df . shape [ 0 ] ) 
df [ num_conf [ 'name' ] ] = num_column . round ( num_conf . get ( 'digits' , 4 ) ) 
~~ def two_values_melt ( 
first_value_vars : List [ str ] , 
second_value_vars : List [ str ] , 
var_name : str , 
value_name : str 
value_name_first = value_name + '_first' 
value_name_second = value_name + '_second' 
melt_first_value = pd . melt ( df , 
id_vars = [ col for col in list ( df ) if 
col not in first_value_vars ] , 
value_vars = first_value_vars , 
var_name = var_name , 
value_name = value_name_first ) 
melt_first_value . drop ( second_value_vars , axis = 1 , inplace = True ) 
melt_second_value = pd . melt ( df , 
col not in second_value_vars ] , 
value_vars = second_value_vars , 
value_name = value_name_second ) 
normalize_types = { k : v for k , v in zip ( second_value_vars , first_value_vars ) } 
melt_second_value . replace ( normalize_types , inplace = True ) 
melt_second_value . drop ( first_value_vars , axis = 1 , inplace = True ) 
on_cols = list ( melt_first_value ) 
on_cols . remove ( value_name_first ) 
return pd . merge ( melt_first_value , melt_second_value , on = on_cols , how = 'outer' ) 
~~ def concat ( 
columns : List [ str ] , 
new_column : str , 
sep : str = None 
if len ( columns ) < 2 : 
~~ first_col , * other_cols = columns 
df . loc [ : , new_column ] = df [ first_col ] . astype ( str ) . str . cat ( df [ other_cols ] . astype ( str ) , sep = sep ) 
~~ def contains ( 
pat : str , 
case : bool = True , 
na : Any = None , 
regex : bool = True 
df . loc [ : , new_column ] = df [ column ] . str . contains ( pat , case = case , na = na , regex = regex ) 
~~ def repeat ( 
times : int , 
new_column : str = None 
df . loc [ : , new_column ] = df [ column ] . str . repeat ( times ) 
~~ def replace_pattern ( 
repl : str , 
df . loc [ : , new_column ] = df [ column ] . str . replace ( pat , repl , case = case , regex = regex ) 
~~ def catch ( logger ) : 
def decorator ( func ) : 
~~~ @ wraps ( func ) 
~~ def log_message ( logger , message = "" ) : 
~~~ _log_message ( logger , func . __name__ , message ) 
result = func ( * args , ** kwargs ) 
~~ def log_time ( logger ) : 
~~~ start = time . time ( ) 
_log_time ( logger , func . __name__ , start , end ) 
~~ def log_shapes ( logger ) : 
~~~ input_shapes = _get_dfs_shapes ( * args , ** kwargs ) 
output_shapes = _get_dfs_shapes ( result ) 
_log_shapes ( logger , func . __name__ , input_shapes , output_shapes ) 
~~ def log ( logger = None , start_message = 'Starting...' , end_message = 'Done...' ) : 
def actual_log ( f , real_logger = logger ) : 
~~~ logger = real_logger or _logger 
def timed ( * args , ** kwargs ) : 
res = f ( * args , ** kwargs ) 
~~ return timed 
~~ if callable ( logger ) : 
~~~ return actual_log ( logger , real_logger = None ) 
~~ return actual_log 
~~ def domain ( domain_name ) : 
~~~ dfs , * args = args 
if not isinstance ( dfs , dict ) : 
~~ df = dfs . pop ( domain_name ) 
df = func ( df , * args , ** kwargs ) 
return { domain_name : df , ** dfs } 
requires = None , 
disabled = False , 
applied_on_method = False , 
check_param = True , 
limit = None 
if not requires : 
~~~ requires = [ ] 
~~ elif isinstance ( requires , collections . Callable ) : 
~~~ requires = [ requires ] 
~~ if not isinstance ( check_param , ( bool , str ) ) : 
~~~ raise TypeError ( "\ ) 
~~ if limit is not None and not isinstance ( limit , int ) : 
~~ if not hasattr ( cache , 'funcs_references' ) : 
~~ if not hasattr ( cache , 'dependencies' ) : 
~~ if not hasattr ( cache , 'memories' ) : 
~~ def decorator ( func ) : 
cache . funcs_references [ func . __name__ ] = get_orig_function ( func ) 
dependencies_names = [ ] 
for requirement in requires : 
~~~ if isinstance ( requirement , collections . Callable ) : 
~~~ req_name = requirement . __name__ 
cache . funcs_references [ req_name ] = get_orig_function ( requirement ) 
~~ elif requirement not in cache . funcs_references : 
~~~ req_name = requirement 
cache . funcs_references [ req_name ] = None 
~~ dependencies_names . append ( req_name ) 
~~ cache . dependencies [ func . __name__ ] = dependencies_names 
current_memory = cache . memories . get ( current_thread ( ) . name ) 
if disabled is True or current_memory is None : 
~~ concatenated_source_code = '' 
dependencies = resolve_dependencies ( func . __name__ , cache . dependencies ) 
for func_name in dependencies : 
~~~ function = cache . funcs_references [ func_name ] 
if function is None : 
~~~ raise Exception ( f"Can\ ) 
~~ source_code = get_func_sourcecode ( function ) 
concatenated_source_code += source_code 
~~ md5_hash = md5 ( str . encode ( concatenated_source_code ) ) . hexdigest ( ) 
tmp_extra_kwargs = { 
'__func_dependencies_hash__' : md5_hash , 
'__original_func_name__' : func . __name__ , 
if check_param is True : 
~~~ kwargs . update ( tmp_extra_kwargs ) 
if applied_on_method : 
~~~ self_arg , args = args [ 0 ] , args [ 1 : ] 
~~ @ wraps ( func ) 
def f ( * args , ** kwargs ) : 
~~~ for k in tmp_extra_kwargs . keys ( ) : 
~~~ del kwargs [ k ] 
~~ if applied_on_method : 
~~~ args = ( self_arg , ) + args 
~~ return func ( * args , ** kwargs ) 
~~ f = current_memory . cache ( f ) 
result = f ( * args , ** kwargs ) 
~~~ if isinstance ( check_param , str ) : 
~~~ check_only_param_value = get_param_value_from_func_call ( 
param_name = check_param , 
func = func , 
call_args = args , 
call_kwargs = kwargs , 
tmp_extra_kwargs [ '__check_only__' ] = check_only_param_value 
def f ( ** tmp_extra_kwargs ) : 
result = f ( ** tmp_extra_kwargs ) 
~~~ clean_cachedir_old_entries ( f . store_backend , func . __name__ , limit ) 
~~ def setup_cachedir ( cachedir , mmap_mode = None , bytes_limit = None ) : 
if not hasattr ( cache , 'memories' ) : 
~~~ cache . memories = { } 
~~ memory = joblib . Memory ( 
location = cachedir , 
verbose = 0 , 
mmap_mode = mmap_mode , 
bytes_limit = bytes_limit , 
cache . memories [ current_thread ( ) . name ] = memory 
return memory 
~~ def melt ( 
id : List [ str ] , 
value : List [ str ] , 
dropna = False 
df = df [ ( id + value ) ] 
df = pd . melt ( df , id_vars = id , value_vars = value ) 
if dropna : 
~~~ df = df . dropna ( subset = [ 'value' ] ) 
~~ def rename ( 
values : Dict [ str , Dict [ str , str ] ] = None , 
columns : Dict [ str , Dict [ str , str ] ] = None , 
locale : str = None 
if values : 
~~~ to_replace = list ( values . keys ( ) ) 
value = [ values [ term ] [ locale ] for term in values ] 
df = df . replace ( to_replace = to_replace , value = value ) 
~~ if columns : 
~~~ _keys = list ( columns . keys ( ) ) 
_values = [ column [ locale ] for column in columns . values ( ) ] 
columns = dict ( list ( zip ( _keys , _values ) ) ) 
df = df . rename ( columns = columns ) 
~~ def compute_cumsum ( 
value_cols : List [ str ] , 
new_value_cols : List [ str ] = None , 
~~~ cols_to_keep = [ ] 
~~ if new_value_cols is None : 
~~~ new_value_cols = value_cols 
~~ if len ( value_cols ) != len ( new_value_cols ) : 
~~ check_params_columns_duplicate ( id_cols + reference_cols + cols_to_keep + value_cols ) 
levels = list ( range ( 0 , len ( id_cols ) ) ) 
df = df . groupby ( id_cols + reference_cols + cols_to_keep ) . sum ( ) 
df [ new_value_cols ] = df . groupby ( level = levels ) [ value_cols ] . cumsum ( ) 
~~ def combine_columns_aggregation ( 
cols_for_combination : Dict [ str , str ] , 
agg_func : Union [ str , List [ str ] , Dict [ str , str ] ] = 'sum' 
requesters_cols = list ( cols_for_combination . keys ( ) ) 
requester_combination = [ 
list ( item ) for i in range ( 0 , len ( requesters_cols ) + 1 ) 
for item in itertools . combinations ( requesters_cols , i ) ] 
dfs_result = [ ] 
for comb in requester_combination : 
~~~ df_tmp = df . groupby ( id_cols + comb ) . agg ( agg_func ) . reset_index ( ) 
for key in ( set ( cols_for_combination . keys ( ) ) - set ( comb ) ) : 
~~~ df_tmp [ key ] = cols_for_combination [ key ] 
~~ dfs_result . append ( df_tmp ) 
~~ return pd . concat ( dfs_result , sort = False , ignore_index = True ) 
~~ def get_param_value_from_func_call ( param_name , func , call_args , call_kwargs ) : 
signature = inspect . signature ( func ) 
params_list = signature . parameters . keys ( ) 
if param_name not in params_list : 
~~~ raise TypeError ( f"\ 
~~ call = signature . bind ( * call_args , ** call_kwargs ) 
call . apply_defaults ( ) 
return call . arguments [ param_name ] 
~~ def get_func_sourcecode ( func ) : 
def getsource ( func ) : 
~~~ lines , lnum = getsourcelines ( func ) 
return '' . join ( lines ) 
~~ def getsourcelines ( func ) : 
~~~ lines , lnum = findsource ( func ) 
return inspect . getblock ( lines [ lnum : ] ) , lnum + 1 
~~ def findsource ( func ) : 
module = inspect . getmodule ( func , file ) 
lines = linecache . getlines ( file , module . __dict__ ) 
code = func . __code__ 
lnum = code . co_firstlineno - 1 
pat = re . compile ( r'^(\\s*def\\s)|(\\s*async\\s+def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)' ) 
while lnum > 0 : 
~~~ if pat . match ( lines [ lnum ] ) : 
~~ return lines , lnum 
~~ def getfile ( func ) : 
~~~ module = inspect . getmodule ( func ) 
return module . __file__ 
~~~ return inspect . getsource ( func ) 
~~~ return getsource ( func ) 
~~ ~~ def slugify ( name , separator = '-' ) : 
return _slugify ( name , regex_pattern = re . compile ( '[^-_a-z0-9]+' ) , separator = separator ) 
~~ def resolve_dependencies ( func_name , dependencies ) : 
def _resolve_deps ( func_name , func_deps ) : 
if func_name in func_deps : 
~~ func_deps . append ( func_name ) 
for dep in dependencies . get ( func_name , [ ] ) : 
~~~ _resolve_deps ( dep , func_deps ) 
~~ ~~ func_deps = [ ] 
_resolve_deps ( func_name , func_deps ) 
return sorted ( func_deps ) 
~~ def clean_cachedir_old_entries ( cachedir : StoreBackendBase , func_name : str , limit : int ) -> int : 
if limit < 1 : 
~~ cache_entries = get_cachedir_entries ( cachedir , func_name ) 
cache_entries = sorted ( cache_entries , key = lambda e : e . last_access , reverse = True ) 
cache_entries_to_remove = cache_entries [ limit : ] 
for entry in cache_entries_to_remove : 
~~~ shutil . rmtree ( entry . path , ignore_errors = True ) 
~~ return len ( cache_entries_to_remove ) 
~~ def roll_up ( 
levels : List [ str ] , 
groupby_vars : List [ str ] , 
extra_groupby_cols : List [ str ] = None , 
var_name : str = 'type' , 
value_name : str = 'value' , 
agg_func : str = 'sum' , 
drop_levels : List [ str ] = None 
dfs = list ( ) 
groupby_cols_cpy = list ( levels ) 
levels_cpy = list ( levels ) 
levels_cpy . reverse ( ) 
extra_groupby_cols = extra_groupby_cols or [ ] 
drop_levels = drop_levels or [ ] 
previous_level = None 
for top_level in levels_cpy : 
~~~ gb_df = getattr ( 
df . groupby ( groupby_cols_cpy + extra_groupby_cols ) [ groupby_vars ] , 
agg_func ) ( ) . reset_index ( ) 
gb_df [ var_name ] = top_level 
gb_df [ value_name ] = gb_df [ top_level ] 
dfs . append ( gb_df ) 
if previous_level in drop_levels : 
~~~ del dfs [ - 2 ] 
~~ previous_level = top_level 
groupby_cols_cpy . pop ( ) 
~~ return pd . concat ( dfs , sort = False ) . reset_index ( ) 
~~ def argmax ( df , column : str , groups : Union [ str , List [ str ] ] = None ) : 
if groups is None : 
~~~ df = df [ df [ column ] == df [ column ] . max ( ) ] . reset_index ( drop = True ) 
~~~ group_max = df . groupby ( groups ) [ column ] . transform ( 'max' ) 
df = ( df 
. loc [ df [ column ] == group_max , : ] 
. drop_duplicates ( ) 
. reset_index ( drop = True ) 
~~ def argmin ( df , column : str , groups : Union [ str , List [ str ] ] = None ) : 
~~~ df = df [ df [ column ] == df [ column ] . min ( ) ] . reset_index ( drop = True ) 
~~~ group_min = df . groupby ( groups ) [ column ] . transform ( 'min' ) 
. loc [ df [ column ] == group_min , : ] 
~~ def fillna ( df , column : str , value = None , column_value = None ) : 
if column not in df . columns : 
~~~ df [ column ] = nan 
~~ if value is not None and column_value is not None : 
~~ if value is not None : 
~~~ df [ column ] = df [ column ] . fillna ( value ) 
~~ if column_value is not None : 
~~~ if column_value not in df . columns : 
~~~ raise ValueError ( f\ ) 
~~ df [ column ] = df [ column ] . fillna ( df [ column_value ] ) 
~~ def date_requester_generator ( 
date_column : str , 
frequency : str , 
date_column_format : str = None , 
format : str = '%Y-%m-%d' , 
granularities : Dict [ str , str ] = None , 
others_format : Dict [ str , str ] = None , 
times_delta : Dict [ str , str ] = None 
start_date = pd . to_datetime ( df [ date_column ] , format = date_column_format ) . min ( ) 
end_date = pd . to_datetime ( df [ date_column ] , format = date_column_format ) . max ( ) 
granularities = granularities or { 'date' : format } 
others_format = others_format or { } 
times_delta = times_delta or { } 
columns_list = [ 'DATE' , 'DATETIME' , 'GRANULARITY' , * others_format , * times_delta ] 
result_df = { col_name : [ ] for col_name in columns_list } 
date_range = pd . date_range ( start = start_date , end = end_date , freq = frequency ) 
for granularity_name , granularity_format in granularities . items ( ) : 
~~~ date_range_label = date_range . strftime ( granularity_format ) 
a = list ( set ( date_range_label ) ) 
index_unique = list ( set ( [ a . index ( x ) for x in date_range_label ] ) ) 
date_range_datetime = date_range [ index_unique ] 
date_range_label = date_range_label . unique ( ) 
result_df [ 'DATE' ] += list ( date_range_label ) 
result_df [ 'DATETIME' ] += list ( date_range_datetime ) 
result_df [ 'GRANULARITY' ] += [ granularity_name ] * len ( date_range_label ) 
for col_name , other_format in others_format . items ( ) : 
~~~ result_df [ col_name ] += list ( date_range_datetime . strftime ( other_format ) ) 
~~ for col_name , time_delta in times_delta . items ( ) : 
~~~ result_df [ col_name ] += list ( ( date_range_datetime + pd . Timedelta ( time_delta ) ) 
. strftime ( granularity_format ) ) 
~~ ~~ return pd . DataFrame ( result_df ) 
~~ def _norm_date ( datestr : str , date_fmt : str ) -> date : 
~~~ days = { 'TODAY' : 0 , 'YESTERDAY' : - 1 , 'TOMORROW' : 1 } [ datestr . upper ( ) ] 
return date . today ( ) + pd . Timedelta ( days = days ) 
~~~ return datetime . strptime ( datestr , date_fmt ) . date ( ) 
~~ ~~ def add_offset ( dateobj , hr_offset : str , sign : str ) : 
sign_coeff = 1 if sign == '+' else - 1 
~~~ return dateobj + sign_coeff * pd . Timedelta ( hr_offset ) 
~~~ match = TIMEDELTA_RGX . match ( hr_offset ) 
if match is not None : 
~~~ groups = match . groupdict ( ) 
unit = groups [ 'unit' ] . lower ( ) [ 0 ] 
num = sign_coeff * int ( groups [ 'num' ] ) 
if unit == 'w' : 
~~~ return dateobj + num * timedelta ( weeks = 1 ) 
~~ if unit == 'm' : 
~~~ return add_months ( dateobj , num ) 
~~ if unit == 'y' : 
~~~ return add_years ( dateobj , num ) 
~~ ~~ raise 
~~ ~~ def add_months ( dateobj , nb_months : int ) : 
nb_years , nb_months = divmod ( nb_months , 12 ) 
month = dateobj . month + nb_months 
if month > 12 : 
~~~ nb_years += 1 
month -= 12 
~~ year = dateobj . year + nb_years 
lastday = monthrange ( year , month ) [ 1 ] 
return dateobj . replace ( year = year , month = month , day = min ( lastday , dateobj . day ) ) 
~~ def add_years ( dateobj , nb_years ) : 
year = dateobj . year + nb_years 
lastday = monthrange ( year , dateobj . month ) [ 1 ] 
return dateobj . replace ( year = year , day = min ( lastday , dateobj . day ) ) 
~~ def parse_date ( datestr : str , date_fmt : str ) -> date : 
rgx = re . compile ( r'\\((?P<date>.*)\\)(\\s*(?P<sign>[+-])(?P<offset>.*))?$' ) 
datestr = datestr . strip ( ) 
match = rgx . match ( datestr ) 
~~~ return _norm_date ( datestr , date_fmt ) 
~~ datestr = match . group ( 'date' ) . strip ( ) 
dateobj = _norm_date ( datestr , date_fmt ) 
offset = match . group ( 'offset' ) 
~~~ return add_offset ( dateobj , offset , match . group ( 'sign' ) ) 
~~ return dateobj 
~~ def filter_by_date ( 
date_col : str , 
date_format : str = '%Y-%m-%d' , 
start : str = None , 
stop : str = None , 
atdate : str = None 
if start is None and stop is None and atdate is None : 
~~ if start is not None and atdate is not None : 
~~ if stop is not None and atdate is not None : 
~~ filtercol = str ( uuid4 ( ) ) 
df [ filtercol ] = pd . to_datetime ( df [ date_col ] , format = date_format ) 
if atdate is not None : 
~~~ mask = df [ filtercol ] == parse_date ( atdate , date_format ) 
~~ elif start is not None and stop is not None : 
~~~ mask = ( ( df [ filtercol ] >= parse_date ( start , date_format ) ) & 
( df [ filtercol ] < parse_date ( stop , date_format ) ) ) 
~~ elif stop is None : 
~~~ mask = df [ filtercol ] >= parse_date ( start , date_format ) 
~~ elif start is None : 
~~~ mask = df [ filtercol ] < parse_date ( stop , date_format ) 
~~ return df [ mask ] . drop ( filtercol , axis = 1 ) 
~~ def replace ( df , column : str , new_column : str = None , ** kwargs ) : 
df . loc [ : , new_column ] = df [ column ] . replace ( ** kwargs ) 
~~ def percentage ( 
group_cols : Union [ str , List [ str ] ] = None , 
if group_cols is None : 
~~~ df [ new_column ] = 100. * df [ column ] / sum ( df [ column ] ) 
~~~ df [ new_column ] = 100. * df [ column ] / df . groupby ( group_cols ) [ column ] . transform ( sum ) 
~~ def isclose ( a , b , * , rel_tol = 1e-09 , abs_tol = 0.0 ) : 
~~~ return math . isclose ( a , b , rel_tol = rel_tol , abs_tol = abs_tol ) 
~~~ if ( rel_tol < 0.0 ) or ( abs_tol < 0.0 ) : 
~~ if math . isnan ( a ) or math . isnan ( b ) : 
~~ if ( a == b ) : 
~~ if math . isinf ( a ) or math . isinf ( b ) : 
~~ diff = abs ( a - b ) 
return ( diff <= rel_tol * abs ( b ) ) or ( diff <= rel_tol * abs ( a ) ) or ( diff <= abs_tol ) 
~~ ~~ def deprecated ( func ) : 
def new_func ( * args , ** kwargs ) : 
~~ return new_func 
~~ def deserialize ( bstr ) : 
d = pickle . loads ( bstr ) 
seg = pickle . loads ( d [ 'seg' ] ) 
return AudioSegment ( seg , d [ 'name' ] ) 
~~ def from_file ( path ) : 
_name , ext = os . path . splitext ( path ) 
ext = ext . lower ( ) [ 1 : ] 
seg = pydub . AudioSegment . from_file ( path , ext ) 
return AudioSegment ( seg , path ) 
~~ def from_numpy_array ( nparr , framerate ) : 
if nparr . dtype . itemsize not in ( 1 , 2 , 4 ) : 
~~ if len ( nparr . shape ) == 1 : 
~~~ arrays = [ nparr ] 
~~ elif len ( nparr . shape ) == 2 : 
~~~ arrays = [ nparr [ i , : ] for i in range ( nparr . shape [ 0 ] ) ] 
~~ interleaved = np . vstack ( arrays ) . reshape ( ( - 1 , ) , order = 'F' ) 
dubseg = pydub . AudioSegment ( interleaved . tobytes ( ) , 
frame_rate = framerate , 
sample_width = interleaved . dtype . itemsize , 
channels = len ( interleaved . shape ) 
return AudioSegment ( dubseg , "" ) 
~~ def silent ( duration = 1000 , frame_rate = 11025 ) : 
seg = pydub . AudioSegment . silent ( duration = duration , frame_rate = frame_rate ) 
return AudioSegment ( seg , "" ) 
~~ def spl ( self ) : 
arr = self . to_numpy_array ( ) 
if len ( arr ) == 0 : 
~~~ rms = self . rms 
ratio = rms / P_REF_PCM 
return 20.0 * np . log10 ( ratio + 1E-9 ) 
~~ ~~ def filter_bank ( self , lower_bound_hz = 50 , upper_bound_hz = 8E3 , nfilters = 128 , mode = 'mel' ) : 
data = self . to_numpy_array ( ) 
if mode . lower ( ) == 'mel' : 
~~~ frequencies = librosa . core . mel_frequencies ( n_mels = nfilters , fmin = lower_bound_hz , fmax = upper_bound_hz ) 
~~ elif mode . lower ( ) == 'linear' : 
~~~ frequencies = np . linspace ( lower_bound_hz , upper_bound_hz , num = nfilters , endpoint = True ) 
~~ elif mode . lower ( ) == 'log' : 
~~~ start = np . log10 ( lower_bound_hz ) 
stop = np . log10 ( upper_bound_hz ) 
frequencies = np . logspace ( start , stop , num = nfilters , endpoint = True , base = 10.0 ) 
~~~ raise ValueError ( "\ . format ( mode ) ) 
~~ rows = [ filters . bandpass_filter ( data , freq * 0.8 , freq * 1.2 , self . frame_rate ) for freq in frequencies ] 
rows = np . array ( rows ) 
spect = np . vstack ( rows ) 
return spect , frequencies 
~~ def auditory_scene_analysis ( self , debug = False , debugplot = False ) : 
normalized = self . normalize_spl_by_average ( db = 60 ) 
def printd ( * args , ** kwargs ) : 
~~~ if debug : 
~~~ print ( * args , ** kwargs ) 
spect [ spect < 0 ] = 0 
low_boundary = 30 
order = 6 
spect = np . apply_along_axis ( filters . lowpass_filter , 1 , spect , low_boundary , self . frame_rate , order ) 
printd ( "Downsampling" ) 
downsample_freq_hz = 400 
if self . frame_rate > downsample_freq_hz : 
~~~ step = int ( round ( self . frame_rate / downsample_freq_hz ) ) 
spect = spect [ : , : : step ] 
~~ scales = [ ( 6 , 1 / 4 ) , ( 6 , 1 / 14 ) , ( 1 / 2 , 1 / 14 ) ] 
gaussian = lambda x , mu , sig : np . exp ( - np . power ( x - mu , 2.0 ) / ( 2 * np . power ( sig , 2.0 ) ) ) 
gaussian_kernel = lambda sig : gaussian ( np . linspace ( - 10 , 10 , len ( frequencies ) / 2 ) , 0 , sig ) 
spectrograms = [ ] 
for sc , st in scales : 
time_smoothed = np . apply_along_axis ( filters . lowpass_filter , 1 , spect , 1 / st , downsample_freq_hz , 6 ) 
freq_smoothed = np . apply_along_axis ( np . convolve , 0 , time_smoothed , gaussian_kernel ( sc ) , 'same' ) 
freq_smoothed [ freq_smoothed > 1E3 ] = 1E3 
freq_smoothed [ freq_smoothed < - 1E3 ] = - 1E3 
~~ spectrograms . append ( freq_smoothed ) 
~~ segmasks = [ ] 
for spect , ( sc , st ) in zip ( spectrograms , scales ) : 
onsets , gradients = asa . _compute_peaks_or_valleys_of_first_derivative ( spect ) 
offsets , _ = asa . _compute_peaks_or_valleys_of_first_derivative ( spect , do_peaks = False ) 
offsets = asa . _correlate_onsets_and_offsets ( onsets , offsets , gradients ) 
onset_fronts = asa . _form_onset_offset_fronts ( onsets , sample_rate_hz = downsample_freq_hz , threshold_ms = 20 ) 
offset_fronts = asa . _form_onset_offset_fronts ( offsets , sample_rate_hz = downsample_freq_hz , threshold_ms = 20 ) 
asa . _break_poorly_matched_fronts ( onset_fronts ) 
segmentation_mask = asa . _match_fronts ( onset_fronts , offset_fronts , onsets , offsets , debug = debug ) 
segmasks . append ( segmentation_mask ) 
~~ finished_segmentation_mask = segmasks [ 0 ] 
if debugplot : 
~~~ asa . visualize_segmentation_mask ( finished_segmentation_mask , spect , frequencies ) 
~~ times = np . arange ( 2 * downsample_freq_hz * len ( self ) / MS_PER_S ) 
nsamples_for_each_fft = 2 * finished_segmentation_mask . shape [ 0 ] 
stft_frequencies , stft_times , stft = signal . stft ( self . to_numpy_array ( ) , self . frame_rate , nperseg = nsamples_for_each_fft ) 
printd ( "Frequencies:" , stft_frequencies . shape ) 
printd ( "Times:" , stft_times . shape ) 
if stft_frequencies . shape [ 0 ] > finished_segmentation_mask . shape [ 0 ] : 
~~~ stft_frequencies = stft_frequencies [ : finished_segmentation_mask . shape [ 0 ] ] 
stft = stft [ : stft_frequencies . shape [ 0 ] , : ] 
~~ finished_segmentation_mask , times , stft , stft_times = asa . _downsample_one_or_the_other ( stft , stft_times , finished_segmentation_mask , times ) 
finished_segmentation_mask = asa . _map_segmentation_mask_to_stft_domain ( finished_segmentation_mask , times , frequencies , stft_times , stft_frequencies ) 
masks = asa . _separate_masks ( finished_segmentation_mask ) 
if len ( masks ) == 0 : 
~~~ clone = from_numpy_array ( self . to_numpy_array ( ) , self . frame_rate ) 
return [ clone ] 
~~~ ncpus = multiprocessing . cpu_count ( ) 
~~~ ncpus = 2 
~~ ncpus = len ( masks ) if len ( masks ) < ncpus else ncpus 
chunks = np . array_split ( masks , ncpus ) 
assert len ( chunks ) == ncpus 
for i in range ( ncpus ) : 
~~~ p = multiprocessing . Process ( target = asa . _asa_task , 
args = ( queue , chunks [ i ] , stft , self . sample_width , self . frame_rate , nsamples_for_each_fft ) , 
p . start ( ) 
dones = [ ] 
while len ( dones ) < ncpus : 
~~~ item = queue . get ( ) 
if type ( item ) == str and item == "DONE" : 
~~~ dones . append ( item ) 
~~~ wav = from_numpy_array ( item , self . frame_rate ) 
results . append ( wav ) 
~~ def detect_voice ( self , prob_detect_voice = 0.5 ) : 
class model_class : 
~~~ def __init__ ( self , aggressiveness ) : 
~~~ self . v = webrtcvad . Vad ( int ( aggressiveness ) ) 
~~ def predict ( self , vector ) : 
~~~ if self . v . is_speech ( vector . raw_data , vector . frame_rate ) : 
~~ ~~ ~~ model = model_class ( aggressiveness = 2 ) 
p_yes_raw = prob_detect_voice 
filtered = self . detect_event ( model = model , 
ms_per_input = 20 , 
transition_matrix = ( pyesno , pnoyes ) , 
model_stats = ( p_realyes_outputyes , p_realyes_outputno ) , 
event_length_s = 0.25 , 
prob_raw_yes = p_yes_raw ) 
for tup in filtered : 
~~~ t = ( 'v' , tup [ 1 ] ) if tup [ 0 ] == 'y' else ( 'u' , tup [ 1 ] ) 
ret . append ( t ) 
~~ def dice ( self , seconds , zero_pad = False ) : 
~~~ total_s = sum ( seconds ) 
if not ( self . duration_seconds <= total_s + 1 and self . duration_seconds >= total_s - 1 ) : 
~~ starts = [ ] 
stops = [ ] 
time_ms = 0 
for dur in seconds : 
~~~ starts . append ( time_ms ) 
time_ms += dur * MS_PER_S 
stops . append ( time_ms ) 
~~ zero_pad = False 
~~~ starts = range ( 0 , int ( round ( self . duration_seconds * MS_PER_S ) ) , int ( round ( seconds * MS_PER_S ) ) ) 
stops = ( min ( self . duration_seconds * MS_PER_S , start + seconds * MS_PER_S ) for start in starts ) 
~~ outs = [ self [ start : stop ] for start , stop in zip ( starts , stops ) ] 
out_lens = [ out . duration_seconds for out in outs ] 
if zero_pad and not ( out_lens [ - 1 ] <= seconds * MS_PER_S + 1 and out_lens [ - 1 ] >= seconds * MS_PER_S - 1 ) : 
~~~ num_zeros = self . frame_rate * ( seconds * MS_PER_S - out_lens [ - 1 ] ) 
outs [ - 1 ] = outs [ - 1 ] . zero_extend ( num_samples = num_zeros ) 
~~ def detect_event ( self , model , ms_per_input , transition_matrix , model_stats , event_length_s , 
start_as_yes = False , prob_raw_yes = 0.5 ) : 
if ms_per_input < 0 or ms_per_input / MS_PER_S > self . duration_seconds : 
~~ elif not hasattr ( transition_matrix , "__len__" ) or len ( transition_matrix ) != 2 : 
~~ elif not hasattr ( model_stats , "__len__" ) or len ( model_stats ) != 2 : 
~~ elif any ( [ True for prob in transition_matrix if prob > 1.0 or prob < 0.0 ] ) : 
~~ elif any ( [ True for prob in model_stats if prob > 1.0 or prob < 0.0 ] ) : 
~~ elif prob_raw_yes > 1.0 or prob_raw_yes < 0.0 : 
~~ filter_indices = [ yes_or_no for yes_or_no in detect . _get_filter_indices ( self , 
start_as_yes , 
prob_raw_yes , 
ms_per_input , 
model , 
transition_matrix , 
model_stats ) ] 
ret = detect . _homogeneity_filter ( filter_indices , window_size = int ( round ( 0.25 * MS_PER_S / ms_per_input ) ) ) 
ret = detect . _group_filter_values ( self , ret , ms_per_input ) 
real_ret = [ ] 
for i , ( this_yesno , next_timestamp ) in enumerate ( ret ) : 
~~~ _next_yesno , timestamp = ret [ i - 1 ] 
~~~ timestamp = 0 
~~ ms_per_s = 1000 
data = self [ timestamp * ms_per_s : next_timestamp * ms_per_s ] . raw_data 
seg = AudioSegment ( pydub . AudioSegment ( data = data , sample_width = self . sample_width , 
frame_rate = self . frame_rate , channels = self . channels ) , self . name ) 
real_ret . append ( ( this_yesno , seg ) ) 
~~ return real_ret 
~~ def _execute_sox_cmd ( self , cmd , console_output = False ) : 
on_windows = platform . system ( ) . lower ( ) == "windows" 
def _get_random_tmp_file ( ) : 
~~~ if on_windows : 
~~~ rand_string = "" . join ( random . choice ( string . ascii_uppercase + string . digits ) for _ in range ( 8 ) ) 
tmp = self . name + "_" + rand_string 
WinTempFile = collections . namedtuple ( "WinTempFile" , "name" ) 
tmp = WinTempFile ( tmp ) 
~~~ tmp = tempfile . NamedTemporaryFile ( ) 
~~ return tmp 
~~ tmp = _get_random_tmp_file ( ) 
othertmp = _get_random_tmp_file ( ) 
self . export ( tmp . name , format = "WAV" ) 
stdout = stderr = subprocess . PIPE if console_output else subprocess . DEVNULL 
command = cmd . format ( inputfile = tmp . name , outputfile = othertmp . name ) 
other = AudioSegment ( pydub . AudioSegment . from_wav ( othertmp . name ) , self . name ) 
if on_windows : 
~~~ os . remove ( tmp . name ) 
os . remove ( othertmp . name ) 
~~~ tmp . close ( ) 
othertmp . close ( ) 
~~ return other 
~~ def filter_silence ( self , duration_s = 1 , threshold_percentage = 1 , console_output = False ) : 
~~~ result = self . _execute_sox_cmd ( command ) 
~~ except pydub . exceptions . CouldntDecodeError : 
result = AudioSegment ( self . seg , self . name ) 
~~ def fft ( self , start_s = None , duration_s = None , start_sample = None , num_samples = None , zero_pad = False ) : 
if start_s is not None and start_sample is not None : 
~~ if duration_s is not None and num_samples is not None : 
~~ if start_s is None and start_sample is None : 
~~~ start_sample = 0 
~~ if duration_s is None and num_samples is None : 
~~~ num_samples = len ( self . get_array_of_samples ( ) ) - int ( start_sample ) 
~~ if duration_s is not None : 
~~~ num_samples = int ( round ( duration_s * self . frame_rate ) ) 
~~ if start_s is not None : 
~~~ start_sample = int ( round ( start_s * self . frame_rate ) ) 
if end_sample > len ( self . get_array_of_samples ( ) ) and not zero_pad : 
~~ elif end_sample > len ( self . get_array_of_samples ( ) ) and zero_pad : 
~~~ arr = np . array ( self . get_array_of_samples ( ) ) 
zeros = np . zeros ( end_sample - len ( arr ) ) 
arr = np . append ( arr , zeros ) 
~~ audioslice = np . array ( arr [ start_sample : end_sample ] ) 
fft_result = np . fft . fft ( audioslice ) [ range ( int ( round ( num_samples / 2 ) ) + 1 ) ] 
step_size = self . frame_rate / num_samples 
bins = np . arange ( 0 , int ( round ( num_samples / 2 ) ) + 1 , 1.0 ) * step_size 
return bins , fft_result 
~~ def generate_frames ( self , frame_duration_ms , zero_pad = True ) : 
bytes_per_frame = int ( self . frame_rate * ( frame_duration_ms / 1000 ) * self . sample_width ) 
frame_duration_s = ( bytes_per_frame / self . frame_rate ) / self . sample_width 
while offset + bytes_per_frame < len ( self . raw_data ) : 
~~~ yield Frame ( self . raw_data [ offset : offset + bytes_per_frame ] , timestamp , frame_duration_s ) 
timestamp += frame_duration_s 
offset += bytes_per_frame 
~~ if zero_pad : 
~~~ rest = self . raw_data [ offset : ] 
zeros = bytes ( bytes_per_frame - len ( rest ) ) 
yield Frame ( rest + zeros , timestamp , frame_duration_s ) 
~~ ~~ def generate_frames_as_segments ( self , frame_duration_ms , zero_pad = True ) : 
for frame in self . generate_frames ( frame_duration_ms , zero_pad = zero_pad ) : 
~~~ seg = AudioSegment ( pydub . AudioSegment ( data = frame . bytes , sample_width = self . sample_width , 
yield seg , frame . timestamp 
~~ ~~ def human_audible ( self ) : 
hist_bins , hist_vals = self . fft ( ) 
hist_vals_real_normed = np . abs ( hist_vals ) / len ( hist_vals ) 
f_characteristic = hist_bins [ np . argmax ( hist_vals_real_normed ) ] 
threshold_fc = 40.11453 - ( 0.01683697 * f_characteristic ) + ( 1.406211e-6 * f_characteristic ** 2 ) - ( 2.371512e-11 * f_characteristic ** 3 ) 
return self . spl >= threshold_fc 
~~ def normalize_spl_by_average ( self , db ) : 
arr = self . to_numpy_array ( ) . copy ( ) 
~~ def rms ( x ) : 
~~~ return np . sqrt ( np . mean ( np . square ( x ) ) ) 
~~ desired_rms = P_REF_PCM * ( ( 10 ** ( db / 20.0 ) ) - 1E-9 ) 
max_ntries = 50 
res_rms = 0.0 
ntries = 0 
factor = 0.1 
left = 0.0 
right = desired_rms 
while ( ntries < max_ntries ) and not util . isclose ( res_rms , desired_rms , abs_tol = 0.1 ) : 
~~~ res_rms = rms ( arr * factor ) 
if res_rms < desired_rms : 
~~~ left = factor 
~~~ right = factor 
~~ factor = 0.5 * ( left + right ) 
ntries += 1 
~~ dtype_dict = { 1 : np . int8 , 2 : np . int16 , 4 : np . int32 } 
dtype = dtype_dict [ self . sample_width ] 
new_seg = from_numpy_array ( np . array ( arr * factor , dtype = dtype ) , self . frame_rate ) 
return new_seg 
~~ def reduce ( self , others ) : 
ret = AudioSegment ( self . seg , self . name ) 
selfdata = [ self . seg . _data ] 
otherdata = [ o . seg . _data for o in others ] 
ret . seg . _data = b'' . join ( selfdata + otherdata ) 
~~ def resample ( self , sample_rate_Hz = None , sample_width = None , channels = None , console_output = False ) : 
if sample_rate_Hz is None : 
~~~ sample_rate_Hz = self . frame_rate 
~~ if sample_width is None : 
~~~ sample_width = self . sample_width 
~~ if channels is None : 
~~~ channels = self . channels 
return self . _execute_sox_cmd ( command , console_output = console_output ) 
~~ def serialize ( self ) : 
d = self . __getstate__ ( ) 
return pickle . dumps ( { 
'name' : d [ 'name' ] , 
'seg' : pickle . dumps ( d [ 'seg' ] , protocol = - 1 ) , 
} , protocol = - 1 ) 
~~ def spectrogram ( self , start_s = None , duration_s = None , start_sample = None , num_samples = None , 
window_length_s = None , window_length_samples = None , overlap = 0.5 , window = ( 'tukey' , 0.25 ) ) : 
~~ if window_length_s is not None and window_length_samples is not None : 
~~ if window_length_s is None and window_length_samples is None : 
~~ elif start_s is not None : 
~~ elif duration_s is not None : 
~~ if window_length_s is not None : 
~~~ window_length_samples = int ( round ( window_length_s * self . frame_rate ) ) 
~~ if start_sample + num_samples > len ( self . get_array_of_samples ( ) ) : 
~~ arr = self . to_numpy_array ( ) [ start_sample : start_sample + num_samples ] 
fs , ts , sxx = signal . spectrogram ( arr , self . frame_rate , scaling = 'spectrum' , nperseg = window_length_samples , 
noverlap = int ( round ( overlap * window_length_samples ) ) , 
mode = 'magnitude' , window = window ) 
return fs , ts , sxx 
~~ def to_numpy_array ( self ) : 
dtype_dict = { 
1 : np . int8 , 
2 : np . int16 , 
4 : np . int32 
return np . array ( self . get_array_of_samples ( ) , dtype = dtype ) 
~~ def zero_extend ( self , duration_s = None , num_samples = None ) : 
if duration_s is not None and num_samples is not None : 
~~~ num_samples = self . frame_rate * duration_s 
~~ seg = AudioSegment ( self . seg , self . name ) 
zeros = silent ( duration = num_samples / self . frame_rate , frame_rate = self . frame_rate ) 
return zeros . overlay ( seg ) 
~~ def _compute_peaks_or_valleys_of_first_derivative ( s , do_peaks = True ) : 
gradient = np . nan_to_num ( np . apply_along_axis ( np . gradient , 1 , s ) , copy = False ) 
threshold = np . squeeze ( np . nanmean ( gradient , axis = 1 ) + np . nanstd ( gradient , axis = 1 ) ) 
half_window = 4 
if do_peaks : 
~~~ indexes = [ signal . argrelextrema ( gradient [ i , : ] , np . greater , order = half_window ) [ 0 ] for i in range ( gradient . shape [ 0 ] ) ] 
~~~ indexes = [ signal . argrelextrema ( gradient [ i , : ] , np . less , order = half_window ) [ 0 ] for i in range ( gradient . shape [ 0 ] ) ] 
~~ extrema = np . zeros ( s . shape ) 
for row_index , index_array in enumerate ( indexes ) : 
~~~ for col_index in index_array : 
~~~ if do_peaks and ( gradient [ row_index , col_index ] > threshold [ row_index ] ) : 
~~~ extrema [ row_index , col_index ] = 1 
~~ elif not do_peaks : 
~~ ~~ ~~ return extrema , gradient 
~~ def _correlate_onsets_and_offsets ( onsets , offsets , gradients ) : 
for freq_index , ( ons , offs ) in enumerate ( zip ( onsets [ : , : ] , offsets [ : , : ] ) ) : 
~~~ indexes_of_all_ones = np . reshape ( np . where ( ons == 1 ) , ( - 1 , ) ) 
last_idx = indexes_of_all_ones [ 0 ] 
offs [ 0 : last_idx + 1 ] = 0 
if len ( indexes_of_all_ones > 1 ) : 
~~~ for next_idx in indexes_of_all_ones [ 1 : ] : 
~~~ offset_choices = offs [ last_idx : next_idx ] 
offset_choice_indexes = np . where ( offset_choices == 1 ) 
if not np . any ( offset_choices ) : 
offset_choice_indexes = np . reshape ( last_idx + offset_choice_indexes , ( - 1 , ) ) 
assert np . all ( offsets [ freq_index , offset_choice_indexes ] ) 
gradient_values = gradients [ freq_index , offset_choice_indexes ] 
index_of_largest_from_gradient_values = np . where ( gradient_values == np . min ( gradient_values ) ) [ 0 ] 
index_of_largest_offset_choice = offset_choice_indexes [ index_of_largest_from_gradient_values ] 
assert offsets [ freq_index , index_of_largest_offset_choice ] == 1 
offsets [ freq_index , offset_choice_indexes ] = 0 
offsets [ freq_index , index_of_largest_offset_choice ] = 1 
last_idx = next_idx 
~~~ offsets [ freq_index , : ] = 0 
offsets [ freq_index , - 1 ] = 1 
~~ ~~ return offsets 
~~ def _form_onset_offset_fronts ( ons_or_offs , sample_rate_hz , threshold_ms = 20 ) : 
threshold_s = threshold_ms / 1000 
threshold_samples = sample_rate_hz * threshold_s 
ons_or_offs = np . copy ( ons_or_offs ) 
claimed = [ ] 
this_id = 2 
for frequency_index , row in enumerate ( ons_or_offs [ : , : ] ) : 
~~~ ones = np . reshape ( np . where ( row == 1 ) , ( - 1 , ) ) 
for top_level_frequency_one_index in ones : 
~~~ claimed . append ( ( frequency_index , top_level_frequency_one_index ) ) 
found_a_front = False 
for other_frequency_index , other_row in enumerate ( ons_or_offs [ frequency_index + 1 : , : ] , start = frequency_index + 1 ) : 
~~~ upper_limit_index = top_level_frequency_one_index + threshold_samples 
lower_limit_index = top_level_frequency_one_index - threshold_samples 
& ( other_ones <= upper_limit_index ) ) , ( - 1 , ) ) 
if len ( other_ones ) > 0 : 
claimed . append ( ( other_frequency_index , unclaimed_idx ) ) 
~~ elif len ( claimed ) < 3 : 
~~~ ons_or_offs [ frequency_index , top_level_frequency_one_index ] = 0 
~~ elif len ( claimed ) >= 3 : 
~~~ found_a_front = True 
claimed_as_indexes = tuple ( np . array ( claimed ) . T ) 
ons_or_offs [ claimed_as_indexes ] = this_id 
this_id += 1 
~~ ~~ if len ( claimed ) >= 3 : 
~~~ claimed_as_indexes = tuple ( np . array ( claimed ) . T ) 
~~ elif found_a_front : 
~~~ this_id += 1 
~~ ~~ ~~ return ons_or_offs 
~~ def _lookup_offset_by_onset_idx ( onset_idx , onsets , offsets ) : 
frequency_idx , sample_idx = onset_idx 
offset_sample_idxs = np . reshape ( np . where ( offsets [ frequency_idx , : ] == 1 ) , ( - 1 , ) ) 
offset_sample_idxs = offset_sample_idxs [ offset_sample_idxs > sample_idx ] 
if len ( offset_sample_idxs ) == 0 : 
~~~ chosen_offset_sample_idx = offsets . shape [ 1 ] - 1 
assert offsets [ frequency_idx , chosen_offset_sample_idx ] == 0 
~~~ chosen_offset_sample_idx = offset_sample_idxs [ 0 ] 
assert offsets [ frequency_idx , chosen_offset_sample_idx ] != 0 
~~ return frequency_idx , chosen_offset_sample_idx 
~~ def _get_front_idxs_from_id ( fronts , id ) : 
if id == - 1 : 
~~~ freq_idxs = np . arange ( fronts . shape [ 0 ] , dtype = np . int64 ) 
sample_idxs = np . ones ( len ( freq_idxs ) , dtype = np . int64 ) * ( fronts . shape [ 1 ] - 1 ) 
~~~ freq_idxs , sample_idxs = np . where ( fronts == id ) 
~~ return [ ( f , i ) for f , i in zip ( freq_idxs , sample_idxs ) ] 
~~ def _choose_front_id_from_candidates ( candidate_offset_front_ids , offset_fronts , offsets_corresponding_to_onsets ) : 
for offset_front_id in candidate_offset_front_ids : 
~~~ offset_front_f_idxs , offset_front_s_idxs = np . where ( offset_fronts == offset_front_id ) 
offset_front_idxs = [ ( f , i ) for f , i in zip ( offset_front_f_idxs , offset_front_s_idxs ) ] 
noverlap_this_id = len ( set ( offset_front_idxs ) . symmetric_difference ( set ( offsets_corresponding_to_onsets ) ) ) 
noverlaps . append ( ( noverlap_this_id , offset_front_id ) ) 
~~ _overlapped , chosen_offset_front_id = max ( noverlaps , key = lambda t : t [ 0 ] ) 
return int ( chosen_offset_front_id ) 
~~ def _get_offset_front_id_after_onset_sample_idx ( onset_sample_idx , offset_fronts ) : 
offset_front_ids = [ i for i in np . unique ( offset_fronts ) if i != 0 ] 
best_id_so_far = - 1 
closest_offset_sample_idx = sys . maxsize 
for offset_front_id in offset_front_ids : 
~~~ offset_front_idxs = _get_front_idxs_from_id ( offset_fronts , offset_front_id ) 
offset_front_sample_idxs = [ s for _f , s in offset_front_idxs ] 
min_sample_idx = min ( offset_front_sample_idxs ) 
if min_sample_idx > onset_sample_idx and min_sample_idx < closest_offset_sample_idx : 
~~~ closest_offset_sample_idx = min_sample_idx 
best_id_so_far = offset_front_id 
~~ ~~ assert best_id_so_far > 1 or best_id_so_far == - 1 
return best_id_so_far 
~~ def _get_offset_front_id_after_onset_front ( onset_front_id , onset_fronts , offset_fronts ) : 
onset_idxs = _get_front_idxs_from_id ( onset_fronts , onset_front_id ) 
onset_sample_idxs = [ s for _f , s in onset_idxs ] 
latest_onset_in_front = max ( onset_sample_idxs ) 
offset_front_id_after_this_onset_front = _get_offset_front_id_after_onset_sample_idx ( latest_onset_in_front , offset_fronts ) 
return int ( offset_front_id_after_this_onset_front ) 
~~ def _match_offset_front_id_to_onset_front_id ( onset_front_id , onset_fronts , offset_fronts , onsets , offsets ) : 
offset_idxs = [ _lookup_offset_by_onset_idx ( i , onsets , offsets ) for i in onset_idxs ] 
candidate_offset_front_ids = set ( [ int ( offset_fronts [ f , i ] ) for f , i in offset_idxs ] ) 
candidate_offset_front_ids = [ id for id in candidate_offset_front_ids if id != 0 ] 
if candidate_offset_front_ids : 
~~~ chosen_offset_front_id = _choose_front_id_from_candidates ( candidate_offset_front_ids , offset_fronts , offset_idxs ) 
~~~ chosen_offset_front_id = _get_offset_front_id_after_onset_front ( onset_front_id , onset_fronts , offset_fronts ) 
~~ return chosen_offset_front_id 
~~ def _get_consecutive_portions_of_front ( front ) : 
last_f = None 
ls = [ ] 
for f , s in front : 
~~~ if last_f is not None and f != last_f + 1 : 
~~~ yield ls 
~~ ls . append ( ( f , s ) ) 
last_f = f 
~~ yield ls 
~~ def _get_consecutive_and_overlapping_fronts ( onset_fronts , offset_fronts , onset_front_id , offset_front_id ) : 
onset_front = _get_front_idxs_from_id ( onset_fronts , onset_front_id ) 
offset_front = _get_front_idxs_from_id ( offset_fronts , offset_front_id ) 
consecutive_portions_of_onset_front = [ c for c in _get_consecutive_portions_of_front ( onset_front ) ] 
for consecutive_portion_of_onset_front in consecutive_portions_of_onset_front : 
~~~ onset_front_frequency_indexes = [ f for f , _ in consecutive_portion_of_onset_front ] 
overlapping_offset_front = [ ( f , s ) for f , s in offset_front if f in onset_front_frequency_indexes ] 
for consecutive_portion_of_offset_front in _get_consecutive_portions_of_front ( overlapping_offset_front ) : 
~~~ if consecutive_portion_of_offset_front : 
~~~ return consecutive_portion_of_onset_front , consecutive_portion_of_offset_front 
~~ ~~ ~~ return [ ] , [ ] 
~~ def _update_segmentation_mask ( segmentation_mask , onset_fronts , offset_fronts , onset_front_id , offset_front_id_most_overlap ) : 
onset_front_overlap , offset_front_overlap = _get_consecutive_and_overlapping_fronts ( onset_fronts , offset_fronts , onset_front_id , offset_front_id_most_overlap ) 
offset_front = _get_front_idxs_from_id ( offset_fronts , offset_front_id_most_overlap ) 
onset_front , offset_front , onset_front_overlap , offset_front_overlap 
assert onset_front_overlap , msg 
assert offset_front_overlap , msg 
onset_front = onset_front_overlap 
offset_front = offset_front_overlap 
flow_on , _slow_on = onset_front [ 0 ] 
fhigh_on , _shigh_on = onset_front [ - 1 ] 
flow_off , _slow_off = offset_front [ 0 ] 
fhigh_off , _shigh_off = offset_front [ - 1 ] 
flow = max ( flow_on , flow_off ) 
fhigh = min ( fhigh_on , fhigh_off ) 
for fidx , _freqchan in enumerate ( segmentation_mask [ flow : fhigh + 1 , : ] , start = flow ) : 
fidx , flow , len ( onset_front ) , onset_front 
fidx , flow , len ( offset_front ) , offset_front 
_ , beg = onset_front [ fidx - flow ] 
_ , end = offset_front [ fidx - flow ] 
if beg > end : 
~~~ end , beg = beg , end 
~~ assert end >= beg 
segmentation_mask [ fidx , beg : end + 1 ] = onset_front_id 
onset_fronts [ fidx , ( beg + 1 ) : ( end + 1 ) ] = 0 
offset_fronts [ fidx , ( beg + 1 ) : ( end + 1 ) ] = 0 
~~ nfreqs_used_in_onset_front = ( fidx - flow ) + 1 
indexes = np . arange ( flow , fhigh + 1 , 1 , dtype = np . int64 ) 
onset_front_sample_idxs_across_freqs = np . array ( [ s for _ , s in onset_front ] ) 
onset_front_sample_idxs_across_freqs_up_to_break = onset_front_sample_idxs_across_freqs [ : nfreqs_used_in_onset_front ] 
offset_front_sample_idxs_across_freqs = np . array ( [ s for _ , s in offset_front ] ) 
offset_front_sample_idxs_across_freqs_up_to_break = offset_front_sample_idxs_across_freqs [ : nfreqs_used_in_onset_front ] 
offset_fronts [ indexes [ : nfreqs_used_in_onset_front ] , offset_front_sample_idxs_across_freqs_up_to_break ] = 0 
onset_fronts [ indexes [ : nfreqs_used_in_onset_front ] , onset_front_sample_idxs_across_freqs_up_to_break ] = 0 
whole_onset_front_matched = onset_front_id not in np . unique ( onset_fronts ) 
return whole_onset_front_matched 
~~ def _front_id_from_idx ( front , index ) : 
fidx , sidx = index 
id = front [ fidx , sidx ] 
if id == 0 : 
~~~ return - 1 
~~~ return id 
~~ ~~ def _get_front_ids_one_at_a_time ( onset_fronts ) : 
yielded_so_far = set ( ) 
for row in onset_fronts : 
~~~ for id in row : 
~~~ if id != 0 and id not in yielded_so_far : 
~~~ yield id 
yielded_so_far . add ( id ) 
~~ ~~ ~~ ~~ def _get_corresponding_offsets ( onset_fronts , onset_front_id , onsets , offsets ) : 
corresponding_offsets = [ ] 
for index in _get_front_idxs_from_id ( onset_fronts , onset_front_id ) : 
~~~ offset_fidx , offset_sidx = _lookup_offset_by_onset_idx ( index , onsets , offsets ) 
corresponding_offsets . append ( ( offset_fidx , offset_sidx ) ) 
~~ return corresponding_offsets 
~~ def _get_all_offset_fronts_from_offsets ( offset_fronts , corresponding_offsets ) : 
all_offset_fronts_of_interest = [ ] 
ids_ntimes_seen = { } 
for offset_index in corresponding_offsets : 
~~~ offset_id = _front_id_from_idx ( offset_fronts , offset_index ) 
if offset_id not in ids_ntimes_seen : 
~~~ offset_front_idxs = _get_front_idxs_from_id ( offset_fronts , offset_id ) 
all_offset_fronts_of_interest . append ( offset_front_idxs ) 
ids_ntimes_seen [ offset_id ] = 1 
~~~ ids_ntimes_seen [ offset_id ] += 1 
~~ ~~ return all_offset_fronts_of_interest , ids_ntimes_seen 
~~ def _remove_overlaps ( segmentation_mask , fronts ) : 
fidxs , sidxs = np . where ( ( segmentation_mask != fronts ) & ( segmentation_mask != 0 ) & ( fronts != 0 ) ) 
fronts [ fidxs , sidxs ] = 0 
~~ def _match_fronts ( onset_fronts , offset_fronts , onsets , offsets , debug = False ) : 
~~ ~~ onset_fronts = np . copy ( onset_fronts ) 
offset_fronts = np . copy ( offset_fronts ) 
onsets = np . copy ( onsets ) 
offsets = np . copy ( offsets ) 
segmentation_mask = np . zeros_like ( onset_fronts ) 
resulting_onset_fronts = np . copy ( onset_fronts ) 
for onset_front_id in _get_front_ids_one_at_a_time ( onset_fronts ) : 
front_is_complete = False 
while not front_is_complete : 
~~~ corresponding_offsets = _get_corresponding_offsets ( resulting_onset_fronts , onset_front_id , onsets , offsets ) 
if not corresponding_offsets : 
~~ _all_offset_fronts_of_interest , ids_ntimes_seen = _get_all_offset_fronts_from_offsets ( offset_fronts , corresponding_offsets ) 
ntimes_seen_sorted = sorted ( [ ( k , v ) for k , v in ids_ntimes_seen . items ( ) ] , key = lambda tup : ( - 1 * tup [ 1 ] , tup [ 0 ] ) ) 
offset_front_id , _ntimes_seen = ntimes_seen_sorted [ 0 ] 
if offset_front_id == - 1 and len ( ntimes_seen_sorted ) > 1 : 
~~~ offset_front_id , _ntimes_seen = ntimes_seen_sorted [ 1 ] 
~~ offset_front_id_most_overlap = offset_front_id 
front_is_complete = _update_segmentation_mask ( segmentation_mask , 
resulting_onset_fronts , 
offset_fronts , 
onset_front_id , 
offset_front_id_most_overlap ) 
_remove_overlaps ( segmentation_mask , resulting_onset_fronts ) 
_remove_overlaps ( segmentation_mask , offset_fronts ) 
~~ ~~ return segmentation_mask 
~~ def _remove_fronts_that_are_too_small ( fronts , size ) : 
ids = np . unique ( fronts ) 
for id in ids : 
~~~ if id == 0 or id == - 1 : 
~~ front = _get_front_idxs_from_id ( fronts , id ) 
if len ( front ) < size : 
~~~ indexes = ( [ f for f , _ in front ] , [ s for _ , s in front ] ) 
fronts [ indexes ] = 0 
~~ ~~ ~~ def _break_poorly_matched_fronts ( fronts , threshold = 0.1 , threshold_overlap_samples = 3 ) : 
breaks_after = { } 
for front_id in _get_front_ids_one_at_a_time ( fronts ) : 
~~~ front = _get_front_idxs_from_id ( fronts , front_id ) 
for i , ( f , s ) in enumerate ( front ) : 
~~~ if i < len ( front ) - 1 : 
~~~ next_f , next_s = front [ i + 1 ] 
low_s = min ( s , next_s ) 
high_s = max ( s , next_s ) 
sig_this_f = fronts [ f , low_s : high_s ] 
sig_next_f = fronts [ next_f , low_s : high_s ] 
assert len ( sig_next_f ) == len ( sig_this_f ) 
if len ( sig_next_f ) > threshold_overlap_samples : 
~~~ correlation = signal . correlate ( sig_this_f , sig_next_f , mode = 'same' ) 
assert len ( correlation ) > 0 
correlation = correlation / max ( correlation + 1E-9 ) 
similarity = np . sum ( correlation ) / len ( correlation ) 
if similarity < threshold : 
~~~ if front_id in breaks_after : 
~~~ breaks_after [ front_id ] . append ( ( f , s ) ) 
~~~ breaks_after [ front_id ] = [ ] 
~~ ~~ ~~ ~~ ~~ ~~ taken_ids = sorted ( np . unique ( fronts ) ) 
next_id = taken_ids [ - 1 ] + 1 
for id in breaks_after . keys ( ) : 
~~~ for f , s in breaks_after [ id ] : 
~~~ fidxs , sidxs = np . where ( fronts == id ) 
idxs_greater_than_f = [ fidx for fidx in fidxs if fidx > f ] 
start = len ( sidxs ) - len ( idxs_greater_than_f ) 
indexes = ( idxs_greater_than_f , sidxs [ start : ] ) 
fronts [ indexes ] = next_id 
next_id += 1 
~~ ~~ _remove_fronts_that_are_too_small ( fronts , 3 ) 
~~ def _update_segmentation_mask_if_overlap ( toupdate , other , id , otherid ) : 
yourmask = other == otherid 
mymask = toupdate == id 
overlap_exists = np . any ( yourmask & mymask ) 
if not overlap_exists : 
~~ yourfidxs , yoursidxs = np . where ( other == otherid ) 
toupdate [ yourfidxs , yoursidxs ] = id 
~~ def _segments_are_adjacent ( seg1 , seg2 ) : 
lsf1 , lss1 = seg1 
lsf2 , lss2 = seg2 
for i , f1 in enumerate ( lsf1 ) : 
~~~ for j , f2 in enumerate ( lsf2 ) : 
~~~ if f1 <= f2 + 1 and f1 >= f2 - 1 : 
~~~ if lss1 [ i ] <= lss2 [ j ] + 1 and lss1 [ i ] >= lss2 [ j ] - 1 : 
~~ ~~ ~~ ~~ return False 
~~ def _merge_adjacent_segments ( mask ) : 
mask_ids = [ id for id in np . unique ( mask ) if id != 0 ] 
for id in mask_ids : 
~~~ myfidxs , mysidxs = np . where ( mask == id ) 
~~~ if id == other : 
~~~ other_fidxs , other_sidxs = np . where ( mask == other ) 
if _segments_are_adjacent ( ( myfidxs , mysidxs ) , ( other_fidxs , other_sidxs ) ) : 
~~~ mask [ other_fidxs , other_sidxs ] = id 
~~ ~~ ~~ ~~ ~~ def _integrate_segmentation_masks ( segmasks ) : 
if len ( segmasks ) == 1 : 
~~~ return segmasks 
coarse_mask = np . copy ( segmasks [ 0 ] ) 
mask_ids = [ id for id in np . unique ( coarse_mask ) if id != 0 ] 
~~~ for mask in segmasks [ 1 : ] : 
~~~ finer_ids = [ i for i in np . unique ( mask ) if i != 0 ] 
for finer_id in finer_ids : 
~~~ _update_segmentation_mask_if_overlap ( coarse_mask , mask , id , finer_id ) 
#_merge_adjacent_segments(coarse_mask) 
~~ ~~ ~~ return coarse_mask 
~~ def _separate_masks ( mask , threshold = 0.025 ) : 
~~ with multiprocessing . Pool ( processes = ncpus ) as pool : 
~~~ mask_ids = [ id for id in np . unique ( mask ) if id != 0 ] 
thresholds = [ threshold * mask . size for _ in range ( len ( mask_ids ) ) ] 
masks = [ mask for _ in range ( len ( mask_ids ) ) ] 
ms = pool . starmap ( _separate_masks_task , zip ( mask_ids , thresholds , masks ) ) 
~~ return [ m for m in ms if m is not None ] 
~~ def _downsample_one_or_the_other ( mask , mask_indexes , stft , stft_indexes ) : 
if mask . shape [ 1 ] > stft . shape [ 1 ] : 
~~~ downsample_factor = mask . shape [ 1 ] / stft . shape [ 1 ] 
indexes = _get_downsampled_indexes ( mask , downsample_factor ) 
mask = mask [ : , indexes ] 
mask_indexes = np . array ( indexes ) 
~~ elif mask . shape [ 1 ] < stft . shape [ 1 ] : 
~~~ downsample_factor = stft . shape [ 1 ] / mask . shape [ 1 ] 
indexes = _get_downsampled_indexes ( stft , downsample_factor ) 
stft = stft [ : , indexes ] 
stft_indexes = np . array ( indexes ) 
~~ return mask , mask_indexes , stft , stft_indexes 
~~ def _map_segmentation_mask_to_stft_domain ( mask , times , frequencies , stft_times , stft_frequencies ) : 
times . shape , frequencies . shape , mask . shape 
result = np . zeros ( ( stft_frequencies . shape [ 0 ] , stft_times . shape [ 0 ] ) ) 
if len ( stft_times ) > len ( times ) : 
~~~ all_j = [ j for j in range ( len ( stft_times ) ) ] 
idxs = [ int ( i ) for i in np . linspace ( 0 , len ( times ) - 1 , num = len ( stft_times ) ) ] 
all_i = [ all_j [ idx ] for idx in idxs ] 
~~~ all_i = [ i for i in range ( len ( times ) ) ] 
idxs = [ int ( i ) for i in np . linspace ( 0 , len ( stft_times ) - 1 , num = len ( times ) ) ] 
all_j = [ all_i [ idx ] for idx in idxs ] 
~~ for i , j in zip ( all_i , all_j ) : 
~~~ result [ : , j ] = np . interp ( stft_frequencies , frequencies , mask [ : , i ] ) 
~~ def _asa_task ( q , masks , stft , sample_width , frame_rate , nsamples_for_each_fft ) : 
for mask in masks : 
~~~ mask = np . where ( mask > 0 , 1 , 0 ) 
~~ masks = [ mask * stft for mask in masks ] 
nparrs = [ ] 
dtype_dict = { 1 : np . int8 , 2 : np . int16 , 4 : np . int32 } 
dtype = dtype_dict [ sample_width ] 
for m in masks : 
~~~ _times , nparr = signal . istft ( m , frame_rate , nperseg = nsamples_for_each_fft ) 
nparr = nparr . astype ( dtype ) 
nparrs . append ( nparr ) 
~~ for m in nparrs : 
~~~ q . put ( m ) 
~~ q . put ( "DONE" ) 
~~ def _get_filter_indices ( seg , start_as_yes , prob_raw_yes , ms_per_input , model , transition_matrix , model_stats ) : 
filter_triggered = 1 if start_as_yes else 0 
prob_raw_no = 1.0 - prob_raw_yes 
for segment , _timestamp in seg . generate_frames_as_segments ( ms_per_input ) : 
~~~ yield filter_triggered 
observation = int ( round ( model . predict ( segment ) ) ) 
prob_hyp_yes_given_last_hyp = 1.0 - transition_matrix [ 0 ] if filter_triggered else transition_matrix [ 1 ] 
prob_hyp_no_given_last_hyp = transition_matrix [ 0 ] if filter_triggered else 1.0 - transition_matrix [ 1 ] 
prob_hyp_yes_given_data = model_stats [ 0 ] if observation == 1 else model_stats [ 1 ] 
prob_hyp_no_given_data = 1.0 - model_stats [ 0 ] if observation == 1 else 1.0 - model_stats [ 1 ] 
hypothesis_yes = prob_raw_yes * prob_hyp_yes_given_last_hyp * prob_hyp_yes_given_data 
hypothesis_no = prob_raw_no * prob_hyp_no_given_last_hyp * prob_hyp_no_given_data 
distribution = [ 1 for i in range ( int ( round ( hypothesis_yes * 100 ) ) ) ] 
distribution . extend ( [ 0 for i in range ( int ( round ( hypothesis_no * 100 ) ) ) ] ) 
random . shuffle ( distribution ) 
filter_triggered = random . choice ( distribution ) 
~~ ~~ def _group_filter_values ( seg , filter_indices , ms_per_input ) : 
for filter_value , ( _segment , timestamp ) in zip ( filter_indices , seg . generate_frames_as_segments ( ms_per_input ) ) : 
~~~ if filter_value == 1 : 
~~~ if len ( ret ) > 0 and ret [ - 1 ] [ 0 ] == 'n' : 
~~ elif len ( ret ) > 0 and ret [ - 1 ] [ 0 ] == 'y' : 
~~~ ret [ - 1 ] [ 1 ] = timestamp 
~~~ ret . append ( [ 'n' , timestamp ] ) 
~~ ~~ ~~ return ret 
~~ def _homogeneity_filter ( ls , window_size ) : 
k = window_size 
i = k 
while i <= len ( ls ) - k : 
~~~ window = [ ls [ i + j ] for j in range ( k ) ] 
mode = 1 if sum ( window ) >= k / 2 else 0 
for j in range ( k ) : 
~~~ ls [ i + j ] = mode 
~~ i += k 
~~ return ls 
~~ def bandpass_filter ( data , low , high , fs , order = 5 ) : 
nyq = 0.5 * fs 
low = low / nyq 
high = high / nyq 
b , a = signal . butter ( order , [ low , high ] , btype = 'band' ) 
y = signal . lfilter ( b , a , data ) 
~~ def lowpass_filter ( data , cutoff , fs , order = 5 ) : 
normal_cutoff = cutoff / nyq 
b , a = signal . butter ( order , normal_cutoff , btype = 'low' , analog = False ) 
~~ def list_to_tf_input ( data , response_index , num_outcomes ) : 
matrix = np . matrix ( [ row [ : response_index ] + row [ response_index + 1 : ] for row in data ] ) 
outcomes = np . asarray ( [ row [ response_index ] for row in data ] , dtype = np . uint8 ) 
outcomes_onehot = ( np . arange ( num_outcomes ) == outcomes [ : , None ] ) . astype ( np . float32 ) 
return matrix , outcomes_onehot 
~~ def expand_and_standardize_dataset ( response_index , response_header , data_set , col_vals , headers , standardizers , feats_to_ignore , columns_to_expand , outcome_trans_dict ) : 
modified_set = [ ] 
for row_index , row in enumerate ( data_set ) : 
~~~ new_row = [ ] 
for col_index , val in enumerate ( row ) : 
~~~ header = headers [ col_index ] 
if col_index == response_index : 
~~~ new_outcome = outcome_trans_dict [ val ] 
new_row . append ( new_outcome ) 
~~ elif header in feats_to_ignore : 
~~ elif header in columns_to_expand : 
~~~ for poss_val in col_vals [ header ] : 
~~~ if val == poss_val : 
~~~ new_cat_val = 1.0 
~~~ new_cat_val = - 1.0 
~~ new_row . append ( new_cat_val ) 
~~~ new_cont_val = float ( ( val - standardizers [ header ] [ 'mean' ] ) / standardizers [ header ] [ 'std_dev' ] ) 
new_row . append ( new_cont_val ) 
~~ ~~ modified_set . append ( new_row ) 
~~ expanded_headers = [ ] 
for header in headers : 
~~~ if header in feats_to_ignore : 
~~ elif ( header in columns_to_expand ) and ( header is not response_header ) : 
~~~ new_header = '{}_{}' . format ( header , poss_val ) 
expanded_headers . append ( new_header ) 
~~~ expanded_headers . append ( header ) 
~~ ~~ return modified_set , expanded_headers 
~~ def equal_ignore_order ( a , b ) : 
unmatched = list ( b ) 
for element in a : 
~~~ unmatched . remove ( element ) 
~~ ~~ return not unmatched 
~~ def repair ( self , data_to_repair ) : 
~~~ num_cols = len ( data_to_repair [ 0 ] ) 
col_ids = range ( num_cols ) 
col_types = [ "Y" ] * len ( col_ids ) 
for i , col in enumerate ( col_ids ) : 
~~~ if i in self . features_to_ignore : 
~~~ col_types [ i ] = "I" 
~~ elif i == self . feature_to_repair : 
~~~ col_types [ i ] = "X" 
~~ ~~ col_type_dict = { col_id : col_type for col_id , col_type in zip ( col_ids , col_types ) } 
not_I_col_ids = filter ( lambda x : col_type_dict [ x ] != "I" , col_ids ) 
if self . kdd : 
~~~ cols_to_repair = filter ( lambda x : col_type_dict [ x ] == "Y" , col_ids ) 
~~~ cols_to_repair = filter ( lambda x : col_type_dict [ x ] in "YX" , col_ids ) 
~~ safe_stratify_cols = [ self . feature_to_repair ] 
data_dict = { col_id : [ ] for col_id in col_ids } 
for row in data_to_repair : 
~~~ for i in col_ids : 
~~~ data_dict [ i ] . append ( row [ i ] ) 
~~ ~~ repair_types = { } 
for col_id , values in data_dict . items ( ) : 
~~~ if all ( isinstance ( value , float ) for value in values ) : 
~~~ repair_types [ col_id ] = float 
~~ elif all ( isinstance ( value , int ) for value in values ) : 
~~~ repair_types [ col_id ] = int 
~~~ repair_types [ col_id ] = str 
unique_col_vals = { } 
index_lookup = { } 
for col_id in not_I_col_ids : 
~~~ col_values = data_dict [ col_id ] 
col_values = sorted ( list ( set ( col_values ) ) ) 
unique_col_vals [ col_id ] = col_values 
index_lookup [ col_id ] = { col_values [ i ] : i for i in range ( len ( col_values ) ) } 
unique_stratify_values = [ unique_col_vals [ i ] for i in safe_stratify_cols ] 
all_stratified_groups = list ( product ( * unique_stratify_values ) ) 
stratified_group_indices = defaultdict ( list ) 
val_sets = { group : { col_id : set ( ) for col_id in cols_to_repair } 
for group in all_stratified_groups } 
for i , row in enumerate ( data_to_repair ) : 
~~~ group = tuple ( row [ col ] for col in safe_stratify_cols ) 
for col_id in cols_to_repair : 
~~~ val_sets [ group ] [ col_id ] . add ( row [ col_id ] ) 
~~ stratified_group_indices [ group ] . append ( i ) 
stratified_group_data = { group : { } for group in all_stratified_groups } 
for group in all_stratified_groups : 
~~~ for col_id , col_dict in data_dict . items ( ) : 
~~~ indices = { } 
for i in stratified_group_indices [ group ] : 
~~~ value = col_dict [ i ] 
if value not in indices : 
~~~ indices [ value ] = [ ] 
~~ indices [ value ] . append ( i ) 
~~ stratified_col_values = [ ( occurs , val ) for val , occurs in indices . items ( ) ] 
stratified_col_values . sort ( key = lambda tup : tup [ 1 ] ) 
stratified_group_data [ group ] [ col_id ] = stratified_col_values 
~~ ~~ mode_feature_to_repair = get_mode ( data_dict [ self . feature_to_repair ] ) 
~~~ group_offsets = { group : 0 for group in all_stratified_groups } 
col = data_dict [ col_id ] 
num_quantiles = min ( len ( val_sets [ group ] [ col_id ] ) for group in all_stratified_groups ) 
quantile_unit = 1.0 / num_quantiles 
if repair_types [ col_id ] in { int , float } : 
~~~ for quantile in range ( num_quantiles ) : 
~~~ median_at_quantiles = [ ] 
indices_per_group = { } 
~~~ group_data_at_col = stratified_group_data [ group ] [ col_id ] 
num_vals = len ( group_data_at_col ) 
offset = int ( round ( group_offsets [ group ] * num_vals ) ) 
number_to_get = int ( round ( ( group_offsets [ group ] + quantile_unit ) * num_vals ) - offset ) 
group_offsets [ group ] += quantile_unit 
if number_to_get > 0 : 
~~~ offset_data = group_data_at_col [ offset : offset + number_to_get ] 
indices_per_group [ group ] = [ i for val_indices , _ in offset_data for i in val_indices ] 
values = sorted ( [ float ( val ) for _ , val in offset_data ] ) 
median_at_quantiles . append ( get_median ( values , self . kdd ) ) 
~~ ~~ median = get_median ( median_at_quantiles , self . kdd ) 
median_val_pos = index_lookup [ col_id ] [ median ] 
~~~ for index in indices_per_group [ group ] : 
~~~ original_value = col [ index ] 
current_val_pos = index_lookup [ col_id ] [ original_value ] 
distance_to_repair = int ( round ( distance * self . repair_level ) ) 
index_of_repair_value = current_val_pos + distance_to_repair 
repaired_value = unique_col_vals [ col_id ] [ index_of_repair_value ] 
data_dict [ col_id ] [ index ] = repaired_value 
~~ ~~ ~~ ~~ elif repair_types [ col_id ] in { str } : 
~~~ feature = CategoricalFeature ( col ) 
categories = feature . bin_index_dict . keys ( ) 
group_features = get_group_data ( all_stratified_groups , stratified_group_data , col_id ) 
categories_count = get_categories_count ( categories , all_stratified_groups , group_features ) 
categories_count_norm = get_categories_count_norm ( categories , all_stratified_groups , categories_count , group_features ) 
median = get_median_per_category ( categories , categories_count_norm ) 
dist_generator = lambda group_index , category : gen_desired_dist ( group_index , category , col_id , median , self . repair_level , categories_count_norm , self . feature_to_repair , mode_feature_to_repair ) 
count_generator = lambda group_index , group , category : gen_desired_count ( group_index , group , category , median , group_features , self . repair_level , categories_count ) 
group_features , overflow = flow_on_group_features ( all_stratified_groups , group_features , count_generator ) 
group_features , assigned_overflow , distribution = assign_overflow ( all_stratified_groups , categories , overflow , group_features , dist_generator ) 
~~~ indices = stratified_group_indices [ group ] 
for i , index in enumerate ( indices ) : 
~~~ repaired_value = group_features [ group ] . data [ i ] 
~~ ~~ ~~ ~~ repaired_data = [ ] 
for i , orig_row in enumerate ( data_to_repair ) : 
~~~ new_row = [ orig_row [ j ] if j not in cols_to_repair else data_dict [ j ] [ i ] for j in col_ids ] 
repaired_data . append ( new_row ) 
~~ return repaired_data 
~~ def group_audit_ranks ( filenames , measurer , similarity_bound = 0.05 ) : 
def _partition_groups ( feature_scores ) : 
~~~ groups = [ ] 
for feature , score in feature_scores : 
~~~ added_to_group = False 
for i , group in enumerate ( groups ) : 
~~~ mean_score , group_feature_scores = group 
if abs ( mean_score - score ) < similarity_bound : 
~~~ groups [ i ] [ 1 ] . append ( ( feature , score ) ) 
groups [ i ] [ 0 ] = sum ( [ s for _ , s in group_feature_scores ] ) / len ( group_feature_scores ) 
added_to_group = True 
~~ ~~ if not added_to_group : 
~~~ groups . append ( [ score , [ ( feature , score ) ] ] ) 
~~ ~~ return [ [ feature for feature , score in group ] for _ , group in groups ] 
~~ score_dict = { } 
features = [ ] 
~~~ with open ( filename ) as audit_file : 
feature = header_line [ header_line . index ( ":" ) + 1 : ] 
features . append ( feature ) 
~~ confusion_matrices = load_audit_confusion_matrices ( filename ) 
for rep_level , matrix in confusion_matrices : 
~~~ score = measurer ( matrix ) 
if rep_level not in score_dict : 
~~~ score_dict [ rep_level ] = { } 
~~ score_dict [ rep_level ] [ feature ] = score 
~~ ~~ score_keys = sorted ( score_dict . keys ( ) ) 
groups = [ features ] 
while score_keys : 
~~~ key = score_keys . pop ( ) 
new_groups = [ ] 
for group in groups : 
~~~ group_features = [ ( f , score_dict [ key ] [ f ] ) for f in group ] 
sub_groups = _partition_groups ( group_features ) 
new_groups . extend ( sub_groups ) 
~~ groups = new_groups 
~~ return groups 
~~ def accuracy ( conf_matrix ) : 
total , correct = 0.0 , 0.0 
for true_response , guess_dict in conf_matrix . items ( ) : 
~~~ for guess , count in guess_dict . items ( ) : 
~~~ if true_response == guess : 
~~~ correct += count 
~~ total += count 
~~ ~~ return correct / total 
~~ def BCR ( conf_matrix ) : 
parts = [ ] 
~~~ error = 0.0 
total = 0.0 
for guess , count in guess_dict . items ( ) : 
~~~ if true_response != guess : 
~~~ error += count 
~~ parts . append ( error / total ) 
~~ BER = sum ( parts ) / len ( parts ) 
return 1 - BER 
~~ def get_median ( values , kdd ) : 
if not values : 
~~ sorted_values = deepcopy ( values ) 
if kdd : 
~~~ return sorted_values [ len ( values ) // 2 ] 
~~~ if len ( values ) % 2 == 0 : 
~~~ return sorted_values [ len ( values ) // 2 - 1 ] 
~~ ~~ ~~ def expand_to_one_hot ( data , expand = True , use_alternative = False ) : 
~~~ header_dict = { 'ALCABUS' : 0 , 'PRIRCAT' : 1 , 'TMSRVC' : 2 , 'SEX1' : 3 , 'RACE' : 4 , 'RELTYP' : 5 , 'age_1st_arrest' : 6 , 'DRUGAB' : 7 , 'Class' : 8 , 'RLAGE' : 9 , 'NFRCTNS' : 10 } 
for entry in data : 
~~~ temp = { } 
if expand == True : 
~~~ if entry [ header_dict [ "SEX1" ] ] == "FEMALE" : 
~~~ temp [ 'female' ] = 1 
~~~ temp [ 'female' ] = 0 
~~~ temp [ 'prior_alcohol_abuse' ] = 1 
~~~ temp [ 'prior_alcohol_abuse' ] = 0 
~~~ temp [ 'prior_drug_abuse' ] = 1 
~~~ temp [ 'prior_drug_abuse' ] = 0 
~~~ temp [ 'infraction_in_prison' ] = 1 
~~~ temp [ 'infraction_in_prison' ] = 0 
for cat in race_cats : 
~~~ if entry [ header_dict [ 'RACE' ] ] == cat : 
~~~ temp [ 'race_' + cat ] = 1 
~~~ temp [ 'race_' + cat ] = 0 
for cat in release_age_cats : 
~~~ if entry [ header_dict [ 'RLAGE' ] ] == cat : 
~~~ temp [ 'release_age_' + cat ] = 1 
~~~ temp [ 'release_age_' + cat ] = 0 
for cat in time_served_cats : 
~~~ if entry [ header_dict [ 'TMSRVC' ] ] == cat : 
~~~ temp [ 'time_served_' + cat ] = 1 
~~~ temp [ 'time_served_' + cat ] = 0 
for cat in prior_arrest_cats : 
~~~ if entry [ header_dict [ 'PRIRCAT' ] ] == cat : 
~~~ temp [ 'prior_arrest_' + cat ] = 1 
~~~ temp [ 'prior_arrest_' + cat ] = 0 
if entry [ header_dict [ 'RELTYP' ] ] in conditional_release : 
~~~ temp [ 'released_conditional' ] = 1 
temp [ 'released_unconditional' ] = 0 
temp [ 'released_other' ] = 0 
~~ elif entry [ header_dict [ 'RELTYP' ] ] in unconditional_release : 
~~~ temp [ 'released_conditional' ] = 0 
temp [ 'released_unconditional' ] = 1 
temp [ 'released_other' ] = 1 
for cat in first_arrest_cats : 
~~~ if entry [ header_dict [ 'age_1st_arrest' ] ] == cat : 
~~~ temp [ 'age_first_arrest_' + cat ] = 1 
~~~ temp [ 'age_first_arrest_' + cat ] = 0 
~~~ temp [ 'SEX1' ] = entry [ 'SEX1' ] 
temp [ 'RELTYP' ] = entry [ 'RELTYP' ] 
temp [ 'PRIRCAT' ] = entry [ 'PRIRCAT' ] 
temp [ 'ALCABUS' ] = entry [ 'ALCABUS' ] 
temp [ 'DRUGAB' ] = entry [ 'DRUGAB' ] 
temp [ 'RLAGE' ] = entry [ 'RLAGE' ] 
temp [ 'TMSRVC' ] = entry [ 'TMSRVC' ] 
temp [ 'NFRCTNS' ] = entry [ 'NFRCTNS' ] 
temp [ 'RACE' ] = entry [ 'RACE' ] 
~~~ bdate = datetime . date ( int ( entry [ 'YEAROB2' ] ) , int ( entry [ 'MNTHOB2' ] ) , int ( entry [ 'DAYOB2' ] ) ) 
first_arrest = datetime . date ( int ( entry [ 'A001YR' ] ) , int ( entry [ 'A001MO' ] ) , int ( entry [ 'A001DA' ] ) ) 
first_arrest_age = first_arrest - bdate 
temp [ 'age_1st_arrest' ] = first_arrest_age . days 
~~~ temp [ 'age_1st_arrest' ] = 0 
~~ ~~ new_data . append ( temp ) 
~~ fin = [ [ int ( entry [ key ] ) for key in entry . keys ( ) ] for entry in new_data ] 
return fin 
~~ def load_audit_confusion_matrices ( filename ) : 
with open ( filename ) as audit_file : 
confusion_matrices = [ ] 
for line in audit_file : 
~~~ separator = ":" 
separator_index = line . index ( separator ) 
comma_index = line . index ( ',' ) 
repair_level = float ( line [ separator_index + 2 : comma_index ] ) 
raw_confusion_matrix = line [ comma_index + 2 : - 2 ] 
confusion_matrix = json . loads ( raw_confusion_matrix . replace ( "\ , "\\"" ) ) 
confusion_matrices . append ( ( repair_level , confusion_matrix ) ) 
~~ ~~ confusion_matrices . sort ( key = lambda pair : pair [ 0 ] ) 
return confusion_matrices 
return matrix , outcomes 
~~ def FreedmanDiaconisBinSize ( feature_values ) : 
q75 , q25 = numpy . percentile ( feature_values , [ 75 , 25 ] ) 
IQR = q75 - q25 
return 2.0 * IQR * len ( feature_values ) ** ( - 1.0 / 3.0 ) 
~~ def resolve_streams ( wait_time = 1.0 ) : 
buffer = ( c_void_p * 1024 ) ( ) 
num_found = lib . lsl_resolve_all ( byref ( buffer ) , 1024 , c_double ( wait_time ) ) 
return [ StreamInfo ( handle = buffer [ k ] ) for k in range ( num_found ) ] 
~~ def resolve_byprop ( prop , value , minimum = 1 , timeout = FOREVER ) : 
num_found = lib . lsl_resolve_byprop ( byref ( buffer ) , 1024 , 
c_char_p ( str . encode ( prop ) ) , 
c_char_p ( str . encode ( value ) ) , 
minimum , 
c_double ( timeout ) ) 
~~ def resolve_bypred ( predicate , minimum = 1 , timeout = FOREVER ) : 
num_found = lib . lsl_resolve_bypred ( byref ( buffer ) , 1024 , 
c_char_p ( str . encode ( predicate ) ) , 
~~ def handle_error ( errcode ) : 
if type ( errcode ) is c_int : 
~~~ errcode = errcode . value 
~~ if errcode == 0 : 
~~ elif errcode == - 1 : 
~~ elif errcode == - 2 : 
~~ elif errcode == - 3 : 
~~ elif errcode == - 4 : 
~~ elif errcode < 0 : 
~~ ~~ def push_sample ( self , x , timestamp = 0.0 , pushthrough = True ) : 
if len ( x ) == self . channel_count : 
~~~ if self . channel_format == cf_string : 
~~~ x = [ v . encode ( 'utf-8' ) for v in x ] 
~~ handle_error ( self . do_push_sample ( self . obj , self . sample_type ( * x ) , 
c_double ( timestamp ) , 
c_int ( pushthrough ) ) ) 
"stream\ ) 
~~ ~~ def push_chunk ( self , x , timestamp = 0.0 , pushthrough = True ) : 
~~~ n_values = self . channel_count * len ( x ) 
data_buff = ( self . value_type * n_values ) . from_buffer ( x ) 
handle_error ( self . do_push_chunk ( self . obj , data_buff , 
c_long ( n_values ) , 
~~~ if len ( x ) : 
~~~ if type ( x [ 0 ] ) is list : 
~~~ x = [ v for sample in x for v in sample ] 
~~ if self . channel_format == cf_string : 
~~ if len ( x ) % self . channel_count == 0 : 
~~~ constructor = self . value_type * len ( x ) 
handle_error ( self . do_push_chunk ( self . obj , constructor ( * x ) , 
c_long ( len ( x ) ) , 
"channels." ) 
~~ ~~ ~~ ~~ def wait_for_consumers ( self , timeout ) : 
return bool ( lib . lsl_wait_for_consumers ( self . obj , c_double ( timeout ) ) ) 
~~ def info ( self , timeout = FOREVER ) : 
errcode = c_int ( ) 
result = lib . lsl_get_fullinfo ( self . obj , c_double ( timeout ) , 
byref ( errcode ) ) 
handle_error ( errcode ) 
return StreamInfo ( handle = result ) 
~~ def open_stream ( self , timeout = FOREVER ) : 
lib . lsl_open_stream ( self . obj , c_double ( timeout ) , byref ( errcode ) ) 
~~ def time_correction ( self , timeout = FOREVER ) : 
result = lib . lsl_time_correction ( self . obj , c_double ( timeout ) , 
~~ def pull_sample ( self , timeout = FOREVER , sample = None ) : 
if type ( timeout ) is list : 
~~~ assign_to = timeout 
timeout = sample if type ( sample ) is float else 0.0 
~~~ assign_to = None 
~~ errcode = c_int ( ) 
timestamp = self . do_pull_sample ( self . obj , byref ( self . sample ) , 
self . channel_count , c_double ( timeout ) , 
if timestamp : 
~~~ sample = [ v for v in self . sample ] 
if self . channel_format == cf_string : 
~~~ sample = [ v . decode ( 'utf-8' ) for v in sample ] 
~~ if assign_to is not None : 
~~~ assign_to [ : ] = sample 
~~ return sample , timestamp 
~~ ~~ def pull_chunk ( self , timeout = 0.0 , max_samples = 1024 , dest_obj = None ) : 
num_channels = self . channel_count 
max_values = max_samples * num_channels 
if max_samples not in self . buffers : 
~~~ self . buffers [ max_samples ] = ( ( self . value_type * max_values ) ( ) , 
( c_double * max_samples ) ( ) ) 
~~ if dest_obj is not None : 
~~~ data_buff = ( self . value_type * max_values ) . from_buffer ( dest_obj ) 
~~~ data_buff = self . buffers [ max_samples ] [ 0 ] 
~~ ts_buff = self . buffers [ max_samples ] [ 1 ] 
num_elements = self . do_pull_chunk ( self . obj , byref ( data_buff ) , 
byref ( ts_buff ) , max_values , 
max_samples , c_double ( timeout ) , 
num_samples = num_elements / num_channels 
if dest_obj is None : 
~~~ samples = [ [ data_buff [ s * num_channels + c ] for c in range ( num_channels ) ] 
for s in range ( int ( num_samples ) ) ] 
~~~ samples = [ [ v . decode ( 'utf-8' ) for v in s ] for s in samples ] 
free_char_p_array_memory ( data_buff , max_values ) 
~~~ samples = None 
~~ timestamps = [ ts_buff [ s ] for s in range ( int ( num_samples ) ) ] 
return samples , timestamps 
~~ def child ( self , name ) : 
return XMLElement ( lib . lsl_child ( self . e , str . encode ( name ) ) ) 
~~ def next_sibling ( self , name = None ) : 
~~~ return XMLElement ( lib . lsl_next_sibling ( self . e ) ) 
~~~ return XMLElement ( lib . lsl_next_sibling_n ( self . e , str . encode ( name ) ) ) 
~~ ~~ def previous_sibling ( self , name = None ) : 
~~~ return XMLElement ( lib . lsl_previous_sibling ( self . e ) ) 
~~~ return XMLElement ( lib . lsl_previous_sibling_n ( self . e , 
str . encode ( name ) ) ) 
~~ ~~ def child_value ( self , name = None ) : 
~~~ res = lib . lsl_child_value ( self . e ) 
~~~ res = lib . lsl_child_value_n ( self . e , str . encode ( name ) ) 
~~ return res . decode ( 'utf-8' ) 
~~ def append_child_value ( self , name , value ) : 
return XMLElement ( lib . lsl_append_child_value ( self . e , 
str . encode ( name ) , 
str . encode ( value ) ) ) 
~~ def prepend_child_value ( self , name , value ) : 
return XMLElement ( lib . lsl_prepend_child_value ( self . e , 
~~ def set_child_value ( self , name , value ) : 
return XMLElement ( lib . lsl_set_child_value ( self . e , 
~~ def set_name ( self , name ) : 
return bool ( lib . lsl_set_name ( self . e , str . encode ( name ) ) ) 
return bool ( lib . lsl_set_value ( self . e , str . encode ( value ) ) ) 
~~ def append_child ( self , name ) : 
return XMLElement ( lib . lsl_append_child ( self . e , str . encode ( name ) ) ) 
~~ def prepend_child ( self , name ) : 
return XMLElement ( lib . lsl_prepend_child ( self . e , str . encode ( name ) ) ) 
~~ def append_copy ( self , elem ) : 
return XMLElement ( lib . lsl_append_copy ( self . e , elem . e ) ) 
~~ def prepend_copy ( self , elem ) : 
return XMLElement ( lib . lsl_prepend_copy ( self . e , elem . e ) ) 
~~ def remove_child ( self , rhs ) : 
if type ( rhs ) is XMLElement : 
~~~ lib . lsl_remove_child ( self . e , rhs . e ) 
~~~ lib . lsl_remove_child_n ( self . e , rhs ) 
~~ ~~ def results ( self ) : 
num_found = lib . lsl_resolver_results ( self . obj , byref ( buffer ) , 1024 ) 
~~ def pair ( cmd , word ) : 
word = list ( preprocess_query ( word ) ) [ 0 ] 
key = pair_key ( word ) 
tokens = [ t . decode ( ) for t in DB . smembers ( key ) ] 
tokens . sort ( ) 
print ( white ( tokens ) ) 
~~ def do_AUTOCOMPLETE ( cmd , s ) : 
s = list ( preprocess_query ( s ) ) [ 0 ] 
keys = [ k . decode ( ) for k in DB . smembers ( edge_ngram_key ( s ) ) ] 
print ( white ( keys ) ) 
~~ def compute_edge_ngrams ( token , min = None ) : 
if min is None : 
~~~ min = config . MIN_EDGE_NGRAMS 
~~ token = token [ : config . MAX_EDGE_NGRAMS + 1 ] 
return [ token [ : i ] for i in range ( min , len ( token ) ) ] 
~~ def iter_pipe ( pipe , processors ) : 
if isinstance ( pipe , str ) : 
~~~ pipe = [ pipe ] 
~~ for it in processors : 
~~~ pipe = it ( pipe ) 
~~ yield from pipe 
~~ def import_by_path ( path ) : 
if not isinstance ( path , str ) : 
~~ module_path , * name = path . rsplit ( '.' , 1 ) 
func = import_module ( module_path ) 
~~~ func = getattr ( func , name [ 0 ] ) 
~~ return func 
~~ def haversine_distance ( point1 , point2 ) : 
lat1 , lon1 = point1 
lat2 , lon2 = point2 
lon1 , lat1 , lon2 , lat2 = map ( radians , [ lon1 , lat1 , lon2 , lat2 ] ) 
dlon = lon2 - lon1 
dlat = lat2 - lat1 
a = sin ( dlat / 2 ) ** 2 + cos ( lat1 ) * cos ( lat2 ) * sin ( dlon / 2 ) ** 2 
c = 2 * asin ( sqrt ( a ) ) 
km = 6367 * c 
return km 
~~ def imap_unordered ( self , func , iterable , chunksize ) : 
assert self . _state == RUN 
task_batches = Pool . _get_tasks ( func , iterable , chunksize ) 
result = IMapUnorderedIterator ( self . _cache ) 
tasks = ( ( result . _job , i , func , chunk , { } ) 
for i , ( _ , chunk ) in enumerate ( task_batches ) ) 
self . _taskqueue . put ( ( tasks , result . _set_length ) ) 
~~ def make_fuzzy ( word , max = 1 ) : 
neighbors = [ ] 
for i in range ( 0 , len ( word ) - 1 ) : 
~~~ neighbor = list ( word ) 
neighbor [ i ] , neighbor [ i + 1 ] = neighbor [ i + 1 ] , neighbor [ i ] 
neighbors . append ( '' . join ( neighbor ) ) 
~~ for letter in string . ascii_lowercase : 
~~~ for i in range ( 0 , len ( word ) ) : 
if letter != neighbor [ i ] : 
~~~ neighbor [ i ] = letter 
~~ ~~ ~~ for letter in string . ascii_lowercase : 
~~~ for i in range ( 0 , len ( word ) + 1 ) : 
neighbor . insert ( i , letter ) 
~~ ~~ if len ( word ) > 3 : 
del neighbor [ i ] 
~~ ~~ return neighbors 
~~ def do_fuzzy ( self , word ) : 
print ( white ( make_fuzzy ( word ) ) ) 
~~ def do_fuzzyindex ( self , word ) : 
token = Token ( word ) 
neighbors = make_fuzzy ( token ) 
neighbors = [ ( n , DB . zcard ( dbkeys . token_key ( n ) ) ) for n in neighbors ] 
neighbors . sort ( key = lambda n : n [ 1 ] , reverse = True ) 
for token , freq in neighbors : 
~~~ if freq == 0 : 
~~ print ( white ( token ) , blue ( freq ) ) 
~~ ~~ def extend_results_extrapoling_relations ( helper ) : 
if not helper . bucket_dry : 
~~ tokens = set ( helper . meaningful + helper . common ) 
for relation in _extract_manytomany_relations ( tokens ) : 
~~~ helper . add_to_bucket ( [ t . db_key for t in relation ] ) 
if helper . bucket_overflow : 
~~ ~~ def do_help ( self , command ) : 
if command : 
~~~ doc = getattr ( self , 'do_' + command ) . __doc__ 
print ( magenta ( \ ) ) 
names = self . get_names ( ) 
names . sort ( ) 
~~~ if name [ : 3 ] != 'do_' : 
~~ doc = getattr ( self , name ) . __doc__ 
doc = doc . split ( '\\n' ) [ 0 ] 
. replace ( '\\n' , '' ) ) ) ) 
~~ ~~ ~~ def do_BENCH ( self , query ) : 
~~~ count = int ( re . match ( r'^(\\d+).*' , query ) . group ( 1 ) ) 
~~~ count = 100 
~~ self . _search ( query , count = count ) 
~~ def do_INTERSECT ( self , words ) : 
limit = 100 
if 'LIMIT' in words : 
~~~ words , limit = words . split ( 'LIMIT' ) 
~~ tokens = [ keys . token_key ( w ) for w in preprocess_query ( words ) ] 
DB . zinterstore ( words , tokens ) 
results = DB . zrevrange ( words , 0 , limit , withscores = True ) 
DB . delete ( words ) 
for id_ , score in results : 
~~~ r = Result ( id_ ) 
~~ duration = round ( ( time . time ( ) - start ) * 1000 , 1 ) 
~~ def do_DBINFO ( self , * args ) : 
info = DB . info ( ) 
keys = [ 
'keyspace_misses' , 'keyspace_hits' , 'used_memory_human' , 
'total_commands_processed' , 'total_connections_received' , 
'connected_clients' ] 
for key in keys : 
~~ nb_of_redis_db = int ( DB . config_get ( 'databases' ) [ 'databases' ] ) 
for db_index in range ( nb_of_redis_db - 1 ) : 
~~~ db_name = 'db{}' . format ( db_index ) 
if db_name in info : 
~~ ~~ ~~ def do_DBKEY ( self , key ) : 
type_ = DB . type ( key ) . decode ( ) 
if type_ == 'set' : 
~~~ out = DB . smembers ( key ) 
~~ elif type_ == 'string' : 
~~~ out = DB . get ( key ) 
~~ print ( 'type:' , magenta ( type_ ) ) 
print ( 'value:' , white ( out ) ) 
~~ def do_GEODISTANCE ( self , s ) : 
~~~ _id , lat , lon = s . split ( ) 
~~~ result = Result ( keys . document_key ( _id ) ) 
~~~ return self . error ( e ) 
~~ center = ( float ( lat ) , float ( lon ) ) 
km = haversine_distance ( ( float ( result . lat ) , float ( result . lon ) ) , center ) 
score = km_to_score ( km ) 
~~ def do_GEOHASHTOGEOJSON ( self , geoh ) : 
geoh , with_neighbors = self . _match_option ( 'NEIGHBORS' , geoh ) 
bbox = geohash . bbox ( geoh ) 
~~~ with_neighbors = int ( with_neighbors ) 
~~~ with_neighbors = 0 
~~ def expand ( bbox , geoh , depth ) : 
~~~ neighbors = geohash . neighbors ( geoh ) 
for neighbor in neighbors : 
~~~ other = geohash . bbox ( neighbor ) 
if with_neighbors > depth : 
~~~ expand ( bbox , neighbor , depth + 1 ) 
~~~ if other [ 'n' ] > bbox [ 'n' ] : 
~~~ bbox [ 'n' ] = other [ 'n' ] 
~~ if other [ 's' ] < bbox [ 's' ] : 
~~~ bbox [ 's' ] = other [ 's' ] 
~~ if other [ 'e' ] > bbox [ 'e' ] : 
~~~ bbox [ 'e' ] = other [ 'e' ] 
~~ if other [ 'w' ] < bbox [ 'w' ] : 
~~~ bbox [ 'w' ] = other [ 'w' ] 
~~ ~~ ~~ ~~ if with_neighbors > 0 : 
~~~ expand ( bbox , geoh , 0 ) 
~~ geojson = { 
"type" : "Polygon" , 
"coordinates" : [ [ 
[ bbox [ 'w' ] , bbox [ 'n' ] ] , 
[ bbox [ 'e' ] , bbox [ 'n' ] ] , 
[ bbox [ 'e' ] , bbox [ 's' ] ] , 
[ bbox [ 'w' ] , bbox [ 's' ] ] , 
[ bbox [ 'w' ] , bbox [ 'n' ] ] 
] ] 
print ( white ( json . dumps ( geojson ) ) ) 
~~ def do_GEOHASH ( self , latlon ) : 
~~~ lat , lon = map ( float , latlon . split ( ) ) 
~~~ print ( white ( geohash . encode ( lat , lon , config . GEOHASH_PRECISION ) ) ) 
~~ ~~ def do_GEOHASHMEMBERS ( self , geoh ) : 
key = compute_geohash_key ( geoh , with_neighbors != '0' ) 
if key : 
~~~ for id_ in DB . smembers ( key ) : 
~~ ~~ ~~ def do_GET ( self , _id ) : 
doc = doc_by_id ( _id ) 
if not doc : 
~~~ return self . error ( \ . format ( _id ) ) 
~~ for key , value in doc . items ( ) : 
~~~ if key == config . HOUSENUMBERS_FIELD : 
~~ if doc . get ( 'housenumbers' ) : 
~~~ def sorter ( v ) : 
~~~ return int ( re . match ( r'^\\d+' , v [ 'raw' ] ) . group ( ) ) 
~~ ~~ housenumbers = sorted ( doc [ 'housenumbers' ] . values ( ) , key = sorter ) 
print ( white ( 'housenumbers' ) , 
~~ ~~ def do_INDEX ( self , _id ) : 
~~ for field in config . FIELDS : 
~~~ key = field [ 'key' ] 
if key in doc : 
~~~ self . _print_field_index_details ( doc [ key ] , _id ) 
~~ ~~ ~~ def do_BESTSCORE ( self , word ) : 
key = keys . token_key ( indexed_string ( word ) [ 0 ] ) 
for _id , score in DB . zrevrange ( key , 0 , 20 , withscores = True ) : 
~~~ result = Result ( _id ) 
print ( white ( result ) , blue ( score ) , green ( result . _id ) ) 
~~ ~~ def do_REVERSE ( self , latlon ) : 
lat , lon = latlon . split ( ) 
for r in reverse ( float ( lat ) , float ( lon ) ) : 
blue ( r . distance ) , blue ( r . _id ) ) ) 
~~ ~~ def do_STRDISTANCE ( self , s ) : 
s = s . split ( '|' ) 
if not len ( s ) == 2 : 
~~ one , two = s 
print ( white ( compare_str ( one , two ) ) ) 
~~ def do_CONFIG ( self , name ) : 
~~~ for name in self . complete_CONFIG ( ) : 
~~~ self . do_CONFIG ( name ) 
print ( blue ( name ) , white ( format_config ( value ) ) ) 
~~ def do_SCRIPT ( self , args ) : 
~~~ name , keys_count , * args = args . split ( ) 
~~~ keys_count = int ( keys_count ) 
self . do_HELP ( 'SCRIPT' ) 
~~ keys = args [ : keys_count ] 
args = args [ keys_count : ] 
~~~ output = getattr ( scripts , name ) ( keys = keys , args = args ) 
~~ except DB . Error as e : 
~~~ print ( red ( e ) ) 
~~ if not isinstance ( output , list ) : 
~~~ output = [ output ] 
~~ for line in output : 
~~~ print ( white ( line ) ) 
~~ ~~ def getBits_from_array ( array , wordWidth , start , end , 
reinterpretElmToType = None ) : 
inPartOffset = 0 
value = Bits ( end - start , None ) . fromPy ( None ) 
while start != end : 
~~~ assert start < end , ( start , end ) 
dataWordIndex = start // wordWidth 
v = array [ dataWordIndex ] 
if reinterpretElmToType is not None : 
~~~ v = v . _reinterpret_cast ( reinterpretElmToType ) 
~~ endOfWord = ( dataWordIndex + 1 ) * wordWidth 
width = min ( end , endOfWord ) - start 
offset = start % wordWidth 
val = selectBitRange ( v . val , offset , width ) 
vldMask = selectBitRange ( v . vldMask , offset , width ) 
updateTime = v . updateTime 
m = mask ( width ) 
value . val |= ( val & m ) << inPartOffset 
value . vldMask |= ( vldMask & m ) << inPartOffset 
value . updateMask = max ( value . updateTime , updateTime ) 
inPartOffset += width 
start += width 
~~ def reinterptet_harray_to_bits ( typeFrom , sigOrVal , bitsT ) : 
size = int ( typeFrom . size ) 
widthOfElm = typeFrom . elmType . bit_length ( ) 
w = bitsT . bit_length ( ) 
if size * widthOfElm != w : 
~~~ raise TypeConversionErr ( 
~~ partT = Bits ( widthOfElm ) 
parts = [ p . _reinterpret_cast ( partT ) for p in sigOrVal ] 
return Concat ( * reversed ( parts ) ) . _reinterpret_cast ( bitsT ) 
~~ def slice_to_SLICE ( sliceVals , width ) : 
if sliceVals . step is not None : 
~~~ raise NotImplementedError ( ) 
~~ start = sliceVals . start 
stop = sliceVals . stop 
if sliceVals . start is None : 
~~~ start = INT . fromPy ( width ) 
~~~ start = toHVal ( sliceVals . start ) 
~~ if sliceVals . stop is None : 
~~~ stop = INT . fromPy ( 0 ) 
~~~ stop = toHVal ( sliceVals . stop ) 
~~ startIsVal = isinstance ( start , Value ) 
stopIsVal = isinstance ( stop , Value ) 
indexesAreValues = startIsVal and stopIsVal 
if indexesAreValues : 
~~~ updateTime = max ( start . updateTime , stop . updateTime ) 
~~~ updateTime = - 1 
~~ return Slice . getValueCls ( ) ( ( start , stop ) , SLICE , 1 , updateTime ) 
~~ def getBusWordBitRange ( self ) -> Tuple [ int , int ] : 
offset = self . startOfPart % self . parent . wordWidth 
return ( offset + self . bit_length ( ) , offset ) 
~~ def getFieldBitRange ( self ) -> Tuple [ int , int ] : 
offset = self . inFieldOffset 
return ( self . bit_length ( ) + offset , offset ) 
~~ def fill_stm_list_with_enclosure ( parentStm : Optional [ HdlStatement ] , 
current_enclosure : Set [ RtlSignalBase ] , 
statements : List [ "HdlStatement" ] , 
do_enclose_for : List [ RtlSignalBase ] , 
enclosure : Dict [ RtlSignalBase , Union [ Value , RtlSignalBase ] ] ) -> None : 
if statements is None : 
~~~ statements = [ ] 
~~ for e_sig in do_enclose_for : 
~~~ if e_sig in current_enclosure : 
~~ enclosed = False 
for stm in statements : 
~~~ if e_sig in stm . _outputs : 
~~~ if e_sig not in stm . _enclosed_for : 
~~~ stm . _fill_enclosure ( enclosure ) 
~~ enclosed = True 
~~ ~~ if not enclosed : 
~~~ e = enclosure [ e_sig ] 
a = Assignment ( e , e_sig ) 
statements . append ( a ) 
if parentStm is not None : 
~~~ a . _set_parent_stm ( parentStm ) 
~~ ~~ ~~ return statements 
~~ def find_files ( directory , pattern , recursive = True ) : 
if not os . path . isdir ( directory ) : 
~~~ if os . path . exists ( directory ) : 
~~ ~~ if recursive : 
~~~ for root , _ , files in os . walk ( directory ) : 
~~~ for basename in files : 
~~~ if fnmatch . fnmatch ( basename , pattern ) : 
~~~ filename = os . path . join ( root , basename ) 
yield filename 
~~~ root = directory 
for basename in os . listdir ( root ) : 
if os . path . isfile ( filename ) : 
~~ ~~ ~~ ~~ ~~ def SwitchLogic ( cases , default = None ) : 
if default is not None : 
~~~ assigTop = default 
~~~ assigTop = [ ] 
~~ for cond , statements in reversed ( cases ) : 
~~~ assigTop = If ( cond , 
statements 
) . Else ( 
assigTop 
~~ return assigTop 
~~ def In ( sigOrVal , iterable ) : 
res = None 
for i in iterable : 
~~~ i = toHVal ( i ) 
~~~ res = sigOrVal . _eq ( i ) 
~~~ res = res | sigOrVal . _eq ( i ) 
~~ def StaticForEach ( parentUnit , items , bodyFn , name = "" ) : 
items = list ( items ) 
itemsCnt = len ( items ) 
if itemsCnt == 0 : 
~~ elif itemsCnt == 1 : 
~~~ return bodyFn ( items [ 0 ] , 0 ) 
~~~ index = parentUnit . _reg ( name + "for_index" , 
Bits ( log2ceil ( itemsCnt + 1 ) , signed = False ) , 
defVal = 0 ) 
ackSig = parentUnit . _sig ( name + "for_ack" ) 
statementLists = [ ] 
for i , ( statementList , ack ) in [ ( i , bodyFn ( item , i ) ) 
for i , item in enumerate ( items ) ] : 
~~~ statementLists . append ( statementList + [ ( ackSig ( ack ) ) , ] ) 
~~ If ( ackSig , 
If ( index . _eq ( itemsCnt - 1 ) , 
index ( 0 ) 
index ( index + 1 ) 
return Switch ( index ) . addCases ( 
enumerate ( statementLists ) 
) . Default ( 
bodyFn ( items [ 0 ] , 0 ) [ 0 ] , 
ackSig ( True ) 
~~ ~~ def connect ( src , * destinations , exclude : set = None , fit = False ) : 
assignemnts = [ ] 
if isinstance ( src , HObjList ) : 
~~~ for dst in destinations : 
~~~ assert len ( src ) == len ( dst ) , ( src , dst ) 
~~ _destinations = [ iter ( d ) for d in destinations ] 
for _src in src : 
~~~ dsts = [ next ( d ) for d in _destinations ] 
assignemnts . append ( connect ( _src , * dsts , exclude = exclude , fit = fit ) ) 
~~~ assignemnts . append ( _connect ( src , dst , exclude , fit ) ) 
~~ ~~ return assignemnts 
~~ def rol ( sig , howMany ) -> RtlSignalBase : 
width = sig . _dtype . bit_length ( ) 
return sig [ ( width - howMany ) : ] . _concat ( sig [ : ( width - howMany ) ] ) 
~~ def sll ( sig , howMany ) -> RtlSignalBase : 
return sig [ ( width - howMany ) : ] . _concat ( vec ( 0 , howMany ) ) 
~~ def log2ceil ( x ) : 
if not isinstance ( x , ( int , float ) ) : 
~~~ x = int ( x ) 
~~ if x == 0 or x == 1 : 
~~~ res = 1 
~~~ res = math . ceil ( math . log2 ( x ) ) 
~~ return hInt ( res ) 
~~ def isPow2 ( num ) -> bool : 
if not isinstance ( num , int ) : 
~~~ num = int ( num ) 
~~ return num != 0 and ( ( num & ( num - 1 ) ) == 0 ) 
~~ def addCases ( self , tupesValStmnts ) : 
s = self 
for val , statements in tupesValStmnts : 
~~~ s = s . Case ( val , statements ) 
~~ return s 
~~ def Case ( self , caseVal , * statements ) : 
assert self . parentStm is None 
caseVal = toHVal ( caseVal , self . switchOn . _dtype ) 
assert isinstance ( caseVal , Value ) , caseVal 
assert caseVal not in self . _case_value_index , ( 
self . rank += 1 
case = [ ] 
self . _case_value_index [ caseVal ] = len ( self . cases ) 
self . cases . append ( ( caseVal , case ) ) 
cond = self . switchOn . _eq ( caseVal ) 
self . _inputs . append ( cond ) 
cond . endpoints . append ( self ) 
self . _register_stements ( statements , case ) 
~~ def Default ( self , * statements ) : 
self . default = [ ] 
self . _register_stements ( statements , self . default ) 
~~ def Trans ( self , stateFrom , * condAndNextState ) : 
top = [ ] 
last = True 
for cAndS in reversed ( condAndNextState ) : 
~~~ if last is True : 
~~~ last = False 
~~~ condition , newvalue = cAndS 
~~~ top = self . stateReg ( cAndS ) 
~~ top = [ ] 
~~ top = If ( condition , 
self . stateReg ( newvalue ) 
top 
~~ if stateFrom is None : 
~~~ return Switch . Default ( self , top ) 
~~~ return Switch . Case ( self , stateFrom , top ) 
~~ ~~ def vcdTypeInfoForHType ( t ) -> Tuple [ str , int , Callable [ [ RtlSignalBase , Value ] , str ] ] : 
if isinstance ( t , ( SimBitsT , Bits , HBool ) ) : 
~~~ return ( VCD_SIG_TYPE . WIRE , t . bit_length ( ) , vcdBitsFormatter ) 
~~ elif isinstance ( t , HEnum ) : 
~~~ return ( VCD_SIG_TYPE . REAL , 1 , vcdEnumFormatter ) 
~~~ raise ValueError ( t ) 
~~ ~~ def vcdRegisterInterfaces ( self , obj : Union [ Interface , Unit ] , 
parent : Optional [ VcdVarWritingScope ] ) : 
if hasattr ( obj , "_interfaces" ) and obj . _interfaces : 
~~~ name = obj . _name 
parent_ = self . vcdWriter if parent is None else parent 
subScope = parent_ . varScope ( name ) 
self . _obj2scope [ obj ] = subScope 
with subScope : 
~~~ for chIntf in obj . _interfaces : 
~~~ self . vcdRegisterInterfaces ( chIntf , subScope ) 
~~ if isinstance ( obj , ( Unit , SimModel ) ) : 
~~~ for u in obj . _units : 
~~~ self . vcdRegisterInterfaces ( u , subScope ) 
~~ ~~ ~~ return subScope 
~~~ t = obj . _dtype 
if isinstance ( t , self . supported_type_classes ) : 
~~~ tName , width , formatter = vcdTypeInfoForHType ( t ) 
~~~ parent . addVar ( obj , getSignalName ( obj ) , 
tName , width , formatter ) 
~~ except VarAlreadyRegistered : 
~~ ~~ ~~ ~~ def beforeSim ( self , simulator , synthesisedUnit ) : 
vcd = self . vcdWriter 
vcd . date ( datetime . now ( ) ) 
vcd . timescale ( 1 ) 
self . vcdRegisterInterfaces ( synthesisedUnit , None ) 
self . vcdRegisterRemainingSignals ( synthesisedUnit ) 
vcd . enddefinitions ( ) 
~~ def logChange ( self , nowTime , sig , nextVal ) : 
~~~ self . vcdWriter . logChange ( nowTime , sig , nextVal ) 
~~ ~~ def HWProcess ( cls , proc , ctx ) : 
body = proc . statements 
childCtx = ctx . withIndent ( ) 
statemets = [ cls . asHdl ( s , childCtx ) for s in body ] 
proc . name = ctx . scope . checkedName ( proc . name , proc ) 
return cls . methodTmpl . render ( 
indent = getIndent ( ctx . indent ) , 
name = proc . name , 
statements = statemets 
~~ def setLevel ( self , lvl ) : 
while len ( self ) != lvl : 
~~~ if len ( self ) > lvl : 
~~~ self . pop ( ) 
~~~ self . append ( NameScopeItem ( len ( self ) ) ) 
~~ ~~ ~~ def _size ( self ) : 
assert isinstance ( self , Value ) 
return int ( self . val [ 0 ] ) - int ( self . val [ 1 ] ) 
~~ def autoAddAgents ( unit ) : 
proc = [ ] 
for intf in unit . _interfaces : 
~~~ if not intf . _isExtern : 
~~ intf . _initSimAgent ( ) 
assert intf . _ag is not None , intf 
agents = [ intf . _ag , ] 
if intf . _direction == INTF_DIRECTION . MASTER : 
~~~ agProcs = list ( map ( lambda a : a . getMonitors ( ) , agents ) ) 
~~ elif intf . _direction == INTF_DIRECTION . SLAVE : 
~~~ agProcs = list ( map ( lambda a : a . getDrivers ( ) , agents ) ) 
intf . _direction , intf ) ) 
~~ for p in agProcs : 
~~~ proc . extend ( p ) 
~~ ~~ return proc 
~~ def valuesToInts ( values ) : 
append = res . append 
for d in values : 
~~~ if isinstance ( d , int ) : 
~~~ append ( d ) 
~~~ append ( valToInt ( d ) ) 
~~ def _getAssociatedRst ( self ) : 
a = self . _associatedRst 
~~~ return a 
~~ p = self . _parent 
assert p is not None 
if isinstance ( p , UnitBase ) : 
~~~ return getRst ( p ) 
~~~ return p . _getAssociatedRst ( ) 
~~ ~~ def _getAssociatedClk ( self ) : 
a = self . _associatedClk 
~~~ return getClk ( p ) 
~~~ return p . _getAssociatedClk ( ) 
~~ ~~ def Architecture_var ( cls , v , serializerVars , extraTypes , 
extraTypes_serialized , ctx , childCtx ) : 
v . name = ctx . scope . checkedName ( v . name , v ) 
serializedVar = cls . SignalItem ( v , childCtx , declaration = True ) 
serializerVars . append ( serializedVar ) 
~~ def distinctBy ( iterable , fn ) : 
s = set ( ) 
~~~ r = fn ( i ) 
if r not in s : 
~~~ s . add ( r ) 
yield i 
~~ ~~ ~~ def single ( iterable , fn ) : 
ret = None 
~~~ if fn ( i ) : 
~~~ if found : 
~~~ raise DuplicitValueExc ( i ) 
~~ found = True 
ret = i 
~~~ raise NoValueExc ( ) 
~~ def take ( iterrable , howMay ) : 
assert howMay >= 0 
if not howMay : 
~~ last = howMay - 1 
for i , item in enumerate ( iterrable ) : 
if i == last : 
~~ ~~ ~~ def iter_with_last ( iterable ) : 
iterable = iter ( iterable ) 
prev = next ( iterable ) 
for item in iterable : 
~~~ yield False , prev 
prev = item 
~~ yield True , prev 
~~ def groupedby ( collection , fn ) : 
d = { } 
for item in collection : 
~~~ k = fn ( item ) 
~~~ arr = d [ k ] 
~~~ arr = [ ] 
d [ k ] = arr 
~~ arr . append ( item ) 
~~ yield from d . items ( ) 
~~ def flatten ( iterables , level = inf ) : 
if level >= 0 and isinstance ( iterables , ( list , tuple , GeneratorType , 
map , zip ) ) : 
~~~ level -= 1 
for i in iterables : 
~~~ yield from flatten ( i , level = level ) 
~~~ yield iterables 
~~ ~~ def _cut_off_drivers_of ( self , sig : RtlSignalBase ) : 
if len ( self . _outputs ) == 1 and sig in self . _outputs : 
~~~ self . parentStm = None 
~~ child_keep_mask = [ ] 
newIfTrue = [ ] 
all_cut_off = True 
all_cut_off &= self . _cut_off_drivers_of_list ( 
sig , self . ifTrue , child_keep_mask , newIfTrue ) 
self . ifTrue = list ( compress ( self . ifTrue , child_keep_mask ) ) 
newElifs = [ ] 
anyElifHit = False 
for cond , stms in self . elIfs : 
~~~ newCase = [ ] 
child_keep_mask . clear ( ) 
sig , stms , child_keep_mask , newCase ) 
_stms = list ( compress ( stms , child_keep_mask ) ) 
stms . clear ( ) 
stms . extend ( _stms ) 
if newCase : 
~~~ anyElifHit = True 
~~ newElifs . append ( ( cond , newCase ) ) 
~~ newIfFalse = None 
if self . ifFalse : 
~~~ newIfFalse = [ ] 
sig , self . ifFalse , child_keep_mask , newIfFalse ) 
self . ifFalse = list ( compress ( self . ifFalse , child_keep_mask ) ) 
if newIfTrue or newIfFalse or anyElifHit or newIfFalse : 
~~~ cond_sig = self . cond 
n = self . __class__ ( cond_sig , newIfTrue ) 
for c , stms in newElifs : 
~~~ assert len ( c ) == 1 
c_sig = c [ 0 ] 
n . Elif ( c_sig , stms ) 
~~ if newIfFalse is not None : 
~~~ n . Else ( newIfFalse ) 
~~ if self . parentStm is None : 
~~~ ctx = n . _get_rtl_context ( ) 
ctx . statements . add ( n ) 
~~ self . _inputs . clear ( ) 
self . _inputs . append ( cond_sig ) 
for c , _ in self . elIfs : 
~~~ self . _inputs . extend ( c ) 
~~ self . _inputs . append ( cond_sig ) 
self . _outputs . clear ( ) 
out_add = self . _outputs . append 
in_add = self . _inputs . append 
for stm in self . _iter_stms ( ) : 
~~~ for inp in stm . _inputs : 
~~~ in_add ( inp ) 
~~ for outp in stm . _outputs : 
~~~ out_add ( outp ) 
~~ ~~ if self . _sensitivity is not None or self . _enclosed_for is not None : 
~~ return n 
~~ ~~ def _discover_enclosure ( self ) : 
outputs = self . _outputs 
self . _ifTrue_enclosed_for = self . _discover_enclosure_for_statements ( 
self . ifTrue , outputs ) 
elif_encls = self . _elIfs_enclosed_for = [ ] 
for _ , stms in self . elIfs : 
~~~ e = self . _discover_enclosure_for_statements ( 
stms , outputs ) 
elif_encls . append ( e ) 
~~ self . _ifFalse_enclosed_for = self . _discover_enclosure_for_statements ( 
self . ifFalse , outputs ) 
assert self . _enclosed_for is None 
encl = self . _enclosed_for = set ( ) 
for s in self . _ifTrue_enclosed_for : 
~~~ enclosed = True 
for elif_e in elif_encls : 
~~~ if s not in elif_e : 
~~~ enclosed = False 
~~ ~~ if enclosed and s in self . _ifFalse_enclosed_for : 
~~~ encl . add ( s ) 
~~ ~~ ~~ def _discover_sensitivity ( self , seen : set ) -> None : 
assert self . _sensitivity is None , self 
ctx = self . _sensitivity = SensitivityCtx ( ) 
self . _discover_sensitivity_sig ( self . cond , seen , ctx ) 
if ctx . contains_ev_dependency : 
~~ for stm in self . ifTrue : 
~~~ stm . _discover_sensitivity ( seen ) 
ctx . extend ( stm . _sensitivity ) 
~~ for cond , stms in self . elIfs : 
~~~ if ctx . contains_ev_dependency : 
~~ self . _discover_sensitivity_sig ( cond , seen , ctx ) 
~~ for stm in stms : 
~~ stm . _discover_sensitivity ( seen ) 
~~ ~~ if not ctx . contains_ev_dependency and self . ifFalse : 
~~~ for stm in self . ifFalse : 
~~ ~~ def _iter_stms ( self ) : 
yield from self . ifTrue 
~~~ yield from stms 
~~ if self . ifFalse is not None : 
~~~ yield from self . ifFalse 
~~ ~~ def _try_reduce ( self ) -> Tuple [ bool , List [ HdlStatement ] ] : 
io_change = False 
self . ifTrue , rank_decrease , _io_change = self . _try_reduce_list ( 
self . ifTrue ) 
self . rank -= rank_decrease 
io_change |= _io_change 
new_elifs = [ ] 
for cond , statements in self . elIfs : 
~~~ _statements , rank_decrease , _io_change = self . _try_reduce_list ( 
statements ) 
new_elifs . append ( ( cond , _statements ) ) 
~~~ self . ifFalse , rank_decrease , _io_update_required = self . _try_reduce_list ( 
self . ifFalse ) 
~~ reduce_self = not self . condHasEffect ( 
self . ifTrue , self . ifFalse , self . elIfs ) 
if reduce_self : 
~~~ res = self . ifTrue 
~~~ res = [ self , ] 
~~ self . _on_reduce ( reduce_self , io_change , res ) 
if self . ifFalse is not None and len ( self . ifFalse ) == 1 : 
~~~ child = self . ifFalse [ 0 ] 
if isinstance ( child , IfContainer ) : 
~~~ self . _merge_nested_if_from_else ( child ) 
~~ ~~ return res , io_change 
~~ def _merge_nested_if_from_else ( self , ifStm : "IfContainer" ) : 
self . elIfs . append ( ( ifStm . cond , ifStm . ifTrue ) ) 
self . elIfs . extend ( ifStm . elIfs ) 
self . ifFalse = ifStm . ifFalse 
~~ def _merge_with_other_stm ( self , other : "IfContainer" ) -> None : 
merge = self . _merge_statement_lists 
self . ifTrue = merge ( self . ifTrue , other . ifTrue ) 
for ( ( c , elifA ) , ( _ , elifB ) ) in zip ( self . elIfs , other . elIfs ) : 
~~~ new_elifs . append ( ( c , merge ( elifA , elifB ) ) ) 
~~ self . elIfs = new_elifs 
self . ifFalse = merge ( self . ifFalse , other . ifFalse ) 
other . ifTrue = [ ] 
other . elIfs = [ ] 
other . ifFalse = None 
self . _on_merge ( other ) 
~~ def isSame ( self , other : HdlStatement ) -> bool : 
if self is other : 
~~ if self . rank != other . rank : 
~~ if isinstance ( other , IfContainer ) : 
~~~ if self . cond is other . cond : 
~~~ if len ( self . ifTrue ) == len ( other . ifTrue ) and len ( self . ifFalse ) == len ( other . ifFalse ) and len ( self . elIfs ) == len ( other . elIfs ) : 
~~~ if not isSameStatementList ( self . ifTrue , 
other . ifTrue ) or not isSameStatementList ( self . ifFalse , 
other . ifFalse ) : 
~~ for ( ac , astms ) , ( bc , bstms ) in zip ( self . elIfs , 
other . elIfs ) : 
~~~ if not ( ac == bc ) or not isSameStatementList ( astms , bstms ) : 
~~ ~~ ~~ return False 
~~ def removeUnconnectedSignals ( netlist ) : 
toDelete = set ( ) 
toSearch = netlist . signals 
while toSearch : 
~~~ _toSearch = set ( ) 
for sig in toSearch : 
~~~ if not sig . endpoints : 
~~~ if sig . _interface is not None : 
~~ ~~ except AttributeError : 
~~ for e in sig . drivers : 
~~~ if isinstance ( e , Operator ) : 
~~~ inputs = e . operands 
if e . result is sig : 
~~~ e . result = None 
~~~ inputs = e . _inputs 
netlist . statements . discard ( e ) 
~~ for op in inputs : 
~~~ if not isinstance ( op , Value ) : 
~~~ op . endpoints . remove ( e ) 
~~ _toSearch . add ( op ) 
~~ ~~ ~~ toDelete . add ( sig ) 
~~ ~~ if toDelete : 
~~~ for sig in toDelete : 
~~~ if sig . ctx == netlist : 
~~~ netlist . signals . remove ( sig ) 
~~ _toSearch . discard ( sig ) 
~~ toDelete = set ( ) 
~~ toSearch = _toSearch 
~~ ~~ def checkIfIsTooSimple ( proc ) : 
~~~ a , = proc . statements 
if isinstance ( a , Assignment ) : 
~~ def tryToMerge ( procA : HWProcess , procB : HWProcess ) : 
if ( checkIfIsTooSimple ( procA ) or 
checkIfIsTooSimple ( procB ) or 
areSetsIntersets ( procA . outputs , procB . sensitivityList ) or 
areSetsIntersets ( procB . outputs , procA . sensitivityList ) or 
not HdlStatement . _is_mergable_statement_list ( procA . statements , procB . statements ) ) : 
~~~ raise IncompatibleStructure ( ) 
~~ procA . statements = HdlStatement . _merge_statement_lists ( 
procA . statements , procB . statements ) 
procA . outputs . extend ( procB . outputs ) 
procA . inputs . extend ( procB . inputs ) 
procA . sensitivityList . extend ( procB . sensitivityList ) 
return procA 
~~ def reduceProcesses ( processes ) : 
processes . sort ( key = lambda x : ( x . name , maxStmId ( x ) ) , reverse = True ) 
for _ , procs in groupedby ( processes , lambda p : p . rank ) : 
~~~ for iA , pA in enumerate ( procs ) : 
~~~ if pA is None : 
~~ for iB , pB in enumerate ( islice ( procs , iA + 1 , None ) ) : 
~~~ if pB is None : 
~~~ pA = tryToMerge ( pA , pB ) 
~~ except IncompatibleStructure : 
~~ procs [ iA + 1 + iB ] = None 
~~ ~~ for p in procs : 
~~~ if p is not None : 
~~ ~~ ~~ ~~ def onWriteReq ( self , sim , addr , data ) : 
self . requests . append ( ( WRITE , addr , data ) ) 
~~ def asHdl ( cls , obj , ctx : HwtSerializerCtx ) : 
if isinstance ( obj , RtlSignalBase ) : 
~~~ return cls . SignalItem ( obj , ctx ) 
~~ elif isinstance ( obj , Value ) : 
~~~ return cls . Value ( obj , ctx ) 
~~~ serFn = obj . asHwt 
~~~ serFn = None 
~~ if serFn is not None : 
~~~ return serFn ( cls , ctx ) 
~~~ serFn = getattr ( cls , obj . __class__ . __name__ ) 
~~~ return serFn ( obj , ctx ) 
~~ ~~ def Entity ( cls , ent : Entity , ctx : HwtSerializerCtx ) : 
cls . Entity_prepare ( ent , ctx , serialize = False ) 
ent . name = ctx . scope . checkedName ( ent . name , ent , isGlobal = True ) 
ports = list ( 
map ( lambda p : ( p . name , cls . HdlType ( p . _dtype , ctx ) ) , 
ent . ports ) ) 
return unitHeadTmpl . render ( 
name = ent . name , 
ports = ports , 
~~ def toRtl ( unitOrCls : Unit , name : str = None , 
serializer : GenericSerializer = VhdlSerializer , 
targetPlatform = DummyPlatform ( ) , saveTo : str = None ) : 
if not isinstance ( unitOrCls , Unit ) : 
~~~ u = unitOrCls ( ) 
~~~ u = unitOrCls 
~~ u . _loadDeclarations ( ) 
if name is not None : 
~~~ assert isinstance ( name , str ) 
u . _name = name 
~~ globScope = serializer . getBaseNameScope ( ) 
mouduleScopes = { } 
serializedClasses = { } 
serializedConfiguredUnits = { } 
doSerialize = True 
createFiles = saveTo is not None 
if createFiles : 
~~~ os . makedirs ( saveTo , exist_ok = True ) 
files = UniqList ( ) 
~~~ codeBuff = [ ] 
~~ for obj in u . _toRtl ( targetPlatform ) : 
~~~ doSerialize = serializer . serializationDecision ( 
obj , 
serializedClasses , 
serializedConfiguredUnits ) 
if doSerialize : 
~~~ if isinstance ( obj , Entity ) : 
~~~ s = globScope . fork ( 1 ) 
s . setLevel ( 2 ) 
ctx = serializer . getBaseContext ( ) 
ctx . scope = s 
mouduleScopes [ obj ] = ctx 
ctx . currentUnit = obj . origin 
sc = serializer . Entity ( obj , ctx ) 
~~~ fName = obj . name + serializer . fileExtension 
fileMode = 'w' 
~~ ~~ elif isinstance ( obj , Architecture ) : 
~~~ ctx = mouduleScopes [ obj . entity ] 
~~~ raise SerializerException ( 
% ( obj . getEntityName ( ) ) ) 
~~ sc = serializer . Architecture ( obj , ctx ) 
~~~ fName = obj . getEntityName ( ) + serializer . fileExtension 
fileMode = 'a' 
~~~ if hasattr ( obj , "_hdlSources" ) : 
~~~ for fn in obj . _hdlSources : 
~~~ if isinstance ( fn , str ) : 
~~~ shutil . copy2 ( fn , saveTo ) 
files . append ( fn ) 
~~~ sc = serializer . asHdl ( obj ) 
~~ ~~ if sc : 
~~~ if createFiles : 
~~~ fp = os . path . join ( saveTo , fName ) 
files . append ( fp ) 
with open ( fp , fileMode ) as f : 
~~~ if fileMode == 'a' : 
~~~ f . write ( "\\n" ) 
~~ f . write ( 
serializer . formatter ( sc ) 
~~~ codeBuff . append ( sc ) 
~~ ~~ ~~ elif not createFiles : 
~~~ name = \ % obj . name 
~~~ name = "" 
~~ codeBuff . append ( serializer . comment ( 
obj . __class__ . __name__ , name ) ) ) 
~~ ~~ if createFiles : 
~~~ return files 
~~~ return serializer . formatter ( 
"\\n" . join ( codeBuff ) 
~~ ~~ def name_for_process_and_mark_outputs ( statements : List [ HdlStatement ] ) -> str : 
out_names = [ ] 
~~~ for sig in stm . _outputs : 
~~~ if not sig . hasGenericName : 
~~~ out_names . append ( sig . name ) 
~~ ~~ ~~ if out_names : 
~~~ return min ( out_names ) 
~~ ~~ def cut_off_drivers_of ( dstSignal , statements ) : 
separated = [ ] 
stm_filter = [ ] 
~~~ stm . _clean_signal_meta ( ) 
d = stm . _cut_off_drivers_of ( dstSignal ) 
if d is not None : 
~~~ separated . append ( d ) 
~~ f = d is not stm 
stm_filter . append ( f ) 
~~ return list ( compress ( statements , stm_filter ) ) , separated 
~~ def statements_to_HWProcesses ( statements : List [ HdlStatement ] ) -> Generator [ HWProcess , None , None ] : 
statements = copy ( statements ) 
while statements : 
~~~ stm = statements . pop ( ) 
proc_statements = [ stm , ] 
ps = _statements_to_HWProcesses ( proc_statements , True ) 
processes . extend ( ps ) 
~~ yield from reduceProcesses ( processes ) 
~~ def markVisibilityOfSignals ( ctx , ctxName , signals , interfaceSignals ) : 
for sig in signals : 
~~~ driver_cnt = len ( sig . drivers ) 
has_comb_driver = False 
if driver_cnt > 1 : 
~~~ sig . hidden = False 
for d in sig . drivers : 
~~~ if not isinstance ( d , Operator ) : 
~~ is_comb_driver = False 
if isinstance ( d , PortItem ) : 
~~~ is_comb_driver = True 
~~ elif not d . _now_is_event_dependent : 
~~~ for a in walk_assignments ( d , sig ) : 
~~~ if not a . indexes and not a . _is_completly_event_dependent : 
~~ ~~ ~~ if has_comb_driver and is_comb_driver : 
~~~ raise MultipleDriversErr ( 
( ctx . getDebugScopeName ( ) , sig ) ) 
~~ has_comb_driver |= is_comb_driver 
~~ ~~ elif driver_cnt == 1 : 
~~~ if not isinstance ( sig . drivers [ 0 ] , Operator ) : 
if sig not in interfaceSignals : 
~~~ if not sig . defVal . _isFullVld ( ) : 
~~~ raise NoDriverErr ( 
~~ sig . _const = True 
~~ ~~ ~~ ~~ def sig ( self , name , dtype = BIT , clk = None , syncRst = None , defVal = None ) : 
if isinstance ( defVal , RtlSignal ) : 
_defVal = defVal . _auto_cast ( dtype ) 
~~ elif isinstance ( defVal , Value ) : 
~~~ _defVal = defVal . _auto_cast ( dtype ) 
~~ elif isinstance ( defVal , InterfaceBase ) : 
~~~ _defVal = defVal . _sig 
~~~ _defVal = dtype . fromPy ( defVal ) 
~~ if clk is not None : 
~~~ s = RtlSyncSignal ( self , name , dtype , _defVal ) 
if syncRst is not None and defVal is None : 
~~~ raise SigLvlConfErr ( 
~~ if syncRst is not None : 
~~~ r = If ( syncRst . _isOn ( ) , 
RtlSignal . __call__ ( s , _defVal ) 
RtlSignal . __call__ ( s , s . next ) 
~~~ r = [ RtlSignal . __call__ ( s , s . next ) ] 
~~ If ( clk . _onRisingEdge ( ) , 
r 
~~~ if syncRst : 
~~ s = RtlSignal ( self , name , dtype , defVal = _defVal ) 
~~ self . signals . add ( s ) 
~~ def synthesize ( self , name , interfaces , targetPlatform ) : 
ent = Entity ( name ) 
for _ , v in self . params . items ( ) : 
~~~ ent . generics . append ( v ) 
~~ if isinstance ( interfaces , set ) : 
~~~ intfSet = interfaces 
~~~ intfSet = set ( interfaces ) 
~~ for s in interfaces : 
~~~ pi = portItemfromSignal ( s , ent ) 
pi . registerInternSig ( s ) 
ent . ports . append ( pi ) 
s . hidden = False 
~~ removeUnconnectedSignals ( self ) 
markVisibilityOfSignals ( self , name , self . signals , intfSet ) 
for proc in targetPlatform . beforeHdlArchGeneration : 
~~~ proc ( self ) 
~~ arch = Architecture ( ent ) 
for p in statements_to_HWProcesses ( self . statements ) : 
~~~ arch . processes . append ( p ) 
~~ for s in self . signals : 
~~~ if s not in intfSet and not s . hidden : 
~~~ arch . variables . append ( s ) 
~~ ~~ for u in self . subUnits : 
~~~ arch . componentInstances . append ( u ) 
~~ for su in distinctBy ( self . subUnits , lambda x : x . name ) : 
~~~ arch . components . append ( su ) 
~~ self . synthesised = True 
return [ ent , arch ] 
~~ def toHVal ( op : Any , suggestedType : Optional [ HdlType ] = None ) : 
if isinstance ( op , Value ) or isinstance ( op , SignalItem ) : 
~~~ return op 
~~ elif isinstance ( op , InterfaceBase ) : 
~~~ return op . _sig 
~~~ if isinstance ( op , int ) : 
~~~ if suggestedType is not None : 
~~~ return suggestedType . fromPy ( op ) 
~~ if op >= 1 << 31 : 
~~ elif op < - ( 1 << 31 ) : 
~~~ hType = defaultPyConversions [ type ( op ) ] 
~~~ hType = None 
~~ if hType is None : 
~~ return hType . fromPy ( op ) 
~~ ~~ def Value ( cls , val , ctx : SerializerCtx ) : 
t = val . _dtype 
if isinstance ( val , RtlSignalBase ) : 
~~~ return cls . SignalItem ( val , ctx ) 
~~ c = cls . Value_try_extract_as_const ( val , ctx ) 
if c : 
~~~ return c 
~~ if isinstance ( t , Slice ) : 
~~~ return cls . Slice_valAsHdl ( t , val , ctx ) 
~~ elif isinstance ( t , HArray ) : 
~~~ return cls . HArrayValAsHdl ( t , val , ctx ) 
~~ elif isinstance ( t , Bits ) : 
~~~ return cls . Bits_valAsHdl ( t , val , ctx ) 
~~ elif isinstance ( t , HBool ) : 
~~~ return cls . Bool_valAsHdl ( t , val , ctx ) 
~~~ return cls . HEnumValAsHdl ( t , val , ctx ) 
~~ elif isinstance ( t , Integer ) : 
~~~ return cls . Integer_valAsHdl ( t , val , ctx ) 
~~ elif isinstance ( t , String ) : 
~~~ return cls . String_valAsHdl ( t , val , ctx ) 
% ( val ) ) 
~~ ~~ def getMaxStmIdForStm ( stm ) : 
maxId = 0 
if isinstance ( stm , Assignment ) : 
~~~ return stm . _instId 
~~ elif isinstance ( stm , WaitStm ) : 
~~~ return maxId 
~~~ for _stm in stm . _iter_stms ( ) : 
~~~ maxId = max ( maxId , getMaxStmIdForStm ( _stm ) ) 
~~ return maxId 
~~ ~~ def maxStmId ( proc ) : 
for stm in proc . statements : 
~~~ maxId = max ( maxId , getMaxStmIdForStm ( stm ) ) 
~~ def monitor ( self , sim ) : 
if self . notReset ( sim ) and self . _enabled : 
~~~ self . wrRd ( sim . write , 1 ) 
yield sim . waitOnCombUpdate ( ) 
d = self . doRead ( sim ) 
self . data . append ( d ) 
~~~ self . wrRd ( sim . write , 0 ) 
~~ ~~ def doWrite ( self , sim , data ) : 
sim . write ( data , self . intf . data ) 
~~ def driver ( self , sim ) : 
r = sim . read 
if self . actualData is NOP and self . data : 
~~~ self . actualData = self . data . popleft ( ) 
~~ do = self . actualData is not NOP 
if do : 
~~~ self . doWrite ( sim , self . actualData ) 
~~~ self . doWrite ( sim , None ) 
~~ en = self . notReset ( sim ) and self . _enabled 
if not ( en and do ) : 
~~ yield sim . waitOnCombUpdate ( ) 
rd = self . isRd ( r ) 
if en : 
~~~ assert rd . vldMask , ( 
( sim . now , self . intf ) ) 
~~ if rd . val : 
~~~ if self . _debugOutput is not None : 
self . intf . _getFullName ( ) , 
sim . now , self . actualData ) ) 
~~ if self . data : 
~~~ self . actualData = NOP 
~~ ~~ ~~ def fromPy ( cls , val , typeObj , vldMask = None ) : 
assert isinstance ( typeObj , Integer ) 
vld = int ( val is not None ) 
if not vld : 
~~~ assert vldMask is None or vldMask == 0 
~~~ if vldMask == 0 : 
~~~ val = False 
vld = 0 
~~~ val = int ( val ) 
~~ ~~ return cls ( val , typeObj , vld ) 
~~ def _m ( self ) : 
self . _direction = DIRECTION . asIntfDirection ( DIRECTION . opposite ( self . _masterDir ) ) 
~~ def _loadDeclarations ( self ) : 
if not hasattr ( self , "_interfaces" ) : 
~~~ self . _interfaces = [ ] 
~~ self . _setAttrListener = self . _declrCollector 
self . _declr ( ) 
self . _setAttrListener = None 
for i in self . _interfaces : 
~~~ i . _isExtern = self . _isExtern 
i . _loadDeclarations ( ) 
~~ for p in self . _params : 
~~~ p . setReadOnly ( ) 
~~ if self . _isExtern : 
~~~ if self . _direction == INTF_DIRECTION . UNKNOWN : 
~~~ self . _direction = INTF_DIRECTION . MASTER 
~~ self . _setDirectionsLikeIn ( self . _direction ) 
~~ ~~ def _clean ( self , rmConnetions = True , lockNonExternal = True ) : 
if self . _interfaces : 
~~~ for i in self . _interfaces : 
~~~ i . _clean ( rmConnetions = rmConnetions , 
lockNonExternal = lockNonExternal ) 
~~~ self . _sigInside = self . _sig 
del self . _sig 
~~ if lockNonExternal and not self . _isExtern : 
~~~ self . _isAccessible = False 
~~ ~~ def _signalsForInterface ( self , context , prefix = '' , typeTransform = None ) : 
sigs = [ ] 
~~~ for intf in self . _interfaces : 
~~~ sigs . extend ( 
intf . _signalsForInterface ( context , prefix , 
typeTransform = typeTransform ) ) 
~~~ if hasattr ( self , '_sig' ) : 
~~~ sigs = [ self . _sig ] 
~~~ t = self . _dtype 
if typeTransform is not None : 
~~~ t = typeTransform ( t ) 
~~ s = context . sig ( prefix + self . _getPhysicalName ( ) , t ) 
s . _interface = self 
self . _sig = s 
if hasattr ( self , '_boundedEntityPort' ) : 
~~~ self . _boundedEntityPort . connectSig ( self . _sig ) 
~~ sigs = [ s ] 
~~ ~~ return sigs 
~~ def _getPhysicalName ( self ) : 
if hasattr ( self , "_boundedEntityPort" ) : 
~~~ return self . _boundedEntityPort . name 
~~~ return self . _getFullName ( ) . replace ( '.' , self . _NAME_SEPARATOR ) 
~~ ~~ def _replaceParam ( self , p , newP ) : 
i = self . _params . index ( p ) 
pName = p . _scopes [ self ] [ 1 ] 
assert i > - 1 
self . _params [ i ] = newP 
newP . _registerScope ( pName , self ) 
object . __setattr__ ( self , pName , newP ) 
~~ def _updateParamsFrom ( self , otherObj , updater = _default_param_updater , 
exclude = None , prefix = "" ) : 
PropDeclrCollector . _updateParamsFrom ( self , otherObj , updater , exclude , prefix ) 
~~ def _bit_length ( self ) : 
~~~ interfaces = self . _interfaces 
~~~ interfaces = None 
~~ if interfaces is None : 
~~~ _intf = self . _clone ( ) 
_intf . _loadDeclarations ( ) 
interfaces = _intf . _interfaces 
~~ if interfaces : 
~~~ w = 0 
for i in interfaces : 
~~~ w += i . _bit_length ( ) 
~~~ return self . _dtype . bit_length ( ) 
~~ ~~ def _connectTo ( self , master , exclude = None , fit = False ) : 
return list ( self . _connectToIter ( master , exclude , fit ) ) 
~~ def sensitivityByOp ( op ) : 
if op == AllOps . RISING_EDGE : 
~~~ return SENSITIVITY . RISING 
~~ elif op == AllOps . FALLING_EDGE : 
~~~ return SENSITIVITY . FALLING 
~~~ raise TypeError ( ) 
~~ ~~ def eval ( self , operator , simulator = None ) : 
def getVal ( v ) : 
~~~ while not isinstance ( v , Value ) : 
~~~ v = v . _val 
~~ return v 
~~ operands = list ( map ( getVal , operator . operands ) ) 
if isEventDependentOp ( operator . operator ) : 
~~~ operands . append ( simulator . now ) 
~~ elif operator . operator == AllOps . IntToBits : 
~~~ operands . append ( operator . result . _dtype ) 
~~ return self . _evalFn ( * operands ) 
~~ def convertBits ( self , sigOrVal , toType ) : 
if isinstance ( sigOrVal , Value ) : 
~~~ return convertBits__val ( self , sigOrVal , toType ) 
~~ elif isinstance ( toType , HBool ) : 
~~~ if self . bit_length ( ) == 1 : 
~~~ v = 0 if sigOrVal . _dtype . negated else 1 
return sigOrVal . _eq ( self . getValueCls ( ) . fromPy ( v , self ) ) 
~~ ~~ elif isinstance ( toType , Bits ) : 
~~~ if self . bit_length ( ) == toType . bit_length ( ) : 
~~~ return sigOrVal . _convSign ( toType . signed ) 
~~ ~~ elif toType == INT : 
~~~ return Operator . withRes ( AllOps . BitsToInt , [ sigOrVal ] , toType ) 
~~ return default_auto_cast_fn ( self , sigOrVal , toType ) 
~~ def reinterpret_bits_to_hstruct ( sigOrVal , hStructT ) : 
container = hStructT . fromPy ( None ) 
for f in hStructT . fields : 
~~~ t = f . dtype 
width = t . bit_length ( ) 
if f . name is not None : 
~~~ s = sigOrVal [ ( width + offset ) : offset ] 
s = s . _reinterpret_cast ( t ) 
setattr ( container , f . name , s ) 
~~ offset += width 
~~ return container 
~~ def reinterpretBits ( self , sigOrVal , toType ) : 
~~~ return reinterpretBits__val ( self , sigOrVal , toType ) 
~~ elif isinstance ( toType , Bits ) : 
~~~ return fitTo_t ( sigOrVal , toType ) 
~~ elif sigOrVal . _dtype . bit_length ( ) == toType . bit_length ( ) : 
~~~ if isinstance ( toType , HStruct ) : 
~~~ raise reinterpret_bits_to_hstruct ( sigOrVal , toType ) 
~~ elif isinstance ( toType , HUnion ) : 
~~ elif isinstance ( toType , HArray ) : 
~~~ reinterpret_bits_to_harray ( sigOrVal , toType ) 
~~ ~~ return default_auto_cast_fn ( self , sigOrVal , toType ) 
~~ def iterSort ( iterators , cmpFn ) : 
actual = [ ] 
_iterators = [ ] 
for i , it in enumerate ( iterators ) : 
~~~ a = next ( it ) 
_iterators . append ( ( i , it ) ) 
actual . append ( a ) 
~~~ if not _iterators : 
~~ elif len ( _iterators ) == 1 : 
~~~ originIndex , it = _iterators [ 0 ] 
yield originIndex , actual [ 0 ] 
for item in it : 
~~~ yield originIndex , item 
~~ minimum = None 
minimumIndex = None 
secondMin = None 
for i , val in enumerate ( actual ) : 
~~~ skipSecMinCheck = False 
if minimum is None : 
~~~ minimum = val 
minimumIndex = i 
~~ elif cmpFn ( val , minimum ) : 
~~~ secondMin = minimum 
minimum = val 
skipSecMinCheck = True 
~~ elif not skipSecMinCheck and ( 
secondMin is None or cmpFn ( val , secondMin ) ) : 
~~~ secondMin = val 
~~ ~~ actualI , actualIt = _iterators [ minimumIndex ] 
while not cmpFn ( secondMin , minimum ) : 
~~~ yield ( actualI , minimum ) 
~~~ minimum = next ( actualIt ) 
~~~ minimum = None 
~~ ~~ if minimum is None : 
~~~ del _iterators [ minimumIndex ] 
del actual [ minimumIndex ] 
~~~ actual [ minimumIndex ] = minimum 
~~ ~~ ~~ def groupIntoChoices ( splitsOnWord , wordWidth : int , origin : OneOfTransaction ) : 
def cmpWordIndex ( a , b ) : 
~~~ return a . startOfPart // wordWidth < b . startOfPart // wordWidth 
~~ actual = None 
itCnt = len ( splitsOnWord ) 
for i , item in iterSort ( splitsOnWord , cmpWordIndex ) : 
~~~ _actualW = item . startOfPart // wordWidth 
if actual is None : 
~~~ actual = ChoicesOfFrameParts ( item . startOfPart , origin ) 
actual . extend ( 
ChoiceOfFrameParts ( actual , 
origin . possibleTransactions [ _i ] ) 
for _i in range ( itCnt ) ) 
actualW = _actualW 
~~ elif _actualW > actualW : 
~~~ actual . resolveEnd ( ) 
yield actual 
actual = ChoicesOfFrameParts ( item . startOfPart , origin ) 
~~ actual [ i ] . append ( item ) 
~~ if actual is not None : 
~~~ actual . setIsLast ( True ) 
actual . resolveEnd ( ) 
~~ ~~ def fullWordCnt ( self , start : int , end : int ) : 
assert end >= start , ( start , end ) 
gap = max ( 0 , ( end - start ) - ( start % self . wordWidth ) ) 
return gap // self . wordWidth 
~~ def groupByWordIndex ( self , transaction : 'TransTmpl' , offset : int ) : 
actualW = None 
partsInWord = [ ] 
wordWidth = self . wordWidth 
for item in self . splitOnWords ( transaction , offset ) : 
if actualW is None : 
~~~ actualW = _actualW 
partsInWord . append ( item ) 
~~~ yield ( actualW , partsInWord ) 
partsInWord = [ item , ] 
~~~ partsInWord . append ( item ) 
~~ ~~ if partsInWord : 
~~ ~~ def splitOnWords ( self , transaction , addrOffset = 0 ) : 
end = addrOffset 
for tmp in transaction . walkFlatten ( offset = addrOffset ) : 
~~~ if isinstance ( tmp , OneOfTransaction ) : 
~~~ split = [ self . splitOnWords ( ch , end ) 
for ch in tmp . possibleTransactions ] 
yield from groupIntoChoices ( split , wordWidth , tmp ) 
end = addrOffset + tmp . possibleTransactions [ 0 ] . bitAddrEnd 
~~ elif isinstance ( tmp , StreamTransaction ) : 
~~~ ch_len = tmp . child . bit_length ( ) 
if end % self . wordWidth != 0 or ch_len != self . wordWidth : 
~~~ raise NotImplementedError ( tmp ) 
~~~ s = StreamOfFramePars ( end , tmp ) 
s . extend ( self . splitOnWords ( tmp . child , end ) ) 
s . setIsLast ( True ) 
s . resolveEnd ( ) 
yield s 
end = addrOffset + tmp . child . bitAddrEnd 
~~~ ( base , end ) , tmpl = tmp 
startOfPart = base 
while startOfPart != end : 
~~~ wordIndex = startOfPart // wordWidth 
endOfWord = ( wordIndex + 1 ) * wordWidth 
endOfPart = min ( endOfWord , end ) 
inFieldOffset = startOfPart - base 
yield TransPart ( self , tmpl , startOfPart , endOfPart , 
inFieldOffset ) 
startOfPart = endOfPart 
~~ ~~ ~~ ~~ def fromPy ( cls , val , typeObj , vldMask = None ) : 
val = False 
~~~ val = bool ( val ) 
~~ def pprintInterface ( intf , prefix = "" , indent = 0 , file = sys . stdout ) : 
~~~ s = intf . _sig 
~~~ s = "" 
~~ if s is not "" : 
~~ file . write ( "" . join ( [ getIndent ( indent ) , prefix , repr ( intf . _getFullName ( ) ) , 
s ] ) ) 
file . write ( "\\n" ) 
if isinstance ( intf , HObjList ) : 
~~~ for i , p in enumerate ( intf ) : 
~~~ pprintInterface ( p , prefix = prefix , indent = indent + 1 , file = file ) 
~~~ for i in intf . _interfaces : 
~~~ pprintInterface ( i , indent = indent + 1 , file = file ) 
~~ ~~ ~~ def framesFromTransTmpl ( transaction : 'TransTmpl' , 
wordWidth : int , 
maxFrameLen : Union [ int , float ] = inf , 
maxPaddingWords : Union [ int , float ] = inf , 
trimPaddingWordsOnStart : bool = False , 
trimPaddingWordsOnEnd : bool = False ) -> Generator [ 
'FrameTmpl' , None , None ] : 
isFirstInFrame = True 
partsPending = False 
startOfThisFrame = 0 
assert maxFrameLen > 0 
assert maxPaddingWords >= 0 
if maxPaddingWords < inf : 
~~ it = TransTmplWordIterator ( wordWidth ) 
lastWordI = 0 
endOfThisFrame = maxFrameLen 
for wordI , word in it . groupByWordIndex ( transaction , 0 ) : 
~~~ if wordI * wordWidth >= endOfThisFrame : 
~~~ paddingWords = wordI - lastWordI 
if trimPaddingWordsOnEnd and paddingWords > maxPaddingWords : 
~~~ _endOfThisFrame = ( lastWordI + 1 ) * wordWidth 
~~~ _endOfThisFrame = wordI * wordWidth 
~~ yield FrameTmpl ( transaction , 
wordWidth , 
startOfThisFrame , 
_endOfThisFrame , 
parts ) 
startOfThisFrame = _endOfThisFrame 
endOfThisFrame = startOfThisFrame + maxFrameLen 
lastWordI = wordI 
~~ if ( not isFirstInFrame 
and trimPaddingWordsOnEnd 
and wordI - lastWordI > 1 ) : 
yield FrameTmpl ( transaction , 
lastWordI = wordI - 1 
~~ if isFirstInFrame : 
~~~ partsPending = True 
isFirstInFrame = False 
paddingWords = wordI - lastWordI 
if trimPaddingWordsOnStart and paddingWords > maxPaddingWords : 
~~~ startOfThisFrame += paddingWords * wordWidth 
~~ endOfThisFrame = startOfThisFrame + maxFrameLen 
~~ parts . extend ( word ) 
~~ endOfThisFrame = transaction . bitAddrEnd 
withPadding = not ( trimPaddingWordsOnEnd or trimPaddingWordsOnStart ) 
if partsPending or ( withPadding 
and endOfThisFrame != startOfThisFrame ) : 
~~~ endOfLastWord = ( lastWordI + 1 ) * wordWidth 
if endOfThisFrame < endOfLastWord : 
~~~ endOfThisFrame = endOfLastWord 
~~~ paddingWords = it . fullWordCnt ( endOfLastWord , endOfThisFrame ) 
~~~ endOfThisFrame -= paddingWords * wordWidth 
~~ ~~ endOfThisFrame = min ( startOfThisFrame + 
maxFrameLen , endOfThisFrame ) 
endOfThisFrame , 
startOfThisFrame = endOfThisFrame 
~~ while withPadding and startOfThisFrame < transaction . bitAddrEnd : 
~~~ endOfThisFrame = min ( startOfThisFrame + 
maxFrameLen , transaction . bitAddrEnd ) 
~~ ~~ def walkWords ( self , showPadding : bool = False ) : 
wIndex = 0 
lastEnd = self . startBitAddr 
for p in self . parts : 
~~~ end = p . startOfPart 
if showPadding and end != lastEnd : 
~~~ while end != lastEnd : 
~~~ assert end >= lastEnd , ( end , lastEnd ) 
endOfWord = ceil ( 
( lastEnd + 1 ) / self . wordWidth ) * self . wordWidth 
endOfPadding = min ( endOfWord , end ) 
_p = TransPart ( self , None , lastEnd , endOfPadding , 0 ) 
parts . append ( _p ) 
if endOfPadding >= endOfWord : 
~~~ yield ( wIndex , parts ) 
wIndex += 1 
~~ lastEnd = endOfPadding 
~~ ~~ if self . _wordIndx ( lastEnd ) != self . _wordIndx ( p . startOfPart ) : 
lastEnd = p . endOfPart 
~~ parts . append ( p ) 
if lastEnd % self . wordWidth == 0 : 
~~ ~~ if showPadding and ( parts 
or lastEnd != self . endBitAddr 
or lastEnd % self . wordWidth != 0 ) : 
~~~ end = ceil ( self . endBitAddr / self . wordWidth ) * self . wordWidth 
while end != lastEnd : 
endOfWord = ( ( lastEnd // self . wordWidth ) + 1 ) * self . wordWidth 
_p . parent = self 
~~ if parts : 
~~ ~~ ~~ def fieldToDataDict ( dtype , data , res ) : 
for f in dtype . fields : 
~~~ fVal = data [ f . name ] 
~~~ fVal = None 
~~ if isinstance ( f . dtype , Bits ) : 
~~~ if fVal is not None : 
~~~ assert isinstance ( fVal , int ) 
res [ f ] = fVal 
~~ ~~ elif isinstance ( f . dtype , HStruct ) : 
~~~ if fVal : 
~~~ FrameTmpl . fieldToDataDict ( f . dtype , fVal , res ) 
~~ ~~ elif isinstance ( f . dtype , HArray ) : 
~~~ res [ f ] = fVal 
~~ ~~ ~~ return res 
~~ def packData ( self , data ) : 
typeOfWord = simBitsT ( self . wordWidth , None ) 
fieldToVal = self . _fieldToTPart 
if fieldToVal is None : 
~~~ fieldToVal = self . _fieldToTPart = self . fieldToDataDict ( 
self . origin . dtype , 
data , 
{ } ) 
~~ for _ , transParts in self . walkWords ( showPadding = True ) : 
~~~ actualVldMask = 0 
actualVal = 0 
for tPart in transParts : 
~~~ high , low = tPart . getBusWordBitRange ( ) 
fhigh , flow = tPart . getFieldBitRange ( ) 
if not tPart . isPadding : 
~~~ val = fieldToVal . get ( tPart . tmpl . origin , None ) 
~~~ val = None 
~~ if val is None : 
~~~ newBits = 0 
~~~ newBits = selectBitRange ( val , flow , fhigh - flow ) 
vld = mask ( high - low ) << low 
~~ actualVal = setBitRange ( actualVal , low , high - low , newBits ) 
actualVldMask = setBitRange ( actualVal , low , high - low , vld ) 
~~ yield typeOfWord . getValueCls ( ) ( actualVal , typeOfWord , 
actualVldMask , - 1 ) 
~~ ~~ def isSameHVal ( a : Value , b : Value ) -> bool : 
return a is b or ( isinstance ( a , Value ) 
and isinstance ( b , Value ) 
and a . val == b . val 
and a . vldMask == b . vldMask ) 
~~ def areSameHVals ( a : Union [ None , List [ Value ] ] , 
b : Union [ None , List [ Value ] ] ) -> bool : 
if a is b : 
~~ if a is None or b is None : 
~~ if len ( a ) == len ( b ) : 
~~~ for a_ , b_ in zip ( a , b ) : 
~~~ if not isSameHVal ( a_ , b_ ) : 
~~ ~~ def isSameStatementList ( stmListA : List [ HdlStatement ] , 
stmListB : List [ HdlStatement ] ) -> bool : 
if stmListA is stmListB : 
~~ if stmListA is None or stmListB is None : 
~~ for a , b in zip ( stmListA , stmListB ) : 
~~~ if not a . isSame ( b ) : 
~~ def statementsAreSame ( statements : List [ HdlStatement ] ) -> bool : 
iterator = iter ( statements ) 
~~~ first = next ( iterator ) 
~~ return all ( first . isSame ( rest ) for rest in iterator ) 
~~ def _get_stm_with_branches ( stm_it ) : 
last = None 
while last is None or last . rank == 0 : 
~~~ last = next ( stm_it ) 
~~~ last = None 
~~ ~~ return last 
~~ def _clean_signal_meta ( self ) : 
self . _enclosed_for = None 
self . _sensitivity = None 
~~ ~~ def _collect_io ( self ) -> None : 
in_add = self . _inputs . extend 
out_add = self . _outputs . extend 
~~~ in_add ( stm . _inputs ) 
out_add ( stm . _outputs ) 
~~ ~~ def _discover_enclosure_for_statements ( statements : List [ 'HdlStatement' ] , 
outputs : List [ 'HdlStatement' ] ) : 
result = set ( ) 
if not statements : 
~~ for stm in statements : 
~~~ stm . _discover_enclosure ( ) 
~~ for o in outputs : 
~~~ has_driver = False 
~~~ if o in stm . _outputs : 
~~~ assert not has_driver 
has_driver = False 
if o in stm . _enclosed_for : 
~~~ result . add ( o ) 
~~ def _discover_sensitivity_seq ( self , 
signals : List [ RtlSignalBase ] , 
seen : set , ctx : SensitivityCtx ) -> None : 
casualSensitivity = set ( ) 
for s in signals : 
~~~ s . _walk_sensitivity ( casualSensitivity , seen , ctx ) 
~~ ~~ if not ctx . contains_ev_dependency : 
~~~ ctx . extend ( casualSensitivity ) 
~~ ~~ def _get_rtl_context ( self ) : 
for sig in chain ( self . _inputs , self . _outputs ) : 
~~~ if sig . ctx : 
~~~ return sig . ctx 
~~ ~~ raise HwtSyntaxError ( 
~~ def _on_reduce ( self , self_reduced : bool , io_changed : bool , 
result_statements : List [ "HdlStatement" ] ) -> None : 
parentStm = self . parentStm 
if self_reduced : 
~~~ was_top = parentStm is None 
if was_top : 
~~~ ctx = self . _get_rtl_context ( ) 
ctx . statements . remove ( self ) 
ctx . statements . update ( result_statements ) 
for i in self . _inputs : 
~~~ i . endpoints . discard ( self ) 
~~ for o in self . _outputs : 
~~~ o . drivers . remove ( self ) 
~~ ~~ for stm in result_statements : 
~~~ stm . parentStm = parentStm 
if parentStm is None : 
~~~ inp . endpoints . append ( stm ) 
~~~ outp . drivers . append ( stm ) 
~~~ if io_changed : 
~~~ self . _inputs = UniqList ( ) 
self . _outputs = UniqList ( ) 
self . _collect_io ( ) 
~~ ~~ ~~ def _on_merge ( self , other ) : 
self . _inputs . extend ( other . _inputs ) 
self . _outputs . extend ( other . _outputs ) 
if self . _sensitivity is not None : 
~~~ self . _sensitivity . extend ( other . _sensitivity ) 
~~~ assert other . _sensitivity is None 
~~ if self . _enclosed_for is not None : 
~~~ self . _enclosed_for . update ( other . _enclosed_for ) 
~~~ assert other . _enclosed_for is None 
~~ other_was_top = other . parentStm is None 
if other_was_top : 
~~~ other . _get_rtl_context ( ) . statements . remove ( other ) 
for s in other . _inputs : 
~~~ s . endpoints . discard ( other ) 
s . endpoints . append ( self ) 
~~ for s in other . _outputs : 
~~~ s . drivers . discard ( other ) 
s . drivers . append ( self ) 
~~ ~~ ~~ def _is_mergable_statement_list ( cls , stmsA , stmsB ) : 
if stmsA is None and stmsB is None : 
~~ elif stmsA is None or stmsB is None : 
~~ a_it = iter ( stmsA ) 
b_it = iter ( stmsB ) 
a = _get_stm_with_branches ( a_it ) 
b = _get_stm_with_branches ( b_it ) 
while a is not None or b is not None : 
~~~ if a is None or b is None or not a . _is_mergable ( b ) : 
~~ a = _get_stm_with_branches ( a_it ) 
~~ def _merge_statements ( statements : List [ "HdlStatement" ] ) -> Tuple [ List [ "HdlStatement" ] , int ] : 
order = { } 
for i , stm in enumerate ( statements ) : 
~~~ order [ stm ] = i 
~~ new_statements = [ ] 
rank_decrease = 0 
for rank , stms in groupedby ( statements , lambda s : s . rank ) : 
~~~ if rank == 0 : 
~~~ new_statements . extend ( stms ) 
~~~ if len ( stms ) == 1 : 
~~ for iA , stmA in enumerate ( stms ) : 
~~~ if stmA is None : 
~~ for iB , stmB in enumerate ( islice ( stms , iA + 1 , None ) ) : 
~~~ if stmB is None : 
~~ if stmA . _is_mergable ( stmB ) : 
~~~ rank_decrease += stmB . rank 
stmA . _merge_with_other_stm ( stmB ) 
stms [ iA + 1 + iB ] = None 
new_statements . append ( stmA ) 
~~~ new_statements . append ( stmA ) 
new_statements . append ( stmB ) 
~~ ~~ ~~ ~~ ~~ new_statements . sort ( key = lambda stm : order [ stm ] ) 
return new_statements , rank_decrease 
~~ def _merge_statement_lists ( stmsA : List [ "HdlStatement" ] , stmsB : List [ "HdlStatement" ] ) -> List [ "HdlStatement" ] : 
~~ tmp = [ ] 
a_it = iter ( stmsA ) 
a = None 
b = None 
a_empty = False 
b_empty = False 
while not a_empty and not b_empty : 
~~~ while not a_empty : 
~~~ a = next ( a_it , None ) 
if a is None : 
~~~ a_empty = True 
~~ elif a . rank == 0 : 
~~~ tmp . append ( a ) 
~~ ~~ while not b_empty : 
~~~ b = next ( b_it , None ) 
if b is None : 
~~~ b_empty = True 
~~ elif b . rank == 0 : 
~~~ tmp . append ( b ) 
~~ ~~ if a is not None or b is not None : 
~~~ a . _merge_with_other_stm ( b ) 
tmp . append ( a ) 
~~ ~~ return tmp 
~~ def _try_reduce_list ( statements : List [ "HdlStatement" ] ) : 
new_statements = [ ] 
~~~ reduced , _io_change = stm . _try_reduce ( ) 
new_statements . extend ( reduced ) 
~~ new_statements , rank_decrease = HdlStatement . _merge_statements ( 
new_statements ) 
return new_statements , rank_decrease , io_change 
~~ def _on_parent_event_dependent ( self ) : 
if not self . _is_completly_event_dependent : 
~~~ self . _is_completly_event_dependent = True 
~~~ stm . _on_parent_event_dependent ( ) 
~~ ~~ ~~ def _set_parent_stm ( self , parentStm : "HdlStatement" ) : 
was_top = self . parentStm is None 
self . parentStm = parentStm 
if not self . _now_is_event_dependent and parentStm . _now_is_event_dependent : 
~~~ self . _on_parent_event_dependent ( ) 
~~ topStatement = parentStm 
while topStatement . parentStm is not None : 
~~~ topStatement = topStatement . parentStm 
~~ parent_out_add = topStatement . _outputs . append 
parent_in_add = topStatement . _inputs . append 
~~~ for inp in self . _inputs : 
~~~ inp . endpoints . discard ( self ) 
inp . endpoints . append ( topStatement ) 
parent_in_add ( inp ) 
~~ for outp in self . _outputs : 
~~~ outp . drivers . discard ( self ) 
outp . drivers . append ( topStatement ) 
parent_out_add ( outp ) 
~~ ctx = self . _get_rtl_context ( ) 
ctx . statements . discard ( self ) 
~~ parentStm . rank += self . rank 
~~ def _register_stements ( self , statements : List [ "HdlStatement" ] , 
target : List [ "HdlStatement" ] ) : 
for stm in flatten ( statements ) : 
~~~ assert stm . parentStm is None , stm 
stm . _set_parent_stm ( self ) 
target . append ( stm ) 
~~ ~~ def _destroy ( self ) : 
ctx = self . _get_rtl_context ( ) 
~~ ctx . statements . remove ( self ) 
~~ def fromPy ( cls , val , typeObj , vldMask = None ) : 
assert isinstance ( val , str ) or val is None 
vld = 0 if val is None else 1 
val = "" 
~~~ val = "" 
~~ def _reg ( self , name , dtype = BIT , defVal = None , clk = None , rst = None ) : 
if clk is None : 
~~~ clk = getClk ( self ) 
~~ if defVal is None : 
~~~ rst = None 
~~~ rst = getRst ( self ) . _sig 
~~ if isinstance ( dtype , HStruct ) : 
~~~ if defVal is not None : 
~~ container = dtype . fromPy ( None ) 
~~~ if f . name is not None : 
~~~ r = self . _reg ( "%s_%s" % ( name , f . name ) , f . dtype ) 
setattr ( container , f . name , r ) 
~~ ~~ return container 
~~ return self . _ctx . sig ( name , 
clk = clk . _sig , 
syncRst = rst , 
defVal = defVal ) 
~~ def _sig ( self , name , dtype = BIT , defVal = None ) : 
if isinstance ( dtype , HStruct ) : 
~~~ r = self . _sig ( "%s_%s" % ( name , f . name ) , f . dtype ) 
~~ return self . _ctx . sig ( name , dtype = dtype , defVal = defVal ) 
~~ def _cleanAsSubunit ( self ) : 
for pi in self . _entity . ports : 
~~~ pi . connectInternSig ( ) 
~~ for i in chain ( self . _interfaces , self . _private_interfaces ) : 
~~~ i . _clean ( ) 
~~ ~~ def HStruct_selectFields ( structT , fieldsToUse ) : 
template = [ ] 
fieldsToUse = fieldsToUse 
foundNames = set ( ) 
for f in structT . fields : 
~~~ name = None 
subfields = [ ] 
~~~ if isinstance ( fieldsToUse , dict ) : 
~~~ subfields = fieldsToUse [ f . name ] 
name = f . name 
~~~ if f . name in fieldsToUse : 
~~~ name = f . name 
~~ ~~ ~~ except KeyError : 
~~ ~~ if name is not None and subfields : 
~~~ fields = HStruct_selectFields ( f . dtype , subfields ) 
template . append ( HStructField ( fields , name ) ) 
~~~ template . append ( HStructField ( f . dtype , name ) ) 
~~ if f . name is not None : 
~~~ foundNames . add ( f . name ) 
~~ ~~ if isinstance ( fieldsToUse , dict ) : 
~~~ fieldsToUse = set ( fieldsToUse . keys ( ) ) 
~~ assert fieldsToUse . issubset ( foundNames ) 
return HStruct ( * template ) 
~~ def walkFlattenFields ( sigOrVal , skipPadding = True ) : 
t = sigOrVal . _dtype 
if isinstance ( t , Bits ) : 
~~~ yield sigOrVal 
~~ elif isinstance ( t , HUnion ) : 
~~~ yield from walkFlattenFields ( sigOrVal . _val , skipPadding = skipPadding ) 
~~ elif isinstance ( t , HStruct ) : 
~~~ for f in t . fields : 
~~~ isPadding = f . name is None 
if not isPadding or not skipPadding : 
~~~ if isPadding : 
~~~ v = f . dtype . fromPy ( None ) 
~~~ v = getattr ( sigOrVal , f . name ) 
~~ yield from walkFlattenFields ( v ) 
~~ ~~ ~~ elif isinstance ( t , HArray ) : 
~~~ for item in sigOrVal : 
~~~ yield from walkFlattenFields ( item ) 
~~~ raise NotImplementedError ( t ) 
~~ ~~ def HStruct_unpack ( structT , data , getDataFn = None , dataWidth = None ) : 
if getDataFn is None : 
~~~ assert dataWidth is not None 
def _getDataFn ( x ) : 
~~~ return toHVal ( x ) . _auto_cast ( Bits ( dataWidth ) ) 
~~ getDataFn = _getDataFn 
~~ val = structT . fromPy ( None ) 
fData = iter ( data ) 
actualOffset = 0 
actual = None 
for v in walkFlattenFields ( val , skipPadding = False ) : 
~~~ required = v . _dtype . bit_length ( ) 
~~~ actualOffset = 0 
~~~ actual = getDataFn ( next ( fData ) ) 
~~ if dataWidth is None : 
~~~ dataWidth = actual . _dtype . bit_length ( ) 
~~ actuallyHave = dataWidth 
~~~ actuallyHave = actual . _dtype . bit_length ( ) - actualOffset 
~~ while actuallyHave < required : 
~~~ d = getDataFn ( next ( fData ) ) 
~~ actual = d . _concat ( actual ) 
actuallyHave += dataWidth 
~~ if actuallyHave >= required : 
~~~ _v = actual [ ( required + actualOffset ) : actualOffset ] 
_v = _v . _auto_cast ( v . _dtype ) 
v . val = _v . val 
v . vldMask = _v . vldMask 
v . updateTime = _v . updateTime 
actuallyHave -= required 
actualOffset += required 
~~ if actuallyHave == 0 : 
~~~ actual = None 
~~ ~~ if actual is not None : 
~~~ assert actual . _dtype . bit_length ( 
~~ return val 
~~ def _convSign ( self , signed ) : 
if isinstance ( self , Value ) : 
~~~ return self . _convSign__val ( signed ) 
~~~ if self . _dtype . signed == signed : 
~~ t = copy ( self . _dtype ) 
t . signed = signed 
if signed is None : 
~~~ cnv = AllOps . BitsAsVec 
~~ elif signed : 
~~~ cnv = AllOps . BitsAsSigned 
~~~ cnv = AllOps . BitsAsUnsigned 
~~ return Operator . withRes ( cnv , [ self ] , t ) 
~~ ~~ def fromPy ( cls , val , typeObj , vldMask = None ) : 
assert not isinstance ( val , Value ) 
if val is None : 
~~~ vld = 0 
assert vldMask is None or vldMask == 0 
~~~ allMask = typeObj . all_mask ( ) 
w = typeObj . bit_length ( ) 
if isinstance ( val , bytes ) : 
~~~ val = int . from_bytes ( 
val , byteorder = "little" , signed = bool ( typeObj . signed ) ) 
~~ except TypeError as e : 
~~~ if isinstance ( val , enum . Enum ) : 
~~~ val = int ( val . value ) 
~~ ~~ ~~ if vldMask is None : 
~~~ vld = allMask 
~~~ assert vldMask <= allMask and vldMask >= 0 
vld = vldMask 
~~~ assert typeObj . signed 
assert signFix ( val & allMask , w ) == val , ( 
val , signFix ( val & allMask , w ) ) 
val = signFix ( val & vld , w ) 
~~~ if typeObj . signed : 
~~~ msb = 1 << ( w - 1 ) 
if msb & val : 
~~~ assert val < 0 , val 
~~ ~~ if val & allMask != val : 
val , val & allMask ) 
~~ val = val & vld 
~~ def _concat ( self , other ) : 
w = self . _dtype . bit_length ( ) 
~~~ other_bit_length = other . _dtype . bit_length 
~~ other_w = other_bit_length ( ) 
resWidth = w + other_w 
resT = Bits ( resWidth ) 
if areValues ( self , other ) : 
~~~ return self . _concat__val ( other ) 
~~~ w = self . _dtype . bit_length ( ) 
other_w = other . _dtype . bit_length ( ) 
if isinstance ( other , InterfaceBase ) : 
~~~ other = other . _sig 
~~ if isinstance ( other . _dtype , Bits ) : 
~~~ if other . _dtype . signed is not None : 
~~~ other = other . _vec ( ) 
~~ ~~ elif other . _dtype == BOOL : 
~~~ other = other . _auto_cast ( BIT ) 
~~~ raise TypeError ( other . _dtype ) 
~~ if self . _dtype . signed is not None : 
~~~ self = self . _vec ( ) 
~~ return Operator . withRes ( AllOps . CONCAT , [ self , other ] , resT ) . _auto_cast ( Bits ( resWidth , 
signed = self . _dtype . signed ) ) 
~~ ~~ def sensitivity ( proc : HWProcess , * sensitiveTo ) : 
for s in sensitiveTo : 
~~~ if isinstance ( s , tuple ) : 
~~~ sen , s = s 
if sen == SENSITIVITY . ANY : 
~~~ s . simSensProcs . add ( proc ) 
~~ elif sen == SENSITIVITY . RISING : 
~~~ s . simRisingSensProcs . add ( proc ) 
~~ elif sen == SENSITIVITY . FALLING : 
~~~ s . simFallingSensProcs . add ( proc ) 
~~~ raise AssertionError ( sen ) 
~~ ~~ ~~ def simEvalCond ( simulator , * conds ) : 
_cond = True 
_vld = True 
for v in conds : 
~~~ val = bool ( v . val ) 
fullVld = v . vldMask == 1 
if fullVld : 
~~~ if not val : 
~~~ return False , True 
~~~ return False , False 
~~ _cond = _cond and val 
_vld = _vld and fullVld 
~~ return _cond , _vld 
~~ def connectSimPort ( simUnit , subSimUnit , srcName , dstName , direction ) : 
if direction == DIRECTION . OUT : 
~~~ origPort = getattr ( subSimUnit , srcName ) 
newPort = getattr ( simUnit , dstName ) 
setattr ( subSimUnit , srcName , newPort ) 
~~~ origPort = getattr ( subSimUnit , dstName ) 
newPort = getattr ( simUnit , srcName ) 
setattr ( subSimUnit , dstName , newPort ) 
~~ subSimUnit . _ctx . signals . remove ( origPort ) 
~~ def mkUpdater ( nextVal : Value , invalidate : bool ) : 
def updater ( currentVal ) : 
~~~ _nextVal = nextVal . clone ( ) 
if invalidate : 
~~~ _nextVal . vldMask = 0 
~~ return ( valueHasChanged ( currentVal , _nextVal ) , _nextVal ) 
~~ return updater 
~~ def mkArrayUpdater ( nextItemVal : Value , indexes : Tuple [ Value ] , 
invalidate : bool ) : 
~~~ if len ( indexes ) > 1 : 
~~ _nextItemVal = nextItemVal . clone ( ) 
~~~ _nextItemVal . vldMask = 0 
~~ index = indexes [ 0 ] 
change = valueHasChanged ( currentVal . _getitem__val ( index ) , _nextItemVal ) 
currentVal . _setitem__val ( index , _nextItemVal ) 
return ( change , currentVal ) 
size = evalParam ( typeObj . size ) 
if isinstance ( size , Value ) : 
~~~ size = int ( size ) 
~~ elements = { } 
if vldMask == 0 : 
~~ elif isinstance ( val , dict ) : 
~~~ for k , v in val . items ( ) : 
~~~ if not isinstance ( k , int ) : 
~~~ k = int ( k ) 
~~ elements [ k ] = typeObj . elmType . fromPy ( v ) 
~~~ for k , v in enumerate ( val ) : 
~~~ assert v . _dtype == typeObj . elmType 
e = v 
~~~ e = typeObj . elmType . fromPy ( v ) 
~~ elements [ k ] = e 
~~ ~~ _mask = int ( bool ( val ) ) 
if vldMask is None : 
~~~ vldMask = _mask 
~~~ assert ( vldMask == _mask ) 
~~ return cls ( elements , typeObj , vldMask ) 
~~ def _getitem__val ( self , key ) : 
~~~ kv = key . val 
if not key . _isFullVld ( ) : 
~~~ raise KeyError ( ) 
~~~ if kv >= self . _dtype . size : 
~~ ~~ return self . val [ kv ] . clone ( ) 
~~~ return self . _dtype . elmType . fromPy ( None ) 
~~ ~~ def vec ( val , width , signed = None ) : 
return Bits ( width , signed , forceVector = True ) . fromPy ( val ) 
if self . notReset ( sim ) : 
~~~ if self . _lastRd is not 1 : 
self . _lastRd = 1 
~~~ onMonitorReady = self . onMonitorReady 
~~~ onMonitorReady = None 
~~ if onMonitorReady is not None : 
~~~ onMonitorReady ( sim ) 
~~ ~~ yield sim . waitOnCombUpdate ( ) 
vld = self . isVld ( r ) 
assert vld . vldMask , ( sim . now , self . intf , 
if vld . val : 
~~~ d = self . doRead ( sim ) 
if self . _debugOutput is not None : 
~~~ self . _debugOutput . write ( 
sim . now , d ) ) 
~~ self . data . append ( d ) 
if self . _afterRead is not None : 
~~~ self . _afterRead ( sim ) 
~~~ if self . _lastRd is not 0 : 
self . _lastRd = 0 
~~ ~~ ~~ def driver ( self , sim ) : 
~~ doSend = self . actualData is not NOP 
if self . actualData is not self . _lastWritten : 
~~~ if doSend : 
~~ self . _lastWritten = self . actualData 
~~ en = self . notReset ( sim ) 
vld = int ( en and doSend ) 
if self . _lastVld is not vld : 
~~~ self . wrVld ( sim . write , vld ) 
self . _lastVld = vld 
~~ if not self . _enabled : 
~~~ sim . add_process ( self . checkIfRdWillBeValid ( sim ) ) 
assert rd . vldMask , ( sim . now , self . intf , 
sim . now , 
self . actualData ) ) 
~~ a = self . actualData 
if self . data : 
~~ onDriverWriteAck = getattr ( self , "onDriverWriteAck" , None ) 
if onDriverWriteAck is not None : 
~~~ onDriverWriteAck ( sim ) 
~~ onDone = getattr ( a , "onDone" , None ) 
if onDone is not None : 
~~~ onDone ( sim ) 
~~ ~~ ~~ def HWProcess ( cls , proc : HWProcess , ctx : ResourceContext ) -> None : 
seen = ctx . seen 
~~~ encl = stm . _enclosed_for 
full_ev_dep = stm . _is_completly_event_dependent 
now_ev_dep = stm . _now_is_event_dependent 
ev_dep = full_ev_dep or now_ev_dep 
out_mux_dim = count_mux_inputs_for_outputs ( stm ) 
for o in stm . _outputs : 
~~~ if o in seen : 
~~ i = out_mux_dim [ o ] 
if isinstance ( o . _dtype , HArray ) : 
for a in walk_assignments ( stm , o ) : 
addr = a . indexes [ 0 ] 
~~ ctx . registerRAM_write_port ( o , addr , ev_dep ) 
~~ elif ev_dep : 
~~~ ctx . registerFF ( o ) 
if i > 1 : 
~~~ ctx . registerMUX ( stm , o , i ) 
~~ ~~ elif o not in encl : 
~~~ ctx . registerLatch ( o ) 
~~ ~~ elif i > 1 : 
~~ ~~ if isinstance ( stm , SwitchContainer ) : 
~~~ caseEqs = set ( [ stm . switchOn . _eq ( c [ 0 ] ) for c in stm . cases ] ) 
inputs = chain ( 
[ sig for sig in stm . _inputs if sig not in caseEqs ] , [ stm . switchOn ] ) 
~~~ inputs = stm . _inputs 
~~ for i in inputs : 
~~~ if not i . hidden or i in seen : 
~~ cls . HWProcess_operators ( i , ctx , ev_dep ) 
~~ ~~ ~~ def bit_length ( self ) : 
~~~ itemSize = self . elmType . bit_length 
~~~ itemSize = None 
~~ if itemSize is None : 
~~ s = self . size 
if isinstance ( s , RtlSignalBase ) : 
~~~ s = int ( s . staticEval ( ) ) 
~~ return s * itemSize ( ) 
~~ def evalParam ( p ) : 
while isinstance ( p , Param ) : 
~~~ p = p . get ( ) 
~~ if isinstance ( p , RtlSignalBase ) : 
~~~ return p . staticEval ( ) 
~~ return toHVal ( p ) 
~~ def set ( self , val ) : 
val = toHVal ( val ) 
self . defVal = val 
self . _val = val . staticEval ( ) 
self . _dtype = self . _val . _dtype 
~~ def HTypeFromIntfMap ( interfaceMap ) : 
structFields = [ ] 
for m in interfaceMap : 
~~~ f = HTypeFromIntfMapItem ( m ) 
structFields . append ( f ) 
~~ return HStruct ( * structFields ) 
~~ def registerMUX ( self , stm : Union [ HdlStatement , Operator ] , sig : RtlSignal , 
inputs_cnt : int ) : 
assert inputs_cnt > 1 
res = self . resources 
w = sig . _dtype . bit_length ( ) 
k = ( ResourceMUX , w , inputs_cnt ) 
res [ k ] = res . get ( k , 0 ) + 1 
self . resource_for_object [ ( stm , sig ) ] = k 
~~ def finalize ( self ) : 
ff_to_remove = 0 
for m , addrDict in self . memories . items ( ) : 
~~~ rwSyncPorts , rSyncPorts , wSyncPorts = 0 , 0 , 0 
rwAsyncPorts , rAsyncPorts , wAsyncPorts = 0 , 0 , 0 
rSync_wAsyncPorts , rAsync_wSyncPorts = 0 , 0 
for _ , ( rSync , wSync , rAsync , wAsync ) in addrDict . items ( ) : 
~~~ if rSync : 
~~~ ff_to_remove += rSync * m . _dtype . elmType . bit_length ( ) 
~~ rwSync = min ( rSync , wSync ) 
rSync -= rwSync 
wSync -= rwSync 
rwAsync = min ( rAsync , wAsync ) 
rAsync -= rwAsync 
wAsync -= rwAsync 
rSync_wAsync = min ( rSync , wAsync ) 
rSync -= rSync_wAsync 
wAsync -= rSync_wAsync 
rAsync_wSync = min ( rAsync , wSync ) 
rAsync -= rAsync_wSync 
wSync -= rAsync_wSync 
rwSyncPorts += rwSync 
rSyncPorts += rSync 
wSyncPorts += wSync 
rwAsyncPorts += rwAsync 
rAsyncPorts += rAsync 
wAsyncPorts += wAsync 
rSync_wAsyncPorts += rSync_wAsync 
rAsync_wSyncPorts += rAsync_wSync 
~~ k = ResourceRAM ( m . _dtype . elmType . bit_length ( ) , 
int ( m . _dtype . size ) , 
rwSyncPorts , rSyncPorts , wSyncPorts , 
rSync_wAsyncPorts , 
rwAsyncPorts , rAsyncPorts , wAsyncPorts , 
rAsync_wSyncPorts ) 
~~ self . memories . clear ( ) 
if ff_to_remove : 
~~~ ff_cnt = res [ ResourceFF ] 
ff_cnt -= ff_to_remove 
if ff_cnt : 
~~~ res [ ResourceFF ] = ff_cnt 
~~~ del res [ ResourceFF ] 
~~ ~~ ~~ def naryOp ( self , operator , opCreateDelegate , * otherOps ) -> RtlSignalBase : 
k = ( operator , * otherOps ) 
used = self . _usedOps 
~~~ return used [ k ] 
~~ o = opCreateDelegate ( self , * otherOps ) 
~~~ op_instanciated = ( o . origin . operator == operator 
and o . origin . operands [ 0 ] is self ) 
~~~ op_instanciated = False 
~~ if op_instanciated : 
~~~ k_real = ( operator , * o . origin . operands [ 1 : ] ) 
real_o = used . get ( k_real , None ) 
if real_o is not None : 
~~~ ctx = self . ctx 
if ctx is not None : 
~~~ ctx . signals . remove ( o ) 
~~ op = o . origin 
o . origin = None 
o . drivers . clear ( ) 
for inp in op . operands : 
~~~ if isinstance ( inp , RtlSignalBase ) : 
~~~ inp . endpoints . remove ( op ) 
~~ ~~ o = real_o 
~~~ used [ k_real ] = o 
~~ ~~ used [ k ] = o 
return o 
~~ def _eq ( self , other ) : 
return self . naryOp ( AllOps . EQ , tv ( self ) . _eq , other ) 
~~ def _getIndexCascade ( self ) : 
~~~ d = self . singleDriver ( ) 
~~~ op = d . operator 
~~ if op == AllOps . INDEX : 
~~~ indexedOn = d . operands [ 0 ] 
if isinstance ( indexedOn , RtlSignalBase ) : 
~~~ return indexedOn , [ d . operands [ 1 ] ] 
~~ ~~ ~~ except ( MultipleDriversErr , NoDriverErr ) : 
~~ ~~ def fromPy ( self , v , vldMask = None ) : 
return self . getValueCls ( ) . fromPy ( v , self , vldMask = vldMask ) 
~~ def auto_cast ( self , sigOrVal , toType ) : 
if sigOrVal . _dtype == toType : 
~~~ return sigOrVal 
~~~ c = self . _auto_cast_fn 
~~~ c = self . get_auto_cast_fn ( ) 
self . _auto_cast_fn = c 
~~ return c ( self , sigOrVal , toType ) 
~~ def reinterpret_cast ( self , sigOrVal , toType ) : 
~~~ return self . auto_cast ( sigOrVal , toType ) 
~~ except TypeConversionErr : 
~~~ r = self . _reinterpret_cast_fn 
~~~ r = self . get_reinterpret_cast_fn ( ) 
self . _reinterpret_cast_fn = r 
~~ return r ( self , sigOrVal , toType ) 
~~ def walkParams ( intf , discovered ) : 
for si in intf . _interfaces : 
~~~ yield from walkParams ( si , discovered ) 
~~ for p in intf . _params : 
~~~ if p not in discovered : 
~~~ discovered . add ( p ) 
yield p 
~~ ~~ ~~ def connectPacked ( srcPacked , dstInterface , exclude = None ) : 
connections = [ ] 
for i in reversed ( list ( walkPhysInterfaces ( dstInterface ) ) ) : 
~~~ if exclude is not None and i in exclude : 
~~ sig = i . _sig 
t = sig . _dtype 
if t == BIT : 
~~~ s = srcPacked [ offset ] 
offset += 1 
~~~ w = t . bit_length ( ) 
s = srcPacked [ ( w + offset ) : offset ] 
offset += w 
~~ connections . append ( sig ( s ) ) 
~~ return connections 
~~ def walkFlatten ( interface , shouldEnterIntfFn ) : 
_shouldEnter , _shouldYield = shouldEnterIntfFn ( interface ) 
if _shouldYield : 
~~~ yield interface 
~~ if shouldEnterIntfFn : 
~~~ for intf in interface . _interfaces : 
~~~ yield from walkFlatten ( intf , shouldEnterIntfFn ) 
~~ ~~ ~~ def packIntf ( intf , masterDirEqTo = DIRECTION . OUT , exclude = None ) : 
if not intf . _interfaces : 
~~~ if intf . _masterDir == masterDirEqTo : 
~~~ return intf . _sig 
~~ res = None 
for i in intf . _interfaces : 
~~ if i . _interfaces : 
~~~ if i . _masterDir == DIRECTION . IN : 
~~~ d = DIRECTION . opposite ( masterDirEqTo ) 
~~~ d = masterDirEqTo 
~~ s = i . _pack ( d , exclude = exclude ) 
~~~ if i . _masterDir == masterDirEqTo : 
~~~ s = i . _sig 
~~~ s = None 
~~ ~~ if s is not None : 
~~~ if res is None : 
~~~ res = s 
~~~ res = res . _concat ( s ) 
~~ def hardcodeRomIntoProcess ( cls , rom ) : 
signals = [ ] 
for e in rom . endpoints : 
~~~ assert isinstance ( e , Operator ) and e . operator == AllOps . INDEX , e 
me , index = e . operands 
assert me is rom 
romValSig = rom . ctx . sig ( rom . name , dtype = e . result . _dtype ) 
signals . append ( romValSig ) 
romValSig . hidden = False 
cases = [ ( toHVal ( i ) , [ romValSig ( v ) , ] ) 
for i , v in enumerate ( rom . defVal . val ) ] 
statements = [ SwitchContainer ( index , cases ) , ] 
for ( _ , ( stm , ) ) in cases : 
~~~ stm . parentStm = statements [ 0 ] 
~~ p = HWProcess ( rom . name , statements , { index , } , 
{ index , } , { romValSig , } ) 
processes . append ( p ) 
def replaceOrigRomIndexExpr ( x ) : 
~~~ if x is e . result : 
~~~ return romValSig 
~~ ~~ for _e in e . result . endpoints : 
~~~ _e . operands = tuple ( map ( replaceOrigRomIndexExpr , _e . operands ) ) 
e . result = romValSig 
~~ ~~ return processes , signals 
~~ def Architecture_var ( cls , v , serializerVars , extraTypes , 
t = v . _dtype 
if isinstance ( t , HArray ) and v . defVal . vldMask : 
~~~ if v . drivers : 
~~ eProcs , eVars = cls . hardcodeRomIntoProcess ( v ) 
for _v in eVars : 
~~~ _procs = cls . Architecture_var ( _v , serializerVars , extraTypes , 
extraTypes_serialized , ctx , 
childCtx ) 
eProcs . extend ( _procs ) 
~~ return eProcs 
~~ v . name = ctx . scope . checkedName ( v . name , v ) 
~~ def _toRtl ( self , targetPlatform : DummyPlatform ) : 
assert not self . _wasSynthetised ( ) 
self . _targetPlatform = targetPlatform 
if not hasattr ( self , "_name" ) : 
~~~ self . _name = self . _getDefaultName ( ) 
~~ for proc in targetPlatform . beforeToRtl : 
~~ self . _ctx . params = self . _buildParams ( ) 
self . _externInterf = [ ] 
for u in self . _units : 
~~~ yield from u . _toRtl ( targetPlatform ) 
~~ for u in self . _units : 
~~~ subUnitName = u . _name 
u . _signalsForMyEntity ( self . _ctx , "sig_" + subUnitName ) 
~~ for i in self . _interfaces : 
~~~ signals = i . _signalsForInterface ( self . _ctx ) 
if i . _isExtern : 
~~~ self . _externInterf . extend ( signals ) 
~~ ~~ for proc in targetPlatform . beforeToRtlImpl : 
~~ self . _loadMyImplementations ( ) 
yield from self . _lazyLoaded 
if not self . _externInterf : 
~~~ raise IntfLvlConfErr ( 
% self . _name ) 
~~ for proc in targetPlatform . afterToRtlImpl : 
~~ yield from self . _synthetiseContext ( self . _externInterf ) 
self . _checkArchCompInstances ( ) 
for proc in targetPlatform . afterToRtl : 
~~ ~~ def _loadDeclarations ( self ) : 
~~ if not hasattr ( self , "_private_interfaces" ) : 
~~~ self . _private_interfaces = [ ] 
~~ if not hasattr ( self , "_units" ) : 
~~~ self . _units = [ ] 
~~~ self . _loadInterface ( i , True ) 
~~~ u . _loadDeclarations ( ) 
~~ ~~ def _registerIntfInImpl ( self , iName , intf ) : 
self . _registerInterface ( iName , intf , isPrivate = True ) 
self . _loadInterface ( intf , False ) 
intf . _signalsForInterface ( self . _ctx ) 
~~ def reverseByteOrder ( signalOrVal ) : 
w = signalOrVal . _dtype . bit_length ( ) 
i = w 
while i > 0 : 
~~~ lower = max ( i - 8 , 0 ) 
items . append ( signalOrVal [ i : lower ] ) 
i -= 8 
~~ return Concat ( * items ) 
~~ def tryReduceAnd ( sig , val ) : 
m = sig . _dtype . all_mask ( ) 
if val . _isFullVld ( ) : 
~~~ v = val . val 
if v == m : 
~~~ return sig 
~~ elif v == 0 : 
~~~ return val 
~~ ~~ ~~ def tryReduceXor ( sig , val ) : 
if not val . vldMask : 
~~ if val . _isFullVld ( ) : 
~~~ return ~ sig 
~~ ~~ ~~ def getBaseNameScope ( cls ) : 
s = NameScope ( False ) 
s . setLevel ( 1 ) 
s [ 0 ] . update ( cls . _keywords_dict ) 
~~ def asHdl ( cls , obj , ctx : SerializerCtx ) : 
~~ return serFn ( obj , ctx ) 
~~ ~~ def Entity ( cls , ent : Entity , ctx : SerializerCtx ) : 
return "" 
~~ def serializationDecision ( cls , obj , serializedClasses , 
serializedConfiguredUnits ) : 
isDeclaration = isinstance ( obj , Entity ) 
isDefinition = isinstance ( obj , Architecture ) 
if isDeclaration : 
~~~ unit = obj . origin 
~~ elif isDefinition : 
~~~ unit = obj . entity . origin 
~~ assert isinstance ( unit , Unit ) 
sd = unit . _serializeDecision 
if sd is None : 
~~~ prevPriv = serializedClasses . get ( unit . __class__ , None ) 
seriazlize , nextPriv = sd ( unit , obj , isDeclaration , prevPriv ) 
serializedClasses [ unit . __class__ ] = nextPriv 
return seriazlize 
~~ ~~ def HdlType ( cls , typ : HdlType , ctx : SerializerCtx , declaration = False ) : 
if isinstance ( typ , Bits ) : 
~~~ sFn = cls . HdlType_bits 
~~ elif isinstance ( typ , HEnum ) : 
~~~ sFn = cls . HdlType_enum 
~~ elif isinstance ( typ , HArray ) : 
~~~ sFn = cls . HdlType_array 
~~ elif isinstance ( typ , Integer ) : 
~~~ sFn = cls . HdlType_int 
~~ elif isinstance ( typ , HBool ) : 
~~~ sFn = cls . HdlType_bool 
% ( typ . name ) ) 
~~ return sFn ( typ , ctx , declaration = declaration ) 
~~ def IfContainer ( cls , ifc : IfContainer , ctx : SerializerCtx ) : 
def asHdl ( statements ) : 
~~~ return [ cls . asHdl ( s , childCtx ) for s in statements ] 
~~~ cond = cls . condAsHdl ( ifc . cond , True , ctx ) 
~~ except UnsupportedEventOpErr as e : 
~~~ cond = None 
~~ if cond is None : 
~~~ assert not ifc . elIfs 
assert not ifc . ifFalse 
stmBuff = [ cls . asHdl ( s , ctx ) for s in ifc . ifTrue ] 
return "\\n" . join ( stmBuff ) 
~~ elIfs = [ ] 
ifTrue = ifc . ifTrue 
ifFalse = ifc . ifFalse 
if ifFalse is None : 
~~~ ifFalse = [ ] 
~~ for c , statements in ifc . elIfs : 
~~~ elIfs . append ( ( cls . condAsHdl ( c , True , ctx ) , asHdl ( statements ) ) ) 
~~~ if len ( ifc . elIfs ) == 1 and not ifFalse : 
~~~ ifFalse = statements 
~~ ~~ ~~ return cls . ifTmpl . render ( 
cond = cond , 
ifTrue = asHdl ( ifTrue ) , 
elIfs = elIfs , 
ifFalse = asHdl ( ifFalse ) ) 
~~ def pullDownAfter ( sig , initDelay = 6 * Time . ns ) : 
def _pullDownAfter ( s ) : 
~~~ s . write ( True , sig ) 
yield s . wait ( initDelay ) 
s . write ( False , sig ) 
~~ return _pullDownAfter 
~~ def getBaseCond ( c ) : 
isNegated = False 
~~~ drivers = c . drivers 
~~~ return ( c , isNegated ) 
~~ if len ( drivers ) == 1 : 
~~~ d = list ( c . drivers ) [ 0 ] 
if isinstance ( d , Operator ) and d . operator == AllOps . NOT : 
~~~ c = d . operands [ 0 ] 
isNegated = True 
~~ ~~ return ( c , isNegated ) 
~~ def simBitsT ( width : int , signed : Union [ bool , None ] ) : 
k = ( width , signed ) 
~~~ return __simBitsTCache [ k ] 
~~~ t = SimBitsT ( width , signed ) 
__simBitsTCache [ k ] = t 
return t 
~~ return cls ( val , typeObj ) 
~~ def VectSignal ( width , 
signed = None , 
masterDir = D . OUT , 
loadConfig = True ) : 
return Signal ( masterDir , 
Bits ( width , signed , forceVector = True ) , 
loadConfig ) 
~~ def getConstName ( self , val ) : 
~~~ return self . _cache [ val ] 
~~~ if isinstance ( val . val , int ) : 
~~~ name = "const_%d_" % val . val 
~~~ name = "const_" 
~~ c = self . nameCheckFn ( name , val ) 
self . _cache [ val ] = c 
return c 
if self . dst is sig : 
~~ ~~ def _loadFromArray ( self , dtype : HdlType , bitAddr : int ) -> int : 
self . itemCnt = evalParam ( dtype . size ) . val 
self . children = TransTmpl ( 
dtype . elmType , 0 , parent = self , origin = self . origin ) 
return bitAddr + self . itemCnt * self . children . bitAddrEnd 
~~ def _loadFromHStruct ( self , dtype : HdlType , bitAddr : int ) : 
origin = f 
isPadding = f . name is None 
if isPadding : 
~~~ width = t . bit_length ( ) 
bitAddr += width 
~~~ fi = TransTmpl ( t , bitAddr , parent = self , origin = origin ) 
self . children . append ( fi ) 
bitAddr = fi . bitAddrEnd 
~~ ~~ return bitAddr 
~~ def _loadFromUnion ( self , dtype : HdlType , bitAddr : int ) -> int : 
for field in dtype . fields . values ( ) : 
~~~ ch = TransTmpl ( field . dtype , 0 , parent = self , origin = field ) 
self . children . append ( ch ) 
~~ return bitAddr + dtype . bit_length ( ) 
~~ def _loadFromHStream ( self , dtype : HStream , bitAddr : int ) -> int : 
ch = TransTmpl ( dtype . elmType , 0 , parent = self , origin = self . origin ) 
return bitAddr + dtype . elmType . bit_length ( ) 
~~ def _loadFromHType ( self , dtype : HdlType , bitAddr : int ) -> None : 
self . bitAddr = bitAddr 
childrenAreChoice = False 
if isinstance ( dtype , Bits ) : 
~~~ ld = self . _loadFromBits 
~~ elif isinstance ( dtype , HStruct ) : 
~~~ ld = self . _loadFromHStruct 
~~ elif isinstance ( dtype , HArray ) : 
~~~ ld = self . _loadFromArray 
~~ elif isinstance ( dtype , HStream ) : 
~~~ ld = self . _loadFromHStream 
~~ elif isinstance ( dtype , HUnion ) : 
~~~ ld = self . _loadFromUnion 
childrenAreChoice = True 
~~ self . bitAddrEnd = ld ( dtype , bitAddr ) 
self . childrenAreChoice = childrenAreChoice 
~~ def getItemWidth ( self ) -> int : 
if not isinstance ( self . dtype , HArray ) : 
~~ return ( self . bitAddrEnd - self . bitAddr ) // self . itemCnt 
~~ def walkFlatten ( self , offset : int = 0 , 
shouldEnterFn = _default_shouldEnterFn , 
otherObjItCtx : ObjIteratorCtx = _DummyIteratorCtx ( ) 
) -> Generator [ 
Union [ Tuple [ Tuple [ int , int ] , 'TransTmpl' ] , 'OneOfTransaction' ] , 
None , None ] : 
t = self . dtype 
base = self . bitAddr + offset 
end = self . bitAddrEnd + offset 
shouldEnter , shouldYield = shouldEnterFn ( self ) 
if shouldYield : 
~~~ yield ( ( base , end ) , self ) 
~~ if shouldEnter : 
~~~ if isinstance ( t , Bits ) : 
~~~ for ch in self . children : 
~~~ with otherObjItCtx ( ch . origin . name ) : 
~~~ yield from ch . walkFlatten ( 
offset , 
shouldEnterFn , 
otherObjItCtx ) 
~~~ itemSize = ( self . bitAddrEnd - self . bitAddr ) // self . itemCnt 
for i in range ( self . itemCnt ) : 
~~~ with otherObjItCtx ( i ) : 
~~~ yield from self . children . walkFlatten ( 
base + i * itemSize , 
~~ ~~ ~~ elif isinstance ( t , HUnion ) : 
~~~ yield OneOfTransaction ( self , offset , shouldEnterFn , 
self . children ) 
~~ elif isinstance ( t , HStream ) : 
~~~ assert len ( self . children ) == 1 
yield StreamTransaction ( self , offset , shouldEnterFn , 
self . children [ 0 ] ) 
~~~ raise TypeError ( t ) 
~~ ~~ ~~ def walkFlattenChilds ( self ) -> Generator [ 
Union [ Tuple [ Tuple [ int , int ] , TransTmpl ] , 'OneOfTransaction' ] , 
for p in self . possibleTransactions : 
~~~ yield p . walkFlatten ( offset = self . offset , 
shouldEnterFn = self . shouldEnterFn ) 
~~ ~~ def signFix ( val , width ) : 
if val > 0 : 
~~~ msb = 1 << ( width - 1 ) 
if val & msb : 
~~~ val -= mask ( width ) + 1 
~~ ~~ return val 
~~ def bitsCmp ( self , other , op , evalFn = None ) : 
other = toHVal ( other ) 
t = self . _dtype 
ot = other . _dtype 
iamVal = isinstance ( self , Value ) 
otherIsVal = isinstance ( other , Value ) 
if evalFn is None : 
~~~ evalFn = op . _evalFn 
~~ if iamVal and otherIsVal : 
~~~ if ot == BOOL : 
~~~ self = self . _auto_cast ( BOOL ) 
~~ elif t == ot : 
~~ elif isinstance ( ot , Integer ) : 
~~~ other = other . _auto_cast ( t ) 
self . _dtype , other . _dtype ) ) 
~~ return bitsCmp__val ( self , other , op , evalFn ) 
~~~ other = other . _auto_cast ( self . _dtype ) 
if otherIsVal and other . _isFullVld ( ) : 
~~~ res = bitsCmp_detect_useless_cmp ( self , other , op ) 
~~ elif iamVal and self . _isFullVld ( ) : 
~~~ res = bitsCmp_detect_useless_cmp ( other , self , CMP_OP_REVERSE [ op ] ) 
~~ if res is None : 
~~ elif isinstance ( res , Value ) : 
~~~ assert res == AllOps . EQ , res 
op = res 
~~ return Operator . withRes ( op , [ self , other ] , BOOL ) 
~~ ~~ def bitsBitOp ( self , other , op , getVldFn , reduceCheckFn ) : 
if iamVal and otherIsVal : 
return bitsBitOp__val ( self , other , op , getVldFn ) 
~~~ if other . _dtype == BOOL : 
return op . _evalFn ( self , other ) 
~~ elif self . _dtype == other . _dtype : 
( op , self . _dtype , other . _dtype ) ) 
~~ if otherIsVal : 
~~~ r = reduceCheckFn ( self , other ) 
if r is not None : 
~~~ return r 
~~ ~~ elif iamVal : 
~~~ r = reduceCheckFn ( other , self ) 
~~ ~~ return Operator . withRes ( op , [ self , other ] , self . _dtype ) 
valid = False 
val = typeObj . _allValues [ 0 ] 
~~~ if vldMask is None or vldMask == 1 : 
~~~ assert isinstance ( val , str ) 
valid = True 
~~~ valid = False 
val = None 
~~ ~~ return cls ( val , typeObj , valid ) 
~~ def _discover_sensitivity ( self , seen ) -> None : 
casual_sensitivity = set ( ) 
self . switchOn . _walk_sensitivity ( casual_sensitivity , seen , ctx ) 
~~~ raise HwtSyntaxError ( 
~~ ctx . extend ( casual_sensitivity ) 
~~ ~~ def _fill_enclosure ( self , enclosure : Dict [ RtlSignalBase , HdlStatement ] ) -> None : 
select = [ ] 
for e in enclosure . keys ( ) : 
~~~ if e in outputs : 
~~~ select . append ( e ) 
~~ ~~ for ( _ , stms ) , e in zip ( self . cases , self . _case_enclosed_for ) : 
~~~ fill_stm_list_with_enclosure ( self , e , stms , select , enclosure ) 
e . update ( select ) 
~~ t = self . switchOn . _dtype 
default_required = len ( self . cases ) < t . domain_size ( ) 
if self . default is not None or default_required : 
~~~ self . default = fill_stm_list_with_enclosure ( 
self , self . _default_enclosed_for , self . default , select , enclosure ) 
self . _default_enclosed_for . update ( select ) 
~~ self . _enclosed_for . update ( select ) 
~~ def _iter_stms ( self ) : 
for _ , stms in self . cases : 
~~ if self . default is not None : 
~~~ yield from self . default 
~~ ~~ def _is_mergable ( self , other ) -> bool : 
if not isinstance ( other , SwitchContainer ) : 
~~ if not ( self . switchOn is other . switchOn and 
len ( self . cases ) == len ( other . cases ) and 
self . _is_mergable_statement_list ( self . default , other . default ) ) : 
~~ for ( vA , caseA ) , ( vB , caseB ) in zip ( self . cases , other . cases ) : 
~~~ if vA != vB or not self . _is_mergable_statement_list ( caseA , caseB ) : 
newCases = [ ] 
for ( c , caseA ) , ( _ , caseB ) in zip ( self . cases , other . cases ) : 
~~~ newCases . append ( ( c , merge ( caseA , caseB ) ) ) 
~~ self . cases = newCases 
if self . default is not None : 
~~~ self . default = merge ( self . default , other . default ) 
~~ self . _on_merge ( other ) 
~~ def _try_reduce ( self ) -> Tuple [ List [ "HdlStatement" ] , bool ] : 
new_cases = [ ] 
for val , statements in self . cases : 
new_cases . append ( ( val , _statements ) ) 
~~ self . cases = new_cases 
~~~ self . default , rank_decrease , _io_change = self . _try_reduce_list ( 
self . default ) 
~~ reduce_self = not self . _condHasEffect ( ) 
~~~ if self . cases : 
~~~ res = self . cases [ 0 ] [ 1 ] 
~~ elif self . default is not None : 
~~~ res = self . default 
~~~ res = [ ] 
if not self . default : 
~~~ t = self . switchOn . _dtype 
if isinstance ( t , HEnum ) : 
~~~ dom_size = t . domain_size ( ) 
val_cnt = len ( t . _allValues ) 
if len ( self . cases ) == val_cnt and val_cnt < dom_size : 
~~~ _ , stms = self . cases . pop ( ) 
self . default = stms 
~~ ~~ ~~ return res , io_change 
~~ def _condHasEffect ( self ) -> bool : 
if not self . cases : 
~~ type_domain_covered = bool ( self . default ) or len ( 
self . cases ) == self . switchOn . _dtype . domain_size ( ) 
stmCnt = len ( self . cases [ 0 ] ) 
if type_domain_covered and reduce ( 
and_ , 
[ len ( stm ) == stmCnt 
for _ , stm in self . cases ] , 
True ) and ( self . default is None 
or len ( self . default ) == stmCnt ) : 
~~~ stms = list ( self . _iter_stms ( ) ) 
if statementsAreSame ( stms ) : 
~~ if isinstance ( other , SwitchContainer ) and isSameHVal ( self . switchOn , other . switchOn ) and len ( self . cases ) == len ( other . cases ) and isSameStatementList ( self . default , other . default ) : 
~~~ for ( ac , astm ) , ( bc , bstm ) in zip ( self . cases , other . cases ) : 
~~~ if not isSameHVal ( ac , bc ) or not isSameStatementList ( astm , bstm ) : 
~~ def discoverEventDependency ( sig ) : 
~~~ drivers = sig . drivers 
~~~ d = drivers [ 0 ] 
if isinstance ( d , Operator ) : 
~~~ if isEventDependentOp ( d . operator ) : 
~~~ yield ( d . operator , d . operands [ 0 ] ) 
~~~ for op in d . operands : 
~~~ yield from discoverEventDependency ( op ) 
~~ ~~ ~~ ~~ ~~ def getIndent ( indentNum ) : 
~~~ return _indentCache [ indentNum ] 
~~~ i = "" . join ( [ _indent for _ in range ( indentNum ) ] ) 
_indentCache [ indentNum ] = i 
return i 
~~ ~~ def verilogTypeOfSig ( signalItem ) : 
driver_cnt = len ( signalItem . drivers ) 
if signalItem . _const or driver_cnt > 1 or arr_any ( signalItem . drivers , _isEventDependentDriver ) : 
~~~ return SIGNAL_TYPE . REG 
~~~ if driver_cnt == 1 : 
~~~ d = signalItem . drivers [ 0 ] 
if not isinstance ( d , ( Assignment , PortItem ) ) : 
~~ ~~ return SIGNAL_TYPE . WIRE 
~~ ~~ def toHdlConversion ( self , top , topName : str , saveTo : str ) -> List [ str ] : 
return toRtl ( top , 
saveTo = saveTo , 
name = topName , 
serializer = self . serializer , 
targetPlatform = self . targetPlatform ) 
~~ def serializeType ( self , hdlType : HdlType ) -> str : 
def createTmpVar ( suggestedName , dtype ) : 
~~ return VhdlSerializer . HdlType ( hdlType , VhdlSerializer . getBaseContext ( ) ) 
~~ def getVectorFromType ( self , dtype ) -> Union [ bool , None , Tuple [ int , int ] ] : 
if dtype == BIT : 
~~ elif isinstance ( dtype , Bits ) : 
~~~ return [ evalParam ( dtype . width ) - 1 , hInt ( 0 ) ] 
~~ ~~ def getExprVal ( self , val , do_eval = False ) : 
ctx = VhdlSerializer . getBaseContext ( ) 
val ) 
~~ ctx . createTmpVarFn = createTmpVar 
if do_eval : 
~~~ val = val . staticEval ( ) 
~~ val = VivadoTclExpressionSerializer . asHdl ( val , ctx ) 
return val 
~~ def getTypeWidth ( self , dtype : HdlType , do_eval = False ) -> Tuple [ int , Union [ int , RtlSignal ] , bool ] : 
width = dtype . width 
if isinstance ( width , int ) : 
~~~ widthStr = str ( width ) 
~~~ widthStr = self . getExprVal ( width , do_eval = do_eval ) 
~~ return width , widthStr , False 
~~ def getObjDebugName ( self , obj : Union [ Interface , Unit , Param ] ) -> str : 
return obj . _getFullName ( ) 
~~ def serialzeValueToTCL ( self , val , do_eval = False ) -> Tuple [ str , str , bool ] : 
if isinstance ( val , int ) : 
~~~ val = hInt ( val ) 
~~ if do_eval : 
~~ if isinstance ( val , RtlSignalBase ) : 
~~~ ctx = VivadoTclExpressionSerializer . getBaseContext ( ) 
tclVal = VivadoTclExpressionSerializer . asHdl ( val , ctx ) 
tclValVal = VivadoTclExpressionSerializer . asHdl ( 
val . staticEval ( ) ) 
return tclVal , tclValVal , False 
~~~ tclVal = VivadoTclExpressionSerializer . asHdl ( val , None ) 
return tclVal , tclVal , True 
~~ ~~ def nameAvailabilityCheck ( obj , propName , prop ) : 
if getattr ( obj , propName , None ) is not None : 
( obj , propName , repr ( getattr ( obj , propName ) ) , prop ) ) 
~~ ~~ def _registerParameter ( self , pName , parameter ) -> None : 
nameAvailabilityCheck ( self , pName , parameter ) 
~~~ hasName = parameter . _name is not None 
~~~ hasName = False 
~~ if not hasName : 
~~~ parameter . _name = pName 
~~ parameter . _registerScope ( pName , self ) 
if parameter . hasGenericName : 
~~~ parameter . name = pName 
~~ if parameter . _parent is None : 
~~~ parameter . _parent = self 
~~ self . _params . append ( parameter ) 
~~ def _paramsShared ( self , exclude = None , prefix = "" ) -> MakeParamsShared : 
return MakeParamsShared ( self , exclude = exclude , prefix = prefix ) 
~~ def _make_association ( self , clk = None , rst = None ) -> None : 
if clk is not None : 
~~~ assert self . _associatedClk is None 
self . _associatedClk = clk 
~~ if rst is not None : 
~~~ assert self . _associatedRst is None 
self . _associatedRst = rst 
~~ ~~ def _updateParamsFrom ( self , otherObj : "PropDeclrCollector" , updater , exclude : set , prefix : str ) -> None : 
excluded = set ( ) 
~~~ exclude = set ( exclude ) 
~~ for myP in self . _params : 
~~~ pPName = prefix + myP . _scopes [ self ] [ 1 ] 
~~~ otherP = getattr ( otherObj , pPName ) 
if not isinstance ( otherP , Param ) : 
~~ if exclude and otherP in exclude : 
~~~ excluded . add ( otherP ) 
~~ updater ( self , myP , otherP ) 
~~ if exclude is not None : 
~~~ assert excluded == exclude 
~~ ~~ def _registerUnit ( self , uName , unit ) : 
nameAvailabilityCheck ( self , uName , unit ) 
assert unit . _parent is None 
unit . _parent = self 
unit . _name = uName 
self . _units . append ( unit ) 
~~ def _registerInterface ( self , iName , intf , isPrivate = False ) : 
nameAvailabilityCheck ( self , iName , intf ) 
assert intf . _parent is None 
intf . _parent = self 
intf . _name = iName 
intf . _ctx = self . _ctx 
if isPrivate : 
~~~ self . _private_interfaces . append ( intf ) 
intf . _isExtern = False 
~~~ self . _interfaces . append ( intf ) 
intf . _isExtern = True 
~~ ~~ def _registerArray ( self , name , items ) : 
items . _parent = self 
items . _name = name 
~~~ setattr ( self , "%s_%d" % ( name , i ) , item ) 
~~ ~~ def _registerUnitInImpl ( self , uName , u ) : 
self . _registerUnit ( uName , u ) 
u . _loadDeclarations ( ) 
self . _lazyLoaded . extend ( u . _toRtl ( self . _targetPlatform ) ) 
u . _signalsForMyEntity ( self . _ctx , "sig_" + uName ) 
~~ def singleDriver ( self ) : 
drv_cnt = len ( self . drivers ) 
if not drv_cnt : 
~~~ raise NoDriverErr ( self ) 
~~ elif drv_cnt != 1 : 
~~~ raise MultipleDriversErr ( self ) 
~~ return self . drivers [ 0 ] 
~~ def registerSignals ( self , outputs = [ ] ) : 
for o in self . operands : 
~~~ if isinstance ( o , RtlSignalBase ) : 
~~~ if o in outputs : 
~~~ o . drivers . append ( self ) 
~~~ o . endpoints . append ( self ) 
~~ ~~ elif isinstance ( o , Value ) : 
~~ ~~ ~~ def staticEval ( self ) : 
~~~ o . staticEval ( ) 
~~ self . result . _val = self . evalFn ( ) 
~~ def withRes ( opDef , operands , resT , outputs = [ ] ) : 
op = Operator ( opDef , operands ) 
out = RtlSignal ( getCtxFromOps ( operands ) , None , resT ) 
out . _const = arr_all ( op . operands , isConst ) 
out . drivers . append ( op ) 
out . origin = op 
op . result = out 
op . registerSignals ( outputs ) 
if out . _const : 
~~~ out . staticEval ( ) 
~~ def withIndent ( self , indent = 1 ) : 
ctx = copy ( self ) 
ctx . indent += indent 
return ctx 
~~ def _tryConnect ( src , unit , intfName ) : 
~~~ dst = getattr ( unit , intfName ) 
~~ if not dst . _sig . drivers : 
~~~ connect ( src , dst ) 
~~ ~~ def propagateClk ( obj ) : 
clk = obj . clk 
for u in obj . _units : 
~~~ _tryConnect ( clk , u , 'clk' ) 
~~ ~~ def propagateClkRstn ( obj ) : 
rst_n = obj . rst_n 
_tryConnect ( rst_n , u , 'rst_n' ) 
_tryConnect ( ~ rst_n , u , 'rst' ) 
~~ ~~ def propagateClkRst ( obj ) : 
rst = obj . rst 
_tryConnect ( ~ rst , u , 'rst_n' ) 
_tryConnect ( rst , u , 'rst' ) 
~~ ~~ def propagateRstn ( obj ) : 
~~~ _tryConnect ( rst_n , u , 'rst_n' ) 
~~ ~~ def propagateRst ( obj ) : 
~~~ _tryConnect ( ~ rst , u , 'rst_n' ) 
~~ ~~ def fitTo_t ( what : Union [ RtlSignal , Value ] , where_t : HdlType , 
extend : bool = True , shrink : bool = True ) : 
whatWidth = what . _dtype . bit_length ( ) 
toWidth = where_t . bit_length ( ) 
if toWidth == whatWidth : 
~~~ return what 
~~ elif toWidth < whatWidth : 
~~~ if not shrink : 
~~~ raise BitWidthErr ( ) 
~~ return what [ toWidth : ] 
~~~ if not extend : 
~~ w = toWidth - whatWidth 
if what . _dtype . signed : 
~~~ msb = what [ whatWidth - 1 ] 
ext = reduce ( lambda a , b : a . _concat ( b ) , [ msb for _ in range ( w ) ] ) 
~~~ ext = vec ( 0 , w ) 
~~ return ext . _concat ( what ) 
~~ ~~ def iterBits ( sigOrVal : Union [ RtlSignal , Value ] , bitsInOne : int = 1 , 
skipPadding : bool = True , fillup : bool = False ) : 
bw = BitWalker ( sigOrVal , skipPadding , fillup ) 
for _ in range ( ceil ( sigOrVal . _dtype . bit_length ( ) / bitsInOne ) ) : 
~~~ yield bw . get ( bitsInOne ) 
~~ bw . assertIsOnEnd ( ) 
~~ def _get ( self , numberOfBits : int , doCollect : bool ) : 
if not isinstance ( numberOfBits , int ) : 
~~~ numberOfBits = int ( numberOfBits ) 
~~ while self . actuallyHave < numberOfBits : 
~~~ f = next ( self . it ) 
~~~ if self . fillup and self . actual is not None : 
~~~ raise NotEnoughtBitsErr ( ) 
~~ ~~ thisFieldLen = f . _dtype . bit_length ( ) 
if self . actual is None : 
~~~ if not doCollect and thisFieldLen <= numberOfBits : 
~~~ numberOfBits -= thisFieldLen 
~~~ self . actual = f 
self . actuallyHave = thisFieldLen 
~~~ if not doCollect and self . actuallyHave < numberOfBits : 
~~~ self . actuallyHave = thisFieldLen 
self . actual = f 
~~~ self . actuallyHave += thisFieldLen 
self . actual = f . _concat ( self . actual ) 
~~ ~~ ~~ actual = self . actual 
actualOffset = self . actualOffset 
if self . actuallyHave < numberOfBits : 
~~~ assert self . fillup 
if doCollect : 
~~~ t = self . actual . _dtype 
fillupW = numberOfBits - self . actuallyHave 
padding_t = Bits ( fillupW , signed = t . signed , negated = t . negated ) 
padding = padding_t . fromPy ( None ) 
actual = padding . _concat ( actual ) 
~~ self . actuallyHave = 0 
~~ self . actuallyHave -= numberOfBits 
self . actualOffset += numberOfBits 
if self . actuallyHave == 0 : 
~~~ self . actual = None 
self . actualOffset = 0 
~~ if doCollect : 
~~~ if numberOfBits == 1 : 
~~~ return actual [ actualOffset ] 
~~~ return actual [ ( actualOffset + numberOfBits ) : actualOffset ] 
~~ ~~ ~~ def get ( self , numberOfBits : int ) -> Union [ RtlSignal , Value ] : 
return self . _get ( numberOfBits , True ) 
~~ def _serializeExclude_eval ( parentUnit , obj , isDeclaration , priv ) : 
~~~ prepareEntity ( obj , parentUnit . __class__ . __name__ , priv ) 
~~ if priv is None : 
~~~ priv = parentUnit 
~~ return False , priv 
~~ def _serializeOnce_eval ( parentUnit , obj , isDeclaration , priv ) : 
clsName = parentUnit . __class__ . __name__ 
~~~ obj . name = clsName 
~~ elif isDeclaration : 
~~~ prepareEntity ( obj , clsName , parentUnit ) 
~~ serialize = priv is parentUnit 
return serialize , priv 
~~ def _serializeParamsUniq_eval ( parentUnit , obj , isDeclaration , priv ) : 
params = paramsToValTuple ( parentUnit ) 
if priv is None : 
~~~ priv = { } 
~~ if isDeclaration : 
~~~ prevUnit = priv [ params ] 
~~~ priv [ params ] = parentUnit 
return True , priv 
~~ prepareEntity ( obj , prevUnit . _entity . name , prevUnit ) 
return False , priv 
~~ return priv [ params ] is parentUnit , priv 
~~ def _getFullName ( self ) : 
name = "" 
tmp = self 
while isinstance ( tmp , ( InterfaceBase , HObjList ) ) : 
~~~ if hasattr ( tmp , "_name" ) : 
~~~ n = tmp . _name 
~~~ n = '' 
~~ if name == '' : 
~~~ name = n 
~~~ name = n + '.' + name 
~~ if hasattr ( tmp , "_parent" ) : 
~~~ tmp = tmp . _parent 
~~~ tmp = None 
~~ ~~ return name 
~~ def _make_association ( self , * args , ** kwargs ) : 
for o in self : 
~~~ o . _make_association ( * args , ** kwargs ) 
~~ ~~ def _updateParamsFrom ( self , * args , ** kwargs ) : 
~~~ o . _updateParamsFrom ( * args , ** kwargs ) 
~~ ~~ def simPrepare ( unit : Unit , modelCls : Optional [ SimModel ] = None , 
targetPlatform = DummyPlatform ( ) , 
dumpModelIn : str = None , onAfterToRtl = None ) : 
if modelCls is None : 
~~~ modelCls = toSimModel ( 
unit , targetPlatform = targetPlatform , dumpModelIn = dumpModelIn ) 
~~~ toSimModel ( unit ) 
~~ if onAfterToRtl : 
~~~ onAfterToRtl ( unit , modelCls ) 
~~ reconnectUnitSignalsToModel ( unit , modelCls ) 
model = modelCls ( ) 
procs = autoAddAgents ( unit ) 
return unit , model , procs 
~~ def toSimModel ( unit , targetPlatform = DummyPlatform ( ) , dumpModelIn = None ) : 
sim_code = toRtl ( unit , 
targetPlatform = targetPlatform , 
saveTo = dumpModelIn , 
serializer = SimModelSerializer ) 
if dumpModelIn is not None : 
~~~ d = os . path . join ( os . getcwd ( ) , dumpModelIn ) 
dInPath = d in sys . path 
if not dInPath : 
~~~ sys . path . insert ( 0 , d ) 
~~ if unit . _name in sys . modules : 
~~~ del sys . modules [ unit . _name ] 
~~ simModule = importlib . import_module ( unit . _name ) 
~~~ sys . path . remove ( d ) 
~~~ simModule = ModuleType ( 'simModule' ) 
exec ( sim_code , simModule . __dict__ ) 
~~ return simModule . __dict__ [ unit . _name ] 
~~ def reconnectUnitSignalsToModel ( synthesisedUnitOrIntf , modelCls ) : 
obj = synthesisedUnitOrIntf 
subInterfaces = obj . _interfaces 
if subInterfaces : 
~~~ for intf in subInterfaces : 
~~~ reconnectUnitSignalsToModel ( intf , modelCls ) 
~~~ s = synthesisedUnitOrIntf 
s . _sigInside = getattr ( modelCls , s . _sigInside . name ) 
~~ ~~ def simUnitVcd ( simModel , stimulFunctions , outputFile = sys . stdout , 
until = 100 * Time . ns ) : 
if isinstance ( outputFile , str ) : 
~~~ d = os . path . dirname ( outputFile ) 
if d : 
~~~ os . makedirs ( d , exist_ok = True ) 
~~ with open ( outputFile , 'w' ) as f : 
~~~ return _simUnitVcd ( simModel , stimulFunctions , 
f , until ) 
outputFile , until ) 
~~ ~~ def _simUnitVcd ( simModel , stimulFunctions , outputFile , until ) : 
sim = HdlSimulator ( ) 
sim . config = VcdHdlSimConfig ( outputFile ) 
sim . simUnit ( simModel , until = until , extraProcesses = stimulFunctions ) 
return sim 
~~ def oscilate ( sig , period = 10 * Time . ns , initWait = 0 ) : 
def oscillateStimul ( s ) : 
~~~ s . write ( False , sig ) 
halfPeriod = period / 2 
yield s . wait ( initWait ) 
~~~ yield s . wait ( halfPeriod ) 
s . write ( True , sig ) 
yield s . wait ( halfPeriod ) 
~~ ~~ return oscillateStimul 
~~ def onTWriteCallback__init ( self , sim ) : 
yield from self . onTWriteCallback ( sim ) 
self . intf . t . _sigInside . registerWriteCallback ( 
self . onTWriteCallback , 
self . getEnable ) 
self . intf . o . _sigInside . registerWriteCallback ( 
~~ def connectSig ( self , signal ) : 
if self . direction == DIRECTION . IN : 
~~~ if self . src is not None : 
% ( self . name , self . src ) ) 
~~ self . src = signal 
signal . endpoints . append ( self ) 
~~ elif self . direction == DIRECTION . OUT : 
~~~ if self . dst is not None : 
% ( self . name , self . dst ) ) 
~~ self . dst = signal 
signal . drivers . append ( self ) 
~~~ raise NotImplementedError ( self ) 
~~ signal . hidden = False 
signal . ctx . subUnits . add ( self . unit ) 
~~ def registerInternSig ( self , signal ) : 
if self . direction == DIRECTION . OUT : 
% ( self . name , str ( self . src ) ) ) 
~~ elif self . direction == DIRECTION . IN : 
% ( self . name , str ( self . dst ) ) ) 
~~~ raise NotImplementedError ( self . direction ) 
~~ ~~ def connectInternSig ( self ) : 
d = self . direction 
if d == DIRECTION . OUT : 
~~~ self . src . endpoints . append ( self ) 
~~ elif d == DIRECTION . IN or d == DIRECTION . INOUT : 
~~~ self . dst . drivers . append ( self ) 
~~~ raise NotImplementedError ( d ) 
~~ ~~ def getInternSig ( self ) : 
if d == DIRECTION . IN : 
~~~ return self . dst 
~~ elif d == DIRECTION . OUT : 
~~~ return self . src 
~~ ~~ def isEvDependentOn ( sig , process ) -> bool : 
if sig is None : 
~~ return process in sig . simFallingSensProcs or process in sig . simRisingSensProcs 
~~ def _add_process ( self , proc , priority ) -> None : 
self . _events . push ( self . now , priority , proc ) 
~~ def _addHdlProcToRun ( self , trigger : SimSignal , proc ) -> None : 
if not self . _applyValPlaned : 
~~~ self . _scheduleApplyValues ( ) 
~~ if isEvDependentOn ( trigger , proc ) : 
~~~ if self . now == 0 : 
~~ self . _seqProcsToRun . append ( proc ) 
~~~ self . _combProcsToRun . append ( proc ) 
~~ ~~ def _initUnitSignals ( self , unit : Unit ) -> None : 
for s in unit . _ctx . signals : 
~~~ if s . defVal . vldMask : 
~~~ v = s . defVal . clone ( ) 
s . simUpdateVal ( self , mkUpdater ( v , False ) ) 
~~ ~~ for u in unit . _units : 
~~~ self . _initUnitSignals ( u ) 
~~ for p in unit . _processes : 
~~~ self . _addHdlProcToRun ( None , p ) 
~~ for p , outputs in unit . _outputs . items ( ) : 
~~~ containerNames = list ( map ( lambda x : x [ 0 ] , outputs ) ) 
class SpecificIoContainer ( IoContainer ) : 
~~~ __slots__ = containerNames 
~~ self . _outputContainers [ p ] = SpecificIoContainer ( outputs ) 
~~ ~~ def _scheduleCombUpdateDoneEv ( self ) -> Event : 
assert not self . _combUpdateDonePlaned , self . now 
cud = Event ( self ) 
cud . process_to_wake . append ( self . __deleteCombUpdateDoneEv ( ) ) 
self . _add_process ( cud , PRIORITY_AGENTS_UPDATE_DONE ) 
self . _combUpdateDonePlaned = True 
self . combUpdateDoneEv = cud 
return cud 
~~ def _scheduleApplyValues ( self ) -> None : 
assert not self . _applyValPlaned , self . now 
self . _add_process ( self . _applyValues ( ) , PRIORITY_APPLY_COMB ) 
self . _applyValPlaned = True 
if self . _runSeqProcessesPlaned : 
~~ assert not self . _seqProcsToRun and not self . _runSeqProcessesPlaned , self . now 
self . _add_process ( self . _runSeqProcesses ( ) , PRIORITY_APPLY_SEQ ) 
self . _runSeqProcessesPlaned = True 
~~ def _conflictResolveStrategy ( self , newValue : set ) -> Tuple [ Callable [ [ Value ] , bool ] , bool ] : 
invalidate = False 
resLen = len ( newValue ) 
if resLen == 3 : 
~~~ val , indexes , isEvDependent = newValue 
return ( mkArrayUpdater ( val , indexes , invalidate ) , isEvDependent ) 
~~~ val , isEvDependent = newValue 
return ( mkUpdater ( val , invalidate ) , isEvDependent ) 
~~ ~~ def _runCombProcesses ( self ) -> None : 
for proc in self . _combProcsToRun : 
~~~ cont = self . _outputContainers [ proc ] 
proc ( self , cont ) 
for sigName , sig in cont . _all_signals : 
~~~ newVal = getattr ( cont , sigName ) 
if newVal is not None : 
~~~ res = self . _conflictResolveStrategy ( newVal ) 
updater , isEvDependent = res 
self . _valuesToApply . append ( 
( sig , updater , isEvDependent , proc ) ) 
setattr ( cont , sigName , None ) 
~~ ~~ ~~ self . _combProcsToRun = UniqList ( ) 
~~ def _runSeqProcesses ( self ) -> Generator [ None , None , None ] : 
updates = [ ] 
for proc in self . _seqProcsToRun : 
~~~ outContainer = self . _outputContainers [ proc ] 
~~~ outContainer = None 
~~ proc ( self , outContainer ) 
if outContainer is not None : 
~~~ updates . append ( outContainer ) 
~~ ~~ self . _seqProcsToRun = UniqList ( ) 
self . _runSeqProcessesPlaned = False 
for cont in updates : 
~~~ for sigName , sig in cont . _all_signals : 
~~~ v = self . _conflictResolveStrategy ( newVal ) 
updater , _ = v 
sig . simUpdateVal ( self , updater ) 
~~ def _applyValues ( self ) -> Generator [ None , None , None ] : 
va = self . _valuesToApply 
self . _applyValPlaned = False 
lav = self . config . logApplyingValues 
if va and lav : 
~~~ lav ( self , va ) 
~~ self . _valuesToApply = [ ] 
addSp = self . _seqProcsToRun . append 
for s , vUpdater , isEventDependent , comesFrom in va : 
~~~ if isEventDependent : 
~~~ addSp ( comesFrom ) 
~~~ s . simUpdateVal ( self , vUpdater ) 
~~ ~~ self . _runCombProcesses ( ) 
if self . _valuesToApply and not self . _applyValPlaned : 
~~ def read ( self , sig ) -> Value : 
~~~ v = sig . _val 
~~~ v = sig . _sigInside . _val 
~~ return v . clone ( ) 
~~ def write ( self , val , sig : SimSignal ) -> None : 
~~~ simSensProcs = sig . simSensProcs 
~~~ sig = sig . _sigInside 
simSensProcs = sig . simSensProcs 
~~ t = sig . _dtype 
if isinstance ( val , Value ) : 
~~~ v = val . clone ( ) 
v = v . _auto_cast ( t ) 
~~~ v = t . fromPy ( val ) 
~~ sig . simUpdateVal ( self , lambda curentV : ( 
valueHasChanged ( curentV , v ) , v ) ) 
~~~ if not ( simSensProcs or 
sig . simRisingSensProcs or 
sig . simFallingSensProcs ) : 
~~ elif ( sig . _writeCallbacks or 
sig . _writeCallbacksToEn ) : 
~~ ~~ ~~ def run ( self , until : float ) -> None : 
assert until > self . now 
events = self . _events 
schedule = events . push 
next_event = events . pop 
schedule ( until , PRIORITY_URGENT , raise_StopSimulation ( self ) ) 
~~~ nextTime , priority , process = next_event ( ) 
self . now = nextTime 
if isinstance ( process , Event ) : 
~~~ process = iter ( process ) 
~~~ ev = next ( process ) 
~~ if isinstance ( ev , Wait ) : 
~~~ schedule ( nextTime + ev . time , priority , process ) 
~~ elif isinstance ( ev , Event ) : 
~~~ ev . process_to_wake . append ( process ) 
~~~ schedule ( nextTime , priority , ev ) 
~~ ~~ ~~ ~~ except StopSimumulation : 
~~ ~~ def add_process ( self , proc ) -> None : 
self . _events . push ( self . now , PRIORITY_NORMAL , proc ) 
~~ def simUnit ( self , synthesisedUnit : Unit , until : float , extraProcesses = [ ] ) : 
beforeSim = self . config . beforeSim 
if beforeSim is not None : 
~~~ beforeSim ( self , synthesisedUnit ) 
~~ add_proc = self . add_process 
for p in extraProcesses : 
~~~ add_proc ( p ( self ) ) 
~~ self . _initUnitSignals ( synthesisedUnit ) 
self . run ( until ) 
~~ def _mkOp ( fn ) : 
def op ( * operands , key = None ) -> RtlSignalBase : 
assert operands , operands 
top = None 
if key is not None : 
~~~ operands = map ( key , operands ) 
~~ for s in operands : 
~~~ if top is None : 
~~~ top = s 
~~~ top = fn ( top , s ) 
~~ ~~ return top 
~~ return op 
~~ def systemCTypeOfSig ( signalItem ) : 
if signalItem . _const or arr_any ( signalItem . drivers , 
lambda d : isinstance ( d , HdlStatement ) 
and d . _now_is_event_dependent ) : 
~~~ return SIGNAL_TYPE . WIRE 
~~ ~~ def ternaryOpsToIf ( statements ) : 
stms = [ ] 
for st in statements : 
~~~ if isinstance ( st , Assignment ) : 
~~~ if not isinstance ( st . src , RtlSignalBase ) : 
~~~ raise DoesNotContainsTernary ( ) 
~~ d = st . src . singleDriver ( ) 
if not isinstance ( d , Operator ) or d . operator != AllOps . TERNARY : 
~~~ ops = d . operands 
ifc = IfContainer ( ops [ 0 ] , 
[ Assignment ( ops [ 1 ] , st . dst ) ] , 
[ Assignment ( ops [ 2 ] , st . dst ) ] 
stms . append ( ifc ) 
~~ ~~ except ( MultipleDriversErr , DoesNotContainsTernary ) : 
~~ except NoDriverErr : 
~~~ assert ( hasattr ( st . src , "_interface" ) 
and st . src . _interface is not None ) or st . src . defVal . vldMask , st . src 
~~ ~~ stms . append ( st ) 
~~ return stms 
~~ def HWProcess ( cls , proc , ctx ) : 
extraVars = [ ] 
extraVarsSerialized = [ ] 
hasToBeVhdlProcess = arr_any ( body , 
lambda x : isinstance ( x , 
( IfContainer , 
SwitchContainer , 
WhileContainer , 
WaitStm ) ) ) 
sensitivityList = sorted ( 
map ( lambda s : cls . sensitivityListItem ( s , ctx ) , 
proc . sensitivityList ) ) 
if hasToBeVhdlProcess : 
~~~ childCtx = ctx . withIndent ( ) 
~~~ childCtx = copy ( ctx ) 
~~ def createTmpVarFn ( suggestedName , dtype ) : 
~~~ s = RtlSignal ( None , None , dtype , virtualOnly = True ) 
s . name = ctx . scope . checkedName ( suggestedName , s ) 
serializedS = cls . SignalItem ( s , childCtx , declaration = True ) 
extraVars . append ( s ) 
extraVarsSerialized . append ( serializedS ) 
~~ childCtx . createTmpVarFn = createTmpVarFn 
extraVarsInit = [ ] 
for s in extraVars : 
~~~ if isinstance ( s . defVal , RtlSignalBase ) or s . defVal . vldMask : 
~~~ a = Assignment ( s . defVal , s , virtualOnly = True ) 
extraVarsInit . append ( cls . Assignment ( a , childCtx ) ) 
~~~ assert s . drivers , s 
~~ for d in s . drivers : 
~~~ extraVarsInit . append ( cls . asHdl ( d , childCtx ) ) 
~~ ~~ _hasToBeVhdlProcess = hasToBeVhdlProcess 
hasToBeVhdlProcess = extraVars or hasToBeVhdlProcess 
if hasToBeVhdlProcess and not _hasToBeVhdlProcess : 
~~~ oneIndent = getIndent ( 1 ) 
statemets = list ( map ( lambda x : oneIndent + x , statemets ) ) 
~~ return cls . processTmpl . render ( 
hasToBeVhdlProcess = hasToBeVhdlProcess , 
extraVars = extraVarsSerialized , 
statements = extraVarsInit + statemets 
~~ def setup_platform ( hass , config , add_entities , discovery_info = None ) : 
host = config . get ( CONF_HOST ) 
token = config . get ( CONF_ACCESS_TOKEN ) 
name = config . get ( CONF_NAME ) 
volume_step = config . get ( CONF_VOLUME_STEP ) 
device_type = config . get ( CONF_DEVICE_CLASS ) 
device = VizioDevice ( host , token , name , volume_step , device_type ) 
if device . validate_setup ( ) is False : 
~~ elif ( token is None or token == "" ) and device_type == "tv" : 
~~ if config . get ( CONF_SUPPRESS_WARNING ) : 
~~~ from requests . packages import urllib3 
urllib3 . disable_warnings ( urllib3 . exceptions . InsecureRequestWarning ) 
~~ add_entities ( [ device ] , True ) 
~~ def update ( self ) : 
is_on = self . _device . get_power_state ( ) 
if is_on : 
~~~ self . _state = STATE_ON 
volume = self . _device . get_current_volume ( ) 
if volume is not None : 
~~~ self . _volume_level = float ( volume ) / self . _max_volume 
~~ input_ = self . _device . get_current_input ( ) 
if input_ is not None : 
~~~ self . _current_input = input_ . meta_name 
~~ inputs = self . _device . get_inputs ( ) 
if inputs is not None : 
~~~ self . _available_inputs = [ input_ . name for input_ in inputs ] 
~~~ if is_on is None : 
~~~ self . _state = None 
~~~ self . _state = STATE_OFF 
~~ self . _volume_level = None 
self . _current_input = None 
self . _available_inputs = None 
~~ ~~ def mute_volume ( self , mute ) : 
if mute : 
~~~ self . _device . mute_on ( ) 
~~~ self . _device . mute_off ( ) 
~~ ~~ def volume_up ( self ) : 
self . _volume_level += self . _volume_step / self . _max_volume 
self . _device . vol_up ( num = self . _volume_step ) 
~~ def volume_down ( self ) : 
self . _volume_level -= self . _volume_step / self . _max_volume 
self . _device . vol_down ( num = self . _volume_step ) 
~~ def set_volume_level ( self , volume ) : 
if self . _volume_level is not None : 
~~~ if volume > self . _volume_level : 
~~~ num = int ( self . _max_volume * ( volume - self . _volume_level ) ) 
self . _volume_level = volume 
self . _device . vol_up ( num = num ) 
~~ elif volume < self . _volume_level : 
~~~ num = int ( self . _max_volume * ( self . _volume_level - volume ) ) 
self . _device . vol_down ( num = num ) 
~~ ~~ ~~ def parse_commits ( data ) : 
raw_commits = RE_COMMIT . finditer ( data ) 
for rc in raw_commits : 
~~~ full_commit = rc . groups ( ) [ 0 ] 
parts = RE_COMMIT . match ( full_commit ) . groupdict ( ) 
parsed_commit = parse_commit ( parts ) 
yield parsed_commit 
~~ ~~ def parse_commit ( parts ) : 
commit = { } 
commit [ 'commit' ] = parts [ 'commit' ] 
commit [ 'tree' ] = parts [ 'tree' ] 
parent_block = parts [ 'parents' ] 
commit [ 'parents' ] = [ 
parse_parent_line ( parentline ) 
for parentline in 
parent_block . splitlines ( ) 
commit [ 'author' ] = parse_author_line ( parts [ 'author' ] ) 
commit [ 'committer' ] = parse_committer_line ( parts [ 'committer' ] ) 
message_lines = [ 
parse_message_line ( msgline ) 
for msgline in 
parts [ 'message' ] . split ( "\\n" ) 
commit [ 'message' ] = "\\n" . join ( 
msgline 
message_lines 
if msgline is not None 
commit [ 'changes' ] = [ 
parse_numstat_line ( numstat ) 
for numstat in 
parts [ 'numstats' ] . splitlines ( ) 
return commit 
~~ def run_git_log ( git_dir = None , git_since = None ) : 
import subprocess 
if git_dir : 
~~~ command = [ 
'git' , 
'--git-dir=' + git_dir , 
'log' , 
'--numstat' , 
'--pretty=raw' 
~~~ command = [ 'git' , 'log' , '--numstat' , '--pretty=raw' ] 
~~ if git_since is not None : 
~~~ command . append ( '--since=' + git_since ) 
~~ raw_git_log = subprocess . Popen ( 
command , 
stdout = subprocess . PIPE 
if sys . version_info < ( 3 , 0 ) : 
~~~ return raw_git_log . stdout 
~~~ return raw_git_log . stdout . read ( ) . decode ( 'utf-8' , 'ignore' ) 
~~ ~~ def load_config_from_cli ( config : GoodConf , argv : List [ str ] ) -> List [ str ] : 
original_parser = BaseCommand . create_parser 
def patched_parser ( self , prog_name , subcommand ) : 
~~~ parser = original_parser ( self , prog_name , subcommand ) 
argparser_add_argument ( parser , config ) 
~~ BaseCommand . create_parser = patched_parser 
~~~ parser = argparse . ArgumentParser ( add_help = False ) 
config_arg , default_args = parser . parse_known_args ( argv ) 
config . load ( config_arg . config ) 
yield default_args 
~~~ BaseCommand . create_parser = original_parser 
~~ ~~ def execute_from_command_line_with_config ( config : GoodConf , argv : List [ str ] ) : 
~~~ """Load\ 
with load_config_from_cli ( config , argv ) as args : 
~~~ from django . core . management import execute_from_command_line 
execute_from_command_line ( args ) 
~~ ~~ def argparser_add_argument ( parser : argparse . ArgumentParser , config : GoodConf ) : 
if config . file_env_var : 
~~ if config . default_files : 
~~ parser . add_argument ( '-C' , '--config' , metavar = 'FILE' , help = help ) 
~~ def _load_config ( path : str ) -> dict : 
__ , ext = os . path . splitext ( path ) 
if ext in [ '.yaml' , '.yml' ] : 
~~~ import ruamel . yaml 
loader = ruamel . yaml . safe_load 
~~~ loader = json . load 
~~ with open ( path ) as f : 
~~~ config = loader ( f ) 
~~ def load ( self , filename : str = None ) : 
~~~ self . config_file = _find_file ( filename ) 
~~~ if self . file_env_var and self . file_env_var in os . environ : 
~~~ self . config_file = _find_file ( os . environ [ self . file_env_var ] ) 
~~ if not self . config_file : 
~~~ for filename in self . default_files : 
~~~ self . config_file = _find_file ( filename , require = False ) 
if self . config_file : 
~~ ~~ ~~ ~~ if self . config_file : 
~~~ config = _load_config ( self . config_file ) 
~~~ config = { } 
~~ self . set_values ( config ) 
~~ def generate_yaml ( cls , ** override ) : 
import ruamel . yaml 
yaml_str = StringIO ( ) 
yaml . dump ( cls . get_initial ( ** override ) , stream = yaml_str ) 
yaml_str . seek ( 0 ) 
dict_from_yaml = yaml . load ( yaml_str ) 
if cls . __doc__ : 
~~~ dict_from_yaml . yaml_set_start_comment ( 
'\\n' + cls . __doc__ + '\\n\\n' ) 
~~ for k in dict_from_yaml . keys ( ) : 
~~~ if cls . _values [ k ] . help : 
~~~ dict_from_yaml . yaml_set_comment_before_after_key ( 
k , before = '\\n' + cls . _values [ k ] . help ) 
~~ ~~ yaml_str = StringIO ( ) 
yaml . dump ( dict_from_yaml , yaml_str ) 
return yaml_str . read ( ) 
~~ def generate_markdown ( cls ) : 
~~ for k , v in cls . _values . items ( ) : 
if v . required : 
~~ if v . help : 
if v . default is not None : 
~~ def cast ( self , val : str ) : 
~~~ return getattr ( self , 'cast_as_{}' . format ( 
self . cast_as . __name__ . lower ( ) ) ) ( val ) 
~~~ return self . cast_as ( val ) 
~~ ~~ def animate ( frames , interval , name , iterations = 2 ) : 
for i in range ( iterations ) : 
~~~ for frame in frames : 
~~~ frame = get_coded_text ( frame ) 
sys . stdout . write ( output ) 
sys . stdout . write ( CLEAR_LINE ) 
sys . stdout . flush ( ) 
time . sleep ( 0.001 * interval ) 
~~ ~~ ~~ def tostring ( self , cnf ) : 
self . varname_dict = { } 
self . varobj_dict = { } 
varis = set ( ) 
for d in cnf . dis : 
~~~ for v in d : 
~~~ varis . add ( v . name ) 
varis = dict ( list ( zip ( sorted ( list ( varis ) ) , list ( map ( str , list ( range ( 1 , len ( varis ) + 1 ) ) ) ) ) ) ) 
for v in varis : 
~~~ vo = Variable ( v ) 
self . varname_dict [ vo ] = varis [ v ] 
self . varobj_dict [ varis [ v ] ] = vo 
~~ for d in cnf . dis : 
~~~ ret += "\\n" 
vnamelist = [ ] 
for v in d : 
~~~ vnamelist . append ( ( "-" if v . inverted else "" ) + varis [ v . name ] ) 
~~ def reduceCnf ( cnf ) : 
output = Cnf ( ) 
for x in cnf . dis : 
~~~ dont_add = False 
for y in x : 
~~~ for z in x : 
~~~ if z == - y : 
~~~ dont_add = True 
~~ ~~ if dont_add : break 
~~ if dont_add : continue 
if x not in output . dis : 
~~~ output . dis |= frozenset ( [ x ] ) 
~~ ~~ return output 
~~ def load ( self , name ) : 
s = self . sets . get ( name ) 
if s is None : 
~~~ self . sets [ name ] = s = np . load ( self . path ( 'jpl-%s.npy' % name ) ) 
~~ def position ( self , name , tdb , tdb2 = 0.0 ) : 
bundle = self . compute_bundle ( name , tdb , tdb2 ) 
return self . position_from_bundle ( bundle ) 
~~ def position_and_velocity ( self , name , tdb , tdb2 = 0.0 ) : 
position = self . position_from_bundle ( bundle ) 
velocity = self . velocity_from_bundle ( bundle ) 
return position , velocity 
~~ def compute ( self , name , tdb ) : 
bundle = self . compute_bundle ( name , tdb , 0.0 ) 
return np . concatenate ( ( position , velocity ) ) 
~~ def compute_bundle ( self , name , tdb , tdb2 = 0.0 ) : 
input_was_scalar = getattr ( tdb , 'shape' , ( ) ) == ( ) 
if input_was_scalar : 
~~~ tdb = np . array ( ( tdb , ) ) 
~~ coefficient_sets = self . load ( name ) 
number_of_sets , axis_count , coefficient_count = coefficient_sets . shape 
jalpha , jomega = self . jalpha , self . jomega 
days_per_set = ( jomega - jalpha ) / number_of_sets 
index , offset = divmod ( ( tdb - jalpha ) + tdb2 , days_per_set ) 
index = index . astype ( int ) 
if ( index < 0 ) . any ( ) or ( number_of_sets < index ) . any ( ) : 
% ( self . name , jalpha , jomega ) ) 
~~ omegas = ( index == number_of_sets ) 
index [ omegas ] -= 1 
offset [ omegas ] += days_per_set 
coefficients = np . rollaxis ( coefficient_sets [ index ] , 1 ) 
T = np . empty ( ( coefficient_count , len ( index ) ) ) 
T [ 0 ] = 1.0 
T [ 1 ] = t1 = 2.0 * offset / days_per_set - 1.0 
twot1 = t1 + t1 
for i in range ( 2 , coefficient_count ) : 
~~~ T [ i ] = twot1 * T [ i - 1 ] - T [ i - 2 ] 
~~ bundle = coefficients , days_per_set , T , twot1 
return bundle 
~~ def position_from_bundle ( self , bundle ) : 
coefficients , days_per_set , T , twot1 = bundle 
return ( T . T * coefficients ) . sum ( axis = 2 ) 
~~ def velocity_from_bundle ( self , bundle ) : 
coefficient_count = coefficients . shape [ 2 ] 
dT = np . empty_like ( T ) 
dT [ 0 ] = 0.0 
dT [ 1 ] = 1.0 
dT [ 2 ] = twot1 + twot1 
for i in range ( 3 , coefficient_count ) : 
~~~ dT [ i ] = twot1 * dT [ i - 1 ] - dT [ i - 2 ] + T [ i - 1 ] + T [ i - 1 ] 
~~ dT *= 2.0 
dT /= days_per_set 
return ( dT . T * coefficients ) . sum ( axis = 2 ) 
~~ def read_record ( self , n ) : 
self . file . seek ( n * K - K ) 
return self . file . read ( K ) 
~~ def write_record ( self , n , data ) : 
return self . file . write ( data ) 
~~ def map_words ( self , start , end ) : 
i , j = 8 * start - 8 , 8 * end 
~~~ fileno = self . file . fileno ( ) 
~~ except ( AttributeError , io . UnsupportedOperation ) : 
~~~ fileno = None 
~~ if fileno is None : 
~~~ skip = 0 
self . file . seek ( i ) 
m = self . file . read ( j - i ) 
~~~ skip = i % mmap . ALLOCATIONGRANULARITY 
r = mmap . ACCESS_READ 
m = mmap . mmap ( fileno , length = j - i + skip , access = r , offset = i - skip ) 
~~ if sys . version_info > ( 3 , ) : 
~~ return m , skip 
~~ def comments ( self ) : 
record_numbers = range ( 2 , self . fward ) 
if not record_numbers : 
~~ data = b'' . join ( self . read_record ( n ) [ 0 : 1000 ] for n in record_numbers ) 
~~~ return data [ : data . find ( b'\\4' ) ] . decode ( 'ascii' ) . replace ( '\\0' , '\\n' ) 
~~ ~~ def read_array ( self , start , end ) : 
f = self . file 
f . seek ( 8 * ( start - 1 ) ) 
length = 1 + end - start 
data = f . read ( 8 * length ) 
return ndarray ( length , self . endian + 'd' , data ) 
~~ def map_array ( self , start , end ) : 
if self . _array is None : 
~~~ self . _map , skip = self . map_words ( 1 , self . free - 1 ) 
assert skip == 0 
self . _array = ndarray ( self . free - 1 , self . endian + 'd' , self . _map ) 
~~ return self . _array [ start - 1 : end ] 
~~ def summary_records ( self ) : 
record_number = self . fward 
unpack = self . summary_control_struct . unpack 
while record_number : 
~~~ data = self . read_record ( record_number ) 
next_number , previous_number , n_summaries = unpack ( data [ : 24 ] ) 
yield record_number , n_summaries , data 
record_number = int ( next_number ) 
~~ ~~ def summaries ( self ) : 
length = self . summary_length 
step = self . summary_step 
for record_number , n_summaries , summary_data in self . summary_records ( ) : 
~~~ name_data = self . read_record ( record_number + 1 ) 
for i in range ( 0 , int ( n_summaries ) * step , step ) : 
~~~ j = self . summary_control_struct . size + i 
name = name_data [ i : i + step ] . strip ( ) 
data = summary_data [ j : j + length ] 
values = self . summary_struct . unpack ( data ) 
yield name , values 
~~ ~~ ~~ def add_array ( self , name , values , array ) : 
scs = self . summary_control_struct 
record_number = self . bward 
data = bytearray ( self . read_record ( record_number ) ) 
next_record , previous_record , n_summaries = scs . unpack ( data [ : 24 ] ) 
if n_summaries < self . summaries_per_record : 
~~~ summary_record = record_number 
name_record = summary_record + 1 
data [ : 24 ] = scs . pack ( next_record , previous_record , n_summaries + 1 ) 
self . write_record ( summary_record , data ) 
~~~ summary_record = ( ( self . free - 1 ) * 8 + 1023 ) // 1024 + 1 
free_record = summary_record + 2 
n_summaries = 0 
data [ : 24 ] = scs . pack ( summary_record , previous_record , n_summaries ) 
self . write_record ( record_number , data ) 
summaries = scs . pack ( 0 , record_number , 1 ) . ljust ( 1024 , b'\\0' ) 
names = b'\\0' * 1024 
self . write_record ( summary_record , summaries ) 
self . write_record ( name_record , names ) 
self . bward = summary_record 
self . free = ( free_record - 1 ) * 1024 // 8 + 1 
~~ start_word = self . free 
f . seek ( ( start_word - 1 ) * 8 ) 
f . write ( array . view ( ) ) 
end_word = f . tell ( ) // 8 
self . free = end_word + 1 
self . write_file_record ( ) 
values = values [ : self . nd + self . ni - 2 ] + ( start_word , end_word ) 
base = 1024 * ( summary_record - 1 ) 
offset = int ( n_summaries ) * self . summary_step 
f . seek ( base + scs . size + offset ) 
f . write ( self . summary_struct . pack ( * values ) ) 
f . seek ( base + 1024 + offset ) 
self . daf . file . close ( ) 
for segment in self . segments : 
~~~ if hasattr ( segment , '_data' ) : 
~~~ del segment . _data 
~~ ~~ self . daf . _array = None 
self . daf . _map = None 
~~ def describe ( self , verbose = True ) : 
if verbose : 
. format ( self , self . source . decode ( 'ascii' ) ) ) 
~~ def compute ( self , tdb , tdb2 = 0.0 ) : 
for position in self . generate ( tdb , tdb2 ) : 
~~~ return position 
~~ ~~ def close ( self ) : 
~~ ~~ ~~ def describe ( self , verbose = True ) : 
if self . data_type == 2 : 
~~~ component_count = 3 
~~ init , intlen , rsize , n = self . daf . read_array ( self . end_i - 3 , self . end_i ) 
initial_epoch = jd ( init ) 
interval_length = intlen / S_PER_DAY 
coefficient_count = int ( rsize - 2 ) // component_count 
coefficients = self . daf . map_array ( self . start_i , self . end_i - 4 ) 
coefficients . shape = ( int ( n ) , int ( rsize ) ) 
coefficients . shape = ( int ( n ) , component_count , coefficient_count ) 
coefficients = rollaxis ( coefficients , 1 ) 
return initial_epoch , interval_length , coefficients 
~~ def compute ( self , tdb , tdb2 , derivative = True ) : 
scalar = not getattr ( tdb , 'shape' , 0 ) and not getattr ( tdb2 , 'shape' , 0 ) 
if scalar : 
~~~ tdb = array ( ( tdb , ) ) 
~~ data = self . _data 
~~~ self . _data = data = self . _load ( ) 
~~ initial_epoch , interval_length , coefficients = data 
component_count , n , coefficient_count = coefficients . shape 
index , offset = divmod ( ( tdb - initial_epoch ) + tdb2 , interval_length ) 
if ( index < 0 ) . any ( ) or ( index > n ) . any ( ) : 
~~~ final_epoch = initial_epoch + interval_length * n 
% ( initial_epoch , final_epoch ) ) 
~~ omegas = ( index == n ) 
offset [ omegas ] += interval_length 
coefficients = coefficients [ : , index ] 
T = empty ( ( coefficient_count , len ( index ) ) ) 
T [ 1 ] = t1 = 2.0 * offset / interval_length - 1.0 
~~ components = ( T . T * coefficients ) . sum ( axis = 2 ) 
~~~ components = components [ : , 0 ] 
~~ if not derivative : 
~~~ return components 
~~ dT = empty_like ( T ) 
if coefficient_count > 2 : 
~~~ dT [ 2 ] = twot1 + twot1 
~~ ~~ dT *= 2.0 
dT /= interval_length 
rates = ( dT . T * coefficients ) . sum ( axis = 2 ) 
~~~ rates = rates [ : , 0 ] 
~~ return components , rates 
~~ def notify ( msg , msg_type = 0 , t = None ) : 
if platform . system ( ) == 'Darwin' : 
~~~ command = notify_command_osx ( msg , msg_type , t ) 
~~~ command = notify_command_linux ( msg , t ) 
~~ os . system ( command . encode ( 'utf-8' ) ) 
~~ def geturls_new_api ( song_ids ) : 
alters = NetEase ( ) . songs_detail_new_api ( song_ids ) 
urls = [ alter [ 'url' ] for alter in alters ] 
return urls 
~~ def visit_Call ( self , node ) : 
if self . within_logging_statement ( ) : 
~~~ if self . within_logging_argument ( ) and self . is_format_call ( node ) : 
~~~ self . violations . append ( ( node , STRING_FORMAT_VIOLATION ) ) 
super ( LoggingVisitor , self ) . generic_visit ( node ) 
~~ ~~ logging_level = self . detect_logging_level ( node ) 
if logging_level and self . current_logging_level is None : 
~~~ self . current_logging_level = logging_level 
~~ if logging_level is None : 
~~~ super ( LoggingVisitor , self ) . generic_visit ( node ) 
~~ self . current_logging_call = node 
if logging_level == "warn" : 
~~~ self . violations . append ( ( node , WARN_VIOLATION ) ) 
~~ self . check_exc_info ( node ) 
for index , child in enumerate ( iter_child_nodes ( node ) ) : 
~~~ if index == 1 : 
~~~ self . current_logging_argument = child 
~~ if index >= 1 : 
~~~ self . check_exception_arg ( child ) 
~~ if index > 1 and isinstance ( child , keyword ) and child . arg == "extra" : 
~~~ self . current_extra_keyword = child 
~~ super ( LoggingVisitor , self ) . visit ( child ) 
self . current_logging_argument = None 
self . current_extra_keyword = None 
~~ self . current_logging_call = None 
self . current_logging_level = None 
~~ def visit_BinOp ( self , node ) : 
if self . within_logging_statement ( ) and self . within_logging_argument ( ) : 
~~~ if isinstance ( node . op , Mod ) : 
~~~ self . violations . append ( ( node , PERCENT_FORMAT_VIOLATION ) ) 
~~ if isinstance ( node . op , Add ) : 
~~~ self . violations . append ( ( node , STRING_CONCAT_VIOLATION ) ) 
~~ ~~ super ( LoggingVisitor , self ) . generic_visit ( node ) 
~~ def visit_Dict ( self , node ) : 
if self . should_check_whitelist ( node ) : 
~~~ for key in node . keys : 
~~~ if key . s in self . whitelist or key . s . startswith ( "debug_" ) : 
~~ self . violations . append ( ( self . current_logging_call , WHITELIST_VIOLATION . format ( key . s ) ) ) 
~~ ~~ if self . should_check_extra_exception ( node ) : 
~~~ for value in node . values : 
~~~ self . check_exception_arg ( value ) 
~~ def visit_JoinedStr ( self , node ) : 
if version_info >= ( 3 , 6 ) : 
~~~ if self . within_logging_statement ( ) : 
~~~ if any ( isinstance ( i , FormattedValue ) for i in node . values ) : 
~~~ if self . within_logging_argument ( ) : 
~~~ self . violations . append ( ( node , FSTRING_VIOLATION ) ) 
~~ ~~ ~~ ~~ ~~ def visit_keyword ( self , node ) : 
~~~ if node . arg not in self . whitelist and not node . arg . startswith ( "debug_" ) : 
~~~ self . violations . append ( ( self . current_logging_call , WHITELIST_VIOLATION . format ( node . arg ) ) ) 
~~~ self . check_exception_arg ( node . value ) 
~~ super ( LoggingVisitor , self ) . generic_visit ( node ) 
~~ def visit_ExceptHandler ( self , node ) : 
name = self . get_except_handler_name ( node ) 
~~ self . current_except_names . append ( name ) 
self . current_except_names . pop ( ) 
~~ def detect_logging_level ( self , node ) : 
~~~ if self . get_id_attr ( node . func . value ) == "warnings" : 
~~ if node . func . attr in LOGGING_LEVELS : 
~~~ return node . func . attr 
~~ def get_except_handler_name ( self , node ) : 
~~ if version_info < ( 3 , ) : 
~~~ return name . id 
~~ return name 
~~ def get_id_attr ( self , value ) : 
if not hasattr ( value , "id" ) and hasattr ( value , "value" ) : 
~~~ value = value . value 
~~ return value . id 
~~ def is_bare_exception ( self , node ) : 
return isinstance ( node , Name ) and node . id in self . current_except_names 
~~ def is_str_exception ( self , node ) : 
isinstance ( node , Call ) 
and isinstance ( node . func , Name ) 
and node . func . id in ( 'str' , 'unicode' ) 
and node . args 
and self . is_bare_exception ( node . args [ 0 ] ) 
~~ def check_exc_info ( self , node ) : 
if self . current_logging_level not in ( 'error' , 'exception' ) : 
~~ for kw in node . keywords : 
~~~ if kw . arg == 'exc_info' : 
~~~ if self . current_logging_level == 'error' : 
~~~ violation = ERROR_EXC_INFO_VIOLATION 
~~~ violation = REDUNDANT_EXC_INFO_VIOLATION 
~~ self . violations . append ( ( node , violation ) ) 
~~ ~~ ~~ def parse_file ( self , file_path , currency ) -> List [ PriceModel ] : 
contents = self . load_file ( file_path ) 
prices = [ ] 
for line in contents : 
~~~ price = self . parse_line ( line ) 
assert isinstance ( price , PriceModel ) 
price . currency = currency 
prices . append ( price ) 
~~ return prices 
~~ def load_file ( self , file_path ) -> List [ str ] : 
content = [ ] 
content = read_lines_from_file ( file_path ) 
~~ def parse_line ( self , line : str ) -> PriceModel : 
line = line . rstrip ( ) 
parts = line . split ( ',' ) 
result = PriceModel ( ) 
result . symbol = self . translate_symbol ( parts [ 0 ] ) 
result . value = Decimal ( parts [ 1 ] ) 
date_str = parts [ 2 ] 
date_str = date_str . replace ( \ , '' ) 
date_parts = date_str . split ( '/' ) 
year_str = date_parts [ 2 ] 
month_str = date_parts [ 1 ] 
day_str = date_parts [ 0 ] 
result . datetime = datetime ( int ( year_str ) , int ( month_str ) , int ( day_str ) ) 
~~ def translate_symbol ( self , in_symbol : str ) -> str : 
if not self . symbol_maps : 
~~~ self . __load_symbol_maps ( ) 
~~ result = self . symbol_maps [ in_symbol ] if in_symbol in self . symbol_maps else in_symbol 
~~ def __load_symbol_maps ( self ) : 
repo = SymbolMapRepository ( self . __get_session ( ) ) 
all_maps = repo . get_all ( ) 
self . symbol_maps = { } 
for item in all_maps : 
~~~ self . symbol_maps [ item . in_symbol ] = item . out_symbol 
~~ ~~ def __get_session ( self ) : 
if not self . session : 
~~~ self . session = dal . get_default_session ( ) 
~~ return self . session 
~~ def add ( symbol : str , date , value , currency : str ) : 
symbol = symbol . upper ( ) 
currency = currency . upper ( ) 
app = PriceDbApplication ( ) 
price = PriceModel ( ) 
price . symbol . parse ( symbol ) 
price . datum . from_iso_date_string ( date ) 
price . value = Decimal ( value ) 
app . add_price ( price ) 
app . save ( ) 
~~ def import_csv ( filepath : str , currency : str ) : 
app . logger = logger 
app . import_prices ( filepath , currency ) 
~~ def last ( symbol : str ) : 
if symbol : 
~~~ symbol = symbol . upper ( ) 
sec_symbol = SecuritySymbol ( "" , "" ) 
sec_symbol . parse ( symbol ) 
latest = app . get_latest_price ( sec_symbol ) 
assert isinstance ( latest , PriceModel ) 
print ( f"{latest}" ) 
~~~ latest = app . get_latest_prices ( ) 
for price in latest : 
~~~ print ( f"{price}" ) 
~~ ~~ ~~ def list_prices ( date , currency , last ) : 
~~~ prices = app . get_latest_prices ( ) 
~~~ prices = app . get_prices ( date , currency ) 
~~ for price in prices : 
~~~ print ( price ) 
~~ def download ( ctx , help : bool , symbol : str , namespace : str , agent : str , currency : str ) : 
if help : 
~~~ click . echo ( ctx . get_help ( ) ) 
ctx . exit ( ) 
~~ app = PriceDbApplication ( ) 
if currency : 
~~~ currency = currency . strip ( ) 
~~ app . download_prices ( currency = currency , agent = agent , symbol = symbol , namespace = namespace ) 
~~ def prune ( symbol : str , all : str ) : 
if symbol is not None : 
~~~ sec_symbol = SecuritySymbol ( "" , "" ) 
deleted = app . prune ( sec_symbol ) 
if deleted : 
~~~ count = 1 
~~~ count = app . prune_all ( ) 
~~ def get_default_session ( ) : 
from . config import Config , ConfigKeys 
db_path = Config ( ) . get ( ConfigKeys . price_database ) 
if not db_path : 
~~ return get_session ( db_path ) 
~~ def add_map ( incoming , outgoing ) : 
db_path = Config ( ) . get ( ConfigKeys . pricedb_path ) 
session = get_session ( db_path ) 
new_map = SymbolMap ( ) 
new_map . in_symbol = incoming 
new_map . out_symbol = outgoing 
session . add ( new_map ) 
session . commit ( ) 
~~ def list_maps ( ) : 
maps = session . query ( SymbolMap ) . all ( ) 
for item in maps : 
~~~ click . echo ( item ) 
~~ ~~ def get_by_id ( self , symbol : str ) -> SymbolMap : 
return self . query . filter ( SymbolMap . in_symbol == symbol ) . first ( ) 
~~ def read_lines_from_file ( file_path : str ) -> List [ str ] : 
with open ( file_path ) as csv_file : 
~~~ content = csv_file . readlines ( ) 
~~ return content 
~~ def map_entity ( self , entity : dal . Price ) -> PriceModel : 
if not entity : 
~~ result = PriceModel ( ) 
result . currency = entity . currency 
dt_string = entity . date 
format_string = "%Y-%m-%d" 
if entity . time : 
~~~ dt_string += f"T{entity.time}" 
format_string += "T%H:%M:%S" 
~~ price_datetime = datetime . strptime ( dt_string , format_string ) 
result . datum = Datum ( ) 
result . datum . from_datetime ( price_datetime ) 
assert isinstance ( result . datum , Datum ) 
result . symbol = SecuritySymbol ( entity . namespace , entity . symbol ) 
value = Decimal ( entity . value ) / Decimal ( entity . denom ) 
result . value = Decimal ( value ) 
~~ def map_model ( self , model : PriceModel ) -> Price : 
assert isinstance ( model . symbol , SecuritySymbol ) 
assert isinstance ( model . datum , Datum ) 
entity = Price ( ) 
date_iso = f"{model.datum.value.year}-{model.datum.value.month:02d}-{model.datum.value.day:02d}" 
entity . date = date_iso 
entity . time = f"{model.datum.value.hour:02d}:{model.datum.value.minute:02d}:{model.datum.value.second:02d}" 
if model . symbol . namespace : 
~~~ entity . namespace = model . symbol . namespace . upper ( ) 
~~ entity . symbol = model . symbol . mnemonic . upper ( ) 
assert isinstance ( model . value , Decimal ) 
dec_places = abs ( model . value . as_tuple ( ) . exponent ) 
entity . denom = 10 ** dec_places 
entity . value = int ( model . value * entity . denom ) 
entity . currency = model . currency . upper ( ) 
return entity 
~~ def __read_config ( self , file_path : str ) : 
if not os . path . exists ( file_path ) : 
~~ if not os . path . isfile ( file_path ) : 
~~ self . config . read ( file_path ) 
~~ def __get_config_template_path ( self ) -> str : 
filename = resource_filename ( 
Requirement . parse ( package_name ) , 
template_path + config_filename ) 
return filename 
~~ def __create_user_config ( self ) : 
src_path = self . __get_config_template_path ( ) 
src = os . path . abspath ( src_path ) 
if not os . path . exists ( src ) : 
self . logger . error ( message ) 
raise FileNotFoundError ( message ) 
~~ dst = os . path . abspath ( self . get_config_path ( ) ) 
shutil . copyfile ( src , dst ) 
if not os . path . exists ( dst ) : 
~~ ~~ def get_config_path ( self ) -> str : 
dst_dir = self . __get_user_path ( ) 
dst = dst_dir + "/" + config_filename 
return dst 
~~ def get_contents ( self ) -> str : 
content = None 
in_memory = io . StringIO ( "" ) 
self . config . write ( in_memory ) 
in_memory . seek ( 0 ) 
content = in_memory . read ( ) 
in_memory . close ( ) 
~~ def set ( self , option : ConfigKeys , value ) : 
assert isinstance ( option , ConfigKeys ) 
section = SECTION 
self . config . set ( section , option . name , value ) 
~~ def get ( self , option : ConfigKeys ) : 
return self . config . get ( section , option . name ) 
~~ def save ( self ) : 
file_path = self . get_config_path ( ) 
contents = self . get_contents ( ) 
with open ( file_path , mode = 'w' ) as cfg_file : 
~~~ cfg_file . write ( contents ) 
~~ ~~ def parse ( self , symbol : str ) -> ( str , str ) : 
symbol_parts = symbol . split ( ":" ) 
namespace = None 
mnemonic = symbol 
if len ( symbol_parts ) > 1 : 
~~~ namespace = symbol_parts [ 0 ] 
mnemonic = symbol_parts [ 1 ] 
~~ self . namespace = namespace 
self . mnemonic = mnemonic 
return namespace , mnemonic 
~~ def add_price ( self , price : PriceModel ) : 
if not price : 
~~ mapper = mappers . PriceMapper ( ) 
entity = mapper . map_model ( price ) 
self . add_price_entity ( entity ) 
~~ def add_price_entity ( self , price : dal . Price ) : 
from decimal import Decimal 
repo = self . get_price_repository ( ) 
existing = ( 
repo . query 
. filter ( dal . Price . namespace == price . namespace ) 
. filter ( dal . Price . symbol == price . symbol ) 
. filter ( dal . Price . date == price . date ) 
. filter ( dal . Price . time == price . time ) 
. first ( ) 
if existing : 
~~~ new_value = Decimal ( price . value ) / Decimal ( price . denom ) 
if price . currency != existing . currency : 
~~ if existing . value != price . value : 
~~~ existing . value = price . value 
~~ if existing . denom != price . denom : 
~~~ existing . denom = price . denom 
~~~ self . session . add ( price ) 
~~ ~~ def download_price ( self , symbol : str , currency : str , agent : str ) -> PriceModel : 
price = self . __download_price ( symbol , currency , agent ) 
return price 
~~ def download_prices ( self , ** kwargs ) : 
currency : str = kwargs . get ( 'currency' , None ) 
~~~ currency = currency . upper ( ) 
~~ agent : str = kwargs . get ( 'agent' , None ) 
if agent : 
~~~ agent = agent . upper ( ) 
~~ symbol : str = kwargs . get ( 'symbol' , None ) 
~~ namespace : str = kwargs . get ( 'namespace' , None ) 
if namespace : 
~~~ namespace = namespace . upper ( ) 
~~ securities = self . __get_securities ( currency , agent , symbol , namespace ) 
#self.logger.debug(securities) 
for sec in securities : 
~~~ symbol = f"{sec.namespace}:{sec.symbol}" 
currency = sec . currency 
agent = sec . updater 
~~~ self . __download_price ( symbol . strip ( ) , currency , agent ) 
~~~ self . logger . error ( str ( e ) ) 
~~ ~~ self . save ( ) 
~~ def import_prices ( self , file_path : str , currency_symbol : str ) : 
from . csv import CsvParser 
assert isinstance ( file_path , str ) 
assert isinstance ( currency_symbol , str ) 
parser = CsvParser ( ) 
prices = parser . parse_file ( file_path , currency_symbol ) 
counter = 0 
session = self . session 
mapper = mappers . PriceMapper ( ) 
for price in prices : 
~~~ new_price = mapper . map_model ( price ) 
self . add_price_entity ( new_price ) 
counter += 1 
~~ session . commit ( ) 
~~ def session ( self ) : 
if not self . __session : 
~~~ self . __session = dal . get_default_session ( ) 
~~ return self . __session 
~~ def get_prices ( self , date : str , currency : str ) -> List [ PriceModel ] : 
from . repositories import PriceRepository 
repo = PriceRepository ( session ) 
query = repo . query 
~~~ query = query . filter ( dal . Price . date == date ) 
~~ if currency : 
~~~ query = query . filter ( dal . Price . currency == currency ) 
~~ query = query . order_by ( dal . Price . namespace , dal . Price . symbol ) 
price_entities = query . all ( ) 
for entity in price_entities : 
~~~ model = mapper . map_entity ( entity ) 
result . append ( model ) 
~~ def get_prices_on ( self , on_date : str , namespace : str , symbol : str ) : 
query = ( 
repo . query . filter ( dal . Price . namespace == namespace ) 
. filter ( dal . Price . symbol == symbol ) 
. filter ( dal . Price . date == on_date ) 
. order_by ( dal . Price . time . desc ( ) ) 
result = query . first ( ) 
~~ def get_price_repository ( self ) : 
if not self . price_repo : 
~~~ self . price_repo = PriceRepository ( self . session ) 
~~ return self . price_repo 
~~ def get_security_repository ( self ) : 
from . repositories import SecurityRepository 
if not self . security_repo : 
~~~ self . security_repo = SecurityRepository ( self . session ) 
~~ return self . security_repo 
~~ def prune_all ( self ) -> int : 
repo = PriceRepository ( ) 
items = repo . query . distinct ( dal . Price . namespace , dal . Price . symbol ) . all ( ) 
~~~ symbol = SecuritySymbol ( item . namespace , item . symbol ) 
deleted = self . prune ( symbol ) 
~~~ count += 1 
~~ ~~ return count 
~~ def prune ( self , symbol : SecuritySymbol ) : 
assert isinstance ( symbol , SecuritySymbol ) 
repo . query . filter ( dal . Price . namespace == symbol . namespace ) 
. filter ( dal . Price . symbol == symbol . mnemonic ) 
. order_by ( dal . Price . date . desc ( ) ) 
all_prices = query . all ( ) 
deleted = False 
for single in all_prices : 
~~~ if not first : 
~~~ repo . query . filter ( dal . Price . id == single . id ) . delete ( ) 
deleted = True 
~~~ first = False 
~~ ~~ repo . save ( ) 
return deleted 
if self . __session : 
~~~ self . session . commit ( ) 
~~ ~~ def __download_price ( self , symbol : str , currency : str , agent : str ) : 
from finance_quote_python import Quote 
assert isinstance ( symbol , str ) 
assert isinstance ( currency , str ) 
assert isinstance ( agent , str ) 
if not symbol : 
~~ dl = Quote ( ) 
dl . logger = self . logger 
dl . set_source ( agent ) 
dl . set_currency ( currency ) 
result = dl . fetch ( agent , [ symbol ] ) 
~~ price = result [ 0 ] 
~~~ self . add_price ( price ) 
~~ return price 
~~ def __get_securities ( self , currency : str , agent : str , symbol : str , 
namespace : str ) -> List [ dal . Security ] : 
repo = self . get_security_repository ( ) 
if currency is not None : 
~~~ query = query . filter ( dal . Security . currency == currency ) 
~~ if agent is not None : 
~~~ query = query . filter ( dal . Security . updater == agent ) 
~~ if symbol is not None : 
~~~ query = query . filter ( dal . Security . symbol == symbol ) 
~~ if namespace is not None : 
~~~ query = query . filter ( dal . Security . namespace == namespace ) 
~~ query = query . order_by ( dal . Security . namespace , dal . Security . symbol ) 
securities = query . all ( ) 
return securities 
~~ def partial ( self ) : 
ba = self . data [ "bound_args" ] 
return state_partial ( self . data [ "func" ] , * ba . args [ 1 : ] , ** ba . kwargs ) 
~~ def update_child_calls ( self ) : 
for node in filter ( lambda n : len ( n . arg_name ) , self . child_list ) : 
~~~ self . data [ "bound_args" ] . arguments [ node . arg_name ] = node . partial ( ) 
~~ self . updated = True 
~~ def descend ( self , include_me = True ) : 
if include_me : 
~~~ yield self 
~~ for child in self . child_list : 
~~~ yield child 
yield from child . descend ( ) 
~~ ~~ def multi_dec ( f ) : 
~~~ args = ( 
args [ 0 ] if len ( args ) == 1 and isinstance ( args [ 0 ] , ( list , tuple ) ) else args 
for arg in args : 
~~~ if isinstance ( arg , Node ) and arg . parent . name is "root" : 
~~~ arg . parent . remove_child ( arg ) 
arg . update_child_calls ( ) 
~~ ~~ return f ( * args , ** kwargs ) 
~~ def has_equal_part_len ( state , name , unequal_msg ) : 
stu_len = len ( state . student_parts [ name ] ) , sol_len = len ( state . solution_parts [ name ] ) 
if d [ "stu_len" ] != d [ "sol_len" ] : 
~~~ _msg = state . build_message ( unequal_msg , d ) 
state . report ( Feedback ( _msg , state ) ) 
~~ def has_equal_ast ( state , incorrect_msg = None , code = None , exact = True , append = None ) : 
if utils . v2_only ( ) : 
~~~ state . assert_is_not ( [ "object_assignments" ] , "has_equal_ast" , [ "check_object" ] ) 
state . assert_is_not ( [ "function_calls" ] , "has_equal_ast" , [ "check_function" ] ) 
~~ if code and incorrect_msg is None : 
~~~ raise InstructorError ( 
append is None 
~~~ append = incorrect_msg is None 
~~ if incorrect_msg is None : 
~~ def parse_tree ( tree ) : 
~~~ crnt = ( 
tree . body [ 0 ] 
if isinstance ( tree , ast . Module ) and len ( tree . body ) == 1 
else tree 
return ast . dump ( crnt . value if isinstance ( crnt , ast . Expr ) else crnt ) 
~~ stu_rep = parse_tree ( state . student_ast ) 
sol_rep = parse_tree ( state . solution_ast if not code else ast . parse ( code ) ) 
fmt_kwargs = { 
"sol_str" : state . solution_code if not code else code , 
"stu_str" : state . student_code , 
_msg = state . build_message ( incorrect_msg , fmt_kwargs , append = append ) 
if exact and not code : 
~~~ state . do_test ( EqualTest ( stu_rep , sol_rep , Feedback ( _msg , state ) ) ) 
~~ elif not sol_rep in stu_rep : 
~~~ state . report ( Feedback ( _msg , state ) ) 
~~ def has_code ( state , text , pattern = True , not_typed_msg = None ) : 
if not not_typed_msg : 
~~~ if pattern : 
~~ ~~ student_code = state . student_code 
_msg = state . build_message ( not_typed_msg ) 
state . do_test ( 
StringContainsTest ( student_code , text , pattern , Feedback ( _msg , state ) ) 
~~ def has_import ( 
state , 
name , 
same_as = False , 
student_imports = state . ast_dispatcher ( "imports" , state . student_ast ) 
solution_imports = state . ast_dispatcher ( "imports" , state . solution_ast ) 
if name not in solution_imports : 
% name 
~~ fmt_kwargs = { "pkg" : name , "alias" : solution_imports [ name ] } 
_msg = state . build_message ( not_imported_msg , fmt_kwargs ) 
state . do_test ( DefinedCollTest ( name , student_imports , _msg ) ) 
if same_as : 
~~~ _msg = state . build_message ( incorrect_as_msg , fmt_kwargs ) 
state . do_test ( EqualTest ( solution_imports [ name ] , student_imports [ name ] , _msg ) ) 
~~ def has_output ( state , text , pattern = True , no_output_msg = None ) : 
if not no_output_msg : 
~~ _msg = state . build_message ( no_output_msg ) 
state . do_test ( StringContainsTest ( state . raw_student_output , text , pattern , _msg ) ) 
~~ def has_printout ( 
state , index , not_printed_msg = None , pre_code = None , name = None , copy = False 
state . assert_root ( "has_printout" , extra_msg = extra_msg ) 
if not_printed_msg is None : 
~~~ not_printed_msg = ( 
~~~ sol_call_ast = state . ast_dispatcher ( "function_calls" , state . solution_ast ) [ 
"print" 
] [ index ] [ "node" ] 
~~ except ( KeyError , IndexError ) : 
index , utils . get_ord ( index + 1 ) 
~~ out_sol , str_sol = getOutputInProcess ( 
tree = sol_call_ast , 
process = state . solution_process , 
context = state . solution_context , 
env = state . solution_env , 
pre_code = pre_code , 
copy = copy , 
sol_call_str = state . solution_ast_tokens . get_text ( sol_call_ast ) 
if isinstance ( str_sol , Exception ) : 
~~ _msg = state . build_message ( not_printed_msg , { "sol_call" : sol_call_str } ) 
has_output ( state , out_sol . strip ( ) , pattern = False , no_output_msg = _msg ) 
~~ def has_no_error ( 
state . assert_root ( "has_no_error" ) 
if state . reporter . errors : 
~~~ _msg = state . build_message ( 
incorrect_msg , { "error" : str ( state . reporter . errors [ 0 ] ) } 
~~ def has_chosen ( state , correct , msgs ) : 
if not issubclass ( type ( correct ) , int ) : 
~~ student_process = state . student_process 
if not isDefinedInProcess ( MC_VAR_NAME , student_process ) : 
~~~ selected_option = getOptionFromProcess ( student_process , MC_VAR_NAME ) 
if not issubclass ( type ( selected_option ) , int ) : 
~~ if selected_option < 1 or correct < 1 : 
~~ if selected_option > len ( msgs ) or correct > len ( msgs ) : 
~~ feedback_msg = msgs [ selected_option - 1 ] 
state . reporter . success_msg = msgs [ correct - 1 ] 
state . do_test ( EqualTest ( selected_option , correct , feedback_msg ) ) 
~~ ~~ def check_function ( 
index = 0 , 
missing_msg = None , 
params_not_matched_msg = None , 
expand_msg = None , 
signature = True , 
append_missing = missing_msg is None 
append_params_not_matched = params_not_matched_msg is None 
if missing_msg is None : 
~~~ missing_msg = MISSING_MSG 
~~ if expand_msg is None : 
~~~ expand_msg = PREPEND_MSG 
~~ if params_not_matched_msg is None : 
~~~ params_not_matched_msg = SIG_ISSUE_MSG 
~~ stu_out = state . ast_dispatcher ( "function_calls" , state . student_ast ) 
sol_out = state . ast_dispatcher ( "function_calls" , state . solution_ast ) 
student_mappings = state . ast_dispatcher ( "mappings" , state . student_ast ) 
"times" : get_times ( index + 1 ) , 
"ord" : get_ord ( index + 1 ) , 
"index" : index , 
"mapped_name" : get_mapped_name ( name , student_mappings ) , 
~~~ sol_parts = { ** sol_out [ name ] [ index ] } 
% ( index + 1 , name ) 
~~~ stu_parts = { ** stu_out [ name ] [ index ] } 
~~~ _msg = state . build_message ( missing_msg , fmt_kwargs , append = append_missing ) 
~~ if signature : 
~~~ signature = None if isinstance ( signature , bool ) else signature 
get_sig = partial ( 
getSignatureInProcess , 
signature = signature , 
manual_sigs = state . get_manual_sigs ( ) , 
~~~ sol_sig = get_sig ( 
mapped_name = sol_parts [ "name" ] , process = state . solution_process 
sol_parts [ "args" ] = bind_args ( sol_sig , sol_parts [ "args" ] ) 
% ( get_ord ( index + 1 ) , name , e ) 
~~~ stu_sig = get_sig ( 
mapped_name = stu_parts [ "name" ] , process = state . student_process 
stu_parts [ "args" ] = bind_args ( stu_sig , stu_parts [ "args" ] ) 
params_not_matched_msg , fmt_kwargs , append = append_params_not_matched 
state . report ( 
Feedback ( 
_msg , StubState ( stu_parts [ "node" ] , state . highlighting_disabled ) 
~~ ~~ append_message = { "msg" : expand_msg , "kwargs" : fmt_kwargs } 
child = part_to_child ( 
stu_parts , sol_parts , append_message , state , node_name = "function_calls" 
return child 
~~ def process_task ( f ) : 
sig = inspect . signature ( f ) 
~~~ ba = sig . bind_partial ( * args , ** kwargs ) 
process = ba . arguments . get ( "process" ) 
if process : 
~~~ ba . arguments [ "process" ] = None 
pf = partial ( wrapper , * ba . args , ** ba . kwargs ) 
return process . executeTask ( pf ) 
~~ return f ( * ba . args , ** ba . kwargs ) 
~~ def getResultFromProcess ( res , tempname , process ) : 
if not isinstance ( res , ( UndefinedValue , Exception ) ) : 
~~~ value = getRepresentation ( tempname , process ) 
return value , res 
~~~ return res , str ( res ) 
~~ ~~ def assign_from_ast ( node , expr ) : 
if isinstance ( expr , str ) : 
~~~ expr = ast . Name ( id = expr , ctx = ast . Load ( ) ) 
~~ mod = ast . Module ( [ ast . Assign ( targets = [ node ] , value = expr ) ] ) 
ast . fix_missing_locations ( mod ) 
return compile ( mod , "<assignment_script>" , "exec" ) 
~~ def override ( state , solution ) : 
old_ast = state . solution_ast 
new_ast = ast . parse ( solution ) 
if not isinstance ( old_ast , ast . Module ) and len ( new_ast . body ) == 1 : 
~~~ expr = new_ast . body [ 0 ] 
candidates = [ expr , expr . value ] if isinstance ( expr , ast . Expr ) else [ expr ] 
for node in candidates : 
~~~ if isinstance ( node , old_ast . __class__ ) : 
~~~ new_ast = node 
~~ ~~ ~~ kwargs = state . messages [ - 1 ] if state . messages else { } 
child = state . to_child ( 
solution_ast = new_ast , 
student_ast = state . student_ast , 
highlight = state . highlight , 
append_message = { "msg" : "" , "kwargs" : kwargs } , 
~~ def set_context ( state , * args , ** kwargs ) : 
stu_crnt = state . student_context . context 
sol_crnt = state . solution_context . context 
if len ( args ) > 0 and len ( kwargs ) > 0 : 
~~ if args : 
~~~ if len ( args ) > len ( sol_crnt ) : 
len ( sol_crnt ) , len ( args ) 
~~ upd_sol = sol_crnt . update ( dict ( zip ( sol_crnt . keys ( ) , args ) ) ) 
upd_stu = stu_crnt . update ( dict ( zip ( stu_crnt . keys ( ) , args ) ) ) 
~~~ upd_sol = sol_crnt 
upd_stu = stu_crnt 
~~ if kwargs : 
~~~ if set ( kwargs ) - set ( upd_sol ) : 
upd_sol or "missing" , sorted ( list ( kwargs . keys ( ) ) ) 
~~ out_sol = upd_sol . update ( kwargs ) 
match_keys = dict ( zip ( sol_crnt . keys ( ) , stu_crnt . keys ( ) ) ) 
out_stu = upd_stu . update ( 
{ match_keys [ k ] : v for k , v in kwargs . items ( ) if k in match_keys } 
~~~ out_sol = upd_sol 
out_stu = upd_stu 
~~ return state . to_child ( 
student_context = out_stu , solution_context = out_sol , highlight = state . highlight 
~~ def set_env ( state , ** kwargs ) : 
stu_crnt = state . student_env . context 
sol_crnt = state . solution_env . context 
stu_new = stu_crnt . update ( kwargs ) 
sol_new = sol_crnt . update ( kwargs ) 
return state . to_child ( 
student_env = stu_new , solution_env = sol_new , highlight = state . highlight 
~~ def check_object ( 
state , index , missing_msg = None , expand_msg = None , typestr = "variable" 
if v2_only ( ) : 
state . assert_root ( "check_object" , extra_msg = extra_msg ) 
~~ if missing_msg is None : 
not isDefinedInProcess ( index , state . solution_process ) 
and state . has_different_processes ( ) 
% index 
~~ append_message = { "msg" : expand_msg , "kwargs" : { "index" : index , "typestr" : typestr } } 
fallback = lambda : ObjectAssignmentParser . get_part ( index ) 
stu_part = state . ast_dispatcher ( "object_assignments" , state . student_ast ) . get ( index , fallback ( ) ) 
sol_part = state . ast_dispatcher ( "object_assignments" , state . solution_ast ) . get ( index , fallback ( ) ) 
_msg = state . build_message ( missing_msg , append_message [ "kwargs" ] ) 
state . do_test ( DefinedProcessTest ( index , state . student_process , Feedback ( _msg ) ) ) 
stu_part , sol_part , append_message , state , node_name = "object_assignments" 
~~ def is_instance ( state , inst , not_instance_msg = None ) : 
state . assert_is ( [ "object_assignments" ] , "is_instance" , [ "check_object" ] ) 
sol_name = state . solution_parts . get ( "name" ) 
stu_name = state . student_parts . get ( "name" ) 
if not_instance_msg is None : 
~~ if not isInstanceInProcess ( sol_name , inst , state . solution_process ) : 
% ( sol_name , inst . __name__ ) 
~~ _msg = state . build_message ( not_instance_msg , { "inst" : inst } ) 
feedback = Feedback ( _msg , state ) 
state . do_test ( InstanceProcessTest ( stu_name , inst , state . student_process , feedback ) ) 
~~ def check_df ( 
state , index , missing_msg = None , not_instance_msg = None , expand_msg = None 
child = check_object ( 
index , 
missing_msg = missing_msg , 
expand_msg = expand_msg , 
is_instance ( child , pd . DataFrame , not_instance_msg = not_instance_msg ) 
~~ def check_keys ( state , key , missing_msg = None , expand_msg = None ) : 
state . assert_is ( [ "object_assignments" ] , "is_instance" , [ "check_object" , "check_df" ] ) 
~~ sol_name = state . solution_parts . get ( "name" ) 
if not isDefinedCollInProcess ( sol_name , key , state . solution_process ) : 
% ( key , sol_name ) 
~~ _msg = state . build_message ( missing_msg , { "key" : key } ) 
DefinedCollProcessTest ( 
stu_name , key , state . student_process , Feedback ( _msg , state ) 
def get_part ( name , key , highlight ) : 
~~~ if isinstance ( key , str ) : 
~~~ slice_val = ast . Str ( s = key ) 
~~~ slice_val = ast . parse ( str ( key ) ) . body [ 0 ] . value 
~~ expr = ast . Subscript ( 
value = ast . Name ( id = name , ctx = ast . Load ( ) ) , 
slice = ast . Index ( value = slice_val ) , 
ctx = ast . Load ( ) , 
ast . fix_missing_locations ( expr ) 
return { "node" : expr , "highlight" : highlight } 
~~ stu_part = get_part ( stu_name , key , state . student_parts . get ( "highlight" ) ) 
sol_part = get_part ( sol_name , key , state . solution_parts . get ( "highlight" ) ) 
append_message = { "msg" : expand_msg , "kwargs" : { "key" : key } } 
child = part_to_child ( stu_part , sol_part , append_message , state ) 
~~ def defined_items ( self ) : 
[ ( k , v ) for k , v in self . items ( ) if v is not self . EMPTY ] , is_empty = False 
~~ def to_child ( self , append_message = "" , node_name = "" , ** kwargs ) : 
base_kwargs = { 
attr : getattr ( self , attr ) 
for attr in self . params 
if attr not in [ "highlight" ] 
if not isinstance ( append_message , dict ) : 
~~~ append_message = { "msg" : append_message , "kwargs" : { } } 
~~ kwargs [ "messages" ] = [ * self . messages , append_message ] 
kwargs [ "parent_state" ] = self 
for kwarg in [ "solution_context" , "student_context" ] : 
~~~ if kwarg in kwargs and not kwargs [ kwarg ] : 
~~~ kwargs . pop ( kwarg , None ) 
~~ ~~ def update_kwarg ( name , func ) : 
~~~ kwargs [ name ] = func ( kwargs [ name ] ) 
~~ def update_context ( name ) : 
~~~ update_kwarg ( name , getattr ( self , name ) . update_ctx ) 
~~ if isinstance ( kwargs . get ( "student_ast" , None ) , list ) : 
~~~ update_kwarg ( "student_ast" , wrap_in_module ) 
~~ if isinstance ( kwargs . get ( "solution_ast" , None ) , list ) : 
~~~ update_kwarg ( "solution_ast" , wrap_in_module ) 
~~ if "student_ast" in kwargs : 
~~~ kwargs [ "student_code" ] = self . student_ast_tokens . get_text ( 
kwargs [ "student_ast" ] 
~~ if "solution_ast" in kwargs : 
~~~ kwargs [ "solution_code" ] = self . solution_ast_tokens . get_text ( 
kwargs [ "solution_ast" ] 
~~ if "solution_context" in kwargs : 
~~~ update_context ( "solution_context" ) 
~~ if "student_context" in kwargs : 
~~~ update_context ( "student_context" ) 
~~ if "solution_env" in kwargs : 
~~~ update_context ( "solution_env" ) 
~~ if "student_env" in kwargs : 
~~~ update_context ( "student_env" ) 
~~ klass = self . SUBCLASSES [ node_name ] if node_name else State 
child = klass ( ** { ** base_kwargs , ** kwargs } ) 
~~ def _getx ( self , Parser , ext_attr , tree ) : 
cache_key = Parser . __name__ + str ( hash ( tree ) ) 
if self . _parser_cache . get ( cache_key ) : 
~~~ p = self . _parser_cache [ cache_key ] 
~~~ p = Parser ( ) 
if ext_attr != "mappings" and Parser in [ 
FunctionParser , 
ObjectAccessParser , 
~~~ p . mappings = self . context_mappings . copy ( ) 
~~ p . visit ( tree ) 
self . _parser_cache [ cache_key ] = p 
~~ return getattr ( p , ext_attr ) 
~~ def has_context_loop ( state , incorrect_msg , exact_names ) : 
return _test ( 
incorrect_msg or MSG_INCORRECT_LOOP , 
exact_names , 
tv_name = "_target_vars" , 
highlight_name = "target" , 
~~ def has_context_with ( state , incorrect_msg , exact_names ) : 
for i in range ( len ( state . solution_parts [ "context" ] ) ) : 
_has_context ( ctxt_state , incorrect_msg or MSG_INCORRECT_WITH , exact_names ) 
~~ def check_part ( state , name , part_msg , missing_msg = None , expand_msg = None ) : 
~~ if not part_msg : 
~~~ part_msg = name 
~~ append_message = { "msg" : expand_msg , "kwargs" : { "part" : part_msg } } 
has_part ( state , name , missing_msg , append_message [ "kwargs" ] ) 
stu_part = state . student_parts [ name ] 
sol_part = state . solution_parts [ name ] 
assert_ast ( state , sol_part , append_message [ "kwargs" ] ) 
return part_to_child ( stu_part , sol_part , append_message , state ) 
~~ def check_part_index ( state , name , index , part_msg , missing_msg = None , expand_msg = None ) : 
~~ ordinal = get_ord ( index + 1 ) if isinstance ( index , int ) else "" 
fmt_kwargs = { "index" : index , "ordinal" : ordinal } 
fmt_kwargs . update ( part = render ( part_msg , fmt_kwargs ) ) 
append_message = { "msg" : expand_msg , "kwargs" : fmt_kwargs } 
has_part ( state , name , missing_msg , fmt_kwargs , index ) 
if isinstance ( index , list ) : 
~~~ for ind in index : 
~~~ stu_part = stu_part [ ind ] 
sol_part = sol_part [ ind ] 
~~~ stu_part = stu_part [ index ] 
sol_part = sol_part [ index ] 
~~ assert_ast ( state , sol_part , fmt_kwargs ) 
~~ def check_args ( state , name , missing_msg = None ) : 
~~~ return check_part ( state , name , name , missing_msg = missing_msg ) 
~~~ if name [ 0 ] == "args" : 
get_ord ( name [ 1 ] + 1 ) 
~~~ arg_str = ( 
if isinstance ( name , int ) 
~~ return check_part_index ( state , "args" , name , arg_str , missing_msg = missing_msg ) 
~~ ~~ def check_call ( state , callstr , argstr = None , expand_msg = None ) : 
state . assert_is ( 
[ "function_defs" , "lambda_functions" ] , 
"check_call" , 
[ "check_function_def" , "check_lambda_function" ] , 
if expand_msg is None : 
~~ stu_part , _argstr = build_call ( callstr , state . student_parts [ "node" ] ) 
sol_part , _ = build_call ( callstr , state . solution_parts [ "node" ] ) 
append_message = { "msg" : expand_msg , "kwargs" : { "argstr" : argstr or _argstr } } 
~~ def search ( self , query = None , args = None ) : 
if query is not None : 
~~~ return self . _container_search ( query ) 
~~ return self . _search_all ( ) 
~~ def search_all ( self , quiet = False ) : 
for obj in self . bucket . objects . all ( ) : 
~~~ subsrc = obj . Object ( ) 
metadata = dict ( ( k . lower ( ) , v ) for k , v in subsrc . metadata . items ( ) ) 
size = '' 
datestr = "%s-%s-%s" % ( obj . last_modified . month , 
obj . last_modified . day , 
obj . last_modified . year ) 
if 'sizemb' in metadata : 
~~~ size = '%sMB' % metadata [ 'sizemb' ] 
~~ results . append ( [ obj . key , datestr , size ] ) 
~~ if len ( results ) == 0 : 
~~ if not quiet : 
~~~ bot . info ( "Containers" ) 
bot . table ( results ) 
~~ def container_search ( self , query , across_collections = False ) : 
results = self . _search_all ( quiet = True ) 
~~~ if query in result [ 0 ] : 
~~~ matches . append ( result ) 
~~ ~~ if len ( matches ) > 0 : 
bot . table ( matches ) 
~~~ return self . _collection_search ( query ) 
~~ elif query . startswith ( '/' ) : 
~~~ return self . _container_search ( query , across_collections = True ) 
~~ elif "/" in query or ":" in query : 
~~ return self . _collection_search ( query = query ) 
~~ def collection_search ( self , query ) : 
query = query . lower ( ) . strip ( '/' ) 
url = '%s/collection/%s' % ( self . base , query ) 
result = self . _get ( url ) 
if len ( result ) == 0 : 
~~ bot . custom ( prefix = "COLLECTION" , message = query ) 
rows = [ ] 
for container in result [ 'containers' ] : 
~~~ rows . append ( [ container [ 'uri' ] , 
container [ 'detail' ] ] ) 
~~ bot . table ( rows ) 
return rows 
~~ def label_search ( self , key = None , value = None ) : 
~~~ key = key . lower ( ) 
~~~ value = value . lower ( ) 
~~ show_details = True 
if key is None and value is None : 
~~~ url = '%s/labels/search' % ( self . base ) 
show_details = False 
~~ elif key is not None and value is not None : 
~~~ url = '%s/labels/search/%s/key/%s/value' % ( self . base , key , value ) 
~~ elif key is None : 
~~~ url = '%s/labels/search/%s/value' % ( self . base , value ) 
~~~ url = '%s/labels/search/%s/key' % ( self . base , key ) 
~~ result = self . _get ( url ) 
~~ bot . info ( "Labels\\n" ) 
for l in result : 
~~~ if show_details is True : 
~~~ entry = [ "%s:%s" % ( l [ 'key' ] , l [ 'value' ] ) , 
"\\n%s\\n\\n" % "\\n" . join ( l [ 'containers' ] ) ] 
~~~ entry = [ "N=%s" % len ( l [ 'containers' ] ) , 
"%s:%s" % ( l [ 'key' ] , l [ 'value' ] ) ] 
~~ rows . append ( entry ) 
q = parse_image_name ( remove_uri ( query ) , defaults = False ) 
if q [ 'tag' ] is not None : 
~~~ if across_collections is True : 
~~~ url = '%s/container/search/name/%s/tag/%s' % ( self . base , q [ 'image' ] , q [ 'tag' ] ) 
~~~ url = '%s/container/search/collection/%s/name/%s/tag/%s' % ( self . base , q [ 'collection' ] , q [ 'image' ] , q [ 'tag' ] ) 
~~ ~~ elif q [ 'tag' ] is None : 
~~~ url = '%s/container/search/name/%s' % ( self . base , q [ 'image' ] ) 
~~~ url = '%s/container/search/collection/%s/name/%s' % ( self . base , q [ 'collection' ] , q [ 'image' ] ) 
~~ ~~ result = self . _get ( url ) 
if "containers" in result : 
~~~ result = result [ 'containers' ] 
~~ if len ( result ) == 0 : 
for c in result : 
~~~ rows . append ( [ '%s/%s' % ( c [ 'collection' ] , c [ 'name' ] ) , 
c [ 'tag' ] ] ) 
if query is None : 
~~ return self . _search_all ( query ) 
~~ def search_all ( self , collection , job_id = None ) : 
results = [ [ 'job_id' , 'browser' ] ] 
url = "%s/projects/%s/jobs" % ( self . api_base , 
quote_plus ( collection . strip ( '/' ) ) ) 
response = requests . get ( url , headers = self . headers ) 
~~~ jobs = response . json ( ) 
~~~ if job [ 'status' ] == 'success' : 
~~~ name = job [ 'name' ] 
for artifact in job [ 'artifacts' ] : 
~~~ if artifact [ 'filename' ] . endswith ( 'zip' ) : 
~~~ artifact_url = ( "%s/%s/-/jobs/%s/artifacts/browse/%s" 
% ( self . base , 
collection , 
job [ 'id' ] , 
name ) ) 
results . append ( [ str ( job [ 'id' ] ) , artifact_url ] ) 
~~ ~~ ~~ ~~ ~~ if len ( results ) == 1 : 
~~ def _client_tagged ( self , tags ) : 
name = self . client_name . lower ( ) 
tags = [ t . lower ( ) for t in tags ] 
if name not in tags : 
~~ ~~ def speak ( self ) : 
if self . quiet is False : 
self . database ) ) 
self . _speak ( ) 
~~ ~~ def announce ( self , command = None ) : 
if command is not None : 
~~~ if command not in [ 'get' ] and self . quiet is False : 
~~~ self . speak ( ) 
~~ ~~ ~~ def _update_secrets ( self ) : 
env = 'SREGISTRY_GOOGLE_DRIVE_CREDENTIALS' 
self . _secrets = self . _get_and_update_setting ( env ) 
self . _base = self . _get_and_update_setting ( 'SREGISTRY_GOOGLE_DRIVE_ROOT' ) 
if self . _base is None : 
~~~ self . _base = 'sregistry' 
~~ if self . _secrets is None : 
bot . info ( "https://singularityhub.github.io/sregistry-cli/client-google-drive" ) 
~~ ~~ def _get_service ( self , version = 'v3' ) : 
invalid = True 
if self . _credential_cache is not None : 
~~~ storage = Storage ( self . _credential_cache ) 
if os . path . exists ( self . _credential_cache ) : 
~~~ credentials = storage . get ( ) 
if not credentials . invalid : 
~~~ invalid = False 
~~ ~~ ~~ if invalid is True : 
~~~ class flags : 
~~~ auth_host_name = 'localhost' 
auth_host_port = [ 8080 ] 
noauth_local_webserver = False 
logging_level = 'INFO' 
~~ flow = oclient . flow_from_clientsecrets ( self . _secrets , self . _scope ) 
credentials = tools . run_flow ( flow , storage , flags ) 
~~~ storage . put ( credentials ) 
~~ ~~ http = credentials . authorize ( httplib2 . Http ( ) ) 
return build ( 'drive' , version , http = http ) 
~~ def add ( self , image_path = None , 
image_uri = None , 
image_name = None , 
url = None , 
metadata = None , 
save = True , 
copy = False ) : 
if image_path is not None : 
~~~ if not os . path . exists ( image_path ) : 
~~ ~~ if image_uri is None : 
~~ names = parse_image_name ( remove_uri ( image_uri ) ) 
class DummyContainer : 
~~~ def __init__ ( self , image_path , client_name , url , names ) : 
~~~ self . image = image_path 
self . client = client_name 
self . url = url 
self . name = names [ 'image' ] 
self . tag = names [ 'tag' ] 
self . uri = names [ 'uri' ] 
~~ ~~ container = DummyContainer ( image_path , self . client_name , url , names ) 
return container 
~~ def search ( self , query = None , ** kwargs ) : 
~~~ return self . _search_collection ( query ) 
~~ return self . list ( ) 
~~ def list_all ( self , ** kwargs ) : 
quiet = False 
if "quiet" in kwargs : 
~~~ quiet = kwargs [ 'quiet' ] 
~~ bot . spinner . start ( ) 
url = '%s/collections/' % self . base 
results = self . _paginate_get ( url ) 
bot . spinner . stop ( ) 
if len ( results ) == 0 : 
~~ rows = [ ] 
~~~ if "containers" in result : 
~~~ if result [ 'id' ] not in [ 37 , 38 , 39 ] : 
~~~ for c in result [ 'containers' ] : 
~~~ rows . append ( [ c [ 'detail' ] , "%s:%s" % ( c [ 'name' ] , c [ 'tag' ] ) ] ) 
~~ ~~ ~~ ~~ if quiet is False : 
~~~ bot . info ( "Collections" ) 
bot . table ( rows ) 
~~ return rows 
~~ def search_collection ( self , query ) : 
containers = self . list ( quiet = True ) 
for result in containers : 
~~~ if re . search ( query , result [ 1 ] ) : 
~~~ rows . append ( result ) 
~~ ~~ if len ( rows ) > 0 : 
~~~ bot . table ( rows ) 
~~ def pull ( self , images , file_name = None , save = True , ** kwargs ) : 
force = False 
if "force" in kwargs : 
~~~ force = kwargs [ 'force' ] 
~~ if not isinstance ( images , list ) : 
~~~ images = [ images ] 
finished = [ ] 
for image in images : 
~~~ job_id , collection , job_name = self . _parse_image_name ( image ) 
names = parse_image_name ( remove_uri ( collection ) ) 
if file_name is None : 
~~~ file_name = self . _get_storage_name ( names ) 
~~ if os . path . exists ( file_name ) and force is False : 
~~ image_name = "Singularity.%s.simg" % ( names [ 'tag' ] ) 
if names [ 'tag' ] == 'latest' : 
~~~ image_name = "Singularity.simg" 
~~ artifact_path = "%s/%s" % ( self . artifacts , image_name ) 
job_name , 
job_id ) ) 
project = quote_plus ( collection . strip ( '/' ) ) 
url = "%s/%s/-/jobs/%s/artifacts/raw/%s/?inline=false" % ( self . base , 
job_id , 
artifact_path ) 
bot . info ( url ) 
image_file = self . download ( url = url , 
file_name = file_name , 
show_progress = True ) 
metadata = self . _get_metadata ( ) 
metadata [ 'collection' ] = collection 
metadata [ 'job_id' ] = job_id 
metadata [ 'job_name' ] = job_name 
metadata [ 'artifact_path' ] = artifact_path 
metadata [ 'sregistry_pull' ] = image 
if save is True : 
~~~ container = self . add ( image_path = image_file , 
image_uri = image , 
url = url ) 
image_file = container . image 
~~ if os . path . exists ( image_file ) : 
bot . custom ( prefix = "Success!" , message = image_file ) 
finished . append ( image_file ) 
~~ ~~ if len ( finished ) == 1 : 
~~~ finished = finished [ 0 ] 
~~ return finished 
~~ def run ( self , func , tasks , func2 = None ) : 
progress = 1 
total = len ( tasks ) 
if len ( tasks ) == 0 : 
~~ if func2 is not None : 
~~~ total = total * 2 
~~ finished = [ ] 
level1 = [ ] 
~~~ prefix = "[%s/%s]" % ( progress , total ) 
bot . show_progress ( 0 , total , length = 35 , prefix = prefix ) 
pool = multiprocessing . Pool ( self . workers , init_worker ) 
self . start ( ) 
~~~ result = pool . apply_async ( multi_wrapper , 
multi_package ( func , [ task ] ) ) 
level1 . append ( result . _job ) 
~~ while len ( results ) > 0 : 
~~~ result = results . pop ( ) 
result . wait ( ) 
bot . show_progress ( progress , total , length = 35 , prefix = prefix ) 
progress += 1 
prefix = "[%s/%s]" % ( progress , total ) 
if func2 is not None and result . _job in level1 : 
multi_package ( func2 , 
[ ( result . get ( ) , ) ] ) ) 
~~~ finished . append ( result . get ( ) ) 
~~ ~~ self . end ( ) 
pool . terminate ( ) 
~~~ bot . error ( e ) 
~~ def get_cache ( subfolder = None , quiet = False ) : 
DISABLE_CACHE = convert2boolean ( getenv ( "SINGULARITY_DISABLE_CACHE" , 
default = False ) ) 
if DISABLE_CACHE : 
~~~ SINGULARITY_CACHE = tempfile . mkdtemp ( ) 
~~~ userhome = pwd . getpwuid ( os . getuid ( ) ) [ 5 ] 
_cache = os . path . join ( userhome , ".singularity" ) 
SINGULARITY_CACHE = getenv ( "SINGULARITY_CACHEDIR" , default = _cache ) 
~~ cache_base = clean_path ( SINGULARITY_CACHE ) 
if subfolder is not None : 
~~~ cache_base = "%s/%s" % ( cache_base , subfolder ) 
~~ mkdir_p ( cache_base ) 
~~ return cache_base 
~~ def push ( self , path , name , tag = None ) : 
path = os . path . abspath ( path ) 
~~ names = parse_image_name ( remove_uri ( name ) , tag = tag ) 
if names [ 'version' ] is None : 
~~~ version = get_image_hash ( path ) 
names = parse_image_name ( remove_uri ( name ) , tag = tag , version = version ) 
~~ metadata = self . get_metadata ( path , names = names ) 
if "data" in metadata : 
~~~ metadata = metadata [ 'data' ] 
~~ metadata . update ( names ) 
manifest = self . _upload ( source = path , 
destination = names [ 'storage' ] , 
metadata = metadata ) 
print ( manifest [ 'mediaLink' ] ) 
~~ def upload ( self , source , 
destination , 
bucket , 
chunk_size = 2 * 1024 * 1024 , 
keep_private = True ) : 
env = 'SREGISTRY_GOOGLE_STORAGE_PRIVATE' 
keep_private = self . _get_and_update_setting ( env ) or keep_private 
media = MediaFileUpload ( source , chunksize = chunk_size , resumable = True ) 
request = self . _storage_service . objects ( ) . insert ( bucket = bucket . name , 
name = destination , 
media_body = media ) 
total = request . resumable . _size / ( 1024 * 1024.0 ) 
bar = ProgressBar ( expected_size = total , filled_char = '=' , hide = self . quiet ) 
while response is None : 
~~~ error = None 
~~~ progress , response = request . next_chunk ( ) 
if progress : 
~~~ bar . show ( progress . resumable_progress / ( 1024 * 1024.0 ) ) 
~~ ~~ blob = bucket . blob ( destination ) 
if blob . exists ( ) : 
~~~ if not keep_private : 
~~~ blob . make_public ( ) 
~~ if metadata is not None : 
~~~ body = prepare_metadata ( metadata ) 
blob . metadata = metadata 
blob . _properties [ 'metadata' ] = metadata 
blob . patch ( ) 
~~ def update_headers ( self , fields = None ) : 
do_reset = True 
if hasattr ( self , 'headers' ) : 
~~~ if self . headers is not None : 
~~~ do_reset = False 
~~ ~~ if do_reset is True : 
~~~ self . _reset_headers ( ) 
~~ if fields is not None : 
~~~ for key , value in fields . items ( ) : 
~~~ self . headers [ key ] = value 
~~ ~~ header_names = "," . join ( list ( self . headers . keys ( ) ) ) 
~~ def download_task ( url , headers , destination , download_type = 'layer' ) : 
file_name = "%s.%s" % ( destination , 
next ( tempfile . _get_candidate_names ( ) ) ) 
tar_download = download ( url , file_name , headers = headers ) 
~~~ shutil . move ( tar_download , destination ) 
bot . error ( msg ) 
~~ return destination 
~~ def post ( url , data = None , return_json = True ) : 
return call ( url , 
func = requests . post , 
return_json = return_json ) 
~~ def get ( url , headers = None , token = None , data = None , return_json = True ) : 
func = requests . get , 
~~ def call ( url , func , data = None , headers = None , 
return_json = True , stream = False , 
retry = True ) : 
if DISABLE_SSL_CHECK is True : 
~~ if data is not None : 
~~~ if not isinstance ( data , dict ) : 
~~~ data = json . dumps ( data ) 
~~ ~~ response = func ( url = url , 
verify = not DISABLE_SSL_CHECK , 
stream = stream ) 
if response . status_code in [ 500 , 502 ] : 
response . status_code ) ) 
~~ if response . status_code == 404 : 
~~ if response . status_code == 401 : 
~~~ if retry is True : 
~~~ headers = update_token ( response , headers ) 
return call ( url , func , data = data , 
return_json = return_json , 
stream = stream , retry = False ) 
~~ elif response . status_code == 200 : 
~~~ if return_json : 
~~~ response = response . json ( ) 
~~ ~~ ~~ return response 
~~ def require_secrets ( self , params = None ) : 
name = self . client_name 
has_secrets = True 
if not hasattr ( self , 'secrets' ) : 
~~~ has_secrets = False 
~~ elif hasattr ( self , 'secrets' ) : 
~~~ if self . secrets is None : 
~~ ~~ elif self . client_name not in self . secrets : 
~~ if has_secrets is False : 
bot . error ( message ) 
~~~ if not isinstance ( params , list ) : 
~~~ params = [ params ] 
~~ for param in params : 
~~~ if param not in self . secrets [ name ] : 
~~ elif self . secrets [ name ] [ param ] in [ None , '' ] : 
~~ ~~ if has_secrets is False : 
~~ ~~ ~~ def auth_flow ( self , url ) : 
get_input = getattr ( __builtins__ , 'raw_input' , input ) 
code = get_input ( message ) . strip ( ) 
return code 
~~ def download ( url , file_name , headers = None , show_progress = True ) : 
fd , tmp_file = tempfile . mkstemp ( prefix = ( "%s.tmp." % file_name ) ) 
~~ verify = not DISABLE_SSL_CHECK 
response = stream ( url , headers = headers , stream_to = tmp_file ) 
shutil . move ( tmp_file , file_name ) 
return file_name 
~~ def stream ( url , headers , stream_to = None , retry = True ) : 
~~ response = requests . get ( url , 
stream = True ) 
if response . status_code in [ 401 , 403 ] : 
~~~ headers = update_token ( headers ) 
return stream ( url , headers , stream_to , retry = False ) 
~~~ content_size = None 
if 'Content-Length' in response . headers : 
~~~ progress = 0 
content_size = int ( response . headers [ 'Content-Length' ] ) 
bot . show_progress ( progress , content_size , length = 35 ) 
~~ chunk_size = 1 << 20 
with open ( stream_to , 'wb' ) as filey : 
~~~ for chunk in response . iter_content ( chunk_size = chunk_size ) : 
~~~ filey . write ( chunk ) 
if content_size is not None : 
~~~ progress += chunk_size 
bot . show_progress ( iteration = progress , 
total = content_size , 
length = 35 , 
carriage_return = False ) 
~~ ~~ ~~ sys . stdout . write ( '\\n' ) 
return stream_to 
~~ def update_token ( headers ) : 
~~~ from awscli . clidriver import create_clidriver 
~~ driver = create_clidriver ( ) 
aws = driver . session . create_client ( 'ecr' ) 
tokens = aws . get_authorization_token ( ) 
token = tokens [ 'authorizationData' ] [ 0 ] [ 'authorizationToken' ] 
headers . update ( token ) 
~~ def get_or_create_folder ( self , folder ) : 
q = "mimeType=\ % folder 
response = self . _service . files ( ) . list ( q = q , 
spaces = 'drive' ) . execute ( ) . get ( 'files' , [ ] ) 
if len ( response ) == 0 : 
~~~ folder = self . _create_folder ( folder ) 
~~~ folder = response [ 0 ] 
~~ return folder 
~~ def create_folder ( self , folder ) : 
folder_metadata = { 
'name' : os . path . basename ( folder ) , 
'mimeType' : 'application/vnd.google-apps.folder' 
created = self . _service . files ( ) . create ( body = folder_metadata , 
fields = 'id' ) . execute ( ) 
return created 
~~ def _read_response ( self , response , field = "detail" ) : 
~~~ message = json . loads ( response . _content . decode ( 'utf-8' ) ) [ field ] 
~~~ message = response . reason 
~~ def get_bucket_name ( self ) : 
bucket_name = 'sregistry-%s' % RobotNamer ( ) . generate ( ) 
self . bucket_name = self . _get_and_update_setting ( 'SREGISTRY_S3_BUCKET' , 
bucket_name ) 
~~ def get_bucket ( self ) : 
for attr in [ 'bucket_name' , 's3' ] : 
~~ ~~ self . bucket = None 
for bucket in self . s3 . buckets . all ( ) : 
~~~ if bucket . name == self . bucket_name : 
~~~ self . bucket = bucket 
~~ ~~ if self . bucket is None : 
~~~ self . bucket = self . s3 . create_bucket ( Bucket = self . bucket_name ) 
~~ return self . bucket 
~~ def get_resource ( self ) : 
if self . base != None : 
~~~ self . s3 = boto3 . resource ( 's3' , 
endpoint_url = self . base , 
aws_access_key_id = self . _id , 
aws_secret_access_key = self . _key , 
config = boto3 . session . Config ( signature_version = self . _signature ) ) 
~~~ self . s3 = boto3 . client ( 's3' ) 
~~ ~~ def _update_secrets ( self , base = None ) : 
self . base = self . _get_and_update_setting ( 'SREGISTRY_S3_BASE' , self . base ) 
self . _id = self . _required_get_and_update ( 'AWS_ACCESS_KEY_ID' ) 
self . _key = self . _required_get_and_update ( 'AWS_SECRET_ACCESS_KEY' ) 
self . _signature = self . _get_and_update_setting ( 'SREGISTRY_S3_SIGNATURE' ) 
if self . _signature == 's3' : 
~~~ self . _signature = 's3' 
~~~ self . _signature = 's3v4' 
~~ self . get_bucket_name ( ) 
self . get_resource ( ) 
self . get_bucket ( ) 
~~ def _add_https ( self , q ) : 
if not q [ 'registry' ] . startswith ( 'http' ) : 
~~~ if q [ 'original' ] . startswith ( 'http:' ) : 
~~~ q [ 'registry' ] = 'http://%s' % q [ 'registry' ] 
~~ elif q [ 'original' ] . startswith ( 'https:' ) : 
~~~ q [ 'registry' ] = 'https://%s' % q [ 'registry' ] 
~~~ prefix = 'https://' 
nohttps = os . environ . get ( 'SREGISTRY_REGISTRY_NOHTTPS' ) 
if nohttps != None : 
~~~ prefix = 'http://' 
~~ q [ 'registry' ] = '%s%s' % ( prefix , q [ 'registry' ] ) 
~~ ~~ return q 
~~ def _update_secrets ( self ) : 
self . secrets = read_client_secrets ( ) 
if self . secrets is not None : 
~~~ if "registry" in self . secrets : 
~~~ if "base" in self . secrets [ 'registry' ] : 
~~~ self . base = self . secrets [ 'registry' ] [ 'base' ] 
self . _update_base ( ) 
~~ ~~ ~~ ~~ def _init_clients ( self ) : 
self . _client = globus_sdk . NativeAppAuthClient ( self . _client_id ) 
self . _load_secrets ( ) 
~~ def _load_secrets ( self ) : 
self . auth = self . _get_and_update_setting ( 'GLOBUS_AUTH_RESPONSE' ) 
self . transfer = self . _get_and_update_setting ( 'GLOBUS_TRANSFER_RESPONSE' ) 
~~ def _tokens_need_update ( self ) : 
needs_update = True 
if self . auth is not None : 
~~~ if self . auth [ 'expires_at_seconds' ] > time . time ( ) : 
~~~ needs_update = False 
~~ ~~ return needs_update 
~~ def _update_tokens ( self ) : 
self . _client . oauth2_start_flow ( refresh_tokens = True ) 
authorize_url = self . _client . oauth2_get_authorize_url ( ) 
auth_code = raw_input ( 
self . _response = self . _client . oauth2_exchange_code_for_tokens ( auth_code ) 
self . auth = self . _response . by_resource_server [ 'auth.globus.org' ] 
self . transfer = self . _response . by_resource_server [ 'transfer.api.globus.org' ] 
self . _update_setting ( 'GLOBUS_TRANSFER_RESPONSE' , self . transfer ) 
self . _update_setting ( 'GLOBUS_AUTH_RESPONSE' , self . auth ) 
~~ def logs ( self , name = None ) : 
results = self . _list_logs ( ) 
print ( results ) 
~~~ for result in results : 
~~~ matches = False 
if name in result . name : 
~~~ matches = True 
~~ for key , val in result . metadata . items ( ) : 
~~~ if name in val : 
~~ ~~ if matches is True : 
~~~ content = self . _print_log ( result . name ) 
~~~ if len ( results ) > 0 : 
~~~ latest = results [ 0 ] 
~~~ if result . time_created >= latest . time_created : 
~~~ latest = result 
~~ ~~ content = self . _print_log ( result . name ) 
~~ ~~ return content 
~~ def list_logs ( self ) : 
for image in self . _bucket . list_blobs ( ) : 
~~~ if image . name . endswith ( 'log' ) : 
~~~ results . append ( image ) 
~~ ~~ if len ( results ) == 0 : 
~~ def print_log ( self , logname ) : 
logfile = self . _bucket . get_blob ( logname ) 
print ( logname ) 
if logfile : 
~~~ bot . info ( '[%s]' % logname ) 
content = requests . get ( logfile . media_link ) . text 
print ( content ) 
~~ def parse_endpoint_name ( self , endpoint ) : 
parts = [ x for x in endpoint . split ( ':' ) if x ] 
endpoint = parts [ 0 ] 
if len ( parts ) == 1 : 
~~~ path = '' 
~~~ path = '/' . join ( parts [ 1 : ] ) 
~~ return endpoint , path 
~~ def create_endpoint_folder ( self , endpoint_id , folder ) : 
~~~ res = self . transfer_client . operation_mkdir ( endpoint_id , folder ) 
~~ except TransferAPIError : 
~~ ~~ def get_endpoint_path ( self , endpoint_id ) : 
config = os . path . expanduser ( "~/.globusonline/lta/config-paths" ) 
if not os . path . exists ( config ) : 
~~ path = None 
config = [ x . split ( ',' ) [ 0 ] for x in read_file ( config ) ] 
for path in config : 
~~ ~~ if path is None : 
~~ return path 
~~ def init_transfer_client ( self ) : 
if self . _tokens_need_update ( ) : 
~~~ self . _update_tokens ( ) 
~~ access_token = self . transfer [ 'access_token' ] 
authorizer = globus_sdk . RefreshTokenAuthorizer ( 
self . transfer [ 'refresh_token' ] , 
self . _client , 
access_token = self . transfer [ 'access_token' ] , 
expires_at = self . transfer [ 'expires_at_seconds' ] ) 
self . transfer_client = globus_sdk . TransferClient ( authorizer = authorizer ) 
~~ def get_endpoint ( self , endpoint_id ) : 
endpoint = None 
if not hasattr ( self , 'transfer_client' ) : 
~~~ self . _init_transfer_client ( ) 
~~~ endpoint = self . transfer_client . get_endpoint ( endpoint_id ) . data 
~~ return endpoint 
~~ def get_endpoints ( self , query = None ) : 
self . endpoints = { } 
~~ scopes = { 'my-endpoints' : None , 
'shared-with-me' : None } 
~~~ scopes . update ( { 'all' : query } ) 
~~ for scope , q in scopes . items ( ) : 
~~~ self . endpoints [ scope ] = { } 
for ep in self . transfer_client . endpoint_search ( q , filter_scope = scope ) : 
~~~ ep = ep . __dict__ [ '_data' ] 
self . endpoints [ scope ] [ ep [ 'id' ] ] = ep 
~~ ~~ if len ( self . endpoints [ 'my-endpoints' ] ) == 0 : 
bot . warning ( 'https://www.globus.org/globus-connect-personal' ) 
~~ return self . endpoints 
~~ def list_containers ( self ) : 
folder = self . _get_or_create_folder ( self . _base ) 
next_page = None 
containers = [ ] 
response = self . _service . files ( ) . list ( q = query , 
spaces = 'drive' , 
pageToken = next_page ) . execute ( ) 
containers += response . get ( 'files' , [ ] ) 
next_page = response . get ( 'nextPageToken' ) 
if not next_page : 
~~ ~~ if len ( containers ) == 0 : 
~~ return containers 
~~ def search_all ( self ) : 
results = self . _list_containers ( ) 
for i in results : 
~~~ uri = i [ 'name' ] . replace ( '.simg' , '' ) 
if 'properties' in i : 
~~~ if 'uri' in i [ 'properties' ] : 
~~~ uri = i [ 'properties' ] [ 'uri' ] 
~~ ~~ rows . append ( [ i [ 'id' ] , uri ] ) 
i [ 'uri' ] = uri 
matches . append ( i ) 
message = "\\t\\t[id]\\t[uri]" , 
color = "PURPLE" ) 
return matches 
~~ def container_query ( self , query , quiet = False ) : 
~~~ is_match = False 
if query in result [ 'id' ] : 
~~~ is_match = True 
~~ elif query in result [ 'name' ] : 
~~~ for key , val in result [ 'properties' ] . items ( ) : 
~~~ if query in val and is_match is False : 
~~ ~~ ~~ if is_match is True : 
for image in matches : 
~~~ if 'properties' in image : 
~~~ image . update ( image [ 'properties' ] ) 
~~ bot . info ( image [ 'uri' ] ) 
for key in sorted ( image , key = len ) : 
~~~ val = image [ key ] 
if isinstance ( val , str ) : 
~~~ bot . custom ( prefix = key . ljust ( 10 ) , message = val , color = "CYAN" ) 
~~ ~~ bot . newline ( ) 
~~ ~~ return matches 
~~ def status ( backend ) : 
settings = read_client_secrets ( ) 
if 'SREGISTRY_CLIENT' in settings : 
update_secrets ( settings ) 
~~ ~~ def add ( backend , variable , value , force = False ) : 
print ( '[add]' ) 
prefix = 'SREGISTRY_%s_' % backend . upper ( ) 
if not variable . startswith ( prefix ) : 
~~~ variable = '%s%s' % ( prefix , variable ) 
~~ variable = variable . upper ( ) 
if backend in settings : 
~~~ if variable in settings [ backend ] and force is False : 
~~~ previous = settings [ backend ] [ variable ] 
~~ ~~ if backend not in settings : 
~~~ settings [ backend ] = { } 
~~ settings [ backend ] [ variable ] = value 
~~ def remove ( backend , variable ) : 
print ( '[remove]' ) 
prefixed = variable 
~~~ prefixed = '%s%s' % ( prefix , variable ) 
bot . info ( variable ) 
~~~ if variable in settings [ backend ] : 
~~~ del settings [ backend ] [ variable ] 
~~ if prefixed in settings [ backend ] : 
~~~ del settings [ backend ] [ prefixed ] 
~~ update_secrets ( settings ) 
~~ ~~ def activate ( backend ) : 
if backend is not None : 
~~~ settings [ 'SREGISTRY_CLIENT' ] = backend 
~~ ~~ def delete_backend ( backend ) : 
~~~ del settings [ backend ] 
~~~ if settings [ 'SREGISTRY_CLIENT' ] == backend : 
~~~ del settings [ 'SREGISTRY_CLIENT' ] 
~~ ~~ update_secrets ( settings ) 
~~~ if backend is not None : 
~~ ~~ ~~ def list_backends ( backend = None ) : 
backends = list ( settings . keys ( ) ) 
backends = [ b for b in backends if b != 'SREGISTRY_CLIENT' ] 
if backend in backends : 
~~~ bot . info ( backend ) 
print ( json . dumps ( settings [ backend ] , indent = 4 , sort_keys = True ) ) 
print ( '\\n' . join ( backends ) ) 
~~ ~~ def pull ( self , images , file_name = None , save = True , ** kwargs ) : 
~~~ names = parse_image_name ( remove_uri ( image ) ) 
dropbox_path = '/%s' % names [ 'storage' ] 
~~ if self . exists ( dropbox_path ) is True : 
~~~ metadata , response = self . dbx . files_download ( dropbox_path ) 
image_file = self . _stream ( response , stream_to = file_name ) 
metadata = self . _get_metadata ( image_file , metadata ) 
image_uri = dropbox_path . strip ( '/' ) , 
url = response . url ) 
metadata = self . get_metadata ( path , names = names ) 
file_size = os . path . getsize ( path ) 
chunk_size = 4 * 1024 * 1024 
storage_path = "/%s" % names [ 'storage' ] 
progress = 0 
bot . show_progress ( progress , file_size , length = 35 ) 
with open ( path , 'rb' ) as F : 
~~~ if file_size <= chunk_size : 
~~~ self . dbx . files_upload ( F . read ( ) , storage_path ) 
~~~ start = self . dbx . files_upload_session_start ( F . read ( chunk_size ) ) 
cursor = dropbox . files . UploadSessionCursor ( session_id = start . session_id , 
offset = F . tell ( ) ) 
commit = dropbox . files . CommitInfo ( path = storage_path ) 
while F . tell ( ) < file_size : 
if ( ( file_size - F . tell ( ) ) <= chunk_size ) : 
~~~ self . dbx . files_upload_session_finish ( F . read ( chunk_size ) , 
cursor , 
commit ) 
~~~ self . dbx . files_upload_session_append ( F . read ( chunk_size ) , 
cursor . session_id , 
cursor . offset ) 
cursor . offset = F . tell ( ) 
~~ bot . show_progress ( iteration = progress , 
total = file_size , 
~~ ~~ ~~ bot . show_progress ( iteration = file_size , 
carriage_return = True ) 
~~ def _update_base ( self , image ) : 
base = None 
if "gcr.io" in image : 
~~~ base = 'gcr.io' 
self . _set_base ( default_base = base ) 
self . _update_secrets ( ) 
~~ return base 
~~ def _set_base ( self , default_base = None ) : 
base = self . _get_setting ( 'SREGISTRY_DOCKERHUB_BASE' ) 
version = self . _get_setting ( 'SREGISTRY_DOCKERHUB_VERSION' ) 
if base is None : 
~~~ if default_base is None : 
~~~ base = "index.docker.io" 
~~~ base = default_base 
~~ ~~ if version is None : 
~~~ version = "v2" 
~~ nohttps = self . _get_setting ( 'SREGISTRY_DOCKERHUB_NOHTTPS' ) 
if nohttps is None : 
~~~ nohttps = "https://" 
~~~ nohttps = "http://" 
~~ self . _base = "%s%s" % ( nohttps , base ) 
self . _version = version 
self . base = "%s%s/%s" % ( nohttps , base . strip ( '/' ) , version ) 
credentials = self . _get_setting ( 'SREGISTRY_DOCKERHUB_SECRETS' ) 
username = self . _get_setting ( 'SINGULARITY_DOCKER_USERNAME' ) 
password = self . _get_setting ( 'SINGULARITY_DOCKER_PASSWORD' ) 
username = self . _get_setting ( 'SREGISTRY_DOCKERHUB_USERNAME' , username ) 
password = self . _get_setting ( 'SREGISTRY_DOCKERHUB_PASSWORD' , password ) 
auth = None 
if username is not None and password is not None : 
~~~ auth = basic_auth_header ( username , password ) 
self . headers . update ( auth ) 
~~ if credentials is not None and auth is None : 
~~~ if os . path . exists ( credentials ) : 
~~~ credentials = read_json ( credentials ) 
if "auths" in credentials : 
~~~ for auths , params in credentials [ 'auths' ] . items ( ) : 
~~~ if self . _base in auths : 
~~~ if 'auth' in params : 
self . headers [ 'Authorization' ] = auth 
~~ ~~ ~~ ~~ if 'HttpHeaders' in credentials : 
~~~ for key , value in credentials [ 'HttpHeaders' ] . items ( ) : 
~~ ~~ ~~ def share ( self , query , share_to ) : 
images = self . _container_query ( query , quiet = True ) 
if len ( images ) == 0 : 
~~ image = images [ 0 ] 
def callback ( request_id , response , exception ) : 
~~~ if exception : 
~~~ print ( exception ) 
~~~ share_id = response . get ( 'id' ) 
~~ ~~ batch = self . _service . new_batch_http_request ( callback = callback ) 
user_permission = { 
'type' : 'user' , 
'role' : 'reader' , 
'emailAddress' : share_to 
batch . add ( self . _service . permissions ( ) . create ( 
fileId = image [ 'id' ] , 
body = user_permission , 
fields = 'id' , 
batch . execute ( ) 
if not isinstance ( images , list ) : 
~~~ q = parse_image_name ( remove_uri ( image ) ) 
matches = self . _container_query ( q [ 'uri' ] , quiet = True ) 
~~ if file_name is None : 
~~~ file_name = q [ 'storage' ] . replace ( '/' , '-' ) 
~~ image = matches [ 0 ] 
request = self . _service . files ( ) . get_media ( fileId = image [ 'id' ] ) 
with open ( file_name , 'wb' ) as fh : 
~~~ downloader = MediaIoBaseDownload ( fh , request ) 
bar = None 
while done is False : 
~~~ status , done = downloader . next_chunk ( ) 
if bar is None : 
~~~ total = status . total_size / ( 1024 * 1024.0 ) 
bar = ProgressBar ( expected_size = total , 
filled_char = '=' , 
hide = self . quiet ) 
~~ bar . show ( status . resumable_progress / ( 1024 * 1024.0 ) ) 
~~ ~~ if save is True : 
~~~ image_uri = q [ 'uri' ] 
if "uri" in image : 
~~~ image_uri = image [ 'uri' ] 
~~ image [ 'selfLink' ] = downloader . _uri 
container = self . add ( image_path = image_file , 
image_uri = image_uri , 
metadata = image , 
url = downloader . _uri ) 
~~ def basic_auth_header ( username , password ) : 
s = "%s:%s" % ( username , password ) 
~~~ s = bytes ( s , 'utf-8' ) 
credentials = base64 . b64encode ( s ) . decode ( 'utf-8' ) 
~~~ credentials = base64 . b64encode ( s ) 
return auth 
~~ def generate_signature ( payload , secret ) : 
payload = _encode ( payload ) 
secret = _encode ( secret ) 
return hmac . new ( secret , digestmod = hashlib . sha256 , 
msg = payload ) . hexdigest ( ) 
~~ def generate_credential ( s ) : 
~~ def generate_header_signature ( secret , payload , request_type ) : 
timestamp = generate_timestamp ( ) 
credential = "%s/%s" % ( request_type , timestamp ) 
signature = generate_signature ( payload , secret ) 
~~ def delete ( self , url , 
headers = None , 
return_json = True , 
default_headers = True ) : 
return self . _call ( url , 
func = requests . delete , 
default_headers = default_headers ) 
~~ def head ( self , url ) : 
return self . _call ( url , func = requests . head ) 
~~ def healthy ( self , url ) : 
response = requests . get ( url ) 
status_code = response . status_code 
if status_code != 200 : 
~~ def post ( self , url , 
data = None , 
~~ def get ( self , url , 
token = None , 
default_headers = True , 
quiet = False ) : 
default_headers = default_headers , 
quiet = quiet ) 
~~ def paginate_get ( self , url , 
start_page = None ) : 
geturl = '%s&page=1' % ( url ) 
if start_page is not None : 
~~~ geturl = '%s&page=%s' % ( url , start_page ) 
while geturl is not None : 
~~~ result = self . _get ( url , headers = headers , return_json = return_json ) 
if isinstance ( result , dict ) : 
~~~ if 'results' in result : 
~~~ results = results + result [ 'results' ] 
~~ geturl = result [ 'next' ] 
~~ def verify ( self ) : 
from sregistry . defaults import DISABLE_SSL_CHECK 
~~ return not DISABLE_SSL_CHECK 
~~ def call ( self , url , func , data = None , 
stream = False , 
retry = True , 
~~ ~~ heads = dict ( ) 
if default_headers is True : 
~~~ heads = self . headers . copy ( ) 
~~ if headers is not None : 
~~~ if isinstance ( headers , dict ) : 
~~~ heads . update ( headers ) 
headers = heads , 
verify = self . _verify ( ) , 
~~~ if quiet is False : 
~~~ if retry is True and hasattr ( self , '_update_token' ) : 
~~~ self . _update_token ( response ) 
return self . _call ( url , func , data = data , 
~~ self . require_secrets ( ) 
if q [ 'registry' ] == None : 
~~~ q [ 'registry' ] = self . base 
~~ q = self . _add_https ( q ) 
if not q [ 'registry' ] . endswith ( 'api' ) : 
~~~ q [ 'registry' ] = '%s/api' % q [ 'registry' ] 
~~ url = "%s/container/%s/%s:%s" % ( q [ 'registry' ] , 
q [ 'collection' ] , 
q [ 'image' ] , 
q [ 'tag' ] ) 
~~~ manifest = self . _get ( url ) 
~~ except SSLError : 
~~ if isinstance ( manifest , Response ) : 
~~~ if manifest . status_code == 403 : 
~~~ SREGISTRY_EVENT = self . authorize ( request_type = "pull" , 
names = q ) 
headers = { 'Authorization' : SREGISTRY_EVENT } 
self . _update_headers ( headers ) 
manifest = self . _get ( url ) 
if isinstance ( manifest , Response ) : 
~~~ manifest = 403 
~~ ~~ ~~ ~~ if isinstance ( manifest , int ) : 
~~~ if manifest == 400 : 
~~ elif manifest == 404 : 
~~ elif manifest == 403 : 
~~ if "image" in manifest : 
~~~ manifest [ 'selfLink' ] = url 
~~ image_file = self . download ( url = manifest [ 'image' ] , 
show_progress = not self . quiet ) 
~~~ image_uri = "%s/%s:%s" % ( manifest [ 'collection' ] , 
manifest [ 'name' ] , 
manifest [ 'tag' ] ) 
metadata = manifest , 
url = manifest [ 'image' ] ) 
~~~ image = remove_uri ( image ) 
names = parse_image_name ( image ) 
~~~ file_name = names [ 'storage' ] . replace ( '/' , '-' ) 
~~ uri = names [ 'storage_uri' ] 
~~~ self . bucket . download_file ( uri , file_name ) 
~~ except botocore . exceptions . ClientError as e : 
~~~ if e . response [ 'Error' ] [ 'Code' ] == "404" : 
results = self . _search_all ( query = image ) 
if len ( results ) > 0 : 
~~ ~~ found = None 
for obj in self . bucket . objects . filter ( Prefix = image ) : 
~~~ if image in obj . key : 
~~~ found = obj 
~~ ~~ metadata = { } 
if found != None : 
~~~ metadata = found . get ( ) [ 'Metadata' ] 
metadata = dict ( ( k . lower ( ) , v ) for k , v in metadata . items ( ) ) 
if save is True and os . path . exists ( file_name ) : 
~~~ container = self . add ( image_path = file_name , 
image_uri = names [ 'tag_uri' ] , 
file_name = container . image 
~~ if os . path . exists ( file_name ) : 
~~~ bot . custom ( prefix = "Success!" , message = file_name ) 
finished . append ( file_name ) 
~~ if len ( finished ) == 1 : 
~~ ~~ def remove ( self , image , force = False ) : 
q = parse_image_name ( remove_uri ( image ) ) 
url = '%s/container/%s/%s:%s' % ( q [ 'registry' ] , 
q [ "collection" ] , 
q [ "image" ] , 
q [ "tag" ] ) 
SREGISTRY_EVENT = self . authorize ( request_type = "delete" , names = q ) 
self . _update_headers ( fields = headers ) 
continue_delete = True 
if force is False : 
while len ( response ) < 1 or response [ 0 ] . lower ( ) . strip ( ) not in "ynyesno" : 
~~ if response [ 0 ] . lower ( ) . strip ( ) in "no" : 
~~~ continue_delete = False 
~~ ~~ if continue_delete is True : 
~~~ response = self . _delete ( url ) 
message = self . _read_response ( response ) 
~~ ~~ def get_lookup ( ) : 
lookup = dict ( ) 
version_file = os . path . join ( 'sregistry' , 'version.py' ) 
with open ( version_file ) as filey : 
~~~ exec ( filey . read ( ) , lookup ) 
~~ return lookup 
~~ def get_reqs ( lookup = None , key = 'INSTALL_REQUIRES' ) : 
if lookup == None : 
~~~ lookup = get_lookup ( ) 
~~ install_requires = [ ] 
for module in lookup [ key ] : 
~~~ module_name = module [ 0 ] 
module_meta = module [ 1 ] 
if "exact_version" in module_meta : 
~~~ dependency = "%s==%s" % ( module_name , module_meta [ 'exact_version' ] ) 
~~ elif "min_version" in module_meta : 
~~~ if module_meta [ 'min_version' ] == None : 
~~~ dependency = module_name 
~~~ dependency = "%s>=%s" % ( module_name , module_meta [ 'min_version' ] ) 
~~ ~~ install_requires . append ( dependency ) 
~~ return install_requires 
~~ def get_singularity_version ( singularity_version = None ) : 
if singularity_version is None : 
~~~ singularity_version = os . environ . get ( "SINGULARITY_VERSION" ) 
~~ if singularity_version is None : 
~~~ cmd = [ 'singularity' , '--version' ] 
output = run_command ( cmd ) 
if isinstance ( output [ 'message' ] , bytes ) : 
~~~ output [ 'message' ] = output [ 'message' ] . decode ( 'utf-8' ) 
~~ singularity_version = output [ 'message' ] . strip ( '\\n' ) 
~~~ singularity_version = None 
~~ ~~ return singularity_version 
~~ def check_install ( software = None , quiet = True ) : 
if software is None : 
~~~ software = "singularity" 
~~ cmd = [ software , '--version' ] 
~~~ version = run_command ( cmd , software ) 
~~ if version is not None : 
~~~ if quiet is False and version [ 'return_code' ] == 0 : 
~~~ version = version [ 'message' ] 
~~ def get_installdir ( ) : 
return os . path . abspath ( os . path . dirname ( os . path . dirname ( __file__ ) ) ) 
~~ def get_thumbnail ( ) : 
from sregistry . defaults import SREGISTRY_THUMBNAIL 
if SREGISTRY_THUMBNAIL is not None : 
~~~ if os . path . exists ( SREGISTRY_THUMBNAIL ) : 
~~~ return SREGISTRY_THUMBNAIL 
~~ ~~ return "%s/database/robot.png" % get_installdir ( ) 
~~ def run_command ( cmd , sudo = False ) : 
if sudo is True : 
~~~ cmd = [ 'sudo' ] + cmd 
~~~ output = Popen ( cmd , stderr = STDOUT , stdout = PIPE ) 
~~~ cmd . pop ( 0 ) 
output = Popen ( cmd , stderr = STDOUT , stdout = PIPE ) 
~~ t = output . communicate ( ) [ 0 ] , output . returncode 
output = { 'message' : t [ 0 ] , 
'return_code' : t [ 1 ] } 
~~ def _speak ( self ) : 
if hasattr ( self , 'account' ) : 
~~ ~~ def _get_metadata ( self , image_file = None , dbx_metadata = None ) : 
metadata = dict ( ) 
if dbx_metadata is not None : 
~~~ for key in dbx_metadata . __dir__ ( ) : 
~~~ value = getattr ( dbx_metadata , key ) 
if type ( value ) in [ str , datetime . datetime , bool , int , float ] : 
~~~ metadata [ key . strip ( '_' ) ] = value 
~~ ~~ ~~ return self . get_metadata ( image_file , names = metadata ) 
token = self . _required_get_and_update ( 'SREGISTRY_DROPBOX_TOKEN' ) 
self . dbx = Dropbox ( token ) 
~~~ self . account = self . dbx . users_get_current_account ( ) 
~~ except AuthError as err : 
~~ ~~ def print_output ( response , output_file = None ) : 
if response [ 'status' ] == 'SUCCESS' : 
~~~ bucket = response [ 'artifacts' ] [ 'objects' ] [ 'location' ] 
obj = response [ 'artifacts' ] [ 'objects' ] [ 'paths' ] [ 0 ] 
bot . custom ( "MD5HASH" , response [ 'file_hash' ] , 'CYAN' ) 
bot . custom ( "SIZE" , response [ 'size' ] , 'CYAN' ) 
bot . custom ( response [ 'status' ] , bucket + obj , 'CYAN' ) 
~~ bot . custom ( "LOGS" , response [ 'logUrl' ] , 'CYAN' ) 
if "public_url" in response : 
~~~ bot . custom ( 'URL' , response [ 'public_url' ] , 'CYAN' ) 
~~ if output_file != None : 
~~~ with open ( output_file , 'w' ) as filey : 
~~~ if response [ 'status' ] == 'SUCCESS' : 
~~ ~~ ~~ ~~ def kill ( args ) : 
from sregistry . main import Client as cli 
if len ( args . commands ) > 0 : 
~~~ for name in args . commands : 
~~~ cli . destroy ( name ) 
~~ ~~ sys . exit ( 0 ) 
~~ def templates ( args , template_name = None ) : 
from sregistry . main import get_client 
cli = get_client ( init = False ) 
~~~ template_name = args . commands . pop ( 0 ) 
~~ cli . list_templates ( template_name ) 
~~ def list_logs ( args , container_name = None ) : 
~~~ container_name = args . commands . pop ( 0 ) 
~~ cli . logs ( container_name ) 
~~ def get_collections ( self ) : 
collections = [ ] 
for container in self . conn . get_account ( ) [ 1 ] : 
~~~ collections . append ( container [ 'name' ] ) 
~~ return collections 
~~ def _get_or_create_collection ( self , name ) : 
~~~ collection = self . _get_collection ( name ) 
collection = self . conn . put_container ( name ) 
~~ return collection 
self . config [ 'SREGISTRY_SWIFT_AUTHTYPE' ] = self . _required_get_and_update ( 
'SREGISTRY_SWIFT_AUTHTYPE' ) 
if self . config [ 'SREGISTRY_SWIFT_AUTHTYPE' ] == 'preauth' : 
~~~ for envar in [ 'SREGISTRY_SWIFT_OS_AUTH_TOKEN' , 
'SREGISTRY_SWIFT_OS_STORAGE_URL' ] : 
~~~ self . config [ envar ] = self . _required_get_and_update ( envar ) 
~~ self . conn = swiftclient . Connection ( 
preauthurl = self . config [ 'SREGISTRY_SWIFT_OS_STORAGE_URL' ] , 
preauthtoken = self . config [ 'SREGISTRY_SWIFT_OS_AUTH_TOKEN' ] 
~~ elif self . config [ 'SREGISTRY_SWIFT_AUTHTYPE' ] == 'keystonev3' : 
~~~ for envar in [ 'SREGISTRY_SWIFT_USER' , 
'SREGISTRY_SWIFT_TOKEN' , 
'SREGISTRY_SWIFT_URL' ] : 
~~ auth_url = '%s/v3' % self . config [ 'SREGISTRY_SWIFT_URL' ] 
_os_options = { 
'user_domain_name' : 'Default' , 
'project_domain_name' : 'Default' , 
'project_name' : 'Default' 
self . conn = swiftclient . Connection ( 
user = self . config [ 'SREGISTRY_SWIFT_USER' ] , 
key = self . config [ 'SREGISTRY_SWIFT_TOKEN' ] , 
os_options = _os_options , 
authurl = auth_url , 
auth_version = '3' 
~~ elif self . config [ 'SREGISTRY_SWIFT_AUTHTYPE' ] == 'keystonev2' : 
'SREGISTRY_SWIFT_TENANT' , 
'SREGISTRY_SWIFT_REGION' , 
~~ auth_url = '%s/v2.0/' % self . config [ 'SREGISTRY_SWIFT_URL' ] 
'tenant_name' : self . config [ 'SREGISTRY_SWIFT_TENANT' ] , 
'region_name' : self . config [ 'SREGISTRY_SWIFT_REGION' ] 
auth_version = '2' 
~~ auth_url = '%s/auth/' % self . config [ 'SREGISTRY_SWIFT_URL' ] 
~~ ~~ def _update_secrets ( self ) : 
env = 'GOOGLE_APPLICATION_CREDENTIALS' 
if self . _secrets is None : 
~~ ~~ def _init_client ( self ) : 
self . _get_services ( ) 
env = 'SREGISTRY_GOOGLE_STORAGE_BUCKET' 
self . _bucket_name = self . _get_and_update_setting ( env ) 
if self . _bucket_name is None : 
~~~ self . _bucket_name = 'sregistry-gcloud-build-%s' % os . environ [ 'USER' ] 
~~ self . _build_bucket_name = "%s_cloudbuild" % self . _bucket_name 
self . _bucket = self . _get_bucket ( self . _bucket_name ) 
self . _build_bucket = self . _get_bucket ( self . _build_bucket_name ) 
~~ def _get_bucket ( self , bucket_name ) : 
~~~ bucket = self . _bucket_service . get_bucket ( bucket_name ) 
~~ except google . cloud . exceptions . NotFound : 
~~~ bucket = self . _bucket_service . create_bucket ( bucket_name ) 
~~ return bucket 
~~ def get_client ( image = None , quiet = False , ** kwargs ) : 
from sregistry . defaults import SREGISTRY_CLIENT 
if not check_install ( ) : 
~~ client_name = get_uri ( image ) 
if client_name is not None : 
~~~ SREGISTRY_CLIENT = client_name 
~~ if SREGISTRY_CLIENT == 'aws' : from . aws import Client 
elif SREGISTRY_CLIENT == 'docker' : from . docker import Client 
elif SREGISTRY_CLIENT == 'dropbox' : from . dropbox import Client 
elif SREGISTRY_CLIENT == 'gitlab' : from . gitlab import Client 
elif SREGISTRY_CLIENT == 'globus' : from . globus import Client 
elif SREGISTRY_CLIENT == 'nvidia' : from . nvidia import Client 
elif SREGISTRY_CLIENT == 'hub' : from . hub import Client 
elif SREGISTRY_CLIENT == 'google-drive' : from . google_drive import Client 
elif SREGISTRY_CLIENT == 'google-compute' : from . google_storage import Client 
elif SREGISTRY_CLIENT == 'google-storage' : from . google_storage import Client 
elif SREGISTRY_CLIENT == 'google-build' : from . google_build import Client 
elif SREGISTRY_CLIENT == 'registry' : from . registry import Client 
elif SREGISTRY_CLIENT == 's3' : from . s3 import Client 
elif SREGISTRY_CLIENT == 'swift' : from . swift import Client 
else : from . hub import Client 
Client . client_name = SREGISTRY_CLIENT 
Client . quiet = quiet 
Client . _credential_cache = get_credential_cache ( ) 
if SREGISTRY_DATABASE is not None : 
~~~ from sregistry . database import ( 
init_db , add , cp , get , mv , rm , rmi , 
images , inspect , 
rename , 
get_container , 
get_collection , 
get_or_create_collection 
Client . _init_db = init_db 
Client . add = add 
Client . cp = cp 
Client . get = get 
Client . inspect = inspect 
Client . mv = mv 
Client . rename = rename 
Client . rm = rm 
Client . rmi = rmi 
Client . images = images 
Client . get_or_create_collection = get_or_create_collection 
Client . get_container = get_container 
Client . get_collection = get_collection 
~~~ from sregistry . database import ( add , init_db ) 
~~ cli = Client ( ) 
if hasattr ( Client , '_init_db' ) : 
~~~ cli . _init_db ( SREGISTRY_DATABASE ) 
~~ return cli 
~~ def pull ( self , images , file_name = None , save = True , force = False , ** kwargs ) : 
~~~ q = parse_image_name ( remove_uri ( image ) , lowercase = False ) 
url = "%s/container/%s/%s:%s" % ( self . base , q [ 'collection' ] , 
manifest [ 'selfLink' ] = url 
if "version" in manifest : 
~~~ q = parse_image_name ( '%s@%s' % ( q [ 'uri' ] , manifest [ 'version' ] ) ) 
~~~ file_name = self . _get_storage_name ( q ) 
~~ file_name = os . path . abspath ( file_name ) 
if os . path . exists ( file_name ) and force is False : 
~~ show_bar = not bool ( self . quiet ) 
image_file = self . download ( url = manifest [ 'image' ] , 
file_name = os . path . basename ( file_name ) , 
show_progress = show_bar ) 
~~~ image_uri = "%s:%s@%s" % ( manifest [ 'name' ] , manifest [ 'tag' ] , manifest [ 'version' ] ) 
image_name = file_name , 
~~ file_name = None 
~~ def ipython ( args ) : 
client = get_client ( args . endpoint ) 
client . announce ( args . command ) 
from IPython import embed 
embed ( ) 
~~ def update_token ( self , response ) : 
not_asking_auth = "Www-Authenticate" not in response . headers 
if response . status_code != 401 or not_asking_auth : 
~~ challenge = response . headers [ "Www-Authenticate" ] 
regexp = \ 
match = re . match ( regexp , challenge ) 
~~ realm = match . group ( 1 ) 
service = match . group ( 2 ) 
scope = match . group ( 3 ) . split ( ',' ) [ 0 ] 
token_url = realm + '?service=' + service + '&expires_in=900&scope=' + scope 
response = self . _get ( token_url ) 
~~~ token = response [ "token" ] 
self . headers . update ( token ) 
~~ ~~ def get_manifests ( self , repo_name , digest = None ) : 
if not hasattr ( self , 'manifests' ) : 
~~~ self . manifests = { } 
~~ schemaVersions = [ 'v1' , 'v2' , 'config' ] 
for schemaVersion in schemaVersions : 
~~~ manifest = self . _get_manifest ( repo_name , digest , schemaVersion ) 
if manifest is not None : 
~~~ if schemaVersion == "v2" and "config" in manifest : 
url = self . _get_layerLink ( repo_name , manifest [ 'config' ] [ 'digest' ] ) 
headers = { 'Accept' : manifest [ 'config' ] [ 'mediaType' ] } 
self . manifests [ 'config' ] = self . _get ( url , headers = headers ) 
~~ self . manifests [ schemaVersion ] = manifest 
~~ ~~ return self . manifests 
~~ def get_manifest_selfLink ( self , repo_name , digest = None ) : 
url = "%s/%s/manifests" % ( self . base , repo_name ) 
if digest is None : 
~~~ digest = 'latest' 
~~ return "%s/%s" % ( url , digest ) 
~~ def get_manifest ( self , repo_name , digest = None , version = "v1" ) : 
accepts = { 'config' : "application/vnd.docker.container.image.v1+json" , 
'v1' : "application/vnd.docker.distribution.manifest.v1+json" , 
'v2' : "application/vnd.docker.distribution.manifest.v2+json" } 
url = self . _get_manifest_selfLink ( repo_name , digest ) 
headers = { 'Accept' : accepts [ version ] } 
~~~ manifest = self . _get ( url , headers = headers , quiet = True ) 
~~~ manifest = None 
~~ return manifest 
~~ def download_layers ( self , repo_name , digest = None , destination = None ) : 
from sregistry . main . workers import ( Workers , download_task ) 
~~~ self . _get_manifests ( repo_name , digest ) 
~~ digests = self . _get_digests ( ) 
destination = self . _get_download_cache ( destination ) 
workers = Workers ( ) 
tasks = [ ] 
layers = [ ] 
for digest in digests : 
~~~ targz = "%s/%s.tar.gz" % ( destination , digest ) 
if not os . path . exists ( targz ) : 
~~~ url = "%s/%s/blobs/%s" % ( self . base , repo_name , digest ) 
tasks . append ( ( url , self . headers , targz ) ) 
~~ layers . append ( targz ) 
~~ if len ( tasks ) > 0 : 
~~~ download_layers = workers . run ( func = download_task , 
tasks = tasks ) 
~~ metadata = self . _create_metadata_tar ( destination ) 
if metadata is not None : 
~~~ layers . append ( metadata ) 
~~ def get_download_cache ( self , destination , subfolder = 'docker' ) : 
if destination is None : 
~~~ destination = self . _get_setting ( 'SINGULARITY_CACHEDIR' , 
SINGULARITY_CACHE ) 
destination = get_tmpdir ( destination ) 
~~ if not destination . endswith ( subfolder ) : 
~~~ destination = "%s/%s" % ( destination , subfolder ) 
~~ mkdir_p ( destination ) 
return destination 
~~ def get_digests ( self ) : 
~~ digests = [ ] 
reverseLayers = False 
schemaVersions = list ( self . manifests . keys ( ) ) 
schemaVersions . reverse ( ) 
~~~ manifest = self . manifests [ schemaVersion ] 
if manifest [ 'schemaVersion' ] == 1 : 
~~~ reverseLayers = True 
~~ layer_key = 'layers' 
digest_key = 'digest' 
if 'layers' in manifest : 
~~ elif 'fsLayers' in manifest : 
~~~ layer_key = 'fsLayers' 
digest_key = 'blobSum' 
~~ ~~ for layer in manifest [ layer_key ] : 
~~~ if digest_key in layer : 
digests . append ( layer [ digest_key ] ) 
~~ ~~ if reverseLayers is True : 
bot . debug ( message ) 
digests . reverse ( ) 
~~ return digests 
~~ def get_layer ( self , image_id , repo_name , download_folder = None ) : 
url = self . _get_layerLink ( repo_name , image_id ) 
download_folder = get_tmpdir ( download_folder ) 
download_folder = "%s/%s.tar.gz" % ( download_folder , image_id ) 
file_name = "%s.%s" % ( download_folder , 
tar_download = self . download ( url , file_name ) 
~~~ shutil . move ( tar_download , download_folder ) 
~~ return download_folder 
~~ def get_size ( self , add_padding = True , round_up = True , return_mb = True ) : 
for schemaVersion , manifest in self . manifests . items ( ) : 
~~~ if "layers" in manifest : 
~~~ size = 0 
for layer in manifest [ "layers" ] : 
~~~ if "size" in layer : 
~~~ size += layer [ 'size' ] 
~~ ~~ if add_padding is True : 
~~~ size = size * 5 
~~ if return_mb is True : 
~~ if round_up is True : 
~~~ size = math . ceil ( size ) 
~~ size = int ( size ) 
~~ return size 
~~ ~~ def get_config ( self , key = "Entrypoint" , delim = None ) : 
~~ cmd = None 
for version in [ 'config' , 'v1' ] : 
~~~ if cmd is None and 'config' in self . manifests : 
~~~ manifest = self . manifests [ 'config' ] 
if "config" in manifest : 
~~~ if key in manifest [ 'config' ] : 
~~~ cmd = manifest [ 'config' ] [ key ] 
~~ ~~ if cmd is None and "history" in manifest : 
~~~ for entry in manifest [ 'history' ] : 
~~~ if 'v1Compatibility' in entry : 
~~~ entry = json . loads ( entry [ 'v1Compatibility' ] ) 
if "config" in entry : 
~~~ if key in entry [ "config" ] : 
~~~ cmd = entry [ "config" ] [ key ] 
~~ ~~ ~~ ~~ ~~ ~~ ~~ if isinstance ( cmd , list ) : 
~~~ if delim is not None : 
~~~ cmd = delim . join ( cmd ) 
return cmd 
~~ def get_environment_tar ( self ) : 
from sregistry . utils import ( which , run_command ) 
res = which ( 'singularity' ) [ 'message' ] 
libexec = res . replace ( '/bin/singularity' , '' ) 
envtar = '%s/libexec/singularity/bootstrap-scripts/environment.tar' % libexec 
if os . path . exists ( envtar ) : 
~~~ return envtar 
~~~ res = which ( 'dpkg-architecture' ) [ 'message' ] 
if res is not None : 
~~~ cmd = [ 'dpkg-architecture' , '-qDEB_HOST_MULTIARCH' ] 
triplet = run_command ( cmd ) [ 'message' ] . strip ( '\\n' ) 
envtar = '/usr/lib/%s/singularity/bootstrap-scripts/environment.tar' % triplet 
~~ ~~ ~~ except : 
~~ return "%s/environment.tar" % os . path . abspath ( os . path . dirname ( __file__ ) ) 
~~ def create_metadata_tar ( self , destination = None , metadata_folder = ".singularity.d" ) : 
tar_file = None 
environ = self . _extract_env ( ) 
if environ not in [ None , "" ] : 
template = get_template ( 'tarinfo' ) 
template [ 'name' ] = './%s/env/10-docker.sh' % ( metadata_folder ) 
template [ 'content' ] = environ 
files . append ( template ) 
~~ labels = self . _extract_labels ( ) 
if labels is not None : 
~~~ labels = print_json ( labels ) 
template [ 'name' ] = "./%s/labels.json" % metadata_folder 
template [ 'content' ] = labels 
~~ runscript = self . _extract_runscript ( ) 
if runscript is not None : 
template [ 'name' ] = "./%s/runscript" % metadata_folder 
template [ 'content' ] = runscript 
~~ if len ( files ) > 0 : 
~~~ dest = self . _get_download_cache ( destination , subfolder = 'metadata' ) 
tar_file = create_tar ( files , dest ) 
~~ return tar_file 
~~ def extract_env ( self ) : 
environ = self . _get_config ( 'Env' ) 
if environ is not None : 
~~~ if not isinstance ( environ , list ) : 
~~~ environ = [ environ ] 
~~ lines = [ ] 
for line in environ : 
~~~ line = re . findall ( "(?P<var_name>.+?)=(?P<var_value>.+)" , line ) 
line = [ \ % ( x [ 0 ] , x [ 1 ] ) for x in line ] 
lines = lines + line 
~~ environ = "\\n" . join ( lines ) 
~~ return environ 
~~ def extract_runscript ( self ) : 
use_cmd = self . _get_setting ( 'SREGISTRY_DOCKER_CMD' ) 
commands = [ "Entrypoint" , "Cmd" ] 
if use_cmd is not None : 
~~~ commands . reverse ( ) 
~~ for command in commands : 
~~~ cmd = self . _get_config ( command ) 
if cmd is not None : 
~~ ~~ if cmd is not None : 
% command . upper ( ) ) 
bot . debug ( cmd ) 
if not isinstance ( cmd , list ) : 
~~~ cmd = [ cmd ] 
cmd = \ % cmd 
cmd = "#!/bin/sh\\n\\n%s\\n" % cmd 
~~ def _update_base ( self ) : 
self . base = self . _get_and_update_setting ( 'SREGISTRY_GITLAB_BASE' , 
"https://gitlab.com/" ) 
self . api_base = "%s/api/v4" % self . base . strip ( '/' ) 
self . artifacts = self . _get_and_update_setting ( 'SREGISTRY_GITLAB_FOLDER' , 
'build' ) 
self . job = self . _get_and_update_setting ( 'SREGISTRY_GITLAB_JOB' , 'build' ) 
self . token = self . _required_get_and_update ( 'SREGISTRY_GITLAB_TOKEN' ) 
self . headers [ "Private-Token" ] = self . token 
~~ def _get_metadata ( self ) : 
metadata = { 'SREGISTRY_GITLAB_FOLDER' : self . artifacts , 
'api_base' : self . api_base , 
'SREGISTRY_GITLAB_BASE' : self . base , 
'SREGISTRY_GITLAB_JOB' : self . job } 
return metadata 
~~ def _parse_image_name ( self , image , retry = True ) : 
~~~ job_id , collection , job_name = image . split ( ',' ) 
~~~ if retry : 
~~~ return self . _parse_image_name ( "%s,%s" % ( image , self . job ) , 
retry = False ) 
~~ return job_id , collection , job_name 
~~ def get_settings ( self , client_name = None ) : 
if client_name is not None and client_name in settings : 
~~~ return settings [ client_name ] 
~~ return settings 
~~ def get_setting ( self , name , default = None ) : 
setting = os . environ . get ( name ) 
if setting is None : 
~~~ secrets = read_client_secrets ( ) 
if self . client_name in secrets : 
~~~ secrets = secrets [ self . client_name ] 
if name in secrets : 
~~~ setting = secrets [ name ] 
~~ ~~ ~~ if setting is None and default is not None : 
~~~ setting = default 
~~ return setting 
~~ def get_and_update_setting ( self , name , default = None ) : 
setting = self . _get_setting ( name ) 
if setting is None and default is not None : 
~~ if setting is not None : 
~~~ updates = { name : setting } 
update_client_secrets ( backend = self . client_name , 
updates = updates ) 
~~ def required_get_and_update ( self , name , default = None ) : 
setting = self . _get_and_update_setting ( name , default = None ) 
if setting in [ None , "" ] : 
~~ def update_setting ( self , name , value ) : 
if value is not None : 
~~~ updates = { name : value } 
~~ ~~ def get_storage_name ( self , names , remove_dir = False ) : 
storage_folder = os . path . dirname ( names [ 'storage' ] ) 
if not hasattr ( self , 'storage' ) : 
~~~ return os . path . basename ( names [ 'storage' ] ) 
~~ storage_folder = "%s/%s" % ( self . storage , storage_folder ) 
mkdir_p ( storage_folder ) 
file_name = names [ 'storage' ] . replace ( '/' , '-' ) 
storage_path = "%s/%s" % ( self . storage , file_name ) 
if remove_dir is True : 
~~~ return file_name 
~~ return storage_path 
~~ def authorize ( self , names , payload = None , request_type = "push" ) : 
~~~ timestamp = generate_timestamp ( ) 
credential = generate_credential ( self . secrets [ 'registry' ] [ 'username' ] ) 
credential = "%s/%s/%s" % ( request_type , credential , timestamp ) 
if payload is None : 
~~~ payload = "%s|%s|%s|%s|%s|" % ( request_type , 
names [ 'collection' ] , 
timestamp , 
names [ 'image' ] , 
names [ 'tag' ] ) 
~~ signature = generate_signature ( payload , self . secrets [ 'registry' ] [ 'token' ] ) 
~~ ~~ ~~ def push ( self , path , name , tag = None ) : 
collection = self . _get_or_create_collection ( names [ 'collection' ] ) 
image_name = os . path . basename ( names [ 'storage' ] ) 
~~~ self . conn . put_object ( names [ 'collection' ] , image_name , 
contents = F . read ( ) , 
content_type = 'application/octet-stream' ) 
~~ bot . show_progress ( iteration = file_size , 
~~ def build ( self , repo , 
config = None , 
commit = None , 
tag = "latest" , 
recipe = "Singularity" , 
preview = False ) : 
if not self . _healthy ( repo ) : 
~~~ sys . exit ( 1 ) 
~~ config = self . _load_build_config ( config ) 
~~~ name = '/' . join ( repo . split ( '/' ) [ - 2 : ] ) 
~~ names = parse_image_name ( remove_uri ( name ) ) 
names [ 'tag' ] = tag or names [ 'tag' ] 
if names [ 'tag' ] == "latest" and recipe != "Singularity" : 
~~~ tag = get_recipe_tag ( recipe ) 
commit = commit or names [ 'version' ] 
config = self . _setup_build ( name = names [ 'url' ] , recipe = recipe , 
repo = repo , config = config , 
tag = tag , commit = commit ) 
if preview is True : 
~~~ return config 
~~ return self . _run_build ( config ) 
~~ def list_builders ( self , project = None , zone = 'us-west1-a' ) : 
builders = [ ] 
instances = self . _get_instances ( project , zone ) 
for instance in instances [ 'items' ] : 
~~~ builders . append ( [ instance [ 'name' ] , instance [ 'status' ] ] ) 
bot . table ( builders ) 
bot . newline ( ) 
~~ def list_templates ( self , name = None ) : 
configs = self . _get_templates ( ) 
~~~ matches = self . _load_templates ( name ) 
for match in matches : 
~~~ print ( json . dumps ( match , indent = 4 , sort_keys = True ) ) 
~~~ for config in configs [ 'data' ] : 
~~~ rows . append ( [ config [ 'name' ] ] ) 
~~ ~~ def get_templates ( self ) : 
base = 'https://singularityhub.github.io/builders' 
base = self . _get_and_update_setting ( 'SREGISTRY_BUILDER_REPO' , base ) 
base = "%s/configs.json" % base 
return self . _get ( base ) 
~~ def load_templates ( self , name ) : 
templates = [ ] 
matches = [ x for x in configs [ 'data' ] if name in x [ 'name' ] ] 
if len ( matches ) > 0 : 
~~~ response = self . _get ( match [ 'id' ] ) 
templates . append ( response ) 
~~ return templates 
~~ def get_instances ( self , project = None , zone = 'us-west1-a' ) : 
project = self . _get_project ( project ) 
zone = self . _get_zone ( zone ) 
return self . _compute_service . instances ( ) . list ( project = project , 
zone = zone ) . execute ( ) 
~~ def get_ipaddress ( self , name , retries = 3 , delay = 3 ) : 
for rr in range ( retries ) : 
~~~ instances = self . _get_instances ( ) 
~~~ if instance [ 'name' ] == name : 
~~~ for network in instance [ 'networkInterfaces' ] : 
~~~ if network [ 'name' ] == 'nic0' : 
~~~ for subnet in network [ 'accessConfigs' ] : 
~~~ if 'natIP' in subnet : 
~~~ return subnet [ 'natIP' ] 
~~ ~~ ~~ ~~ ~~ ~~ ~~ sleep ( delay ) 
~~ def load_build_config ( self , config = None ) : 
~~ if config is None : 
~~~ config = self . _get_and_update_setting ( 'SREGISTRY_COMPUTE_CONFIG' , 
'google/compute/ubuntu/securebuild-2.4.3' ) 
~~ elif os . path . exists ( config ) : 
~~~ return read_json ( config ) 
~~ configs = self . _load_templates ( config ) 
if configs is None : 
config = configs [ 0 ] 
~~ def setup_build ( self , name , repo , config , tag = None , commit = None , 
recipe = "Singularity" , startup_script = None ) : 
manager = self . _get_and_update_setting ( 'SREGISTRY_BUILDER_MANAGER' , 'apt' ) 
startup_script = get_build_template ( startup_script , manager ) 
config = self . _load_build_config ( config ) 
if not config : 
~~ self . _client_tagged ( config [ 'data' ] [ 'tags' ] ) 
defaults = config [ 'data' ] [ 'metadata' ] 
selfLink = config [ 'links' ] [ 'self' ] 
builder_repo = config [ 'data' ] [ 'repo' ] 
builder_bundle = config [ 'data' ] [ 'path' ] 
builder_id = config [ 'data' ] [ 'id' ] 
config = config [ 'data' ] [ 'config' ] 
image_project = defaults . get ( 'GOOGLE_COMPUTE_PROJECT' , 'debian-cloud' ) 
image_family = defaults . get ( 'GOOGLE_COMPUTE_IMAGE_FAMILY' , 'debian-8' ) 
robot_name = RobotNamer ( ) . generate ( ) 
project = self . _get_project ( ) 
zone = self . _get_zone ( ) 
machine_type = defaults . get ( 'SREGISTRY_BUILDER_machine_type' , 'n1-standard-1' ) 
machine_type = "zones/%s/machineTypes/%s" % ( zone , machine_type ) 
disk_size = defaults . get ( 'SREGISTRY_BUILDER_disk_size' , '100' ) 
image_response = self . _compute_service . images ( ) . getFromFamily ( 
project = image_project , 
family = image_family ) . execute ( ) 
source_disk_image = image_response [ 'selfLink' ] 
storage_bucket = self . _bucket_name 
config [ 'name' ] = robot_name 
config [ 'description' ] = instance_name 
config [ 'machineType' ] = machine_type 
config [ 'disks' ] . append ( { 
"autoDelete" : True , 
"boot" : True , 
"initializeParams" : { 'sourceImage' : source_disk_image , 
'diskSizeGb' : disk_size } 
metadata = { 'items' : 
[ { 'key' : 'startup-script' , 
'value' : startup_script } , 
{ 'key' : 'SREGISTRY_BUILDER_STORAGE_BUCKET' , 
'value' : self . _bucket_name } ] } 
defaults = setconfig ( defaults , 'SREGISTRY_USER_REPO' , repo ) 
defaults = setconfig ( defaults , 'SREGISTRY_CONTAINER_NAME' , name ) 
defaults = setconfig ( defaults , 'SREGISTRY_USER_COMMIT' , commit ) 
defaults = setconfig ( defaults , 'SREGISTRY_USER_BRANCH' , "master" ) 
defaults = setconfig ( defaults , 'SREGISTRY_USER_TAG' , tag ) 
defaults = setconfig ( defaults , 'SREGISTRY_BUILDER_REPO' , builder_repo ) 
defaults = setconfig ( defaults , 'SREGISTRY_BUILDER_COMMIT' ) 
defaults = setconfig ( defaults , 'SREGISTRY_BUILDER_RUNSCRIPT' , "run.sh" ) 
defaults = setconfig ( defaults , 'SREGISTRY_BUILDER_BRANCH' , "master" ) 
defaults = setconfig ( defaults , 'SREGISTRY_BUILDER_ID' , builder_id ) 
defaults = setconfig ( defaults , 'SREGISTRY_BUILDER_BUNDLE' , builder_bundle ) 
defaults = setconfig ( defaults , 'SREGISTRY_BUILDER_DEBUGHOURS' , "4" ) 
defaults = setconfig ( defaults , 'SREGISTRY_BUILDER_KILLHOURS' , "10" ) 
defaults = setconfig ( defaults , 'SINGULARITY_RECIPE' , recipe ) 
defaults = setconfig ( defaults , 'SINGULARITY_BRANCH' ) 
defaults = setconfig ( defaults , 'SINGULARITY_COMMIT' ) 
defaults = setconfig ( defaults , 'SINGULARITY_REPO' , 
'https://github.com/cclerget/singularity.git' ) 
seen = [ 'SREGISTRY_BUILDER_STORAGE_BUCKET' , 'startup-script' ] 
for key , value in defaults . items ( ) : 
~~~ if value not in seen : 
~~~ entry = { "key" : key , 'value' : value } 
metadata [ 'items' ] . append ( entry ) 
seen . append ( key ) 
~~ ~~ config [ 'metadata' ] = metadata 
~~ def setconfig ( lookup , key , value = None ) : 
lookup [ key ] = value or lookup . get ( key ) 
return lookup 
~~ def run_build ( self , config ) : 
bot . custom ( prefix = 'INSTANCE' , message = config [ 'name' ] , color = "CYAN" ) 
bot . info ( config [ 'description' ] ) 
response = self . _compute_service . instances ( ) . insert ( project = project , 
zone = zone , 
body = config ) . execute ( ) 
ipaddress = self . _get_ipaddress ( config [ 'name' ] ) 
results = set ( ) 
~~~ for result in self . conn . get_container ( container [ 'name' ] ) [ 1 ] : 
~~~ results . add ( '%s/%s' % ( container [ 'name' ] , result [ 'name' ] ) ) 
~~ bot . info ( "Collections" ) 
bot . table ( [ [ x ] for x in list ( results ) ] ) 
return list ( results ) 
~~ def container_query ( self , query ) : 
query = remove_uri ( query ) 
~~~ if query in collection [ 'name' ] : 
~~ ~~ ~~ if len ( results ) == 0 : 
bot . table ( [ list ( results ) ] ) 
~~~ if image . metadata is not None : 
~~~ if "type" in image . metadata : 
~~~ if image . metadata [ 'type' ] == "container" : 
~~ ~~ ~~ ~~ if len ( results ) == 0 : 
~~~ size = round ( i . size / ( 1024 * 1024.0 ) ) 
rows . append ( [ size , i . metadata [ 'name' ] ] ) 
~~~ for key , val in result . metadata . items ( ) : 
~~~ if query in val and result not in matches : 
~~ ~~ ~~ if not quiet : 
~~~ size = round ( image . size / ( 1024 * 1024.0 ) ) 
bot . custom ( prefix = image . name , color = "CYAN" ) 
bot . custom ( prefix = 'updated:' , message = image . updated ) 
if "public_url" in image . metadata : 
~~~ public_url = image . metadata [ 'public_url' ] 
~~ bot . newline ( ) 
for entry in self . dbx . files_list_folder ( '' ) . entries : 
~~~ for item in self . dbx . files_list_folder ( entry . path_lower ) . entries : 
~~~ name = item . name . replace ( '.simg' , '' ) 
results . append ( [ "%s/%s" % ( entry . name , name ) ] ) 
~~ def main ( args , parser , subparser ) : 
cli = get_client ( quiet = args . quiet ) 
for query in args . query : 
~~~ if query in [ '' , '*' ] : 
~~~ query = None 
~~ cli . ls ( query = query ) 
matches = self . _container_query ( q [ 'tag_uri' ] , quiet = True ) 
~~~ file_name = q [ 'storage_version' ] . replace ( '/' , '-' ) 
image_file = self . download ( url = image . media_link , 
~~~ image_uri = q [ 'tag_uri' ] 
metadata = image . metadata 
if "public_url" in metadata : 
~~~ metadata [ 'url' ] = metadata [ 'public_url' ] 
~~ metadata [ 'selfLink' ] = image . self_link 
url = image . media_link ) 
images = args . image 
~~ for image in images : 
~~~ print ( image ) 
cli = get_client ( image , quiet = args . quiet ) 
cli . announce ( args . command ) 
cli . share ( image , share_to = args . share_to ) 
~~ ~~ def init_db ( self , db_path ) : 
self . database = 'sqlite:///%s' % db_path 
self . storage = SREGISTRY_STORAGE 
self . engine = create_engine ( self . database , convert_unicode = True ) 
self . session = scoped_session ( sessionmaker ( autocommit = False , 
autoflush = False , 
bind = self . engine ) ) 
Base . query = self . session . query_property ( ) 
Base . metadata . create_all ( bind = self . engine ) 
self . Base = Base 
~~ def get_uri ( self ) : 
uri = "%s/%s:%s" % ( self . collection . name , self . name , self . tag ) 
if self . version not in [ None , '' ] : 
~~~ uri = "%s@%s" % ( uri , self . version ) 
~~ def get_build_template ( ) : 
base = get_installdir ( ) 
name = "%s/main/templates/build/singularity-cloudbuild.json" % base 
if os . path . exists ( name ) : 
return read_json ( name ) 
~~~ if args . endpoint is None : 
return self . _list_endpoints ( ) 
~~~ return self . _list_endpoint ( args . endpoint ) 
~~ ~~ if args . endpoint is None : 
return self . _list_endpoints ( query ) 
~~ return self . _list_endpoint ( endpoint = args . endpoint , 
query = query ) 
~~ def list_endpoints ( self , query = None ) : 
endpoints = self . _get_endpoints ( query ) 
bot . custom ( prefix = "Globus" , message = "Endpoints" , color = "CYAN" ) 
for kind , eps in endpoints . items ( ) : 
~~~ for epid , epmeta in eps . items ( ) : 
~~~ rows . append ( [ epid , '[%s]' % kind , epmeta [ 'name' ] ] ) 
~~ ~~ bot . table ( rows ) 
~~ def list_endpoint ( self , endpoint , query = None ) : 
~~ endpoint , path = self . _parse_endpoint_name ( endpoint ) 
~~~ result = self . transfer_client . operation_ls ( endpoint , path = path ) 
~~ except TransferAPIError as err : 
~~~ bot . custom ( prefix = 'ERROR' , message = err , color = 'RED' ) 
for filey in result : 
~~~ name = filey [ 'name' ] 
if query is None or query in name : 
~~~ if name . endswith ( 'img' ) : 
~~~ name = bot . addColor ( 'PURPLE' , name ) 
~~ rows . append ( [ filey [ 'type' ] , 
filey [ 'permissions' ] , 
str ( filey [ 'size' ] ) , 
name ] ) 
~~~ rows = [ [ "type" , "[perm]" , "[size]" , "[name]" ] ] + rows 
~~ def share ( self , query , share_to = None ) : 
names = parse_image_name ( remove_uri ( query ) ) 
if self . exists ( dropbox_path ) is True : 
~~~ share = self . dbx . sharing_create_shared_link_with_settings ( dropbox_path ) 
~~ except ApiError as err : 
~~~ share = self . dbx . sharing_create_shared_link ( dropbox_path ) 
~~ bot . info ( share . url ) 
~~ return share . url 
~~ def _set_base ( self ) : 
base = self . _get_setting ( 'SREGISTRY_NVIDIA_BASE' ) 
version = self . _get_setting ( 'SREGISTRY_NVIDIA_VERSION' ) 
~~~ base = "nvcr.io" 
~~ if version is None : 
~~ nohttps = self . _get_setting ( 'SREGISTRY_NVIDIA_NOHTTPS' ) 
~~ self . base = "%s%s/%s" % ( nohttps , base . strip ( '/' ) , version ) 
token = self . _required_get_and_update ( 'SREGISTRY_NVIDIA_TOKEN' ) 
username = self . _get_and_update_setting ( 'SREGISTRY_NVIDIA_USERNAME' ) 
if username is None : 
~~~ username = "$oauthtoken" 
~~ if token is not None : 
~~~ auth = basic_auth_header ( username , token ) 
~~~ endpoint , remote = self . _parse_endpoint_name ( image ) 
source = self . transfer_client . get_endpoint ( endpoint ) 
name = os . path . basename ( remote ) 
q = parse_image_name ( name , default_collection = source [ 'name' ] ) 
endpoints = self . _get_endpoints ( ) 
if len ( endpoints [ 'my-endpoints' ] ) == 0 : 
~~ dest = None 
for eid , contender in endpoints [ 'my-endpoints' ] . items ( ) : 
~~~ if contender [ 'gcp_connected' ] is True : 
~~~ dest = contender 
~~ ~~ if dest is None : 
~~ base = self . _get_endpoint_path ( dest [ 'id' ] ) 
storage_folder = '%s/%s' % ( base , q [ 'collection' ] ) 
self . _create_endpoint_folder ( dest [ 'id' ] , storage_folder ) 
tdata = globus_sdk . TransferData ( self . transfer_client , 
source [ 'id' ] , 
dest [ 'id' ] , 
sync_level = "checksum" ) 
image = os . path . join ( base , q [ 'storage' ] ) 
tdata . add_item ( remote , image ) 
transfer_result = self . transfer_client . submit_transfer ( tdata ) 
bot . info ( transfer_result [ 'message' ] ) 
finished . append ( transfer_result ) 
~~ def get_credential_cache ( ) : 
from sregistry . defaults import ( CREDENTIAL_CACHE , SREGISTRY_CLIENT ) 
client_credential_cache = None 
if CREDENTIAL_CACHE is not None : 
~~~ env = 'SREGISTRY_DISABLE_CREDENTIAL_%s' % SREGISTRY_CLIENT . upper ( ) 
if os . environ . get ( env ) is not None : 
CREDENTIAL_CACHE = None 
~~ ~~ if CREDENTIAL_CACHE is not None : 
~~~ if not os . path . exists ( CREDENTIAL_CACHE ) : 
~~~ mkdir_p ( CREDENTIAL_CACHE ) 
~~ client_credential_cache = '%s/%s' % ( CREDENTIAL_CACHE , SREGISTRY_CLIENT ) 
~~ if client_credential_cache is not None : 
~~ return client_credential_cache 
~~ def update_client_secrets ( backend , updates , secrets = None , save = True ) : 
if secrets is None : 
~~ if backend not in secrets : 
~~~ secrets [ backend ] = { } 
~~ secrets [ backend ] . update ( updates ) 
~~~ secrets_file = get_secrets_file ( ) 
if secrets_file is not None : 
~~~ write_json ( secrets , secrets_file ) 
~~ ~~ return secrets 
~~ def read_client_secrets ( ) : 
client_secrets = _default_client_secrets ( ) 
secrets = get_secrets_file ( ) 
if secrets is not None : 
~~~ client_secrets = read_json ( secrets ) 
~~~ from sregistry . defaults import SREGISTRY_CLIENT_SECRETS 
write_json ( client_secrets , SREGISTRY_CLIENT_SECRETS ) 
~~ return client_secrets 
~~ def _init_client ( self ) : 
~~~ self . _bucket_name = 'sregistry-%s' % os . environ [ 'USER' ] 
~~ self . _get_bucket ( ) 
~~ def _get_services ( self , version = 'v1' ) : 
self . _bucket_service = storage . Client ( ) 
creds = GoogleCredentials . get_application_default ( ) 
self . _storage_service = discovery_build ( 'storage' , version , credentials = creds ) 
self . _compute_service = discovery_build ( 'compute' , version , credentials = creds ) 
~~ def _get_bucket ( self ) : 
~~~ self . _bucket = self . _bucket_service . get_bucket ( self . _bucket_name ) 
~~~ self . _bucket = self . _bucket_service . create_bucket ( self . _bucket_name ) 
~~ return self . _bucket 
~~ def delete_object ( service , bucket_name , object_name ) : 
~~~ operation = service . objects ( ) . delete ( bucket = bucket_name , 
object = object_name ) . execute ( ) 
~~ except HttpError as e : 
operation = e 
~~ return operation 
~~ def delete ( self , name ) : 
for file_object in files : 
~~~ if isinstance ( file_object , dict ) : 
~~~ if "kind" in file_object : 
~~~ if file_object [ 'kind' ] == "storage#object" : 
~~~ object_name = "/" . join ( file_object [ 'id' ] . split ( '/' ) [ : - 1 ] ) 
object_name = re . sub ( '%s/' % self . _bucket [ 'name' ] , '' , object_name , 1 ) 
delete_object ( service = self . _bucket_service , 
bucket_name = bucket [ 'name' ] , 
object_name = object_name ) 
~~ ~~ ~~ ~~ ~~ def destroy ( self , name ) : 
instances = self . _get_instances ( ) 
if 'items' in instances : 
~~~ for instance in instances [ 'items' ] : 
~~~ found = True 
~~ ~~ ~~ if found : 
return self . _compute_service . instances ( ) . delete ( project = project , 
instance = name ) . execute ( ) 
~~ ~~ def get_subparsers ( parser ) : 
actions = [ action for action in parser . _actions 
if isinstance ( action , argparse . _SubParsersAction ) ] 
subparsers = dict ( ) 
for action in actions : 
~~~ for choice , subparser in action . choices . items ( ) : 
~~~ subparsers [ choice ] = subparser 
~~ ~~ return subparsers 
parser = get_parser ( ) 
subparsers = get_subparsers ( parser ) 
def help ( return_code = 0 ) : 
version = sregistry . __version__ 
name = cli . client_name 
parser . print_help ( ) 
sys . exit ( return_code ) 
~~ if len ( sys . argv ) == 1 : 
~~~ help ( ) 
~~~ args = parser . parse_args ( ) 
~~~ sys . exit ( 0 ) 
~~ if args . debug is False : 
~~~ os . environ [ 'MESSAGELEVEL' ] = "DEBUG" 
~~ if args . command == "version" : 
~~~ print ( sregistry . __version__ ) 
~~ from sregistry . logger import bot 
if args . command == "add" : from . add import main 
elif args . command == "backend" : from . backend import main 
elif args . command == "build" : from . build import main 
elif args . command == "get" : from . get import main 
elif args . command == "delete" : from . delete import main 
elif args . command == "inspect" : from . inspect import main 
elif args . command == "images" : from . images import main 
elif args . command == "labels" : from . labels import main 
elif args . command == "mv" : from . mv import main 
elif args . command == "push" : from . push import main 
elif args . command == "pull" : from . pull import main 
elif args . command == "rename" : from . rename import main 
elif args . command == "rm" : from . rm import main 
elif args . command == "rmi" : from . rmi import main 
elif args . command == "search" : from . search import main 
elif args . command == "share" : from . share import main 
elif args . command == "shell" : from . shell import main 
return_code = 0 
~~~ main ( args = args , 
parser = parser , 
subparser = subparsers [ args . command ] ) 
~~~ return_code = 1 
~~ help ( return_code ) 
~~ def generate ( self , delim = '-' , length = 4 , chars = '0123456789' ) : 
descriptor = self . _select ( self . _descriptors ) 
noun = self . _select ( self . _nouns ) 
numbers = '' . join ( ( self . _select ( chars ) for _ in range ( length ) ) ) 
return delim . join ( [ descriptor , noun , numbers ] ) 
~~ def mkdir_p ( path ) : 
~~~ os . makedirs ( path ) 
~~~ if e . errno == errno . EEXIST and os . path . isdir ( path ) : 
~~ ~~ ~~ def get_tmpfile ( requested_tmpdir = None , prefix = "" ) : 
tmpdir = get_tmpdir ( requested_tmpdir ) 
if tmpdir is not None : 
~~~ prefix = os . path . join ( tmpdir , os . path . basename ( prefix ) ) 
~~ fd , tmp_file = tempfile . mkstemp ( prefix = prefix ) 
return tmp_file 
~~ def get_tmpdir ( requested_tmpdir = None , prefix = "" , create = True ) : 
from sregistry . defaults import SREGISTRY_TMPDIR 
tmpdir = requested_tmpdir or SREGISTRY_TMPDIR 
prefix = prefix or "sregistry-tmp" 
prefix = "%s.%s" % ( prefix , next ( tempfile . _get_candidate_names ( ) ) ) 
tmpdir = os . path . join ( tmpdir , prefix ) 
if not os . path . exists ( tmpdir ) and create is True : 
~~~ os . mkdir ( tmpdir ) 
~~ return tmpdir 
~~ def extract_tar ( archive , output_folder , handle_whiteout = False ) : 
from . terminal import run_command 
if handle_whiteout is True : 
~~~ return _extract_tar ( archive , output_folder ) 
~~ args = '-xf' 
if archive . endswith ( ".tar.gz" ) : 
~~~ args = '-xzf' 
~~ command = [ "tar" , args , archive , "-C" , output_folder , "--exclude=dev/*" ] 
if not bot . is_quiet ( ) : 
~~ return run_command ( command ) 
~~ def _extract_tar ( archive , output_folder ) : 
from . terminal import ( run_command , which ) 
result = which ( 'blob2oci' ) 
if result [ 'return_code' ] != 0 : 
~~ script = result [ 'message' ] 
command = [ 'exec' , script , '--layer' , archive , '--extract' , output_folder ] 
~~ def create_tar ( files , output_folder = None ) : 
if output_folder is None : 
~~~ output_folder = tempfile . mkdtemp ( ) 
~~ finished_tar = None 
additions = [ ] 
contents = [ ] 
for entity in files : 
~~~ info = tarfile . TarInfo ( name = entity [ 'name' ] ) 
info . mode = entity [ 'mode' ] 
info . mtime = int ( datetime . datetime . now ( ) . strftime ( '%s' ) ) 
info . uid = entity [ "uid" ] 
info . gid = entity [ "gid" ] 
info . uname = entity [ "uname" ] 
info . gname = entity [ "gname" ] 
filey = io . StringIO ( ) 
~~~ info . size = filey . write ( entity [ 'content' ] ) 
content = io . BytesIO ( entity [ 'content' ] . encode ( 'utf8' ) ) 
~~~ info . size = int ( filey . write ( entity [ 'content' ] . decode ( 'utf-8' ) ) ) 
if content is not None : 
~~~ addition = { 'content' : content , 
'info' : info } 
additions . append ( addition ) 
contents . append ( content ) 
~~ ~~ if len ( additions ) > 0 : 
~~~ hashy = get_content_hash ( contents ) 
finished_tar = "%s/sha256:%s.tar.gz" % ( output_folder , hashy ) 
if os . path . exists ( finished_tar ) : 
bot . debug ( msg ) 
~~ tar = tarfile . open ( finished_tar , "w:gz" ) 
for a in additions : 
~~~ tar . addfile ( a [ "info" ] , a [ "content" ] ) 
~~ tar . close ( ) 
~~ return finished_tar 
~~ def get_content_hash ( contents ) : 
hasher = hashlib . sha256 ( ) 
~~~ if isinstance ( content , io . BytesIO ) : 
~~~ content = content . getvalue ( ) 
~~ if not isinstance ( content , bytes ) : 
~~~ content = bytes ( content ) 
~~ hasher . update ( content ) 
~~ return hasher . hexdigest ( ) 
~~ def get_file_hash ( filename ) : 
with open ( filename , "rb" ) as f : 
~~~ for chunk in iter ( lambda : f . read ( 4096 ) , b"" ) : 
~~~ hasher . update ( chunk ) 
~~ ~~ return hasher . hexdigest ( ) 
~~ def write_json ( json_obj , filename , mode = "w" , print_pretty = True ) : 
with open ( filename , mode ) as filey : 
~~~ if print_pretty : 
~~~ filey . writelines ( print_json ( json_obj ) ) 
~~~ filey . writelines ( json . dumps ( json_obj ) ) 
~~ ~~ return filename 
~~ def read_file ( filename , mode = "r" , readlines = True ) : 
~~~ if readlines is True : 
~~~ content = filey . readlines ( ) 
~~~ content = filey . read ( ) 
~~ def read_json ( filename , mode = 'r' ) : 
~~~ data = json . load ( filey ) 
~~ def clean_up ( files ) : 
if not isinstance ( files , list ) : 
~~~ files = [ files ] 
~~ for f in files : 
~~~ if os . path . exists ( f ) : 
os . remove ( f ) 
~~ ~~ ~~ def pull ( self , images , file_name = None , save = True , force = False , ** kwargs ) : 
~~~ q = parse_image_name ( remove_uri ( image ) , 
default_collection = 'aws' ) 
image_file = self . _pull ( file_name = file_name , 
save = save , 
force = force , 
names = q , 
kwargs = kwargs ) 
image = os . path . basename ( path ) 
image_size = os . path . getsize ( path ) >> 20 
metadata = { 'sizemb' : "%s" % image_size , 
'client' : 'sregistry' } 
self . bucket . upload_file ( path , names [ 'storage_uri' ] , { "Metadata" : metadata } ) 
~~ def get_or_create_collection ( self , name ) : 
from sregistry . database . models import Collection 
collection = self . get_collection ( name ) 
if collection is None : 
~~~ collection = Collection ( name = name ) 
self . session . add ( collection ) 
~~ def get_collection ( self , name ) : 
return Collection . query . filter ( Collection . name == name ) . first ( ) 
~~ def get_container ( self , name , collection_id , tag = "latest" , version = None ) : 
from sregistry . database . models import Container 
~~~ container = Container . query . filter_by ( collection_id = collection_id , 
tag = tag ) . first ( ) 
tag = tag , 
version = version ) . first ( ) 
~~ def get ( self , name , quiet = False ) : 
from sregistry . database . models import Collection , Container 
names = parse_image_name ( remove_uri ( name ) ) 
collection = self . get_collection ( name = names [ 'collection' ] ) 
container = None 
if collection is not None : 
~~~ container = self . get_container ( collection_id = collection . id , 
name = names [ 'image' ] , 
tag = names [ 'tag' ] , 
version = names [ 'version' ] ) 
if container is not None and quiet is False : 
~~~ if container . image is not None : 
~~~ print ( container . image ) 
~~ elif container . url is not None : 
~~~ print ( container . url ) 
~~ ~~ ~~ return container 
~~ def images ( self , query = None ) : 
~~~ like = "%" + query + "%" 
containers = Container . query . filter ( or_ ( Container . name == query , 
Container . tag . like ( like ) , 
Container . uri . like ( like ) , 
Container . name . like ( like ) ) ) . all ( ) 
~~~ containers = Container . query . all ( ) 
~~ if len ( containers ) > 0 : 
bot . custom ( prefix = 'Containers:' , message = message , color = "RED" ) 
for c in containers : 
~~~ uri = c . get_uri ( ) 
~~ def inspect ( self , name ) : 
print ( name ) 
container = self . get ( name ) 
if container is not None : 
~~~ collection = container . collection . name 
fields = container . __dict__ . copy ( ) 
fields [ 'collection' ] = collection 
fields [ 'metrics' ] = json . loads ( fields [ 'metrics' ] ) 
del fields [ '_sa_instance_state' ] 
fields [ 'created_at' ] = str ( fields [ 'created_at' ] ) 
print ( json . dumps ( fields , indent = 4 , sort_keys = True ) ) 
~~ ~~ def rename ( self , image_name , path ) : 
container = self . get ( image_name , quiet = True ) 
~~~ dirname = os . path . dirname ( container . image ) 
names = parse_image_name ( remove_uri ( path ) ) 
storage = os . path . join ( self . storage , 
os . path . dirname ( names [ 'storage' ] ) ) 
if not os . path . exists ( storage ) : 
~~~ os . mkdir ( storage ) 
~~ fullpath = os . path . abspath ( os . path . join ( dirname , names [ 'storage' ] ) ) 
container = self . cp ( move_to = fullpath , 
container = container , 
command = "rename" ) 
~~~ container . uri = names [ 'uri' ] 
~~ def mv ( self , image_name , path ) : 
~~~ name = container . uri or container . get_uri ( ) 
image = container . image or '' 
if os . path . exists ( image ) : 
~~~ filename = os . path . basename ( image ) 
filedir = os . path . abspath ( path ) 
if not os . path . isdir ( path ) : 
~~~ filename = os . path . basename ( path ) 
filedir = os . path . dirname ( path ) 
~~ if filedir == '' : 
~~~ filedir = os . getcwd ( ) 
~~ fullpath = os . path . abspath ( os . path . join ( filedir , filename ) ) 
return self . cp ( move_to = fullpath , 
command = "move" ) 
~~ def cp ( self , move_to , image_name = None , container = None , command = "copy" ) : 
if container is None and image_name is None : 
~~ if container is None : 
~~~ container = self . get ( image_name , quiet = True ) 
~~ image = container . image or '' 
~~~ filedir = os . path . dirname ( move_to ) 
if move_to == image : 
~~ if not os . path . exists ( filedir ) : 
~~ if not os . access ( filedir , os . W_OK ) : 
~~ original = os . path . basename ( image ) 
~~~ shutil . move ( image , move_to ) 
container . image = move_to 
~~ def rmi ( self , image_name ) : 
container = self . rm ( image_name , delete = True ) 
~~ ~~ def rm ( self , image_name , delete = False ) : 
container = self . get ( image_name ) 
image = container . image 
self . session . delete ( container ) 
if image is not None : 
~~~ if os . path . exists ( image ) and delete is True : 
~~~ os . remove ( container . image ) 
~~ ~~ def add ( self , image_path = None , 
from sregistry . database . models import ( 
Container , 
Collection 
~~~ if not os . path . exists ( image_path ) and save is True : 
metadata = self . get_metadata ( image_path , names = names ) 
collection = self . get_or_create_collection ( names [ 'collection' ] ) 
version = names . get ( 'version' ) 
if version == None : 
~~~ if image_path != None : 
~~~ version = get_image_hash ( image_path ) 
~~ names = parse_image_name ( remove_uri ( image_uri ) , version = version ) 
~~ if save is True and image_path is not None : 
~~~ if image_name is None : 
~~~ image_name = self . _get_storage_name ( names ) 
~~ if copy is True : 
~~~ copyfile ( image_path , image_name ) 
~~~ shutil . move ( image_path , image_name ) 
~~ image_path = image_name 
~~ if url is None and "url" in metadata : 
~~~ url = metadata [ 'url' ] 
~~ container = self . get_container ( name = names [ 'image' ] , 
collection_id = collection . id , 
version = version ) 
if container is None : 
~~~ action = "new" 
container = Container ( metrics = json . dumps ( metadata ) , 
image = image_path , 
client = self . client_name , 
version = version , 
uri = names [ 'uri' ] , 
collection_id = collection . id ) 
self . session . add ( container ) 
collection . containers . append ( container ) 
~~~ action = "update" 
metrics = json . loads ( container . metrics ) 
metrics . update ( metadata ) 
container . url = url 
container . client = self . client_name 
~~~ container . image = image_path 
~~ container . metrics = json . dumps ( metrics ) 
names = parse_image_name ( remove_uri ( name ) , tag = tag ) 
if names [ 'registry' ] == None : 
~~~ names [ 'registry' ] = self . base 
~~ names = self . _add_https ( names ) 
url = '%s/push/' % names [ 'registry' ] 
auth_url = '%s/upload/chunked_upload' % names [ 'registry' ] 
SREGISTRY_EVENT = self . authorize ( request_type = "push" , 
names = names ) 
fields = { 'collection' : names [ 'collection' ] , 
'name' : names [ 'image' ] , 
'tag' : names [ 'tag' ] } 
r = requests . post ( auth_url , json = fields , headers = headers ) 
message = self . _read_response ( r ) 
if r . status_code != 200 : 
~~ url = '%s/upload' % names [ 'registry' ] . replace ( '/api' , '' ) 
cid = r . json ( ) [ 'cid' ] 
upload_to = os . path . basename ( names [ 'storage' ] ) 
SREGISTRY_EVENT = self . authorize ( request_type = "upload" , 
encoder = MultipartEncoder ( fields = { 'SREGISTRY_EVENT' : SREGISTRY_EVENT , 
'collection' : str ( cid ) , 
'tag' : names [ 'tag' ] , 
'file1' : ( upload_to , open ( path , 'rb' ) , 'text/plain' ) } ) 
progress_callback = create_callback ( encoder , self . quiet ) 
monitor = MultipartEncoderMonitor ( encoder , progress_callback ) 
headers = { 'Content-Type' : monitor . content_type , 
'Authorization' : SREGISTRY_EVENT } 
~~~ r = requests . post ( url , data = monitor , headers = headers ) 
message = r . json ( ) [ 'message' ] 
~~ except requests . HTTPError as e : 
~~~ print ( e ) 
~~ ~~ def parse_header ( recipe , header = "from" , remove_header = True ) : 
parsed_header = None 
fromline = [ x for x in recipe . split ( '\\n' ) if "%s:" % header in x . lower ( ) ] 
if len ( fromline ) == 0 : 
~~ if len ( fromline ) > 0 : 
~~~ fromline = fromline [ 0 ] 
parsed_header = fromline . strip ( ) 
~~ if remove_header is True : 
~~~ parsed_header = fromline . split ( ':' , 1 ) [ - 1 ] . strip ( ) 
~~ return parsed_header 
~~ def find_recipes ( folders , pattern = None , base = None ) : 
if folders is None : 
~~~ folders = os . getcwd ( ) 
~~ if not isinstance ( folders , list ) : 
~~~ folders = [ folders ] 
~~ manifest = dict ( ) 
for base_folder in folders : 
~~~ custom_pattern = None 
~~~ manifest = find_single_recipe ( filename = base_folder , 
pattern = pattern , 
manifest = manifest ) 
~~ elif not os . path . isdir ( base_folder ) : 
~~~ custom_pattern = base_folder . split ( '/' ) [ - 1 : ] [ 0 ] 
base_folder = "/" . join ( base_folder . split ( '/' ) [ 0 : - 1 ] ) 
~~ manifest = find_folder_recipes ( base_folder = base_folder , 
pattern = custom_pattern or pattern , 
manifest = manifest , 
base = base ) 
~~ def find_folder_recipes ( base_folder , 
pattern = "Singularity" , 
manifest = None , 
base = None ) : 
if manifest is None : 
~~~ manifest = dict ( ) 
~~ for root , dirnames , filenames in os . walk ( base_folder ) : 
~~~ for filename in fnmatch . filter ( filenames , pattern ) : 
~~~ container_path = os . path . join ( root , filename ) 
if base is not None : 
~~~ container_base = container_path . replace ( base , '' ) . strip ( '/' ) 
collection = container_base . split ( '/' ) [ 0 ] 
recipe = os . path . basename ( container_base ) 
container_uri = "%s/%s" % ( collection , recipe ) 
~~~ container_uri = '/' . join ( container_path . strip ( '/' ) . split ( '/' ) [ - 2 : ] ) 
~~ add_container = True 
if container_uri in manifest : 
~~~ if manifest [ container_uri ] [ 'modified' ] > os . path . getmtime ( container_path ) : 
~~~ add_container = False 
~~ ~~ if add_container : 
~~~ manifest [ container_uri ] = { 'path' : os . path . abspath ( container_path ) , 
'modified' : os . path . getmtime ( container_path ) } 
~~ ~~ ~~ return manifest 
~~ def find_single_recipe ( filename , pattern = "Singularity" , manifest = None ) : 
if pattern is None : 
~~~ pattern = "Singularity*" 
~~ recipe = None 
file_basename = os . path . basename ( filename ) 
if fnmatch . fnmatch ( file_basename , pattern ) : 
~~~ recipe = { 'path' : os . path . abspath ( filename ) , 
'modified' : os . path . getmtime ( filename ) } 
~~ if manifest is not None and recipe is not None : 
~~~ container_uri = '/' . join ( filename . split ( '/' ) [ - 2 : ] ) 
~~~ if manifest [ container_uri ] [ 'modified' ] < os . path . getmtime ( filename ) : 
~~~ manifest [ container_uri ] = recipe 
~~ return recipe 
~~ def build ( self , name , 
context = None , 
config = self . _load_build_config ( name = names [ 'uri' ] , recipe = recipe ) 
build_package = [ recipe ] 
if context not in [ None , '' , [ ] ] : 
~~~ if '.' in context : 
~~~ context = glob ( os . getcwd ( ) + '/**/*' , recursive = True ) 
~~ build_package = build_package + context 
~~ package = create_build_package ( build_package ) 
destination = 'source/%s' % os . path . basename ( package ) 
blob = self . _build_bucket . blob ( destination ) 
if not blob . exists ( ) and preview is False : 
manifest = self . _upload ( source = package , 
bucket = self . _build_bucket , 
destination = destination ) 
~~ config [ "source" ] [ "storageSource" ] [ 'bucket' ] = self . _build_bucket . name 
config [ "source" ] [ "storageSource" ] [ 'object' ] = destination 
if preview is False : 
~~~ config = self . _run_build ( config , self . _bucket , names ) 
~~ if not self . _get_and_update_setting ( 'SREGISTRY_GOOGLE_BUILD_CACHE' ) : 
~~~ blob . delete ( ) 
~~ shutil . rmtree ( os . path . dirname ( package ) ) 
~~ def create_build_package ( package_files ) : 
for package_file in package_files : 
~~~ if not os . path . exists ( package_file ) : 
build_dir = get_tmpdir ( prefix = "sregistry-build" ) 
build_tar = '%s/build.tar.gz' % build_dir 
tar = tarfile . open ( build_tar , "w:gz" ) 
~~~ tar . add ( package_file ) 
sha256 = get_file_hash ( build_tar ) 
hash_tar = "%s/%s.tar.gz" % ( build_dir , sha256 ) 
shutil . move ( build_tar , hash_tar ) 
return hash_tar 
~~ def load_build_config ( self , name , recipe ) : 
version_envar = 'SREGISTRY_GOOGLE_BUILD_SINGULARITY_VERSION' 
version = self . _get_and_update_setting ( version_envar , '3.0.2-slim' ) 
config = get_build_template ( ) 
container_name = '%s.sif' % name . replace ( '/' , '-' , 1 ) 
config [ 'steps' ] [ 0 ] [ 'name' ] = 'singularityware/singularity:%s' % version 
config [ 'steps' ] [ 0 ] [ 'args' ] = [ 'build' , container_name , recipe ] 
config [ "artifacts" ] [ "objects" ] [ "location" ] = "gs://%s" % self . _bucket_name 
config [ "artifacts" ] [ "objects" ] [ "paths" ] = [ container_name ] 
~~ def run_build ( self , config , bucket , names ) : 
bot . custom ( 'PROJECT' , project , "CYAN" ) 
response = self . _build_service . projects ( ) . builds ( ) . create ( body = config , 
projectId = project ) . execute ( ) 
build_id = response [ 'metadata' ] [ 'build' ] [ 'id' ] 
status = response [ 'metadata' ] [ 'build' ] [ 'status' ] 
while status not in [ 'COMPLETE' , 'FAILURE' , 'SUCCESS' ] : 
~~~ time . sleep ( 15 ) 
response = self . _build_service . projects ( ) . builds ( ) . get ( id = build_id , 
build_id = response [ 'id' ] 
status = response [ 'status' ] 
if status == 'SUCCESS' : 
~~~ env = 'SREGISTRY_GOOGLE_STORAGE_PRIVATE' 
blob = bucket . blob ( response [ 'artifacts' ] [ 'objects' ] [ 'paths' ] [ 0 ] ) 
if self . _get_and_update_setting ( env ) == None : 
response [ 'public_url' ] = blob . public_url 
~~ update_blob_metadata ( blob , response , config , bucket , names ) 
response [ 'media_link' ] = blob . media_link 
response [ 'size' ] = blob . size 
response [ 'file_hash' ] = blob . md5_hash 
~~ def update_blob_metadata ( blob , response , config , bucket , names ) : 
manifest = os . path . basename ( response [ 'results' ] [ 'artifactManifest' ] ) 
manifest = json . loads ( bucket . blob ( manifest ) . download_as_string ( ) ) 
metadata = { 'file_hash' : manifest [ 'file_hash' ] [ 0 ] [ 'file_hash' ] [ 0 ] [ 'value' ] , 
'artifactManifest' : response [ 'results' ] [ 'artifactManifest' ] , 
'location' : manifest [ 'location' ] , 
'storageSourceBucket' : config [ 'source' ] [ 'storageSource' ] [ 'bucket' ] , 
'storageSourceObject' : config [ 'source' ] [ 'storageSource' ] [ 'object' ] , 
'builder' : config [ 'steps' ] [ 0 ] [ 'name' ] , 
'media_link' : blob . media_link , 
'self_link' : blob . self_link , 
'size' : blob . size , 
'name' : names [ 'tag_uri' ] , 
~~~ return self . _container_query ( query ) 
url = '...' 
~~~ rows . append ( [ c [ 'uri' ] , 
c [ 'detail' ] ] ) 
~~ ~~ ~~ bot . table ( rows ) 
~~ def parse_image_name ( image_name , 
tag = None , 
version = None , 
defaults = True , 
ext = "sif" , 
default_collection = "library" , 
default_tag = "latest" , 
base = None , 
lowercase = True ) : 
original = image_name 
~~~ image_name = image_name . replace ( base , '' ) . strip ( '/' ) 
~~ image_name = re . sub ( '[.](img|simg|sif)' , '' , image_name ) 
uri_regexes = [ _reduced_uri , 
_default_uri , 
_docker_uri ] 
for r in uri_regexes : 
~~~ match = r . match ( image_name ) 
~~ ~~ if not match : 
~~~ bot . exit ( \ % image ) 
~~ registry = match . group ( 'registry' ) 
collection = match . group ( 'collection' ) 
repo_name = match . group ( 'repo' ) 
repo_tag = match . group ( 'tag' ) 
version = match . group ( 'version' ) 
assert ( repo_name ) 
collection = set_default ( collection , default_collection , defaults ) 
repo_tag = set_default ( repo_tag , default_tag , defaults ) 
if lowercase : 
~~~ collection = collection . lower ( ) . rstrip ( '/' ) 
repo_name = repo_name . lower ( ) 
repo_tag = repo_tag . lower ( ) 
~~~ collection = collection . rstrip ( '/' ) 
~~ if version != None : 
~~~ version = version . lower ( ) 
~~ if registry == None : 
~~~ uri = "%s/%s" % ( collection , repo_name ) 
~~~ uri = "%s/%s/%s" % ( registry , collection , repo_name ) 
~~ url = uri 
if repo_tag != None : 
~~~ uri = "%s-%s" % ( uri , repo_tag ) 
tag_uri = "%s:%s" % ( url , repo_tag ) 
~~ storage_version = None 
if version is not None : 
~~~ uri = "%s@%s" % ( uri , version ) 
tag_uri = "%s@%s" % ( tag_uri , version ) 
storage_version = "%s@%s.%s" % ( tag_uri , version , ext ) 
~~ storage = "%s.%s" % ( uri , ext ) 
storage_uri = "%s.%s" % ( tag_uri , ext ) 
result = { "collection" : collection , 
"original" : original , 
"registry" : registry , 
"image" : repo_name , 
"url" : url , 
"tag" : repo_tag , 
"version" : version , 
"storage" : storage , 
"storage_uri" : storage_uri , 
"storage_version" : storage_version or storage_uri , 
"tag_uri" : tag_uri , 
"uri" : uri } 
~~ def format_container_name ( name , special_characters = None ) : 
if special_characters is None : 
~~~ special_characters = [ ] 
~~ return '' . join ( e . lower ( ) 
for e in name if e . isalnum ( ) or e in special_characters ) 
~~ def get_uri ( image ) : 
image = image or '' 
regexp = re . compile ( '^.+://' ) 
uri = regexp . match ( image ) 
if uri is not None : 
~~~ uri = ( uri . group ( ) . lower ( ) 
. replace ( '_' , '-' ) 
. replace ( '://' , '' ) ) 
accepted_uris = [ 'aws' , 
'docker' , 
'dropbox' , 
'gitlab' , 
'globus' , 
'google-build' , 
'google-storage' , 
'google-drive' , 
'hub' , 
'nvidia' , 
'registry' , 
's3' , 
'swift' ] 
if "shub" in uri : uri = "hub" 
if uri not in accepted_uris : 
uri = None 
~~ ~~ return uri 
collection = self . _get_collection ( names [ 'collection' ] ) 
collections = self . get_collections ( ) 
if collections : 
~~ image_name = os . path . basename ( names [ 'storage' ] ) 
~~~ obj_tuple = self . conn . get_object ( names [ 'collection' ] , image_name ) 
~~ except ClientException : 
~~ if names [ 'version' ] == None : 
~~~ names [ 'version' ] = obj_tuple [ 0 ] [ 'etag' ] 
~~ with open ( file_name , 'wb' ) as filey : 
~~~ filey . write ( obj_tuple [ 1 ] ) 
~~ if save is True : 
~~~ names . update ( obj_tuple [ 0 ] ) 
container = self . add ( image_path = file_name , 
image_uri = names [ 'uri' ] , 
metadata = names ) 
if os . path . exists ( image_file ) : 
parent = self . _get_or_create_folder ( self . _base ) 
metadata = metadata [ 'data' ] 
metadata . update ( names ) 
metadata . update ( metadata [ 'attributes' ] [ 'labels' ] ) 
del metadata [ 'attributes' ] 
file_metadata = { 
'name' : names [ 'storage' ] , 
'mimeType' : 'application/octet-stream' , 
'parents' : [ parent [ 'id' ] ] , 
'properties' : metadata 
media = MediaFileUpload ( path , resumable = True ) 
~~~ bot . spinner . start ( ) 
image = self . _service . files ( ) . create ( body = file_metadata , 
media_body = media , 
thumbnail = get_thumbnail ( ) 
with open ( thumbnail , "rb" ) as f : 
~~~ body = { "contentHints" : { 
"thumbnail" : { "image" : base64 . urlsafe_b64encode ( f . read ( ) ) . decode ( 'utf8' ) , 
"mimeType" : "image/png" } 
} } 
image = self . _service . files ( ) . update ( fileId = image [ 'id' ] , 
body = body ) . execute ( ) 
~~ bot . spinner . stop ( ) 
print ( image [ 'name' ] ) 
~~ except HttpError : 
~~ def _set_base ( self , zone = None ) : 
if hasattr ( self . aws . _client_config , 'region_name' ) : 
~~~ zone = self . aws . _client_config . region_name 
~~ aws_id = self . _required_get_and_update ( 'SREGISTRY_AWS_ID' ) 
aws_zone = self . _required_get_and_update ( 'SREGISTRY_AWS_ZONE' , zone ) 
version = self . _get_setting ( 'SREGISTRY_AWS_VERSION' , 'v2' ) 
base = self . _get_setting ( 'SREGISTRY_AWS_BASE' ) 
~~~ base = "%s.dkr.ecr.%s.amazonaws.com" % ( aws_id , aws_zone ) 
~~ nohttps = self . _get_setting ( 'SREGISTRY_AWS_NOHTTPS' ) 
self . aws = driver . session . create_client ( 'ecr' ) 
~~ def get_logging_level ( ) : 
level = os . environ . get ( "MESSAGELEVEL" , INFO ) 
if isinstance ( level , int ) : 
~~~ return level 
~~ if level == "CRITICAL" : 
~~~ return CRITICAL 
~~ elif level == "ABORT" : 
~~~ return ABORT 
~~ elif level == "ERROR" : 
~~~ return ERROR 
~~ elif level == "WARNING" : 
~~~ return WARNING 
~~ elif level == "LOG" : 
~~~ return LOG 
~~ elif level == "INFO" : 
~~~ return INFO 
~~ elif level == "QUIET" : 
~~~ return QUIET 
~~ elif level . startswith ( "VERBOSE" ) : 
~~~ return VERBOSE3 
~~ elif level == "DEBUG" : 
~~~ return DEBUG 
~~ return level 
~~ def useColor ( self ) : 
COLORIZE = get_user_color_preference ( ) 
if COLORIZE is not None : 
~~~ return COLORIZE 
~~ streams = [ self . errorStream , self . outputStream ] 
for stream in streams : 
~~~ if not hasattr ( stream , 'isatty' ) : 
~~ if not stream . isatty ( ) : 
~~ def addColor ( self , level , text ) : 
if self . colorize : 
~~~ if level in self . colors : 
~~~ text = "%s%s%s" % ( self . colors [ level ] , 
text , 
self . colors [ "OFF" ] ) 
~~ ~~ return text 
~~ def emitError ( self , level ) : 
if level in [ ABORT , 
ERROR , 
WARNING , 
VERBOSE , 
VERBOSE1 , 
VERBOSE2 , 
VERBOSE3 , 
DEBUG ] : 
~~ def emit ( self , level , message , prefix = None , color = None ) : 
~~~ color = level 
~~ if prefix is not None : 
~~~ prefix = "" 
message = self . addColor ( color , message ) 
~~ message = "%s%s" % ( prefix , message ) 
if not message . endswith ( '\\n' ) : 
~~~ message = "%s\\n" % message 
~~ if self . level == QUIET : 
~~ elif self . isEnabledFor ( level ) : 
~~~ if self . emitError ( level ) : 
~~~ self . write ( self . errorStream , message ) 
~~~ self . write ( self . outputStream , message ) 
~~ ~~ self . history . append ( message ) 
~~ def write ( self , stream , message ) : 
if isinstance ( message , bytes ) : 
~~~ message = message . decode ( 'utf-8' ) 
~~ stream . write ( message ) 
~~ def get_logs ( self , join_newline = True ) : 
if join_newline : 
~~~ return '\\n' . join ( self . history ) 
~~ return self . history 
~~ def show_progress ( self , 
iteration , 
total , 
length = 40 , 
min_level = 0 , 
prefix = None , 
carriage_return = True , 
suffix = None , 
symbol = None ) : 
if not self . level == QUIET : 
~~~ percent = 100 * ( iteration / float ( total ) ) 
progress = int ( length * iteration // total ) 
if suffix is None : 
~~~ suffix = '' 
~~ if prefix is None : 
~~~ prefix = 'Progress' 
~~ if percent >= 100 : 
~~~ percent = 100 
progress = length 
~~ if symbol is None : 
~~~ symbol = "=" 
~~ if progress < length : 
~~~ bar = symbol * progress + '|' + '-' * ( length - progress - 1 ) 
~~~ bar = symbol * progress + '-' * ( length - progress ) 
~~ if self . level > min_level : 
~~~ percent = "%5s" % ( "{0:.1f}" ) . format ( percent ) 
sys . stdout . write ( output ) , 
if iteration == total and carriage_return : 
~~~ sys . stdout . write ( '\\n' ) 
~~ sys . stdout . flush ( ) 
~~ ~~ ~~ def table ( self , rows , col_width = 2 ) : 
labels = [ str ( x ) for x in range ( 1 , len ( rows ) + 1 ) ] 
if isinstance ( rows , dict ) : 
~~~ labels = list ( rows . keys ( ) ) 
rows = list ( rows . values ( ) ) 
~~ for row in rows : 
~~~ label = labels . pop ( 0 ) 
label = label . ljust ( col_width ) 
message = "\\t" . join ( row ) 
self . custom ( prefix = label , 
message = message ) 
~~ ~~ def push ( self , path , name , tag = None ) : 
bot . spinner . start ( ) 
endpoint , remote = self . _parse_endpoint_name ( name ) 
q = parse_image_name ( image ) 
~~ if not hasattr ( self , 'transfer_client' ) : 
~~ endpoints = self . _get_endpoints ( ) 
~~ source_endpoint = None 
~~~ source_endpoint = contender 
~~ ~~ if source_endpoint is None : 
~~ self . _create_endpoint_cache ( endpoint ) 
added = self . add ( image_path = path , 
image_uri = q [ 'uri' ] , 
copy = True ) 
source_endpoint [ 'id' ] , 
image = ".singularity/shub/%s" % image 
tdata . add_item ( added . image , image ) 
endpoint , image ) ) 
return transfer_result 
~~ def get_template ( name ) : 
name = name . lower ( ) 
templates = dict ( ) 
templates [ 'tarinfo' ] = { "gid" : 0 , 
"uid" : 0 , 
"uname" : "root" , 
"gname" : "root" , 
"mode" : 493 } 
if name in templates : 
return templates [ name ] 
~~ ~~ def update_token ( self ) : 
tokens = self . aws . get_authorization_token ( ) 
~~ ~~ def download_layers ( self , repo_name , digest = None , destination = None ) : 
from sregistry . main . workers import Workers 
from sregistry . main . workers . aws import download_task 
self . _get_manifest ( repo_name , digest ) 
digests = self . _get_digests ( repo_name , digest ) 
self . _update_token ( ) 
~~~ targz = "%s/%s.tar.gz" % ( destination , digest [ 'digest' ] ) 
url = '%s/%s/blobs/%s' % ( self . base , repo_name , digest [ 'digest' ] ) 
~~~ tasks . append ( ( url , self . headers , targz ) ) 
~~ return layers , url 
~~ def get_manifest ( self , repo_name , tag ) : 
repo = self . aws . describe_images ( repositoryName = repo_name ) 
if 'imageDetails' in repo : 
~~~ for contender in repo . get ( 'imageDetails' ) : 
~~~ if tag in contender [ 'imageTags' ] : 
~~~ image = contender 
~~ ~~ ~~ if image is None : 
~~ digest = image [ 'imageDigest' ] 
digests = self . aws . batch_get_image ( repositoryName = repo_name , 
imageIds = [ { "imageDigest" : digest , 
"imageTag" : tag } ] ) 
self . manifest = json . loads ( digests [ 'images' ] [ 0 ] [ 'imageManifest' ] ) 
return self . manifest 
~~ def get_digests ( self , repo_name , tag ) : 
if not hasattr ( self , 'manifest' ) : 
~~ return self . manifest [ 'layers' ] 
~~ def prepare_metadata ( metadata ) : 
pairs = { 
'metadata' : { 
'items' : [ { 
'key' : 'client' , 
'value' : 'sregistry' 
for key , val in metadata . items ( ) : 
~~~ if not isinstance ( val , dict ) and not isinstance ( val , list ) : 
~~~ pairs [ 'metadata' ] [ 'items' ] . append ( { 'key' : key , 'value' : val } ) 
~~~ if not isinstance ( v , dict ) and not isinstance ( v , list ) : 
~~~ pairs [ 'metadata' ] [ 'items' ] . append ( { 'key' : k , 'value' : v } ) 
~~ ~~ ~~ ~~ return pairs 
~~ def get_build_template ( name = None , manager = 'apt' ) : 
~~~ name = "%s/main/templates/build/singularity-builder-%s.sh" % ( base , 
manager ) 
~~ if os . path . exists ( name ) : 
return '' . join ( read_file ( name ) ) 
~~ def get_metadata ( self , image_file , names = { } ) : 
if image_file is not None : 
~~~ if not os . path . exists ( image_file ) : 
return names or metadata 
~~ ~~ if not names : 
~~~ names = parse_image_name ( remove_uri ( image_file ) ) 
~~ singularity = which ( 'singularity' ) [ 'message' ] 
if os . path . exists ( singularity ) and image_file is not None : 
~~~ from spython . main import Client as Singularity 
~~~ Singularity . quiet = True 
updates = Singularity . inspect ( image = image_file ) 
updates = None 
~~ if updates is not None : 
~~~ updates = json . loads ( updates ) 
metadata . update ( updates ) 
~~ ~~ ~~ metadata . update ( names ) 
~~ def _pull ( self , 
file_name , 
names , 
force = False , 
uri = "docker://" , 
~~ digest = names [ 'version' ] or names [ 'tag' ] 
sandbox = get_tmpdir ( prefix = "sregistry-sandbox" ) 
layers = self . _download_layers ( names [ 'url' ] , digest ) 
url = self . _get_manifest_selfLink ( names [ 'url' ] , digest ) 
envtar = self . _get_environment_tar ( ) 
layers = [ envtar ] + layers 
result = extract_tar ( layer , sandbox , handle_whiteout = True ) 
~~~ bot . error ( result [ 'message' ] ) 
~~ ~~ sudo = kwargs . get ( 'sudo' , False ) 
image_file = Singularity . build ( image = file_name , 
recipe = sandbox , 
sudo = sudo ) 
if image_file is None : 
image = image . replace ( 'docker://' , uri ) 
image_file = Singularity . pull ( image , pull_folder = sandbox ) 
~~~ manifests = { } 
if hasattr ( self , 'manifests' ) : 
~~~ manifests = self . manifests 
~~ container = self . add ( image_path = image_file , 
metadata = manifests , 
~~ shutil . rmtree ( sandbox ) 
return image_file 
setting = self . _get_setting ( 'SREGISTRY_MYCLIENT_VAR' ) 
setting = self . _get_and_update_setting ( 'SREGISTRY_MYCLIENT_VAR' ) 
~~ ~~ def _make_repr ( class_name , * args , ** kwargs ) : 
arguments = [ repr ( arg ) for arg in args ] 
arguments . extend ( 
"{}={!r}" . format ( name , value ) 
for name , ( value , default ) in sorted ( kwargs . items ( ) ) 
if value != default 
~~ def s3errors ( path ) : 
~~ except ClientError as error : 
~~~ _error = error . response . get ( "Error" , { } ) 
error_code = _error . get ( "Code" , None ) 
response_meta = error . response . get ( "ResponseMetadata" , { } ) 
http_status = response_meta . get ( "HTTPStatusCode" , 200 ) 
error_msg = _error . get ( "Message" , None ) 
if error_code == "NoSuchBucket" : 
~~~ raise errors . ResourceError ( path , exc = error , msg = error_msg ) 
~~ if http_status == 404 : 
~~~ raise errors . ResourceNotFound ( path ) 
~~ elif http_status == 403 : 
~~~ raise errors . PermissionDenied ( path = path , msg = error_msg ) 
~~~ raise errors . OperationFailed ( path = path , exc = error ) 
~~ ~~ except SSLError as error : 
~~~ raise errors . OperationFailed ( path , exc = error ) 
~~ except EndpointConnectionError as error : 
~~~ raise errors . RemoteConnectionError ( path , exc = error , msg = "{}" . format ( error ) ) 
~~ ~~ def factory ( cls , filename , mode , on_close ) : 
_temp_file = tempfile . TemporaryFile ( ) 
proxy = cls ( _temp_file , filename , mode , on_close = on_close ) 
return proxy 
~~ def graph_coloring_qubo ( graph , k ) : 
K = nx . complete_graph ( k ) 
g1 = nx . cartesian_product ( nx . create_empty_copy ( graph ) , K ) 
g2 = nx . cartesian_product ( graph , nx . create_empty_copy ( K ) ) 
return nx . compose ( g1 , g2 ) 
~~ def chimera_blocks ( M = 16 , N = 16 , L = 4 ) : 
for x in xrange ( M ) : 
~~~ for y in xrange ( N ) : 
~~~ for u in ( 0 , 1 ) : 
~~~ yield tuple ( ( x , y , u , k ) for k in xrange ( L ) ) 
~~ ~~ ~~ ~~ def chimera_block_quotient ( G , blocks ) : 
from networkx import Graph 
from itertools import product 
BG = Graph ( ) 
blockid = { } 
for i , b in enumerate ( blocks ) : 
~~~ BG . add_node ( i ) 
if not b or not all ( G . has_node ( x ) for x in b ) : 
~~ for q in b : 
~~~ if q in blockid : 
~~ blockid [ q ] = i 
~~ ~~ for q , u in blockid . items ( ) : 
~~~ ublock = blocks [ u ] 
for p in G [ q ] : 
~~~ if p not in blockid : 
~~ v = blockid [ p ] 
if BG . has_edge ( u , v ) or u == v : 
~~ vblock = blocks [ v ] 
if ublock [ 0 ] [ 2 ] == vblock [ 0 ] [ 2 ] : 
~~~ block_edges = zip ( ublock , vblock ) 
~~~ block_edges = product ( ublock , vblock ) 
~~ if all ( G . has_edge ( x , y ) for x , y in block_edges ) : 
~~~ BG . add_edge ( u , v ) 
~~ ~~ ~~ return BG 
~~ def embed_with_quotient ( source_graph , target_graph , M = 16 , N = 16 , L = 4 , ** args ) : 
from random import sample 
blocks = list ( chimera_blocks ( M , N , L ) ) 
BG = chimera_block_quotient ( target_graph , blocks ) 
ublocks = { block : ( block [ 0 ] [ 2 ] , i ) 
for ( i , block ) in enumerate ( blocks ) if BG . has_node ( i ) } 
source_e = list ( source_graph . edges ( ) ) 
source_n = { x for e in source_e for x in e } 
fabric_e = list ( BG . edges ( ) ) 
fix_chains = { } 
for z in source_n : 
~~~ source_e . append ( ( ( z , u ) , z ) ) 
fix_chains [ z , u ] = [ ( z , u ) ] 
~~ for u , i in ublocks . values ( ) : 
~~~ fabric_e . append ( ( ( z , u ) , i ) ) 
~~ ~~ embs = filter ( None , [ find_embedding ( source_e , fabric_e , 
fixed_chains = fix_chains , 
chainlength_patience = 0 , 
** args ) for _ in range ( 10 ) ] ) 
emb = min ( embs , key = lambda e : sorted ( ( len ( c ) 
for c in e . values ( ) ) , reverse = True ) ) 
for _ in range ( 10 ) : 
~~~ emb = find_embedding ( source_e , fabric_e , 
initial_chains = emb , 
chainlength_patience = 3 , 
skip_initialization = True , 
** args ) 
~~ newemb = { } 
for v in source_n : 
~~~ for k in range ( L ) : 
~~~ newemb [ v , k ] = [ blocks [ i ] [ k ] for i in emb [ v ] ] 
~~ ~~ return newemb 
~~ def get_stats ( self , username = '' , password = '' , organization = 'llnl' , 
force = True , repo_type = 'public' ) : 
date = str ( datetime . date . today ( ) ) 
file_path = ( '../github_stats_output/' + date [ : 4 ] + '/' + date [ : 7 ] + '/' 
+ date + '.csv' ) 
if force or not os . path . isfile ( file_path ) : 
~~~ my_github . login ( username , password ) 
calls_beginning = self . logged_in_gh . ratelimit_remaining + 1 
my_github . get_org ( organization ) 
count_members = my_github . get_mems_of_org ( ) 
count_teams = my_github . get_teams_of_org ( ) 
my_github . repos ( repo_type = repo_type , organization = organization ) 
my_github . write_org_json ( dict_to_write = self . members_json , 
path_ending_type = 'members' , is_list = True ) 
my_github . write_org_json ( dict_to_write = 
{ 'singleton' : self . org_retrieved . to_json ( ) } , 
path_ending_type = 'organization' ) 
my_github . write_org_json ( dict_to_write = self . teams_json , 
path_ending_type = 'teams' , is_list = True ) 
my_github . write_repo_json ( dict_to_write = self . repos_json , 
path_ending_type = 'repo' ) 
my_github . write_repo_json ( dict_to_write = self . contributors_json , 
path_ending_type = 'contributors' , is_list = True ) 
my_github . write_repo_json ( dict_to_write = self . pull_requests_json , 
path_ending_type = 'pull-requests' , is_list = True ) 
my_github . write_repo_json ( dict_to_write = self . issues_json , 
path_ending_type = 'issues' , is_list = True ) 
my_github . write_repo_json ( dict_to_write = self . languages_json , 
path_ending_type = 'languages' , is_dict = True ) 
my_github . write_repo_json ( dict_to_write = self . commits_json , 
path_ending_type = 'commits' , is_list = True ) 
my_github . write_to_file ( file_path , 
organization , 
count_members , 
count_teams ) 
calls_remaining = self . logged_in_gh . ratelimit_remaining 
calls_used = calls_beginning - calls_remaining 
~~ ~~ def get_mems_of_org ( self ) : 
for member in self . org_retrieved . iter_members ( ) : 
~~~ self . members_json [ member . id ] = member . to_json ( ) 
~~ return counter 
~~ def get_teams_of_org ( self ) : 
for team in self . org_retrieved . iter_teams ( ) : 
~~~ self . teams_json [ team . id ] = team . to_json ( ) 
~~ def repos ( self , repo_type = 'public' , organization = 'llnl' ) : 
for repo in self . org_retrieved . iter_repos ( type = repo_type ) : 
#JSON 
~~~ json = repo . to_json ( ) 
self . repos_json [ repo . name ] = json 
#CSV 
temp_repo = my_repo . My_Repo ( ) 
temp_repo . name = repo . full_name 
self . total_repos += 1 
temp_repo . contributors = my_github . get_total_contributors ( repo ) 
self . total_contributors += temp_repo . contributors 
temp_repo . forks = repo . forks_count 
self . total_forks += temp_repo . forks 
temp_repo . stargazers = repo . stargazers 
self . total_stars += temp_repo . stargazers 
temp_repo . pull_requests_open , temp_repo . pull_requests_closed = my_github . get_pull_reqs ( repo ) 
temp_repo . pull_requests = ( temp_repo . pull_requests_open 
+ temp_repo . pull_requests_closed ) 
self . total_pull_reqs += temp_repo . pull_requests_open 
self . total_pull_reqs += temp_repo . pull_requests_closed 
self . total_pull_reqs_open += temp_repo . pull_requests_open 
self . total_pull_reqs_closed += temp_repo . pull_requests_closed 
temp_repo . open_issues = repo . open_issues_count 
self . total_open_issues += temp_repo . open_issues 
temp_repo . closed_issues = my_github . get_issues ( repo , organization = organization ) 
temp_repo . issues = temp_repo . closed_issues + temp_repo . open_issues 
self . total_closed_issues += temp_repo . closed_issues 
self . total_issues += temp_repo . issues 
my_github . get_languages ( repo , temp_repo ) 
temp_repo . readme = my_github . get_readme ( repo ) 
temp_repo . commits = self . get_commits ( repo = repo , organization = organization ) 
self . total_commits += temp_repo . commits 
self . all_repos . append ( temp_repo ) 
~~ ~~ def get_total_contributors ( self , repo ) : 
repo_contributors = 0 
for contributor in repo . iter_contributors ( ) : 
~~~ repo_contributors += 1 
self . unique_contributors [ contributor . id ] . append ( repo . name ) 
self . contributors_json [ repo . name ] . append ( contributor . to_json ( ) ) 
~~ return repo_contributors 
~~ def get_pull_reqs ( self , repo ) : 
pull_reqs_open = 0 
pull_reqs_closed = 0 
for pull_request in repo . iter_pulls ( state = 'all' ) : 
~~~ self . pull_requests_json [ repo . name ] . append ( pull_request . to_json ( ) ) 
if pull_request . closed_at is not None : 
~~~ pull_reqs_closed += 1 
~~~ pull_reqs_open += 1 
~~ ~~ return pull_reqs_open , pull_reqs_closed 
~~ def get_issues ( self , repo , organization = 'llnl' ) : 
path = ( '../github-data/' + organization + '/' + repo . name + '/issues' ) 
is_only_today = False 
~~~ all_issues = repo . iter_issues ( state = 'all' ) 
is_only_today = True 
~~~ files = os . listdir ( path ) 
date = str ( files [ - 1 ] [ : - 5 ] ) 
if date == str ( datetime . date . today ( ) ) : 
~~~ if len ( files ) > 2 : 
~~~ date = str ( files [ - 2 ] [ : - 5 ] ) 
~~~ all_issues = repo . iter_issues ( since = date , state = 'all' ) 
~~ ~~ for issue in all_issues : 
~~~ self . issues_json [ repo . name ] . append ( issue . to_json ( ) ) 
~~ closed_issues = 0 
for issue in repo . iter_issues ( state = 'closed' ) : 
~~~ if issue is not None : 
~~~ closed_issues += 1 
~~ ~~ return closed_issues 
~~ def get_languages ( self , repo , temp_repo ) : 
~~~ self . languages [ repo . language ] += 1 
~~~ count = self . languages [ repo . language ] = 1 
~~ for repo_languages in repo . iter_languages ( ) : 
~~~ self . languages_json [ repo . name ] [ repo_languages [ 0 ] ] = repo_languages [ 1 ] 
for language in repo_languages : 
~~~ temp_repo . languages . append ( language ) 
self . previous_language = language 
~~~ self . languages_size [ self . previous_language ] += language 
~~~ size = self . languages_size [ self . previous_language ] = language 
~~ ~~ ~~ ~~ ~~ def get_readme ( self , repo ) : 
readme_contents = repo . readme ( ) 
if readme_contents is not None : 
~~~ self . total_readmes += 1 
return 'MD' 
~~ if self . search_limit >= 28 : 
time . sleep ( 60 ) 
self . search_limit = 0 
~~ self . search_limit += 1 
search_results = self . logged_in_gh . search_code ( 'readme' 
~~~ for result in search_results : 
~~~ path = result . path [ 1 : ] 
if '/' not in path and 'readme' in path . lower ( ) : 
return path 
~~ ~~ return 'MISS' 
~~ except ( github3 . models . GitHubError , StopIteration ) as e : 
~~~ return 'MISS' 
~~ ~~ def get_license ( self , repo ) : 
if self . search_limit >= 28 : 
search_results = self . logged_in_gh . search_code ( 'license' 
if '/' not in path and 'license' in path . lower ( ) : 
~~~ self . total_licenses += 1 
~~ except ( StopIteration ) as e : 
~~ ~~ def get_commits ( self , repo , organization = 'llnl' ) : 
path = ( '../github-data/' + organization + '/' + repo . name + '/commits' ) 
~~~ all_commits = repo . iter_commits ( ) 
~~~ all_commits = repo . iter_commits ( since = date ) 
~~ ~~ for commit in all_commits : 
~~~ self . commits_json [ repo . name ] . append ( commit . to_json ( ) ) 
~~ count = 0 
for commit in repo . iter_commits ( ) : 
~~ return count 
~~ def write_org_json ( self , date = ( datetime . date . today ( ) ) , 
organization = 'llnl' , dict_to_write = { } , path_ending_type = '' , 
is_list = False ) : 
path = ( '../github-data/' + organization + '-org/' 
+ path_ending_type + '/' + str ( date ) + '.json' ) 
self . checkDir ( path ) 
~~~ out_clear . close ( ) 
~~ with open ( path , 'a' ) as out : 
~~~ out . write ( '[' ) 
~~ for item in dict_to_write : 
~~~ out . write ( json . dumps ( dict_to_write [ item ] , sort_keys = True , 
out . truncate ( ) 
if is_list : 
~~~ out . write ( ']' ) 
~~ ~~ out . close ( ) 
~~ def write_repo_json ( self , date = ( datetime . date . today ( ) ) , 
is_list = False , is_dict = False ) : 
for repo in dict_to_write : 
~~~ path = ( '../github-data/' + organization + '/' + repo + '/' + 
path_ending_type + '/' + str ( date ) + '.json' ) 
with open ( path , 'w' ) as out : 
~~~ if is_list : 
for value in dict_to_write [ repo ] : 
~~~ if is_dict : 
~~~ for inner_dict in value : 
~~~ out . write ( json . dumps ( inner_dict , sort_keys = True , 
~~~ out . write ( json . dumps ( value , sort_keys = True , 
out . write ( ']' ) 
~~~ out . write ( json . dumps ( dict_to_write [ repo ] , sort_keys = True , 
~~ ~~ def write_to_file ( self , file_path = '' , date = str ( datetime . date . today ( ) ) , 
organization = 'N/A' , members = 0 , teams = 0 ) : 
self . checkDir ( file_path ) 
with open ( file_path , 'w+' ) as output : 
~~~ output . write ( 'date,organization,members,teams,unique_contributors,' 
+ 'repository,contributors,forks,stargazers,pull_requests,' 
+ 'open_issues,has_readme,has_license,languages,pull_requests_open,' 
+ 'pull_requests_closed,commits,closed_issues,issues\\n' + date + ',' 
+ organization + ',' + str ( members ) + ',' + str ( teams ) + ',' 
+ str ( len ( self . unique_contributors ) ) + '\\n' ) 
for repo in self . all_repos : 
~~~ output . write ( ',,,,,' + repo . name + ',' + str ( repo . contributors ) 
+ ',' + str ( repo . forks ) + ',' 
+ str ( repo . stargazers ) + ',' + str ( repo . pull_requests ) + ',' 
+ str ( repo . open_issues ) + ',' + str ( repo . readme ) + ',' 
+ ',' + str ( repo . pull_requests_open ) + ',' 
+ str ( repo . pull_requests_closed ) + ',' + str ( repo . commits ) 
+ ',' + str ( repo . closed_issues ) + ',' + str ( repo . issues ) 
+ '\\n' ) 
~~ output . write ( ',,,,total,' + str ( self . total_repos ) + ',' 
+ str ( self . total_contributors ) + ',' 
+ str ( self . total_forks ) + ',' + str ( self . total_stars ) + ',' 
+ str ( self . total_pull_reqs ) + ',' + str ( self . total_open_issues ) 
+ ',' + str ( self . total_readmes ) + ',' + str ( self . total_licenses ) 
+ ',,' + str ( self . total_pull_reqs_open ) + ',' 
+ str ( self . total_pull_reqs_closed ) + ',' 
+ str ( self . total_commits ) + ',' + str ( self . total_closed_issues ) 
+ ',' + str ( self . total_issues ) ) 
~~ output . close ( ) 
self . write_totals ( file_path = "../github_stats_output/total.csv" , date = date , 
organization = organization , members = members , teams = teams ) 
self . write_languages ( file_path = '../github_stats_output/languages.csv' , 
date = date ) 
~~ def write_totals ( self , file_path = '' , date = str ( datetime . date . today ( ) ) , 
total_exists = os . path . isfile ( file_path ) 
with open ( file_path , 'a' ) as out_total : 
~~~ if not total_exists : 
~~~ out_total . write ( 'date,organization,repos,members,teams,' 
+ 'unique_contributors,total_contributors,forks,' 
+ 'stargazers,pull_requests,open_issues,has_readme,' 
+ 'has_license,pull_requests_open,pull_requests_closed,' 
+ 'commits,id,closed_issues,issues\\n' ) 
~~ self . delete_last_line ( date = date , file_path = file_path ) 
~~ out_total . close ( ) 
with open ( file_path , 'r' ) as file_read : 
~~~ row_count = sum ( 1 for row in file_read ) - 1 
~~ file_read . close ( ) 
~~~ out_total . write ( date + ',' + organization + ',' 
+ str ( self . total_repos ) + ',' + str ( members ) + ',' + str ( teams ) 
+ ',' + str ( len ( self . unique_contributors ) ) + ',' 
+ str ( self . total_contributors ) + ',' + str ( self . total_forks ) 
+ ',' + str ( self . total_stars ) + ',' + str ( self . total_pull_reqs ) 
+ ',' + str ( self . total_open_issues ) + ',' 
+ str ( self . total_readmes ) + ',' + str ( self . total_licenses ) + ',' 
+ str ( self . total_pull_reqs_open ) + ',' 
+ str ( self . total_commits ) + ',' + str ( row_count ) + ',' 
+ str ( self . total_closed_issues ) + ',' + str ( self . total_issues ) 
~~ def write_languages ( self , file_path = '' , date = str ( datetime . date . today ( ) ) ) : 
self . remove_date ( file_path = file_path , date = date ) 
languages_exists = os . path . isfile ( file_path ) 
with open ( file_path , 'a' ) as out_languages : 
~~~ if not languages_exists : 
~~~ out_languages . write ( 'date,language,count,size,size_log\\n' ) 
~~ languages_sorted = sorted ( self . languages_size ) 
for language in languages_sorted : 
~~~ out_languages . write ( date + ',' + language + ',' 
+ str ( self . languages [ language ] ) + ',' 
+ str ( self . languages_size [ language ] ) + ',' 
+ str ( math . log10 ( int ( self . languages_size [ language ] ) ) ) 
~~ except ( TypeError , KeyError ) as e : 
+ str ( 0 ) + ',' 
~~ ~~ ~~ ~~ def checkDir ( self , file_path = '' ) : 
if not os . path . exists ( os . path . dirname ( file_path ) ) : 
~~~ os . makedirs ( os . path . dirname ( file_path ) ) 
~~~ if e . errno != errno . EEXIST : 
~~ ~~ ~~ ~~ def remove_date ( self , file_path = '' , date = str ( datetime . date . today ( ) ) ) : 
if languages_exists : 
~~~ with open ( file_path , 'rb' ) as inp , open ( 'temp.csv' , 'wb' ) as out : 
~~~ writer = csv . writer ( out ) 
for row in csv . reader ( inp ) : 
~~~ if row [ 0 ] != date : 
~~~ writer . writerow ( row ) 
~~ ~~ ~~ inp . close ( ) 
out . close ( ) 
os . remove ( file_path ) 
os . rename ( "temp.csv" , file_path ) 
~~ ~~ def delete_last_line ( self , file_path = '' , date = str ( datetime . date . today ( ) ) ) : 
deleted_line = False 
~~~ with open ( file_path , 'r+' ) as file : 
~~~ reader = csv . reader ( file , delimiter = ',' ) 
~~~ if date == row [ 0 ] : 
~~~ file . seek ( 0 , os . SEEK_END ) 
pos = file . tell ( ) - 1 
while pos > 0 and file . read ( 1 ) != "\\n" : 
~~~ pos -= 1 
file . seek ( pos , os . SEEK_SET ) 
~~ if pos > 0 : 
~~~ file . seek ( pos , os . SEEK_SET ) 
file . truncate ( ) 
deleted_line = True 
~~ ~~ ~~ if deleted_line : file . write ( '\\n' ) 
~~ file . close ( ) 
~~ ~~ def gov_orgs ( ) : 
us_gov_github_orgs = set ( ) 
gov_orgs = requests . get ( 'https://government.github.com/organizations.json' ) . json ( ) 
return list ( us_gov_github_orgs ) 
~~ def create_session ( token = None ) : 
~~~ token = os . environ . get ( 'GITHUB_API_TOKEN' , None ) 
~~ gh_session = github3 . login ( token = token ) 
if gh_session is None : 
~~ return gh_session 
~~ def create_enterprise_session ( url , token = None ) : 
gh_session = github3 . enterprise_login ( url = url , token = token ) 
raise RuntimeError ( msg , url ) 
~~ def _check_api_limits ( gh_session , api_required = 250 , sleep_time = 15 ) : 
api_rates = gh_session . rate_limit ( ) 
api_remaining = api_rates [ 'rate' ] [ 'remaining' ] 
api_reset = api_rates [ 'rate' ] [ 'reset' ] 
if api_remaining > api_required : 
~~ now_time = time . time ( ) 
time_to_reset = int ( api_reset - now_time ) 
while now_time < api_reset : 
~~~ time . sleep ( 10 ) 
now_time = time . time ( ) 
~~ def connect ( url = 'https://github.com' , token = None ) : 
gh_session = None 
if url == 'https://github.com' : 
~~~ gh_session = create_session ( token ) 
~~~ gh_session = create_enterprise_session ( url , token ) 
~~ if gh_session is None : 
return gh_session 
~~ def query_repos ( gh_session , orgs = None , repos = None , public_only = True ) : 
if orgs is None : 
~~~ orgs = [ ] 
~~ if repos is None : 
~~~ repos = [ ] 
~~ if public_only : 
~~~ privacy = 'public' 
~~~ privacy = 'all' 
~~ _check_api_limits ( gh_session , 10 ) 
for org_name in orgs : 
~~~ org = gh_session . organization ( org_name ) 
num_repos = org . public_repos_count 
_check_api_limits ( gh_session , _num_requests_needed ( num_repos ) ) 
for repo in org . repositories ( type = privacy ) : 
~~~ _check_api_limits ( gh_session , 10 ) 
~~ ~~ for repo_name in repos : 
org , name = repo_name . split ( '/' ) 
yield gh_session . repository ( org , name ) 
~~ if not ( orgs or repos ) : 
~~~ for repo in gh_session . all_repositories ( ) : 
~~~ yield repo 
~~ ~~ ~~ def get_stats ( self , username = '' , password = '' , organization = 'llnl' , force = True ) : 
stargazers_file_path = ( '../github_stats_output/stargazers.csv' ) 
my_github . get_repos ( ) 
my_github . write_to_file ( file_path = stargazers_file_path ) 
#my_github.write_to_file(file_path=stargazers_file_path) 
~~ ~~ def get_org ( self , organization_name = '' ) : 
self . organization_name = organization_name 
if ( organization_name == '' ) : 
self . org_retrieved = self . logged_in_gh . organization ( organization_name ) 
~~ def get_repos ( self ) : 
temp_count = 0 
for repo in self . org_retrieved . iter_repos ( ) : 
~~~ temp_count += 1 
url = ( 'https://api.github.com/repos/' + self . organization_name + '/' + repo . name ) 
self . repos [ repo . name ] = self . get_stargazers ( url = url , headers = headers ) 
~~ self . calc_stargazers ( start_count = 650 ) 
~~ def get_stargazers ( self , url , headers = { } ) : 
url = url + '/stargazers?per_page=100&page=%s' 
gazers = [ ] 
json_data = requests . get ( url % page , headers = headers ) . json ( ) 
while json_data : 
~~~ gazers . extend ( json_data ) 
~~ return gazers 
~~ def write_to_file ( self , file_path = '' , date = ( datetime . date . today ( ) ) , 
organization = 'llnl' ) : 
with open ( file_path , 'w+' ) as out : 
~~~ out . write ( 'date,organization,stargazers\\n' ) 
for star in sorted_stargazers : 
~~~ out . write ( star + ',' + str ( self . stargazers [ star ] ) + '\\n' ) 
~~ def from_github3 ( klass , repository , labor_hours = True ) : 
if not isinstance ( repository , github3 . repos . repo . _Repository ) : 
project = klass ( ) 
project [ 'name' ] = repository . name 
project [ 'repositoryURL' ] = repository . git_url 
project [ 'description' ] = repository . description 
~~~ repo_license = repository . license ( ) 
~~ except github3 . exceptions . NotFoundError : 
repo_license = None 
~~ if repo_license : 
~~~ license = repo_license . license 
if license . url is None : 
~~~ project [ 'permissions' ] [ 'licenses' ] = [ { "name" : license . spdx_id } ] 
~~~ project [ 'permissions' ] [ 'licenses' ] = [ { "URL" : license . url , "name" : license . spdx_id } ] 
~~~ project [ 'permissions' ] [ 'licenses' ] = None 
~~ ~~ public_server = repository . html_url . startswith ( 'https://github.com' ) 
if not repository . private and public_server : 
~~~ project [ 'permissions' ] [ 'usageType' ] = 'openSource' 
~~ elif date_parse ( repository . created_at ) < POLICY_START_DATE : 
~~~ project [ 'permissions' ] [ 'usageType' ] = 'exemptByPolicyDate' 
~~ if labor_hours : 
~~~ project [ 'laborHours' ] = labor_hours_from_url ( project [ 'repositoryURL' ] ) 
~~~ project [ 'laborHours' ] = 0 
~~ project [ 'tags' ] = [ 'github' ] 
old_accept = repository . session . headers [ 'Accept' ] 
repository . session . headers [ 'Accept' ] = 'application/vnd.github.mercy-preview+json' 
topics = repository . _get ( repository . url + '/topics' ) . json ( ) 
project [ 'tags' ] . extend ( topics . get ( 'names' , [ ] ) ) 
repository . session . headers [ 'Accept' ] = old_accept 
owner_url = repository . owner . url 
owner_api_response = repository . _get ( owner_url ) 
organization = repository . _json ( owner_api_response , 200 ) 
project [ 'contact' ] [ 'email' ] = organization [ 'email' ] 
project [ 'contact' ] [ 'URL' ] = organization [ 'html_url' ] 
project [ 'organization' ] = organization [ 'name' ] 
project [ 'status' ] = 'Development' 
project [ 'vcs' ] = 'git' 
project [ 'homepageURL' ] = repository . html_url 
project [ 'downloadURL' ] = repository . downloads_url 
project [ 'languages' ] = [ l for l , _ in repository . languages ( ) ] 
~~~ created_at = repository . created_at . date ( ) 
~~~ created_at = date_parse ( repository . created_at ) . date ( ) 
~~~ updated_at = repository . updated_at . date ( ) 
~~~ updated_at = date_parse ( repository . updated_at ) . date ( ) 
~~ project [ 'date' ] = { 
'created' : created_at . isoformat ( ) , 
'lastModified' : updated_at . isoformat ( ) , 
'metadataLastUpdated' : '' , 
_prune_dict_null_str ( project ) 
return project 
~~ def from_gitlab ( klass , repository , labor_hours = True ) : 
if not isinstance ( repository , gitlab . v4 . objects . Project ) : 
~~ project = klass ( ) 
repository . id , 
repository . path_with_namespace , 
project [ 'repositoryURL' ] = repository . http_url_to_repo 
project [ 'permissions' ] [ 'licenses' ] = None 
web_url = repository . web_url 
public_server = web_url . startswith ( 'https://gitlab.com' ) 
if repository . visibility in ( 'public' ) and public_server : 
~~ project [ 'tags' ] = [ 'gitlab' ] + repository . tag_list 
project [ 'contact' ] = { 
'email' : '' , 
'URL' : web_url , 
project [ 'organization' ] = repository . namespace [ 'name' ] 
project [ 'homepageURL' ] = repository . web_url 
api_url = repository . manager . gitlab . _url 
archive_suffix = '/projects/%s/repository/archive' % repository . get_id ( ) 
project [ 'downloadURL' ] = api_url + archive_suffix 
project [ 'date' ] = { 
'created' : date_parse ( repository . created_at ) . date ( ) . isoformat ( ) , 
'lastModified' : date_parse ( repository . last_activity_at ) . date ( ) . isoformat ( ) , 
~~ def from_stashy ( klass , repository , labor_hours = True ) : 
if not isinstance ( repository , dict ) : 
repository [ 'name' ] , 
repository [ 'project' ] [ 'key' ] , 
project [ 'name' ] = repository [ 'name' ] 
clone_urls = [ clone [ 'href' ] for clone in repository [ 'links' ] [ 'clone' ] ] 
for url in clone_urls : 
~~~ if url . startswith ( 'ssh://' ) : 
~~~ project [ 'repositoryURL' ] = url 
~~ ~~ description = repository [ 'project' ] . get ( 'description' , '' ) 
~~ project [ 'permissions' ] [ 'licenses' ] = None 
web_url = repository [ 'links' ] [ 'self' ] [ 0 ] [ 'href' ] 
public_server = web_url . startswith ( 'https://bitbucket.org' ) 
if repository [ 'public' ] and public_server : 
~~ project [ 'tags' ] = [ 'bitbucket' ] 
project [ 'contact' ] [ 'email' ] = '' 
project [ 'contact' ] [ 'URL' ] = repository [ 'links' ] [ 'self' ] [ 0 ] [ 'href' ] 
project [ 'vcs' ] = repository [ 'scmId' ] 
project [ 'homepageURL' ] = repository [ 'links' ] [ 'self' ] [ 0 ] [ 'href' ] 
~~ def from_doecode ( klass , record ) : 
if not isinstance ( record , dict ) : 
project [ 'name' ] = record [ 'software_title' ] 
logger . debug ( \ , record [ 'software_title' ] ) 
link = record . get ( 'repository_link' , '' ) 
if not link : 
~~~ link = record . get ( 'landing_page' ) 
~~ project [ 'repositoryURL' ] = link 
project [ 'description' ] = record [ 'description' ] 
licenses = set ( record [ 'licenses' ] ) 
licenses . discard ( None ) 
license_objects = [ ] 
if 'Other' in licenses : 
~~~ licenses . remove ( 'Other' ) 
license_objects = [ { 
'name' : 'Other' , 
'URL' : record [ 'proprietary_url' ] 
} ] 
~~ if licenses : 
~~~ license_objects . extend ( [ _license_obj ( license ) for license in licenses ] ) 
~~ project [ 'permissions' ] [ 'licenses' ] = license_objects 
if record [ 'open_source' ] : 
~~~ usage_type = 'openSource' 
~~~ usage_type = 'exemptByLaw' 
~~ project [ 'permissions' ] [ 'usageType' ] = usage_type 
project [ 'laborHours' ] = 0 
lab_name = record . get ( 'lab_display_name' ) 
if lab_name is not None : 
~~~ project [ 'tags' ] . append ( lab_name ) 
~~ project [ 'contact' ] [ 'email' ] = record [ 'owner' ] 
if 'version_number' in record and record [ 'version_number' ] : 
~~~ project [ 'version' ] = record [ 'version_number' ] 
~~ if lab_name is not None : 
~~~ project [ 'organization' ] = lab_name 
~~ status = record . get ( 'ever_announced' ) 
if status is None : 
~~~ status = 'Production' 
~~~ status = 'Development' 
~~ project [ 'status' ] = status 
vcs = None 
link = project [ 'repositoryURL' ] 
if 'github.com' in link : 
~~~ vcs = 'git' 
~~ if vcs is None : 
~~~ logger . debug ( \ , project [ 'name' ] , link ) 
vcs = '' 
~~ if vcs : 
~~~ project [ 'vcs' ] = vcs 
~~ url = record . get ( 'landing_page' , '' ) 
~~~ project [ 'homepageURL' ] = url 
~~ if 'programming_languages' in record : 
~~~ project [ 'languages' ] = record [ 'programming_languages' ] 
~~ if 'date_record_added' in record and 'date_record_updated' in record : 
~~~ project [ 'date' ] = { 
'created' : record [ 'date_record_added' ] , 
'metadataLastUpdated' : record [ 'date_record_updated' ] 
~~ return project 
~~ def from_tfs ( klass , tfs_project , labor_hours = True ) : 
project_web_url = '' 
project [ 'name' ] = tfs_project . projectInfo . name 
if 'web' in tfs_project . projectInfo . _links . additional_properties : 
~~~ if 'href' in tfs_project . projectInfo . _links . additional_properties [ 'web' ] : 
~~~ project_web_url = requote_uri ( tfs_project . projectInfo . _links . additional_properties [ 'web' ] [ 'href' ] ) 
~~ ~~ project [ 'repositoryURL' ] = project_web_url 
project [ 'homepageURL' ] = project_web_url 
project [ 'description' ] = tfs_project . projectInfo . description 
project [ 'vcs' ] = 'TFS/AzureDevOps' 
project [ 'permissions' ] [ 'license' ] = None 
project [ 'tags' ] = [ ] 
if labor_hours : 
~~ if tfs_project . projectCreateInfo . last_update_time < POLICY_START_DATE : 
~~~ project [ 'permissions' ] [ 'usageType' ] = 'exemptByAgencyMission' 
~~ project [ 'contact' ] = { 
'URL' : project_web_url 
'lastModified' : tfs_project . projectLastUpdateInfo . last_update_time . date ( ) . isoformat ( ) , 
'created' : tfs_project . projectCreateInfo . last_update_time . date ( ) . isoformat ( ) , 
'metadataLastUpdated' : '' 
~~ def process_config ( config ) : 
agency = config . get ( 'agency' , 'UNKNOWN' ) 
method = config . get ( 'method' , 'other' ) 
compute_labor_hours = config . get ( 'compute_labor_hours' , True ) 
if config . get ( 'contact_email' , None ) is None : 
~~~ logger . warning ( \ ) 
code_gov_metadata = Metadata ( agency , method ) 
github_instances = config . get ( 'GitHub' , [ ] ) 
if config . get ( 'github_gov_orgs' , False ) : 
~~~ github_instances . append ( { 
'url' : 'https://github.com' , 
'orgs' : gov_orgs ( ) , 
~~ for instance in github_instances : 
~~~ url = instance . get ( 'url' , 'https://github.com' ) 
orgs = instance . get ( 'orgs' , [ ] ) 
repos = instance . get ( 'repos' , [ ] ) 
public_only = instance . get ( 'public_only' , True ) 
excluded = instance . get ( 'exclude' , [ ] ) 
token = instance . get ( 'token' , None ) 
gh_session = github . connect ( url , token ) 
for repo in github . query_repos ( gh_session , orgs , repos , public_only ) : 
~~~ if repo . owner . login in excluded or repo . full_name in excluded : 
~~ code_gov_project = Project . from_github3 ( repo , labor_hours = compute_labor_hours ) 
code_gov_metadata [ 'releases' ] . append ( code_gov_project ) 
~~ ~~ gitlab_instances = config . get ( 'GitLab' , [ ] ) 
for instance in gitlab_instances : 
~~~ url = instance . get ( 'url' ) 
gl_session = gitlab . connect ( url , token ) 
for repo in gitlab . query_repos ( gl_session , repos ) : 
~~~ namespace = repo . namespace [ 'path' ] 
path_with_namespace = repo . path_with_namespace 
if namespace in excluded or path_with_namespace in excluded : 
~~ code_gov_project = Project . from_gitlab ( repo , labor_hours = compute_labor_hours ) 
~~ ~~ bitbucket_instances = config . get ( 'Bitbucket' , [ ] ) 
for instance in bitbucket_instances : 
username = instance . get ( 'username' ) 
password = instance . get ( 'password' ) 
bb_session = bitbucket . connect ( url , username , password ) 
for repo in bitbucket . all_repos ( bb_session ) : 
~~~ project = repo [ 'project' ] [ 'key' ] 
project_repo = '%s/%s' % ( project , repo [ 'slug' ] ) 
if project in excluded or project_repo in excluded : 
~~ code_gov_project = Project . from_stashy ( repo , labor_hours = compute_labor_hours ) 
~~ ~~ tfs_instances = config . get ( 'TFS' , [ ] ) 
for instance in tfs_instances : 
projects = tfs . get_projects_metadata ( url , token ) 
for project in projects : 
~~~ code_gov_project = Project . from_tfs ( project , labor_hours = compute_labor_hours ) 
doecode_json = doecode_config . get ( 'json' , None ) 
doecode_url = doecode_config . get ( 'url' , None ) 
doecode_key = doecode_config . get ( 'api_key' , None ) 
for record in doecode . process ( doecode_json , doecode_url , doecode_key ) : 
~~~ code_gov_project = Project . from_doecode ( record ) 
~~ return code_gov_metadata 
~~ def force_attributes ( metadata , config ) : 
organization = config . get ( 'organization' , '' ) 
contact_email = config . get ( 'contact_email' ) 
permissions = config . get ( 'permissions' , { } ) 
default_usage = permissions . get ( 'usageType' , '' ) 
default_exemption_text = permissions . get ( 'exemptionText' , '' ) 
if organization : 
~~ if contact_email : 
~~ for release in metadata [ 'releases' ] : 
~~~ if organization : 
~~~ release [ 'organization' ] = organization 
~~~ release [ 'contact' ] [ 'email' ] = contact_email 
~~ if 'licenses' not in release [ 'permissions' ] : 
~~~ release [ 'permissions' ] [ 'licenses' ] = None 
~~ if 'description' not in release : 
~~ if 'usageType' not in release [ 'permissions' ] : 
~~~ release [ 'permissions' ] [ 'usageType' ] = default_usage 
release [ 'permissions' ] [ 'exemptionText' ] = default_exemption_text 
~~ ~~ return metadata 
~~ def _license_obj ( license ) : 
obj = None 
~~~ obj = { 
'URL' : 'https://api.github.com/licenses/mit' , 
'name' : 'MIT' 
~~ elif license in ( \ ) : 
'URL' : 'https://api.github.com/licenses/bsd-2-clause' , 
'name' : 'BSD-2-Clause' 
'URL' : 'https://api.github.com/licenses/bsd-3-clause' , 
'name' : 'BSD-3-Clause' 
'URL' : 'https://api.github.com/licenses/apache-2.0' , 
'name' : 'Apache-2.0' 
'URL' : 'https://api.github.com/licenses/gpl-2.1' , 
'name' : 'GPL-2.1' 
'URL' : 'https://api.github.com/licenses/gpl-2.0' , 
'name' : 'GPL-2.0' 
'URL' : 'https://api.github.com/licenses/lgpl-2.1' , 
'name' : 'LGPL-2.1' 
'URL' : 'https://api.github.com/licenses/gpl-3.0' , 
'name' : 'GPL-3.0' 
'URL' : 'https://api.github.com/licenses/lgpl-3.0' , 
'name' : 'LGPL-3.0' 
'URL' : 'https://api.github.com/licenses/epl-1.0' , 
'name' : 'EPL-1.0' , 
'URL' : 'https://api.github.com/licenses/mpl-2.0' , 
'name' : 'MPL-2.0' , 
'URL' : 'https://api.github.com/licenses/unlicense' , 
'name' : 'Unlicense' , 
'URL' : 'https://api.github.com/licenses/agpl-3.0' , 
'name' : 'AGPL-3.0' , 
'URL' : 'https://api.github.com/licenses/epl-2.0' , 
'name' : 'EPL-2.0' , 
~~ if obj is None : 
raise ValueError ( 'Aborting!' ) 
~~ def get_stats ( self , username = '' , password = '' , organization = 'llnl' , force = True ) : 
referrers_file_path = ( '../github_stats_output/referrers.csv' ) 
views_file_path = ( '../github_stats_output/views.csv' ) 
clones_file_path = ( '../github_stats_output/clones.csv' ) 
my_github . get_traffic ( ) 
views_row_count = my_github . check_data_redundancy ( file_path = views_file_path , 
dict_to_check = self . views ) 
clones_row_count = my_github . check_data_redundancy ( file_path = clones_file_path , 
dict_to_check = self . clones ) 
my_github . write_to_file ( referrers_file_path = referrers_file_path , 
views_file_path = views_file_path , 
clones_file_path = clones_file_path , 
views_row_count = views_row_count , 
clones_row_count = clones_row_count ) 
my_github . write_json ( dict_to_write = self . referrers_json , 
path_ending_type = 'traffic_popular_referrers' ) 
my_github . write_json ( dict_to_write = self . views_json , 
path_ending_type = 'traffic_views' ) 
my_github . write_json ( dict_to_write = self . clones_json , 
path_ending_type = 'traffic_clones' ) 
my_github . write_json ( dict_to_write = self . releases_json , 
path_ending_type = 'releases' ) 
~~ ~~ def get_traffic ( self ) : 
for repo in self . org_retrieved . iter_repos ( type = 'public' ) : 
~~~ url = ( 'https://api.github.com/repos/' + self . organization_name 
+ '/' + repo . name ) 
self . get_referrers ( url = url , headers = headers , repo_name = repo . name ) 
self . get_paths ( url = url , headers = headers ) 
self . get_data ( url = url , headers = headers , dict_to_store = self . views , 
type = 'views' , repo_name = repo . name ) 
self . get_data ( url = url , headers = headers , dict_to_store = self . clones , 
type = 'clones' , repo_name = repo . name ) 
self . get_releases ( url = url , headers = headers_release , repo_name = repo . name ) 
~~ ~~ def get_releases ( self , url = '' , headers = { } , repo_name = '' ) : 
url_releases = ( url + '/releases' ) 
r = requests . get ( url_releases , headers = headers ) 
self . releases_json [ repo_name ] = r . json ( ) 
~~ def get_referrers ( self , url = '' , headers = { } , repo_name = '' ) : 
url_referrers = ( url + '/traffic/popular/referrers' ) 
r1 = requests . get ( url_referrers , headers = headers ) 
referrers_json = r1 . json ( ) 
self . referrers_json [ repo_name ] = referrers_json 
for referrer in referrers_json : 
~~~ ref_name = referrer [ 'referrer' ] 
self . referrers [ ref_name ] [ 1 ] + tuple_in [ 1 ] ) 
~~~ tuple = self . referrers [ ref_name ] = ( referrer [ 'count' ] , 
referrer [ 'uniques' ] ) 
self . referrers_lower [ ref_name . lower ( ) ] = ref_name 
~~ ~~ ~~ def get_data ( self , url = '' , headers = { } , date = str ( datetime . date . today ( ) ) , 
dict_to_store = { } , type = '' , repo_name = '' ) : 
url = ( url + '/traffic/' + type ) 
r3 = requests . get ( url , headers = headers ) 
json = r3 . json ( ) 
if type == 'views' : 
~~~ self . views_json [ repo_name ] = json 
~~ elif type == 'clones' : 
~~~ self . clones_json [ repo_name ] = json 
~~ for day in json [ type ] : 
~~~ timestamp_seconds = day [ 'timestamp' ] / 1000 
~~~ date_timestamp = datetime . datetime . utcfromtimestamp ( 
timestamp_seconds ) . strftime ( '%Y-%m-%d' ) 
if date_timestamp != date : 
~~~ tuple_in = ( day [ 'count' ] , day [ 'uniques' ] ) 
tuple = ( dict_to_store [ timestamp_seconds ] [ 0 ] + tuple_in [ 0 ] , 
dict_to_store [ timestamp_seconds ] [ 1 ] + tuple_in [ 1 ] ) 
dict_to_store [ timestamp_seconds ] = tuple 
~~~ tuple = dict_to_store [ timestamp_seconds ] = ( day [ 'count' ] , 
day [ 'uniques' ] ) 
~~ ~~ ~~ def write_json ( self , date = ( datetime . date . today ( ) ) , 
organization = 'llnl' , dict_to_write = { } , path_ending_type = '' ) : 
~~ out . close ( ) 
~~ ~~ ~~ def write_to_file ( self , referrers_file_path = '' , views_file_path = '' , 
clones_file_path = '' , date = ( datetime . date . today ( ) ) , organization = 'llnl' , 
views_row_count = 0 , clones_row_count = 0 ) : 
self . write_referrers_to_file ( file_path = referrers_file_path ) 
self . write_data_to_file ( file_path = views_file_path , 
dict_to_write = self . views , name = 'views' , 
row_count = views_row_count ) 
self . write_data_to_file ( file_path = clones_file_path , 
dict_to_write = self . clones , name = 'clones' , 
row_count = clones_row_count ) 
~~ def check_data_redundancy ( self , file_path = '' , dict_to_check = { } ) : 
exists = os . path . isfile ( file_path ) 
previous_dates = { } 
if exists : 
~~~ with open ( file_path , 'r' ) as input : 
for row in csv . reader ( input ) : 
~~~ timestamp = calendar . timegm ( time . strptime ( row [ 0 ] , 
'%Y-%m-%d' ) ) 
~~~ del dict_to_check [ timestamp ] 
~~ ~~ input . close ( ) 
~~ def write_data_to_file ( self , file_path = '' , date = str ( datetime . date . today ( ) ) , 
organization = 'llnl' , dict_to_write = { } , name = '' , row_count = 0 ) : 
with open ( file_path , 'a' ) as out : 
~~~ if not exists : 
~~~ out . write ( 'date,organization,' + name + ',unique_' + name 
+ ',id\\n' ) 
~~ sorted_dict = sorted ( dict_to_write ) 
for day in sorted_dict : 
~~~ day_formatted = datetime . datetime . utcfromtimestamp ( 
day ) . strftime ( '%Y-%m-%d' ) 
out . write ( day_formatted + ',' + organization + ',' 
+ str ( dict_to_write [ day ] [ 0 ] ) + ',' + str ( dict_to_write [ day ] [ 1 ] ) 
+ ',' + str ( row_count ) + '\\n' ) 
row_count += 1 
~~ ~~ ~~ def write_referrers_to_file ( self , file_path = '' , 
date = str ( datetime . date . today ( ) ) , organization = 'llnl' ) : 
referrers_exists = os . path . isfile ( file_path ) 
~~~ if not referrers_exists : 
~~~ out . write ( 'date,organization,referrer,count,count_log,uniques,' 
+ 'uniques_logged\\n' ) 
for referrer in sorted_referrers : 
count = self . referrers [ ref_name ] [ 0 ] 
uniques = self . referrers [ ref_name ] [ 1 ] 
~~~ count = 1.5 
~~ if uniques == 1 : 
~~~ uniques = 1.5 
~~ count_logged = math . log ( count ) 
uniques_logged = math . log ( uniques ) 
out . write ( date + ',' + organization + ',' 
+ ref_name + ',' + str ( count ) + ',' + str ( count_logged ) + ',' 
+ str ( uniques ) + ',' + str ( uniques_logged ) + '\\n' ) 
~~ def process_json ( filename ) : 
doecode_json = json . load ( open ( filename ) ) 
for record in doecode_json [ 'records' ] : 
~~~ yield record 
~~ ~~ def process_url ( url , key ) : 
if key is None : 
doecode_json = response . json ( ) 
~~ ~~ def process ( filename = None , url = None , key = None ) : 
~~~ yield from process_json ( filename ) 
~~ elif url and key : 
~~~ yield from process_url ( url , key ) 
~~ ~~ def login ( self , username = '' , password = '' ) : 
~~~ token = '' 
id = '' 
if not os . path . isfile ( 'CREDENTIALS_FILE' ) : 
~~~ if ( username == '' or password == '' ) : 
note_url = 'http://software.llnl.gov/' 
scopes = [ 'user' , 'repo' ] 
auth = github3 . authorize ( username , password , scopes , note , 
note_url , two_factor_callback = self . prompt_2fa ) 
token = auth . token 
id = auth . id 
with open ( 'CREDENTIALS_FILE' , 'w+' ) as fd : 
~~~ fd . write ( token + '\\n' ) 
fd . write ( str ( id ) ) 
~~ fd . close ( ) 
~~~ with open ( 'CREDENTIALS_FILE' , 'r' ) as fd : 
~~~ token = fd . readline ( ) . strip ( ) 
id = fd . readline ( ) . strip ( ) 
self . logged_in_gh = github3 . login ( token = token , two_factor_callback = self . prompt_2fa ) 
self . logged_in_gh . user ( ) . to_json ( ) 
~~ except ( ValueError , AttributeError , github3 . models . GitHubError ) as e : 
self . login ( ) 
~~~ login = member . to_json ( ) [ 'login' ] 
user_email = self . logged_in_gh . user ( login ) . to_json ( ) [ 'email' ] 
if user_email is not None : 
~~~ self . emails [ login ] = user_email 
~~~ self . emails [ login ] = 'none' 
~~ self . logins_lower [ login . lower ( ) ] = login 
~~ ~~ def write_to_file ( self , file_path = '' ) : 
for login in sorted_names : 
~~~ out . write ( self . logins_lower [ login ] + ',' 
+ self . emails [ self . logins_lower [ login ] ] + '\\n' ) 
~~ def connect ( url , username , password ) : 
bb_session = stashy . connect ( url , username , password ) 
return bb_session 
~~ def get_stargazers ( url , session = None ) : 
headers = { 'Accept' : 'application/vnd.github.v3.star+json' } 
url = url + '?per_page=100&page=%s' 
response = github . get ( url % page , headers = headers ) 
gazers . extend ( response . json ( ) ) 
json_data = github . get ( url % page , headers = headers ) . json ( ) 
~~ def connect ( url = 'https://gitlab.com' , token = None ) : 
~~~ token = os . environ . get ( 'GITLAB_API_TOKEN' , None ) 
~~ gl_session = gitlab . Gitlab ( url , token ) 
~~~ gl_session . version ( ) 
~~ except ( gitlab . execeptions . GitlabAuthenticationError ) : 
return gl_session 
~~ def query_repos ( gl_session , repos = None ) : 
if repos is None : 
~~ for repo in repos : 
~~~ yield gl_session . projects . get ( repo ) 
~~ if not repos : 
~~~ for project in gl_session . projects . list ( as_list = False ) : 
~~~ yield project 
~~ ~~ ~~ def git_repo_to_sloc ( url ) : 
with tempfile . TemporaryDirectory ( ) as tmp_dir : 
tmp_clone = os . path . join ( tmp_dir , 'clone-dir' ) 
cmd = [ 'git' , 'clone' , '--depth=1' , url , tmp_clone ] 
execute ( cmd ) 
cmd = [ 'cloc' , '--json' , tmp_clone ] 
out , _ = execute ( cmd ) 
~~~ json_start = out . find ( \ ) 
json_blob = out [ json_start : ] . replace ( '\\\\n' , '' ) . replace ( '\\'' , '' ) 
cloc_json = json . loads ( json_blob ) 
sloc = cloc_json [ 'SUM' ] [ 'code' ] 
sloc = 0 
return sloc 
~~ def compute_labor_hours ( sloc , month_hours = 'cocomo_book' ) : 
if month_hours == 'hours_per_year' : 
~~~ HOURS_PER_PERSON_MONTH = 40.0 * 52 / 12 
~~~ HOURS_PER_PERSON_MONTH = 152.0 
~~ cocomo_url = 'http://csse.usc.edu/tools/cocomoii.php' 
page = requests . post ( cocomo_url , data = { 'new_size' : sloc } ) 
~~~ person_months = float ( EFFORT_REGEX . search ( page . text ) . group ( 1 ) ) 
person_months = 0 
~~ labor_hours = person_months * HOURS_PER_PERSON_MONTH 
return labor_hours 
~~ def _prune_dict_null_str ( dictionary ) : 
for key , value in list ( dictionary . items ( ) ) : 
~~~ if value is None or str ( value ) == '' : 
~~~ del dictionary [ key ] 
~~ if isinstance ( value , dict ) : 
~~~ dictionary [ key ] = _prune_dict_null_str ( dictionary [ key ] ) 
~~ ~~ return dictionary 
~~ def _readGQL ( self , filePath , verbose = False ) : 
if not os . path . isfile ( filePath ) : 
~~ lastModified = os . path . getmtime ( filePath ) 
absPath = os . path . abspath ( filePath ) 
if absPath == self . __queryPath and lastModified == self . __queryTimestamp : 
query_in = self . __query 
with open ( filePath , "r" ) as q : 
~~~ query_in = re . sub ( r'#.*(\\n|\\Z)' , '\\n' , q . read ( ) ) 
query_in = re . sub ( r'(\\A\\s+)|(\\s+\\Z)' , '' , query_in ) 
self . __queryPath = absPath 
self . __queryTimestamp = lastModified 
self . __query = query_in 
~~ return query_in 
~~ def queryGitHubFromFile ( self , filePath , gitvars = { } , verbosity = 0 , ** kwargs ) : 
gitquery = self . _readGQL ( filePath , verbose = ( verbosity >= 0 ) ) 
return self . queryGitHub ( gitquery , gitvars = gitvars , verbosity = verbosity , ** kwargs ) 
~~ def queryGitHub ( self , gitquery , gitvars = { } , verbosity = 0 , paginate = False , cursorVar = None , keysToList = [ ] , rest = False , requestCount = 0 , pageNum = 0 ) : 
requestCount += 1 
~~~ pageNum = 0 
~~ pageNum += 1 
if paginate : 
response = self . _submitQuery ( gitquery , gitvars = gitvars , verbose = ( verbosity > 0 ) , rest = rest ) 
_vPrint ( ( verbosity >= 0 ) , response [ "headDict" ] [ "http" ] ) 
statusNum = response [ "statusNum" ] 
pageNum -= 1 
~~~ apiStatus = { 
"limit" : int ( response [ "headDict" ] [ "X-RateLimit-Limit" ] ) , 
"remaining" : int ( response [ "headDict" ] [ "X-RateLimit-Remaining" ] ) , 
"reset" : int ( response [ "headDict" ] [ "X-RateLimit-Reset" ] ) 
if not apiStatus [ "remaining" ] > 0 : 
self . _awaitReset ( apiStatus [ "reset" ] ) 
return self . queryGitHub ( gitquery , gitvars = gitvars , verbosity = verbosity , paginate = paginate , cursorVar = cursorVar , keysToList = keysToList , rest = rest , requestCount = ( requestCount - 1 ) , pageNum = pageNum ) 
~~ if statusNum == 202 : 
~~~ if requestCount >= self . maxRetry : 
return self . queryGitHub ( gitquery , gitvars = gitvars , verbosity = verbosity , paginate = paginate , cursorVar = cursorVar , keysToList = keysToList , rest = rest , requestCount = requestCount , pageNum = pageNum ) 
~~ ~~ if statusNum == 502 or statusNum == 503 : 
~~ ~~ if statusNum >= 400 or statusNum == 204 : 
outObj = json . loads ( response [ "result" ] ) 
if not rest and "errors" in outObj : 
~~ elif len ( outObj [ "errors" ] ) == 1 and len ( outObj [ "errors" ] [ 0 ] ) == 1 : 
~~ ~~ pageNum += 1 
~~~ if rest and response [ "linkDict" ] : 
~~~ if "next" in response [ "linkDict" ] : 
~~~ nextObj = self . queryGitHub ( response [ "linkDict" ] [ "next" ] , gitvars = gitvars , verbosity = verbosity , paginate = paginate , cursorVar = cursorVar , keysToList = keysToList , rest = rest , requestCount = 0 , pageNum = pageNum ) 
outObj . extend ( nextObj ) 
~~ ~~ elif not rest : 
~~~ if not cursorVar : 
~~ if not len ( keysToList ) > 0 : 
~~ aPage = outObj 
for key in keysToList [ 0 : - 1 ] : 
~~~ aPage = aPage [ key ] 
~~ gitvars [ cursorVar ] = aPage [ "pageInfo" ] [ "endCursor" ] 
if aPage [ "pageInfo" ] [ "hasNextPage" ] : 
~~~ nextObj = self . queryGitHub ( gitquery , gitvars = gitvars , verbosity = verbosity , paginate = paginate , cursorVar = cursorVar , keysToList = keysToList , rest = rest , requestCount = 0 , pageNum = pageNum ) 
newPage = nextObj 
~~~ newPage = newPage [ key ] 
~~ aPage [ keysToList [ - 1 ] ] . extend ( newPage [ keysToList [ - 1 ] ] ) 
~~ aPage . pop ( "pageInfo" , None ) 
~~ ~~ return outObj 
~~ def _submitQuery ( self , gitquery , gitvars = { } , verbose = False , rest = False ) : 
errOut = DEVNULL if not verbose else None 
bashcurl_list = bashcurl . split ( ) 
bashcurl_list [ 2 ] = authhead 
if not rest : 
~~~ gitqueryJSON = json . dumps ( { 'query' : gitquery , 'variables' : json . dumps ( gitvars ) } ) 
bashcurl_list [ 6 ] = gitqueryJSON 
~~ fullResponse = check_output ( bashcurl_list , stderr = errOut ) . decode ( ) 
_vPrint ( verbose , "\\n" + fullResponse ) 
fullResponse = fullResponse . split ( '\\r\\n\\r\\n' ) 
heads = fullResponse [ 0 ] . split ( '\\r\\n' ) 
if len ( fullResponse ) > 1 : 
~~~ result = fullResponse [ 1 ] 
~~~ result = "" 
~~ http = heads [ 0 ] . split ( ) 
statusNum = int ( http [ 1 ] ) 
headDict = { } 
headDict [ "http" ] = heads [ 0 ] 
for header in heads [ 1 : ] : 
headDict [ h [ 0 ] ] = h [ 1 ] 
~~ linkDict = None 
if "Link" in headDict : 
propDict = { } 
for item in linkProperties : 
~~~ divided = re . split ( r\ , item ) 
propDict [ divided [ 2 ] ] = divided [ 1 ] 
~~ linkDict = propDict 
~~ return { 'statusNum' : statusNum , 'headDict' : headDict , 'linkDict' : linkDict , 'result' : result } 
~~ def _awaitReset ( self , utcTimeStamp , verbose = True ) : 
resetTime = pytz . utc . localize ( datetime . utcfromtimestamp ( utcTimeStamp ) ) 
now = pytz . utc . localize ( datetime . utcnow ( ) ) 
waitTime = round ( ( resetTime - now ) . total_seconds ( ) ) + 1 
if waitTime <= 0 : 
~~~ waitTime = self . __retryDelay 
~~ for remaining in range ( waitTime , 0 , - 1 ) : 
~~~ _vPrint ( verbose , "\\r" + printString % ( len ( str ( waitTime ) ) , remaining ) , end = "" , flush = True ) 
~~ if verbose : 
~~~ _vPrint ( verbose , "\\r" + printString % ( len ( str ( waitTime ) ) , 0 ) ) 
~~ ~~ def fileLoad ( self , filePath = None , updatePath = True ) : 
if not filePath : 
~~~ filePath = self . filePath 
~~ if not os . path . isfile ( filePath ) : 
~~~ data_raw = q . read ( ) 
~~ print ( "Imported!" ) 
self . data = json . loads ( data_raw ) 
if updatePath : 
~~~ self . filePath = filePath 
~~ ~~ ~~ def fileSave ( self , filePath = None , updatePath = False ) : 
if not os . path . exists ( os . path . split ( filePath ) [ 0 ] ) : 
~~~ os . makedirs ( os . path . split ( filePath ) [ 0 ] ) 
~~ ~~ dataJsonString = json . dumps ( self . data , indent = 4 , sort_keys = True ) 
with open ( filePath , "w" ) as fileout : 
~~~ fileout . write ( dataJsonString ) 
~~ ~~ def create_tfs_connection ( url , token ) : 
~~~ token = os . environ . get ( 'TFS_API_TOKEN' , None ) 
~~ tfs_credentials = BasicAuthentication ( '' , token ) 
tfs_connection = VssConnection ( base_url = url , creds = tfs_credentials ) 
return tfs_connection 
~~ def create_tfs_project_analysis_client ( url , token = None ) : 
~~ tfs_connection = create_tfs_connection ( url , token ) 
project_analysis_client = tfs_connection . get_client ( 'vsts.project_analysis.v4_1.project_analysis_client.ProjectAnalysisClient' ) 
if project_analysis_client is None : 
~~ return project_analysis_client 
~~ def create_tfs_core_client ( url , token = None ) : 
tfs_client = tfs_connection . get_client ( 'vsts.core.v4_1.core_client.CoreClient' ) 
if tfs_client is None : 
~~ return tfs_client 
~~ def create_tfs_git_client ( url , token = None ) : 
tfs_git_client = tfs_connection . get_client ( 'vsts.git.v4_1.git_client.GitClient' ) 
if tfs_git_client is None : 
~~ return tfs_git_client 
~~ def create_tfs_tfvc_client ( url , token = None ) : 
tfs_tfvc_client = tfs_connection . get_client ( 'vsts.tfvc.v4_1.tfvc_client.TfvcClient' ) 
if tfs_tfvc_client is None : 
~~ return tfs_tfvc_client 
~~ def get_all_projects ( url , token , top = HARD_CODED_TOP ) : 
project_list = [ ] 
tfs_client = create_tfs_core_client ( url , token ) 
collections = tfs_client . get_project_collections ( top = top ) 
for collection in collections : 
~~~ collection_client = create_tfs_core_client ( '{url}/{collection_name}' . format ( url = url , collection_name = collection . name ) , token ) 
projects = collection_client . get_projects ( top = HARD_CODED_TOP ) 
collection_history_list = collection_client . get_project_history_entries ( ) 
projectInfo = collection_client . get_project ( project . id , True , True ) 
tfsProject = TFSProject ( projectInfo , collection ) 
tfsProject . projectLastUpdateInfo = get_project_last_update_time ( collection_history_list , project . id ) 
tfsProject . projectCreateInfo = get_project_create_time ( collection_history_list , project . id ) 
project_list . append ( tfsProject ) 
~~ ~~ return project_list 
~~ def get_git_repos ( url , token , collection , project ) : 
git_client = create_tfs_git_client ( '{url}/{collection_name}' . format ( url = url , collection_name = collection . name ) , token ) 
return git_client . get_repositories ( project . id ) 
~~ def get_tfvc_repos ( url , token , collection , project ) : 
branch_list = [ ] 
tfvc_client = create_tfs_tfvc_client ( '{url}/{collection_name}' . format ( url = url , collection_name = collection . name ) , token ) 
branches = tfvc_client . get_branches ( project . id , True , True , False , True ) 
if branches : 
~~~ branch_list . extend ( branches ) 
~~ return branch_list 
~~ def get_year_commits ( self , username = '' , password = '' , organization = 'llnl' , force = True ) : 
file_path = ( 'year_commits.csv' ) 
my_github . repos ( building_stats = True ) 
time . sleep ( 30 ) 
my_github . repos ( building_stats = False ) 
my_github . calc_total_commits ( starting_commits = 35163 ) 
my_github . write_to_file ( ) 
~~ ~~ def repos ( self , building_stats = False ) : 
~~~ for activity in repo . iter_commit_activity ( ) : 
~~~ if not building_stats : 
~~~ self . commits_dict_list . append ( activity ) 
~~ ~~ ~~ ~~ def calc_total_commits ( self , starting_commits = 0 ) : 
for week_of_commits in self . commits_dict_list : 
~~~ self . commits [ week_of_commits [ 'week' ] ] -= week_of_commits [ 'total' ] 
~~~ total = self . commits [ week_of_commits [ 'week' ] ] = - week_of_commits [ 'total' ] 
~~ ~~ self . sorted_weeks = sorted ( self . commits ) 
for week in reversed ( self . sorted_weeks ) : 
~~~ self . commits [ week ] = self . commits [ week ] + starting_commits 
starting_commits = self . commits [ week ] 
~~ ~~ def write_to_file ( self ) : 
with open ( '../github_stats_output/last_year_commits.csv' , 'w+' ) as output : 
~~~ output . write ( 'date,organization,repos,members,teams,' 
+ 'commits\\n' ) 
previous_commits = 0 
for week in self . sorted_weeks : 
~~~ week_formatted = datetime . datetime . utcfromtimestamp ( 
week ) . strftime ( '%Y-%m-%d' ) 
output . write ( week_formatted 
+ ',llnl,0,0,0,0,0,0,0,0,0,0,0,0,0,' 
+ str ( self . commits [ week ] ) + '\\n' ) 
previous_commits = str ( self . commits [ week ] ) 
~~ ~~ ~~ ~~ def order_enum ( field , members ) : 
members = list ( members ) 
return Case ( 
* ( When ( ** { field : member , 'then' : i } ) 
for i , member in enumerate ( members ) ) , 
default = len ( members ) , 
output_field = IntegerField ( ) ) 
~~ def from_db_value ( self , value , expression , connection , context ) : 
~~ return self . enum [ value ] 
~~ if isinstance ( value , self . enum ) : 
~~ def get_prep_value ( self , value ) : 
~~~ return value . name 
value = value , cls = type ( value ) ) ) 
~~ def count_id ( w0 ) : 
def f ( w1 ) : 
~~~ count = [ set ( w0 . root ) . intersection ( w1 . root ) , 
set ( w0 . flexing ) . intersection ( w1 . flexing ) , 
set ( w0 . root ) . intersection ( w1 . flexing ) | set ( w1 . root ) . intersection ( w0 . flexing ) ] 
if any ( count ) : 
~~~ return max ( ( 1 , 2 , 3 ) , key = lambda i : len ( count [ i - 1 ] ) ) 
~~ ~~ return f 
~~ def count_relations ( w0 ) : 
root_w0_relations = set ( chain . from_iterable ( relations [ t . index , : ] . indices for t in w0 . root ) ) 
flexing_w0_relations = set ( chain . from_iterable ( relations [ t . index , : ] . indices for t in w0 . flexing ) ) 
~~~ root_w1 = set ( t . index for t in w1 . root ) 
flexing_w1 = set ( t . index for t in w1 . flexing ) 
count = [ root_w0_relations . intersection ( root_w1 ) , 
flexing_w0_relations . intersection ( flexing_w1 ) , 
root_w0_relations . intersection ( flexing_w1 ) | flexing_w0_relations . intersection ( root_w1 ) ] 
~~ def t_parse ( self , s ) : 
with self . lock : 
~~~ return self . parser . parse ( s , lexer = self . lexer , debug = False ) 
~~ except CannotParse as e : 
~~~ e . s = s 
~~ ~~ ~~ def p_path ( self , p ) : 
if len ( p [ 1 ] . children ) == 1 : 
~~~ p [ 0 ] = p [ 1 ] . children [ 0 ] 
~~~ p [ 0 ] = p [ 1 ] 
~~ ~~ def p_path_sum ( self , p ) : 
if len ( p ) == 2 : 
~~~ p [ 0 ] = [ p [ 1 ] ] 
~~~ p [ 0 ] = p [ 1 ] + [ p [ 3 ] ] 
~~ ~~ def p_ctx_path ( self , p ) : 
if len ( p [ 1 ] ) == 1 : 
~~~ p [ 0 ] = p [ 1 ] [ 0 ] 
~~~ p [ 0 ] = ContextPath ( p [ 1 ] ) 
~~ ~~ def p_ctx_coords ( self , p ) : 
~~ ~~ def p_product ( self , p ) : 
~~~ p [ 0 ] = p [ 1 ] + [ p [ 2 ] ] 
~~ ~~ def p_coordinate ( self , p ) : 
~~~ p [ 0 ] = Coordinate ( p [ 1 ] ) 
~~~ p [ 0 ] = Coordinate ( p [ 1 ] , int ( p [ 2 ] ) ) 
~~ ~~ def diff ( version0 , version1 ) : 
version0 . load ( ) 
version1 . load ( ) 
deleted = set ( version0 . terms ) - set ( version1 . terms ) 
added = set ( version1 . terms ) - set ( version0 . terms ) 
if self . loaded : 
~~ file_name = "%s.json" % str ( self ) 
file = os . path . join ( VERSIONS_FOLDER , file_name ) 
if not os . path . isfile ( file ) : 
~~~ DICTIONARY_BUCKET_URL = get_configuration ( ) . get ( 'VERSIONS' , 'versionsurl' ) 
url = urllib . parse . urljoin ( DICTIONARY_BUCKET_URL , file_name ) 
urlretrieve ( url , file ) 
~~ with open ( file , 'r' ) as fp : 
~~~ self . __setstate__ ( json . load ( fp ) ) 
~~ ~~ def _rotate_sc_additive ( s ) : 
if isinstance ( s , AdditiveScript ) : 
~~~ return AdditiveScript ( [ _rotate_sc ( _s ) for _s in s ] ) 
~~~ return _rotate_sc ( s ) 
~~ ~~ def _promote_and_split ( s ) : 
subst , attr , mode = s 
subst0 , subst1 , _mode = subst 
assert isinstance ( _mode , NullScript ) 
return m ( m ( m ( subst0 ) ) , m ( m ( subst1 ) , attr ) , m ( mode ) ) 
~~ def _transfer_substance ( s ) : 
attr0 , attr1 , attr2 = attr 
assert isinstance ( attr1 , NullScript ) and isinstance ( attr2 , NullScript ) 
subst , subst1 , subst2 = subst 
assert isinstance ( subst1 , NullScript ) and isinstance ( subst2 , NullScript ) 
subst0 , subst1 , subst2 = subst 
assert isinstance ( subst2 , NullScript ) 
return m ( m ( m ( subst0 ) ) , m ( m ( subst1 ) , attr0 ) , mode ) 
~~ def _add_mode_t ( s ) : 
assert isinstance ( mode , NullScript ) 
return m ( subst , attr , script ( 't.' ) ) 
~~ def _insert_f_additive ( s ) : 
~~~ """i.B:.-+u.M:.-O:.-\ 
if isinstance ( subst , AdditiveScript ) : 
~~~ subst = AdditiveScript ( [ _insert_attr_f ( _s ) for _s in subst ] ) 
~~~ subst = _insert_attr_f ( subst ) 
~~ return m ( subst , attr ) 
~~ def _fix_typo ( s ) : 
~~~ """M:.-O:.-\ 
return m ( subst , attr , script ( "t.-x.-s.y.-\ ) ) 
~~ def translate_script ( to_translate ) : 
version = DictionaryVersion ( latest_dictionary_version ( ) ) 
version . load ( ) 
to_add = { 
'terms' : [ ] , 
'roots' : [ ] , 
'inhibitions' : { } , 
'translations' : { l : { } for l in LANGUAGES } 
for root , func in to_translate . items ( ) : 
~~~ root = script ( root ) 
terms = list ( filter ( lambda s : s in root , map ( script , version . terms ) ) ) 
new_root = func ( root ) 
new_terms = [ func ( s ) for s in terms ] 
to_add [ 'terms' ] . extend ( map ( str , new_terms ) ) 
to_add [ 'roots' ] . append ( str ( new_root ) ) 
to_add [ 'inhibitions' ] . update ( { str ( new_root ) : version . inhibitions [ root ] } ) 
for l in LANGUAGES : 
~~~ to_add [ 'translations' ] [ l ] . update ( { str ( func ( s ) ) : version . translations [ l ] [ s ] for s in terms } ) 
~~ to_remove . extend ( map ( str , terms ) ) 
~~ return create_dictionary_version ( version , add = to_add , remove = to_remove ) 
~~ def translate_mouvements_et_milieux ( s ) : 
~~~ """i.f.B:.-+u.f.M:.-O:.-\ 
~~~ subst = AdditiveScript ( [ _remove_attr_f ( _s ) for _s in subst ] ) 
~~~ subst = _remove_attr_f ( subst ) 
~~ def translate_competence_en_curr_data ( s ) : 
attr_s , attr_a , attr_m = attr 
assert isinstance ( attr_m , NullScript ) 
subst_s , subst_a , subst_m = subst 
assert isinstance ( subst_m , NullScript ) 
first_M = subst_s . children [ 0 ] . children [ 0 ] 
return m ( m ( mode , m ( attr_a ) ) , m ( m ( m ( m ( first_M , attr_s . children [ 0 ] . children [ 0 ] ) ) ) ) , m ( m ( subst_a ) ) ) 
~~ def translate_noetic ( s ) : 
~~~ """M:.O:.-O:.O:.-B:.T:.n.-\ 
return m ( script ( 's.' ) , 
m ( subst . children [ 0 ] . children [ 0 ] , subst . children [ 1 ] . children [ 0 ] ) , 
m ( attr . children [ 0 ] . children [ 0 ] , attr . children [ 1 ] . children [ 0 ] ) ) 
~~ def translate_tisse_intl_col ( s ) : 
~~~ """O:M:.-O:M:.-we.h.-\ 
return m ( m ( subst ) , m ( attr ) , script ( "s.o.-k.o.-\ ) ) 
~~ def translate_formes_visuelles ( s ) : 
~~~ """s.u.-\ 
def set_bSU_subst ( s ) : 
~~~ subst , attr , mode = s 
return m ( script ( "b.-S:.U:.-\ ) , attr , mode ) 
~~ if isinstance ( s , AdditiveScript ) : 
~~~ return AdditiveScript ( [ set_bSU_subst ( i ) for i in s . children ] ) 
~~~ return set_bSU_subst ( s ) 
~~ ~~ def translate_ecosystem_intl_col ( s ) : 
return m ( script ( "s.o.-k.o.-\ ) , m ( m ( m ( attr . children [ 0 ] , subst . children [ 0 ] ) ) ) ) 
~~ def translate_ecosystem_intl_col_tern ( s ) : 
~~~ """O:.M:.-M:.-\ 
return m ( translate_ecosystem_intl_col ( subst ) , m ( m ( attr ) ) ) 
~~ def parse ( self , s ) : 
~~~ return self . parser . parse ( s , lexer = self . lexer ) 
~~ except InvalidIEMLObjectArgument as e : 
~~~ raise CannotParse ( s , str ( e ) ) 
~~ ~~ ~~ def p_literal_list ( self , p ) : 
if len ( p ) == 3 : 
~~~ p [ 0 ] = p [ 1 ] + [ p [ 2 ] [ 1 : - 1 ] ] 
~~~ p [ 0 ] = [ p [ 1 ] [ 1 : - 1 ] ] 
~~ ~~ def p_word ( self , p ) : 
~~~ term = self . _get_term ( p [ 1 if len ( p ) == 2 else 2 ] ) 
~~ except TermNotFoundInDictionary as e : 
~~~ raise CannotParse ( self . _ieml , str ( e ) ) 
~~ if len ( p ) == 5 : 
~~~ p [ 0 ] = Word ( term , literals = p [ 4 ] ) 
~~~ p [ 0 ] = Word ( term ) 
~~ ~~ def p_proposition_sum ( self , p ) : 
if len ( p ) == 4 : 
~~ elif len ( p ) == 3 : 
~~ ~~ def p_topic ( self , p ) : 
~~~ p [ 0 ] = Topic ( root = tuple ( p [ 2 ] ) , flexing = ( ) ) 
~~ elif len ( p ) == 5 : 
~~~ p [ 0 ] = Topic ( root = tuple ( p [ 2 ] ) , flexing = ( ) , literals = p [ 4 ] ) 
~~ elif len ( p ) == 6 : 
~~~ p [ 0 ] = Topic ( root = tuple ( p [ 2 ] ) , flexing = tuple ( p [ 4 ] ) ) 
~~~ p [ 0 ] = Topic ( root = tuple ( p [ 2 ] ) , flexing = tuple ( p [ 4 ] ) , literals = p [ 6 ] ) 
~~ ~~ def p_fact ( self , p ) : 
~~~ p [ 0 ] = Fact ( p [ 2 ] ) 
~~~ p [ 0 ] = Fact ( p [ 2 ] , literals = p [ 4 ] ) 
~~ ~~ def p_theory ( self , p ) : 
~~~ p [ 0 ] = Theory ( p [ 2 ] ) 
~~~ p [ 0 ] = Theory ( p [ 2 ] , literals = p [ 4 ] ) 
~~ ~~ def p_closed_proposition_list ( self , p ) : 
~~~ p [ 0 ] = p [ 1 ] + [ p [ 4 ] ] 
~~ ~~ def pack_factorisation ( facto_list ) : 
_sum = [ ] 
for f in facto_list : 
~~~ if isinstance ( f , Script ) : 
~~~ _sum . append ( f ) 
~~~ _sum . append ( MultiplicativeScript ( children = ( pack_factorisation ( l_f ) for l_f in f ) ) ) 
~~ ~~ if len ( _sum ) == 1 : 
~~~ return _sum [ 0 ] 
~~~ return AdditiveScript ( children = _sum ) 
~~ ~~ def connexity ( self ) : 
return np . matrix ( sum ( self . relations . values ( ) ) . todense ( ) , dtype = bool ) 
~~ def _resolve_path ( obj , path ) : 
if obj . __class__ not in path . context . accept : 
~~~ result = set ( ) 
for ctx in path . context . accept : 
~~~ result |= { e for u in obj [ ctx ] for e in _resolve_path ( u , path ) } 
~~ if isinstance ( obj , Text ) : 
~~~ if path . index is not None : 
~~~ return { obj . children [ path . index ] } 
~~ return set ( obj . children ) 
~~ if isinstance ( obj , ( Fact , Theory ) ) : 
~~~ return _resolve_path_tree_graph ( obj . tree_graph , path ) 
~~ if isinstance ( obj , Topic ) : 
~~~ if path . kind == 'r' : 
~~~ return { obj . root [ path . index ] } 
~~ return set ( obj . root ) 
~~~ return { obj . flexing [ path . index ] } 
~~ return set ( obj . flexing ) 
~~ ~~ ~~ def _resolve_ctx ( rules ) : 
if not rules : 
~~ if len ( rules ) == 1 and rules [ 0 ] [ 0 ] is None : 
~~~ return rules [ 0 ] [ 1 ] 
~~ if any ( r [ 0 ] is None for r in rules ) : 
~~ if any ( not isinstance ( r [ 0 ] , Path ) for r in rules ) : 
~~ r0 = rules [ 0 ] 
types = _inferred_types ( * r0 ) 
for r in rules [ 1 : ] : 
~~~ types = types . intersection ( _inferred_types ( * r ) ) 
~~ if not types : 
~~ if len ( types ) > 1 : 
~~ type = next ( types . __iter__ ( ) ) 
if type == Topic : 
~~~ error , deps = _build_deps_topic ( rules ) 
if error : 
~~ flexing = None 
if deps [ 'f' ] : 
~~~ flexing = deps [ 'f' ] 
~~ if not deps [ 'r' ] : 
~~ return topic ( deps [ 'r' ] , flexing ) 
~~ if type == Text : 
~~~ error , deps = _build_deps_text ( rules ) 
~~ return text ( deps ) 
~~ if type in ( Theory , Fact ) : 
~~~ error , deps = _build_deps_tree_graph ( rules ) 
~~ if type == Fact : 
~~~ clauses = [ ] 
for s , a , m in deps : 
~~~ clauses . append ( ( s , a , m ) ) 
~~ return fact ( clauses ) 
~~ return theory ( clauses ) 
~~ def project_usls_on_dictionary ( usls , allowed_terms = None ) : 
cells_to_usls = defaultdict ( set ) 
tables = set ( ) 
for u in usls : 
~~~ for t in u . objects ( Term ) : 
~~~ for c in t . singular_sequences : 
~~~ if not cells_to_usls [ c ] : 
~~~ tables . update ( c . relations . contained ) 
~~ cells_to_usls [ c ] . add ( u ) 
~~ ~~ ~~ if allowed_terms : 
~~~ allowed_terms = set ( allowed_terms ) 
tables = tables & allowed_terms 
cells_to_usls = { c : l for c , l in cells_to_usls . items ( ) if c in allowed_terms } 
~~ tables_to_usls = { 
table : list ( set ( u for c in table . singular_sequences for u in cells_to_usls [ c ] ) ) 
for table in tables if not isinstance ( table , TableSet ) 
return tables_to_usls 
~~ def project_usl_with_data ( usls_data , metric = None ) : 
projection = project_usls_on_dictionary ( usls_data ) 
all_terms = set ( c for u in usls_data for t in u . objects ( Term ) for c in t . singular_sequences ) 
if metric is None : 
~~~ metric = lambda e : len ( e [ 'posts' ] ) * len ( all_terms . intersection ( e [ 'table' ] . singular_sequences ) ) 
~~ return sorted ( ( { 
'table' : table , 
'usls' : usls , 
'posts' : list ( set ( chain . from_iterable ( usls_data [ u ] for u in usls ) ) ) 
} for table , usls in projection . items ( ) ) , key = metric , reverse = True ) 
~~ def p_script_lvl_0 ( self , p ) : 
if p [ 1 ] == 'E' : 
~~~ p [ 0 ] = NullScript ( layer = 0 ) 
~~ elif p [ 1 ] in REMARKABLE_ADDITION : 
~~~ p [ 0 ] = AdditiveScript ( character = p [ 1 ] ) 
~~~ p [ 0 ] = MultiplicativeScript ( character = p [ 1 ] ) 
~~ ~~ def p_sum_lvl_0 ( self , p ) : 
~~~ p [ 3 ] . append ( p [ 1 ] ) 
p [ 0 ] = p [ 3 ] 
~~ ~~ def p_script_lvl_1 ( self , p ) : 
if isinstance ( p [ 1 ] , AdditiveScript ) : 
~~~ if len ( p ) == 3 : 
~~~ p [ 0 ] = MultiplicativeScript ( substance = p [ 1 ] ) 
~~ elif len ( p ) == 4 : 
~~~ p [ 0 ] = MultiplicativeScript ( substance = p [ 1 ] , 
attribute = p [ 2 ] ) 
attribute = p [ 2 ] , 
mode = p [ 3 ] ) 
~~ ~~ def p_sum_lvl_1 ( self , p ) : 
~~ ~~ def square_order_matrix ( usl_list ) : 
usl_list = list ( usl_list ) 
indexes = { 
u : i for i , u in enumerate ( usl_list ) 
order_mat = np . zeros ( shape = ( len ( usl_list ) , len ( usl_list ) ) , dtype = int ) 
for u in usl_list : 
~~~ sorted_list = QuerySort ( u ) . sort ( collection = usl_list ) 
for i , u_s in enumerate ( sorted_list ) : 
~~~ order_mat [ indexes [ u ] , indexes [ u_s ] ] = i 
~~ ~~ return order_mat 
~~ def accept_script ( self , script ) : 
if isinstance ( self . parent , TableSet ) : 
~~ tables = [ table for table in self . script . tables_script if table in script ] 
if len ( tables ) >= 1 and { ss for t in tables for ss in t . singular_sequences } == set ( script . singular_sequences ) : 
~~~ return True , False 
~~ return False , False 
~~ def _build_pools ( self ) : 
if self . level >= Topic : 
~~~ self . topics_pool = set ( self . topic ( ) for i in range ( self . pool_size ) ) 
~~ if self . level >= Fact : 
~~~ self . facts_pool = set ( self . fact ( ) for i in range ( self . pool_size ) ) 
~~ if self . level >= Theory : 
~~~ self . theories_pool = set ( self . theory ( ) for i in range ( self . pool_size ) ) 
~~ if self . level >= Text : 
~~~ self . propositions_pool = set ( chain . from_iterable ( ( self . topics_pool , self . facts_pool , self . theories_pool ) ) ) 
~~ ~~ def view ( injector ) : 
handler = create_handler ( View , injector ) 
apply_http_methods ( handler , injector ) 
return injector . let ( as_view = handler . as_view ) 
~~ def form_view ( injector ) : 
handler = create_handler ( FormView , injector ) 
apply_form_methods ( handler , injector ) 
~~ def method_view ( injector ) : 
handler = create_handler ( MethodView ) 
~~ def api_view ( injector ) : 
handler = create_handler ( APIView , injector ) 
apply_api_view_methods ( handler , injector ) 
~~ def generic_api_view ( injector ) : 
handler = create_handler ( GenericAPIView , injector ) 
apply_generic_api_view_methods ( handler , injector ) 
~~ def model_view_set ( injector ) : 
handler = create_handler ( ModelViewSet , injector ) 
apply_model_view_set_methods ( handler , injector ) 
return injector . let ( as_viewset = lambda : handler ) 
~~ def parse_log ( log_file ) : 
template = OrderedDict ( [ 
( "clean_len" , 0 ) , 
( "total_trim" , 0 ) , 
( "total_trim_perc" , 0 ) , 
( "5trim" , 0 ) , 
( "3trim" , 0 ) , 
( "bad_reads" , 0 ) 
with open ( log_file ) as fh : 
~~~ fields = [ int ( x ) for x in line . strip ( ) . split ( ) [ - 4 : ] ] 
if not fields [ 0 ] : 
~~~ template [ "bad_reads" ] += 1 
~~ template [ "5trim" ] += fields [ 1 ] 
template [ "3trim" ] += fields [ 3 ] 
template [ "total_trim" ] += fields [ 1 ] + fields [ 3 ] 
template [ "clean_len" ] += fields [ 0 ] 
~~ total_len = template [ "clean_len" ] + template [ "total_trim" ] 
if total_len : 
~~~ template [ "total_trim_perc" ] = round ( 
( template [ "total_trim" ] / total_len ) * 100 , 2 ) 
~~~ template [ "total_trim_perc" ] = 0 
~~ ~~ return template 
~~ def clean_up ( fastq_pairs , clear ) : 
unpaired_fastq = [ f for f in os . listdir ( "." ) 
if f . endswith ( "_U.fastq.gz" ) ] 
for fpath in unpaired_fastq : 
~~~ os . remove ( fpath ) 
~~ expected_out = [ f for f in os . listdir ( "." ) if f . endswith ( "_trim.fastq.gz" ) ] 
if clear == "true" and len ( expected_out ) == 2 : 
~~~ for fq in fastq_pairs : 
~~~ rp = os . path . realpath ( fq ) 
if re . match ( ".*/work/.{2}/.{30}/.*" , rp ) : 
~~~ os . remove ( rp ) 
~~ ~~ ~~ ~~ def merge_default_adapters ( ) : 
default_adapters = [ os . path . join ( ADAPTERS_PATH , x ) for x in 
os . listdir ( ADAPTERS_PATH ) ] 
filepath = os . path . join ( os . getcwd ( ) , "default_adapters.fasta" ) 
with open ( filepath , "w" ) as fh , fileinput . input ( default_adapters ) as in_fh : 
~~~ for line in in_fh : 
~~~ fh . write ( "{}{}" . format ( line , "\\\\n" ) ) 
~~ ~~ return filepath 
~~ def main ( sample_id , fastq_pair , trim_range , trim_opts , phred , adapters_file , 
clear ) : 
cli = [ 
"java" , 
"-jar" , 
TRIM_PATH . strip ( ) , 
"PE" , 
"-threads" , 
"$task.cpus" 
~~~ phred = int ( phred ) 
phred_flag = "-phred{}" . format ( str ( phred ) ) 
cli += [ phred_flag ] 
~~ cli += fastq_pair 
output_names = [ ] 
for i in range ( len ( fastq_pair ) ) : 
~~~ output_names . append ( "{}_{}_trim.fastq.gz" . format ( 
SAMPLE_ID , str ( i + 1 ) ) ) 
output_names . append ( "{}_{}_U.fastq.gz" . format ( 
~~ cli += output_names 
if trim_range != [ "None" ] : 
~~~ cli += [ 
"CROP:{}" . format ( trim_range [ 1 ] ) , 
"HEADCROP:{}" . format ( trim_range [ 0 ] ) , 
~~ if os . path . exists ( adapters_file ) : 
adapters_file ) ) 
adapters_file = merge_default_adapters ( ) 
~~ cli += [ 
"ILLUMINACLIP:{}:3:30:10:6:true" . format ( adapters_file ) 
logfile = os . path . join ( tempfile . mkdtemp ( prefix = 'tmp' ) , "{}_trimlog.txt" . format ( sample_id ) ) 
cli += [ 
"SLIDINGWINDOW:{}" . format ( trim_opts [ 0 ] ) , 
"LEADING:{}" . format ( trim_opts [ 1 ] ) , 
"TRAILING:{}" . format ( trim_opts [ 2 ] ) , 
"MINLEN:{}" . format ( trim_opts [ 3 ] ) , 
"TOPHRED33" , 
"-trimlog" , 
logfile 
p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE ) 
stdout , stderr = p . communicate ( ) 
~~~ stderr = stderr . decode ( "utf8" ) 
~~ except ( UnicodeDecodeError , AttributeError ) : 
~~~ stderr = str ( stderr ) 
"======================================\\\\n{}" . format ( stdout ) ) 
"======================================\\\\n{}" . format ( stderr ) ) 
p . returncode ) ) 
trimmomatic_log ( logfile , sample_id ) 
if p . returncode == 0 and os . path . exists ( "{}_1_trim.fastq.gz" . format ( 
SAMPLE_ID ) ) : 
~~~ clean_up ( fastq_pair , clear ) 
~~ with open ( ".status" , "w" ) as status_fh : 
~~~ if p . returncode != 0 : 
~~~ status_fh . write ( "fail" ) 
~~~ status_fh . write ( "pass" ) 
~~ ~~ ~~ def depth_file_reader ( depth_file ) : 
depth_dic_coverage = { } 
for line in depth_file : 
position = tab_split [ 1 ] 
num_reads_align = float ( tab_split [ 2 ] . rstrip ( ) ) 
if reference not in depth_dic_coverage : 
~~~ depth_dic_coverage [ reference ] = { } 
~~ depth_dic_coverage [ reference ] [ position ] = num_reads_align 
depth_file . close ( ) 
asizeof ( depth_dic_coverage ) / 1024 ) ) 
return depth_dic_coverage 
~~ def main ( depth_file , json_dict , cutoff , sample_id ) : 
~~~ cutoff_val = float ( cutoff ) 
if cutoff_val < 0.4 : 
~~ plasmid_length = json . load ( open ( json_dict ) ) 
if plasmid_length : 
~~ depth_file_in = open ( depth_file ) 
depth_dic_coverage = depth_file_reader ( depth_file_in ) 
percentage_bases_covered , dict_cov = generate_jsons ( depth_dic_coverage , 
plasmid_length , 
cutoff_val ) 
if percentage_bases_covered and dict_cov : 
str ( len ( percentage_bases_covered ) ) ) ) 
"empty." ) 
with open ( "{}_mapping.json" . format ( depth_file ) , "w" ) as output_json : 
~~~ output_json . write ( json . dumps ( percentage_bases_covered ) ) 
~~ json_dic = { 
"tableRow" : [ { 
"sample" : sample_id , 
"data" : [ { 
"header" : "Mapping" , 
"table" : "plasmids" , 
"patlas_mapping" : percentage_bases_covered , 
"value" : len ( percentage_bases_covered ) 
} ] , 
"plotData" : [ { 
"patlasMappingSliding" : dict_cov 
with open ( ".report.json" , "w" ) as json_report : 
~~~ json_report . write ( json . dumps ( json_dic , separators = ( "," , ":" ) ) ) 
~~ ~~ def _set_template ( self , template ) : 
tpl_dir = join ( dirname ( abspath ( __file__ ) ) , "templates" ) 
tpl_path = join ( tpl_dir , template + ".nf" ) 
if not os . path . exists ( tpl_path ) : 
~~~ raise eh . ProcessError ( 
~~ self . _template_path = join ( tpl_dir , template + ".nf" ) 
~~ def set_main_channel_names ( self , input_suffix , output_suffix , lane ) : 
self . input_channel = "{}_in_{}" . format ( self . template , input_suffix ) 
self . output_channel = "{}_out_{}" . format ( self . template , output_suffix ) 
self . lane = lane 
~~ def get_user_channel ( self , input_channel , input_type = None ) : 
res = { "input_channel" : input_channel } 
itype = input_type if input_type else self . input_type 
if itype in self . RAW_MAPPING : 
~~~ channel_info = self . RAW_MAPPING [ itype ] 
return { ** res , ** channel_info } 
~~ ~~ def render ( template , context ) : 
path , filename = os . path . split ( template ) 
return jinja2 . Environment ( 
loader = jinja2 . FileSystemLoader ( path or './' ) 
) . get_template ( filename ) . render ( context ) 
~~ def template_str ( self ) : 
if not self . _context : 
self . template , self . _context 
x = self . render ( self . _template_path , self . _context ) 
~~ def set_channels ( self , ** kwargs ) : 
if not self . pid : 
~~~ self . pid = "{}_{}" . format ( self . lane , kwargs . get ( "pid" ) ) 
~~ for i in self . status_channels : 
~~~ if i . startswith ( "STATUS_" ) : 
~~~ self . status_strs . append ( "{}_{}" . format ( i , self . pid ) ) 
~~~ self . status_strs . append ( "STATUS_{}_{}" . format ( i , self . pid ) ) 
~~ ~~ if self . main_forks : 
self . main_forks ) ) 
operator = "set" if len ( self . main_forks ) == 1 else "into" 
self . output_channel , operator , ";" . join ( self . main_forks ) ) ] 
~~ self . _context = { ** kwargs , ** { "input_channel" : self . input_channel , 
"output_channel" : self . output_channel , 
"template" : self . template , 
"forks" : "\\n" . join ( self . forks ) , 
"pid" : self . pid } } 
~~ def update_main_forks ( self , sink ) : 
if not self . main_forks : 
~~~ self . main_forks = [ self . output_channel ] 
self . output_channel = "_{}" . format ( self . output_channel ) 
~~ self . main_forks . append ( sink ) 
self . _context = { ** self . _context , 
** { "forks" : "" . join ( self . forks ) , 
"output_channel" : self . output_channel } } 
~~ def set_secondary_channel ( self , source , channel_list ) : 
source , channel_list ) ) 
source = "{}_{}" . format ( source , self . pid ) 
channel_list = sorted ( list ( set ( channel_list ) ) ) 
op = "set" if len ( channel_list ) == 1 else "into" 
source , op , ";" . join ( channel_list ) ) ) 
self . _context = { ** self . _context , ** { "forks" : "\\n" . join ( self . forks ) } } 
~~ def update_attributes ( self , attr_dict ) : 
valid_directives = [ "pid" , "ignore_type" , "ignore_pid" , "extra_input" , 
"group" , "input_type" ] 
for attribute , val in attr_dict . items ( ) : 
~~~ if attribute in valid_directives and hasattr ( self , attribute ) : 
~~~ setattr ( self , attribute , val ) 
~~ elif attribute == "params" : 
~~~ for name , value in val . items ( ) : 
~~~ if name in self . params : 
~~~ self . params [ name ] [ "default" ] = value 
~~~ for p in self . directives : 
~~~ self . directives [ p ] [ attribute ] = val 
~~ ~~ ~~ ~~ def set_compiler_channels ( self , channel_list , operator = "mix" ) : 
if not channel_list : 
"pipeline" ) 
~~ if len ( channel_list ) == 1 : 
channel_list [ 0 ] ) ) 
self . _context = { "compile_channels" : channel_list [ 0 ] } 
~~~ first_status = channel_list [ 0 ] 
if operator == "mix" : 
~~~ lst = "," . join ( channel_list [ 1 : ] ) 
s = "{}.mix({})" . format ( first_status , lst ) 
~~ elif operator == "join" : 
~~~ s = first_status 
for ch in channel_list [ 1 : ] : 
~~~ s += ".join({})" . format ( ch ) 
self . _context = { "compile_channels" : s } 
~~ ~~ def set_raw_inputs ( self , raw_input ) : 
raw_input ) ) 
primary_inputs = [ ] 
for input_type , el in raw_input . items ( ) : 
~~~ primary_inputs . append ( el [ "channel_str" ] ) 
raw_channel = self . RAW_MAPPING [ input_type ] 
self . params [ input_type ] = { 
"default" : raw_channel [ "default_value" ] , 
"description" : raw_channel [ "description" ] 
op = "set" if len ( el [ "raw_forks" ] ) == 1 else "into" 
el [ "channel" ] , op , ";" . join ( el [ "raw_forks" ] ) 
** { "forks" : "\\n" . join ( self . forks ) , 
"main_inputs" : "\\n" . join ( primary_inputs ) } } 
~~ def set_secondary_inputs ( self , channel_dict ) : 
secondary_input_str = "\\n" . join ( list ( channel_dict . values ( ) ) ) 
** { "secondary_inputs" : secondary_input_str } } 
~~ def set_extra_inputs ( self , channel_dict ) : 
extra_inputs = [ ] 
for param , info in channel_dict . items ( ) : 
~~~ raw_channel = self . RAW_MAPPING [ info [ "input_type" ] ] 
self . params [ param ] = { 
channel_name = "IN_{}_extraInput" . format ( param ) 
channel_str = self . RAW_MAPPING [ info [ "input_type" ] ] [ "channel_str" ] 
channel_str . format ( param ) ) ) 
op = "set" if len ( info [ "channels" ] ) == 1 else "into" 
channel_name , op , ";" . join ( info [ "channels" ] ) ) ) 
~~ self . _context = { 
** self . _context , 
** { "extra_inputs" : "\\n" . join ( extra_inputs ) } 
~~ def main ( sample_id , assembly_file , minsize ) : 
warnings = [ ] 
fails = "" 
assembly_obj = Assembly ( assembly_file , 0 , 0 , 
sample_id , minsize ) 
if 'spades' in assembly_file : 
~~~ assembler = "SPAdes" 
~~~ assembler = "MEGAHIT" 
~~ with open ( ".warnings" , "w" ) as warn_fh : 
~~~ t_80 = int ( minsize ) * 0.8 
t_150 = int ( minsize ) * 1.5 
assembly_len = assembly_obj . get_assembly_length ( ) 
if assembly_obj . nORFs < 1 : 
warn_fh . write ( warn_msg ) 
fails = warn_msg 
~~ if assembly_len < t_80 : 
"{}" . format ( assembly_len ) ) 
if assembly_len < t_80 : 
assembly_len ) 
logger . warning ( warn_msg ) 
~~ ~~ if assembly_len > t_150 : 
~~ ~~ with open ( ".report.json" , "w" ) as json_report : 
~~~ json_dic = { 
"data" : [ 
"value" : len ( assembly_obj . contigs ) , 
"table" : "assembly" , 
"columnBar" : True } , 
"value" : assembly_len , 
{ "header" : "ORFs" , 
"value" : assembly_obj . nORFs , 
"columnBar" : False } 
if warnings : 
~~~ json_dic [ "warnings" ] = [ { 
"value" : warnings 
~~ if fails : 
~~~ json_dic [ "fail" ] = [ { 
"value" : [ fails ] 
~~ json_report . write ( json . dumps ( json_dic , separators = ( "," , ":" ) ) ) 
~~ ~~ def _parse_coverage ( header_str ) : 
cov = None 
for i in header_str . split ( "_" ) [ : : - 1 ] : 
~~~ cov = float ( i ) 
~~ ~~ return cov 
~~ def _parse_assembly ( self , assembly_file ) : 
seq_temp = [ ] 
contig_id = 0 
cov , header = None , None 
with open ( assembly_file ) as fh : 
assembly_file ) ) 
for line in fh : 
~~~ if not line . strip ( ) : 
~~ if line . startswith ( ">" ) : 
~~~ if seq_temp : 
~~~ seq = "" . join ( seq_temp ) 
contig_id , header , cov ) ) 
self . _populate_contigs ( contig_id , header , cov , seq ) 
contig_id += 1 
~~ header = line [ 1 : ] 
cov = self . _parse_coverage ( line ) 
~~~ seq_temp . append ( line ) 
seq = "" . join ( seq_temp ) 
~~ ~~ def _populate_contigs ( self , contig_id , header , cov , sequence ) : 
gc_kwargs = self . _get_gc_content ( sequence , len ( sequence ) ) 
self . contigs [ contig_id ] = { 
"header" : header , 
"sequence" : sequence , 
"length" : len ( sequence ) , 
"kmer_cov" : cov , 
** gc_kwargs 
~~ def _get_gc_content ( sequence , length ) : 
at = sum ( map ( sequence . count , [ "A" , "T" ] ) ) 
gc = sum ( map ( sequence . count , [ "G" , "C" ] ) ) 
n = length - ( at + gc ) 
at_prop = at / length 
gc_prop = gc / length 
n_prop = n / length 
return { "at" : at , "gc" : gc , "n" : n , 
"at_prop" : at_prop , "gc_prop" : gc_prop , "n_prop" : n_prop } 
~~ def filter_contigs ( self , * comparisons ) : 
self . filtered_ids = [ ] 
self . report = { } 
gc_filters = [ 
[ "gc_prop" , ">=" , self . min_gc ] , 
[ "gc_prop" , "<=" , 1 - self . min_gc ] 
self . filters = list ( comparisons ) + gc_filters 
self . filters ) ) 
for contig_id , contig in self . contigs . items ( ) : 
~~~ for key , op , value in list ( comparisons ) + gc_filters : 
~~~ if not self . _test_truth ( contig [ key ] , op , value ) : 
~~~ self . filtered_ids . append ( contig_id ) 
self . report [ contig_id ] = "{}/{}/{}" . format ( key , 
contig [ key ] , 
value ) 
~~~ self . report [ contig_id ] = "pass" 
~~ ~~ ~~ ~~ def get_assembly_length ( self ) : 
return sum ( 
[ vals [ "length" ] for contig_id , vals in self . contigs . items ( ) 
if contig_id not in self . filtered_ids ] ) 
~~ def write_assembly ( self , output_file , filtered = True ) : 
output_file ) ) 
with open ( output_file , "w" ) as fh : 
~~~ for contig_id , contig in self . contigs . items ( ) : 
~~~ if contig_id not in self . filtered_ids and filtered : 
~~~ fh . write ( ">{}_{}\\\\n{}\\\\n" . format ( self . sample , 
contig [ "header" ] , 
contig [ "sequence" ] ) ) 
~~ ~~ ~~ ~~ def write_report ( self , output_file ) : 
~~~ for contig_id , vals in self . report . items ( ) : 
~~ ~~ ~~ def guess_process ( query_str , process_map ) : 
save_list = [ ] 
for process in process_map : 
~~~ similarity = SequenceMatcher ( None , process , query_str ) 
if similarity . ratio ( ) > 0.5 : 
~~~ save_list . append ( process ) 
~~ ~~ if save_list : 
~~~ logger . info ( colored_print ( 
~~ def remove_inner_forks ( text ) : 
while n : 
~~~ text , n = re . subn ( r'\\([^()]*\\)' , '' , text ) 
~~ def brackets_insanity_check ( p_string ) : 
if p_string . count ( FORK_TOKEN ) != p_string . count ( CLOSE_TOKEN ) : 
~~~ dict_values = { 
FORK_TOKEN : p_string . count ( FORK_TOKEN ) , 
CLOSE_TOKEN : p_string . count ( CLOSE_TOKEN ) 
max_bracket = max ( dict_values , key = dict_values . get ) 
raise SanityError ( 
str ( abs ( 
p_string . count ( FORK_TOKEN ) - p_string . count ( CLOSE_TOKEN ) ) ) , 
max_bracket ) ) 
~~ ~~ def fork_procs_insanity_check ( p_string ) : 
if FORK_TOKEN + LANE_TOKEN in p_string or LANE_TOKEN + CLOSE_TOKEN in p_string or LANE_TOKEN + FORK_TOKEN in p_string : 
~~ ~~ def inner_fork_insanity_checks ( pipeline_string ) : 
for pos , char in enumerate ( pipeline_string ) : 
~~~ if char == FORK_TOKEN : 
~~~ left_indexes . append ( pos ) 
~~ elif char == CLOSE_TOKEN and len ( left_indexes ) > 0 : 
~~~ list_of_forks . append ( pipeline_string [ left_indexes [ - 1 ] + 1 : pos ] ) 
left_indexes = left_indexes [ : - 1 ] 
~~ ~~ list_of_forks . sort ( key = lambda x : x . count ( FORK_TOKEN ) , reverse = True ) 
for fork in list_of_forks : 
~~~ for subfork in list_of_forks : 
~~~ if subfork in list_of_forks and subfork != fork : 
~~~ fork_simplified = fork . replace ( "({})" . format ( subfork ) , "" ) 
~~~ fork_simplified = fork 
~~ ~~ if not len ( fork_simplified . split ( LANE_TOKEN ) ) > 1 : 
~~ ~~ ~~ def insanity_checks ( pipeline_str ) : 
checks = [ 
[ p_string , [ 
empty_tasks , 
brackets_but_no_lanes , 
brackets_insanity_check , 
lane_char_insanity_check , 
final_char_insanity_check , 
fork_procs_insanity_check , 
start_proc_insanity_check , 
late_proc_insanity_check 
[ pipeline_str , [ 
inner_fork_insanity_checks 
for param , func_list in checks : 
~~~ for func in func_list : 
~~~ func ( param ) 
~~ ~~ ~~ def parse_pipeline ( pipeline_str ) : 
if os . path . exists ( pipeline_str ) : 
with open ( pipeline_str ) as fh : 
~~~ pipeline_str = "" . join ( [ x . strip ( ) for x in fh . readlines ( ) ] ) 
logger . info ( colored_print ( pipeline_str + "\\n" ) ) 
insanity_checks ( pipeline_str ) 
pipeline_links = [ ] 
lane = 1 
pipeline_str_modified , identifiers_to_tags = add_unique_identifiers ( 
pipeline_str ) 
nforks = pipeline_str_modified . count ( FORK_TOKEN ) 
if not nforks : 
pipeline_str ) ) 
linear_pipeline = [ "__init__" ] + pipeline_str_modified . split ( ) 
pipeline_links . extend ( linear_connection ( linear_pipeline , lane ) ) 
pipeline_links = remove_unique_identifiers ( identifiers_to_tags , 
pipeline_links ) 
return pipeline_links 
~~ for i in range ( nforks ) : 
fields = pipeline_str_modified . split ( FORK_TOKEN , i + 1 ) 
previous_process = fields [ - 2 ] . split ( LANE_TOKEN ) [ - 1 ] . split ( ) 
next_lanes = get_lanes ( fields [ - 1 ] ) 
fork_sink = [ x [ 0 ] for x in next_lanes ] 
~~~ if not previous_process : 
~~~ previous_process = [ "__init__" ] 
lane = 0 
~~~ previous_process = [ "__init__" ] + previous_process 
~~ pipeline_links . extend ( 
linear_connection ( previous_process , lane ) ) 
~~ fork_source = previous_process [ - 1 ] 
fork_lane = get_source_lane ( previous_process , pipeline_links ) 
pipeline_links . extend ( 
fork_connection ( fork_source , fork_sink , fork_lane , lane ) ) 
linear_lane_connection ( next_lanes , lane ) ) 
lane += len ( fork_sink ) 
~~ pipeline_links = remove_unique_identifiers ( identifiers_to_tags , 
~~ def get_source_lane ( fork_process , pipeline_list ) : 
fork_source = fork_process [ - 1 ] 
fork_sig = [ x for x in fork_process if x != "__init__" ] 
for position , p in enumerate ( pipeline_list [ : : - 1 ] ) : 
~~~ if p [ "output" ] [ "process" ] == fork_source : 
~~~ lane = p [ "output" ] [ "lane" ] 
lane_sequence = [ x [ "output" ] [ "process" ] for x in pipeline_list 
if x [ "output" ] [ "lane" ] == lane ] 
if lane_sequence == fork_sig : 
~~~ return p [ "output" ] [ "lane" ] 
~~ ~~ ~~ return 0 
~~ def get_lanes ( lanes_str ) : 
parsed_lanes = "" 
infork = 0 
for i in lanes_str : 
~~~ if i == FORK_TOKEN : 
~~~ infork += 1 
~~ if i == CLOSE_TOKEN : 
~~~ infork -= 1 
~~ if infork < 0 : 
~~ if infork == 0 : 
~~~ if i not in [ FORK_TOKEN , CLOSE_TOKEN ] : 
~~~ parsed_lanes += i 
~~ ~~ ~~ return [ x . split ( ) for x in parsed_lanes . split ( LANE_TOKEN ) ] 
~~ def linear_connection ( plist , lane ) : 
previous = None 
for p in plist : 
~~~ if not previous : 
~~~ previous = p 
~~ res . append ( { 
"input" : { 
"process" : previous , 
"lane" : lane 
"output" : { 
"process" : p , 
previous = p 
~~ def fork_connection ( source , sink , source_lane , lane ) : 
source , sink , source_lane , lane ) ) 
lane_counter = lane + 1 
for p in sink : 
~~~ res . append ( { 
"process" : source , 
"lane" : source_lane 
"lane" : lane_counter 
lane_counter += 1 
~~ def add_unique_identifiers ( pipeline_str ) : 
reg_find_proc = r"[^\\s{}{}{}]+" . format ( LANE_TOKEN , FORK_TOKEN , CLOSE_TOKEN ) 
process_names = re . findall ( reg_find_proc , pipeline_str_modified ) 
identifiers_to_tags = { } 
new_process_names = [ ] 
for index , val in enumerate ( process_names ) : 
~~~ if "=" in val : 
~~~ parts = val . split ( "=" ) 
new_id = "{}_{}={}" . format ( parts [ 0 ] , index , parts [ 1 ] ) 
~~~ new_id = "{}_{}" . format ( val , index ) 
~~ new_process_names . append ( new_id ) 
identifiers_to_tags [ new_id ] = val 
find = r'[{}{}{}]+' . format ( FORK_TOKEN , LANE_TOKEN , CLOSE_TOKEN ) 
pipeline_str_modified = re . sub ( find , match_result , pipeline_str_modified ) 
~~~ find = r'{}[^_]' . format ( val ) . replace ( "\\\\" , "\\\\\\\\" ) 
pipeline_str_modified , 1 ) 
~~ return pipeline_str_modified , identifiers_to_tags 
~~ def remove_unique_identifiers ( identifiers_to_tags , pipeline_links ) : 
for index , val in enumerate ( pipeline_links ) : 
~~~ if val [ "input" ] [ "process" ] != "__init__" : 
~~~ val [ "input" ] [ "process" ] = identifiers_to_tags [ 
val [ "input" ] [ "process" ] ] 
~~ if val [ "output" ] [ "process" ] != "__init__" : 
~~~ val [ "output" ] [ "process" ] = identifiers_to_tags [ 
val [ "output" ] [ "process" ] ] 
~~ ~~ return pipeline_links 
~~ def signal_handler ( screen ) : 
if screen : 
~~~ screen . clear ( ) 
screen . refresh ( ) 
curses . nocbreak ( ) 
screen . keypad ( 0 ) 
curses . echo ( ) 
curses . endwin ( ) 
~~ def _check_required_files ( self ) : 
if not os . path . exists ( self . trace_file ) : 
~~ if not os . path . exists ( self . log_file ) : 
~~ ~~ def _header_mapping ( header ) : 
return dict ( 
( x . strip ( ) , pos ) for pos , x in enumerate ( header . split ( "\\t" ) ) 
~~ def _expand_path ( hash_str ) : 
~~~ first_hash , second_hash = hash_str . split ( "/" ) 
first_hash_path = join ( abspath ( "work" ) , first_hash ) 
for l in os . listdir ( first_hash_path ) : 
~~~ if l . startswith ( second_hash ) : 
~~~ return join ( first_hash_path , l ) 
~~ ~~ ~~ except FileNotFoundError : 
~~ ~~ def _hms ( s ) : 
if s == "-" : 
~~ if s . endswith ( "ms" ) : 
~~~ return float ( s . rstrip ( "ms" ) ) / 1000 
~~ fields = list ( map ( float , re . split ( "[dhms]" , s ) [ : - 1 ] ) ) 
if len ( fields ) == 4 : 
~~~ return fields [ 0 ] * 24 * 3600 + fields [ 1 ] * 3600 + fields [ 2 ] * 60 + fields [ 3 ] 
~~ if len ( fields ) == 3 : 
~~~ return fields [ 0 ] * 3600 + fields [ 1 ] * 60 + fields [ 2 ] 
~~ elif len ( fields ) == 2 : 
~~~ return fields [ 0 ] * 60 + fields [ 1 ] 
~~~ return fields [ 0 ] 
~~ ~~ def _size_coverter ( s ) : 
if s . upper ( ) . endswith ( "KB" ) : 
~~~ return float ( s . rstrip ( "KB" ) ) / 1024 
~~~ return float ( s . rstrip ( "B" ) ) / 1024 / 1024 
~~ elif s . upper ( ) . endswith ( "MB" ) : 
~~~ return float ( s . rstrip ( "MB" ) ) 
~~ elif s . upper ( ) . endswith ( "GB" ) : 
~~~ return float ( s . rstrip ( "GB" ) ) * 1024 
~~ elif s . upper ( ) . endswith ( "TB" ) : 
~~~ return float ( s . rstrip ( "TB" ) ) * 1024 * 1024 
~~~ return float ( s ) 
~~ ~~ def _get_pipeline_processes ( self ) : 
with open ( self . log_file ) as fh : 
process = match . group ( 1 ) 
if any ( [ process . startswith ( x ) for x in self . _blacklist ] ) : 
~~ if process not in self . skip_processes : 
~~~ self . processes [ match . group ( 1 ) ] = { 
"barrier" : "W" , 
"submitted" : set ( ) , 
"finished" : set ( ) , 
"failed" : set ( ) , 
"retry" : set ( ) , 
"cpus" : None , 
"memory" : None 
self . process_tags [ process ] = { } 
self . pipeline_tag = tag_match . group ( 1 ) if tag_match else "?" 
self . pipeline_name = name_match . group ( 1 ) if name_match else "?" 
~~ ~~ ~~ self . content_lines = len ( self . processes ) 
~~ def _clear_inspect ( self ) : 
self . trace_info = defaultdict ( list ) 
self . process_tags = { } 
self . process_stats = { } 
self . samples = [ ] 
self . stored_ids = [ ] 
self . stored_log_ids = [ ] 
self . time_start = None 
self . time_stop = None 
self . execution_command = None 
self . nextflow_version = None 
self . abort_cause = None 
self . _c = 0 
for p in self . processes . values ( ) : 
~~~ p [ "barrier" ] = "W" 
for i in [ "submitted" , "finished" , "failed" , "retry" ] : 
~~~ p [ i ] = set ( ) 
~~ ~~ ~~ def _update_pipeline_status ( self ) : 
~~~ first_line = next ( fh ) 
self . time_start = time_str 
if not self . execution_command : 
~~~ self . execution_command = re . match ( 
~~~ self . execution_command = "Unknown" 
~~ ~~ for line in fh : 
~~~ if not self . nextflow_version : 
~~~ vline = next ( fh ) 
self . nextflow_version = re . match ( 
~~~ self . nextflow_version = "Unknown" 
~~~ self . run_status = "aborted" 
~~~ self . abort_cause = re . match ( 
~~~ self . abort_cause = "Unknown" 
self . time_stop = time_str 
self . send = True 
~~~ self . run_status = "complete" 
~~ ~~ ~~ if self . run_status not in [ "running" , "" ] : 
~~~ self . _clear_inspect ( ) 
sleep ( 5 ) 
self . _get_pipeline_processes ( ) 
~~ self . run_status = "running" 
~~ def _update_tag_status ( self , process , vals ) : 
good_status = [ "COMPLETED" , "CACHED" ] 
for v in list ( vals ) [ : : - 1 ] : 
~~~ p = self . processes [ process ] 
tag = v [ "tag" ] 
if tag in p [ "submitted" ] : 
~~~ p [ "submitted" ] . remove ( tag ) 
if v [ "status" ] in good_status : 
~~~ p [ "finished" ] . add ( tag ) 
~~ elif v [ "status" ] == "FAILED" : 
~~~ if not v [ "work_dir" ] : 
~~~ v [ "work_dir" ] = "" 
~~ self . process_tags [ process ] [ tag ] [ "log" ] = self . _retrieve_log ( join ( v [ "work_dir" ] , ".command.log" ) ) 
p [ "failed" ] . add ( tag ) 
~~ ~~ elif tag in p [ "retry" ] : 
~~~ if v [ "status" ] in good_status : 
~~~ p [ "retry" ] . remove ( tag ) 
p [ "failed" ] . remove ( tag ) 
del self . process_tags [ process ] [ tag ] [ "log" ] 
~~ elif self . run_status == "aborted" : 
~~ ~~ elif v [ "status" ] in good_status : 
~~ if v [ "status" ] not in good_status : 
~~~ if v [ "tag" ] in list ( p [ "submitted" ] ) + list ( p [ "finished" ] ) : 
~~~ vals . remove ( v ) 
~~ ~~ ~~ return vals 
~~ def _update_barrier_status ( self ) : 
if process_m : 
~~~ process = process_m . group ( 1 ) 
if process in self . processes : 
~~~ self . processes [ process ] [ "barrier" ] = "C" 
~~ ~~ ~~ ~~ ~~ ~~ def _retrieve_log ( path ) : 
~~ with open ( path ) as fh : 
~~~ return fh . readlines ( ) 
~~ ~~ def _update_trace_info ( self , fields , hm ) : 
process = fields [ hm [ "process" ] ] 
if process not in self . processes : 
~~ info = dict ( ( column , fields [ pos ] ) for column , pos in hm . items ( ) ) 
process_tag_headers = [ "realtime" , "rss" , "rchar" , "wchar" ] 
for h in process_tag_headers : 
~~~ if info [ "tag" ] not in self . process_tags [ process ] : 
~~~ timestart = info [ "start" ] . split ( ) [ 1 ] 
~~~ timestart = None 
~~ self . process_tags [ process ] [ info [ "tag" ] ] = { 
"workdir" : self . _expand_path ( info [ "hash" ] ) , 
"start" : timestart 
~~ if h in info and info [ "tag" ] != "-" : 
~~~ if h != "realtime" and info [ h ] != "-" : 
~~~ self . process_tags [ process ] [ info [ "tag" ] ] [ h ] = round ( self . _size_coverter ( info [ h ] ) , 2 ) 
~~~ self . process_tags [ process ] [ info [ "tag" ] ] [ h ] = info [ h ] 
~~ ~~ ~~ if "cpus" in info and not self . processes [ process ] [ "cpus" ] : 
~~~ self . processes [ process ] [ "cpus" ] = info [ "cpus" ] 
~~ if "memory" in info and not self . processes [ process ] [ "memory" ] : 
~~~ self . processes [ process ] [ "memory" ] = self . _size_coverter ( 
info [ "memory" ] ) 
~~~ self . processes [ process ] [ "memory" ] = None 
~~ ~~ if info [ "hash" ] in self . stored_ids : 
~~ if "hash" in info : 
~~~ hs = info [ "hash" ] 
info [ "work_dir" ] = self . _expand_path ( hs ) 
~~ if "tag" in info : 
~~~ tag = info [ "tag" ] 
if tag != "-" and tag not in self . samples and tag . split ( ) [ 0 ] not in self . samples : 
~~~ self . samples . append ( tag ) 
~~ ~~ self . trace_info [ process ] . append ( info ) 
self . stored_ids . append ( info [ "hash" ] ) 
~~ def _update_process_resources ( self , process , vals ) : 
resources = [ "cpus" ] 
for r in resources : 
~~~ if not self . processes [ process ] [ r ] : 
~~~ self . processes [ process ] [ r ] = vals [ 0 ] [ "cpus" ] 
~~ ~~ ~~ ~~ def _cpu_load_parser ( self , cpus , cpu_per , t ) : 
~~~ _cpus = float ( cpus ) 
_cpu_per = float ( cpu_per . replace ( "," , "." ) . replace ( "%" , "" ) ) 
hours = self . _hms ( t ) / 60 / 24 
return ( ( _cpu_per / ( 100 * _cpus ) ) * _cpus ) * hours 
~~ ~~ def _assess_resource_warnings ( self , process , vals ) : 
cpu_warnings = { } 
mem_warnings = { } 
for i in vals : 
~~~ expected_load = float ( i [ "cpus" ] ) * 100 
cpu_load = float ( i [ "%cpu" ] . replace ( "," , "." ) . replace ( "%" , "" ) ) 
if expected_load * 0.9 > cpu_load > expected_load * 1.10 : 
~~~ cpu_warnings [ i [ "tag" ] ] = { 
"expected" : expected_load , 
"value" : cpu_load 
~~ ~~ except ( ValueError , KeyError ) : 
~~~ rss = self . _size_coverter ( i [ "rss" ] ) 
mem_allocated = self . _size_coverter ( i [ "memory" ] ) 
if rss > mem_allocated * 1.10 : 
~~~ mem_warnings [ i [ "tag" ] ] = { 
"expected" : mem_allocated , 
"value" : rss 
~~ ~~ return cpu_warnings , mem_warnings 
~~ def _update_process_stats ( self ) : 
for process , vals in self . trace_info . items ( ) : 
~~~ vals = self . _update_tag_status ( process , vals ) 
self . _update_process_resources ( process , vals ) 
self . process_stats [ process ] = { } 
inst = self . process_stats [ process ] 
inst [ "completed" ] = "{}" . format ( 
len ( [ x for x in vals if x [ "status" ] in good_status ] ) ) 
~~~ time_array = [ self . _hms ( x [ "realtime" ] ) for x in vals ] 
mean_time = round ( sum ( time_array ) / len ( time_array ) , 1 ) 
mean_time_str = strftime ( '%H:%M:%S' , gmtime ( mean_time ) ) 
inst [ "realtime" ] = mean_time_str 
~~~ inst [ "realtime" ] = "-" 
~~~ cpu_hours = [ self . _cpu_load_parser ( 
x [ "cpus" ] , x [ "%cpu" ] , x [ "realtime" ] ) for x in vals ] 
inst [ "cpuhour" ] = round ( sum ( cpu_hours ) , 2 ) 
~~~ inst [ "cpuhour" ] = "-" 
~~ inst [ "cpu_warnings" ] , inst [ "mem_warnings" ] = self . _assess_resource_warnings ( process , vals ) 
~~~ rss_values = [ self . _size_coverter ( x [ "rss" ] ) for x in vals 
if x [ "rss" ] != "-" ] 
if rss_values : 
~~~ max_rss = round ( max ( rss_values ) ) 
rss_str = self . _size_compress ( max_rss ) 
~~~ rss_str = "-" 
~~ inst [ "maxmem" ] = rss_str 
~~~ inst [ "maxmem" ] = "-" 
~~~ rchar_values = [ self . _size_coverter ( x [ "rchar" ] ) for x in vals 
if x [ "rchar" ] != "-" ] 
if rchar_values : 
~~~ avg_rchar = round ( sum ( rchar_values ) / len ( rchar_values ) ) 
rchar_str = self . _size_compress ( avg_rchar ) 
~~~ rchar_str = "-" 
~~ inst [ "avgread" ] = rchar_str 
~~~ wchar_values = [ self . _size_coverter ( x [ "wchar" ] ) for x in vals 
if x [ "wchar" ] != "-" ] 
if wchar_values : 
~~~ avg_wchar = round ( sum ( wchar_values ) / len ( wchar_values ) ) 
wchar_str = self . _size_compress ( avg_wchar ) 
~~~ wchar_str = "-" 
~~ inst [ "avgwrite" ] = wchar_str 
~~ ~~ def trace_parser ( self ) : 
size_stamp = os . path . getsize ( self . trace_file ) 
self . trace_retry = 0 
if size_stamp and size_stamp == self . trace_sizestamp : 
self . trace_sizestamp = size_stamp 
~~ with open ( self . trace_file ) as fh : 
~~~ header = next ( fh ) . strip ( ) 
while not header : 
~~ hm = self . _header_mapping ( header ) 
~~~ if line . strip ( ) == "" : 
~~ fields = line . strip ( ) . split ( "\\t" ) 
if fields [ hm [ "task_id" ] ] in self . stored_ids : 
~~ self . _update_trace_info ( fields , hm ) 
~~ ~~ self . _update_process_stats ( ) 
self . _update_barrier_status ( ) 
~~ def log_parser ( self ) : 
size_stamp = os . path . getsize ( self . log_file ) 
self . log_retry = 0 
if size_stamp and size_stamp == self . log_sizestamp : 
self . log_sizestamp = size_stamp 
~~~ m = re . match ( r , line ) 
~~ time_start = m . group ( 1 ) 
workdir = m . group ( 2 ) 
process = m . group ( 3 ) 
tag = m . group ( 4 ) 
if time_start + tag not in self . stored_log_ids : 
~~~ self . stored_log_ids . append ( time_start + tag ) 
~~ if process not in self . processes : 
~~ p = self . processes [ process ] 
if tag in list ( p [ "finished" ] ) + list ( p [ "retry" ] ) : 
~~~ p [ "retry" ] . add ( tag ) 
~~ p [ "barrier" ] = "R" 
if tag not in p [ "submitted" ] : 
~~~ p [ "submitted" ] . add ( tag ) 
if tag not in self . process_tags [ process ] : 
~~~ self . process_tags [ process ] [ tag ] = { 
"workdir" : self . _expand_path ( workdir ) , 
"start" : time_start 
~~ elif not self . process_tags [ process ] [ tag ] [ "start" ] : 
~~~ self . process_tags [ process ] [ tag ] [ "start" ] = time_start 
~~ ~~ ~~ ~~ ~~ self . _update_pipeline_status ( ) 
~~ def update_inspection ( self ) : 
~~~ self . log_parser ( ) 
~~ except ( FileNotFoundError , StopIteration ) as e : 
self . log_retry += 1 
if self . log_retry == self . MAX_RETRIES : 
~~~ self . trace_parser ( ) 
self . trace_retry += 1 
if self . trace_retry == self . MAX_RETRIES : 
~~ ~~ ~~ def display_overview ( self ) : 
stay_alive = True 
self . screen = curses . initscr ( ) 
self . screen . keypad ( True ) 
self . screen . nodelay ( - 1 ) 
curses . cbreak ( ) 
curses . noecho ( ) 
curses . start_color ( ) 
self . screen_lines = self . screen . getmaxyx ( ) [ 0 ] 
~~~ while stay_alive : 
~~~ self . _curses_keybindings ( ) 
self . update_inspection ( ) 
self . flush_overview ( ) 
sleep ( self . refresh_rate ) 
~~~ sys . stderr . write ( colored_print ( 
"reachable!" , "red_bold" ) ) 
~~~ sys . stderr . write ( str ( e ) ) 
~~~ curses . nocbreak ( ) 
self . screen . keypad ( 0 ) 
~~ ~~ def _updown ( self , direction ) : 
if direction == "up" and self . top_line != 0 : 
~~~ self . top_line -= 1 
~~ elif direction == "down" and self . screen . getmaxyx ( ) [ 0 ] + self . top_line <= self . content_lines + 3 : 
~~~ self . top_line += 1 
~~ ~~ def _rightleft ( self , direction ) : 
if direction == "left" and self . padding != 0 : 
~~~ self . padding -= 1 
~~ if direction == "right" and self . screen . getmaxyx ( ) [ 1 ] + self . padding < self . max_width : 
~~~ self . padding += 1 
~~ ~~ def flush_overview ( self ) : 
colors = { 
"W" : 1 , 
"R" : 2 , 
"C" : 3 
pc = { 
"running" : 3 , 
"complete" : 3 , 
"aborted" : 4 , 
"error" : 4 
curses . init_pair ( 1 , curses . COLOR_WHITE , curses . COLOR_BLACK ) 
curses . init_pair ( 2 , curses . COLOR_BLUE , curses . COLOR_BLACK ) 
curses . init_pair ( 3 , curses . COLOR_GREEN , curses . COLOR_BLACK ) 
curses . init_pair ( 4 , curses . COLOR_MAGENTA , curses . COLOR_BLACK ) 
height , width = self . screen . getmaxyx ( ) 
win = curses . newpad ( height , 2000 ) 
win . addstr ( 0 , 0 , header ) 
win . addstr ( 0 , len ( header ) , self . run_status , 
curses . color_pair ( pc [ self . run_status ] ) ) 
sum ( [ len ( x [ "submitted" ] ) for x in self . processes . values ( ) ] ) 
sum ( [ len ( x [ "failed" ] ) for x in self . processes . values ( ) ] ) 
sum ( [ len ( x [ "retry" ] ) for x in self . processes . values ( ) ] ) 
sum ( [ len ( x [ "finished" ] ) for x in self . processes . values ( ) ] ) 
win . addstr ( 
1 , 0 , submission_str , curses . color_pair ( 1 ) 
headers = [ "" , "Process" , "Running" , "Complete" , "Error" , 
self . max_width = len ( header_str ) 
win . addstr ( 3 , 0 , header_str , curses . A_UNDERLINE | curses . A_REVERSE ) 
top = self . top_line 
bottom = self . screen_lines - 4 + self . top_line 
for p , process in enumerate ( 
list ( self . processes . keys ( ) ) [ top : bottom ] ) : 
~~~ if process not in self . process_stats : 
~~~ vals = [ "-" ] * 8 
txt_fmt = curses . A_NORMAL 
~~~ ref = self . process_stats [ process ] 
vals = [ ref [ "completed" ] , 
len ( self . processes [ process ] [ "failed" ] ) , 
ref [ "realtime" ] , 
ref [ "maxmem" ] , ref [ "avgread" ] , 
ref [ "avgwrite" ] ] 
txt_fmt = curses . A_BOLD 
~~ proc = self . processes [ process ] 
if proc [ "retry" ] : 
~~~ completed = "{}({})" . format ( len ( proc [ "submitted" ] ) , 
len ( proc [ "retry" ] ) ) 
~~~ completed = "{}" . format ( len ( proc [ "submitted" ] ) ) 
~~ win . addstr ( 
proc [ "barrier" ] , 
process , 
completed , 
* vals ) , 
curses . color_pair ( colors [ proc [ "barrier" ] ] ) | txt_fmt ) 
~~ win . clrtoeol ( ) 
win . refresh ( 0 , self . padding , 0 , 0 , height - 1 , width - 1 ) 
~~ def _get_log_lines ( self , n = 300 ) : 
~~~ last_lines = fh . readlines ( ) [ - n : ] 
~~ return last_lines 
~~ def _prepare_static_info ( self ) : 
pipeline_files = { } 
with open ( join ( self . workdir , self . pipeline_name ) ) as fh : 
~~~ pipeline_files [ "pipelineFile" ] = fh . readlines ( ) 
~~ nf_config = join ( self . workdir , "nextflow.config" ) 
if os . path . exists ( nf_config ) : 
~~~ with open ( nf_config ) as fh : 
~~~ pipeline_files [ "configFile" ] = fh . readlines ( ) 
~~ ~~ configs = { 
"params.config" : "paramsFile" , 
"resources.config" : "resourcesFile" , 
"containers.config" : "containersFile" , 
"user.config" : "userFile" , 
for config , key in configs . items ( ) : 
~~~ cfile = join ( self . workdir , config ) 
if os . path . exists ( cfile ) : 
~~~ with open ( cfile ) as fh : 
~~~ pipeline_files [ key ] = fh . readlines ( ) 
~~ ~~ ~~ return pipeline_files 
~~ def _dag_file_to_dict ( self ) : 
~~~ dag_file = open ( os . path . join ( self . workdir , ".treeDag.json" ) ) 
dag_json = json . load ( dag_file ) 
~~ except ( FileNotFoundError , json . decoder . JSONDecodeError ) : 
~~~ logger . warning ( colored_print ( 
"red_bold" ) ) 
dag_json = { } 
~~ return dag_json 
~~ def _get_run_hash ( self ) : 
pipeline_path = get_nextflow_filepath ( self . log_file ) 
pipeline_hash = hashlib . md5 ( ) 
with open ( pipeline_path , "rb" ) as fh : 
~~~ for chunk in iter ( lambda : fh . read ( 4096 ) , b"" ) : 
~~~ pipeline_hash . update ( chunk ) 
~~ ~~ workdir = self . workdir . encode ( "utf8" ) 
hostname = socket . gethostname ( ) . encode ( "utf8" ) 
hardware_addr = str ( uuid . getnode ( ) ) . encode ( "utf8" ) 
dir_hash = hashlib . md5 ( workdir + hostname + hardware_addr ) 
return pipeline_hash . hexdigest ( ) + dir_hash . hexdigest ( ) 
~~ def get_nextflow_filepath ( log_file ) : 
~~~ line = fh . readline ( ) 
~~~ pipeline_path = re . match ( ".*\\s(.*.nf).*" , line ) . group ( 1 ) 
return pipeline_path 
~~ ~~ ~~ ~~ def main ( sample_id , assembly , min_size ) : 
f_open = open ( assembly , "rU" ) 
entry = ( x [ 1 ] for x in groupby ( f_open , lambda line : line [ 0 ] == ">" ) ) 
success = 0 
for header in entry : 
~~~ headerStr = header . __next__ ( ) [ 1 : ] . strip ( ) 
seq = "" . join ( s . strip ( ) for s in entry . __next__ ( ) ) 
if len ( seq ) >= min_size : 
success += 1 
~~ ~~ ~~ f_open . close ( ) 
~~ def main ( sample_id , trace_file , workdir ) : 
stats_suffix = ".stats.json" 
stats_path = join ( workdir , sample_id + stats_suffix ) 
trace_path = join ( workdir , trace_file ) 
stats_array = get_previous_stats ( stats_path ) 
with open ( trace_path ) as fh : 
~~~ header = next ( fh ) . strip ( ) . split ( ) 
~~~ fields = line . strip ( ) . split ( "\\t" ) 
if tag in fields [ 2 ] and fields [ 3 ] == "COMPLETED" : 
line ) ) 
current_json = get_json_info ( fields , header ) 
stats_array [ fields [ 0 ] ] = current_json 
~~ ~~ ~~ with open ( join ( stats_path ) , "w" ) as fh , open ( ".report.json" , "w" ) as rfh : 
~~~ fh . write ( json . dumps ( stats_array , separators = ( "," , ":" ) ) ) 
rfh . write ( json . dumps ( stats_array , separators = ( "," , ":" ) ) ) 
~~ ~~ def brew_innuendo ( args ) : 
automatic_pipeline = Innuendo ( ) 
if not args . tasks : 
automatic_pipeline . process_descriptions . keys ( ) ) 
~~~ input_processes = args . tasks 
~~ validated = automatic_pipeline . validate_pipeline ( input_processes ) 
if not validated : 
~~ pipeline_string = automatic_pipeline . run_auto_pipeline ( input_processes ) 
return pipeline_string 
~~ def brew_recipe ( recipe_name ) : 
prefix = "{}." . format ( recipes . __name__ ) 
for importer , modname , _ in pkgutil . iter_modules ( recipes . __path__ , prefix ) : 
~~~ _module = importer . find_module ( modname ) . load_module ( modname ) 
_recipe_classes = [ cls for cls in _module . __dict__ . values ( ) if 
isinstance ( cls , type ) ] 
for cls in _recipe_classes : 
~~~ recipe_cls = cls ( ) 
if getattr ( recipe_cls , "name" , None ) == recipe_name : 
~~~ return recipe_cls . brew ( ) 
~~ ~~ ~~ logger . error ( 
~~ def list_recipes ( full = False ) : 
logger . info ( colored_print ( 
"green_bold" ) ) 
if hasattr ( recipe_cls , "name" ) : 
if full : 
~~ ~~ ~~ ~~ sys . exit ( 0 ) 
~~ def validate_pipeline ( pipeline_string ) : 
if "(" in pipeline_string or ")" in pipeline_string or "|" in pipeline_string : 
~~ def build_upstream ( self , process_descriptions , task , all_tasks , 
task_pipeline , 
count_forks , total_tasks , forks ) : 
if task in process_descriptions : 
~~~ if process_descriptions [ task ] [ 1 ] is not None : 
~~~ if len ( process_descriptions [ task ] [ 1 ] . split ( "|" ) ) > 1 : 
~~~ local_forks = process_descriptions [ task ] [ 1 ] . split ( "|" ) 
for local_fork in local_forks : 
~~~ if local_fork in total_tasks : 
~~~ count_forks += 1 
task_pipeline . insert ( 
process_descriptions [ task ] [ 1 ] 
self . define_pipeline_string ( 
process_descriptions , 
local_fork , 
count_forks , 
total_tasks , 
forks 
~~ ~~ return task_pipeline 
~~~ if process_descriptions [ task ] [ 1 ] in total_tasks : 
~~~ task_pipeline . insert ( 
process_descriptions [ task ] [ 1 ] . split ( "|" ) [ 0 ] 
self . build_upstream ( 
process_descriptions [ task ] [ 1 ] . split ( "|" ) [ 0 ] , 
all_tasks , 
process_descriptions [ task ] [ 1 ] , task ) , "red_bold" 
sys . exit ( ) 
~~ return task_pipeline 
~~~ return task_pipeline 
~~ ~~ ~~ def build_downstream ( self , process_descriptions , task , all_tasks , 
~~~ if process_descriptions [ task ] [ 2 ] is not None : 
~~~ if len ( process_descriptions [ task ] [ 2 ] . split ( "|" ) ) > 1 : 
~~~ local_forks = process_descriptions [ task ] [ 2 ] . split ( "|" ) 
task_pipeline . append ( process_descriptions [ task ] [ 2 ] ) 
~~~ if process_descriptions [ task ] [ 2 ] in total_tasks : 
~~~ task_pipeline . append ( process_descriptions [ task ] [ 2 ] . split ( "|" ) [ 0 ] ) 
self . build_downstream ( 
process_descriptions [ task ] [ 2 ] . split ( "|" ) [ 0 ] , 
~~ ~~ ~~ def define_pipeline_string ( self , process_descriptions , tasks , 
check_upstream , 
check_downstream , count_forks , total_tasks , 
forks ) : 
tasks_array = tasks . split ( ) 
for task_unsplit in tasks_array : 
~~~ task = task_unsplit . split ( "=" ) [ 0 ] 
if task not in process_descriptions . keys ( ) : 
colored_print ( 
"red_bold" 
~~~ process_split = task_unsplit . split ( "=" ) 
if len ( process_split ) > 1 : 
~~~ self . process_to_id [ process_split [ 0 ] ] = process_split [ 1 ] 
~~ ~~ if not bool ( [ x for x in forks if task in x ] ) and not bool ( [ y for y in forks if process_descriptions [ task ] [ 2 ] in y ] ) : 
~~~ task_pipeline = [ ] 
~~~ if check_upstream : 
~~~ task_pipeline = self . build_upstream ( 
tasks_array , 
~~ task_pipeline . append ( task ) 
if check_downstream : 
~~~ task_pipeline = self . build_downstream ( 
~~ ~~ forks . append ( list ( OrderedDict . fromkeys ( task_pipeline ) ) ) 
~~ elif bool ( [ y for y in forks if process_descriptions [ task ] [ 2 ] in y ] ) : 
~~~ for fork in forks : 
~~~ if task not in fork : 
~~~ dependent_index = fork . index ( process_descriptions [ task ] [ 2 ] ) 
fork . insert ( dependent_index , task ) 
~~ ~~ ~~ ~~ ~~ for i in range ( 0 , len ( forks ) ) : 
~~~ for j in range ( 0 , len ( forks [ i ] ) ) : 
~~~ if len ( forks [ i ] [ j ] . split ( "|" ) ) > 1 : 
~~~ forks [ i ] [ j ] = forks [ i ] [ j ] . split ( "|" ) 
tmp_fork = [ ] 
for s in forks [ i ] [ j ] : 
~~~ if s in total_tasks : 
~~~ tmp_fork . append ( s ) 
~~ ~~ forks [ i ] [ j ] = tmp_fork 
~~ ~~ ~~ return forks 
~~ def build_pipeline_string ( self , forks ) : 
final_forks = [ ] 
for i in range ( 0 , len ( forks ) ) : 
~~~ needs_merge = [ False , 0 , 0 , 0 , 0 , "" ] 
is_merged = False 
for i2 in range ( 0 , len ( forks [ i ] ) ) : 
~~~ for j in range ( i , len ( forks ) ) : 
~~~ needs_merge [ 0 ] = False 
for j2 in range ( 0 , len ( forks [ j ] ) ) : 
~~~ j2_fork = forks [ j ] [ j2 ] . split ( "|" ) 
~~~ j2_fork = forks [ j ] [ j2 ] 
~~ if forks [ i ] [ i2 ] in j2_fork and ( i2 == 0 or j2 == 0 ) and i != j : 
~~~ needs_merge [ 0 ] = True 
needs_merge [ 1 ] = i 
needs_merge [ 2 ] = i2 
needs_merge [ 3 ] = j 
needs_merge [ 4 ] = j2 
needs_merge [ 5 ] = forks [ i ] [ i2 ] 
~~ ~~ if needs_merge [ 0 ] : 
~~~ index_merge_point = forks [ needs_merge [ 3 ] ] [ - 1 ] . index ( needs_merge [ 5 ] ) 
if needs_merge [ 2 ] == 0 : 
~~~ if len ( forks [ needs_merge [ 3 ] ] [ - 1 ] ) < 2 : 
~~~ forks [ needs_merge [ 3 ] ] = forks [ needs_merge [ 3 ] ] [ : - 1 ] + forks [ needs_merge [ 1 ] ] [ : : ] 
~~~ forks [ needs_merge [ 3 ] ] [ - 1 ] [ index_merge_point ] = forks [ needs_merge [ 1 ] ] 
~~ ~~ elif needs_merge [ 4 ] == 0 : 
~~ ~~ is_merged = True 
~~ ~~ ~~ if needs_merge [ 0 ] is not None and not is_merged : 
~~~ if bool ( [ nf for nf in forks [ i ] if "|" in nf ] ) : 
~~ final_forks . append ( forks [ i ] ) 
~~ ~~ if len ( final_forks ) == 1 : 
~~~ final_forks = str ( final_forks [ 0 ] ) 
if pipeline_string [ - 1 ] == "|" : 
~~~ pipeline_string = pipeline_string [ : - 1 ] 
for key , val in self . process_to_id . items ( ) : 
~~~ pipeline_string = pipeline_string . replace ( to_search . format ( key ) , 
to_replace . format ( key , val ) ) 
~~ return pipeline_string 
~~ def run_auto_pipeline ( self , tasks ) : 
self . forks = self . define_pipeline_string ( 
self . process_descriptions , 
self . count_forks , 
self . forks 
self . pipeline_string = self . build_pipeline_string ( self . forks ) 
return self . pipeline_string 
~~ def _get_component_str ( component , params = None , directives = None ) : 
final_directives = { } 
if directives : 
~~~ final_directives = directives 
~~ if params : 
~~~ final_directives [ "params" ] = params 
~~ if final_directives : 
~~~ return "{}={}" . format ( 
component , json . dumps ( final_directives , separators = ( "," , ":" ) ) ) 
~~~ return component 
~~ ~~ def write_report ( storage_dic , output_file , sample_id ) : 
with open ( output_file , "w" ) as fh , open ( ".report.json" , "w" ) as json_rep : 
"bad_reads\\\\n" ) 
for sample , vals in storage_dic . items ( ) : 
~~~ fh . write ( "{},{}\\\\n" . format ( 
sample , "," . join ( [ str ( x ) for x in vals . values ( ) ] ) ) ) 
json_dic = { 
{ "header" : "trimmed" , 
"value" : vals [ "total_trim_perc" ] , 
"table" : "qc" , 
"sparkline" : vals [ "clean_len" ] 
"badReads" : vals [ "bad_reads" ] 
json_rep . write ( json . dumps ( json_dic , separators = ( "," , ":" ) ) ) 
~~ ~~ ~~ def main ( log_files ) : 
log_storage = OrderedDict ( ) 
for log in log_files : 
~~~ log_id = log . rstrip ( "_trimlog.txt" ) 
log_storage [ log_id ] = parse_log ( log ) 
os . remove ( log ) 
~~ write_report ( log_storage , "trimmomatic_report.csv" , log_id ) 
~~ def fix_contig_names ( asseembly_path ) : 
fixed_assembly = "fixed_assembly.fa" 
with open ( asseembly_path ) as in_hf , open ( fixed_assembly , "w" ) as ou_fh : 
~~~ for line in in_hf : 
~~~ if line . startswith ( ">" ) : 
ou_fh . write ( fixed_line ) 
~~~ ou_fh . write ( line ) 
~~ ~~ ~~ return fixed_assembly 
~~ def clean_up ( fastq ) : 
for fq in fastq : 
~~ ~~ ~~ def parse_files ( self , fls ) : 
for f in fls : 
~~~ self . _parser ( f ) 
~~ ~~ ~~ def _parser ( self , fl ) : 
with open ( fl ) as fh : 
~~~ if line . startswith ( "#" ) or line . strip ( ) == "" : 
~~~ coverage = float ( fields [ 8 ] ) 
~~~ coverage = None 
~~~ identity = float ( fields [ 9 ] ) 
~~~ identity = None 
~~~ accession = fields [ 11 ] 
~~~ accession = None 
~~ self . storage [ self . _key ] = { 
"log_file" : os . path . basename ( fl ) , 
"infile" : fields [ 0 ] , 
"reference" : fields [ 1 ] , 
"seq_range" : ( int ( fields [ 2 ] ) , int ( fields [ 3 ] ) ) , 
"gene" : fields [ 4 ] , 
"accession" : accession , 
"database" : fields [ 10 ] , 
"coverage" : coverage , 
"identity" : identity 
self . _key += 1 
~~ ~~ ~~ def iter_filter ( self , filters , databases = None , fields = None , 
filter_behavior = "and" ) : 
if filter_behavior not in [ "and" , "or" ] : 
~~ for dic in self . storage . values ( ) : 
~~~ _pass = False 
flag = [ ] 
if databases : 
~~~ if dic [ "database" ] not in databases : 
~~ ~~ for f in filters : 
~~~ val = dic [ f [ 0 ] ] 
if not self . _test_truth ( val , f [ 1 ] , f [ 2 ] ) : 
~~~ flag . append ( False ) 
~~~ flag . append ( True ) 
~~ ~~ if filter_behavior == "and" : 
~~~ if all ( flag ) : 
~~~ _pass = True 
~~ ~~ elif filter_behavior == "or" : 
~~~ if any ( flag ) : 
~~ ~~ if _pass : 
~~~ if fields : 
~~~ yield dict ( ( x , y ) for x , y in dic . items ( ) if x in fields ) 
~~~ yield dic 
~~ ~~ ~~ ~~ def _get_contig_id ( contig_str ) : 
contig_id = contig_str 
~~~ contig_id = re . search ( ".*NODE_([0-9]*)_.*" , contig_str ) . group ( 1 ) 
~~~ contig_id = re . search ( ".*Contig_([0-9]*)_.*" , contig_str ) . group ( 1 ) 
~~ return contig_id 
~~ def get_plot_data ( self ) : 
json_dic = { "plotData" : [ ] } 
sample_dic = { } 
sample_assembly_map = { } 
for entry in self . storage . values ( ) : 
~~~ sample_id = re . match ( "(.*)_abr" , entry [ "log_file" ] ) . groups ( ) [ 0 ] 
if sample_id not in sample_dic : 
~~~ sample_dic [ sample_id ] = { } 
~~ contig_id = self . _get_contig_id ( entry [ "reference" ] ) 
database = entry [ "database" ] 
if database not in sample_dic [ sample_id ] : 
~~~ sample_dic [ sample_id ] [ database ] = [ ] 
~~ if sample_id not in sample_assembly_map : 
~~~ sample_assembly_map [ sample_id ] = entry [ "infile" ] 
~~ sample_dic [ sample_id ] [ database ] . append ( 
{ "contig" : contig_id , 
"seqRange" : entry [ "seq_range" ] , 
"gene" : entry [ "gene" ] . replace ( "\ , "" ) , 
"accession" : entry [ "accession" ] , 
"coverage" : entry [ "coverage" ] , 
"identity" : entry [ "identity" ] , 
~~ for sample , data in sample_dic . items ( ) : 
~~~ json_dic [ "plotData" ] . append ( 
"sample" : sample , 
"data" : { "abricateXrange" : data } , 
"assemblyFile" : sample_assembly_map [ sample ] 
~~ return json_dic 
~~ def write_report_data ( self ) : 
json_plot = self . get_plot_data ( ) 
json_table = self . get_table_data ( ) 
json_dic = { ** json_plot , ** json_table } 
~~ ~~ def main ( sample_id , assembly_file , coverage_bp_file = None ) : 
assembly_obj = Assembly ( assembly_file , sample_id ) 
assembly_obj . get_summary_stats ( "{}_assembly_report.csv" . format ( sample_id ) ) 
size_dist = [ len ( x ) for x in assembly_obj . contigs . values ( ) ] 
{ "header" : "Contigs" , 
"value" : assembly_obj . summary_info [ "ncontigs" ] , 
"value" : assembly_obj . summary_info [ "total_len" ] , 
"size_dist" : size_dist 
if coverage_bp_file : 
~~~ window = 2000 
gc_sliding_data = assembly_obj . get_gc_sliding ( window = window ) 
cov_sliding_data = assembly_obj . get_coverage_sliding ( coverage_bp_file , 
total_bp = sum ( 
[ sum ( x ) for x in assembly_obj . contig_coverage . values ( ) ] 
json_dic [ "plotData" ] [ 0 ] [ "data" ] [ "genomeSliding" ] = { 
"gcData" : gc_sliding_data , 
"covData" : cov_sliding_data , 
"window" : window , 
"xbars" : assembly_obj . _get_window_labels ( window ) , 
"assemblyFile" : os . path . basename ( assembly_file ) 
json_dic [ "plotData" ] [ 0 ] [ "data" ] [ "sparkline" ] = total_bp 
"{}" . format ( traceback . format_exc ( ) ) ) 
~~ ~~ def _parse_assembly ( self , assembly_file ) : 
~~~ header = None 
~~~ header = line [ 1 : ] . strip ( ) 
self . contigs [ header ] = [ ] 
~~~ self . contigs [ header ] . append ( line . strip ( ) ) 
~~ ~~ self . contigs = OrderedDict ( 
( header , "" . join ( seq ) ) for header , seq in self . contigs . items ( ) ) 
~~ ~~ def get_summary_stats ( self , output_csv = None ) : 
contig_size_list = [ ] 
self . summary_info [ "ncontigs" ] = len ( self . contigs ) 
for contig_id , sequence in self . contigs . items ( ) : 
contig_len = len ( sequence ) 
contig_size_list . append ( contig_len ) 
self . summary_info [ "total_len" ] += contig_len 
self . summary_info [ "avg_gc" ] . append ( 
sum ( map ( sequence . count , [ "G" , "C" ] ) ) / contig_len 
self . summary_info [ "missing_data" ] += sequence . count ( "N" ) 
self . summary_info [ "avg_contig_size" ] = sum ( contig_size_list ) / len ( contig_size_list ) 
self . summary_info [ "avg_gc" ] = sum ( self . summary_info [ "avg_gc" ] ) / len ( self . summary_info [ "avg_gc" ] ) 
cum_size = 0 
for l in sorted ( contig_size_list , reverse = True ) : 
~~~ cum_size += l 
if cum_size >= self . summary_info [ "total_len" ] / 2 : 
~~~ self . summary_info [ "n50" ] = l 
~~ ~~ if output_csv : 
with open ( output_csv , "w" ) as fh : 
self . sample , "," . join ( 
[ str ( x ) for x in self . summary_info . values ( ) ] ) ) 
fh . write ( summary_line ) 
~~ ~~ ~~ def _get_window_labels ( self , window ) : 
if not self . summary_info : 
~~~ self . get_summary_stats ( ) 
~~ c = 0 
xbars = [ ] 
for contig , seq in self . contigs . items ( ) : 
~~~ contig_id = self . _get_contig_id ( contig ) 
self . contig_boundaries [ contig_id ] = [ c , c + len ( seq ) ] 
c += len ( seq ) 
xbars . append ( ( contig_id , c , contig ) ) 
~~ return xbars 
~~ def _gc_prop ( s , length ) : 
gc = sum ( map ( s . count , [ "c" , "g" ] ) ) 
return gc / length 
~~ def get_gc_sliding ( self , window = 2000 ) : 
gc_res = [ ] 
complete_seq = "" . join ( self . contigs . values ( ) ) . lower ( ) 
for i in range ( 0 , len ( complete_seq ) , window ) : 
~~~ seq_window = complete_seq [ i : i + window ] 
gc_res . append ( round ( self . _gc_prop ( seq_window , len ( seq_window ) ) , 2 ) ) 
~~ return gc_res 
~~ def main ( sample_id , fastq_pair , clear ) : 
if "_trim." in fastq_pair [ 0 ] : 
~~~ sample_id += "_trim" 
~~ version = __get_version_skesa ( ) [ "version" ] 
output_file = "{}_skesa{}.fasta" . format ( sample_id , version . replace ( "." , "" ) ) 
"skesa" , 
"--fastq" , 
"{},{}" . format ( fastq_pair [ 0 ] , fastq_pair [ 1 ] ) , 
"--gz" , 
"--use_paired_ends" , 
"--cores" , 
"${task.cpus}" 
~~~ p = subprocess . Popen ( cli , stdout = fh , stderr = PIPE ) 
~~ stdout , stderr = p . communicate ( ) 
stdout = stdout . decode ( "utf8" ) 
stdout = str ( stdout ) 
if clear == "true" and os . path . exists ( output_file ) : 
~~~ clean_up ( fastq_pair ) 
~~ with open ( ".status" , "w" ) as fh : 
~~~ fh . write ( "error" ) 
raise SystemExit ( p . returncode ) 
~~~ fh . write ( "pass" ) 
~~ ~~ ~~ def write_json_report ( sample_id , data1 , data2 ) : 
parser_map = { 
"base_sequence_quality" : { "status" : None , "data" : [ ] } , 
"sequence_quality" : { "status" : None , "data" : [ ] } , 
"base_gc_content" : { "status" : None , "data" : [ ] } , 
"base_n_content" : { "status" : None , "data" : [ ] } , 
"sequence_length_dist" : { "status" : None , "data" : [ ] } , 
"per_base_sequence_content" : { "status" : None , "data" : [ ] } 
for cat , start_str in parser_map . items ( ) : 
~~~ if cat == "per_base_sequence_content" : 
~~~ fs = 1 
fe = 5 
fe = 2 
~~ report1 , status1 = _get_quality_stats ( data1 , start_str , 
field_start = fs , field_end = fe ) 
report2 , status2 = _get_quality_stats ( data2 , start_str , 
status = None 
for i in [ "fail" , "warn" , "pass" ] : 
~~~ if i in [ status1 , status2 ] : 
~~~ status = i 
~~ ~~ json_dic [ "plotData" ] [ 0 ] [ "data" ] [ cat ] [ "data" ] = [ report1 , report2 ] 
json_dic [ "plotData" ] [ 0 ] [ "data" ] [ cat ] [ "status" ] = status 
~~ def get_trim_index ( biased_list ) : 
if set ( biased_list ) == { False } : 
~~ if set ( biased_list [ : 5 ] ) == { False } : 
~~ for i , val in enumerate ( biased_list ) : 
~~~ if val and set ( biased_list [ i + 1 : i + 3 ] ) == { False } : 
~~~ return i + 1 
~~ ~~ return len ( biased_list ) 
~~ def trim_range ( data_file ) : 
"{}" . format ( target_nuc_bias ) ) 
gather = False 
biased = [ ] 
with open ( data_file ) as fh : 
~~~ if line . startswith ( target_nuc_bias ) : 
next ( fh ) 
gather = True 
~~ elif line . startswith ( ">>END_MODULE" ) and gather : 
~~ elif gather : 
~~~ g , a , t , c = [ float ( x ) for x in line . strip ( ) . split ( ) [ 1 : ] ] 
gc = ( g + 0.1 ) / ( c + 0.1 ) 
at = ( a + 0.1 ) / ( t + 0.1 ) 
if 0.8 <= gc <= 1.2 and 0.8 <= at <= 1.2 : 
~~~ biased . append ( False ) 
~~~ biased . append ( True ) 
biased_5end , biased_3end = biased [ : int ( len ( biased ) / 2 ) ] , biased [ int ( len ( biased ) / 2 ) : ] [ : : - 1 ] 
trim_nt = [ 0 , 0 ] 
trim_nt [ 0 ] = get_trim_index ( biased_5end ) 
trim_nt [ 1 ] = len ( biased ) - get_trim_index ( biased_3end ) 
return trim_nt 
~~ def get_sample_trim ( p1_data , p2_data ) : 
sample_ranges = [ trim_range ( x ) for x in [ p1_data , p2_data ] ] 
optimal_5trim = max ( [ x [ 0 ] for x in sample_ranges ] ) 
optimal_3trim = min ( [ x [ 1 ] for x in sample_ranges ] ) 
return optimal_5trim , optimal_3trim 
~~ def get_summary ( summary_file ) : 
summary_info = OrderedDict ( ) 
summary_file ) ) 
with open ( summary_file ) as fh : 
~~ fields = [ x . strip ( ) for x in line . split ( "\\t" ) ] 
summary_info [ fields [ 1 ] ] = fields [ 0 ] 
summary_info ) ) 
return summary_info 
~~ def check_summary_health ( summary_file , ** kwargs ) : 
fail_sensitive = kwargs . get ( "fail_sensitive" , [ 
must_pass = kwargs . get ( "must_pass" , [ 
warning_fail_sensitive = kwargs . get ( "warning_fail_sensitive" , [ 
warning_must_pass = kwargs . get ( "warning_must_pass" , [ 
summary_info = get_summary ( summary_file ) 
health = True 
failed = [ ] 
warning = [ ] 
for cat , test in summary_info . items ( ) : 
if cat in fail_sensitive and test == "FAIL" : 
~~~ health = False 
failed . append ( "{}:{}" . format ( cat , test ) ) 
"category" . format ( cat ) ) 
~~ if cat in must_pass and test != "PASS" : 
cat ) ) 
~~ if cat in warning_fail_sensitive and test == "FAIL" : 
~~ if cat in warning_must_pass and test != "PASS" : 
~~ ~~ return health , failed , warning 
~~ def main ( sample_id , result_p1 , result_p2 , opts ) : 
json_dic = { } 
with open ( "{}_trim_report" . format ( sample_id ) , "w" ) as trep_fh , open ( "optimal_trim" , "w" ) as trim_fh , open ( "{}_status_report" . format ( sample_id ) , "w" ) as rep_fh , open ( ".status" , "w" ) as status_fh , open ( ".warning" , "w" ) as warn_fh , open ( ".fail" , "w" ) as fail_fh , open ( ".report.json" , "w" ) as report_fh : 
~~~ if "--ignore-tests" not in opts : 
~~~ json_dic = write_json_report ( sample_id , result_p1 [ 0 ] , 
result_p2 [ 0 ] ) 
for p , fastqc_summary in enumerate ( [ result_p1 [ 1 ] , result_p2 [ 1 ] ] ) : 
health , f_cat , warnings = check_summary_health ( fastqc_summary ) 
"value" : [ ] 
for w in warnings : 
~~~ warn_fh . write ( "{}\\\\n" . format ( w ) ) 
json_dic [ "warnings" ] [ 0 ] [ "value" ] . append ( w ) 
~~ ~~ output_file = "{}_{}_summary.txt" . format ( sample_id , p ) 
os . rename ( fastqc_summary , output_file ) 
if not health : 
logger . warning ( fail_msg ) 
fail_fh . write ( fail_msg ) 
json_dic [ "fail" ] = [ { 
"value" : [ fail_msg ] 
report_fh . write ( 
json . dumps ( json_dic , separators = ( "," , ":" ) ) ) 
status_fh . write ( "fail" ) 
trim_fh . write ( "fail" ) 
trep_fh . write ( "{},fail,fail\\\\n" . format ( sample_id ) ) 
~~ status_fh . write ( "pass" ) 
optimal_trim = get_sample_trim ( result_p1 [ 0 ] , result_p2 [ 0 ] ) 
trep_fh . write ( "{},{},{}\\\\n" . format ( sample_id , optimal_trim [ 0 ] , 
optimal_trim [ 1 ] ) ) 
if json_dic : 
~~~ report_fh . write ( json . dumps ( json_dic , separators = ( "," , ":" ) ) ) 
~~ ~~ ~~ def main ( sample_id , bowite_log ) : 
bowtie_info = Bowtie ( sample_id , bowite_log ) 
print ( bowtie_info . overall_rate ) 
{ "header" : "Reads" , 
"value" : int ( bowtie_info . n_reads ) , 
"table" : "mapping" , 
"columnBar" : False } , 
{ "header" : "Unmapped" , 
"value" : int ( bowtie_info . align_0x ) , 
"value" : int ( bowtie_info . align_1x ) , 
"value" : int ( bowtie_info . align_mt1x ) , 
"value" : float ( bowtie_info . overall_rate ) , 
~~ ~~ def parse_log ( self , bowtie_log ) : 
regexes = { 
'unpaired' : { 
'paired' : { 
with open ( bowtie_log , "r" ) as f : 
~~~ for l in f : 
~~~ print ( l ) 
print ( total ) 
if total : 
~~~ print ( total ) 
self . set_n_reads ( total . group ( 1 ) ) 
if paired : 
~~~ paired_total = int ( paired . group ( 1 ) ) 
paired_numbers = { } 
l = f . readline ( ) 
~~~ for k , r in regexes [ 'paired' ] . items ( ) : 
~~~ match = re . search ( r , l ) 
~~~ paired_numbers [ k ] = int ( match . group ( 1 ) ) 
~~ ~~ l = f . readline ( ) 
~~ align_zero_times = paired_numbers [ 'paired_aligned_none' ] + paired_numbers [ 'paired_aligned_mate_none' ] 
if align_zero_times : 
~~~ self . set_align_0x ( align_zero_times ) 
~~ align_one_time = paired_numbers [ 'paired_aligned_one' ] + paired_numbers [ 'paired_aligned_mate_one' ] 
if align_one_time : 
~~~ self . set_align_1x ( align_one_time ) 
~~ align_more_than_one_time = paired_numbers [ 'paired_aligned_multi' ] + paired_numbers [ 'paired_aligned_mate_multi' ] 
if align_more_than_one_time : 
~~~ self . set_align_mt1x ( align_more_than_one_time ) 
if overall : 
~~~ self . overall_rate = float ( overall . group ( 1 ) ) 
~~ ~~ ~~ ~~ def _parse_process_name ( name_str ) : 
directives = None 
fields = name_str . split ( "=" ) 
process_name = fields [ 0 ] 
if len ( fields ) == 2 : 
~~~ _directives = fields [ 1 ] . replace ( "\ , \ ) 
~~~ directives = json . loads ( _directives ) 
process_name , name_str ) ) 
~~ ~~ return process_name , directives 
~~ def _build_connections ( self , process_list , ignore_dependencies , 
auto_dependency ) : 
logger . debug ( "=============================" ) 
for p , con in enumerate ( process_list ) : 
in_lane = con [ "input" ] [ "lane" ] 
out_lane = con [ "output" ] [ "lane" ] 
if out_lane > self . lanes : 
~~~ self . lanes = out_lane 
~~ p_in_name , p_out_name , out_directives = self . _get_process_names ( 
con , p ) 
if p_out_name not in self . process_map : 
~~~ logger . error ( colored_print ( 
. format ( p_out_name ) , "red_bold" ) ) 
guess_process ( p_out_name , self . process_map ) 
~~ out_process = self . process_map [ p_out_name ] ( template = p_out_name ) 
if out_directives : 
~~~ out_process . update_attributes ( out_directives ) 
~~ input_suf = "{}_{}" . format ( in_lane , p ) 
output_suf = "{}_{}" . format ( out_lane , p ) 
p , input_suf , output_suf ) ) 
out_process . set_main_channel_names ( input_suf , output_suf , out_lane ) 
if p_in_name != "__init__" : 
~~~ in_process = self . process_map [ p_in_name ] ( template = p_in_name ) 
self . _test_connection ( in_process , out_process ) 
out_process . parent_lane = in_lane 
~~~ out_process . parent_lane = None 
p , out_process . parent_lane ) ) 
if in_lane != out_lane : 
self . _fork_tree [ in_lane ] . append ( out_lane ) 
~~~ parent_process = [ 
x for x in self . processes if x . lane == in_lane and 
x . template == p_in_name 
] [ 0 ] 
out_process . input_channel ) ) 
parent_process . update_main_forks ( out_process . input_channel ) 
~~~ parent_process = self . processes [ - 1 ] 
if parent_process . lane and parent_process . lane != out_lane : 
~~~ parent_process = [ x for x in self . processes [ : : - 1 ] 
if x . lane == out_lane ] [ 0 ] 
~~ if parent_process . output_channel : 
p , parent_process . output_channel ) ) 
out_process . input_channel = parent_process . output_channel 
~~ ~~ if out_process . dependencies and not ignore_dependencies : 
"{}" . format ( p , p_out_name , 
out_process . dependencies ) ) 
parent_lanes = self . _get_fork_tree ( out_lane ) 
for dep in out_process . dependencies : 
~~~ if not self . _search_tree_backwards ( dep , parent_lanes ) : 
~~~ if auto_dependency : 
~~~ self . _add_dependency ( 
out_process , dep , in_lane , out_lane , p ) 
~~ elif not self . export_parameters : 
~~ ~~ ~~ ~~ self . processes . append ( out_process ) 
~~ def _get_process_names ( self , con , pid ) : 
~~~ _p_in_name = con [ "input" ] [ "process" ] 
p_in_name , _ = self . _parse_process_name ( _p_in_name ) 
_p_out_name = con [ "output" ] [ "process" ] 
p_out_name , out_directives = self . _parse_process_name ( 
_p_out_name ) 
~~ except eh . ProcessError as ex : 
~~~ logger . error ( colored_print ( ex . value , "red_bold" ) ) 
~~ return p_in_name , p_out_name , out_directives 
~~ def _add_dependency ( self , p , template , inlane , outlane , pid ) : 
dependency_proc = self . process_map [ template ] ( template = template ) 
if dependency_proc . input_type != p . input_type : 
p . template , p . input_type , template , 
dependency_proc . input_type ) ) 
~~ input_suf = "{}_{}_dep" . format ( inlane , pid ) 
output_suf = "{}_{}_dep" . format ( outlane , pid ) 
dependency_proc . set_main_channel_names ( input_suf , output_suf , outlane ) 
dependency_proc . input_channel = p . input_channel 
p . input_channel = dependency_proc . output_channel 
if not p . parent_lane : 
~~~ p . parent_lane = outlane 
dependency_proc . parent_lane = None 
~~~ dependency_proc . parent_lane = inlane 
p . parent_lane = outlane 
~~ self . processes . append ( dependency_proc ) 
~~ def _search_tree_backwards ( self , template , parent_lanes ) : 
for p in self . processes [ : : - 1 ] : 
~~~ if p . lane not in parent_lanes : 
~~ if p . template == template : 
~~ def _build_header ( self ) : 
logger . debug ( "===============" ) 
self . template += hs . header 
~~ def _build_footer ( self ) : 
self . template += fs . footer 
~~ def _update_raw_input ( self , p , sink_channel = None , input_type = None ) : 
process_input = input_type if input_type else p . input_type 
process_channel = sink_channel if sink_channel else p . input_channel 
raw_in = p . get_user_channel ( process_channel , process_input ) 
raw_in ) ) 
if process_input in self . main_raw_inputs : 
~~~ self . main_raw_inputs [ process_input ] [ "raw_forks" ] . append ( 
raw_in [ "input_channel" ] ) 
~~~ self . main_raw_inputs [ process_input ] = { 
"channel" : raw_in [ "channel" ] , 
raw_in [ "checks" ] . format ( raw_in [ "params" ] ) , 
raw_in [ "channel" ] , 
raw_in [ "channel_str" ] . format ( raw_in [ "params" ] ) ) , 
"raw_forks" : [ raw_in [ "input_channel" ] ] 
p . template , self . main_raw_inputs ) ) 
~~ def _update_extra_inputs ( self , p ) : 
if p . extra_input : 
p . template , p . extra_input ) ) 
if p . extra_input == "default" : 
~~~ if p . input_type in self . main_raw_inputs : 
~~ param = p . input_type 
~~~ param = p . extra_input 
~~ dest_channel = "EXTRA_{}_{}" . format ( p . template , p . pid ) 
if param not in self . extra_inputs : 
~~~ self . extra_inputs [ param ] = { 
"input_type" : p . input_type , 
"channels" : [ dest_channel ] 
~~~ if self . extra_inputs [ param ] [ "input_type" ] != p . input_type : 
p . input_type , p . template , 
self . extra_inputs [ param ] [ "input_type" ] ) , 
~~ self . extra_inputs [ param ] [ "channels" ] . append ( dest_channel ) 
"" . format ( p . template , param , 
self . extra_inputs [ param ] ) ) 
p . update_main_input ( 
"{}.mix({})" . format ( p . input_channel , dest_channel ) 
~~ ~~ def _update_secondary_channels ( self , p ) : 
if p . link_start : 
p . template , p . link_start ) ) 
for l in p . link_start : 
~~~ if l in self . secondary_channels : 
~~~ self . secondary_channels [ l ] [ p . lane ] = { "p" : p , "end" : [ ] } 
~~~ self . secondary_channels [ l ] = { p . lane : { "p" : p , "end" : [ ] } } 
~~ ~~ ~~ if p . link_end : 
p . template , p . link_end ) ) 
for l in p . link_end : 
~~~ parent_forks = self . _get_fork_tree ( p . lane ) 
if l [ "link" ] . startswith ( "__" ) : 
~~~ self . _set_implicit_link ( p , l ) 
~~ if l [ "link" ] not in self . secondary_channels : 
~~ for lane in parent_forks : 
~~~ if lane in self . secondary_channels [ l [ "link" ] ] : 
~~~ self . secondary_channels [ 
l [ "link" ] ] [ lane ] [ "end" ] . append ( "{}" . format ( 
"{}_{}" . format ( l [ "alias" ] , p . pid ) ) ) 
p . template , self . secondary_channels ) ) 
~~ def _set_channels ( self ) : 
logger . debug ( "=====================" ) 
for i , p in enumerate ( self . processes ) : 
p . template , i ) ) 
p . set_channels ( pid = i ) 
if not p . parent_lane and p . input_type : 
~~~ self . _update_raw_input ( p ) 
~~ self . _update_extra_inputs ( p ) 
self . _update_secondary_channels ( p ) 
~~ ~~ def _set_init_process ( self ) : 
logger . debug ( "========================" ) 
init_process = self . processes [ 0 ] 
"{}" . format ( self . main_raw_inputs ) ) 
init_process . set_raw_inputs ( self . main_raw_inputs ) 
init_process . set_extra_inputs ( self . extra_inputs ) 
~~ def _set_secondary_channels ( self ) : 
logger . debug ( "==========================" ) 
self . secondary_channels ) ) 
for source , lanes in self . secondary_channels . items ( ) : 
~~~ for vals in lanes . values ( ) : 
~~~ if not vals [ "end" ] : 
vals [ "p" ] . template ) ) 
vals [ "end" ] ) ) 
vals [ "p" ] . set_secondary_channel ( source , vals [ "end" ] ) 
~~ ~~ ~~ def _set_general_compilers ( self ) : 
for c , c_info in self . compilers . items ( ) : 
~~~ compiler_cls = c_info [ "cls" ] ( template = c_info [ "template" ] ) 
c_info [ "channels" ] = [ ] 
for p in self . processes : 
~~~ if not any ( [ isinstance ( p , x ) for x in self . skip_class ] ) : 
~~~ if c in p . compiler : 
~~~ channels = [ "{}_{}" . format ( i , p . pid ) for i in 
p . compiler [ c ] ] 
c_info [ "channels" ] . extend ( channels ) 
~~ ~~ ~~ if c_info [ "channels" ] : 
~~~ compiler_cls . set_compiler_channels ( c_info [ "channels" ] , 
operator = "join" ) 
self . processes . append ( compiler_cls ) 
~~ ~~ ~~ def _set_status_channels ( self ) : 
status_inst = pc . StatusCompiler ( template = "status_compiler" ) 
report_inst = pc . ReportCompiler ( template = "report_compiler" ) 
status_channels = [ ] 
for p in [ p for p in self . processes ] : 
~~~ status_channels . extend ( p . status_strs ) 
~~ ~~ if not status_channels : 
"process" ) 
if len ( status_channels ) != len ( set ( status_channels ) ) : 
~~ status_inst . set_compiler_channels ( status_channels ) 
report_channels = [ "REPORT_{}" . format ( x . lstrip ( "STATUS_" ) ) for x in 
status_channels ] 
report_inst . set_compiler_channels ( report_channels ) 
self . processes . extend ( [ status_inst , report_inst ] ) 
~~ def _get_resources_string ( res_dict , pid ) : 
config_str = "" 
ignore_directives = [ "container" , "version" ] 
for p , directives in res_dict . items ( ) : 
~~~ for d , val in directives . items ( ) : 
~~~ if d in ignore_directives : 
~~ ~~ return config_str 
~~ def _get_container_string ( cont_dict , pid ) : 
for p , directives in cont_dict . items ( ) : 
~~~ container = "" 
if "container" in directives : 
~~~ container += directives [ "container" ] 
if "version" in directives : 
~~~ container += ":{}" . format ( directives [ "version" ] ) 
~~~ container += ":latest" 
~~ ~~ if container : 
~~~ config_str += \ . format ( p , pid , container ) 
~~ def _get_params_string ( self ) : 
params_str = "" 
p . template , p . params ) 
if p . params and p . template != "init" : 
~~~ p . set_param_id ( "_{}" . format ( p . pid ) ) 
params_str += "\\n\\t/*" 
p . pid ) 
params_str += "\\t{}\\n" . format ( "-" * ( len ( p . template ) + len ( p . pid ) + 12 ) ) 
params_str += "\\t*/\\n" 
~~ for param , val in p . params . items ( ) : 
~~~ if p . template == "init" : 
~~~ param_id = param 
~~~ param_id = "{}_{}" . format ( param , p . pid ) 
~~ ~~ return params_str 
~~ def _get_merged_params_string ( self ) : 
params_temp = { } 
p . params ) ) 
for param , val in p . params . items ( ) : 
~~~ params_temp [ param ] = val [ "default" ] 
~~ ~~ config_str = "\\n\\t" + "\\n\\t" . join ( [ 
return config_str 
~~ def _get_manifest_string ( self ) : 
config_str += \ . format ( self . pipeline_name ) 
config_str += \ . format ( self . nf_file ) 
~~ def _set_configurations ( self ) : 
logger . debug ( "======================" ) 
resources = "" 
containers = "" 
params = "" 
manifest = "" 
if self . merge_params : 
~~~ params += self . _get_merged_params_string ( ) 
help_list = self . _get_merged_params_help ( ) 
~~~ params += self . _get_params_string ( ) 
help_list = self . _get_params_help ( ) 
~~ for p in self . processes : 
~~~ if not p . directives : 
p . template , p . directives ) ) 
resources += self . _get_resources_string ( p . directives , p . pid ) 
containers += self . _get_container_string ( p . directives , p . pid ) 
~~ manifest = self . _get_manifest_string ( ) 
self . resources = self . _render_config ( "resources.config" , { 
"process_info" : resources 
self . containers = self . _render_config ( "containers.config" , { 
"container_info" : containers 
self . params = self . _render_config ( "params.config" , { 
"params_info" : params 
self . manifest = self . _render_config ( "manifest.config" , { 
"manifest_info" : manifest 
self . help = self . _render_config ( "Helper.groovy" , { 
"nf_file" : basename ( self . nf_file ) , 
"help_list" : help_list , 
self . user_config = self . _render_config ( "user.config" , { } ) 
~~ def dag_to_file ( self , dict_viz , output_file = ".treeDag.json" ) : 
outfile_dag = open ( os . path . join ( dirname ( self . nf_file ) , output_file ) 
, "w" ) 
outfile_dag . write ( json . dumps ( dict_viz ) ) 
outfile_dag . close ( ) 
~~ def render_pipeline ( self ) : 
dict_viz = { 
"name" : "root" , 
"children" : [ ] 
last_of_us = { } 
f_tree = self . _fork_tree if self . _fork_tree else { 1 : [ 1 ] } 
for x , ( k , v ) in enumerate ( f_tree . items ( ) ) : 
~~~ for p in self . processes [ 1 : ] : 
~~~ if x == 0 and p . lane not in [ k ] + v : 
~~ if x > 0 and p . lane not in v : 
~~ if not p . parent_lane : 
~~~ lst = dict_viz [ "children" ] 
~~~ lst = last_of_us [ p . parent_lane ] 
~~ tooltip = { 
"name" : "{}_{}" . format ( p . template , p . pid ) , 
"process" : { 
"pid" : p . pid , 
"input" : p . input_type , 
"output" : p . output_type if p . output_type else "None" , 
"lane" : p . lane , 
dir_var = "" 
for k2 , v2 in p . directives . items ( ) : 
~~~ dir_var += k2 
for d in v2 : 
~~~ directive = v2 [ d ] . replace ( "\ , "" ) . replace ( \ , '' ) if isinstance ( v2 [ d ] , str ) else v2 [ d ] 
~~ ~~ ~~ if dir_var : 
~~~ tooltip [ "process" ] [ "directives" ] = dir_var 
~~~ tooltip [ "process" ] [ "directives" ] = "N/A" 
~~ lst . append ( tooltip ) 
last_of_us [ p . lane ] = lst [ - 1 ] [ "children" ] 
~~ ~~ self . dag_to_file ( dict_viz ) 
with open ( os . path . join ( dirname ( self . nf_file ) , 
".forkTree.json" ) , "w" ) as fh : 
~~~ fh . write ( json . dumps ( self . _fork_tree ) ) 
~~ return self . _render_config ( "pipeline_graph.html" , { "data" : dict_viz } ) 
~~ def write_configs ( self , project_root ) : 
with open ( join ( project_root , "resources.config" ) , "w" ) as fh : 
~~~ fh . write ( self . resources ) 
~~ with open ( join ( project_root , "containers.config" ) , "w" ) as fh : 
~~~ fh . write ( self . containers ) 
~~ with open ( join ( project_root , "params.config" ) , "w" ) as fh : 
~~~ fh . write ( self . params ) 
~~ with open ( join ( project_root , "manifest.config" ) , "w" ) as fh : 
~~~ fh . write ( self . manifest ) 
~~ if not exists ( join ( project_root , "user.config" ) ) : 
~~~ with open ( join ( project_root , "user.config" ) , "w" ) as fh : 
~~~ fh . write ( self . user_config ) 
~~ ~~ lib_dir = join ( project_root , "lib" ) 
if not exists ( lib_dir ) : 
~~~ os . makedirs ( lib_dir ) 
~~ with open ( join ( lib_dir , "Helper.groovy" ) , "w" ) as fh : 
~~~ fh . write ( self . help ) 
~~ pipeline_to_json = self . render_pipeline ( ) 
with open ( splitext ( self . nf_file ) [ 0 ] + ".html" , "w" ) as fh : 
~~~ fh . write ( pipeline_to_json ) 
~~ ~~ def export_params ( self ) : 
params_json = { } 
for p in self . processes [ 1 : ] : 
~~~ params_json [ p . template ] = p . params 
~~ sys . stdout . write ( json . dumps ( params_json ) ) 
~~ def export_directives ( self ) : 
directives_json = { } 
~~~ directives_json [ p . template ] = p . directives 
~~ sys . stdout . write ( json . dumps ( directives_json ) ) 
~~ def fetch_docker_tags ( self ) : 
dict_of_parsed = { } 
terminal_width = shutil . get_terminal_size ( ) . columns - 3 
tags_list = [ 
"=" * int ( terminal_width / 4 ) , 
"{0}{1}{0}" . format ( 
"=" * int ( ( ( terminal_width / 2 - len ( center_string ) ) / 2 ) ) , 
center_string ) 
, 
"{}\\n" . format ( "=" * int ( terminal_width / 4 ) ) 
[ "component" , "container" , "tags" ] , 
"=" * int ( terminal_width / 2 ) , 
"=" * int ( terminal_width / 4 ) 
~~~ template = p . template 
if template in dict_of_parsed : 
~~ dict_of_parsed [ template ] = { 
"container" : [ ] 
for directives in p . directives . values ( ) : 
~~~ repo = directives [ "container" ] 
default_version = directives [ "version" ] 
~~~ repo = "flowcraft/flowcraft_base" 
default_version = "1.0.0-1" 
~~ repo_version = repo + default_version 
if repo_version not in dict_of_parsed [ template ] [ "container" ] : 
~~~ r = requests . get ( 
"https://hub.docker.com/v2/repositories/{}/tags/" 
. format ( repo ) 
if r . status_code != 404 : 
~~~ r_content = json . loads ( r . content ) [ "results" ] 
for version in r_content : 
~~~ printed_version = ( version [ "name" ] + "*" ) if version [ "name" ] == default_version else version [ "name" ] 
tags_list . append ( [ template , repo , printed_version ] ) 
~~ ~~ dict_of_parsed [ template ] [ "container" ] . append ( repo_version ) 
~~ ~~ for x , entry in enumerate ( tags_list ) : 
~~~ color = "blue_bold" if x < 3 else ( "white" if x % 2 != 0 else "0;37;40m" ) 
final_width = [ 
int ( terminal_width / 4 ) , 
int ( terminal_width / 2 ) , 
int ( terminal_width / 4 ) 
sys . stdout . write ( 
* entry , * final_width ) , color ) 
terminal_width + 3 ) ) 
len ( self . processes [ 1 : ] ) , len ( self . _fork_tree ) , self . lanes ) ) ) 
self . _build_header ( ) 
self . _set_channels ( ) 
self . _set_init_process ( ) 
self . _set_secondary_channels ( ) 
len ( self . secondary_channels ) ) ) ) 
self . _set_compiler_channels ( ) 
self . _set_configurations ( ) 
~~~ self . template += "\\n{}" . format ( p . template_str ) 
~~ self . _build_footer ( ) 
project_root = dirname ( self . nf_file ) 
self . write_configs ( project_root ) 
with open ( self . nf_file , "w" ) as fh : 
~~~ fh . write ( self . template ) 
~~ logger . info ( colored_print ( 
~~ def set_kmers ( kmer_opt , max_read_len ) : 
if kmer_opt == "auto" : 
~~~ if max_read_len >= 175 : 
~~~ kmers = [ 55 , 77 , 99 , 113 , 127 ] 
~~~ kmers = [ 21 , 33 , 55 , 67 , 77 ] 
~~ elif len ( kmer_opt . split ( ) ) > 1 : 
~~~ kmers = kmer_opt . split ( ) 
~~~ kmers = [ ] 
~~ return kmers 
~~ def main ( sample_id , fastq_pair , max_len , kmer , clear ) : 
kmers = set_kmers ( kmer , max_len ) 
"metaspades.py" , 
"--only-assembler" , 
"--threads" , 
"$task.cpus" , 
"-o" , 
"." 
if kmers : 
"-1" , 
fastq_pair [ 0 ] , 
"-2" , 
fastq_pair [ 1 ] 
with open ( ".status" , "w" ) as fh : 
~~ ~~ if "_trim." in fastq_pair [ 0 ] : 
~~ assembly_file = "{}_metaspades.fasta" . format ( 
sample_id ) 
os . rename ( "contigs.fasta" , assembly_file ) 
if clear == "true" and os . path . exists ( assembly_file ) : 
~~ ~~ def _get_report_id ( self ) : 
if self . watch : 
~~~ pipeline_path = get_nextflow_filepath ( self . log_file ) 
~~ ~~ workdir = os . getcwd ( ) . encode ( "utf8" ) 
~~~ with open ( self . report_file ) as fh : 
~~~ report_json = json . loads ( fh . read ( ) ) 
~~ metadata = report_json [ "data" ] [ "results" ] [ 0 ] [ "nfMetadata" ] 
~~~ report_id = metadata [ "scriptId" ] + metadata [ "sessionId" ] 
~~ return report_id 
~~ ~~ def _update_pipeline_status ( self ) : 
prev_status = self . status_info 
~~~ self . status_info = "aborted" 
self . send = True if prev_status != self . status_info else self . send 
~~~ self . status_info = "complete" 
~~ ~~ self . status_info = "running" 
~~ ~~ def update_trace_watch ( self ) : 
~~ if fields [ hm [ "process" ] ] == "report" : 
~~~ self . report_queue . append ( 
self . _expand_path ( fields [ hm [ "hash" ] ] ) 
~~ self . stored_ids . append ( fields [ hm [ "task_id" ] ] ) 
~~ ~~ ~~ def update_log_watch ( self ) : 
~~ self . _update_pipeline_status ( ) 
~~ def _send_live_report ( self , report_id ) : 
buffer_size = 100 
for i in range ( 0 , len ( self . report_queue ) , buffer_size ) : 
~~~ reports_compilation = [ ] 
for report in self . report_queue [ i : i + buffer_size ] : 
~~~ report_file = [ x for x in os . listdir ( report ) 
if x . endswith ( ".json" ) ] [ 0 ] 
~~ with open ( join ( report , report_file ) ) as fh : 
~~~ reports_compilation . append ( json . loads ( fh . read ( ) ) ) 
asizeof ( json . dumps ( reports_compilation ) ) 
~~~ requests . put ( 
self . broadcast_address , 
json = { "run_id" : report_id , 
"report_json" : reports_compilation , 
"status" : self . status_info } 
"connection." , "red_bold" ) ) 
~~ ~~ if not self . report_queue : 
"report_json" : [ ] , 
~~ ~~ self . report_queue = [ ] 
~~ def _init_live_reports ( self , report_id ) : 
~~~ with open ( ".metadata.json" ) as fh : 
~~~ metadata = [ json . load ( fh ) ] 
~~~ metadata = [ ] 
~~ start_json = { 
"data" : { "results" : metadata } 
~~~ requests . post ( 
json = { "run_id" : report_id , "report_json" : start_json , 
~~ ~~ def _close_connection ( self , report_id ) : 
self . broadcast_address ) ) 
~~~ r = requests . delete ( self . broadcast_address , 
json = { "run_id" : report_id } ) 
if r . status_code != 202 : 
~~ ~~ except requests . exceptions . ConnectionError : 
~~ ~~ def convert_adatpers ( adapter_fasta ) : 
adapter_out = "fastqc_adapters.tab" 
~~~ with open ( adapter_fasta ) as fh , open ( adapter_out , "w" ) as adap_fh : 
~~~ head = line [ 1 : ] . strip ( ) 
sequence = next ( fh ) . strip ( ) 
adap_fh . write ( "{}\\\\t{}\\\\n" . format ( head , sequence ) ) 
return adapter_out 
adapter_fasta ) ) 
~~ ~~ def main ( fastq_pair , adapter_file , cpus ) : 
if os . path . exists ( adapter_file ) : 
adapters = convert_adatpers ( adapter_file ) 
"exist" . format ( adapter_file ) ) 
adapters = None 
~~ cli = [ 
"fastqc" , 
"--extract" , 
"--nogroup" , 
"--format" , 
"fastq" , 
str ( cpus ) 
if adapters : 
~~~ cli += [ "--adapters" , "{}" . format ( adapters ) ] 
p = subprocess . Popen ( cli , stdout = PIPE , stderr = PIPE , shell = False ) 
with open ( ".status" , "w" ) as status_fh : 
~~~ for fastq in fastq_pair : 
~~~ fpath = join ( fastq . rsplit ( "." , 2 ) [ 0 ] + "_fastqc" , 
"fastqc_data.txt" ) 
if not exists ( fpath ) : 
status_fh . write ( "pass" ) 
for i , fastq in enumerate ( fastq_pair ) : 
~~~ fastqc_dir = fastq . rsplit ( "." , 2 ) [ 0 ] + "_fastqc" 
summary_file = join ( fastqc_dir , "summary.txt" ) 
fastqc_data_file = join ( fastqc_dir , "fastqc_data.txt" ) 
os . rename ( fastqc_data_file , "pair_{}_data" . format ( i + 1 ) ) 
os . rename ( summary_file , "pair_{}_summary" . format ( i + 1 ) ) 
~~ ~~ def send_to_output ( master_dict , mash_output , sample_id , assembly_file ) : 
plot_dict = { } 
if master_dict : 
~~~ out_file = open ( "{}.json" . format ( 
"" . join ( mash_output . split ( "." ) [ 0 ] ) ) , "w" ) 
out_file . write ( json . dumps ( master_dict ) ) 
out_file . close ( ) 
for k , v in master_dict . items ( ) : 
~~~ if not v [ 2 ] in plot_dict : 
~~~ plot_dict [ v [ 2 ] ] = [ k ] 
~~~ plot_dict [ v [ 2 ] ] . append ( k ) 
~~ ~~ number_hits = len ( master_dict ) 
~~~ number_hits = 0 
"patlas_mashdist" : master_dict , 
"value" : number_hits 
"patlasMashDistXrange" : plot_dict 
"assemblyFile" : assembly_file 
~~ ~~ def main ( mash_output , hash_cutoff , sample_id , assembly_file ) : 
input_f = open ( mash_output , "r" ) 
master_dict = { } 
for line in input_f : 
~~~ tab_split = line . split ( "\\t" ) 
current_seq = tab_split [ 1 ] . strip ( ) 
ref_accession = "_" . join ( tab_split [ 0 ] . strip ( ) . split ( "_" ) [ 0 : 3 ] ) 
mash_dist = tab_split [ 2 ] . strip ( ) 
hashes_list = tab_split [ - 1 ] . strip ( ) . split ( "/" ) 
perc_hashes = float ( hashes_list [ 0 ] ) / float ( hashes_list [ 1 ] ) 
if ref_accession in master_dict . keys ( ) : 
~~ if perc_hashes > float ( hash_cutoff ) : 
~~~ master_dict [ ref_accession ] = [ 
round ( 1 - float ( mash_dist ) , 2 ) , 
round ( perc_hashes , 2 ) , 
current_seq 
~~ ~~ send_to_output ( master_dict , mash_output , sample_id , assembly_file ) 
~~ def build_versions ( self ) : 
version_storage = [ ] 
template_version = self . context . get ( "__version__" , None ) 
template_program = self . context . get ( "__template__" , None ) 
template_build = self . context . get ( "__build__" , None ) 
if template_version and template_program and template_build : 
~~~ if self . logger : 
"{}" . format ( template_program , 
template_version , 
template_build ) ) 
~~ version_storage . append ( { 
"program" : template_program , 
"version" : template_version , 
"build" : template_build 
~~ for var , obj in self . context . items ( ) : 
~~~ if var . startswith ( "__get_version" ) : 
~~~ ver = obj ( ) 
version_storage . append ( ver ) 
if self . logger : 
"{}" . format ( ver ) ) 
~~ ~~ ~~ with open ( ".versions" , "w" ) as fh : 
~~~ fh . write ( json . dumps ( version_storage , separators = ( "," , ":" ) ) ) 
~~ ~~ def main ( mash_output , sample_id ) : 
read_mash_output = open ( mash_output ) 
dic = { } 
median_list = [ ] 
filtered_dic = { } 
for line in read_mash_output : 
identity = tab_split [ 0 ] 
median_multiplicity = tab_split [ 2 ] 
query_id = tab_split [ 4 ] 
dic [ query_id ] = [ identity , median_multiplicity ] 
median_list . append ( float ( median_multiplicity ) ) 
if len ( median_list ) > 0 : 
~~~ median_cutoff = median ( median_list ) 
for k , v in dic . items ( ) : 
~~~ copy_number = int ( float ( v [ 1 ] ) / median_cutoff ) 
if float ( v [ 1 ] ) > median_cutoff : 
~~~ filtered_dic [ "_" . join ( k . split ( "_" ) [ 0 : 3 ] ) ] = [ 
round ( float ( v [ 0 ] ) , 2 ) , 
copy_number 
~~ ~~ logger . info ( 
~~ output_json . write ( json . dumps ( filtered_dic ) ) 
output_json . close ( ) 
"patlas_mashscreen" : filtered_dic , 
"value" : len ( filtered_dic ) 
~~ ~~ def colored_print ( msg , color_label = "white_bold" ) : 
if sys . stdout . encoding != "UTF-8" : 
~~~ msg = "" . join ( [ i if ord ( i ) < 128 else "" for i in msg ] ) 
~~~ col = COLORS [ color_label ] 
~~~ col = color_label 
~~ return "\\x1b[{}{}\\x1b[0m" . format ( col , msg ) 
~~ def procs_dict_parser ( procs_dict ) : 
procs_dict_ordered = { k : procs_dict [ k ] for k in sorted ( procs_dict ) } 
for template , dict_proc_info in procs_dict_ordered . items ( ) : 
logger . info ( colored_print ( template_str , "blue_bold" ) ) 
for info in dict_proc_info : 
~~~ info_str = "{}:" . format ( info ) 
if isinstance ( dict_proc_info [ info ] , list ) : 
~~~ if not dict_proc_info [ info ] : 
~~~ arg_msg = "None" 
~~ ~~ elif info == "directives" : 
templt , 
for dr , val in drs . items ( ) ] ) ) 
for templt , drs in dict_proc_info [ info ] . items ( ) 
arg_msg = "" . join ( list_msg ) 
~~~ arg_msg = dict_proc_info [ info ] 
colored_print ( info_str , "white_underline" ) , arg_msg 
~~ ~~ ~~ def proc_collector ( process_map , args , pipeline_string ) : 
arguments_list = [ ] 
if args . detailed_list : 
~~~ arguments_list += [ 
"input_type" , 
"output_type" , 
"description" , 
"dependencies" , 
"conflicts" , 
"directives" 
~~ if args . short_list : 
"description" 
~~ if arguments_list : 
~~~ procs_dict = { } 
for name , cls in process_map . items ( ) : 
~~~ cls_inst = cls ( template = name ) 
if pipeline_string : 
~~~ if name not in pipeline_string : 
~~ ~~ d = { arg_key : vars ( cls_inst ) [ arg_key ] for arg_key in 
vars ( cls_inst ) if arg_key in arguments_list } 
procs_dict [ name ] = d 
~~ procs_dict_parser ( procs_dict ) 
~~ ~~ def guess_file_compression ( file_path , magic_dict = None ) : 
if not magic_dict : 
~~~ magic_dict = MAGIC_DICT 
~~ max_len = max ( len ( x ) for x in magic_dict ) 
with open ( file_path , "rb" ) as f : 
~~~ file_start = f . read ( max_len ) 
for magic , file_type in magic_dict . items ( ) : 
~~~ if file_start . startswith ( magic ) : 
~~~ return file_type 
~~ def get_qual_range ( qual_str ) : 
vals = [ ord ( c ) for c in qual_str ] 
return min ( vals ) , max ( vals ) 
~~ def get_encodings_in_range ( rmin , rmax ) : 
valid_encodings = [ ] 
valid_phred = [ ] 
for encoding , ( phred , ( emin , emax ) ) in RANGES . items ( ) : 
~~~ if rmin >= emin and rmax <= emax : 
~~~ valid_encodings . append ( encoding ) 
valid_phred . append ( phred ) 
~~ ~~ return valid_encodings , valid_phred 
~~ def main ( sample_id , fastq_pair , gsize , minimum_coverage , opts ) : 
if "-e" in opts : 
~~~ skip_encoding = True 
~~~ skip_encoding = False 
~~ gmin , gmax = 99 , 0 
encoding = [ ] 
phred = None 
chars = 0 
nreads = 0 
max_read_length = 0 
file_objects = [ ] 
for fastq in fastq_pair : 
ftype = guess_file_compression ( fastq ) 
if ftype : 
fastq , ftype ) ) 
file_objects . append ( COPEN [ ftype ] ( fastq , "rt" ) ) 
file_objects . append ( open ( fastq ) ) 
with open ( "{}_encoding" . format ( sample_id ) , "w" ) as enc_fh , open ( "{}_phred" . format ( sample_id ) , "w" ) as phred_fh , open ( "{}_coverage" . format ( sample_id ) , "w" ) as cov_fh , open ( "{}_report" . format ( sample_id ) , "w" ) as cov_rep , open ( "{}_max_len" . format ( sample_id ) , "w" ) as len_fh , open ( ".report.json" , "w" ) as json_report , open ( ".status" , "w" ) as status_fh , open ( ".fail" , "w" ) as fail_fh : 
~~~ for i , line in enumerate ( chain ( * file_objects ) ) : 
~~~ if ( i + 1 ) % 4 == 0 and not skip_encoding : 
~~~ lmin , lmax = get_qual_range ( line . strip ( ) ) 
if lmin < gmin or lmax > gmax : 
~~~ gmin , gmax = min ( lmin , gmin ) , max ( lmax , gmax ) 
encoding , phred = get_encodings_in_range ( gmin , gmax ) 
i , [ lmin , lmax ] , encoding , phred ) ) 
~~ ~~ if ( i + 3 ) % 4 == 0 : 
~~~ read_len = len ( line . strip ( ) ) 
chars += read_len 
nreads += 1 
if read_len > max_read_length : 
max_read_length = read_len 
exp_coverage = round ( chars / ( gsize * 1e6 ) , 2 ) 
if "-e" not in opts : 
"value" : chars , 
"value" : nreads , 
{ "header" : "Coverage" , 
"value" : exp_coverage , 
"columnBar" : True , 
"failThreshold" : minimum_coverage 
"sparkline" : chars 
~~ if len ( encoding ) > 0 : 
~~~ encoding = set ( encoding ) 
phred = set ( phred ) 
enc = "{}" . format ( "," . join ( [ x for x in encoding ] ) ) 
phred = "{}" . format ( "," . join ( str ( x ) for x in phred ) ) 
enc_fh . write ( enc ) 
phred_fh . write ( phred ) 
~~~ if not skip_encoding : 
logger . warning ( encoding_msg ) 
json_dic [ "warnings" ] = [ { 
"value" : [ encoding_msg ] 
enc_fh . write ( "None" ) 
phred_fh . write ( "None" ) 
"{}" . format ( gsize ) ) 
if exp_coverage >= minimum_coverage : 
~~~ cov_rep . write ( "{},{},{}\\\\n" . format ( 
sample_id , str ( exp_coverage ) , "PASS" ) ) 
cov_fh . write ( str ( exp_coverage ) ) 
logger . error ( fail_msg ) 
cov_fh . write ( "fail" ) 
cov_rep . write ( "{},{},{}\\\\n" . format ( 
sample_id , str ( exp_coverage ) , "FAIL" ) ) 
len_fh . write ( "{}" . format ( max_read_length ) ) 
~~ except EOFError : 
for fh in [ enc_fh , phred_fh , cov_fh , cov_rep , len_fh ] : 
~~~ fh . write ( "corrupt" ) 
~~ ~~ ~~ ~~ def parse_coverage_table ( coverage_file ) : 
coverage_dict = OrderedDict ( ) 
total_cov = 0 
with open ( coverage_file ) as fh : 
~~~ contig , cov = line . strip ( ) . split ( ) 
coverage_dict [ contig ] = { "cov" : int ( cov ) } 
total_cov += int ( cov ) 
"" . format ( contig , cov ) ) 
~~ ~~ return coverage_dict , total_cov 
~~ def filter_assembly ( assembly_file , minimum_coverage , coverage_info , 
output_file ) : 
write_flag = False 
with open ( assembly_file ) as fh , open ( output_file , "w" ) as out_fh : 
~~~ write_flag = False 
header = line . strip ( ) [ 1 : ] 
contig_cov = coverage_info [ header ] [ "cov" ] 
if contig_cov >= minimum_coverage : 
~~~ write_flag = True 
out_fh . write ( line ) 
~~ ~~ elif write_flag : 
~~~ out_fh . write ( line ) 
~~ ~~ ~~ ~~ def filter_bam ( coverage_info , bam_file , min_coverage , output_bam ) : 
contig_list = [ x for x , vals in coverage_info . items ( ) 
if vals [ "cov" ] >= min_coverage ] 
"samtools" , 
"view" , 
"-bh" , 
"-F" , 
"4" , 
output_bam , 
"-@" , 
"1" , 
bam_file , 
cli += contig_list 
cli ) ) 
if not p . returncode : 
~~~ cli = [ 
"index" , 
output_bam 
"{}" . format ( cli ) ) 
"======================================\\\\n{}" . format ( 
stdout ) ) 
stderr ) ) 
~~ ~~ def check_filtered_assembly ( coverage_info , coverage_bp , minimum_coverage , 
genome_size , contig_size , max_contigs , 
sample_id ) : 
assembly_len = sum ( [ v for k , v in contig_size . items ( ) 
if coverage_info [ k ] [ "cov" ] >= minimum_coverage ] ) 
ncontigs = len ( [ x for x in coverage_info . values ( ) 
if x [ "cov" ] >= minimum_coverage ] ) 
filtered_contigs = [ k for k , v in coverage_info . items ( ) 
if v [ "cov" ] >= minimum_coverage ] 
total_assembled_bp = sum ( [ sum ( coverage_bp [ x ] ) for x in filtered_contigs 
if x in coverage_bp ] ) 
"{}" . format ( total_assembled_bp ) ) 
fails = [ ] 
with open ( ".warnings" , "w" ) as warn_fh , open ( ".report.json" , "w" ) as json_report : 
assembly_len ) ) 
if assembly_len > genome_size * 1e6 * 1.5 : 
fails . append ( "Large_genome_size_({})" . format ( assembly_len ) ) 
len ( coverage_info ) ) ) 
contig_threshold = max_contigs * genome_size / 1.5 
if ncontigs > contig_threshold : 
ncontigs , round ( contig_threshold , 1 ) ) 
warnings . append ( warn_msg ) 
~~ if assembly_len < genome_size * 1e6 * 0.8 : 
fails . append ( "Small_genome_size_({})" . format ( assembly_len ) ) 
assembly_len = sum ( [ v for v in contig_size . values ( ) ] ) 
total_assembled_bp = sum ( 
[ sum ( coverage_bp [ x ] ) for x in coverage_info if x in 
coverage_bp ] ) 
health = False 
"sparkline" : total_assembled_bp 
~~ return health 
~~ def evaluate_min_coverage ( coverage_opt , assembly_coverage , assembly_size ) : 
if coverage_opt == "auto" : 
~~~ min_coverage = ( assembly_coverage / assembly_size ) * .3 
"{}" . format ( min_coverage ) ) 
if min_coverage < 10 : 
min_coverage = 10 
~~~ min_coverage = int ( coverage_opt ) 
min_coverage ) ) 
~~ return min_coverage 
~~ def get_assembly_size ( assembly_file ) : 
assembly_size = 0 
contig_size = { } 
header = "" 
~~~ header = line . strip ( ) [ 1 : ] 
contig_size [ header ] = 0 
~~~ line_len = len ( line . strip ( ) ) 
assembly_size += line_len 
contig_size [ header ] += line_len 
~~ ~~ ~~ return assembly_size , contig_size 
~~ def main ( sample_id , assembly_file , coverage_file , coverage_bp_file , bam_file , 
opts , gsize ) : 
min_assembly_coverage , max_contigs = opts 
coverage_info , a_cov = parse_coverage_table ( coverage_file ) 
a_size , contig_size = get_assembly_size ( assembly_file ) 
coverage_bp_data = get_coverage_from_file ( coverage_bp_file ) 
min_coverage = evaluate_min_coverage ( min_assembly_coverage , a_cov , a_size ) 
filtered_assembly = "{}_filt.fasta" . format ( 
os . path . splitext ( assembly_file ) [ 0 ] ) 
filtered_bam = "filtered.bam" 
if check_filtered_assembly ( coverage_info , coverage_bp_data , min_coverage , 
gsize , contig_size , int ( max_contigs ) , 
filter_assembly ( assembly_file , min_coverage , coverage_info , 
filtered_assembly ) 
filter_bam ( coverage_info , bam_file , min_coverage , filtered_bam ) 
~~~ shutil . copy ( assembly_file , filtered_assembly ) 
shutil . copy ( bam_file , filtered_bam ) 
shutil . copy ( bam_file + ".bai" , filtered_bam + ".bai" ) 
~~ ~~ def main ( sample_id , assembly_file , gsize , opts , assembler ) : 
min_contig_len , min_kmer_cov , max_contigs = [ int ( x ) for x in opts ] 
min_contig_len ) ) 
assembly_obj = Assembly ( assembly_file , min_contig_len , min_kmer_cov , 
with open ( ".warnings" , "w" ) as warn_fh : 
~~~ t_80 = gsize * 1000000 * 0.8 
t_150 = gsize * 1000000 * 1.5 
assembly_obj . filter_contigs ( * [ 
[ "length" , ">=" , min_contig_len ] 
len ( assembly_obj . contigs ) ) ) 
contig_threshold = ( max_contigs * gsize ) / 1.5 
if len ( assembly_obj . contigs ) > contig_threshold : 
len ( assembly_obj . contigs ) , 
max_contigs , 
round ( contig_threshold , 1 ) ) 
"{}.old" . format ( assembly_file ) ) ) 
assembly_obj . write_assembly ( "{}_proc.fasta" . format ( 
os . path . splitext ( assembly_file ) [ 0 ] ) ) 
output_report = "{}.report.csv" . format ( sample_id ) 
assembly_obj . write_report ( output_report ) 
"columnBar" : True } 
~~ ~~ def convert_camel_case ( name ) : 
s1 = re . sub ( '(.)([A-Z][a-z]+)' , r'\\1_\\2' , name ) 
return re . sub ( '([a-z0-9])([A-Z])' , r'\\1_\\2' , s1 ) . lower ( ) 
~~ def collect_process_map ( ) : 
process_map = { } 
prefix = "{}." . format ( components . __name__ ) 
for importer , modname , _ in pkgutil . iter_modules ( components . __path__ , 
prefix ) : 
_component_classes = [ 
cls for cls in _module . __dict__ . values ( ) if 
isinstance ( cls , type ) and cls . __name__ != "Process" 
for cls in _component_classes : 
~~~ process_map [ convert_camel_case ( cls . __name__ ) ] = cls 
~~ ~~ return process_map 
~~ def main ( newick ) : 
print ( newick ) 
tree = dendropy . Tree . get ( file = open ( newick , 'r' ) , schema = "newick" ) 
tree . reroot_at_midpoint ( ) 
"treeData" : [ { 
"trees" : [ 
to_write 
json_report . write ( json . dumps ( json_dic , separators = ( "," , ":" ) ) ) 
~~ ~~ def remove_nodes ( network , rm_nodes ) : 
rm_nodes = set ( rm_nodes ) 
ndf = network . nodes_df 
edf = network . edges_df 
nodes_to_keep = ~ ndf . index . isin ( rm_nodes ) 
edges_to_keep = ~ ( edf [ 'from' ] . isin ( rm_nodes ) | edf [ 'to' ] . isin ( rm_nodes ) ) 
return ndf . loc [ nodes_to_keep ] , edf . loc [ edges_to_keep ] 
~~ def network_to_pandas_hdf5 ( network , filename , rm_nodes = None ) : 
if rm_nodes is not None : 
~~~ nodes , edges = remove_nodes ( network , rm_nodes ) 
~~~ nodes , edges = network . nodes_df , network . edges_df 
~~ with pd . HDFStore ( filename , mode = 'w' ) as store : 
~~~ store [ 'nodes' ] = nodes 
store [ 'edges' ] = edges 
store [ 'two_way' ] = pd . Series ( [ network . _twoway ] ) 
store [ 'impedance_names' ] = pd . Series ( network . impedance_names ) 
~~ ~~ def network_from_pandas_hdf5 ( cls , filename ) : 
with pd . HDFStore ( filename ) as store : 
~~~ nodes = store [ 'nodes' ] 
edges = store [ 'edges' ] 
two_way = store [ 'two_way' ] [ 0 ] 
imp_names = store [ 'impedance_names' ] . tolist ( ) 
~~ return cls ( 
nodes [ 'x' ] , nodes [ 'y' ] , edges [ 'from' ] , edges [ 'to' ] , edges [ imp_names ] , 
twoway = two_way ) 
~~ def bbox ( self ) : 
return [ self . nodes_df . x . min ( ) , self . nodes_df . y . min ( ) , 
self . nodes_df . x . max ( ) , self . nodes_df . y . max ( ) ] 
~~ def set ( self , node_ids , variable = None , name = "tmp" ) : 
if variable is None : 
~~~ variable = pd . Series ( np . ones ( len ( node_ids ) ) , index = node_ids . index ) 
~~ df = pd . DataFrame ( { name : variable , 
"node_idx" : self . _node_indexes ( node_ids ) } ) 
length = len ( df ) 
df = df . dropna ( how = "any" ) 
newl = len ( df ) 
if length - newl > 0 : 
( length - newl ) ) 
~~ self . variable_names . add ( name ) 
self . net . initialize_access_var ( name . encode ( 'utf-8' ) , 
df . node_idx . values . astype ( 'int' ) , 
df [ name ] . values . astype ( 'double' ) ) 
~~ def aggregate ( self , distance , type = "sum" , decay = "linear" , imp_name = None , 
name = "tmp" ) : 
imp_num = self . _imp_name_to_num ( imp_name ) 
type = type . lower ( ) 
if type == "ave" : 
res = self . net . get_all_aggregate_accessibility_variables ( distance , 
name . encode ( 'utf-8' ) , 
type . encode ( 'utf-8' ) , 
decay . encode ( 'utf-8' ) , 
imp_num ) 
return pd . Series ( res , index = self . node_ids ) 
~~ def get_node_ids ( self , x_col , y_col , mapping_distance = None ) : 
xys = pd . DataFrame ( { 'x' : x_col , 'y' : y_col } ) 
distances , indexes = self . kdtree . query ( xys . as_matrix ( ) ) 
indexes = np . transpose ( indexes ) [ 0 ] 
distances = np . transpose ( distances ) [ 0 ] 
node_ids = self . nodes_df . iloc [ indexes ] . index 
df = pd . DataFrame ( { "node_id" : node_ids , "distance" : distances } , 
index = xys . index ) 
if mapping_distance is not None : 
~~~ df = df [ df . distance <= mapping_distance ] 
~~ return df . node_id 
~~ def plot ( 
self , data , bbox = None , plot_type = 'scatter' , 
fig_kwargs = None , bmap_kwargs = None , plot_kwargs = None , 
cbar_kwargs = None ) : 
from mpl_toolkits . basemap import Basemap 
fig_kwargs = fig_kwargs or { } 
bmap_kwargs = bmap_kwargs or { } 
plot_kwargs = plot_kwargs or { } 
cbar_kwargs = cbar_kwargs or { } 
if not bbox : 
~~~ bbox = ( 
self . nodes_df . y . min ( ) , 
self . nodes_df . x . min ( ) , 
self . nodes_df . y . max ( ) , 
self . nodes_df . x . max ( ) ) 
~~ fig , ax = plt . subplots ( ** fig_kwargs ) 
bmap = Basemap ( 
bbox [ 1 ] , bbox [ 0 ] , bbox [ 3 ] , bbox [ 2 ] , ax = ax , ** bmap_kwargs ) 
bmap . drawcoastlines ( ) 
bmap . drawmapboundary ( ) 
x , y = bmap ( self . nodes_df . x . values , self . nodes_df . y . values ) 
if plot_type == 'scatter' : 
~~~ plot = bmap . scatter ( 
x , y , c = data . values , ** plot_kwargs ) 
~~ elif plot_type == 'hexbin' : 
~~~ plot = bmap . hexbin ( 
x , y , C = data . values , ** plot_kwargs ) 
~~ bmap . colorbar ( plot , ** cbar_kwargs ) 
return bmap , fig , ax 
~~ def set_pois ( self , category , maxdist , maxitems , x_col , y_col ) : 
if category not in self . poi_category_names : 
~~~ self . poi_category_names . append ( category ) 
~~ self . max_pois = maxitems 
node_ids = self . get_node_ids ( x_col , y_col ) 
self . poi_category_indexes [ category ] = node_ids . index 
node_idx = self . _node_indexes ( node_ids ) 
self . net . initialize_category ( maxdist , maxitems , category . encode ( 'utf-8' ) , node_idx . values ) 
~~ def nearest_pois ( self , distance , category , num_pois = 1 , max_distance = None , 
imp_name = None , include_poi_ids = False ) : 
if max_distance is None : 
~~~ max_distance = distance 
~~ if category not in self . poi_category_names : 
~~ if num_pois > self . max_pois : 
~~ imp_num = self . _imp_name_to_num ( imp_name ) 
dists , poi_ids = self . net . find_all_nearest_pois ( 
distance , 
num_pois , 
category . encode ( 'utf-8' ) , 
dists [ dists == - 1 ] = max_distance 
df = pd . DataFrame ( dists , index = self . node_ids ) 
df . columns = list ( range ( 1 , num_pois + 1 ) ) 
if include_poi_ids : 
~~~ df2 = pd . DataFrame ( poi_ids , index = self . node_ids ) 
df2 . columns = [ "poi%d" % i for i in range ( 1 , num_pois + 1 ) ] 
for col in df2 . columns : 
~~~ s = df2 [ col ] . astype ( 'int' ) 
df2 [ col ] = self . poi_category_indexes [ category ] . values [ s ] 
df2 . loc [ s == - 1 , col ] = np . nan 
~~ df = pd . concat ( [ df , df2 ] , axis = 1 ) 
~~ def low_connectivity_nodes ( self , impedance , count , imp_name = None ) : 
self . set ( self . node_ids . to_series ( ) , name = 'counter' ) 
agg = self . aggregate ( 
impedance , type = 'count' , imp_name = imp_name , name = 'counter' ) 
return np . array ( agg [ agg < count ] . index ) 
~~ def pdna_network_from_bbox ( 
lat_min = None , lng_min = None , lat_max = None , lng_max = None , bbox = None , 
network_type = 'walk' , two_way = True , 
timeout = 180 , memory = None , max_query_area_size = 50 * 1000 * 50 * 1000 ) : 
nodes , edges = network_from_bbox ( lat_min = lat_min , lng_min = lng_min , 
lat_max = lat_max , lng_max = lng_max , 
bbox = bbox , network_type = network_type , 
two_way = two_way , timeout = timeout , 
max_query_area_size = max_query_area_size ) 
return Network ( 
nodes [ 'x' ] , nodes [ 'y' ] , 
edges [ 'from' ] , edges [ 'to' ] , edges [ [ 'distance' ] ] ) 
~~ def process_node ( e ) : 
uninteresting_tags = { 
'source' , 
'source_ref' , 
'source:ref' , 
'history' , 
'attribution' , 
'created_by' , 
'tiger:tlid' , 
'tiger:upload_uuid' , 
node = { 
'id' : e [ 'id' ] , 
'lat' : e [ 'lat' ] , 
'lon' : e [ 'lon' ] 
if 'tags' in e : 
~~~ for t , v in list ( e [ 'tags' ] . items ( ) ) : 
~~~ if t not in uninteresting_tags : 
~~~ node [ t ] = v 
~~ ~~ ~~ return node 
~~ def make_osm_query ( query ) : 
osm_url = 'http://www.overpass-api.de/api/interpreter' 
req = requests . get ( osm_url , params = { 'data' : query } ) 
req . raise_for_status ( ) 
return req . json ( ) 
~~ def build_node_query ( lat_min , lng_min , lat_max , lng_max , tags = None ) : 
if tags is not None : 
~~~ if isinstance ( tags , str ) : 
~~~ tags = [ tags ] 
~~ tags = '' . join ( '[{}]' . format ( t ) for t in tags ) 
~~~ tags = '' 
~~ query_fmt = ( 
'[out:json];' 
'(' 
');' 
'out;' ) 
return query_fmt . format ( 
lat_min = lat_min , lng_min = lng_min , lat_max = lat_max , lng_max = lng_max , 
tags = tags ) 
~~ def node_query ( lat_min , lng_min , lat_max , lng_max , tags = None ) : 
node_data = make_osm_query ( build_node_query ( 
lat_min , lng_min , lat_max , lng_max , tags = tags ) ) 
if len ( node_data [ 'elements' ] ) == 0 : 
~~ nodes = [ process_node ( n ) for n in node_data [ 'elements' ] ] 
return pd . DataFrame . from_records ( nodes , index = 'id' ) 
~~ def format_output ( data , headers , format_name , ** kwargs ) : 
formatter = TabularOutputFormatter ( format_name = format_name ) 
return formatter . format_output ( data , headers , ** kwargs ) 
~~ def format_name ( self , format_name ) : 
if format_name in self . supported_formats : 
~~~ self . _format_name = format_name 
~~~ raise ValueError ( \ . format ( 
format_name ) ) 
~~ ~~ def register_new_formatter ( cls , format_name , handler , preprocessors = ( ) , 
kwargs = None ) : 
cls . _output_formats [ format_name ] = OutputFormatHandler ( 
format_name , preprocessors , handler , kwargs or { } ) 
~~ def format_output ( self , data , headers , format_name = None , 
preprocessors = ( ) , column_types = None , ** kwargs ) : 
format_name = format_name or self . _format_name 
if format_name not in self . supported_formats : 
~~~ raise ValueError ( \ . format ( format_name ) ) 
~~ ( _ , _preprocessors , formatter , 
fkwargs ) = self . _output_formats [ format_name ] 
fkwargs . update ( kwargs ) 
if column_types is None : 
~~~ data = list ( data ) 
column_types = self . _get_column_types ( data ) 
~~ for f in unique_items ( preprocessors + _preprocessors ) : 
~~~ data , headers = f ( data , headers , column_types = column_types , 
** fkwargs ) 
~~ return formatter ( list ( data ) , headers , column_types = column_types , ** fkwargs ) 
~~ def _get_column_types ( self , data ) : 
columns = list ( zip_longest ( * data ) ) 
return [ self . _get_column_type ( column ) for column in columns ] 
~~ def _get_column_type ( self , column ) : 
type_values = [ TYPES [ self . _get_type ( v ) ] for v in column ] 
inverse_types = { v : k for k , v in TYPES . items ( ) } 
return inverse_types [ max ( type_values ) ] 
~~ def _get_type ( self , value ) : 
~~~ return type ( None ) 
~~ elif type ( value ) in int_types : 
~~~ return int 
~~ elif type ( value ) in float_types : 
~~~ return float 
~~ elif isinstance ( value , binary_type ) : 
~~~ return binary_type 
~~~ return text_type 
~~ ~~ def adapter ( data , headers , table_format = None , preserve_whitespace = False , 
keys = ( 'floatfmt' , 'numalign' , 'stralign' , 'showindex' , 'disable_numparse' ) 
tkwargs = { 'tablefmt' : table_format } 
tkwargs . update ( filter_dict_by_key ( kwargs , keys ) ) 
if table_format in supported_markup_formats : 
~~~ tkwargs . update ( numalign = None , stralign = None ) 
~~ tabulate . PRESERVE_WHITESPACE = preserve_whitespace 
return iter ( tabulate . tabulate ( data , headers , ** tkwargs ) . split ( '\\n' ) ) 
~~ def get_user_config_dir ( app_name , app_author , roaming = True , force_xdg = True ) : 
if WIN : 
~~~ key = 'APPDATA' if roaming else 'LOCALAPPDATA' 
folder = os . path . expanduser ( os . environ . get ( key , '~' ) ) 
return os . path . join ( folder , app_author , app_name ) 
~~ if MAC and not force_xdg : 
~~~ return os . path . join ( os . path . expanduser ( 
~~ return os . path . join ( 
os . path . expanduser ( os . environ . get ( 'XDG_CONFIG_HOME' , '~/.config' ) ) , 
_pathify ( app_name ) ) 
~~ def get_system_config_dirs ( app_name , app_author , force_xdg = True ) : 
~~~ folder = os . environ . get ( 'PROGRAMDATA' ) 
return [ os . path . join ( folder , app_author , app_name ) ] 
~~ dirs = os . environ . get ( 'XDG_CONFIG_DIRS' , '/etc/xdg' ) 
paths = [ os . path . expanduser ( x ) for x in dirs . split ( os . pathsep ) ] 
return [ os . path . join ( d , _pathify ( app_name ) ) for d in paths ] 
~~ def read_default_config ( self ) : 
if self . validate : 
~~~ self . default_config = ConfigObj ( configspec = self . default_file , 
list_values = False , _inspec = True , 
encoding = 'utf8' ) 
valid = self . default_config . validate ( Validator ( ) , copy = True , 
preserve_errors = True ) 
if valid is not True : 
~~~ for name , section in valid . items ( ) : 
~~~ if section is True : 
~~ for key , value in section . items ( ) : 
~~~ if isinstance ( value , ValidateError ) : 
~~~ raise DefaultConfigValidationError ( 
name , key , value ) ) 
~~ ~~ ~~ ~~ ~~ elif self . default_file : 
~~~ self . default_config , _ = self . read_config_file ( self . default_file ) 
~~ self . update ( self . default_config ) 
if self . default_file : 
~~~ self . read_default_config ( ) 
~~ return self . read_config_files ( self . all_config_files ( ) ) 
~~ def user_config_file ( self ) : 
return os . path . join ( 
get_user_config_dir ( self . app_name , self . app_author ) , 
self . filename ) 
~~ def system_config_files ( self ) : 
return [ os . path . join ( f , self . filename ) for f in get_system_config_dirs ( 
self . app_name , self . app_author ) ] 
~~ def additional_files ( self ) : 
return [ os . path . join ( f , self . filename ) for f in self . additional_dirs ] 
~~ def write_default_config ( self , overwrite = False ) : 
destination = self . user_config_file ( ) 
if not overwrite and os . path . exists ( destination ) : 
~~ with io . open ( destination , mode = 'wb' ) as f : 
~~~ self . default_config . write ( f ) 
~~ ~~ def write ( self , outfile = None , section = None ) : 
with io . open ( outfile or self . user_config_file ( ) , 'wb' ) as f : 
~~~ self . data . write ( outfile = f , section = section ) 
~~ ~~ def read_config_file ( self , f ) : 
configspec = self . default_file if self . validate else None 
~~~ config = ConfigObj ( infile = f , configspec = configspec , 
interpolation = False , encoding = 'utf8' ) 
~~ except ConfigObjError as e : 
e . line_number , f ) ) 
config = e . config 
~~ valid = True 
~~~ valid = config . validate ( Validator ( ) , preserve_errors = True , 
~~ if bool ( config ) : 
~~~ self . config_filenames . append ( config . filename ) 
~~ return config , valid 
~~ def read_config_files ( self , files ) : 
for _file in files : 
~~~ config , valid = self . read_config_file ( _file ) 
self . update ( config ) 
~~~ errors [ _file ] = valid 
~~ ~~ return errors or True 
~~ def bytes_to_string ( b ) : 
if isinstance ( b , binary_type ) : 
~~~ return b . decode ( 'utf8' ) 
~~~ return '0x' + binascii . hexlify ( b ) . decode ( 'ascii' ) 
~~ ~~ return b 
~~ def truncate_string ( value , max_width = None ) : 
if isinstance ( value , text_type ) and max_width is not None and len ( value ) > max_width : 
~~~ return value [ : max_width ] 
~~ def filter_dict_by_key ( d , keys ) : 
return { k : v for k , v in d . items ( ) if k in keys } 
~~ def unique_items ( seq ) : 
seen = set ( ) 
return [ x for x in seq if not ( x in seen or seen . add ( x ) ) ] 
~~ def replace ( s , replace ) : 
for r in replace : 
~~~ s = s . replace ( * r ) 
~~ def adapter ( data , headers , ** kwargs ) : 
for row in chain ( ( headers , ) , data ) : 
~~~ yield "\\t" . join ( ( replace ( r , ( ( '\\n' , r'\\n' ) , ( '\\t' , r'\\t' ) ) ) for r in row ) ) 
~~ ~~ def call_and_exit ( self , cmd , shell = True ) : 
sys . exit ( subprocess . call ( cmd , shell = shell ) ) 
~~ def call_in_sequence ( self , cmds , shell = True ) : 
for cmd in cmds : 
~~~ if subprocess . call ( cmd , shell = shell ) == 1 : 
~~ ~~ ~~ def apply_options ( self , cmd , options = ( ) ) : 
for option in ( self . default_cmd_options + options ) : 
~~~ cmd = self . apply_option ( cmd , option , 
active = getattr ( self , option , False ) ) 
~~ return cmd 
~~ def apply_option ( self , cmd , option , active = True ) : 
return re . sub ( r'{{{}\\:(?P<option>[^}}]*)}}' . format ( option ) , 
'\\g<option>' if active else '' , cmd ) 
~~ def initialize_options ( self ) : 
self . branch = 'master' 
self . fix = False 
super ( lint , self ) . initialize_options ( ) 
cmd = cmd . format ( branch = self . branch ) 
self . call_and_exit ( self . apply_options ( cmd , ( 'fix' , ) ) ) 
cmds = ( self . clean_docs_cmd , self . html_docs_cmd , self . view_docs_cmd ) 
self . call_in_sequence ( cmds ) 
~~ def truncate_string ( data , headers , max_field_width = None , ** _ ) : 
return ( ( [ utils . truncate_string ( v , max_field_width ) for v in row ] for row in data ) , 
[ utils . truncate_string ( h , max_field_width ) for h in headers ] ) 
~~ def convert_to_string ( data , headers , ** _ ) : 
return ( ( [ utils . to_string ( v ) for v in row ] for row in data ) , 
[ utils . to_string ( h ) for h in headers ] ) 
~~ def override_missing_value ( data , headers , missing_value = '' , ** _ ) : 
return ( ( [ missing_value if v is None else v for v in row ] for row in data ) , 
headers ) 
return ( ( [ v . replace ( '\\t' , new_value ) if isinstance ( v , text_type ) else v 
for v in row ] for row in data ) , 
~~ def bytes_to_string ( data , headers , ** _ ) : 
return ( ( [ utils . bytes_to_string ( v ) for v in row ] for row in data ) , 
[ utils . bytes_to_string ( h ) for h in headers ] ) 
~~ def align_decimals ( data , headers , column_types = ( ) , ** _ ) : 
pointpos = len ( headers ) * [ 0 ] 
data = list ( data ) 
for row in data : 
~~~ for i , v in enumerate ( row ) : 
~~~ if column_types [ i ] is float and type ( v ) in float_types : 
~~~ v = text_type ( v ) 
pointpos [ i ] = max ( utils . intlen ( v ) , pointpos [ i ] ) 
~~ ~~ ~~ def results ( data ) : 
~~~ for row in data : 
for i , v in enumerate ( row ) : 
~~~ result . append ( v ) 
~~ ~~ yield result 
~~ ~~ return results ( data ) , headers 
~~ def quote_whitespaces ( data , headers , quotestyle = "\ , ** _ ) : 
whitespace = tuple ( string . whitespace ) 
quote = len ( headers ) * [ False ] 
if v . startswith ( whitespace ) or v . endswith ( whitespace ) : 
~~~ quote [ i ] = True 
~~~ quotation = quotestyle if quote [ i ] else '' 
result . append ( '{quotestyle}{value}{quotestyle}' . format ( 
quotestyle = quotation , value = v ) ) 
~~ yield result 
~~ def style_output ( data , headers , style = None , 
header_token = 'Token.Output.Header' , 
odd_row_token = 'Token.Output.OddRow' , 
even_row_token = 'Token.Output.EvenRow' , ** _ ) : 
if style and HAS_PYGMENTS : 
~~~ formatter = Terminal256Formatter ( style = style ) 
def style_field ( token , field ) : 
s = StringIO ( ) 
formatter . format ( ( ( token , field ) , ) , s ) 
return s . getvalue ( ) 
~~ headers = [ style_field ( header_token , header ) for header in headers ] 
data = ( [ style_field ( odd_row_token if i % 2 else even_row_token , f ) 
for f in r ] for i , r in enumerate ( data , 1 ) ) 
~~ return iter ( data ) , headers 
~~ def format_numbers ( data , headers , column_types = ( ) , integer_format = None , 
float_format = None , ** _ ) : 
if ( integer_format is None and float_format is None ) or not column_types : 
~~~ return iter ( data ) , headers 
~~ def _format_number ( field , column_type ) : 
~~~ if integer_format and column_type is int and type ( field ) in int_types : 
~~~ return format ( field , integer_format ) 
~~ elif float_format and column_type is float and type ( field ) in float_types : 
~~~ return format ( field , float_format ) 
~~ return field 
~~ data = ( [ _format_number ( v , column_types [ i ] ) for i , v in enumerate ( row ) ] for row in data ) 
return data , headers 
~~ def _get_separator ( num , sep_title , sep_character , sep_length ) : 
left_divider_length = right_divider_length = sep_length 
if isinstance ( sep_length , tuple ) : 
~~~ left_divider_length , right_divider_length = sep_length 
~~ left_divider = sep_character * left_divider_length 
right_divider = sep_character * right_divider_length 
title = sep_title . format ( n = num + 1 ) 
left_divider = left_divider , right_divider = right_divider , title = title ) 
~~ def _format_row ( headers , row ) : 
return '\\n' . join ( formatted_row ) 
sep_length = 27 ) : 
header_len = max ( [ len ( x ) for x in headers ] ) 
padded_headers = [ x . ljust ( header_len ) for x in headers ] 
formatted_rows = [ _format_row ( padded_headers , row ) for row in data ] 
for i , result in enumerate ( formatted_rows ) : 
~~~ yield _get_separator ( i , sep_title , sep_character , sep_length ) + result 
~~ ~~ def adapter ( data , headers , ** kwargs ) : 
keys = ( 'sep_title' , 'sep_character' , 'sep_length' ) 
return vertical_table ( data , headers , ** filter_dict_by_key ( kwargs , keys ) ) 
~~ def adapter ( data , headers , table_format = 'csv' , ** kwargs ) : 
keys = ( 'dialect' , 'delimiter' , 'doublequote' , 'escapechar' , 
'quotechar' , 'quoting' , 'skipinitialspace' , 'strict' ) 
if table_format == 'csv' : 
~~~ delimiter = ',' 
~~ elif table_format == 'csv-tab' : 
~~~ delimiter = '\\t' 
~~ ckwargs = { 'delimiter' : delimiter , 'lineterminator' : '' } 
ckwargs . update ( filter_dict_by_key ( kwargs , keys ) ) 
l = linewriter ( ) 
writer = csv . writer ( l , ** ckwargs ) 
writer . writerow ( headers ) 
yield l . line 
~~~ l . reset ( ) 
~~ ~~ def adapter ( data , headers , table_format = None , ** kwargs ) : 
keys = ( 'title' , ) 
table = table_format_handler [ table_format ] 
t = table ( [ headers ] + list ( data ) , ** filter_dict_by_key ( kwargs , keys ) ) 
dimensions = terminaltables . width_and_alignment . max_dimensions ( 
t . table_data , 
t . padding_left , 
t . padding_right ) [ : 3 ] 
for r in t . gen_table ( * dimensions ) : 
~~~ yield u'' . join ( r ) 
~~ ~~ def panel ( context , panel , build , bed , version ) : 
adapter = context . obj [ 'adapter' ] 
chromosomes_found = set ( ) 
if not panel : 
context . abort ( ) 
if bed : 
~~~ if version : 
~~~ version = [ version ] 
~~ lines = export_panels ( 
adapter = adapter , 
panels = panel , 
versions = version , 
build = build , 
~~~ lines = export_gene_panels ( 
~~ for line in lines : 
~~~ click . echo ( line ) 
~~ ~~ def load_panel ( panel_path , adapter , date = None , display_name = None , version = None , panel_type = None , 
panel_id = None , institute = None ) : 
panel_lines = get_file_handle ( panel_path ) 
~~~ panel_info = get_panel_info ( 
panel_lines = panel_lines , 
panel_id = panel_id , 
institute = institute , 
date = date , 
display_name = display_name 
~~~ raise err 
~~ version = None 
if panel_info . get ( 'version' ) : 
~~~ version = float ( panel_info [ 'version' ] ) 
~~ panel_id = panel_info [ 'panel_id' ] 
display_name = panel_info [ 'display_name' ] or panel_id 
institute = panel_info [ 'institute' ] 
date = panel_info [ 'date' ] 
if not institute : 
~~ if not adapter . institute ( institute ) : 
~~ if not panel_id : 
~~ if version : 
~~~ existing_panel = adapter . gene_panel ( panel_id , version ) 
~~~ existing_panel = adapter . gene_panel ( panel_id ) 
version = 1.0 
~~ if existing_panel : 
if version == existing_panel [ 'version' ] : 
raise SyntaxError ( ) 
~~ display_name = display_name or existing_panel [ 'display_name' ] 
institute = institute or existing_panel [ 'institute' ] 
~~ parsed_panel = parse_gene_panel ( 
path = panel_path , 
panel_type = panel_type , 
display_name = display_name , 
~~~ adapter . load_panel ( parsed_panel = parsed_panel ) 
~~ ~~ def load_panel_app ( adapter , panel_id = None , institute = 'cust000' ) : 
base_url = 'https://panelapp.genomicsengland.co.uk/WebServices/{0}/' 
hgnc_map = adapter . genes_by_alias ( ) 
if panel_id : 
~~~ panel_ids = [ panel_id ] 
data = get_request ( base_url . format ( 'list_panels' ) ) 
json_lines = json . loads ( data ) 
panel_ids = [ panel_info [ 'Panel_Id' ] for panel_info in json_lines [ 'result' ] ] 
~~ for panel_id in panel_ids : 
~~~ panel_data = get_request ( base_url . format ( 'get_panel' ) + panel_id ) 
parsed_panel = parse_panel_app_panel ( 
panel_info = json . loads ( panel_data ) [ 'result' ] , 
hgnc_map = hgnc_map , 
institute = institute 
parsed_panel [ 'panel_id' ] = panel_id 
if len ( parsed_panel [ 'genes' ] ) == 0 : 
~~ ~~ ~~ def export_variants ( adapter , collaborator , document_id = None , case_id = None ) : 
variants = [ ] 
if document_id : 
~~~ yield adapter . variant ( document_id ) 
~~ variant_ids = adapter . get_causatives ( 
institute_id = collaborator , 
case_id = case_id 
for document_id in variant_ids : 
~~~ variant_obj = adapter . variant ( document_id ) 
chrom = variant_obj [ 'chromosome' ] 
chrom_int = CHROMOSOME_INTEGERS . get ( chrom ) 
if not chrom_int : 
~~ variants . append ( ( chrom_int , variant_obj [ 'position' ] , variant_obj ) ) 
~~ variants . sort ( key = lambda x : ( x [ 0 ] , x [ 1 ] ) ) 
for variant in variants : 
~~~ variant_obj = variant [ 2 ] 
yield variant_obj 
~~ ~~ def export_verified_variants ( aggregate_variants , unique_callers ) : 
document_lines = [ ] 
for variant in aggregate_variants : 
~~~ samples = [ ] 
for sample in variant [ 'samples' ] : 
line . append ( variant [ 'institute' ] ) 
line . append ( variant [ 'category' ] ) 
line . append ( variant [ 'variant_type' ] ) 
local_link = '/' . join ( [ '' , variant [ 'institute' ] , case_name , variant [ '_id' ] ] ) 
line . append ( local_link ) 
line . append ( variant . get ( 'validation' ) ) 
line . append ( case_name ) 
case_individual = next ( ind for ind in variant [ 'case_obj' ] [ 'individuals' ] if ind [ 'individual_id' ] == sample [ 'sample_id' ] ) 
if case_individual [ 'phenotype' ] == 2 : 
~~~ line . append ( sample . get ( 'display_name' ) ) 
genes = [ ] 
prot_effect = [ ] 
funct_anno = [ ] 
~~~ genes . append ( gene . get ( 'hgnc_symbol' , '' ) ) 
funct_anno . append ( gene . get ( 'functional_annotation' ) ) 
for transcript in gene . get ( 'transcripts' ) : 
~~~ if transcript . get ( 'is_canonical' ) and transcript . get ( 'protein_sequence_name' ) : 
~~~ prot_effect . append ( urllib . parse . unquote ( transcript . get ( 'protein_sequence_name' ) ) ) 
~~ ~~ ~~ line . append ( ',' . join ( prot_effect ) ) 
line . append ( ',' . join ( funct_anno ) ) 
line . append ( ',' . join ( genes ) ) 
line . append ( variant . get ( 'rank_score' ) ) 
line . append ( variant . get ( 'cadd_score' ) ) 
line . append ( sample . get ( 'genotype_call' ) ) 
line . append ( sample [ 'allele_depths' ] [ 0 ] ) 
line . append ( sample [ 'allele_depths' ] [ 1 ] ) 
line . append ( sample [ 'genotype_quality' ] ) 
for caller in unique_callers : 
~~~ if variant . get ( caller ) : 
~~~ line . append ( variant . get ( caller ) ) 
~~~ line . append ( '-' ) 
~~ ~~ document_lines . append ( line ) 
~~ ~~ return document_lines 
~~ def export_mt_variants ( variants , sample_id ) : 
~~~ line = [ ] 
position = variant . get ( 'position' ) 
change = '>' . join ( [ variant . get ( 'reference' ) , variant . get ( 'alternative' ) ] ) 
line . append ( position ) 
line . append ( change ) 
line . append ( str ( position ) + change ) 
for gene in variant . get ( 'genes' ) : 
ref_ad = '' 
alt_ad = '' 
~~~ if sample . get ( 'sample_id' ) == sample_id : 
~~~ ref_ad = sample [ 'allele_depths' ] [ 0 ] 
alt_ad = sample [ 'allele_depths' ] [ 1 ] 
~~ ~~ line . append ( ref_ad ) 
line . append ( alt_ad ) 
document_lines . append ( line ) 
~~ return document_lines 
~~ def user ( context , user_id , update_role , add_institute , remove_admin , remove_institute ) : 
user_obj = adapter . user ( user_id ) 
if not user_obj : 
~~ existing_roles = set ( user_obj . get ( 'roles' , [ ] ) ) 
if update_role : 
~~~ if not update_role in user_obj [ 'roles' ] : 
~~~ existing_roles = set ( user_obj [ 'roles' ] ) 
existing_roles . add ( update_role ) 
~~ ~~ if remove_admin : 
~~~ existing_roles . remove ( 'admin' ) 
~~ except KeyError as err : 
~~ ~~ user_obj [ 'roles' ] = list ( existing_roles ) 
existing_institutes = set ( user_obj . get ( 'institutes' , [ ] ) ) 
for institute_id in add_institute : 
~~~ institute_obj = adapter . institute ( institute_id ) 
if not institute_obj : 
~~~ existing_institutes . add ( institute_id ) 
~~ ~~ for institute_id in remove_institute : 
~~~ existing_institutes . remove ( institute_id ) 
~~ ~~ user_obj [ 'institutes' ] = list ( existing_institutes ) 
updated_user = adapter . update_user ( user_obj ) 
~~ def variants ( institute_id , case_name ) : 
page = int ( request . form . get ( 'page' , 1 ) ) 
institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) 
variant_type = request . args . get ( 'variant_type' , 'clinical' ) 
default_panels = [ ] 
for panel in case_obj [ 'panels' ] : 
~~~ if panel . get ( 'is_default' ) : 
~~~ default_panels . append ( panel [ 'panel_name' ] ) 
~~ ~~ request . form . get ( 'gene_panels' ) 
if bool ( request . form . get ( 'clinical_filter' ) ) : 
~~~ clinical_filter = MultiDict ( { 
'variant_type' : 'clinical' , 
'region_annotations' : [ 'exonic' , 'splicing' ] , 
'functional_annotations' : SEVERE_SO_TERMS , 
'clinsig' : [ 4 , 5 ] , 
'clinsig_confident_always_returned' : True , 
'gnomad_frequency' : str ( institute_obj [ 'frequency_cutoff' ] ) , 
'gene_panels' : default_panels 
~~ if ( request . method == "POST" ) : 
~~~ if bool ( request . form . get ( 'clinical_filter' ) ) : 
~~~ form = FiltersForm ( clinical_filter ) 
form . csrf_token = request . args . get ( 'csrf_token' ) 
~~~ form = FiltersForm ( request . form ) 
~~~ form = FiltersForm ( request . args ) 
~~ available_panels = case_obj . get ( 'panels' , [ ] ) + [ 
{ 'panel_name' : 'hpo' , 'display_name' : 'HPO' } ] 
panel_choices = [ ( panel [ 'panel_name' ] , panel [ 'display_name' ] ) 
for panel in available_panels ] 
form . gene_panels . choices = panel_choices 
if ( request . files ) : 
~~~ file = request . files [ form . symbol_file . name ] 
~~ if request . files and file and file . filename != '' : 
~~~ stream = io . StringIO ( file . stream . read ( ) . decode ( 'utf-8' ) , newline = None ) 
~~ except UnicodeDecodeError as error : 
return redirect ( request . referrer ) 
~~ hgnc_symbols_set = set ( form . hgnc_symbols . data ) 
new_hgnc_symbols = controllers . upload_panel ( store , institute_id , case_name , stream ) 
hgnc_symbols_set . update ( new_hgnc_symbols ) 
form . hgnc_symbols . data = hgnc_symbols_set 
form . gene_panels . data = '' 
~~ if case_obj [ 'status' ] == 'inactive' and not current_user . is_admin : 
user_obj = store . user ( current_user . email ) 
case_link = url_for ( 'cases.case' , institute_id = institute_obj [ '_id' ] , 
case_name = case_obj [ 'display_name' ] ) 
store . update_status ( institute_obj , case_obj , user_obj , 'active' , case_link ) 
~~ hgnc_symbols = [ ] 
non_clinical_symbols = [ ] 
not_found_symbols = [ ] 
not_found_ids = [ ] 
if ( form . hgnc_symbols . data ) and len ( form . hgnc_symbols . data ) > 0 : 
~~~ is_clinical = form . data . get ( 'variant_type' , 'clinical' ) == 'clinical' 
clinical_symbols = store . clinical_symbols ( case_obj ) if is_clinical else None 
for hgnc_symbol in form . hgnc_symbols . data : 
~~~ if hgnc_symbol . isdigit ( ) : 
~~~ hgnc_gene = store . hgnc_gene ( int ( hgnc_symbol ) ) 
if hgnc_gene is None : 
~~~ not_found_ids . append ( hgnc_symbol ) 
~~~ hgnc_symbols . append ( hgnc_gene [ 'hgnc_symbol' ] ) 
~~ ~~ elif store . hgnc_genes ( hgnc_symbol ) . count ( ) == 0 : 
~~~ not_found_symbols . append ( hgnc_symbol ) 
~~ elif is_clinical and ( hgnc_symbol not in clinical_symbols ) : 
~~~ non_clinical_symbols . append ( hgnc_symbol ) 
~~~ hgnc_symbols . append ( hgnc_symbol ) 
~~ ~~ ~~ if ( not_found_ids ) : 
~~ if ( not_found_symbols ) : 
~~ if ( non_clinical_symbols ) : 
~~ form . hgnc_symbols . data = hgnc_symbols 
if form . data [ 'gene_panels' ] == [ 'hpo' ] : 
~~~ hpo_symbols = list ( set ( term_obj [ 'hgnc_symbol' ] for term_obj in 
case_obj [ 'dynamic_gene_list' ] ) ) 
form . hgnc_symbols . data = hpo_symbols 
~~ variants_query = store . variants ( case_obj [ '_id' ] , query = form . data ) 
if request . form . get ( 'export' ) : 
~~~ document_header = controllers . variants_export_header ( case_obj ) 
export_lines = [ ] 
if form . data [ 'chrom' ] == 'MT' : 
~~~ export_lines = controllers . variant_export_lines ( store , case_obj , variants_query ) 
~~~ export_lines = controllers . variant_export_lines ( store , case_obj , variants_query . limit ( 500 ) ) 
~~ def generate ( header , lines ) : 
~~~ yield header + '\\n' 
~~~ yield line + '\\n' 
~~ ~~ headers = Headers ( ) 
headers . add ( 'Content-Disposition' , 'attachment' , filename = str ( case_obj [ 'display_name' ] ) + '-filtered_variants.csv' ) 
return Response ( generate ( "," . join ( document_header ) , export_lines ) , mimetype = 'text/csv' , 
headers = headers ) 
~~ data = controllers . variants ( store , institute_obj , case_obj , variants_query , page ) 
return dict ( institute = institute_obj , case = case_obj , form = form , 
severe_so_terms = SEVERE_SO_TERMS , page = page , ** data ) 
~~ def variant ( institute_id , case_name , variant_id ) : 
data = controllers . variant ( store , institute_obj , case_obj , variant_id = variant_id ) 
~~ if current_app . config . get ( 'LOQUSDB_SETTINGS' ) : 
~~~ data [ 'observations' ] = controllers . observations ( store , loqusdb , 
case_obj , data [ 'variant' ] ) 
~~ data [ 'cancer' ] = request . args . get ( 'cancer' ) == 'yes' 
return dict ( institute = institute_obj , case = case_obj , ** data ) 
~~ def str_variants ( institute_id , case_name ) : 
page = int ( request . args . get ( 'page' , 1 ) ) 
form = StrFiltersForm ( request . args ) 
query = form . data 
query [ 'variant_type' ] = variant_type 
variants_query = store . variants ( case_obj [ '_id' ] , category = 'str' , 
data = controllers . str_variants ( store , institute_obj , case_obj , 
variants_query , page ) 
return dict ( institute = institute_obj , case = case_obj , 
variant_type = variant_type , form = form , page = page , ** data ) 
~~ def sv_variants ( institute_id , case_name ) : 
form = SvFiltersForm ( request . form ) 
~~~ if panel [ 'is_default' ] : 
'thousand_genomes_frequency' : str ( institute_obj [ 'frequency_cutoff' ] ) , 
'clingen_ngi' : 10 , 
'swegen' : 10 , 
'size' : 100 , 
~~~ form = SvFiltersForm ( clinical_filter ) 
~~~ form = SvFiltersForm ( request . form ) 
~~~ form = SvFiltersForm ( request . args ) 
~~ variants_query = store . variants ( case_obj [ '_id' ] , category = 'sv' , 
query = form . data ) 
export_lines = controllers . variant_export_lines ( store , case_obj , variants_query . limit ( 500 ) ) 
def generate ( header , lines ) : 
headers . add ( 'Content-Disposition' , 'attachment' , filename = str ( case_obj [ 'display_name' ] ) + '-filtered_sv-variants.csv' ) 
~~~ data = controllers . sv_variants ( store , institute_obj , case_obj , 
~~ return dict ( institute = institute_obj , case = case_obj , variant_type = variant_type , 
form = form , severe_so_terms = SEVERE_SO_TERMS , page = page , ** data ) 
~~ def sv_variant ( institute_id , case_name , variant_id ) : 
data = controllers . sv_variant ( store , institute_id , case_name , variant_id ) 
~~ def str_variant ( institute_id , case_name , variant_id ) : 
data = controllers . str_variant ( store , institute_id , case_name , variant_id ) 
~~ def variant_update ( institute_id , case_name , variant_id ) : 
variant_obj = store . variant ( variant_id ) 
link = request . referrer 
manual_rank = request . form . get ( 'manual_rank' ) 
if manual_rank : 
~~~ new_manual_rank = int ( manual_rank ) if manual_rank != '-1' else None 
store . update_manual_rank ( institute_obj , case_obj , user_obj , link , variant_obj , 
new_manual_rank ) 
if new_manual_rank : 
~~ ~~ elif request . form . get ( 'acmg_classification' ) : 
~~~ new_acmg = request . form [ 'acmg_classification' ] 
acmg_classification = variant_obj . get ( 'acmg_classification' ) 
if isinstance ( acmg_classification , int ) and ( new_acmg == ACMG_MAP [ acmg_classification ] ) : 
~~~ new_acmg = None 
~~ store . update_acmg ( institute_obj , case_obj , user_obj , link , variant_obj , new_acmg ) 
~~ new_dismiss = request . form . getlist ( 'dismiss_variant' ) 
if request . form . getlist ( 'dismiss_variant' ) : 
~~~ store . update_dismiss_variant ( institute_obj , case_obj , user_obj , link , variant_obj , 
new_dismiss ) 
if new_dismiss : 
~~ ~~ if variant_obj . get ( 'dismiss_variant' ) and not new_dismiss : 
~~~ if 'dismiss' in request . form : 
~~ ~~ mosaic_tags = request . form . getlist ( 'mosaic_tags' ) 
if mosaic_tags : 
~~~ store . update_mosaic_tags ( institute_obj , case_obj , user_obj , link , variant_obj , 
mosaic_tags ) 
~~ ~~ if variant_obj . get ( 'mosaic_tags' ) and not mosaic_tags : 
~~~ if 'mosaic' in request . form : 
~~ ~~ return redirect ( request . referrer ) 
~~ def verify ( institute_id , case_name , variant_id , variant_category , order ) : 
comment = request . form . get ( 'verification_comment' ) 
~~~ controllers . variant_verification ( store = store , mail = mail , institute_obj = institute_obj , case_obj = case_obj , user_obj = user_obj , comment = comment , 
variant_obj = variant_obj , sender = current_app . config [ 'MAIL_USERNAME' ] , variant_url = request . referrer , order = order , url_builder = url_for ) 
~~ except controllers . MissingVerificationRecipientError : 
~~ return redirect ( request . referrer ) 
~~ def clinvar ( institute_id , case_name , variant_id ) : 
data = controllers . clinvar_export ( store , institute_id , case_name , variant_id ) 
~~ else : #POST 
~~~ form_dict = request . form . to_dict ( ) 
open_submission = store . get_open_clinvar_submission ( current_user . email , institute_id ) 
updated_submission = store . add_to_submission ( open_submission [ '_id' ] , submission_objects ) 
return redirect ( url_for ( 'cases.clinvar_submissions' , institute_id = institute_id ) ) 
~~ ~~ def cancer_variants ( institute_id , case_name ) : 
data = controllers . cancer_variants ( store , request . args , institute_id , case_name ) 
~~ def variant_acmg ( institute_id , case_name , variant_id ) : 
~~~ data = controllers . variant_acmg ( store , institute_id , case_name , variant_id ) 
~~~ criteria = [ ] 
criteria_terms = request . form . getlist ( 'criteria' ) 
for term in criteria_terms : 
~~~ criteria . append ( dict ( 
term = term , 
comment = request . form . get ( "comment-{}" . format ( term ) ) , 
links = [ request . form . get ( "link-{}" . format ( term ) ) ] , 
~~ acmg = controllers . variant_acmg_post ( store , institute_id , case_name , variant_id , 
current_user . email , criteria ) 
return redirect ( url_for ( '.variant' , institute_id = institute_id , case_name = case_name , 
variant_id = variant_id ) ) 
~~ ~~ def evaluation ( evaluation_id ) : 
evaluation_obj = store . get_evaluation ( evaluation_id ) 
controllers . evaluation ( store , evaluation_obj ) 
~~~ link = url_for ( '.variant' , institute_id = evaluation_obj [ 'institute' ] [ '_id' ] , 
case_name = evaluation_obj [ 'case' ] [ 'display_name' ] , 
variant_id = evaluation_obj [ 'variant_specific' ] ) 
store . delete_evaluation ( evaluation_obj ) 
return redirect ( link ) 
~~ return dict ( evaluation = evaluation_obj , institute = evaluation_obj [ 'institute' ] , 
case = evaluation_obj [ 'case' ] , variant = evaluation_obj [ 'variant' ] , 
CRITERIA = ACMG_CRITERIA ) 
~~ def acmg ( ) : 
criteria = request . args . getlist ( 'criterion' ) 
classification = get_acmg ( criteria ) 
return jsonify ( dict ( classification = classification ) ) 
~~ def upload_panel ( institute_id , case_name ) : 
file = form . symbol_file . data 
if file . filename == '' : 
~~ category = request . args . get ( 'category' ) 
if ( category == 'sv' ) : 
~~ hgnc_symbols = set ( form . hgnc_symbols . data ) 
hgnc_symbols . update ( new_hgnc_symbols ) 
form . hgnc_symbols . data = ',' . join ( hgnc_symbols ) 
~~~ return redirect ( url_for ( '.sv_variants' , institute_id = institute_id , case_name = case_name , 
** form . data ) , code = 307 ) 
~~~ return redirect ( url_for ( '.variants' , institute_id = institute_id , case_name = case_name , 
~~ ~~ def download_verified ( ) : 
user_institutes = user_obj . get ( 'institutes' ) 
temp_excel_dir = os . path . join ( variants_bp . static_folder , 'verified_folder' ) 
os . makedirs ( temp_excel_dir , exist_ok = True ) 
written_files = controllers . verified_excel_file ( store , user_institutes , temp_excel_dir ) 
if written_files : 
~~~ today = datetime . datetime . now ( ) . strftime ( '%Y-%m-%d' ) 
data = io . BytesIO ( ) 
with zipfile . ZipFile ( data , mode = 'w' ) as z : 
~~~ for f_name in pathlib . Path ( temp_excel_dir ) . iterdir ( ) : 
~~~ zipfile . ZipFile 
z . write ( f_name , os . path . basename ( f_name ) ) 
~~ ~~ data . seek ( 0 ) 
shutil . rmtree ( temp_excel_dir ) 
return send_file ( 
mimetype = 'application/zip' , 
as_attachment = True , 
attachment_filename = '_' . join ( [ 'scout' , 'verified_variants' , today ] ) + '.zip' 
~~ ~~ def genes_by_alias ( hgnc_genes ) : 
alias_genes = { } 
for hgnc_id in hgnc_genes : 
~~~ gene = hgnc_genes [ hgnc_id ] 
hgnc_symbol = gene [ 'hgnc_symbol' ] 
for alias in gene [ 'previous_symbols' ] : 
~~~ true_id = None 
if alias == hgnc_symbol : 
~~~ true_id = hgnc_id 
~~ if alias in alias_genes : 
~~~ alias_genes [ alias . upper ( ) ] [ 'ids' ] . add ( hgnc_id ) 
if true_id : 
~~~ alias_genes [ alias . upper ( ) ] [ 'true_id' ] = hgnc_id 
~~~ alias_genes [ alias . upper ( ) ] = { 
'true' : true_id , 
'ids' : set ( [ hgnc_id ] ) 
~~ ~~ ~~ return alias_genes 
~~ def add_ensembl_info ( genes , ensembl_lines ) : 
if isinstance ( ensembl_lines , DataFrame ) : 
~~~ ensembl_genes = parse_ensembl_gene_request ( ensembl_lines ) 
~~~ ensembl_genes = parse_ensembl_genes ( ensembl_lines ) 
~~ for ensembl_gene in ensembl_genes : 
~~~ gene_obj = genes . get ( ensembl_gene [ 'hgnc_id' ] ) 
if not gene_obj : 
~~ gene_obj [ 'chromosome' ] = ensembl_gene [ 'chrom' ] 
gene_obj [ 'start' ] = ensembl_gene [ 'gene_start' ] 
gene_obj [ 'end' ] = ensembl_gene [ 'gene_end' ] 
gene_obj [ 'ensembl_gene_id' ] = ensembl_gene [ 'ensembl_gene_id' ] 
~~ ~~ def add_exac_info ( genes , alias_genes , exac_lines ) : 
for exac_gene in parse_exac_genes ( exac_lines ) : 
~~~ hgnc_symbol = exac_gene [ 'hgnc_symbol' ] . upper ( ) 
pli_score = exac_gene [ 'pli_score' ] 
for hgnc_id in get_correct_ids ( hgnc_symbol , alias_genes ) : 
~~~ genes [ hgnc_id ] [ 'pli_score' ] = pli_score 
~~ ~~ ~~ def add_omim_info ( genes , alias_genes , genemap_lines , mim2gene_lines ) : 
omim_genes = get_mim_genes ( genemap_lines , mim2gene_lines ) 
for hgnc_symbol in omim_genes : 
~~~ omim_info = omim_genes [ hgnc_symbol ] 
inheritance = omim_info . get ( 'inheritance' , set ( ) ) 
~~~ gene_info = genes [ hgnc_id ] 
gene_info [ 'omim_id' ] = omim_info [ 'mim_number' ] 
gene_info [ 'inheritance_models' ] = list ( inheritance ) 
gene_info [ 'phenotypes' ] = omim_info . get ( 'phenotypes' , [ ] ) 
~~ ~~ ~~ def add_incomplete_penetrance ( genes , alias_genes , hpo_lines ) : 
for hgnc_symbol in get_incomplete_penetrance_genes ( hpo_lines ) : 
~~~ for hgnc_id in get_correct_ids ( hgnc_symbol , alias_genes ) : 
~~~ genes [ hgnc_id ] [ 'incomplete_penetrance' ] = True 
~~ ~~ ~~ def get_correct_ids ( hgnc_symbol , alias_genes ) : 
hgnc_ids = set ( ) 
hgnc_symbol = hgnc_symbol . upper ( ) 
if hgnc_symbol in alias_genes : 
~~~ hgnc_id_info = alias_genes [ hgnc_symbol ] 
if hgnc_id_info [ 'true' ] : 
~~~ return set ( [ hgnc_id_info [ 'true' ] ] ) 
~~~ return set ( hgnc_id_info [ 'ids' ] ) 
~~ ~~ return hgnc_ids 
~~ def link_genes ( ensembl_lines , hgnc_lines , exac_lines , mim2gene_lines , 
genemap_lines , hpo_lines ) : 
genes = { } 
for hgnc_gene in parse_hgnc_genes ( hgnc_lines ) : 
~~~ hgnc_id = hgnc_gene [ 'hgnc_id' ] 
genes [ hgnc_id ] = hgnc_gene 
~~ add_ensembl_info ( genes , ensembl_lines ) 
symbol_to_id = genes_by_alias ( genes ) 
add_exac_info ( genes , symbol_to_id , exac_lines ) 
add_omim_info ( genes , symbol_to_id , genemap_lines , mim2gene_lines ) 
add_incomplete_penetrance ( genes , symbol_to_id , hpo_lines ) 
return genes 
~~ def matchmaker_request ( url , token , method , content_type = None , accept = None , data = None ) : 
headers = Headers ( ) 
headers = { 'X-Auth-Token' : token } 
if content_type : 
~~ if accept : 
~~~ headers [ 'Accept' ] = accept 
~~ req_data = data or { 'timestamp' : datetime . datetime . now ( ) . timestamp ( ) } 
json_response = None 
method , url , req_data ) ) 
resp = requests . request ( 
data = json . dumps ( req_data ) 
json_response = resp . json ( ) 
if isinstance ( json_response , str ) : 
~~~ json_response = { 
'message' : json_response , 
~~~ return json_response 
~~ json_response [ 'status_code' ] = resp . status_code 
json_response = { 
'message' : str ( err ) 
~~ return json_response 
~~ def mme_nodes ( mme_base_url , token ) : 
if not mme_base_url or not token : 
~~~ return nodes 
~~ url = '' . join ( [ mme_base_url , '/nodes' ] ) 
nodes = matchmaker_request ( url = url , token = token , method = 'GET' ) 
return nodes 
~~ def get_cytoband_coordinates ( chrom , pos ) : 
coordinate = "" 
if chrom in CYTOBANDS : 
~~~ for interval in CYTOBANDS [ chrom ] [ pos ] : 
~~~ coordinate = interval . data 
~~ ~~ return coordinate 
~~ def get_sub_category ( alt_len , ref_len , category , svtype = None ) : 
subcategory = '' 
if category in ( 'snv' , 'indel' , 'cancer' ) : 
~~~ if ref_len == alt_len : 
~~~ subcategory = 'snv' 
~~~ subcategory = 'indel' 
~~ ~~ elif category == 'sv' : 
~~~ subcategory = svtype 
~~ return subcategory 
~~ def get_length ( alt_len , ref_len , category , pos , end , svtype = None , svlen = None ) : 
length = - 1 
~~~ length = alt_len 
~~~ length = abs ( ref_len - alt_len ) 
~~~ if svtype == 'bnd' : 
~~~ length = int ( 10e10 ) 
~~~ if svlen : 
~~~ length = abs ( int ( svlen ) ) 
~~ elif end : 
~~~ if end != pos : 
~~~ length = end - pos 
~~ ~~ ~~ ~~ return length 
~~ def get_end ( pos , alt , category , snvend = None , svend = None , svlen = None ) : 
end = pos 
~~~ end = snvend 
~~ elif category == 'sv' : 
~~~ end = svend 
if svend == pos : 
~~~ end = pos + svlen 
~~ ~~ if ':' in alt : 
~~~ match = BND_ALT_PATTERN . match ( alt ) 
~~~ end = int ( match . group ( 2 ) ) 
~~ ~~ ~~ return end 
~~ def parse_coordinates ( variant , category ) : 
ref = variant . REF 
if variant . ALT : 
~~~ alt = variant . ALT [ 0 ] 
~~ if category == "str" and not variant . ALT : 
~~~ alt = '.' 
~~ chrom_match = CHR_PATTERN . match ( variant . CHROM ) 
chrom = chrom_match . group ( 2 ) 
svtype = variant . INFO . get ( 'SVTYPE' ) 
if svtype : 
~~~ svtype = svtype . lower ( ) 
~~ mate_id = variant . INFO . get ( 'MATEID' ) 
svlen = variant . INFO . get ( 'SVLEN' ) 
svend = variant . INFO . get ( 'END' ) 
snvend = int ( variant . end ) 
position = int ( variant . POS ) 
ref_len = len ( ref ) 
alt_len = len ( alt ) 
sub_category = get_sub_category ( alt_len , ref_len , category , svtype ) 
end = get_end ( position , alt , category , snvend , svend ) 
length = get_length ( alt_len , ref_len , category , position , end , svtype , svlen ) 
end_chrom = chrom 
if sub_category == 'bnd' : 
~~~ if ':' in alt : 
~~~ other_chrom = match . group ( 1 ) 
match = CHR_PATTERN . match ( other_chrom ) 
end_chrom = match . group ( 2 ) 
~~ ~~ ~~ cytoband_start = get_cytoband_coordinates ( chrom , position ) 
cytoband_end = get_cytoband_coordinates ( end_chrom , end ) 
coordinates = { 
'position' : position , 
'end' : end , 
'length' : length , 
'sub_category' : sub_category , 
'mate_id' : mate_id , 
'cytoband_start' : cytoband_start , 
'cytoband_end' : cytoband_end , 
'end_chrom' : end_chrom , 
return coordinates 
~~ def parse_cytoband ( lines ) : 
cytobands = { } 
splitted_line = line . split ( '\\t' ) 
chrom = splitted_line [ 0 ] . lstrip ( 'chr' ) 
start = int ( splitted_line [ 1 ] ) 
stop = int ( splitted_line [ 2 ] ) 
name = splitted_line [ 3 ] 
if chrom in cytobands : 
~~~ cytobands [ chrom ] [ start : stop ] = name 
~~~ new_tree = intervaltree . IntervalTree ( ) 
new_tree [ start : stop ] = name 
cytobands [ chrom ] = new_tree 
~~ ~~ return cytobands 
~~ def cli ( infile ) : 
lines = get_file_handle ( infile ) 
cytobands = parse_cytoband ( lines ) 
intervals = cytobands [ '1' ] [ 2 ] 
~~~ print ( interval ) 
print ( interval . begin ) 
print ( interval . end ) 
print ( interval . data ) 
~~ print ( cytobands [ '1' ] [ 2 ] ) 
print ( cytobands [ '8' ] [ 101677777 ] ) 
print ( cytobands [ 'X' ] [ 4200000 : 6000000 ] ) 
~~ def update_panel ( adapter , panel_name , panel_version , new_version = None , new_date = None ) : 
panel_obj = adapter . gene_panel ( panel_name , panel_version ) 
if not panel_obj : 
~~ updated_panel = adapter . update_panel ( panel_obj , new_version , new_date ) 
panel_id = updated_panel [ '_id' ] 
update = { '$set' : { } } 
if new_version : 
~~~ update [ '$set' ] [ 'panels.$.version' ] = updated_panel [ 'version' ] 
~~ if new_date : 
~~~ update [ '$set' ] [ 'panels.$.updated_at' ] = updated_panel [ 'date' ] 
query = { 'panels' : { '$elemMatch' : { 'panel_name' : panel_name } } } 
adapter . case_collection . update_many ( query , update ) 
return updated_panel 
~~ def cli ( context , mongodb , username , password , authdb , host , port , loglevel , config , demo ) : 
log_format = None 
coloredlogs . install ( level = loglevel , fmt = log_format ) 
mongo_config = { } 
cli_config = { } 
with open ( config , 'r' ) as in_handle : 
~~~ cli_config = yaml . load ( in_handle ) 
~~ ~~ mongo_config [ 'mongodb' ] = ( mongodb or cli_config . get ( 'mongodb' ) or 'scout' ) 
if demo : 
~~~ mongo_config [ 'mongodb' ] = 'scout-demo' 
~~ mongo_config [ 'host' ] = ( host or cli_config . get ( 'host' ) or 'localhost' ) 
mongo_config [ 'port' ] = ( port or cli_config . get ( 'port' ) or 27017 ) 
mongo_config [ 'username' ] = username or cli_config . get ( 'username' ) 
mongo_config [ 'password' ] = password or cli_config . get ( 'password' ) 
mongo_config [ 'authdb' ] = authdb or cli_config . get ( 'authdb' ) or mongo_config [ 'mongodb' ] 
mongo_config [ 'omim_api_key' ] = cli_config . get ( 'omim_api_key' ) 
if context . invoked_subcommand in ( 'setup' , 'serve' ) : 
~~~ mongo_config [ 'adapter' ] = None 
~~~ client = get_connection ( ** mongo_config ) 
~~ except ConnectionFailure : 
~~~ context . abort ( ) 
~~ database = client [ mongo_config [ 'mongodb' ] ] 
mongo_config [ 'client' ] = client 
adapter = MongoAdapter ( database ) 
mongo_config [ 'adapter' ] = adapter 
~~~ for ins_obj in adapter . institutes ( ) : 
~~ ~~ except OperationFailure as err : 
~~ ~~ context . obj = mongo_config 
~~ def parse_exac_line ( line , header ) : 
exac_gene = { } 
splitted_line = line . rstrip ( ) . split ( '\\t' ) 
exac_gene = dict ( zip ( header , splitted_line ) ) 
exac_gene [ 'hgnc_symbol' ] = exac_gene [ 'gene' ] 
exac_gene [ 'pli_score' ] = float ( exac_gene [ 'pLI' ] ) 
exac_gene [ 'raw' ] = line 
return exac_gene 
~~ def parse_exac_genes ( lines ) : 
header = [ ] 
for index , line in enumerate ( lines ) : 
~~~ if index == 0 : 
~~~ header = line . rstrip ( ) . split ( '\\t' ) 
~~ elif len ( line ) > 0 : 
~~~ exac_gene = parse_exac_line ( line , header ) 
yield exac_gene 
~~ ~~ ~~ def panels ( ) : 
~~~ csv_file = request . files [ 'csv_file' ] 
content = csv_file . stream . read ( ) 
lines = None 
~~~ if b'\\n' in content : 
~~~ lines = content . decode ( 'utf-8' , 'ignore' ) . split ( '\\n' ) 
~~~ lines = content . decode ( 'windows-1252' ) . split ( '\\r' ) 
~~ ~~ except Exception as err : 
~~ new_panel_name = request . form . get ( 'new_panel_name' ) 
~~~ new_panel_id = controllers . new_panel ( 
store = store , 
institute_id = request . form [ 'institute' ] , 
panel_name = new_panel_name , 
display_name = request . form [ 'display_name' ] , 
csv_lines = lines , 
if new_panel_id is None : 
~~ return redirect ( url_for ( 'panels.panel' , panel_id = new_panel_id ) ) 
~~~ update_option = request . form [ 'modify_option' ] 
panel_obj = controllers . update_panel ( 
panel_name = request . form [ 'panel_name' ] , 
option = update_option 
if panel_obj is None : 
~~~ return redirect ( url_for ( 'panels.panel' , panel_id = panel_obj [ '_id' ] ) ) 
~~ ~~ ~~ institutes = list ( user_institutes ( store , current_user ) ) 
panel_names = [ name 
for institute in institutes 
for name in 
store . gene_panels ( institute_id = institute [ '_id' ] ) . distinct ( 'panel_name' ) ] 
panel_versions = { } 
for name in panel_names : 
~~~ panel_versions [ name ] = store . gene_panels ( panel_id = name ) 
~~ panel_groups = [ ] 
for institute_obj in institutes : 
~~~ institute_panels = store . latest_panels ( institute_obj [ '_id' ] ) 
panel_groups . append ( ( institute_obj , institute_panels ) ) 
~~ return dict ( panel_groups = panel_groups , panel_names = panel_names , 
panel_versions = panel_versions , institutes = institutes ) 
~~ def panel ( panel_id ) : 
panel_obj = store . gene_panel ( panel_id ) or store . panel ( panel_id ) 
~~~ raw_hgnc_id = request . form [ 'hgnc_id' ] 
if '|' in raw_hgnc_id : 
~~ hgnc_id = 0 
~~~ hgnc_id = int ( raw_hgnc_id ) 
~~ action = request . form [ 'action' ] 
gene_obj = store . hgnc_gene ( hgnc_id ) 
if gene_obj is None : 
~~ if action == 'add' : 
~~~ panel_gene = controllers . existing_gene ( store , panel_obj , hgnc_id ) 
if panel_gene : 
'warning' ) 
~~~ return redirect ( url_for ( '.gene_edit' , panel_id = panel_id , 
hgnc_id = hgnc_id ) ) 
~~ ~~ elif action == 'delete' : 
panel_obj = store . add_pending ( panel_obj , gene_obj , action = 'delete' ) 
~~ ~~ data = controllers . panel ( store , panel_obj ) 
if request . args . get ( 'case_id' ) : 
~~~ data [ 'case' ] = store . case ( request . args [ 'case_id' ] ) 
~~ if request . args . get ( 'institute_id' ) : 
~~~ data [ 'institute' ] = store . institute ( request . args [ 'institute_id' ] ) 
~~ def panel_update ( panel_id ) : 
panel_obj = store . panel ( panel_id ) 
update_version = request . form . get ( 'version' , None ) 
new_panel_id = store . apply_pending ( panel_obj , update_version ) 
return redirect ( url_for ( 'panels.panel' , panel_id = new_panel_id ) ) 
~~ def panel_export ( panel_id ) : 
data = controllers . panel_export ( store , panel_obj ) 
data [ 'report_created_at' ] = datetime . datetime . now ( ) . strftime ( "%Y-%m-%d" ) 
html_report = render_template ( 'panels/panel_pdf_simple.html' , ** data ) 
return render_pdf ( HTML ( string = html_report ) , download_filename = data [ 'panel' ] [ 'panel_name' ] + '_' + str ( data [ 'panel' ] [ 'version' ] ) + '_' + datetime . datetime . now ( ) . strftime ( "%Y-%m-%d" ) + '_scout.pdf' ) 
~~ def gene_edit ( panel_id , hgnc_id ) : 
hgnc_gene = store . hgnc_gene ( hgnc_id ) 
panel_gene = controllers . existing_gene ( store , panel_obj , hgnc_id ) 
form = PanelGeneForm ( ) 
transcript_choices = [ ] 
for transcript in hgnc_gene [ 'transcripts' ] : 
~~~ if transcript . get ( 'refseq_id' ) : 
~~~ refseq_id = transcript . get ( 'refseq_id' ) 
transcript_choices . append ( ( refseq_id , refseq_id ) ) 
~~ ~~ form . disease_associated_transcripts . choices = transcript_choices 
if form . validate_on_submit ( ) : 
~~~ action = 'edit' if panel_gene else 'add' 
info_data = form . data . copy ( ) 
if 'csrf_token' in info_data : 
~~~ del info_data [ 'csrf_token' ] 
~~ store . add_pending ( panel_obj , hgnc_gene , action = action , info = info_data ) 
return redirect ( url_for ( '.panel' , panel_id = panel_id ) ) 
~~ if panel_gene : 
~~~ for field_key in [ 'disease_associated_transcripts' , 'reduced_penetrance' , 
'mosaicism' , 'inheritance_models' , 'database_entry_version' , 'comment' ] : 
~~~ form_field = getattr ( form , field_key ) 
if not form_field . data : 
~~~ panel_value = panel_gene . get ( field_key ) 
if panel_value is not None : 
~~~ form_field . process_data ( panel_value ) 
~~ ~~ ~~ ~~ return dict ( panel = panel_obj , form = form , gene = hgnc_gene , panel_gene = panel_gene ) 
~~ def delivery_report ( context , case_id , report_path , 
update ) : 
~~~ load_delivery_report ( adapter = adapter , case_id = case_id , 
report_path = report_path , update = update ) 
~~~ LOG . error ( e ) 
~~ ~~ def parse_peddy_ped ( lines ) : 
peddy_ped = [ ] 
for i , line in enumerate ( lines ) : 
~~~ header = line . lstrip ( '#' ) . split ( '\\t' ) 
~~~ ind_info = dict ( zip ( header , line . split ( '\\t' ) ) ) 
ind_info [ 'PC1' ] = convert_number ( ind_info [ 'PC1' ] ) 
ind_info [ 'PC2' ] = convert_number ( ind_info [ 'PC2' ] ) 
ind_info [ 'PC3' ] = convert_number ( ind_info [ 'PC3' ] ) 
ind_info [ 'het_call_rate' ] = convert_number ( ind_info [ 'het_call_rate' ] ) 
ind_info [ 'het_idr_baf' ] = convert_number ( ind_info [ 'het_idr_baf' ] ) 
ind_info [ 'het_mean_depth' ] = convert_number ( ind_info [ 'het_mean_depth' ] ) 
peddy_ped . append ( ind_info ) 
~~ ~~ return peddy_ped 
~~ def parse_peddy_ped_check ( lines ) : 
ped_check = [ ] 
~~~ header = line . lstrip ( '#' ) . split ( ',' ) 
~~~ pair_info = dict ( zip ( header , line . split ( ',' ) ) ) 
pair_info [ 'hets_a' ] = convert_number ( pair_info [ 'hets_a' ] ) 
pair_info [ 'hets_b' ] = convert_number ( pair_info [ 'hets_b' ] ) 
pair_info [ 'ibs0' ] = convert_number ( pair_info [ 'ibs0' ] ) 
pair_info [ 'ibs2' ] = convert_number ( pair_info [ 'ibs2' ] ) 
pair_info [ 'n' ] = convert_number ( pair_info [ 'n' ] ) 
pair_info [ 'rel' ] = convert_number ( pair_info [ 'rel' ] ) 
pair_info [ 'pedigree_relatedness' ] = convert_number ( pair_info [ 'pedigree_relatedness' ] ) 
pair_info [ 'rel_difference' ] = convert_number ( pair_info [ 'rel_difference' ] ) 
pair_info [ 'shared_hets' ] = convert_number ( pair_info [ 'shared_hets' ] ) 
pair_info [ 'pedigree_parents' ] = make_bool ( pair_info . get ( 'pedigree_parents' ) ) 
pair_info [ 'predicted_parents' ] = make_bool ( pair_info . get ( 'predicted_parents' ) ) 
pair_info [ 'parent_error' ] = make_bool ( pair_info . get ( 'parent_error' ) ) 
pair_info [ 'sample_duplication_error' ] = make_bool ( pair_info . get ( 'sample_duplication_error' ) ) 
ped_check . append ( pair_info ) 
~~ ~~ return ped_check 
~~ def parse_peddy_sex_check ( lines ) : 
sex_check = [ ] 
~~~ ind_info = dict ( zip ( header , line . split ( ',' ) ) ) 
ind_info [ 'error' ] = make_bool ( ind_info . get ( 'error' ) ) 
ind_info [ 'hom_alt_count' ] = convert_number ( ind_info [ 'hom_alt_count' ] ) 
ind_info [ 'hom_ref_count' ] = convert_number ( ind_info [ 'hom_ref_count' ] ) 
ind_info [ 'het_count' ] = convert_number ( ind_info [ 'het_count' ] ) 
ind_info [ 'het_ratio' ] = convert_number ( ind_info [ 'het_ratio' ] ) 
sex_check . append ( ind_info ) 
~~ ~~ return sex_check 
~~ def hpo_terms ( store , query = None , limit = None ) : 
hpo_phenotypes = { } 
~~~ limit = int ( limit ) 
~~ hpo_phenotypes [ 'phenotypes' ] = list ( store . hpo_terms ( text = query , limit = limit ) ) 
return hpo_phenotypes 
~~ def whitelist ( context ) : 
for whitelist_obj in adapter . whitelist_collection . find ( ) : 
~~~ click . echo ( whitelist_obj [ '_id' ] ) 
~~ ~~ def build_phenotype ( phenotype_id , adapter ) : 
phenotype_obj = { } 
phenotype = adapter . hpo_term ( phenotype_id ) 
if phenotype : 
~~~ phenotype_obj [ 'phenotype_id' ] = phenotype [ 'hpo_id' ] 
phenotype_obj [ 'feature' ] = phenotype [ 'description' ] 
~~ return phenotype 
~~ def build_case ( case_data , adapter ) : 
case_obj = { 
'_id' : case_data [ 'case_id' ] , 
'display_name' : case_data . get ( 'display_name' , case_data [ 'case_id' ] ) , 
~~~ institute_id = case_data [ 'owner' ] 
~~ institute_obj = adapter . institute ( institute_id ) 
~~ case_obj [ 'owner' ] = case_data [ 'owner' ] 
collaborators = set ( case_data . get ( 'collaborators' , [ ] ) ) 
collaborators . add ( case_data [ 'owner' ] ) 
case_obj [ 'collaborators' ] = list ( collaborators ) 
if case_data . get ( 'assignee' ) : 
~~~ case_obj [ 'assignees' ] = [ case_data [ 'assignee' ] ] 
~~ ind_objs = [ ] 
~~~ for individual in case_data . get ( 'individuals' , [ ] ) : 
~~~ ind_objs . append ( build_individual ( individual ) ) 
~~ ~~ except Exception as error : 
~~ sorted_inds = sorted ( ind_objs , key = lambda ind : - ind [ 'phenotype' ] ) 
case_obj [ 'individuals' ] = sorted_inds 
now = datetime . now ( ) 
case_obj [ 'created_at' ] = now 
case_obj [ 'updated_at' ] = now 
if case_data . get ( 'suspects' ) : 
~~~ case_obj [ 'suspects' ] = case_data [ 'suspects' ] 
~~ if case_data . get ( 'causatives' ) : 
~~~ case_obj [ 'causatives' ] = case_data [ 'causatives' ] 
~~ case_obj [ 'synopsis' ] = case_data . get ( 'synopsis' , '' ) 
case_obj [ 'status' ] = 'inactive' 
case_obj [ 'is_research' ] = False 
case_obj [ 'research_requested' ] = False 
case_obj [ 'rerun_requested' ] = False 
analysis_date = case_data . get ( 'analysis_date' ) 
if analysis_date : 
~~~ case_obj [ 'analysis_date' ] = analysis_date 
~~ case_panels = case_data . get ( 'gene_panels' , [ ] ) 
default_panels = case_data . get ( 'default_panels' , [ ] ) 
panels = [ ] 
for panel_name in case_panels : 
~~~ panel_obj = adapter . gene_panel ( panel_name ) 
~~ panel = { 
'panel_id' : panel_obj [ '_id' ] , 
'panel_name' : panel_obj [ 'panel_name' ] , 
'display_name' : panel_obj [ 'display_name' ] , 
'version' : panel_obj [ 'version' ] , 
'updated_at' : panel_obj [ 'date' ] , 
'nr_genes' : len ( panel_obj [ 'genes' ] ) 
if panel_name in default_panels : 
~~~ panel [ 'is_default' ] = True 
~~~ panel [ 'is_default' ] = False 
~~ panels . append ( panel ) 
~~ case_obj [ 'panels' ] = panels 
case_obj [ 'dynamic_gene_list' ] = { } 
genome_build = case_data . get ( 'genome_build' , '37' ) 
if not genome_build in [ '37' , '38' ] : 
~~ case_obj [ 'genome_build' ] = genome_build 
case_obj [ 'genome_version' ] = case_data . get ( 'genome_version' ) 
if case_data . get ( 'rank_model_version' ) : 
~~~ case_obj [ 'rank_model_version' ] = str ( case_data [ 'rank_model_version' ] ) 
~~ if case_data . get ( 'sv_rank_model_version' ) : 
~~~ case_obj [ 'sv_rank_model_version' ] = str ( case_data [ 'sv_rank_model_version' ] ) 
~~ if case_data . get ( 'rank_score_threshold' ) : 
~~~ case_obj [ 'rank_score_threshold' ] = float ( case_data [ 'rank_score_threshold' ] ) 
~~ phenotypes = [ ] 
for phenotype in case_data . get ( 'phenotype_terms' , [ ] ) : 
~~~ phenotype_obj = build_phenotype ( phenotype , adapter ) 
if phenotype_obj : 
~~~ phenotypes . append ( phenotype_obj ) 
~~ ~~ if phenotypes : 
~~~ case_obj [ 'phenotype_terms' ] = phenotypes 
~~ phenotype_groups = [ ] 
for phenotype in case_data . get ( 'phenotype_groups' , [ ] ) : 
~~~ phenotype_groups . append ( phenotype_obj ) 
~~ ~~ if phenotype_groups : 
~~~ case_obj [ 'phenotype_groups' ] = phenotype_groups 
~~ case_obj [ 'madeline_info' ] = case_data . get ( 'madeline_info' ) 
if 'multiqc' in case_data : 
~~~ case_obj [ 'multiqc' ] = case_data . get ( 'multiqc' ) 
~~ case_obj [ 'vcf_files' ] = case_data . get ( 'vcf_files' , { } ) 
case_obj [ 'delivery_report' ] = case_data . get ( 'delivery_report' ) 
case_obj [ 'has_svvariants' ] = False 
if ( case_obj [ 'vcf_files' ] . get ( 'vcf_sv' ) or case_obj [ 'vcf_files' ] . get ( 'vcf_sv_research' ) ) : 
~~~ case_obj [ 'has_svvariants' ] = True 
~~ case_obj [ 'has_strvariants' ] = False 
if ( case_obj [ 'vcf_files' ] . get ( 'vcf_str' ) ) : 
~~~ case_obj [ 'has_strvariants' ] = True 
~~ case_obj [ 'is_migrated' ] = False 
case_obj [ 'track' ] = case_data . get ( 'track' , 'rare' ) 
return case_obj 
~~ def gene ( store , hgnc_id ) : 
res = { 'builds' : { '37' : None , '38' : None } , 'symbol' : None , 'description' : None , 'ensembl_id' : None , 'record' : None } 
for build in res [ 'builds' ] : 
~~~ record = store . hgnc_gene ( hgnc_id , build = build ) 
if record : 
~~~ record [ 'position' ] = "{this[chromosome]}:{this[start]}-{this[end]}" . format ( this = record ) 
res [ 'aliases' ] = record [ 'aliases' ] 
res [ 'hgnc_id' ] = record [ 'hgnc_id' ] 
res [ 'description' ] = record [ 'description' ] 
res [ 'builds' ] [ build ] = record 
res [ 'symbol' ] = record [ 'hgnc_symbol' ] 
res [ 'entrez_id' ] = record . get ( 'entrez_id' ) 
res [ 'pli_score' ] = record . get ( 'pli_score' ) 
add_gene_links ( record , int ( build ) ) 
res [ 'omim_id' ] = record . get ( 'omim_id' ) 
res [ 'incomplete_penetrance' ] = record . get ( 'incomplete_penetrance' , False ) 
res [ 'inheritance_models' ] = record . get ( 'inheritance_models' , [ ] ) 
for transcript in record [ 'transcripts' ] : 
~~~ transcript [ 'position' ] = ( "{this[chrom]}:{this[start]}-{this[end]}" 
. format ( this = transcript ) ) 
add_tx_links ( transcript , build ) 
~~ for phenotype in record . get ( 'phenotypes' , [ ] ) : 
~~~ phenotype [ 'omim_link' ] = omim ( phenotype . get ( 'mim_number' ) ) 
~~ if not res [ 'record' ] : 
~~~ res [ 'record' ] = record 
~~ ~~ ~~ if not any ( res . values ( ) ) : 
~~ def genes_to_json ( store , query ) : 
gene_query = store . hgnc_genes ( query , search = True ) 
'id' : gene [ 'hgnc_id' ] } for gene in gene_query ] 
return json_terms 
accessible_institutes = current_user . institutes 
if not 'admin' in current_user . roles : 
~~~ accessible_institutes = current_user . institutes 
if not accessible_institutes : 
return redirect ( url_for ( 'cases.dahboard_general.html' ) ) 
institutes = [ inst for inst in store . institutes ( accessible_institutes ) ] 
institute_id = None 
slice_query = None 
panel = 1 
~~~ institute_id = request . form . get ( 'institute' ) 
slice_query = request . form . get ( 'query' ) 
panel = request . form . get ( 'pane_id' ) 
~~ elif request . method == 'GET' : 
~~~ institute_id = request . args . get ( 'institute' ) 
slice_query = request . args . get ( 'query' ) 
~~ if not institute_id : 
~~~ institute_id = accessible_institutes [ 0 ] 
~~ elif ( not current_user . is_admin ) and ( slice_query and institute_id == 'None' ) : 
~~ elif ( not institute_id in accessible_institutes ) and not ( institute_id == 'None' ) : 
data = get_dashboard_info ( store , institute_id , slice_query ) 
data [ 'institutes' ] = institutes 
data [ 'choice' ] = institute_id 
total_cases = data [ 'total_cases' ] 
if total_cases == 0 : 
~~ return render_template ( 
'dashboard/dashboard_general.html' , institute = institute_id , query = slice_query , panel = panel , ** data ) 
~~ def get_request ( url ) : 
response = urllib . request . urlopen ( url ) 
if url . endswith ( '.gz' ) : 
~~ decoded_data = data . decode ( 'utf-8' ) 
~~ except HTTPError as err : 
raise err 
~~ except URLError as err : 
~~ if 'Error' in decoded_data : 
~~ return decoded_data 
~~ def fetch_resource ( url ) : 
~~~ data = get_request ( url ) 
lines = data . split ( '\\n' ) 
~~ def fetch_mim_files ( api_key , mim2genes = False , mimtitles = False , morbidmap = False , genemap2 = False ) : 
mim2genes_url = 'https://omim.org/static/omim/data/mim2gene.txt' 
mimtitles_url = 'https://data.omim.org/downloads/{0}/mimTitles.txt' . format ( api_key ) 
morbidmap_url = 'https://data.omim.org/downloads/{0}/morbidmap.txt' . format ( api_key ) 
genemap2_url = 'https://data.omim.org/downloads/{0}/genemap2.txt' . format ( api_key ) 
mim_files = { } 
mim_urls = { } 
if mim2genes is True : 
~~~ mim_urls [ 'mim2genes' ] = mim2genes_url 
~~ if mimtitles is True : 
~~~ mim_urls [ 'mimtitles' ] = mimtitles_url 
~~ if morbidmap is True : 
~~~ mim_urls [ 'morbidmap' ] = morbidmap_url 
~~ if genemap2 is True : 
~~~ mim_urls [ 'genemap2' ] = genemap2_url 
~~ for file_name in mim_urls : 
~~~ url = mim_urls [ file_name ] 
mim_files [ file_name ] = fetch_resource ( url ) 
~~ return mim_files 
~~ def fetch_ensembl_genes ( build = '37' ) : 
if build == '37' : 
~~~ url = 'http://grch37.ensembl.org' 
~~~ url = 'http://www.ensembl.org' 
dataset_name = 'hsapiens_gene_ensembl' 
dataset = pybiomart . Dataset ( name = dataset_name , host = url ) 
attributes = [ 
'chromosome_name' , 
'start_position' , 
'end_position' , 
'ensembl_gene_id' , 
'hgnc_symbol' , 
'hgnc_id' , 
filters = { 
'chromosome_name' : CHROMOSOMES , 
result = dataset . query ( 
attributes = attributes , 
filters = filters , 
use_attr_names = True , 
~~ def fetch_ensembl_exons ( build = '37' ) : 
~~ dataset_name = 'hsapiens_gene_ensembl' 
'ensembl_transcript_id' , 
'ensembl_exon_id' , 
'exon_chrom_start' , 
'exon_chrom_end' , 
'5_utr_start' , 
'5_utr_end' , 
'3_utr_start' , 
'3_utr_end' , 
'strand' , 
'rank' 
filters = filters 
~~ def fetch_hgnc ( ) : 
file_name = "hgnc_complete_set.txt" 
url = 'ftp://ftp.ebi.ac.uk/pub/databases/genenames/new/tsv/{0}' . format ( file_name ) 
hgnc_lines = fetch_resource ( url ) 
return hgnc_lines 
~~ def fetch_exac_constraint ( ) : 
file_name = 'fordist_cleaned_exac_r03_march16_z_pli_rec_null_data.txt' 
url = ( 'ftp://ftp.broadinstitute.org/pub/ExAC_release/release0.3/functional_gene_constraint' 
'/{0}' ) . format ( file_name ) 
~~~ exac_lines = fetch_resource ( url ) 
url = ( "https://storage.googleapis.com/gnomad-public/legacy/exacv1_downloads/release0.3.1" 
"/manuscript_data/forweb_cleaned_exac_r03_march16_z_data_pLI.txt.gz" ) 
~~ exac_lines = fetch_resource ( url ) 
return exac_lines 
~~ def fetch_hpo_files ( hpogenes = False , hpoterms = False , phenotype_to_terms = False , hpodisease = False ) : 
base_url = ( 'http://compbio.charite.de/jenkins/job/hpo.annotations.monthly/' 
'lastStableBuild/artifact/annotation/{}' ) 
hpogenes_url = base_url . format ( 'ALL_SOURCES_ALL_FREQUENCIES_genes_to_phenotype.txt' ) 
hpoterms_url = base_url . format ( 'ALL_SOURCES_ALL_FREQUENCIES_phenotype_to_genes.txt' ) 
hpo_phenotype_to_terms_url = base_url . format ( 'ALL_SOURCES_ALL_FREQUENCIES_diseases_to_genes_to_phenotypes.txt' ) 
hpodisease_url = base_url . format ( 'diseases_to_genes.txt' ) 
hpo_files = { } 
hpo_urls = { } 
if hpogenes is True : 
~~~ hpo_urls [ 'hpogenes' ] = hpogenes_url 
~~ if hpoterms is True : 
~~~ hpo_urls [ 'hpoterms' ] = hpoterms_url 
~~ if phenotype_to_terms is True : 
~~~ hpo_urls [ 'phenotype_to_terms' ] = hpo_phenotype_to_terms_url 
~~ if hpodisease is True : 
~~~ hpo_urls [ 'hpodisease' ] = hpodisease_url 
~~ for file_name in hpo_urls : 
~~~ url = hpo_urls [ file_name ] 
hpo_files [ file_name ] = request_file ( url ) 
~~ return hpo_files 
~~ def transcripts ( context , build , hgnc_id , json ) : 
if not json : 
~~~ click . echo ( "Chromosome\\tstart\\tend\\ttranscript_id\\thgnc_id\\trefseq\\tis_primary" ) 
~~ for tx_obj in adapter . transcripts ( build = build , hgnc_id = hgnc_id ) : 
~~~ if json : 
~~~ pp ( tx_obj ) 
~~ click . echo ( "{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}" . format ( 
tx_obj [ 'chrom' ] , 
tx_obj [ 'start' ] , 
tx_obj [ 'end' ] , 
tx_obj [ 'ensembl_transcript_id' ] , 
tx_obj [ 'hgnc_id' ] , 
tx_obj . get ( 'refseq_id' , '' ) , 
tx_obj . get ( 'is_primary' ) or '' , 
~~ ~~ def variants ( store , institute_obj , case_obj , variants_query , page = 1 , per_page = 50 ) : 
variant_count = variants_query . count ( ) 
skip_count = per_page * max ( page - 1 , 0 ) 
more_variants = True if variant_count > ( skip_count + per_page ) else False 
variant_res = variants_query . skip ( skip_count ) . limit ( per_page ) 
genome_build = case_obj . get ( 'genome_build' , '37' ) 
if genome_build not in [ '37' , '38' ] : 
~~~ genome_build = '37' 
~~ variants = [ ] 
for variant_obj in variant_res : 
~~~ overlapping_svs = [ sv for sv in store . overlapping ( variant_obj ) ] 
variant_obj [ 'overlapping' ] = overlapping_svs or None 
variants . append ( parse_variant ( store , institute_obj , case_obj , variant_obj , 
update = True , genome_build = genome_build ) ) 
'variants' : variants , 
'more_variants' : more_variants , 
~~ def sv_variants ( store , institute_obj , case_obj , variants_query , page = 1 , per_page = 50 ) : 
skip_count = ( per_page * max ( page - 1 , 0 ) ) 
more_variants = True if variants_query . count ( ) > ( skip_count + per_page ) else False 
'variants' : ( parse_variant ( store , institute_obj , case_obj , variant , genome_build = genome_build ) for variant in 
variants_query . skip ( skip_count ) . limit ( per_page ) ) , 
~~ def str_variants ( store , institute_obj , case_obj , variants_query , page = 1 , per_page = 50 ) : 
return variants ( store , institute_obj , case_obj , variants_query , page , per_page ) 
~~ def str_variant ( store , institute_id , case_name , variant_id ) : 
variant_case ( store , case_obj , variant_obj ) 
variant_obj [ 'callers' ] = callers ( variant_obj , category = 'str' ) 
variant_obj [ 'comments' ] = store . events ( institute_obj , case = case_obj , 
variant_id = variant_obj [ 'variant_id' ] , comments = True ) 
'institute' : institute_obj , 
'case' : case_obj , 
'variant' : variant_obj , 
'overlapping_snvs' : overlapping_snvs , 
'manual_rank_options' : MANUAL_RANK_OPTIONS , 
'dismiss_variant_options' : DISMISS_VARIANT_OPTIONS 
~~ def sv_variant ( store , institute_id , case_name , variant_id = None , variant_obj = None , add_case = True , 
get_overlapping = True ) : 
if not variant_obj : 
~~~ variant_obj = store . variant ( variant_id ) 
~~ if add_case : 
~~~ variant_case ( store , case_obj , variant_obj ) 
~~ variant_obj [ 'frequencies' ] = [ 
( '1000G' , variant_obj . get ( 'thousand_genomes_frequency' ) ) , 
( 'SweGen' , variant_obj . get ( 'swegen' ) ) , 
( 'Decipher' , variant_obj . get ( 'decipher' ) ) , 
variant_obj [ 'callers' ] = callers ( variant_obj , category = 'sv' ) 
overlapping_snvs = [ ] 
if get_overlapping : 
~~~ overlapping_snvs = ( parse_variant ( store , institute_obj , case_obj , variant ) for variant in 
store . overlapping ( variant_obj ) ) 
~~ for gene_obj in variant_obj [ 'genes' ] : 
~~~ if gene_obj . get ( 'common' ) : 
~~~ ensembl_id = gene_obj [ 'common' ] [ 'ensembl_id' ] 
~~~ build = int ( gene_obj [ 'common' ] . get ( 'build' , '37' ) ) 
~~~ build = 37 
~~ gene_obj [ 'ensembl_link' ] = ensembl ( ensembl_id , build = build ) 
~~ ~~ variant_obj [ 'comments' ] = store . events ( institute_obj , case = case_obj , 
case_clinvars = store . case_to_clinVars ( case_obj . get ( 'display_name' ) ) 
if variant_id in case_clinvars : 
~~~ variant_obj [ 'clinvar_clinsig' ] = case_clinvars . get ( variant_id ) [ 'clinsig' ] 
~~ if not 'end_chrom' in variant_obj : 
~~~ variant_obj [ 'end_chrom' ] = variant_obj [ 'chromosome' ] 
~~ def parse_variant ( store , institute_obj , case_obj , variant_obj , update = False , genome_build = '37' , 
get_compounds = True ) : 
has_changed = False 
compounds = variant_obj . get ( 'compounds' , [ ] ) 
if compounds and get_compounds : 
~~~ if 'not_loaded' not in compounds [ 0 ] : 
~~~ new_compounds = store . update_variant_compounds ( variant_obj ) 
variant_obj [ 'compounds' ] = new_compounds 
has_changed = True 
~~ variant_obj [ 'compounds' ] = sorted ( variant_obj [ 'compounds' ] , 
key = lambda compound : - compound [ 'combined_score' ] ) 
~~ variant_genes = variant_obj . get ( 'genes' ) 
if variant_genes is not None : 
~~~ for gene_obj in variant_genes : 
~~~ if not gene_obj [ 'hgnc_id' ] : 
~~ if gene_obj . get ( 'hgnc_symbol' ) is None : 
~~~ hgnc_gene = store . hgnc_gene ( gene_obj [ 'hgnc_id' ] , build = genome_build ) 
if not hgnc_gene : 
~~ has_changed = True 
gene_obj [ 'hgnc_symbol' ] = hgnc_gene [ 'hgnc_symbol' ] 
~~ ~~ ~~ if update and has_changed : 
~~~ variant_obj = store . update_variant ( variant_obj ) 
~~ variant_obj [ 'comments' ] = store . events ( institute_obj , case = case_obj , 
if variant_genes : 
~~~ variant_obj . update ( get_predictions ( variant_genes ) ) 
if variant_obj . get ( 'category' ) == 'cancer' : 
~~~ variant_obj . update ( get_variant_info ( variant_genes ) ) 
~~ ~~ for compound_obj in compounds : 
~~~ compound_obj . update ( get_predictions ( compound_obj . get ( 'genes' , [ ] ) ) ) 
~~ if isinstance ( variant_obj . get ( 'acmg_classification' ) , int ) : 
~~~ acmg_code = ACMG_MAP [ variant_obj [ 'acmg_classification' ] ] 
variant_obj [ 'acmg_classification' ] = ACMG_COMPLETE_MAP [ acmg_code ] 
~~ variant_length = variant_obj . get ( 'length' ) 
variant_obj [ 'length' ] = { 100000000000 : 'inf' , - 1 : 'n.d.' } . get ( variant_length , variant_length ) 
if not 'end_chrom' in variant_obj : 
~~ return variant_obj 
~~ def variant_export_lines ( store , case_obj , variants_query ) : 
export_variants = [ ] 
for variant in variants_query : 
~~~ variant_line = [ ] 
position = variant [ 'position' ] 
change = variant [ 'reference' ] + '>' + variant [ 'alternative' ] 
variant_line . append ( variant [ 'rank_score' ] ) 
variant_line . append ( variant [ 'chromosome' ] ) 
variant_line . append ( position ) 
variant_line . append ( change ) 
variant_line . append ( '_' . join ( [ str ( position ) , change ] ) ) 
gene_ids = [ ] 
gene_names = [ ] 
hgvs_c = [ ] 
if len ( gene_list ) > 0 : 
~~~ for gene_obj in gene_list : 
~~~ hgnc_id = gene_obj [ 'hgnc_id' ] 
gene_name = gene ( store , hgnc_id ) [ 'symbol' ] 
gene_ids . append ( hgnc_id ) 
gene_names . append ( gene_name ) 
hgvs_nucleotide = '-' 
transcripts_list = gene_obj . get ( 'transcripts' ) 
for transcript_obj in transcripts_list : 
~~~ if transcript_obj . get ( 'is_canonical' ) and transcript_obj . get ( 'is_canonical' ) is True : 
~~~ hgvs_nucleotide = str ( transcript_obj . get ( 'coding_sequence_name' ) ) 
~~ ~~ hgvs_c . append ( hgvs_nucleotide ) 
~~ variant_line . append ( ';' . join ( str ( x ) for x in gene_ids ) ) 
variant_line . append ( ';' . join ( str ( x ) for x in gene_names ) ) 
variant_line . append ( ';' . join ( str ( x ) for x in hgvs_c ) ) 
~~~ while i < 4 : 
for individual in case_obj [ 'individuals' ] : 
~~~ for variant_gt in variant_gts : 
~~~ if individual [ 'individual_id' ] == variant_gt [ 'sample_id' ] : 
variant_line . append ( variant_gt [ 'genotype_quality' ] ) 
~~ ~~ ~~ variant_line = [ str ( i ) for i in variant_line ] 
export_variants . append ( "," . join ( variant_line ) ) 
~~ return export_variants 
~~ def variants_export_header ( case_obj ) : 
header = header + EXPORT_HEADER 
~~~ display_name = str ( individual [ 'display_name' ] ) 
~~ return header 
~~ def get_variant_info ( genes ) : 
data = { 'canonical_transcripts' : [ ] } 
for gene_obj in genes : 
~~~ if not gene_obj . get ( 'canonical_transcripts' ) : 
~~~ tx = gene_obj [ 'transcripts' ] [ 0 ] 
tx_id = tx [ 'transcript_id' ] 
exon = tx . get ( 'exon' , '-' ) 
c_seq = tx . get ( 'coding_sequence_name' , '-' ) 
~~~ tx_id = gene_obj [ 'canonical_transcripts' ] 
exon = gene_obj . get ( 'exon' , '-' ) 
c_seq = gene_obj . get ( 'hgvs_identifier' , '-' ) 
~~ if len ( c_seq ) > 20 : 
~~~ c_seq = c_seq [ : 20 ] + '...' 
~~ if len ( genes ) == 1 : 
~~~ value = ':' . join ( [ tx_id , exon , c_seq ] ) 
~~~ gene_id = gene_obj . get ( 'hgnc_symbol' ) or str ( gene_obj [ 'hgnc_id' ] ) 
value = ':' . join ( [ gene_id , tx_id , exon , c_seq ] ) 
~~ data [ 'canonical_transcripts' ] . append ( value ) 
~~ def get_predictions ( genes ) : 
'sift_predictions' : [ ] , 
'polyphen_predictions' : [ ] , 
'region_annotations' : [ ] , 
'functional_annotations' : [ ] 
~~~ for pred_key in data : 
~~~ gene_key = pred_key [ : - 1 ] 
if len ( genes ) == 1 : 
~~~ value = gene_obj . get ( gene_key , '-' ) 
value = ':' . join ( [ gene_id , gene_obj . get ( gene_key , '-' ) ] ) 
~~ data [ pred_key ] . append ( value ) 
~~ def variant_case ( store , case_obj , variant_obj ) : 
case_obj [ 'bam_files' ] = [ ] 
case_obj [ 'mt_bams' ] = [ ] 
case_obj [ 'bai_files' ] = [ ] 
case_obj [ 'mt_bais' ] = [ ] 
case_obj [ 'sample_names' ] = [ ] 
~~~ bam_path = individual . get ( 'bam_file' ) 
mt_bam = individual . get ( 'mt_bam' ) 
case_obj [ 'sample_names' ] . append ( individual . get ( 'display_name' ) ) 
if bam_path and os . path . exists ( bam_path ) : 
~~~ case_obj [ 'bam_files' ] . append ( individual [ 'bam_file' ] ) 
case_obj [ 'bai_files' ] . append ( find_bai_file ( individual [ 'bam_file' ] ) ) 
~~ if mt_bam and os . path . exists ( mt_bam ) : 
~~~ case_obj [ 'mt_bams' ] . append ( individual [ 'mt_bam' ] ) 
case_obj [ 'mt_bais' ] . append ( find_bai_file ( individual [ 'mt_bam' ] ) ) 
~~~ genes = variant_obj . get ( 'genes' , [ ] ) 
~~~ hgnc_gene_obj = store . hgnc_gene ( variant_obj [ 'genes' ] [ 0 ] [ 'hgnc_id' ] ) 
if hgnc_gene_obj : 
~~~ vcf_path = store . get_region_vcf ( case_obj , gene_obj = hgnc_gene_obj ) 
case_obj [ 'region_vcf_file' ] = vcf_path 
~~~ case_obj [ 'region_vcf_file' ] = None 
~~ ~~ elif len ( genes ) > 1 : 
~~~ chrom = variant_obj [ 'genes' ] [ 0 ] [ 'common' ] [ 'chromosome' ] 
start = min ( gene [ 'common' ] [ 'start' ] for gene in variant_obj [ 'genes' ] ) 
end = max ( gene [ 'common' ] [ 'end' ] for gene in variant_obj [ 'genes' ] ) 
vcf_path = store . get_region_vcf ( case_obj , chrom = chrom , start = start , end = end ) 
~~ ~~ except ( SyntaxError , Exception ) : 
~~ ~~ def find_bai_file ( bam_file ) : 
bai_file = bam_file . replace ( '.bam' , '.bai' ) 
if not os . path . exists ( bai_file ) : 
~~~ bai_file = "{}.bai" . format ( bam_file ) 
~~ return bai_file 
~~ def variant ( store , institute_obj , case_obj , variant_id = None , variant_obj = None , add_case = True , 
add_other = True , get_overlapping = True ) : 
~~~ default_panels = [ ] 
~~~ if not panel . get ( 'is_default' ) : 
~~ panel_obj = store . gene_panel ( panel [ 'panel_name' ] , panel . get ( 'version' ) ) 
panel [ 'panel_name' ] , panel . get ( 'version' ) ) ) 
~~ default_panels . append ( panel_obj ) 
~~ variant_obj = store . variant ( variant_id , gene_panels = default_panels ) 
~~ genome_build = case_obj . get ( 'genome_build' , '37' ) 
~~ if variant_obj is None : 
~~ events = list ( store . events ( institute_obj , case = case_obj , variant_id = variant_obj [ 'variant_id' ] ) ) 
~~~ event [ 'verb' ] = VERBS_MAP [ event [ 'verb' ] ] 
~~ other_causatives = [ ] 
if add_other : 
~~~ for other_variant in store . other_causatives ( case_obj , variant_obj ) : 
~~~ case_id = other_variant [ 'case_id' ] 
other_case = store . case ( case_id ) 
if not other_case : 
~~ other_variant [ 'case_display_name' ] = other_case . get ( 'display_name' , case_id ) 
other_causatives . append ( other_variant ) 
~~ ~~ variant_obj = parse_variant ( store , institute_obj , case_obj , variant_obj , genome_build = genome_build ) 
variant_obj [ 'end_position' ] = end_position ( variant_obj ) 
variant_obj [ 'frequency' ] = frequency ( variant_obj ) 
variant_obj [ 'clinsig_human' ] = ( clinsig_human ( variant_obj ) if variant_obj . get ( 'clnsig' ) 
else None ) 
variant_obj [ 'thousandg_link' ] = thousandg_link ( variant_obj , genome_build ) 
variant_obj [ 'exac_link' ] = exac_link ( variant_obj ) 
variant_obj [ 'gnomad_link' ] = gnomad_link ( variant_obj ) 
variant_obj [ 'swegen_link' ] = swegen_link ( variant_obj ) 
variant_obj [ 'cosmic_link' ] = cosmic_link ( variant_obj ) 
variant_obj [ 'beacon_link' ] = beacon_link ( variant_obj , genome_build ) 
variant_obj [ 'ucsc_link' ] = ucsc_link ( variant_obj , genome_build ) 
variant_obj [ 'alamut_link' ] = alamut_link ( variant_obj ) 
variant_obj [ 'spidex_human' ] = spidex_human ( variant_obj ) 
variant_obj [ 'expected_inheritance' ] = expected_inheritance ( variant_obj ) 
variant_obj [ 'callers' ] = callers ( variant_obj , category = 'snv' ) 
individuals = { individual [ 'individual_id' ] : individual for individual in 
case_obj [ 'individuals' ] } 
for sample_obj in variant_obj [ 'samples' ] : 
~~~ individual = individuals [ sample_obj . get ( 'sample_id' ) ] 
if not individual : 
~~ sample_obj [ 'is_affected' ] = True if individual [ 'phenotype' ] == 2 else False 
~~ gene_models = set ( ) 
variant_obj [ 'disease_associated_transcripts' ] = [ ] 
for gene_obj in variant_obj . get ( 'genes' , [ ] ) : 
~~~ parse_gene ( gene_obj , genome_build ) 
omim_models = set ( ) 
for disease_term in gene_obj . get ( 'disease_terms' , [ ] ) : 
~~~ omim_models . update ( disease_term . get ( 'inheritance' , [ ] ) ) 
~~ gene_obj [ 'omim_inheritance' ] = list ( omim_models ) 
for refseq_id in gene_obj . get ( 'disease_associated_transcripts' , [ ] ) : 
~~~ hgnc_symbol = ( gene_obj [ 'common' ] [ 'hgnc_symbol' ] if gene_obj . get ( 'common' ) else 
gene_obj [ 'hgnc_id' ] ) 
transcript_str = "{}:{}" . format ( hgnc_symbol , refseq_id ) 
variant_obj [ 'disease_associated_transcripts' ] . append ( transcript_str ) 
~~ gene_models = gene_models | omim_models 
~~ if variant_obj . get ( 'genetic_models' ) : 
~~~ variant_models = set ( model . split ( '_' , 1 ) [ 0 ] for model in variant_obj [ 'genetic_models' ] ) 
variant_obj [ 'is_matching_inheritance' ] = variant_models & gene_models 
~~ evaluations = [ ] 
for evaluation_obj in store . get_evaluations ( variant_obj ) : 
~~~ evaluation ( store , evaluation_obj ) 
evaluations . append ( evaluation_obj ) 
~~ case_clinvars = store . case_to_clinVars ( case_obj . get ( 'display_name' ) ) 
~~ svs = [ ] 
~~~ svs = ( parse_variant ( store , institute_obj , case_obj , variant_obj ) for 
variant_obj in store . overlapping ( variant_obj ) ) 
'causatives' : other_causatives , 
'events' : events , 
'overlapping_svs' : svs , 
'dismiss_variant_options' : DISMISS_VARIANT_OPTIONS , 
'mosaic_variant_options' : MOSAICISM_OPTIONS , 
'ACMG_OPTIONS' : ACMG_OPTIONS , 
'evaluations' : evaluations , 
~~ def observations ( store , loqusdb , case_obj , variant_obj ) : 
composite_id = ( "{this[chromosome]}_{this[position]}_{this[reference]}_" 
"{this[alternative]}" . format ( this = variant_obj ) ) 
obs_data = loqusdb . get_variant ( { '_id' : composite_id } ) or { } 
obs_data [ 'total' ] = loqusdb . case_count ( ) 
obs_data [ 'cases' ] = [ ] 
institute_id = variant_obj [ 'institute' ] 
for case_id in obs_data . get ( 'families' , [ ] ) : 
~~~ if case_id != variant_obj [ 'case_id' ] and case_id . startswith ( institute_id ) : 
~~~ other_variant = store . variant ( variant_obj [ 'variant_id' ] , case_id = case_id ) 
obs_data [ 'cases' ] . append ( dict ( case = other_case , variant = other_variant ) ) 
~~ ~~ return obs_data 
~~ def parse_gene ( gene_obj , build = None ) : 
build = build or 37 
if gene_obj . get ( 'common' ) : 
~~~ add_gene_links ( gene_obj , build ) 
refseq_transcripts = [ ] 
for tx_obj in gene_obj [ 'transcripts' ] : 
~~~ parse_transcript ( gene_obj , tx_obj , build ) 
if not tx_obj . get ( 'refseq_id' ) : 
~~ refseq_transcripts . append ( tx_obj ) 
~~ gene_obj [ 'primary_transcripts' ] = ( refseq_transcripts if refseq_transcripts else [ ] ) 
~~ ~~ def parse_transcript ( gene_obj , tx_obj , build = None ) : 
add_tx_links ( tx_obj , build ) 
if tx_obj . get ( 'refseq_id' ) : 
~~~ gene_name = ( gene_obj [ 'common' ] [ 'hgnc_symbol' ] if gene_obj [ 'common' ] else 
tx_obj [ 'change_str' ] = transcript_str ( tx_obj , gene_name ) 
~~ ~~ def transcript_str ( transcript_obj , gene_name = None ) : 
if transcript_obj . get ( 'exon' ) : 
~~~ gene_part , part_count_raw = 'exon' , transcript_obj [ 'exon' ] 
~~ elif transcript_obj . get ( 'intron' ) : 
~~~ gene_part , part_count_raw = 'intron' , transcript_obj [ 'intron' ] 
~~~ gene_part , part_count_raw = 'intergenic' , '0' 
~~ part_count = part_count_raw . rpartition ( '/' ) [ 0 ] 
change_str = "{}:{}{}:{}:{}" . format ( 
transcript_obj . get ( 'refseq_id' , '' ) , 
gene_part , 
part_count , 
transcript_obj . get ( 'coding_sequence_name' , 'NA' ) , 
transcript_obj . get ( 'protein_sequence_name' , 'NA' ) , 
if gene_name : 
~~~ change_str = "{}:" . format ( gene_name ) + change_str 
~~ return change_str 
~~ def end_position ( variant_obj ) : 
alt_bases = len ( variant_obj [ 'alternative' ] ) 
num_bases = max ( len ( variant_obj [ 'reference' ] ) , alt_bases ) 
return variant_obj [ 'position' ] + ( num_bases - 1 ) 
~~ def frequency ( variant_obj ) : 
most_common_frequency = max ( variant_obj . get ( 'thousand_genomes_frequency' ) or 0 , 
variant_obj . get ( 'exac_frequency' ) or 0 ) 
if most_common_frequency > .05 : 
~~~ return 'common' 
~~ elif most_common_frequency > .01 : 
~~~ return 'uncommon' 
~~~ return 'rare' 
~~ ~~ def clinsig_human ( variant_obj ) : 
for clinsig_obj in variant_obj [ 'clnsig' ] : 
~~~ if isinstance ( clinsig_obj [ 'accession' ] , int ) : 
~~~ link = "https://www.ncbi.nlm.nih.gov/clinvar/variation/{}" 
~~~ link = "https://www.ncbi.nlm.nih.gov/clinvar/{}" 
if clinsig_obj . get ( 'value' ) : 
~~~ int ( clinsig_obj [ 'value' ] ) 
~~~ human_str = clinsig_obj [ 'value' ] 
~~ ~~ clinsig_obj [ 'human' ] = human_str 
clinsig_obj [ 'link' ] = link . format ( clinsig_obj [ 'accession' ] ) 
yield clinsig_obj 
~~ ~~ def thousandg_link ( variant_obj , build = None ) : 
dbsnp_id = variant_obj . get ( 'dbsnp_id' ) 
if not dbsnp_id : 
~~ if build == 37 : 
~~~ url_template = ( "http://grch37.ensembl.org/Homo_sapiens/Variation/Explore" 
"?v={};vdb=variation" ) 
~~~ url_template = ( "http://www.ensembl.org/Homo_sapiens/Variation/Explore" 
~~ return url_template . format ( dbsnp_id ) 
~~ def cosmic_link ( variant_obj ) : 
cosmic_ids = variant_obj . get ( 'cosmic_ids' ) 
if not cosmic_ids : 
~~~ cosmic_id = cosmic_ids [ 0 ] 
url_template = ( "https://cancer.sanger.ac.uk/cosmic/mutation/overview?id={}" ) 
~~ return url_template . format ( cosmic_id ) 
~~ def beacon_link ( variant_obj , build = None ) : 
url_template = ( "https://beacon-network.org/#/search?pos={this[position]}&" 
"chrom={this[chromosome]}&allele={this[alternative]}&" 
"ref={this[reference]}&rs=GRCh37" ) 
return url_template . format ( this = variant_obj ) 
~~ def ucsc_link ( variant_obj , build = None ) : 
url_template = ( "http://genome.ucsc.edu/cgi-bin/hgTracks?db=hg19&" 
"position=chr{this[chromosome]}:{this[position]}" 
"-{this[position]}&dgv=pack&knownGene=pack&omimGene=pack" ) 
if build == 38 : 
~~~ url_template = ( "http://genome.ucsc.edu/cgi-bin/hgTracks?db=hg20&" 
~~ return url_template . format ( this = variant_obj ) 
~~ def spidex_human ( variant_obj ) : 
if variant_obj . get ( 'spidex' ) is None : 
~~~ return 'not_reported' 
~~ elif abs ( variant_obj [ 'spidex' ] ) < SPIDEX_HUMAN [ 'low' ] [ 'pos' ] [ 1 ] : 
~~~ return 'low' 
~~ elif abs ( variant_obj [ 'spidex' ] ) < SPIDEX_HUMAN [ 'medium' ] [ 'pos' ] [ 1 ] : 
~~~ return 'medium' 
~~~ return 'high' 
~~ ~~ def expected_inheritance ( variant_obj ) : 
manual_models = set ( ) 
for gene in variant_obj . get ( 'genes' , [ ] ) : 
~~~ manual_models . update ( gene . get ( 'manual_inheritance' , [ ] ) ) 
~~ return list ( manual_models ) 
~~ def callers ( variant_obj , category = 'snv' ) : 
calls = set ( ) 
for caller in CALLERS [ category ] : 
~~~ if variant_obj . get ( caller [ 'id' ] ) : 
~~~ calls . add ( ( caller [ 'name' ] , variant_obj [ caller [ 'id' ] ] ) ) 
~~ ~~ return list ( calls ) 
~~ def variant_verification ( store , mail , institute_obj , case_obj , user_obj , variant_obj , sender , variant_url , order , comment , url_builder = url_for ) : 
recipients = institute_obj [ 'sanger_recipients' ] 
if len ( recipients ) == 0 : 
~~~ raise MissingSangerRecipientError ( ) 
~~ view_type = None 
email_subject = None 
category = variant_obj . get ( 'category' ) 
display_name = None 
chromosome = variant_obj [ 'chromosome' ] 
end_chrom = variant_obj . get ( 'end_chrom' , chromosome ) 
breakpoint_1 = ':' . join ( [ chromosome , str ( variant_obj [ 'position' ] ) ] ) 
breakpoint_2 = ':' . join ( [ end_chrom , str ( variant_obj . get ( 'end' ) ) ] ) 
variant_size = variant_obj . get ( 'length' ) 
email_subj_gene_symbol = None 
if len ( variant_obj [ 'hgnc_symbols' ] ) > 3 : 
~~~ email_subj_gene_symbol = hgnc_symbol 
sample_obj [ 'genotype_call' ] ) 
for sample_obj in variant_obj [ 'samples' ] ] 
tx_changes = [ ] 
if category == 'snv' : #SNV 
~~~ view_type = 'variants.variant' 
display_name = variant_obj . get ( 'display_name' ) 
~~~ for tx_obj in gene_obj [ 'transcripts' ] : 
~~~ parse_transcript ( gene_obj , tx_obj ) 
~~ for refseq_id in tx_obj . get ( 'refseq_identifiers' ) : 
~~~ transcript_line = [ ] 
if "common" in gene_obj : 
~~~ transcript_line . append ( gene_obj [ 'common' ] [ 'hgnc_symbol' ] ) 
~~~ transcript_line . append ( gene_obj [ 'hgnc_id' ] ) 
~~ transcript_line . append ( '-' . join ( [ refseq_id , tx_obj [ 'transcript_id' ] ] ) ) 
if "exon" in tx_obj : 
~~~ transcript_line . append ( '' . join ( [ "exon" , tx_obj [ "exon" ] ] ) ) 
~~ elif "intron" in tx_obj : 
~~~ transcript_line . append ( '' . join ( [ "intron" , tx_obj [ "intron" ] ] ) ) 
~~~ transcript_line . append ( 'intergenic' ) 
~~ if "coding_sequence_name" in tx_obj : 
~~~ transcript_line . append ( urllib . parse . unquote ( tx_obj [ 'coding_sequence_name' ] ) ) 
~~~ transcript_line . append ( '' ) 
~~ if "protein_sequence_name" in tx_obj : 
~~~ transcript_line . append ( urllib . parse . unquote ( tx_obj [ 'protein_sequence_name' ] ) ) 
~~ tx_changes . append ( "<li>{}</li>" . format ( ':' . join ( transcript_line ) ) ) 
~~ ~~ ~~ ~~ else : #SV 
~~~ view_type = 'variants.sv_variant' 
display_name = '_' . join ( [ breakpoint_1 , variant_obj . get ( 'sub_category' ) . upper ( ) ] ) 
~~ html = verification_email_body ( 
case_name = case_obj [ 'display_name' ] , 
category = category . upper ( ) , 
subcategory = variant_obj . get ( 'sub_category' ) . upper ( ) , 
breakpoint_1 = breakpoint_1 , 
breakpoint_2 = breakpoint_2 , 
hgnc_symbol = hgnc_symbol , 
panels = panels , 
gtcalls = '' . join ( gtcalls ) , 
name = user_obj [ 'name' ] . encode ( 'utf-8' ) , 
comment = comment 
local_link = url_builder ( view_type , institute_id = institute_obj [ '_id' ] , 
variant_id = variant_obj [ '_id' ] ) 
~~~ if case_obj . get ( 'suspects' ) is None or variant_obj [ '_id' ] not in case_obj [ 'suspects' ] : 
~~~ store . pin_variant ( institute_obj , case_obj , user_obj , local_link , variant_obj ) 
store . order_verification ( institute = institute_obj , case = case_obj , user = user_obj , link = local_link , variant = variant_obj ) 
store . cancel_verification ( institute = institute_obj , case = case_obj , user = user_obj , link = local_link , variant = variant_obj ) 
~~ kwargs = dict ( subject = email_subject , html = html , sender = sender , recipients = recipients , 
cc = [ user_obj [ 'email' ] ] ) 
message = Message ( ** kwargs ) 
mail . send ( message ) 
~~ def verification_email_body ( case_name , url , display_name , category , subcategory , breakpoint_1 , breakpoint_2 , hgnc_symbol , panels , gtcalls , tx_changes , name , comment ) : 
case_name = case_name , 
category = category , 
subcategory = subcategory , 
gtcalls = gtcalls , 
tx_changes = tx_changes , 
comment = comment ) 
return html 
~~ def cancer_variants ( store , request_args , institute_id , case_name ) : 
form = CancerFiltersForm ( request_args ) 
variants_query = store . variants ( case_obj [ '_id' ] , category = 'cancer' , query = form . data ) . limit ( 50 ) 
data = dict ( 
institute = institute_obj , 
case = case_obj , 
variants = ( parse_variant ( store , institute_obj , case_obj , variant , update = True ) for 
variant in variants_query ) , 
form = form , 
variant_type = request_args . get ( 'variant_type' , 'clinical' ) , 
~~ def clinvar_export ( store , institute_id , case_name , variant_id ) : 
pinned = [ store . variant ( variant_id ) or variant_id for variant_id in 
case_obj . get ( 'suspects' , [ ] ) ] 
today = str ( date . today ( ) ) , 
variant = variant_obj , 
pinned_vars = pinned 
~~ def get_clinvar_submission ( store , institute_id , case_name , variant_id , submission_id ) : 
clinvar_submission_objs = store . clinvars ( submission_id = submission_id ) 
pinned_vars = pinned , 
clinvars = clinvar_submission_objs 
~~ def variant_acmg ( store , institute_id , case_name , variant_id ) : 
return dict ( institute = institute_obj , case = case_obj , variant = variant_obj , 
CRITERIA = ACMG_CRITERIA , ACMG_OPTIONS = ACMG_OPTIONS ) 
~~ def variant_acmg_post ( store , institute_id , case_name , variant_id , user_email , criteria ) : 
user_obj = store . user ( user_email ) 
variant_link = url_for ( 'variants.variant' , institute_id = institute_id , 
case_name = case_name , variant_id = variant_id ) 
classification = store . submit_evaluation ( 
institute_obj = institute_obj , 
case_obj = case_obj , 
variant_obj = variant_obj , 
user_obj = user_obj , 
link = variant_link , 
criteria = criteria , 
return classification 
~~ def evaluation ( store , evaluation_obj ) : 
evaluation_obj [ 'institute' ] = store . institute ( evaluation_obj [ 'institute_id' ] ) 
evaluation_obj [ 'case' ] = store . case ( evaluation_obj [ 'case_id' ] ) 
evaluation_obj [ 'variant' ] = store . variant ( evaluation_obj [ 'variant_specific' ] ) 
evaluation_obj [ 'criteria' ] = { criterion [ 'term' ] : criterion for criterion in 
evaluation_obj [ 'criteria' ] } 
evaluation_obj [ 'classification' ] = ACMG_COMPLETE_MAP [ evaluation_obj [ 'classification' ] ] 
return evaluation_obj 
~~ def upload_panel ( store , institute_id , case_name , stream ) : 
raw_symbols = [ line . strip ( ) . split ( '\\t' ) [ 0 ] for line in stream if 
line and not line . startswith ( '#' ) ] 
hgnc_symbols = [ ] 
for raw_symbol in raw_symbols : 
~~~ if store . hgnc_genes ( raw_symbol ) . count ( ) == 0 : 
~~~ hgnc_symbols . append ( raw_symbol ) 
~~ ~~ return hgnc_symbols 
~~ 