utf-8 def parse_info_table ( table ) : 
ret = { } 
for tr in list ( table ( 'tr' ) . not_ ( '.thead' ) . items ( ) ) : 
key = th . text ( ) . lower ( ) 
key = re . sub ( r'\\W' , '_' , key ) 
val = sportsref . utils . flatten_links ( td ) 
ret [ key ] = val 
~~ return ret 

~~ def flatten_links ( td , _recurse = False ) : 
def _flatten_node ( c ) : 
~~~ if isinstance ( c , basestring ) : 
~~~ return c . strip ( ) 
~~ elif 'href' in c . attrib : 
~~~ c_id = rel_url_to_id ( c . attrib [ 'href' ] ) 
return c_id if c_id else c . text_content ( ) . strip ( ) 
~~ else : 
~~~ return flatten_links ( pq ( c ) , _recurse = True ) 
~~ ~~ if td is None or not td . text ( ) : 
~~~ return '' if _recurse else None 
~~ td . remove ( 'span.note' ) 
return '' . join ( _flatten_node ( c ) for c in td . contents ( ) ) 
~~ def rel_url_to_id ( url ) : 
yearRegex = r'.*/years/(\\d{4}).*|.*/gamelog/(\\d{4}).*' 
playerRegex = r'.*/players/(?:\\w/)?(.+?)(?:/|\\.html?)' 
boxscoresRegex = r'.*/boxscores/(.+?)\\.html?' 
teamRegex = r'.*/teams/(\\w{3})/.*' 
coachRegex = r'.*/coaches/(.+?)\\.html?' 
stadiumRegex = r'.*/stadiums/(.+?)\\.html?' 
refRegex = r'.*/officials/(.+?r)\\.html?' 
collegeRegex = r'.*/schools/(\\S+?)/.*|.*college=([^&]+)' 
hsRegex = r'.*/schools/high_schools\\.cgi\\?id=([^\\&]{8})' 
bsDateRegex = r'.*/boxscores/index\\.f?cgi\\?(month=\\d+&day=\\d+&year=\\d+)' 
leagueRegex = r'.*/leagues/(.*_\\d{4}).*' 
awardRegex = r'.*/awards/(.+)\\.htm' 
regexes = [ 
yearRegex , 
playerRegex , 
boxscoresRegex , 
teamRegex , 
coachRegex , 
stadiumRegex , 
refRegex , 
collegeRegex , 
hsRegex , 
bsDateRegex , 
leagueRegex , 
awardRegex , 
] 
for regex in regexes : 
~~~ match = re . match ( regex , url , re . I ) 
if match : 
~~~ return [ _f for _f in match . groups ( ) if _f ] [ 0 ] 
~~ ~~ if any ( 
url . startswith ( s ) for s in 
( 
'/play-index/' , 
) 
) : 
~~~ return url 
~~ print ( \ . format ( url ) ) 
return url 
~~ def PlayerSeasonFinder ( ** kwargs ) : 
if 'offset' not in kwargs : 
~~~ kwargs [ 'offset' ] = 0 
~~ playerSeasons = [ ] 
while True : 
~~~ querystring = _kwargs_to_qs ( ** kwargs ) 
url = '{}?{}' . format ( PSF_URL , querystring ) 
if kwargs . get ( 'verbose' , False ) : 
~~~ print ( url ) 
~~ html = utils . get_html ( url ) 
doc = pq ( html ) 
table = doc ( 'table#results' ) 
df = utils . parse_table ( table ) 
if df . empty : 
~~~ break 
~~ thisSeason = list ( zip ( df . player_id , df . year ) ) 
playerSeasons . extend ( thisSeason ) 
if doc ( \ ) : 
~~~ kwargs [ 'offset' ] += 100 
~~ ~~ return playerSeasons 
~~ def _kwargs_to_qs ( ** kwargs ) : 
inpOptDef = inputs_options_defaults ( ) 
opts = { 
name : dct [ 'value' ] 
for name , dct in inpOptDef . items ( ) 
} 
for k , v in kwargs . items ( ) : 
~~~ del kwargs [ k ] 
if isinstance ( v , bool ) : 
~~~ kwargs [ k ] = 'Y' if v else 'N' 
~~ elif k . lower ( ) in ( 'tm' , 'team' ) : 
~~~ kwargs [ 'team_id' ] = v 
~~ elif k . lower ( ) in ( 'yr' , 'year' , 'yrs' , 'years' ) : 
~~~ if isinstance ( v , collections . Iterable ) : 
~~~ lst = list ( v ) 
kwargs [ 'year_min' ] = min ( lst ) 
kwargs [ 'year_max' ] = max ( lst ) 
~~ elif isinstance ( v , basestring ) : 
~~~ v = list ( map ( int , v . split ( ',' ) ) ) 
kwargs [ 'year_min' ] = min ( v ) 
kwargs [ 'year_max' ] = max ( v ) 
~~~ kwargs [ 'year_min' ] = v 
kwargs [ 'year_max' ] = v 
~~ ~~ elif k . lower ( ) in ( 'pos' , 'position' , 'positions' ) : 
~~~ if isinstance ( v , basestring ) : 
~~~ v = v . split ( ',' ) 
~~ elif not isinstance ( v , collections . Iterable ) : 
~~~ v = [ v ] 
~~ kwargs [ 'pos[]' ] = v 
~~ elif k . lower ( ) in ( 
'draft_pos' , 'draftpos' , 'draftposition' , 'draftpositions' , 
'draft_position' , 'draft_positions' 
~~ kwargs [ 'draft_pos[]' ] = v 
~~~ kwargs [ k ] = v 
~~ ~~ for k , v in kwargs . items ( ) : 
~~~ if k in opts or k in ( 'pos[]' , 'draft_pos[]' ) : 
~~ opts [ k ] = v 
~~ if 'draft' in k : 
~~~ opts [ 'draft' ] = [ 1 ] 
~~ ~~ opts [ 'request' ] = [ 1 ] 
opts [ 'offset' ] = [ kwargs . get ( 'offset' , 0 ) ] 
qs = '&' . join ( 
'{}={}' . format ( urllib . parse . quote_plus ( name ) , val ) 
for name , vals in sorted ( opts . items ( ) ) for val in vals 
return qs 
~~ def deobfuscate ( request , key , juice = None ) : 
try : 
~~~ url = decrypt ( str ( key ) , 
settings . UNFRIENDLY_SECRET , 
settings . UNFRIENDLY_IV , 
checksum = settings . UNFRIENDLY_ENFORCE_CHECKSUM ) 
~~ except ( CheckSumError , InvalidKeyError ) : 
~~~ return HttpResponseNotFound ( ) 
~~ try : 
~~~ url = url . decode ( 'utf-8' ) 
~~ except UnicodeDecodeError : 
~~ url_parts = urlparse ( unquote ( url ) ) 
path = url_parts . path 
query = url_parts . query 
~~~ view , args , kwargs = resolve ( path ) 
~~ except Resolver404 : 
~~ environ = request . environ . copy ( ) 
environ [ 'PATH_INFO' ] = path [ len ( environ [ 'SCRIPT_NAME' ] ) : ] 
environ [ 'QUERY_STRING' ] = query 
patched_request = request . __class__ ( environ ) 
missing_items = set ( dir ( request ) ) - set ( dir ( patched_request ) ) 
while missing_items : 
~~~ missing_item = missing_items . pop ( ) 
patched_request . __setattr__ ( missing_item , 
request . __getattribute__ ( missing_item ) ) 
~~ patched_request . META [ 'obfuscated' ] = True 
response = view ( patched_request , * args , ** kwargs ) 
if juice and not response . has_header ( 'Content-Disposition' ) : 
~~ return response 
~~ def _lazysecret ( secret , blocksize = 32 , padding = '}' ) : 
if not len ( secret ) in ( 16 , 24 , 32 ) : 
~~~ return secret + ( blocksize - len ( secret ) ) * padding 
~~ return secret 
~~ def _crc ( plaintext ) : 
if not isinstance ( plaintext , six . binary_type ) : 
~~~ plaintext = six . b ( plaintext ) 
~~ return ( zlib . crc32 ( plaintext ) % 2147483647 ) & 0xffffffff 
~~ def encrypt ( plaintext , secret , inital_vector , checksum = True , lazy = True ) : 
~~ secret = _lazysecret ( secret ) if lazy else secret 
encobj = AES . new ( secret , AES . MODE_CFB , inital_vector ) 
if checksum : 
~~~ packed = _pack_crc ( plaintext ) 
plaintext += base64 . urlsafe_b64encode ( packed ) 
~~ encoded = base64 . urlsafe_b64encode ( encobj . encrypt ( plaintext ) ) 
if isinstance ( plaintext , six . binary_type ) : 
~~~ encoded = encoded . decode ( ) 
~~ return encoded . replace ( '=' , '' ) 
~~ def decrypt ( ciphertext , secret , inital_vector , checksum = True , lazy = True ) : 
secret = _lazysecret ( secret ) if lazy else secret 
~~~ padded = ciphertext + ( '=' * ( len ( ciphertext ) % 4 ) ) 
decoded = base64 . urlsafe_b64decode ( str ( padded ) ) 
plaintext = encobj . decrypt ( decoded ) 
~~ except ( TypeError , binascii . Error ) : 
~~ if checksum : 
~~~ try : 
~~~ crc , plaintext = ( base64 . urlsafe_b64decode ( 
plaintext [ - 8 : ] ) , plaintext [ : - 8 ] ) 
~~ if not crc == _pack_crc ( plaintext ) : 
~~ ~~ return plaintext 
~~ def obfuscate ( value , juice = None ) : 
if not settings . UNFRIENDLY_ENABLE_FILTER : 
~~~ return value 
~~ kwargs = { 
'key' : encrypt ( value , 
checksum = settings . UNFRIENDLY_ENFORCE_CHECKSUM ) , 
if juice : 
~~~ kwargs [ 'juice' ] = slugify ( juice ) 
~~ return reverse ( 'unfriendly-deobfuscate' , kwargs = kwargs ) 
~~ def search_paths ( self ) : 
paths = [ self . path ] 
if os . path . basename ( self . path_without_suffix ) != 'index' : 
~~~ path = os . path . join ( self . path_without_suffix , 'index' ) 
paths . append ( path + '' . join ( self . suffix ) ) 
~~ return paths 
~~ def path_without_suffix ( self ) : 
if self . suffix : 
~~~ return self . path [ : - len ( '' . join ( self . suffix ) ) ] 
~~ return self . path 
~~ def logical_path ( self ) : 
format_extension = self . format_extension or self . compiler_format_extension 
if format_extension is None : 
~~~ return self . path 
~~ return self . path_without_suffix + format_extension 
~~ def extensions ( self ) : 
return re . findall ( r'\\.[^.]+' , os . path . basename ( self . path ) ) 
~~ def format_extension ( self ) : 
for extension in reversed ( self . extensions ) : 
~~~ compiler = self . environment . compilers . get ( extension ) 
if not compiler and self . environment . mimetypes . get ( extension ) : 
~~~ return extension 
~~ ~~ ~~ def unknown_extensions ( self ) : 
unknown_extensions = [ ] 
for extension in self . extensions : 
if compiler or self . environment . mimetypes . get ( extension ) : 
~~~ return unknown_extensions 
~~ unknown_extensions . append ( extension ) 
~~ return unknown_extensions 
~~ def compiler_extensions ( self ) : 
~~~ index = self . extensions . index ( self . format_extension ) 
~~ except ValueError : 
~~~ index = 0 
~~ extensions = self . extensions [ index : ] 
return [ e for e in extensions if self . environment . compilers . get ( e ) ] 
~~ def compilers ( self ) : 
return [ self . environment . compilers . get ( e ) for e in self . compiler_extensions ] 
~~ def processors ( self ) : 
return self . preprocessors + list ( reversed ( self . compilers ) ) + self . postprocessors 
~~ def mimetype ( self ) : 
return ( self . environment . mimetypes . get ( self . format_extension ) or 
self . compiler_mimetype or 'application/octet-stream' ) 
~~ def compiler_mimetype ( self ) : 
for compiler in reversed ( self . compilers ) : 
~~~ if compiler . result_mimetype : 
~~~ return compiler . result_mimetype 
~~ ~~ return None 
~~ def compiler_format_extension ( self ) : 
for extension , mimetype in self . environment . mimetypes . items ( ) : 
~~~ if mimetype == self . compiler_mimetype : 
~~ def register ( self , mimetype , processor ) : 
if mimetype not in self or processor not in self [ mimetype ] : 
~~~ self . setdefault ( mimetype , [ ] ) . append ( processor ) 
~~ ~~ def unregister ( self , mimetype , processor ) : 
if mimetype in self and processor in self [ mimetype ] : 
~~~ self [ mimetype ] . remove ( processor ) 
~~ ~~ def register_defaults ( self ) : 
self . register ( 'text/css' , DirectivesProcessor . as_handler ( ) ) 
self . register ( 'application/javascript' , DirectivesProcessor . as_handler ( ) ) 
~~ def suffixes ( self ) : 
if not hasattr ( self , '_suffixes' ) : 
~~~ suffixes = Suffixes ( ) 
for extension , mimetype in self . mimetypes . items ( ) : 
~~~ suffixes . register ( extension , root = True , mimetype = mimetype ) 
~~ for extension , compiler in self . compilers . items ( ) : 
~~~ suffixes . register ( extension , to = compiler . result_mimetype ) 
~~ self . _suffixes = suffixes 
~~ return self . _suffixes 
~~ def paths ( self ) : 
if not hasattr ( self , '_paths' ) : 
~~~ paths = [ ] 
for finder in self . finders : 
~~~ if hasattr ( finder , 'paths' ) : 
~~~ paths . extend ( finder . paths ) 
~~ ~~ self . _paths = paths 
~~ return self . _paths 
~~ def register_defaults ( self ) : 
self . mimetypes . register_defaults ( ) 
self . preprocessors . register_defaults ( ) 
self . postprocessors . register_defaults ( ) 
~~ def register_entry_points ( self , exclude = ( ) ) : 
for entry_point in iter_entry_points ( 'gears' , 'register' ) : 
~~~ if entry_point . module_name not in exclude : 
~~~ register = entry_point . load ( ) 
register ( self ) 
~~ ~~ ~~ def find ( self , item , logical = False ) : 
if isinstance ( item , AssetAttributes ) : 
~~~ for path in item . search_paths : 
~~~ return self . find ( path , logical ) 
~~ except FileNotFound : 
~~~ continue 
~~ ~~ raise FileNotFound ( item . path ) 
~~ if logical : 
~~~ asset_attributes = AssetAttributes ( self , item ) 
suffixes = self . suffixes . find ( asset_attributes . mimetype ) 
if not suffixes : 
~~~ return self . find ( item ) 
~~ path = asset_attributes . path_without_suffix 
for suffix in suffixes : 
~~~ return self . find ( path + suffix ) 
~~ ~~ ~~ else : 
~~~ for finder in self . finders : 
~~~ absolute_path = finder . find ( item ) 
~~ return AssetAttributes ( self , item ) , absolute_path 
~~ ~~ raise FileNotFound ( item ) 
~~ def list ( self , path , mimetype = None ) : 
basename_pattern = os . path . basename ( path ) 
if path . endswith ( '**' ) : 
~~~ paths = [ path ] 
~~~ paths = AssetAttributes ( self , path ) . search_paths 
~~ paths = map ( lambda p : p if p . endswith ( '*' ) else p + '*' , paths ) 
results = unique ( self . _list_paths ( paths ) , lambda x : x [ 0 ] ) 
for logical_path , absolute_path in results : 
~~~ asset_attributes = AssetAttributes ( self , logical_path ) 
if mimetype is not None and asset_attributes . mimetype != mimetype : 
~~ basename = os . path . basename ( asset_attributes . path_without_suffix ) 
if not fnmatch ( basename , basename_pattern ) and basename != 'index' : 
~~ yield asset_attributes , absolute_path 
~~ ~~ def save ( self ) : 
for asset_attributes , absolute_path in self . list ( '**' ) : 
~~~ logical_path = os . path . normpath ( asset_attributes . logical_path ) 
check_asset = build_asset ( self , logical_path , check = True ) 
if check_asset . is_public : 
~~~ asset = build_asset ( self , logical_path ) 
source = bytes ( asset ) 
self . save_file ( logical_path , source , asset . gzippable ) 
if self . fingerprinting : 
~~~ self . save_file ( asset . hexdigest_path , source , asset . gzippable ) 
self . manifest . files [ logical_path ] = asset . hexdigest_path 
~~ ~~ ~~ self . manifest . dump ( ) 
~~ def as_handler ( cls , ** initkwargs ) : 
@ wraps ( cls , updated = ( ) ) 
def handler ( asset , * args , ** kwargs ) : 
~~~ return handler . handler_class ( ** initkwargs ) ( asset , * args , ** kwargs ) 
~~ handler . handler_class = cls 
handler . supports_check_mode = cls . supports_check_mode 
return handler 
~~ def run ( self , input ) : 
p = self . get_process ( ) 
output , errors = p . communicate ( input = input . encode ( 'utf-8' ) ) 
if p . returncode != 0 : 
~~~ raise AssetHandlerError ( errors ) 
~~ return output . decode ( 'utf-8' ) 
~~ def get_process ( self ) : 
return Popen ( self . get_args ( ) , stdin = PIPE , stdout = PIPE , stderr = PIPE ) 
~~ def table ( name , auth = None , eager = True ) : 
auth = auth or [ ] 
dynamodb = boto . connect_dynamodb ( * auth ) 
table = dynamodb . get_table ( name ) 
return Table ( table = table , eager = eager ) 
~~ def tables ( auth = None , eager = True ) : 
return [ table ( t , auth , eager = eager ) for t in dynamodb . list_tables ( ) ] 
~~ def fetch_items ( self , category , ** kwargs ) : 
from_date = kwargs [ 'from_date' ] 
if category == CATEGORY_CRATES : 
~~~ return self . __fetch_crates ( from_date ) 
~~~ return self . __fetch_summary ( ) 
~~ ~~ def metadata_id ( item ) : 
if Crates . metadata_category ( item ) == CATEGORY_CRATES : 
~~~ return str ( item [ 'id' ] ) 
~~~ ts = item [ 'fetched_on' ] 
ts = str_to_datetime ( ts ) 
return str ( ts . timestamp ( ) ) 
~~ ~~ def metadata_updated_on ( item ) : 
~~~ ts = item [ 'updated_at' ] 
~~ ts = str_to_datetime ( ts ) 
return ts . timestamp ( ) 
~~ def _init_client ( self , from_archive = False ) : 
return CratesClient ( self . sleep_time , self . archive , from_archive ) 
~~ def __fetch_summary ( self ) : 
raw_summary = self . client . summary ( ) 
summary = json . loads ( raw_summary ) 
summary [ 'fetched_on' ] = str ( datetime_utcnow ( ) ) 
yield summary 
~~ def __fetch_crates ( self , from_date ) : 
from_date = datetime_to_utc ( from_date ) 
crates_groups = self . client . crates ( ) 
for raw_crates in crates_groups : 
~~~ crates = json . loads ( raw_crates ) 
for crate_container in crates [ 'crates' ] : 
~~~ if str_to_datetime ( crate_container [ 'updated_at' ] ) < from_date : 
~~ crate_id = crate_container [ 'id' ] 
crate = self . __fetch_crate_data ( crate_id ) 
crate [ 'owner_team_data' ] = self . __fetch_crate_owner_team ( crate_id ) 
crate [ 'owner_user_data' ] = self . __fetch_crate_owner_user ( crate_id ) 
crate [ 'version_downloads_data' ] = self . __fetch_crate_version_downloads ( crate_id ) 
crate [ 'versions_data' ] = self . __fetch_crate_versions ( crate_id ) 
yield crate 
~~ ~~ ~~ def __fetch_crate_owner_team ( self , crate_id ) : 
raw_owner_team = self . client . crate_attribute ( crate_id , 'owner_team' ) 
owner_team = json . loads ( raw_owner_team ) 
return owner_team 
~~ def __fetch_crate_owner_user ( self , crate_id ) : 
raw_owner_user = self . client . crate_attribute ( crate_id , 'owner_user' ) 
owner_user = json . loads ( raw_owner_user ) 
return owner_user 
~~ def __fetch_crate_versions ( self , crate_id ) : 
raw_versions = self . client . crate_attribute ( crate_id , "versions" ) 
version_downloads = json . loads ( raw_versions ) 
return version_downloads 
~~ def __fetch_crate_version_downloads ( self , crate_id ) : 
raw_version_downloads = self . client . crate_attribute ( crate_id , "downloads" ) 
version_downloads = json . loads ( raw_version_downloads ) 
~~ def __fetch_crate_data ( self , crate_id ) : 
raw_crate = self . client . crate ( crate_id ) 
crate = json . loads ( raw_crate ) 
return crate [ 'crate' ] 
~~ def summary ( self ) : 
path = urijoin ( CRATES_API_URL , CATEGORY_SUMMARY ) 
raw_content = self . fetch ( path ) 
return raw_content 
~~ def crates ( self , from_page = 1 ) : 
path = urijoin ( CRATES_API_URL , CATEGORY_CRATES ) 
raw_crates = self . __fetch_items ( path , from_page ) 
return raw_crates 
~~ def crate ( self , crate_id ) : 
path = urijoin ( CRATES_API_URL , CATEGORY_CRATES , crate_id ) 
raw_crate = self . fetch ( path ) 
return raw_crate 
~~ def crate_attribute ( self , crate_id , attribute ) : 
path = urijoin ( CRATES_API_URL , CATEGORY_CRATES , crate_id , attribute ) 
raw_attribute_data = self . fetch ( path ) 
return raw_attribute_data 
~~ def __fetch_items ( self , path , page = 1 ) : 
fetch_data = True 
parsed_crates = 0 
total_crates = 0 
while fetch_data : 
~~~ payload = { 'sort' : 'alphabetical' , 'page' : page } 
raw_content = self . fetch ( path , payload = payload ) 
content = json . loads ( raw_content ) 
parsed_crates += len ( content [ 'crates' ] ) 
if not total_crates : 
~~~ total_crates = content [ 'meta' ] [ 'total' ] 
~~ ~~ except requests . exceptions . HTTPError as e : 
raise e 
~~ yield raw_content 
page += 1 
if parsed_crates >= total_crates : 
~~~ fetch_data = False 
~~ ~~ ~~ def fetch ( self , url , payload = None ) : 
response = super ( ) . fetch ( url , payload = payload ) 
return response . text 
~~ def fetch ( self , category = CATEGORY_QUESTION , offset = DEFAULT_OFFSET ) : 
if not offset : 
~~~ offset = DEFAULT_OFFSET 
~~ kwargs = { "offset" : offset } 
items = super ( ) . fetch ( category , ** kwargs ) 
return items 
offset = kwargs [ 'offset' ] 
self . url , str ( offset ) ) 
page = int ( offset / KitsuneClient . ITEMS_PER_PAGE ) 
page_offset = page * KitsuneClient . ITEMS_PER_PAGE 
drop_questions = offset - page_offset 
current_offset = offset 
questions_page = self . client . get_questions ( offset ) 
~~~ raw_questions = next ( questions_page ) 
~~ except StopIteration : 
~~ except requests . exceptions . HTTPError as e : 
~~~ if e . response . status_code == 500 : 
~~~ logger . exception ( e ) 
KitsuneClient . ITEMS_PER_PAGE ) 
equestions += KitsuneClient . ITEMS_PER_PAGE 
current_offset += KitsuneClient . ITEMS_PER_PAGE 
questions_page = self . client . get_questions ( current_offset ) 
continue 
~~~ raise e 
~~ ~~ try : 
~~~ questions_data = json . loads ( raw_questions ) 
tquestions = questions_data [ 'count' ] 
questions = questions_data [ 'results' ] 
~~ except ( ValueError , KeyError ) as ex : 
~~~ logger . error ( ex ) 
raise ParseError ( cause = cause ) 
~~ for question in questions : 
~~~ if drop_questions > 0 : 
~~~ drop_questions -= 1 
~~ question [ 'offset' ] = current_offset 
current_offset += 1 
question [ 'answers_data' ] = [ ] 
for raw_answers in self . client . get_question_answers ( question [ 'id' ] ) : 
~~~ answers = json . loads ( raw_answers ) [ 'results' ] 
question [ 'answers_data' ] += answers 
~~ yield question 
nquestions += 1 
return KitsuneClient ( self . url , self . archive , from_archive ) 
~~ def get_questions ( self , offset = None ) : 
page = KitsuneClient . FIRST_PAGE 
if offset : 
~~~ page += int ( offset / KitsuneClient . ITEMS_PER_PAGE ) 
~~ while True : 
~~~ api_questions_url = urijoin ( self . base_url , '/question' ) + '/' 
params = { 
"page" : page , 
"ordering" : "updated" 
questions = self . fetch ( api_questions_url , params ) 
yield questions 
questions_json = json . loads ( questions ) 
next_uri = questions_json [ 'next' ] 
if not next_uri : 
~~ page += 1 
~~ ~~ def get_question_answers ( self , question_id ) : 
~~~ api_answers_url = urijoin ( self . base_url , '/answer' ) + '/' 
"question" : question_id , 
answers_raw = self . fetch ( api_answers_url , params ) 
yield answers_raw 
answers = json . loads ( answers_raw ) 
if not answers [ 'next' ] : 
~~ ~~ def fetch ( self , url , params ) : 
url , str ( params ) ) 
response = super ( ) . fetch ( url , payload = params ) 
~~ def fetch ( self , category = CATEGORY_EVENT , offset = REMO_DEFAULT_OFFSET ) : 
~~~ offset = REMO_DEFAULT_OFFSET 
self . url , category , offset ) 
page = int ( offset / ReMoClient . ITEMS_PER_PAGE ) 
page_offset = page * ReMoClient . ITEMS_PER_PAGE 
drop_items = offset - page_offset 
drop_items , offset , page , page_offset ) 
for raw_items in self . client . get_items ( category , offset ) : 
~~~ items_data = json . loads ( raw_items ) 
titems = items_data [ 'count' ] 
titems - current_offset , current_offset ) 
items = items_data [ 'results' ] 
for item in items : 
~~~ if drop_items > 0 : 
~~~ drop_items -= 1 
~~ raw_item_details = self . client . fetch ( item [ '_url' ] ) 
item_details = json . loads ( raw_item_details ) 
item_details [ 'offset' ] = current_offset 
yield item_details 
nitems += 1 
~~ def metadata_updated_on ( item ) : 
if 'end' in item : 
~~~ updated = item [ 'end' ] 
~~ elif 'date_joined_program' in item : 
~~~ updated = item [ 'date_joined_program' ] 
~~ elif 'report_date' in item : 
~~~ updated = item [ 'report_date' ] 
~~~ raise ValueError ( "Can\ + str ( item ) ) 
~~ return float ( str_to_datetime ( updated ) . timestamp ( ) ) 
~~ def metadata_category ( item ) : 
if 'estimated_attendance' in item : 
~~~ category = CATEGORY_EVENT 
~~ elif 'activity' in item : 
~~~ category = CATEGORY_ACTIVITY 
~~ elif 'first_name' in item : 
~~~ category = CATEGORY_USER 
~~ return category 
return ReMoClient ( self . url , self . archive , from_archive ) 
~~ def get_items ( self , category = CATEGORY_EVENT , offset = REMO_DEFAULT_OFFSET ) : 
page = ReMoClient . FIRST_PAGE 
page += int ( offset / ReMoClient . ITEMS_PER_PAGE ) 
if category == CATEGORY_EVENT : 
~~~ api = self . api_events_url 
~~ elif category == CATEGORY_ACTIVITY : 
~~~ api = self . api_activities_url 
~~ elif category == CATEGORY_USER : 
~~~ api = self . api_users_url 
~~ while more : 
~~~ params = { 
"orderby" : "ASC" 
api , str ( params ) ) 
raw_items = self . fetch ( api , payload = params ) 
yield raw_items 
items_data = json . loads ( raw_items ) 
next_uri = items_data [ 'next' ] 
~~~ more = False 
~~~ parsed_uri = urllib . parse . urlparse ( next_uri ) 
parsed_params = urllib . parse . parse_qs ( parsed_uri . query ) 
page = parsed_params [ 'page' ] [ 0 ] 
~~ ~~ ~~ def buffer_list ( self ) : 
if self . _iocb . aio_lio_opcode == libaio . IO_CMD_POLL : 
~~~ raise AttributeError 
~~ return self . _buffer_list 
~~ def io_priority ( self ) : 
return ( 
self . _iocb . aio_reqprio 
if self . _iocb . u . c . flags & libaio . IOCB_FLAG_IOPRIO else 
None 
~~ def close ( self ) : 
if self . _ctx is not None : 
~~~ self . _io_queue_release ( self . _ctx ) 
del self . _ctx 
~~ ~~ def submit ( self , block_list ) : 
submitted_count = libaio . io_submit ( 
self . _ctx , 
len ( block_list ) , 
( libaio . iocb_p * len ( block_list ) ) ( * [ 
pointer ( x . _iocb ) 
for x in block_list 
] ) , 
submitted = self . _submitted 
for block in block_list [ : submitted_count ] : 
~~~ submitted [ addressof ( block . _iocb ) ] = ( block , block . _getSubmissionState ( ) ) 
~~ return submitted_count 
~~ def cancel ( self , block ) : 
event = libaio . io_event ( ) 
~~~ libaio . io_cancel ( self . _ctx , byref ( block . _iocb ) , byref ( event ) ) 
~~ except OSError as exc : 
~~~ if exc . errno == errno . EINPROGRESS : 
~~~ return None 
~~ raise 
~~ return self . _eventToPython ( event ) 
~~ def cancelAll ( self ) : 
cancel = self . cancel 
result = [ ] 
for block , _ in self . _submitted . itervalues ( ) : 
~~~ result . append ( cancel ( block ) ) 
~~~ if exc . errno != errno . EINVAL : 
~~~ raise 
~~ ~~ ~~ return result 
~~ def getEvents ( self , min_nr = 1 , nr = None , timeout = None ) : 
if min_nr is None : 
~~~ min_nr = len ( self . _submitted ) 
~~ if nr is None : 
~~~ nr = max ( len ( self . _submitted ) , self . _maxevents ) 
~~ if timeout is None : 
~~~ timeoutp = None 
~~~ sec = int ( timeout ) 
timeout = libaio . timespec ( sec , int ( ( timeout - sec ) * 1e9 ) ) 
timeoutp = byref ( timeout ) 
~~ event_buffer = ( libaio . io_event * nr ) ( ) 
actual_nr = libaio . io_getevents ( 
min_nr , 
nr , 
event_buffer , 
timeoutp , 
return [ 
self . _eventToPython ( event_buffer [ x ] ) 
for x in xrange ( actual_nr ) 
~~ def fetch ( self , category = CATEGORY_EVENT ) : 
kwargs = { } 
raw_cells = self . client . get_cells ( ) 
parser = MozillaClubParser ( raw_cells ) 
for event in parser . parse ( ) : 
~~~ yield event 
nevents += 1 
return MozillaClubClient ( self . url , self . archive , from_archive ) 
~~ def get_cells ( self ) : 
raw_cells = self . fetch ( self . base_url ) 
return raw_cells . text 
~~ def parse ( self ) : 
nevents_wrong = 0 
feed_json = json . loads ( self . feed ) 
if 'entry' not in feed_json [ 'feed' ] : 
~~~ return 
~~ self . cells = feed_json [ 'feed' ] [ 'entry' ] 
self . ncell = 0 
event_fields = self . __get_event_fields ( ) 
while self . ncell < len ( self . cells ) : 
~~~ event = self . __get_next_event ( event_fields ) 
nevents_wrong += 1 
~~ yield event 
~~ def __get_event_fields ( self ) : 
event_fields = { } 
~~~ cell = self . cells [ self . ncell ] 
row = cell [ 'gs$cell' ] [ 'row' ] 
if int ( row ) > 1 : 
~~ ncol = int ( cell [ 'gs$cell' ] [ 'col' ] ) 
name = cell [ 'content' ] [ '$t' ] 
event_fields [ ncol ] = name 
if ncol in EVENT_TEMPLATE : 
~~~ if event_fields [ ncol ] != EVENT_TEMPLATE [ ncol ] : 
name , EVENT_TEMPLATE [ ncol ] ) 
~~ ~~ else : 
~~ self . ncell += 1 
~~ return event_fields 
test = check ( ) 
if test . succeeded : 
if sdist . succeeded : 
~~~ build = local ( 
if build . succeeded : 
if upload . succeeded : 
~~~ tag ( ) 
~~ ~~ ~~ ~~ ~~ def tag ( version = __version__ ) : 
~~ ~~ def _check_currency_format ( self , format = None ) : 
defaults = self . settings [ 'currency' ] [ 'format' ] 
if hasattr ( format , '__call__' ) : 
~~~ format = format ( ) 
~~ if is_str ( format ) and re . match ( '%v' , format ) : 
~~~ return { 
'pos' : format , 
'neg' : format . replace ( "-" , "" ) . replace ( "%v" , "-%v" ) , 
'zero' : format 
~~ elif not format or not format [ 'por' ] or not re . match ( '%v' , 
format [ 'pos' ] ) : 
~~~ self . settings [ 'currency' ] [ 'format' ] = { 
'pos' : defaults , 
'neg' : defaults . replace ( "%v" , "-%v" ) , 
'zero' : defaults 
return self . settings 
~~ return format 
~~ def _change_precision ( self , val , base = 0 ) : 
if not isinstance ( val , int ) : 
~~ val = round ( abs ( val ) ) 
val = ( lambda num : base if is_num ( num ) else num ) ( val ) 
return val 
~~ def parse ( self , value , decimal = None ) : 
value = value or 0 
if check_type ( value , 'list' ) : 
~~~ return map ( lambda val : self . parse ( val , decimal ) ) 
~~ if check_type ( value , 'int' ) or check_type ( value , 'float' ) : 
~~ decimal = decimal or self . settings . number . decimal 
regex = re . compile ( "[^0-9-" + decimal + "]" ) 
unformatted = str ( value ) 
unformatted = re . sub ( '/\\((.*)\\)/' , "-$1" , unformatted ) 
unformatted = re . sub ( regex , '' , unformatted ) 
unformatted = unformatted . replace ( '.' , decimal ) 
formatted = ( lambda val : unformatted if val else 0 ) ( 
is_num ( unformatted ) ) 
return formatted 
~~ def to_fixed ( self , value , precision ) : 
precision = self . _change_precision ( 
precision , self . settings [ 'number' ] [ 'precision' ] ) 
power = pow ( 10 , precision ) 
power = round ( self . parse ( value ) * power ) / power 
~~ def format ( self , number , ** kwargs ) : 
if check_type ( number , 'list' ) : 
~~~ return map ( lambda val : self . format ( val , ** kwargs ) ) 
~~ number = self . parse ( number ) 
if check_type ( kwargs , 'dict' ) : 
~~~ options = ( self . settings [ 'number' ] . update ( kwargs ) ) 
~~ precision = self . _change_precision ( options [ 'precision' ] ) 
negative = ( lambda num : "-" if num < 0 else "" ) ( number ) 
base = str ( int ( self . to_fixed ( abs ( number ) or 0 , precision ) ) , 10 ) 
mod = ( lambda num : len ( num ) % 3 if len ( num ) > 3 else 0 ) ( base ) 
num = negative + ( lambda num : base [ 0 : num ] if num else '' ) ( mod ) 
num += re . sub ( '/(\\d{3})(?=\\d)/g' , '$1' + 
options [ 'thousand' ] , base [ mod : ] ) 
num += ( lambda val : options [ 
'decimal' ] + self . to_fixed ( abs ( number ) , precision ) 
. split ( '.' ) [ 1 ] if val else '' ) ( precision ) 
return num 
~~ def as_money ( self , number , ** options ) : 
if isinstance ( number , list ) : 
~~~ return map ( lambda val : self . as_money ( val , ** options ) ) 
~~ decimal = options . get ( 'decimal' ) 
number = self . parse ( number , decimal ) 
if check_type ( options , 'dict' ) : 
~~~ options = ( self . settings [ 'currency' ] . update ( options ) ) 
~~ formats = self . _check_currency_format ( options [ 'format' ] ) 
use_format = ( lambda num : formats [ 'pos' ] if num > 0 else formats [ 
'neg' ] if num < 0 else formats [ 'zero' ] ) ( number ) 
precision = self . _change_precision ( number , options [ 'precision' ] ) 
thousands = options [ 'thousand' ] 
decimal = options [ 'decimal' ] 
formater = self . format ( abs ( number ) , precision , thousands , decimal ) 
amount = use_format . replace ( 
'%s' , options [ 'symbol' ] ) . replace ( '%v' , formater ) 
return amount 
~~ def mix_and_match ( name , greeting = 'Hello' , yell = False ) : 
if yell : 
~~~ print '%s!' % say . upper ( ) 
~~~ print '%s.' % say 
~~ ~~ def option_decorator ( name , greeting , yell ) : 
~~ ~~ def run ( self , job : Job ) -> Future [ Result ] : 
if not self . watcher_ready : 
~~ elif not self . can_execute ( job ) : 
job . cancel ( 'invalid' ) 
task = asyncio . ensure_future ( self . _execute ( job ) , loop = self . loop ) 
task . add_done_callback ( job . finish ) 
task . add_done_callback ( L ( self . job_done ) ( job , _ ) ) 
self . current [ job . client ] = job 
~~ return job . status 
~~ def infer_gaps_in_tree ( df_seq , tree , id_col = 'id' , sequence_col = 'sequence' ) : 
taxa = tree . taxon_namespace 
alignment = df_seq . phylo . to_fasta ( id_col = id_col , id_only = True , 
sequence_col = sequence_col ) 
data = dendropy . ProteinCharacterMatrix . get ( 
data = alignment , 
schema = "fasta" , 
taxon_namespace = taxa ) 
taxon_state_sets_map = data . taxon_state_sets_map ( gaps_as_missing = False ) 
dendropy . model . parsimony . fitch_down_pass ( tree . postorder_node_iter ( ) , 
taxon_state_sets_map = taxon_state_sets_map ) 
dendropy . model . parsimony . fitch_up_pass ( tree . preorder_node_iter ( ) ) 
return tree 
~~ def nvim_io_recover ( self , io : NvimIORecover [ A ] ) -> NvimIO [ B ] : 
return eval_step ( self . vim ) ( io . map ( lambda a : a ) ) 
~~ def read_codeml_output ( 
filename , 
df , 
altall_cutoff = 0.2 , 
with open ( filename , 'r' ) as f : 
~~~ data = f . read ( ) 
trees = regex . findall ( data ) 
anc_tree = trees [ 2 ] 
tip_tree = dendropy . Tree . get ( data = trees [ 0 ] , schema = 'newick' ) 
anc_tree = dendropy . Tree . get ( data = trees [ 2 ] , schema = 'newick' ) 
tree = tip_tree 
ancestors = anc_tree . internal_nodes ( ) 
for i , node in enumerate ( tree . internal_nodes ( ) ) : 
~~~ node . label = ancestors [ i ] . label 
~~ df [ 'reconstruct_label' ] = None 
for node in tree . postorder_node_iter ( ) : 
~~~ if node . parent_node is None : 
~~~ pass 
~~ elif node . is_leaf ( ) : 
~~~ node_label = node . taxon . label 
parent_label = node . parent_node . label 
df . loc [ df . uid == node_label , 'reconstruct_label' ] = node_label 
parent_id = df . loc [ df . uid == node_label , 'parent' ] . values [ 0 ] 
df . loc [ df . id == parent_id , 'reconstruct_label' ] = node . parent_node . label 
~~ elif node . is_internal ( ) : 
~~~ label = node . label 
parent_id = df . loc [ df . reconstruct_label == label , 'parent' ] . values [ 0 ] 
node_num_regex = re . compile ( "[0-9]+" ) 
df [ 'ml_sequence' ] = None 
df [ 'ml_posterior' ] = None 
df [ 'alt_sequence' ] = None 
df [ 'alt_posterior' ] = None 
for node in node_regex . findall ( data ) : 
~~~ node_label = node_num_regex . search ( node ) . group ( 0 ) 
ml_sequence , ml_posterior , alt_sequence , alt_posterior = [ ] , [ ] , [ ] , [ ] 
for site in site_regex . findall ( node ) : 
~~~ scores = [ float ( site [ i + 2 : i + 7 ] ) for i in range ( 0 , len ( site ) , 9 ) ] 
residues = [ site [ i ] for i in range ( 0 , len ( site ) , 9 ) ] 
sorted_score_index = [ i [ 0 ] for i in sorted ( 
enumerate ( scores ) , 
key = lambda x : x [ 1 ] , 
reverse = True ) ] 
ml_idx = sorted_score_index [ 0 ] 
alt_idx = sorted_score_index [ 1 ] 
ml_sequence . append ( residues [ ml_idx ] ) 
ml_posterior . append ( scores [ ml_idx ] ) 
if scores [ alt_idx ] < altall_cutoff : 
~~~ alt_idx = ml_idx 
~~ alt_sequence . append ( residues [ alt_idx ] ) 
alt_posterior . append ( scores [ alt_idx ] ) 
~~ keys = [ 
"ml_sequence" , 
"ml_posterior" , 
"alt_sequence" , 
"alt_posterior" 
vals = [ 
"" . join ( ml_sequence ) , 
sum ( ml_posterior ) / len ( ml_posterior ) , 
"" . join ( alt_sequence ) , 
sum ( alt_posterior ) / len ( alt_posterior ) , 
df . loc [ df . reconstruct_label == node_label , keys ] = vals 
~~ return df 
~~ def ugettext ( message , context = None ) : 
stripped = strip_whitespace ( message ) 
message = add_context ( context , stripped ) if context else stripped 
ret = django_ugettext ( message ) 
return stripped if ret == message else ret 
~~ def ungettext ( singular , plural , number , context = None ) : 
singular_stripped = strip_whitespace ( singular ) 
plural_stripped = strip_whitespace ( plural ) 
if context : 
~~~ singular = add_context ( context , singular_stripped ) 
plural = add_context ( context , plural_stripped ) 
~~~ singular = singular_stripped 
plural = plural_stripped 
~~ ret = django_nugettext ( singular , plural , number ) 
if ret == singular : 
~~~ return singular_stripped 
~~ elif ret == plural : 
~~~ return plural_stripped 
~~ def install_jinja_translations ( ) : 
class Translation ( object ) : 
ugettext = staticmethod ( ugettext ) 
ungettext = staticmethod ( ungettext ) 
~~ import jingo 
jingo . env . install_gettext_translations ( Translation ) 
~~ def activate ( locale ) : 
if INSTALL_JINJA_TRANSLATIONS : 
~~~ install_jinja_translations ( ) 
~~ if django . VERSION >= ( 1 , 3 ) : 
~~~ django_trans . _active . value = _activate ( locale ) 
~~~ from django . utils . thread_support import currentThread 
django_trans . _active [ currentThread ( ) ] = _activate ( locale ) 
~~ ~~ def tweak_message ( message ) : 
if isinstance ( message , basestring ) : 
~~~ message = strip_whitespace ( message ) 
~~ elif isinstance ( message , tuple ) : 
~~~ if len ( message ) == 2 : 
~~~ message = add_context ( message [ 1 ] , message [ 0 ] ) 
~~ elif len ( message ) == 3 : 
~~~ if all ( isinstance ( x , basestring ) for x in message [ : 2 ] ) : 
~~~ singular , plural , num = message 
message = ( strip_whitespace ( singular ) , 
strip_whitespace ( plural ) , 
num ) 
~~ ~~ elif len ( message ) == 4 : 
~~~ singular , plural , num , ctxt = message 
message = ( add_context ( ctxt , strip_whitespace ( singular ) ) , 
add_context ( ctxt , strip_whitespace ( plural ) ) , 
~~ ~~ return message 
~~ def exclusive_ns ( guard : StateGuard [ A ] , desc : str , thunk : Callable [ ... , NS [ A , B ] ] , * a : Any ) -> Do : 
yield guard . acquire ( ) 
state , response = yield N . ensure_failure ( thunk ( * a ) . run ( guard . state ) , guard . release ) 
yield N . delay ( lambda v : unsafe_update_state ( guard , state ) ) 
yield guard . release ( ) 
yield N . pure ( response ) 
~~ def validate_twilio_signature ( func = None , backend_name = 'twilio-backend' ) : 
def _dec ( view_func ) : 
~~~ @ functools . wraps ( view_func , assigned = available_attrs ( view_func ) ) 
def _wrapped_view ( request , * args , ** kwargs ) : 
~~~ backend = kwargs . get ( 'backend_name' , backend_name ) 
config = settings . INSTALLED_BACKENDS [ backend ] [ 'config' ] 
validator = RequestValidator ( config [ 'auth_token' ] ) 
signature = request . META . get ( 'HTTP_X_TWILIO_SIGNATURE' , '' ) 
url = request . build_absolute_uri ( ) 
body = { } 
if request . method == 'POST' : 
~~~ body = request . POST 
~~ require_validation = config . get ( 'validate' , True ) 
if validator . validate ( url , body , signature ) or not require_validation : 
~~~ return view_func ( request , * args , ** kwargs ) 
~~~ return HttpResponseBadRequest ( ) 
~~ ~~ return _wrapped_view 
~~ if func is None : 
~~~ return _dec 
~~~ return _dec ( func ) 
~~ ~~ def getCellVertexes ( self , i , j ) : 
x , y = self . _getUnrotatedCellCentroidCoords ( i , j ) 
self . rotatePoint ( x - self . _side , y ) , 
self . rotatePoint ( x - self . _side / 2.0 , y - self . _hexPerp ) , 
self . rotatePoint ( x + self . _side / 2.0 , y - self . _hexPerp ) , 
self . rotatePoint ( x + self . _side , y ) , 
self . rotatePoint ( x + self . _side / 2.0 , y + self . _hexPerp ) , 
self . rotatePoint ( x - self . _side / 2.0 , y + self . _hexPerp ) , 
~~ def rotatePoint ( self , pointX , pointY ) : 
if ( self . angle == 0 or self . angle == None ) : 
~~~ return ( pointX , pointY ) 
~~ length = math . sqrt ( ( pointX - self . xll ) ** 2 + ( pointY - self . yll ) ** 2 ) 
beta = math . acos ( ( pointX - self . xll ) / length ) 
if ( pointY < self . yll ) : 
~~~ beta = math . pi * 2 - beta 
~~ offsetX = math . cos ( beta ) * length - math . cos ( self . _angle_rd + beta ) * length 
offsetY = math . sin ( self . _angle_rd + beta ) * length - math . sin ( beta ) * length 
return ( pointX - offsetX , pointY + offsetY ) 
~~ def floyd ( seqs , f = None , start = None , key = lambda x : x ) : 
~~~ """Floyd\ 
tortise , hare = seqs 
yield hare . next ( ) 
tortise_value = tortise . next ( ) 
hare_value = hare . next ( ) 
while hare_value != tortise_value : 
~~~ yield hare_value 
~~ if f is None : 
~~~ raise CycleDetected ( ) 
~~ hare_value = f ( hare_value ) 
first = 0 
tortise_value = start 
while key ( tortise_value ) != key ( hare_value ) : 
~~~ tortise_value = f ( tortise_value ) 
hare_value = f ( hare_value ) 
first += 1 
~~ period = 1 
hare_value = f ( tortise_value ) 
~~~ hare_value = f ( hare_value ) 
period += 1 
~~ raise CycleDetected ( period = period , first = first ) 
~~ def naive ( seqs , f = None , start = None , key = lambda x : x ) : 
history = { } 
for step , value in enumerate ( seqs [ 0 ] ) : 
~~~ keyed = key ( value ) 
yield value 
if keyed in history : 
~~~ raise CycleDetected ( 
first = history [ keyed ] , period = step - history [ keyed ] ) 
~~ history [ keyed ] = step 
~~ ~~ def gosper ( seqs , f = None , start = None , key = lambda x : x ) : 
~~~ """Gosper\ 
tab = [ ] 
for c , value in enumerate ( seqs [ 0 ] , start = 1 ) : 
~~~ yield value 
~~~ e = tab . index ( key ( value ) ) 
raise CycleDetected ( 
period = c - ( ( ( ( c >> e ) - 1 ) | 1 ) << e ) ) 
~~~ tab [ ( c ^ ( c - 1 ) ) . bit_length ( ) - 1 ] = key ( value ) 
~~ except IndexError : 
~~~ tab . append ( value ) 
~~ ~~ ~~ ~~ def brent ( seqs , f = None , start = None , key = lambda x : x ) : 
~~~ """Brent\ 
power = period = 1 
if power == period : 
~~~ power *= 2 
period = 0 
if f : 
~~~ tortise = f_generator ( f , hare_value ) 
~~~ while tortise_value != hare_value : 
~~~ tortise_value = tortise . next ( ) 
~~ ~~ ~~ hare_value = hare . next ( ) 
~~ first = 0 
tortise_value = hare_value = start 
for _ in xrange ( period ) : 
~~ while key ( tortise_value ) != key ( hare_value ) : 
~~ def setup ( app ) : 
app . setup_extension ( 'sphinx.ext.todo' ) 
app . setup_extension ( 'sphinx.ext.mathjax' ) 
app . setup_extension ( "sphinx.ext.intersphinx" ) 
app . config . intersphinx_mapping . update ( { 
'https://docs.python.org/' : None 
} ) 
sage_doc_url + doc + "/" : None 
for doc in sage_documents 
sage_doc_url + "reference/" + module : None 
for module in sage_modules 
app . setup_extension ( "sphinx.ext.extlinks" ) 
app . config . extlinks . update ( { 
'python' : ( 'https://docs.python.org/release/' + pythonversion + '/%s' , '' ) , 
'doi' : ( 'https://dx.doi.org/%s' , 'doi:' ) , 
'pari' : ( 'http://pari.math.u-bordeaux.fr/dochtml/help/%s' , 'pari:' ) , 
app . config . html_theme = 'sage' 
~~ def themes_path ( ) : 
package_dir = os . path . abspath ( os . path . dirname ( __file__ ) ) 
return os . path . join ( package_dir , 'themes' ) 
~~ def list_dataset_uris ( cls , base_uri , config_path ) : 
uri_list = [ ] 
parse_result = generous_parse_uri ( base_uri ) 
bucket_name = parse_result . netloc 
bucket = boto3 . resource ( 's3' ) . Bucket ( bucket_name ) 
for obj in bucket . objects . filter ( Prefix = 'dtool' ) . all ( ) : 
~~~ uuid = obj . key . split ( '-' , 1 ) [ 1 ] 
uri = cls . generate_uri ( None , uuid , base_uri ) 
storage_broker = cls ( uri , config_path ) 
if storage_broker . has_admin_metadata ( ) : 
~~~ uri_list . append ( uri ) 
~~ ~~ return uri_list 
~~ def get_item_abspath ( self , identifier ) : 
admin_metadata = self . get_admin_metadata ( ) 
uuid = admin_metadata [ "uuid" ] 
dataset_cache_abspath = os . path . join ( self . _s3_cache_abspath , uuid ) 
mkdir_parents ( dataset_cache_abspath ) 
bucket_fpath = self . data_key_prefix + identifier 
obj = self . s3resource . Object ( self . bucket , bucket_fpath ) 
relpath = obj . get ( ) [ 'Metadata' ] [ 'handle' ] 
_ , ext = os . path . splitext ( relpath ) 
local_item_abspath = os . path . join ( 
dataset_cache_abspath , 
identifier + ext 
if not os . path . isfile ( local_item_abspath ) : 
~~~ tmp_local_item_abspath = local_item_abspath + ".tmp" 
self . s3resource . Bucket ( self . bucket ) . download_file ( 
bucket_fpath , 
tmp_local_item_abspath 
os . rename ( tmp_local_item_abspath , local_item_abspath ) 
~~ return local_item_abspath 
~~ def list_overlay_names ( self ) : 
bucket = self . s3resource . Bucket ( self . bucket ) 
overlay_names = [ ] 
for obj in bucket . objects . filter ( 
Prefix = self . overlays_key_prefix 
) . all ( ) : 
~~~ overlay_file = obj . key . rsplit ( '/' , 1 ) [ - 1 ] 
overlay_name , ext = overlay_file . split ( '.' ) 
overlay_names . append ( overlay_name ) 
~~ return overlay_names 
~~ def add_item_metadata ( self , handle , key , value ) : 
identifier = generate_identifier ( handle ) 
suffix = '{}.{}.json' . format ( identifier , key ) 
bucket_fpath = self . fragments_key_prefix + suffix 
self . s3resource . Object ( self . bucket , bucket_fpath ) . put ( 
Body = json . dumps ( value ) 
~~ def iter_item_handles ( self ) : 
for obj in bucket . objects . filter ( Prefix = self . data_key_prefix ) . all ( ) : 
~~~ relpath = obj . get ( ) [ 'Metadata' ] [ 'handle' ] 
yield relpath 
~~ ~~ def get_item_metadata ( self , handle ) : 
metadata = { } 
prefix = self . fragments_key_prefix + '{}' . format ( identifier ) 
for obj in bucket . objects . filter ( Prefix = prefix ) . all ( ) : 
~~~ metadata_key = obj . key . split ( '.' ) [ - 2 ] 
response = obj . get ( ) 
value_as_string = response [ 'Body' ] . read ( ) . decode ( 'utf-8' ) 
value = json . loads ( value_as_string ) 
metadata [ metadata_key ] = value 
~~ return metadata 
~~ def temp ( s , DNA_c = 5000.0 , Na_c = 10.0 , Mg_c = 20.0 , dNTPs_c = 10.0 , uncorrected = False ) : 
s = s . upper ( ) 
dh , ds = _tercorr ( s ) 
k = DNA_c * 1e-9 
dh_coeffs = { "AA" : - 7.9 , "TT" : - 7.9 , 
"AT" : - 7.2 , 
"TA" : - 7.2 , 
"CA" : - 8.5 , "TG" : - 8.5 , 
"GT" : - 8.4 , "AC" : - 8.4 , 
"CT" : - 7.8 , "AG" : - 7.8 , 
"GA" : - 8.2 , "TC" : - 8.2 , 
"CG" : - 10.6 , 
"GC" : - 9.8 , 
"GG" : - 8.0 , "CC" : - 8.0 } 
ds_coeffs = { "AA" : - 22.2 , "TT" : - 22.2 , 
"AT" : - 20.4 , 
"TA" : - 21.3 , 
"CA" : - 22.7 , "TG" : - 22.7 , 
"GT" : - 22.4 , "AC" : - 22.4 , 
"CT" : - 21.0 , "AG" : - 21.0 , 
"GA" : - 22.2 , "TC" : - 22.2 , 
"CG" : - 27.2 , 
"GC" : - 24.4 , 
"GG" : - 19.9 , "CC" : - 19.9 } 
dh = dh + sum ( _overcount ( s , pair ) * coeff for pair , coeff in dh_coeffs . items ( ) ) 
ds = ds + sum ( _overcount ( s , pair ) * coeff for pair , coeff in ds_coeffs . items ( ) ) 
fgc = len ( [ filter ( lambda x : x == 'G' or x == 'C' , s ) ] ) / float ( len ( s ) ) 
tm = ( 1000 * dh ) / ( ds + ( R * log ( k ) ) ) 
if uncorrected : 
~~~ return tm - 273.15 
~~ MNa = Na_c * 1e-3 
MMg = Mg_c * 1e-3 
MdNTPs = dNTPs_c * 1e-3 
D = ( Ka * MdNTPs - Ka * MMg + 1 ) ** 2 + ( 4 * Ka * MMg ) 
Fmg = ( - ( Ka * MdNTPs - Ka * MMg + 1 ) + sqrt ( D ) ) / ( 2 * Ka ) 
cation_ratio = sqrt ( Fmg ) / MNa if MNa > 0 else 7.0 
if cation_ratio < 0.22 : 
~~~ tm = 1 / ( 
( 1 / tm ) + 
( ( 4.29 * fgc - 3.95 ) * log ( MNa ) + 0.94 * log ( MNa ) ** 2 ) * 1e-5 ) 
~~~ a = 3.92 
d = 1.42 
g = 8.31 
Fmg = MMg 
if cation_ratio < 6.0 : 
~~~ a = a * ( 0.843 - 0.352 * sqrt ( MNa ) * log ( MNa ) ) 
d = d * ( 1.279 - 4.03 * log ( MNa ) * 1e-3 - 8.03 * log ( MNa ) ** 2 * 1e-3 ) 
g = g * ( 0.486 - 0.258 * log ( MNa ) + 5.25 * log ( MNa ) ** 3 * 1e-3 ) 
~~ tm = 1 / ( 
( a - 0.911 * log ( Fmg ) + fgc * ( 6.26 + d * log ( Fmg ) ) + 
1 / ( 2 * ( len ( s ) - 1 ) ) * ( - 48.2 + 52.5 * log ( Fmg ) + 
g * log ( Fmg ) ** 2 ) ) * 1e-5 ) 
~~ return tm - 273.15 
~~ def start ( dashboards , once , secrets ) : 
if secrets is None : 
~~~ secrets = os . path . join ( os . path . expanduser ( "~" ) , "/.doodledashboard/secrets" ) 
~~~ loaded_secrets = try_read_secrets_file ( secrets ) 
~~ except InvalidSecretsException as err : 
raise click . Abort ( ) 
for dashboard_file in dashboards : 
~~~ read_configs . append ( read_file ( dashboard_file ) ) 
~~ dashboard_config = DashboardConfigReader ( initialise_component_loader ( ) , loaded_secrets ) 
~~~ dashboard = read_dashboard_from_config ( dashboard_config , read_configs ) 
~~ except YAMLError as err : 
~~~ DashboardValidator ( ) . validate ( dashboard ) 
~~ except ValidationException as err : 
~~ explain_dashboard ( dashboard ) 
~~~ DashboardRunner ( dashboard ) . cycle ( ) 
~~ except SecretNotFound as err : 
~~ if once : 
~~ ~~ ~~ def view ( action , dashboards , secrets ) : 
read_configs = [ read_file ( f ) for f in dashboards ] 
dashboard = read_dashboard_from_config ( dashboard_config , read_configs ) 
~~~ messages = DashboardRunner ( dashboard ) . poll_datafeeds ( ) 
~~ cli_output = { "source-data" : messages } 
if action == "notifications" : 
~~~ cli_output [ "notifications" ] = [ ] 
for notification in dashboard . notifications : 
~~~ notification_output = notification . create ( messages ) 
filtered_messages = messages 
if isinstance ( notification , FilteredNotification ) : 
~~~ filtered_messages = notification . filter_messages ( messages ) 
~~ cli_output [ "notifications" ] . append ( { 
"filtered-messages" : filtered_messages , 
"notification" : str ( notification_output ) 
~~ ~~ json_output = json . dumps ( cli_output , sort_keys = True , indent = 4 , cls = MessageJsonEncoder ) 
click . echo ( json_output ) 
~~ def list ( component_type ) : 
config_loader = initialise_component_loader ( ) 
component_types = sorted ( { 
"displays" : lambda : config_loader . load_by_type ( ComponentType . DISPLAY ) , 
"datafeeds" : lambda : config_loader . load_by_type ( ComponentType . DATA_FEED ) , 
"filters" : lambda : config_loader . load_by_type ( ComponentType . FILTER ) , 
"notifications" : lambda : config_loader . load_by_type ( ComponentType . NOTIFICATION ) 
} . items ( ) , key = lambda t : t [ 0 ] ) 
def print_ids ( creators ) : 
~~~ ids = { c . id_key_value [ 1 ] if hasattr ( c , "id_key_value" ) else c . get_id ( ) for c in creators } 
for i in sorted ( ids ) : 
~~ ~~ for k , v in component_types : 
~~~ if component_type == k or component_type == "all" : 
print_ids ( v ( ) ) 
~~ if component_type == "all" : 
~~~ click . echo ( "" ) 
~~ ~~ ~~ def parse ( self , config ) : 
if "type" not in config : 
~~ component_type = config [ "type" ] 
component_config = self . _get_config_by_id ( self . _component_configs , component_type ) 
if not component_config : 
~~~ raise ComponentNotFoundForType ( component_type ) 
~~ options = config . get ( "options" , { } ) 
component = self . _parse_item ( component_config , options , config ) 
component . name = config . get ( "name" , "" ) 
return component 
~~ def err_msg ( self , instance , value ) : 
if not hasattr ( self , "name" ) : 
~~~ return "" 
~~ return ( 
"{f_type}." . format ( 
f_type = self . field_type , 
inst = instance . __class__ . __name__ , 
attr = self . name , 
val_type = value . __class__ . __name__ , 
val = value ) ) 
~~ def exc_thrown_by_descriptor ( ) : 
traceback = sys . exc_info ( ) [ 2 ] 
tb_locals = traceback . tb_frame . f_locals 
if "self" in tb_locals : 
~~~ if not isinstance ( tb_locals [ "self" ] , Descriptor ) : 
~~~ return False 
~~ return True 
~~ return False 
~~ def create_init ( attrs ) : 
exec ( init_code , globals ( ) ) 
return _init 
~~ def create_setter ( func , attrs ) : 
def _set ( self , instance , value , name = None ) : 
~~~ args = [ getattr ( self , attr ) for attr in attrs ] 
if not func ( value , * args ) : 
~~~ raise ValueError ( self . err_msg ( instance , value ) ) 
~~ ~~ return _set 
~~ def make_class ( clsname , func , attrs ) : 
clsdict = { "__set__" : create_setter ( func , attrs ) } 
if len ( attrs ) > 0 : 
~~~ clsdict [ "__init__" ] = create_init ( attrs ) 
~~ clsobj = type ( str ( clsname ) , ( Descriptor , ) , clsdict ) 
clsobj . __doc__ = docstrings . get ( clsname ) 
return clsobj 
~~ def cycle ( self ) : 
messages = self . poll_datafeeds ( ) 
notifications = self . process_notifications ( messages ) 
self . draw_notifications ( notifications ) 
~~ def try_convert ( value ) : 
convertible = ForceNumeric . is_convertible ( value ) 
if not convertible or isinstance ( value , bool ) : 
~~~ raise ValueError 
~~ if isinstance ( str ( value ) , str ) : 
~~~ return ForceNumeric . str_to_num ( value ) 
~~ return float ( value ) 
~~ def str_to_num ( str_value ) : 
str_value = str ( str_value ) 
~~~ return int ( str_value ) 
~~~ return float ( str_value ) 
~~ ~~ def working_directory ( path ) : 
prev_dir = os . getcwd ( ) 
os . chdir ( str ( path ) ) 
~~~ yield 
~~ finally : 
~~~ os . chdir ( prev_dir ) 
~~ ~~ def strip_prefix ( s , prefix , strict = False ) : 
if s . startswith ( prefix ) : 
~~~ return s [ len ( prefix ) : ] 
~~ elif strict : 
~~ return s 
~~ def strip_suffix ( s , suffix , strict = False ) : 
if s . endswith ( suffix ) : 
~~~ return s [ : len ( s ) - len ( suffix ) ] 
~~ def is_subsequence ( needle , haystack ) : 
it = iter ( haystack ) 
for element in needle : 
~~~ if element not in it : 
~~ ~~ return True 
~~ def search ( self ) : 
search = self . _method ( inputs = self . _inputs , function = self . _function , 
state = self . _state ) 
search . run ( ) 
~~ def execute ( option ) : 
namelist_option = [ ] 
makefile_option = [ ] 
flags = "" 
for entry in option : 
~~~ key = entry . keys ( ) [ 0 ] 
~~~ namelist_option . append ( { "SIZE" : entry [ key ] } ) 
~~ elif key == "F90" : 
~~~ makefile_option . append ( entry ) 
~~ ~~ makefile_option . append ( { "F90FLAGS" : flags } ) 
namelist = create_input ( namelist_option , "namelist" , 
template_location = "templates" ) 
makefile_include = create_input ( makefile_option , "Makefile.include" , 
benchmark_base = "shallow" 
location = benchmark_base + "/original/namelist" 
my_file = open ( location , 'w' ) 
my_file . write ( namelist ) 
my_file . flush ( ) 
location = benchmark_base + "/common/Makefile.include" 
my_file . write ( makefile_include ) 
base_path = benchmark_base + "/original" 
import subprocess 
make_process = subprocess . Popen ( [ "make" , "clean" ] , cwd = base_path , 
stderr = subprocess . PIPE , 
stdout = subprocess . PIPE ) 
if make_process . wait ( ) != 0 : 
~~~ return False , [ ] 
~~ make_process = subprocess . Popen ( [ "make" ] , cwd = base_path , 
~~ make_process = subprocess . Popen ( [ "./shallow_base" ] , cwd = base_path , 
~~ stdout = make_process . stdout . read ( ) 
for line in stdout . split ( "\\n" ) : 
~~~ if "Time-stepping" in line : 
~~~ total_time = line . split ( ) [ 2 ] 
~~ ~~ return True , total_time 
for tag in self . soup . findAll ( 'span' ) : 
~~~ self . create_italic ( tag ) 
self . create_strong ( tag ) 
self . create_underline ( tag ) 
self . unwrap_span ( tag ) 
~~ for tag in self . soup . findAll ( 'a' ) : 
~~~ self . remove_comments ( tag ) 
self . check_next ( tag ) 
~~ if self . soup . body : 
~~~ for tag in self . soup . body . findAll ( ) : 
~~~ self . remove_empty ( tag ) 
self . remove_inline_comment ( tag ) 
self . parse_attrs ( tag ) 
for token , target in self . tokens : 
~~~ self . find_token ( tag , token , target ) 
~~ self . remove_blacklisted_tags ( tag ) 
~~ ~~ ~~ def check_next ( self , tag ) : 
if ( type ( tag . next_sibling ) == element . Tag and 
tag . next_sibling . name == 'a' ) : 
~~~ next_tag = tag . next_sibling 
if tag . get ( 'href' ) and next_tag . get ( 'href' ) : 
~~~ href = self . _parse_href ( tag . get ( 'href' ) ) 
next_href = self . _parse_href ( next_tag . get ( 'href' ) ) 
if href == next_href : 
~~~ next_text = next_tag . get_text ( ) 
tag . append ( next_text ) 
self . tags_blacklist . append ( next_tag ) 
~~ ~~ ~~ ~~ def create_italic ( self , tag ) : 
style = tag . get ( 'style' ) 
if style and 'font-style:italic' in style : 
~~~ tag . wrap ( self . soup . new_tag ( 'em' ) ) 
~~ ~~ def create_strong ( self , tag ) : 
if ( style and 
( 'font-weight:bold' in style or 'font-weight:700' in style ) ) : 
~~~ tag . wrap ( self . soup . new_tag ( 'strong' ) ) 
~~ ~~ def create_underline ( self , tag ) : 
if style and 'text-decoration:underline' in style : 
~~~ tag . wrap ( self . soup . new_tag ( 'u' ) ) 
~~ ~~ def parse_attrs ( self , tag ) : 
if tag . name in ATTR_WHITELIST . keys ( ) : 
~~~ attrs = copy ( tag . attrs ) 
for attr , value in attrs . items ( ) : 
~~~ if attr in ATTR_WHITELIST [ tag . name ] : 
~~~ tag . attrs [ attr ] = self . _parse_attr ( tag . name , attr , value ) 
~~~ del tag . attrs [ attr ] 
~~~ tag . attrs = { } 
~~ ~~ def remove_empty ( self , tag ) : 
has_children = len ( tag . contents ) 
has_text = len ( list ( tag . stripped_strings ) ) 
if not has_children and not has_text and not tag . is_empty_element : 
~~~ tag . extract ( ) 
~~ ~~ def clean_linebreaks ( self , tag ) : 
stripped = tag . decode ( formatter = None ) 
stripped = re . sub ( '\\n' , '' , stripped ) 
return stripped 
~~ def _parse_href ( self , href ) : 
params = parse_qs ( urlsplit ( href ) . query ) 
return params . get ( 'q' ) 
~~ def _parse_attr ( self , tagname , attr , value ) : 
if tagname == 'a' and attr == 'href' : 
~~~ return self . _parse_href ( value ) 
~~ ~~ def _check_inputs ( self ) : 
~~~ _ = self . _inputs [ 0 ] 
~~ except TypeError : 
~~~ raise RuntimeError ( 
"\ . format ( type ( self . _inputs ) , str ( self . _inputs ) ) ) 
~~ from melody . inputs import Input 
for check_input in self . _inputs : 
~~~ if not isinstance ( check_input , Input ) : 
str ( check_input ) ) ) 
~~ ~~ ~~ def _check_function ( self ) : 
if not callable ( self . _function ) : 
format ( str ( self . _function ) ) ) 
~~ from inspect import getargspec 
arg_info = getargspec ( self . _function ) 
if len ( arg_info . args ) != 1 : 
~~~ print str ( arg_info ) 
raise RuntimeError ( 
"{0}" . format ( len ( arg_info . args ) ) ) 
~~ ~~ def _recurse ( self , inputs , output ) : 
if inputs : 
~~~ my_input = inputs [ 0 ] 
name = my_input . name 
if my_input . state : 
~~~ my_options = my_input . options ( self . state ) 
~~~ my_options = my_input . options 
~~ for option in my_options : 
~~~ my_output = list ( output ) 
my_output . append ( { name : option } ) 
self . _recurse ( inputs [ 1 : ] , my_output ) 
~~~ valid , result = self . _function ( output ) 
~~ print output , valid , result 
~~ ~~ def create_input ( option , template_name , template_location = "template" ) : 
jinja2_input = { } 
for item in option : 
~~~ jinja2_input . update ( item ) 
~~ ~~ import jinja2 
~~~ template_loader = jinja2 . FileSystemLoader ( searchpath = template_location ) 
template_env = jinja2 . Environment ( loader = template_loader ) 
template = template_env . get_template ( template_name ) 
output_text = template . render ( jinja2_input ) 
~~ except jinja2 . TemplateNotFound : 
~~ return output_text 
~~ def _recurse ( self , inputs , output , depth , max_depth ) : 
if depth < max_depth : 
~~~ for index , option in enumerate ( inputs ) : 
my_output . append ( option ) 
self . _recurse ( inputs [ index + 1 : ] , my_output , depth + 1 , 
max_depth ) 
~~~ self . _options . append ( output ) 
~~ ~~ def launch ( option ) : 
from melody . inputs import create_input 
_ = create_input ( option , template_name = "input.mdp" ) 
success = True 
results = None 
if success : 
~~~ results = { "rate" : { "value" : 35 , "units" : "ns/day" } , } 
~~ return success , results 
~~ def options ( self , my_psy ) : 
my_options = [ ] 
invokes = my_psy . invokes . invoke_list 
if self . _dependent_invokes : 
"implemented" ) 
~~~ for idx , invoke in enumerate ( invokes ) : 
for loop in invoke . schedule . loops ( ) : 
~~~ if loop . loop_type == "outer" : 
~~~ siblings = loop . parent . children 
my_index = siblings . index ( loop ) 
option = [ ] 
self . _recurse ( siblings , my_index , option , my_options , 
invoke ) 
~~ ~~ ~~ ~~ return my_options 
~~ def transform ( geom , to_sref ) : 
~~~ geom = getattr ( geom , 'polygon' , Envelope ( geom ) . polygon ) 
~~ except ( TypeError , ValueError ) : 
~~~ geom . AssignSpatialReference ( to_sref ) 
~~~ geom_sref = geom . GetSpatialReference ( ) 
~~ except AttributeError : 
~~~ return transform ( Geometry ( geom ) , to_sref ) 
~~ if geom_sref is None : 
~~ if not geom_sref . IsSame ( to_sref ) : 
~~~ geom = geom . Clone ( ) 
geom . TransformTo ( to_sref ) 
~~ return geom 
~~ def Geometry ( * args , ** kwargs ) : 
arg = kwargs . pop ( 'geojson' , None ) or len ( args ) and args [ 0 ] 
~~~ srs = kwargs . pop ( 'srs' , None ) or arg . srs . wkt 
~~~ srs = SpatialReference ( 4326 ) 
~~ if hasattr ( arg , 'keys' ) : 
~~~ geom = ogr . CreateGeometryFromJson ( json . dumps ( arg ) ) 
~~ elif hasattr ( arg , 'startswith' ) : 
i = char if isinstance ( char , int ) else ord ( char ) 
if i in ( 0 , 1 ) : 
~~~ geom = ogr . CreateGeometryFromWkb ( arg ) 
~~ elif arg . startswith ( '{' ) : 
~~~ geom = ogr . CreateGeometryFromJson ( arg ) 
~~ elif arg . startswith ( '<gml' ) : 
~~~ geom = ogr . CreateGeometryFromGML ( arg ) 
~~ ~~ elif hasattr ( arg , 'wkb' ) : 
~~~ geom = ogr . CreateGeometryFromWkb ( bytes ( arg . wkb ) ) 
~~~ geom = ogr . Geometry ( * args , ** kwargs ) 
~~ if geom : 
~~~ if not isinstance ( srs , SpatialReference ) : 
~~~ srs = SpatialReference ( srs ) 
~~ geom . AssignSpatialReference ( srs ) 
~~ def centroid ( self ) : 
return self . min_x + self . width * 0.5 , self . min_y + self . height * 0.5 
~~ def expand ( self , other ) : 
if len ( other ) == 2 : 
~~~ other += other 
~~ mid = len ( other ) // 2 
self . ll = map ( min , self . ll , other [ : mid ] ) 
self . ur = map ( max , self . ur , other [ mid : ] ) 
~~ def intersect ( self , other ) : 
inter = Envelope ( tuple ( self ) ) 
if inter . intersects ( other ) : 
~~~ mid = len ( other ) // 2 
inter . ll = map ( max , inter . ll , other [ : mid ] ) 
inter . ur = map ( min , inter . ur , other [ mid : ] ) 
~~~ inter . ll = ( 0 , 0 ) 
inter . ur = ( 0 , 0 ) 
~~ return inter 
~~ def intersects ( self , other ) : 
~~~ return ( self . min_x <= other . max_x and 
self . max_x >= other . min_x and 
self . min_y <= other . max_y and 
self . max_y >= other . min_y ) 
~~~ return self . intersects ( Envelope ( other ) ) 
~~ ~~ def scale ( self , xfactor , yfactor = None ) : 
yfactor = xfactor if yfactor is None else yfactor 
x , y = self . centroid 
xshift = self . width * xfactor * 0.5 
yshift = self . height * yfactor * 0.5 
return Envelope ( x - xshift , y - yshift , x + xshift , y + yshift ) 
~~ def polygon ( self ) : 
ring = ogr . Geometry ( ogr . wkbLinearRing ) 
for coord in self . ll , self . lr , self . ur , self . ul , self . ll : 
~~~ ring . AddPoint_2D ( * coord ) 
~~ polyg = ogr . Geometry ( ogr . wkbPolygon ) 
polyg . AddGeometryDirectly ( ring ) 
return polyg 
~~ def from_bbox ( bbox , zlevs ) : 
env = Envelope ( bbox ) 
for z in zlevs : 
~~~ corners = [ to_tile ( * coord + ( z , ) ) for coord in ( env . ul , env . lr ) ] 
xs , ys = [ range ( p1 , p2 + 1 ) for p1 , p2 in zip ( * corners ) ] 
for coord in itertools . product ( xs , ys , ( z , ) ) : 
~~~ yield coord 
~~ ~~ ~~ def to_lonlat ( xtile , ytile , zoom ) : 
n = 2.0 ** zoom 
lon = xtile / n * 360.0 - 180.0 
~~~ lat_rad = math . atan ( math . sinh ( math . pi * ( 1 - 2 * ytile / n ) ) ) 
~~ except OverflowError : 
~~ lat = math . degrees ( lat_rad ) 
return lon , lat 
~~ def to_tile ( lon , lat , zoom ) : 
lat_rad = math . radians ( lat ) 
xtile = int ( ( lon + 180.0 ) / 360.0 * n ) 
ytile = int ( ( 1.0 - math . log ( math . tan ( lat_rad ) + 
( 1 / math . cos ( lat_rad ) ) ) / math . pi ) / 2.0 * n ) 
return xtile , ytile 
~~ def vsiprefix ( path ) : 
vpath = path . lower ( ) 
scheme = VSI_SCHEMES . get ( urlparse ( vpath ) . scheme , '' ) 
for ext in VSI_TYPES : 
~~~ if ext in vpath : 
~~~ filesys = VSI_TYPES [ ext ] 
break 
~~~ filesys = '' 
~~ if filesys and scheme : 
~~~ filesys = filesys [ : - 1 ] 
~~ return '' . join ( ( filesys , scheme , path ) ) 
~~ def srid ( self ) : 
epsg_id = ( self . GetAuthorityCode ( 'PROJCS' ) or 
self . GetAuthorityCode ( 'GEOGCS' ) ) 
~~~ return int ( epsg_id ) 
~~ ~~ def update_file ( url , filename ) : 
resp = urlopen ( url ) 
if resp . code != 200 : 
~~ with open ( _get_package_path ( filename ) , 'w' ) as fp : 
~~~ for l in resp : 
~~~ if not l . startswith ( b'#' ) : 
~~~ fp . write ( l . decode ( 'utf8' ) ) 
~~ def available_drivers ( ) : 
drivers = { } 
for i in range ( gdal . GetDriverCount ( ) ) : 
~~~ d = gdal . GetDriver ( i ) 
drivers [ d . ShortName ] = d . GetMetadata ( ) 
~~ return drivers 
~~ def driver_for_path ( path , drivers = None ) : 
ext = ( os . path . splitext ( path ) [ 1 ] [ 1 : ] or path ) . lower ( ) 
drivers = drivers or ImageDriver . registry if ext else { } 
for name , meta in drivers . items ( ) : 
~~~ if ext == meta . get ( 'DMD_EXTENSION' , '' ) . lower ( ) : 
~~~ return ImageDriver ( name ) 
~~ def geom_to_array ( geom , size , affine ) : 
driver = ImageDriver ( 'MEM' ) 
rast = driver . raster ( driver . ShortName , size ) 
rast . affine = affine 
rast . sref = geom . GetSpatialReference ( ) 
with MemoryLayer . from_records ( [ ( 1 , geom ) ] ) as ml : 
~~~ status = gdal . RasterizeLayer ( rast . ds , ( 1 , ) , ml . layer , burn_values = ( 1 , ) ) 
~~ arr = rast . array ( ) 
rast . close ( ) 
return arr 
~~ def rasterize ( layer , rast ) : 
r2 = driver . raster ( driver . ShortName , rast . size ) 
r2 . affine = rast . affine 
sref = rast . sref 
if not sref . srid : 
~~~ sref = SpatialReference ( 4326 ) 
~~ r2 . sref = sref 
ml = MemoryLayer ( sref , layer . GetGeomType ( ) ) 
ml . load ( layer ) 
status = gdal . RasterizeLayer ( 
r2 . ds , ( 1 , ) , ml . layer , options = [ 'ATTRIBUTE=%s' % ml . id ] ) 
ml . close ( ) 
return r2 
~~ def open ( path , mode = gdalconst . GA_ReadOnly ) : 
path = getattr ( path , 'name' , path ) 
~~~ return Raster ( vsiprefix ( path ) , mode ) 
~~~ imgdata = path . read ( ) 
~~~ imgio = MemFileIO ( delete = False ) 
gdal . FileFromMemBuffer ( imgio . name , imgdata ) 
return Raster ( imgio , mode ) 
~~ ~~ raise ValueError ( \ % path ) 
~~ def frombytes ( data , size , bandtype = gdal . GDT_Byte ) : 
r = ImageDriver ( 'MEM' ) . raster ( '' , size , bandtype ) 
r . frombytes ( data ) 
return r 
~~ def project ( self , coords ) : 
geotransform = self . tuple 
for x , y in coords : 
~~~ geo_x = geotransform [ 0 ] + geotransform [ 1 ] * x + geotransform [ 2 ] * y 
geo_y = geotransform [ 3 ] + geotransform [ 4 ] * x + geotransform [ 5 ] * y 
geo_x += geotransform [ 1 ] / 2.0 
geo_y += geotransform [ 5 ] / 2.0 
yield geo_x , geo_y 
~~ ~~ def transform ( self , coords ) : 
origin_x , origin_y = self . origin 
sx , sy = self . scale 
return [ ( int ( math . floor ( ( x - origin_x ) / sx ) ) , 
int ( math . floor ( ( y - origin_y ) / sy ) ) ) 
for x , y in coords ] 
~~ def copy ( self , source , dest ) : 
if not self . copyable : 
~~ if not isinstance ( source , Raster ) : 
~~~ source = Raster ( source ) 
should_close = True 
~~~ should_close = False 
~~ if source . name == dest : 
~~~ raise ValueError ( 
~~ settings = driverdict_tolist ( self . settings ) 
ds = self . CreateCopy ( dest , source . ds , self . strictmode , 
options = settings ) 
if should_close : 
~~~ source . close ( ) 
~~ return Raster ( ds ) 
~~ def Create ( self , * args , ** kwargs ) : 
if not self . writable : 
~~ options = kwargs . pop ( 'options' , { } ) 
kwargs [ 'options' ] = driverdict_tolist ( options or self . settings ) 
return self . _driver . Create ( * args , ** kwargs ) 
~~ def options ( self ) : 
if self . _options is None : 
~~~ elem = ET . fromstring ( 
self . info . get ( 'DMD_CREATIONOPTIONLIST' , '' ) ) 
~~ except ET . ParseError : 
~~~ elem = [ ] 
~~ opts = { } 
for child in elem : 
~~~ choices = [ val . text for val in child ] 
if choices : 
~~~ child . attrib . update ( choices = choices ) 
~~ opts [ child . attrib . pop ( 'name' ) ] = child . attrib 
~~ self . _options = opts 
~~ return self . _options 
~~ def raster ( self , path , size , bandtype = gdal . GDT_Byte ) : 
~~~ is_multiband = len ( size ) > 2 
nx , ny , nbands = size if is_multiband else size + ( 1 , ) 
~~ except ( TypeError , ValueError ) as exc : 
raise 
~~ if nx < 1 or ny < 1 : 
~~ if not self . _is_empty ( path ) : 
~~ ds = self . Create ( path , nx , ny , nbands , bandtype ) 
if not ds : 
~~ def SetGeoTransform ( self , affine ) : 
if isinstance ( affine , collections . Sequence ) : 
~~~ affine = AffineTransform ( * affine ) 
~~ self . _affine = affine 
self . ds . SetGeoTransform ( affine ) 
~~ def array ( self , envelope = ( ) ) : 
args = ( ) 
if envelope : 
~~~ args = self . get_offset ( envelope ) 
~~ return self . ds . ReadAsArray ( * args ) 
~~ def envelope ( self ) : 
if self . _envelope is None : 
~~~ origin = self . affine . origin 
ur_x = origin [ 0 ] + self . ds . RasterXSize * self . affine . scale [ 0 ] 
ll_y = origin [ 1 ] + self . ds . RasterYSize * self . affine . scale [ 1 ] 
self . _envelope = Envelope ( origin [ 0 ] , ll_y , ur_x , origin [ 1 ] ) 
~~ return self . _envelope 
~~ def get_offset ( self , envelope ) : 
if isinstance ( envelope , collections . Sequence ) : 
~~~ envelope = Envelope ( envelope ) 
~~ if not ( self . envelope . contains ( envelope ) or 
self . envelope . intersects ( envelope ) ) : 
~~ coords = self . affine . transform ( ( envelope . ul , envelope . lr ) ) 
nxy = [ ( min ( dest , size ) - origin ) or 1 
for size , origin , dest in zip ( self . size , * coords ) ] 
return coords [ 0 ] + tuple ( nxy ) 
~~ def driver ( self ) : 
if self . _driver is None : 
~~~ self . _driver = ImageDriver ( self . ds . GetDriver ( ) ) 
~~ return self . _driver 
~~ def new ( self , size = ( ) , affine = None ) : 
size = size or self . size + ( len ( self ) , ) 
band = self . ds . GetRasterBand ( 1 ) 
rcopy = driver . raster ( driver . ShortName , size , band . DataType ) 
rcopy . sref = self . GetProjection ( ) 
rcopy . affine = affine or tuple ( self . affine ) 
colors = band . GetColorTable ( ) 
for outband in rcopy : 
~~~ if self . nodata is not None : 
~~~ outband . SetNoDataValue ( self . nodata ) 
~~ if colors : 
~~~ outband . SetColorTable ( colors ) 
~~ ~~ return rcopy 
~~ def masked_array ( self , geometry = None ) : 
if geometry is None : 
~~~ return self . _masked_array ( ) 
~~ geom = transform ( geometry , self . sref ) 
env = Envelope . from_geom ( geom ) . intersect ( self . envelope ) 
arr = self . _masked_array ( env ) 
if geom . GetGeometryType ( ) != ogr . wkbPoint : 
~~~ dims = self . get_offset ( env ) [ 2 : ] 
affine = AffineTransform ( * tuple ( self . affine ) ) 
affine . origin = env . ul 
mask = ~ np . ma . make_mask ( geom_to_array ( geom , dims , affine ) ) 
arr . mask = arr . mask | mask 
~~ return arr 
~~ def nodata ( self ) : 
if self . _nodata is None : 
~~~ self . _nodata = self [ 0 ] . GetNoDataValue ( ) 
~~ return self . _nodata 
~~ def ReadRaster ( self , * args , ** kwargs ) : 
args = args or ( 0 , 0 , self . ds . RasterXSize , self . ds . RasterYSize ) 
return self . ds . ReadRaster ( * args , ** kwargs ) 
~~ def resample ( self , size , interpolation = gdalconst . GRA_NearestNeighbour ) : 
factors = ( size [ 0 ] / float ( self . RasterXSize ) , 
size [ 1 ] / float ( self . RasterYSize ) ) 
affine . scale = ( affine . scale [ 0 ] / factors [ 0 ] , 
affine . scale [ 1 ] / factors [ 1 ] ) 
dest = self . new ( size , affine ) 
gdal . ReprojectImage ( self . ds , dest . ds , None , None , interpolation ) 
return dest 
~~ def save ( self , to , driver = None ) : 
path = getattr ( to , 'name' , to ) 
if not driver and hasattr ( path , 'encode' ) : 
~~~ driver = driver_for_path ( path , self . driver . filter_copyable ( ) ) 
~~ elif hasattr ( driver , 'encode' ) : 
~~~ driver = ImageDriver ( driver ) 
~~ if driver is None or not driver . copyable : 
~~ driver . copy ( self , path ) . close ( ) 
~~ def SetProjection ( self , sref ) : 
if not hasattr ( sref , 'ExportToWkt' ) : 
~~~ sref = SpatialReference ( sref ) 
~~ self . _sref = sref 
self . ds . SetProjection ( sref . ExportToWkt ( ) ) 
~~ def shape ( self ) : 
shp = ( self . ds . RasterYSize , self . ds . RasterXSize , self . ds . RasterCount ) 
return shp [ : 2 ] if shp [ 2 ] <= 1 else shp 
~~ def warp ( self , to_sref , dest = None , interpolation = gdalconst . GRA_NearestNeighbour ) : 
if not hasattr ( to_sref , 'ExportToWkt' ) : 
~~~ to_sref = SpatialReference ( to_sref ) 
~~ dest_wkt = to_sref . ExportToWkt ( ) 
dtype = self [ 0 ] . DataType 
err_thresh = 0.125 
vrt = gdal . AutoCreateWarpedVRT ( self . ds , None , dest_wkt , 
interpolation , err_thresh ) 
if vrt is None : 
~~ warpsize = ( vrt . RasterXSize , vrt . RasterYSize , len ( self ) ) 
warptrans = vrt . GetGeoTransform ( ) 
vrt = None 
if dest is None : 
~~~ imgio = MemFileIO ( ) 
rwarp = self . driver . raster ( imgio , warpsize , dtype ) 
imgio . close ( ) 
~~~ rwarp = self . driver . raster ( dest , warpsize , dtype ) 
~~ rwarp . SetGeoTransform ( warptrans ) 
rwarp . SetProjection ( to_sref ) 
if self . nodata is not None : 
~~~ for band in rwarp : 
~~~ band . SetNoDataValue ( self . nodata ) 
band = None 
~~ ~~ gdal . ReprojectImage ( self . ds , rwarp . ds , None , None , interpolation ) 
return rwarp 
~~ def memoize ( func ) : 
cache = { } 
@ wraps ( func ) 
def inner ( filename ) : 
~~~ if filename not in cache : 
~~~ cache [ filename ] = func ( filename ) 
~~ return cache [ filename ] 
~~ return inner 
~~ def _regexp ( filename ) : 
lines = _get_resource_content ( filename ) . decode ( 'utf-8' ) . splitlines ( ) 
return re . compile ( '|' . join ( lines ) ) 
~~ def normalize_date_format ( date ) : 
if isinstance ( date , int ) : 
time . localtime ( date ) ) 
~~ if isinstance ( date , str ) or isinstance ( date , unicode ) : 
~~~ date = dateutil . parser . parse ( date ) 
~~ if not date . tzinfo : 
~~~ local_tz = pytz . timezone ( _detect_timezone ( ) ) 
local_dt = local_tz . localize ( date , is_dst = None ) 
date = local_dt . astimezone ( pytz . utc ) + pd . datetools . day 
~~ return date 
~~ def _detect_timezone ( ) : 
default_timezone = 'America/New_York' 
locale_code = locale . getdefaultlocale ( ) 
return default_timezone if not locale_code [ 0 ] else str ( pytz . country_timezones [ locale_code [ 0 ] [ - 2 : ] ] [ 0 ] ) 
~~ def api_url ( full_version , resource ) : 
return '/v{}/{}' . format ( dna . utils . Version ( full_version ) . major , resource ) 
~~ def api_doc ( full_version , resource , method = 'GET' , ** kwargs ) : 
params = '&' . join ( [ '{}={}' . format ( k , v ) for k , v in kwargs . iteritems ( ) ] ) 
if params : 
~~~ doc = '?' . join ( [ doc , params ] ) 
~~ return doc 
~~ def activate_pdb_hook ( ) : 
def debug_exception ( type_exception , value , tb ) : 
~~~ import pdb 
pdb . post_mortem ( tb ) 
~~ import sys 
sys . excepthook = debug_exception 
~~ def emphasis ( obj , align = True ) : 
if isinstance ( obj , dict ) : 
~~~ if align : 
~~~ pretty_msg = os . linesep . join ( 
~~~ pretty_msg = json . dumps ( obj , indent = 4 , sort_keys = True ) 
~~~ return obj 
~~ return pretty_msg 
~~ def _send_message ( self , msg ) : 
LWLink . the_queue . put_nowait ( msg ) 
if LWLink . thread is None or not LWLink . thread . isAlive ( ) : 
~~~ LWLink . thread = Thread ( target = self . _send_queue ) 
LWLink . thread . start ( ) 
~~ ~~ def turn_on_light ( self , device_id , name ) : 
self . _send_message ( msg ) 
~~ def turn_on_switch ( self , device_id , name ) : 
~~ def turn_on_with_brightness ( self , device_id , name , brightness ) : 
brightness_value = round ( ( brightness * 31 ) / 255 ) + 1 
device_id , brightness_value , brightness_value , name ) 
~~ def turn_off ( self , device_id , name ) : 
~~ def _send_queue ( self ) : 
while not LWLink . the_queue . empty ( ) : 
~~~ self . _send_reliable_message ( LWLink . the_queue . get_nowait ( ) ) 
~~ ~~ def _send_reliable_message ( self , msg ) : 
result = False 
max_retries = 15 
trans_id = next ( LWLink . transaction_id ) 
msg = "%d,%s" % ( trans_id , msg ) 
~~~ with socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as write_sock , socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) as read_sock : 
~~~ write_sock . setsockopt ( 
socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) 
read_sock . setsockopt ( socket . SOL_SOCKET , 
socket . SO_BROADCAST , 1 ) 
read_sock . settimeout ( self . SOCKET_TIMEOUT ) 
read_sock . bind ( ( '0.0.0.0' , self . RX_PORT ) ) 
while max_retries : 
~~~ max_retries -= 1 
write_sock . sendto ( msg . encode ( 
'UTF-8' ) , ( LWLink . link_ip , self . TX_PORT ) ) 
~~~ response , dummy = read_sock . recvfrom ( 1024 ) 
response = response . decode ( 'UTF-8' ) 
self . register ( ) 
result = True 
~~ if response . startswith ( "%d,OK" % trans_id ) : 
~~~ result = True 
~~ if response . startswith ( "%d,ERR" % trans_id ) : 
~~~ _LOGGER . error ( response ) 
~~ _LOGGER . info ( response ) 
~~ if result : 
~~ time . sleep ( 0.25 ) 
~~ ~~ ~~ except socket . timeout : 
return result 
~~ except Exception as ex : 
~~~ _LOGGER . error ( ex ) 
~~ return result 
~~ def create_adapter ( cmph , ffi , obj ) : 
if is_file_location ( obj ) : 
~~~ fd = open ( obj ) 
adapter = cmph . cmph_io_nlfile_adapter ( fd ) 
def dtor ( ) : 
~~~ cmph . cmph_io_nlfile_adapter_destroy ( adapter ) 
fd . close ( ) 
~~ return _AdapterCxt ( adapter , dtor ) 
~~ elif is_file ( obj ) : 
~~~ adapter = cmph . cmph_io_nlfile_adapter ( obj ) 
dtor = lambda : cmph . cmph_io_nlfile_adapter_destroy ( adapter ) 
return _AdapterCxt ( adapter , dtor ) 
~~ elif isinstance ( obj , Sequence ) : 
~~~ if len ( obj ) == 0 : 
~~ return _create_pyobj_adapter ( cmph , ffi , obj ) 
~~ ~~ def generate_hash ( data , algorithm = 'chd_ph' , hash_fns = ( ) , chd_keys_per_bin = 1 , 
chd_load_factor = None , fch_bits_per_key = None , 
num_graph_vertices = None , brz_memory_size = 8 , 
brz_temp_dir = None , brz_max_keys_per_bucket = 128 , 
bdz_precomputed_rank = 7 , chd_avg_keys_per_bucket = 4 ) : 
cfg = _cfg ( algorithm , hash_fns , chd_keys_per_bin , chd_load_factor , 
fch_bits_per_key , num_graph_vertices , brz_memory_size , 
brz_temp_dir , brz_max_keys_per_bucket , bdz_precomputed_rank , 
chd_avg_keys_per_bucket ) 
with create_adapter ( _cmph , ffi , data ) as source : 
~~~ with _create_config ( source , cfg ) as config : 
~~~ _mph = _cmph . cmph_new ( config ) 
if not _mph : 
~~ return MPH ( _mph ) 
~~ ~~ ~~ def load_hash ( existing_mph ) : 
if is_file_location ( existing_mph ) : 
~~~ with open ( abspath ( existing_mph ) ) as hash_table : 
~~~ _mph = _cmph . cmph_load ( hash_table ) 
~~ ~~ elif is_file ( existing_mph ) : 
~~~ _mph = _cmph . cmph_load ( existing_mph ) 
~~ if not _mph : 
~~ def save ( self , output ) : 
if isinstance ( output , six . string_types ) : 
~~~ with open ( abspath ( output ) , 'w' ) as out : 
~~~ _cmph . cmph_dump ( self . _mph , out ) 
~~~ _cmph . cmph_dump ( self . _mph , output ) 
~~ ~~ def lookup ( self , key ) : 
assert self . _mph 
key = convert_to_bytes ( key ) 
box = ffi . new ( 'char[]' , key ) 
~~~ result = _cmph . cmph_search ( self . _mph , box , len ( key ) ) 
~~~ del box 
~~ ~~ def update_ ( self , sct_dict , conf_arg = True ) : 
for opt , val in sct_dict . items ( ) : 
~~~ if opt not in self . def_ : 
~~ if not conf_arg or self . def_ [ opt ] . conf_arg : 
~~~ self [ opt ] = val 
~~ ~~ ~~ def reset_ ( self ) : 
for opt , meta in self . defaults_ ( ) : 
~~~ self [ opt ] = meta . default 
~~ ~~ def from_dict_ ( cls , conf_dict ) : 
return cls ( ** { name : Section ( ** opts ) 
for name , opts in conf_dict . items ( ) } ) 
~~ def set_config_files_ ( self , * config_files ) : 
self . _config_files = tuple ( pathlib . Path ( path ) for path in config_files ) 
~~ def opt_vals_ ( self ) : 
for sct , opt in self . options_ ( ) : 
~~~ yield sct , opt , self [ sct ] [ opt ] 
~~ ~~ def defaults_ ( self ) : 
~~~ yield sct , opt , self [ sct ] . def_ [ opt ] 
~~ ~~ def create_config_ ( self , index = 0 , update = False ) : 
if not self . config_files_ [ index : ] : 
~~ path = self . config_files_ [ index ] 
if not path . parent . exists ( ) : 
~~~ path . parent . mkdir ( parents = True ) 
~~ conf_dict = { } 
for section in self . sections_ ( ) : 
~~~ conf_opts = [ o for o , m in self [ section ] . defaults_ ( ) if m . conf_arg ] 
if not conf_opts : 
~~ conf_dict [ section ] = { } 
for opt in conf_opts : 
~~~ conf_dict [ section ] [ opt ] = ( self [ section ] [ opt ] if update else 
self [ section ] . def_ [ opt ] . default ) 
~~ ~~ with path . open ( 'w' ) as cfile : 
~~~ toml . dump ( conf_dict , cfile ) 
~~ ~~ def update_ ( self , conf_dict , conf_arg = True ) : 
for section , secdict in conf_dict . items ( ) : 
~~~ self [ section ] . update_ ( secdict , conf_arg ) 
~~ ~~ def read_config_ ( self , cfile ) : 
if not cfile . exists ( ) : 
~~~ return { } 
~~~ conf_dict = toml . load ( str ( cfile ) ) 
~~ except toml . TomlDecodeError : 
~~ self . update_ ( conf_dict ) 
return conf_dict 
~~ def read_configs_ ( self ) : 
if not self . config_files_ : 
~~~ return { } , [ ] , [ ] 
~~ content = { section : { } for section in self } 
empty_files = [ ] 
faulty_files = [ ] 
for cfile in self . config_files_ : 
~~~ conf_dict = self . read_config_ ( cfile ) 
if conf_dict is None : 
~~~ faulty_files . append ( cfile ) 
~~ elif not conf_dict : 
~~~ empty_files . append ( cfile ) 
~~ for section , secdict in conf_dict . items ( ) : 
~~~ content [ section ] . update ( secdict ) 
~~ ~~ return content , empty_files , faulty_files 
~~ def _names ( section , option ) : 
meta = section . def_ [ option ] 
action = meta . cmd_kwargs . get ( 'action' ) 
if action is internal . Switch : 
~~~ names = [ '-{}' . format ( option ) , '+{}' . format ( option ) ] 
if meta . shortname is not None : 
~~~ names . append ( '-{}' . format ( meta . shortname ) ) 
names . append ( '+{}' . format ( meta . shortname ) ) 
~~~ names = [ '--{}' . format ( option ) ] 
~~ ~~ return names 
~~ def sections_list ( self , cmd = None ) : 
sections = list ( self . common . sections ) 
if not cmd : 
~~~ if self . bare is not None : 
~~~ sections . extend ( self . bare . sections ) 
return sections 
~~ return [ ] 
~~ sections . extend ( self . subcmds [ cmd ] . sections ) 
if cmd in self . _conf : 
~~~ sections . append ( cmd ) 
~~ return sections 
~~ def _cmd_opts_solver ( self , cmd_name ) : 
sections = self . sections_list ( cmd_name ) 
cmd_dict = self . _opt_cmds [ cmd_name ] if cmd_name else self . _opt_bare 
for sct in reversed ( sections ) : 
~~~ for opt , opt_meta in self . _conf [ sct ] . def_ . items ( ) : 
~~~ if not opt_meta . cmd_arg : 
~~ if opt not in cmd_dict : 
~~~ cmd_dict [ opt ] = sct 
~~~ warnings . warn ( 
cmd_name , sct , opt , cmd_dict [ opt ] ) , 
error . LoamWarning , stacklevel = 4 ) 
~~ ~~ ~~ ~~ def _add_options_to_parser ( self , opts_dict , parser ) : 
store_bool = ( 'store_true' , 'store_false' ) 
for opt , sct in opts_dict . items ( ) : 
~~~ meta = self . _conf [ sct ] . def_ [ opt ] 
kwargs = copy . deepcopy ( meta . cmd_kwargs ) 
action = kwargs . get ( 'action' ) 
~~~ kwargs . update ( nargs = 0 ) 
~~ elif meta . default is not None and action not in store_bool : 
~~~ kwargs . setdefault ( 'type' , type ( meta . default ) ) 
~~ kwargs . update ( help = meta . help ) 
kwargs . setdefault ( 'default' , self . _conf [ sct ] [ opt ] ) 
parser . add_argument ( * _names ( self . _conf [ sct ] , opt ) , ** kwargs ) 
~~ ~~ def _build_parser ( self ) : 
main_parser = argparse . ArgumentParser ( description = self . common . help , 
prefix_chars = '-+' ) 
self . _add_options_to_parser ( self . _opt_bare , main_parser ) 
main_parser . set_defaults ( ** self . common . defaults ) 
if self . bare is not None : 
~~~ main_parser . set_defaults ( ** self . bare . defaults ) 
~~ subparsers = main_parser . add_subparsers ( dest = 'loam_sub_name' ) 
for cmd_name , meta in self . subcmds . items ( ) : 
~~~ kwargs = { 'prefix_chars' : '+-' , 'help' : meta . help } 
dummy_parser = subparsers . add_parser ( cmd_name , ** kwargs ) 
self . _add_options_to_parser ( self . _opt_cmds [ cmd_name ] , dummy_parser ) 
dummy_parser . set_defaults ( ** meta . defaults ) 
~~ return main_parser 
~~ def parse_args ( self , arglist = None ) : 
args = self . _parser . parse_args ( args = arglist ) 
sub_cmd = args . loam_sub_name 
if sub_cmd is None : 
~~~ for opt , sct in self . _opt_bare . items ( ) : 
~~~ self . _conf [ sct ] [ opt ] = getattr ( args , opt , None ) 
~~~ for opt , sct in self . _opt_cmds [ sub_cmd ] . items ( ) : 
~~ ~~ return args 
~~ def _zsh_comp_command ( self , zcf , cmd , grouping , add_help = True ) : 
if add_help : 
~~~ if grouping : 
~~ print ( "\ , end = BLK , file = zcf ) 
print ( "\ , end = BLK , file = zcf ) 
~~ no_comp = ( 'store_true' , 'store_false' ) 
cmd_dict = self . _opt_cmds [ cmd ] if cmd else self . _opt_bare 
for opt , sct in cmd_dict . items ( ) : 
if meta . cmd_kwargs . get ( 'action' ) == 'append' : 
if meta . comprule is None : 
~~~ meta . comprule = '' 
~~ if meta . cmd_kwargs . get ( 'action' ) in no_comp or meta . cmd_kwargs . get ( 'nargs' ) == 0 : 
~~~ meta . comprule = None 
~~ if meta . comprule is None : 
~~~ compstr = '' 
~~ elif meta . comprule == '' : 
~~~ optfmt = optfmt . split ( '[' ) 
optfmt = optfmt [ 0 ] + '=[' + optfmt [ 1 ] 
~~ if grouping : 
~~~ print ( grpfmt . format ( opt ) , end = BLK , file = zcf ) 
~~ for name in _names ( self . _conf [ sct ] , opt ) : 
~~~ print ( optfmt . format ( name , meta . help . replace ( "\ , "\ ) , 
compstr ) , end = BLK , file = zcf ) 
~~ ~~ ~~ def zsh_complete ( self , path , cmd , * cmds , sourceable = False ) : 
grouping = internal . zsh_version ( ) >= ( 5 , 4 ) 
path = pathlib . Path ( path ) 
firstline = [ '#compdef' , cmd ] 
firstline . extend ( cmds ) 
subcmds = list ( self . subcmds . keys ( ) ) 
with path . open ( 'w' ) as zcf : 
~~~ print ( * firstline , end = '\\n\\n' , file = zcf ) 
if subcmds : 
~~~ substrs = [ "{}\\\\:\ . format ( sub , self . subcmds [ sub ] . help ) 
for sub in subcmds ] 
end = BLK , file = zcf ) 
~~ self . _zsh_comp_command ( zcf , None , grouping ) 
~~~ print ( "\ , file = zcf ) 
for sub in subcmds : 
file = zcf ) 
~~ print ( 'esac' , file = zcf ) 
~~ print ( '}' , file = zcf ) 
print ( '_arguments' , end = BLK , file = zcf ) 
self . _zsh_comp_command ( zcf , sub , grouping ) 
print ( '}' , file = zcf ) 
~~ if sourceable : 
~~ ~~ ~~ def _bash_comp_command ( self , cmd , add_help = True ) : 
out = [ '-h' , '--help' ] if add_help else [ ] 
for opt , sct in cmd_dict : 
~~~ out . extend ( _names ( self . _conf [ sct ] , opt ) ) 
~~ return out 
~~ def bash_complete ( self , path , cmd , * cmds ) : 
with path . open ( 'w' ) as bcf : 
print ( 'COMPREPLY=()' , file = bcf ) 
print ( r\ . format ( optstr ) , end = '\\n\\n' , file = bcf ) 
file = bcf ) 
~~ for sub in subcmds : 
print ( \ . format ( sub , optstr ) , file = bcf ) 
~~ condstr = 'if' 
~~~ print ( condstr , r\ , sub , \ , 
print ( r\ , sub , 
r\ , sep = '' , file = bcf ) 
condstr = 'elif' 
print ( r\ , 
~~~ print ( r'else' , file = bcf ) 
~~ print ( 'fi' , file = bcf ) 
print ( '}' , end = '\\n\\n' , file = bcf ) 
~~ ~~ def zsh_version ( ) : 
~~ except ( FileNotFoundError , subprocess . CalledProcessError ) : 
~~~ return ( 0 , 0 ) 
~~ match = re . search ( br'[0-9]+\\.[0-9]+' , out ) 
return tuple ( map ( int , match . group ( 0 ) . split ( b'.' ) ) ) if match else ( 0 , 0 ) 
~~ def add_timestamp ( logger_class , log_method , event_dict ) : 
event_dict [ 'timestamp' ] = calendar . timegm ( time . gmtime ( ) ) 
return event_dict 
~~ def setup ( level = 'debug' , output = None ) : 
output = output or settings . LOG [ 'file' ] 
level = level . upper ( ) 
handlers = [ 
logbook . NullHandler ( ) 
if output == 'stdout' : 
~~~ handlers . append ( 
logbook . StreamHandler ( sys . stdout , 
format_string = settings . LOG [ 'format' ] , 
level = level ) ) 
logbook . FileHandler ( output , 
~~ sentry_dns = settings . LOG [ 'sentry_dns' ] 
if sentry_dns : 
~~~ handlers . append ( SentryHandler ( sentry_dns , level = 'ERROR' ) ) 
~~ return logbook . NestedSetup ( handlers ) 
~~ def logger ( name = __name__ , output = None , uuid = False , timestamp = False ) : 
processors = [ ] 
if output == 'json' : 
~~~ processors . append ( structlog . processors . JSONRenderer ( ) ) 
~~ if uuid : 
~~~ processors . append ( add_unique_id ) 
~~~ processors . append ( add_timestamp ) 
~~ return structlog . wrap_logger ( 
logbook . Logger ( name ) , 
processors = processors 
~~ def setup ( title , output = 'json' , timezone = None ) : 
timezone = timezone or dna . time_utils . _detect_timezone ( ) 
broker_url = 'redis://{}:{}/{}' . format ( 
os . environ . get ( 'BROKER_HOST' , 'localhost' ) , 
os . environ . get ( 'BROKER_PORT' , 6379 ) , 
0 
app = Celery ( title , broker = broker_url ) 
app . conf . update ( 
CELERY_TASK_SERIALIZER = output , 
CELERY_RESULT_SERIALIZER = output , 
CELERY_RESULT_BACKEND = broker_url , 
CELERY_TIMEZONE = timezone , 
CELERYD_FORCE_EXECV = True , 
CELERY_ENABLE_UTC = True , 
CELERY_IGNORE_RESULT = False 
return app 
~~ def get ( self , worker_id ) : 
code = 200 
if worker_id == 'all' : 
~~~ report = { 'workers' : [ { 
'id' : job , 
'report' : self . _inspect_worker ( job ) } 
for job in self . jobs ] 
~~ elif worker_id in self . jobs : 
~~~ report = { 
'id' : worker_id , 
'report' : self . _inspect_worker ( worker_id ) 
code = 404 
~~ return flask . jsonify ( report ) , code 
~~ def delete ( self , worker_id ) : 
if worker_id in self . jobs : 
~~~ self . jobs [ worker_id ] [ 'worker' ] . revoke ( terminate = True ) 
report = { 
'revoked' : True 
self . jobs . pop ( worker_id ) 
~~ def switch_opt ( default , shortname , help_msg ) : 
return ConfOpt ( bool ( default ) , True , shortname , 
dict ( action = internal . Switch ) , True , help_msg , None ) 
~~ def config_conf_section ( ) : 
config_dict = OrderedDict ( ( 
( 'create' , 
ConfOpt ( None , True , None , { 'action' : 'store_true' } , 
( 'create_local' , 
( 'update' , 
( 'edit' , 
( 'editor' , 
) ) 
return config_dict 
~~ def set_conf_str ( conf , optstrs ) : 
falsy = [ '0' , 'no' , 'n' , 'off' , 'false' , 'f' ] 
bool_actions = [ 'store_true' , 'store_false' , internal . Switch ] 
for optstr in optstrs : 
~~~ opt , val = optstr . split ( '=' , 1 ) 
sec , opt = opt . split ( '.' , 1 ) 
if sec not in conf : 
~~~ raise error . SectionError ( sec ) 
~~ if opt not in conf [ sec ] : 
~~~ raise error . OptionError ( opt ) 
~~ meta = conf [ sec ] . def_ [ opt ] 
if meta . default is None : 
~~~ if 'type' in meta . cmd_kwargs : 
~~~ cast = meta . cmd_kwargs [ 'type' ] 
~~~ act = meta . cmd_kwargs . get ( 'action' ) 
cast = bool if act in bool_actions else str 
~~~ cast = type ( meta . default ) 
~~ if cast is bool and val . lower ( ) in falsy : 
~~~ val = '' 
~~ conf [ sec ] [ opt ] = cast ( val ) 
~~ ~~ def config_cmd_handler ( conf , config = 'config' ) : 
if conf [ config ] . create or conf [ config ] . update : 
~~~ conf . create_config_ ( update = conf [ config ] . update ) 
~~ if conf [ config ] . create_local : 
~~~ conf . create_config_ ( index = - 1 , update = conf [ config ] . update ) 
~~ if conf [ config ] . edit : 
~~~ if not conf . config_files_ [ 0 ] . is_file ( ) : 
conf . config_files_ [ 0 ] ) ) ) 
~~ ~~ def create_complete_files ( climan , path , cmd , * cmds , zsh_sourceable = False ) : 
zsh_dir = path / 'zsh' 
if not zsh_dir . exists ( ) : 
~~~ zsh_dir . mkdir ( parents = True ) 
~~ zsh_file = zsh_dir / '_{}.sh' . format ( cmd ) 
bash_dir = path / 'bash' 
if not bash_dir . exists ( ) : 
~~~ bash_dir . mkdir ( parents = True ) 
~~ bash_file = bash_dir / '{}.sh' . format ( cmd ) 
climan . zsh_complete ( zsh_file , cmd , * cmds , sourceable = zsh_sourceable ) 
climan . bash_complete ( bash_file , cmd , * cmds ) 
~~ def check_credentials ( username , password ) : 
user = models . User . objects ( 
username = username , 
password = password 
) . first ( ) 
return user or None 
~~ def check_token ( token ) : 
user = models . User . objects ( api_key = token ) . first ( ) 
~~ def requires_basic_auth ( resource ) : 
@ functools . wraps ( resource ) 
def decorated ( * args , ** kwargs ) : 
auth = flask . request . authorization 
user = check_credentials ( auth . username , auth . password ) 
if not auth or user is None : 
return auth_failed ( ) 
flask . g . user = user 
return resource ( * args , ** kwargs ) 
~~ return decorated 
~~ def requires_token_auth ( resource ) : 
token = flask . request . headers . get ( 'Authorization' ) 
user = check_token ( token ) 
if not token or user is None : 
~~ flask . g . user = user 
~~ def is_running ( process ) : 
~~~ pgrep = sh . Command ( '/usr/bin/pgrep' ) 
pgrep ( process ) 
flag = True 
~~ except sh . ErrorReturnCode_1 : 
~~~ flag = False 
~~ return flag 
~~ def dynamic_import ( mod_path , obj_name = None ) : 
~~~ module = __import__ ( mod_path , fromlist = [ 'whatever' ] ) 
~~ except ImportError , error : 
~~~ raise errors . DynamicImportFailed ( 
module = '.' . join ( [ mod_path , obj_name ] ) , reason = error ) 
~~ reload ( module ) 
if obj_name is None : 
~~~ obj = module 
~~ elif hasattr ( module , obj_name ) : 
~~~ obj = getattr ( module , obj_name ) 
module = '.' . join ( [ mod_path , obj_name ] ) , 
format ( module . __name__ , obj_name ) ) 
return None 
~~ return obj 
~~ def self_ip ( public = False ) : 
~~~ if public : 
~~~ data = str ( urlopen ( 'http://checkip.dyndns.com/' ) . read ( ) ) 
ip_addr = re . compile ( 
~~~ sock = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) 
sock . connect ( ( 'google.com' , 0 ) ) 
ip_addr = sock . getsockname ( ) [ 0 ] 
~~ ~~ except Exception , error : 
~~ return ip_addr 
~~ def serve ( self , app_docopt = DEFAULT_DOC , description = '' ) : 
exit_status = 0 
if isinstance ( app_docopt , str ) : 
~~~ args = docopt ( app_docopt , version = description ) 
~~ elif isinstance ( app_docopt , dict ) : 
~~~ args = app_docopt 
. format ( type ( app_docopt ) ) ) 
~~ log_level = args . get ( '--log' , 'debug' ) 
is_debug = args . get ( '--debug' , False ) 
log_output = 'stdout' if is_debug else 'apy.log' 
safe_bind = args . get ( '--bind' , '127.0.0.1' ) 
safe_port = int ( args . get ( '--port' , 5000 ) ) 
log_setup = dna . logging . setup ( level = log_level , output = log_output ) 
with log_setup . applicationbound ( ) : 
version = description , 
log = log_level , 
debug = is_debug , 
bind = '{}:{}' . format ( safe_bind , safe_port ) ) 
self . app . run ( host = safe_bind , 
port = safe_port , 
debug = is_debug ) 
~~ except Exception as error : 
~~~ if is_debug : 
exit_status = 1 
~~ ~~ return exit_status 
~~ def render ( self , name , value , attrs = None ) : 
context = attrs or { } 
context . update ( { 'name' : name , 'value' : value , } ) 
return render_to_string ( self . template_name , context ) 
~~ def networkdays ( from_date , to_date , locale = 'en-US' ) : 
holidays = locales [ locale ] 
return workdays . networkdays ( from_date , to_date , holidays ) 
~~ def merge ( dicto , other ) : 
if not isinstance ( dicto , Dicto ) : 
~~~ dicto = Dicto ( dicto ) 
~~ if not isinstance ( other , Dicto ) : 
~~~ other = Dicto ( other ) 
~~ for k , v in other . __dict__ . items ( ) : 
~~~ if k in dicto and isinstance ( dicto [ k ] , Dicto ) and isinstance ( other [ k ] , Dicto ) : 
~~~ dicto [ k ] = merge ( dicto [ k ] , other [ k ] ) 
~~~ dicto [ k ] = other [ k ] 
~~ ~~ return dicto 
~~ def merge_ ( self , merge_dct ) : 
for k , v in merge_dct . items ( ) : 
~~~ if ( k in self and isinstance ( self [ k ] , dict ) and isinstance ( merge_dct [ k ] , collections . Mapping ) ) : 
~~~ self [ k ] . merge_ ( dicto ( merge_dct [ k ] ) ) 
~~~ self [ k ] = merge_dct [ k ] 
~~ ~~ return self 
~~ def relate ( self , part , id = None ) : 
assert part . name . startswith ( self . base ) 
name = part . name [ len ( self . base ) : ] . lstrip ( '/' ) 
rel = Relationship ( self , name , part . rel_type , id = id ) 
self . relationships . add ( rel ) 
return rel 
~~ def related ( self , reltype ) : 
parts = [ ] 
package = getattr ( self , 'package' , None ) or self 
for rel in self . relationships . types . get ( reltype , [ ] ) : 
~~~ parts . append ( package [ posixpath . join ( self . base , rel . target ) ] ) 
~~ return parts 
~~ def _load_rels ( self , source ) : 
self . relationships . load ( source = self , data = source ) 
~~ def add ( self , part , override = True ) : 
ct_add_method = [ 
self . content_types . add_default , 
self . content_types . add_override , 
] [ override ] 
self [ part . name ] = part 
ct_add_method ( part ) 
~~ def _load_part ( self , rel_type , name , data ) : 
if self . content_types . find_for ( name ) is None : 
return 
~~ cls = Part . classes_by_rel_type [ rel_type ] 
part = cls ( self , name ) 
part . load ( data ) 
self [ name ] = part 
return part 
~~ def get_parts_by_class ( self , cls ) : 
return ( part for part in self . parts . values ( ) if isinstance ( part , cls ) ) 
~~ def find_for ( self , name ) : 
map = self . items 
return map . get ( name , None ) or map . get ( get_ext ( name ) or None , None ) 
~~ def from_element ( cls , element ) : 
ns , class_name = parse_tag ( element . tag ) 
class_ = getattr ( ContentType , class_name ) 
if not class_ : 
raise ValueError ( msg ) 
~~ key = element . get ( class_ . key_name ) 
name = element . get ( 'ContentType' ) 
return class_ ( name , key ) 
~~ def parse ( input_string , prefix = '' ) : 
tree = parser . parse ( input_string ) 
visitor = ChatlVisitor ( prefix ) 
visit_parse_tree ( tree , visitor ) 
return visitor . parsed 
~~ def assign_force_field ( ampal_obj , ff ) : 
if hasattr ( ampal_obj , 'ligands' ) : 
~~~ atoms = ampal_obj . get_atoms ( ligands = True , inc_alt_states = True ) 
~~~ atoms = ampal_obj . get_atoms ( inc_alt_states = True ) 
~~ for atom in atoms : 
~~~ w_str = None 
a_ff_id = None 
if atom . element == 'H' : 
~~ elif atom . parent . mol_code . upper ( ) in ff : 
~~~ if atom . res_label . upper ( ) in ff [ atom . parent . mol_code ] : 
~~~ a_ff_id = ( atom . parent . mol_code . upper ( ) , 
atom . res_label . upper ( ) ) 
~~ elif atom . res_label . upper ( ) in ff [ 'WLD' ] : 
~~~ a_ff_id = ( 'WLD' , atom . res_label . upper ( ) ) 
'ignored.' ) . format ( 
atom . res_label , atom . parent . mol_code ) 
~~ ~~ elif atom . res_label . upper ( ) in ff [ 'WLD' ] : 
~~ if w_str : 
~~~ warnings . warn ( w_str , NotParameterisedWarning ) 
~~ atom . tags [ '_buff_ff_id' ] = a_ff_id 
~~ return 
~~ def find_max_rad_npnp ( self ) : 
max_rad = 0 
max_npnp = 0 
for res , _ in self . items ( ) : 
~~~ if res != 'KEY' : 
~~~ for _ , ff_params in self [ res ] . items ( ) : 
~~~ if max_rad < ff_params [ 1 ] : 
~~~ max_rad = ff_params [ 1 ] 
~~ if max_npnp < ff_params [ 4 ] : 
~~~ max_npnp = ff_params [ 4 ] 
~~ ~~ ~~ ~~ return max_rad , max_npnp 
~~ def _make_ff_params_dict ( self ) : 
~~~ ff_params_struct_dict = { } 
for res in self . keys ( ) : 
~~~ if res == 'KEY' : 
~~ if res not in ff_params_struct_dict : 
~~~ ff_params_struct_dict [ res ] = { } 
~~ for atom , params in self [ res ] . items ( ) : 
~~~ ff_params_struct_dict [ res ] [ atom ] = PyAtomData ( 
atom . encode ( ) , params [ 0 ] . encode ( ) , * params [ 1 : ] ) 
~~ ~~ ~~ except TypeError : 
~~~ raise ForceFieldParameterError ( 
~~ return ff_params_struct_dict 
~~ def save ( self , target = None ) : 
target = target or getattr ( self , 'filename' , None ) 
if isinstance ( target , six . string_types ) : 
~~~ self . filename = target 
target = open ( target , 'wb' ) 
~~ if target is None : 
~~~ msg = ( 
% self . __class__ . __name__ 
~~ self . _store ( target ) 
~~ def as_stream ( self ) : 
stream = io . BytesIO ( ) 
self . _store ( stream ) 
stream . seek ( 0 ) 
return stream 
~~ def _get_matching_segments ( self , zf , name ) : 
for n in zf . namelist ( ) : 
~~~ if n . startswith ( name ) : 
~~~ yield zf . read ( n ) 
~~ ~~ ~~ def copy_dir ( bucket_name , src_path , dest_path , 
aws_access_key_id = None , aws_secret_access_key = None , 
aws_profile = None , 
surrogate_key = None , cache_control = None , 
surrogate_control = None , 
create_directory_redirect_object = True ) : 
if not src_path . endswith ( '/' ) : 
~~~ src_path += '/' 
~~ if not dest_path . endswith ( '/' ) : 
~~~ dest_path += '/' 
~~ common_prefix = os . path . commonprefix ( [ src_path , dest_path ] ) 
if common_prefix == src_path : 
common_prefix , src_path ) 
raise RuntimeError ( msg ) 
~~ if common_prefix == dest_path : 
common_prefix , dest_path ) 
~~ delete_dir ( bucket_name , dest_path , 
aws_access_key_id , aws_secret_access_key ) 
session = boto3 . session . Session ( 
aws_access_key_id = aws_access_key_id , 
aws_secret_access_key = aws_secret_access_key , 
profile_name = aws_profile ) 
s3 = session . resource ( 's3' ) 
bucket = s3 . Bucket ( bucket_name ) 
for src_obj in bucket . objects . filter ( Prefix = src_path ) : 
~~~ src_rel_path = os . path . relpath ( src_obj . key , start = src_path ) 
dest_key_path = os . path . join ( dest_path , src_rel_path ) 
head = s3 . meta . client . head_object ( Bucket = bucket_name , 
Key = src_obj . key ) 
metadata = head [ 'Metadata' ] 
content_type = head [ 'ContentType' ] 
if cache_control is None and 'CacheControl' in head : 
~~~ cache_control = head [ 'CacheControl' ] 
~~ if surrogate_control is not None : 
~~~ metadata [ 'surrogate-control' ] = surrogate_control 
~~ if surrogate_key is not None : 
~~~ metadata [ 'surrogate-key' ] = surrogate_key 
~~ s3 . meta . client . copy_object ( 
Bucket = bucket_name , 
Key = dest_key_path , 
CopySource = { 'Bucket' : bucket_name , 'Key' : src_obj . key } , 
MetadataDirective = 'REPLACE' , 
Metadata = metadata , 
ACL = 'public-read' , 
CacheControl = cache_control , 
ContentType = content_type ) 
~~ if create_directory_redirect_object : 
~~~ dest_dirname = dest_path . rstrip ( '/' ) 
obj = bucket . Object ( dest_dirname ) 
metadata = { 'dir-redirect' : 'true' } 
obj . put ( Body = '' , 
CacheControl = cache_control ) 
~~ ~~ def open_bucket ( bucket_name , 
aws_profile = None ) : 
profile_name = aws_profile , 
aws_secret_access_key = aws_secret_access_key ) 
return bucket 
~~ def upload_dir ( bucket_name , path_prefix , source_dir , 
upload_dir_redirect_objects = True , 
surrogate_key = None , 
surrogate_control = None , cache_control = None , 
acl = None , 
logger = logging . getLogger ( __name__ ) 
bucket_name , path_prefix , source_dir ) ) 
if surrogate_key is not None : 
~~ manager = ObjectManager ( session , bucket_name , path_prefix ) 
for ( rootdir , dirnames , filenames ) in os . walk ( source_dir ) : 
~~~ bucket_root = os . path . relpath ( rootdir , start = source_dir ) 
if bucket_root in ( '.' , '/' ) : 
~~~ bucket_root = '' 
~~ bucket_dirnames = manager . list_dirnames_in_directory ( bucket_root ) 
for bucket_dirname in bucket_dirnames : 
~~~ if bucket_dirname not in dirnames : 
bucket_dirname ) ) ) 
manager . delete_directory ( bucket_dirname ) 
~~ ~~ bucket_filenames = manager . list_filenames_in_directory ( bucket_root ) 
for bucket_filename in bucket_filenames : 
~~~ if bucket_filename not in filenames : 
~~~ bucket_filename = os . path . join ( bucket_root , bucket_filename ) 
logger . debug ( 
manager . delete_file ( bucket_filename ) 
~~ ~~ for filename in filenames : 
~~~ local_path = os . path . join ( rootdir , filename ) 
bucket_path = os . path . join ( path_prefix , bucket_root , filename ) 
upload_file ( local_path , bucket_path , bucket , 
metadata = metadata , acl = acl , 
cache_control = cache_control ) 
~~ if upload_dir_redirect_objects is True : 
~~~ bucket_dir_path = os . path . join ( path_prefix , bucket_root ) 
create_dir_redirect_object ( 
bucket_dir_path , 
bucket , 
metadata = metadata , 
acl = acl , 
~~ ~~ ~~ def upload_file ( local_path , bucket_path , bucket , 
metadata = None , acl = None , cache_control = None ) : 
extra_args = { } 
if acl is not None : 
~~~ extra_args [ 'ACL' ] = acl 
~~~ extra_args [ 'Metadata' ] = metadata 
~~ if cache_control is not None : 
~~~ extra_args [ 'CacheControl' ] = cache_control 
~~ content_type , content_encoding = mimetypes . guess_type ( local_path , 
strict = False ) 
if content_type is not None : 
~~~ extra_args [ 'ContentType' ] = content_type 
~~ logger . debug ( str ( extra_args ) ) 
obj = bucket . Object ( bucket_path ) 
obj . upload_file ( local_path , ExtraArgs = extra_args ) 
~~ def upload_object ( bucket_path , bucket , content = '' , 
metadata = None , acl = None , cache_control = None , 
content_type = None ) : 
args = { } 
~~~ args [ 'Metadata' ] = metadata 
~~ if acl is not None : 
~~~ args [ 'ACL' ] = acl 
~~~ args [ 'CacheControl' ] = cache_control 
~~ if content_type is not None : 
~~~ args [ 'ContentType' ] = content_type 
~~ obj . put ( Body = content , ** args ) 
~~ def create_dir_redirect_object ( bucket_dir_path , bucket , metadata = None , 
acl = None , cache_control = None ) : 
bucket_dir_path = bucket_dir_path . rstrip ( '/' ) 
if metadata is not None : 
~~~ metadata = dict ( metadata ) 
~~~ metadata = { } 
~~ metadata [ 'dir-redirect' ] = 'true' 
upload_object ( bucket_dir_path , 
content = '' , 
bucket = bucket , 
~~ def list_filenames_in_directory ( self , dirname ) : 
prefix = self . _create_prefix ( dirname ) 
filenames = [ ] 
for obj in self . _bucket . objects . filter ( Prefix = prefix ) : 
~~~ if obj . key . endswith ( '/' ) : 
~~ obj_dirname = os . path . dirname ( obj . key ) 
if obj_dirname == prefix : 
~~~ filenames . append ( os . path . relpath ( obj . key , 
start = prefix ) ) 
~~ ~~ return filenames 
~~ def list_dirnames_in_directory ( self , dirname ) : 
dirnames = [ ] 
~~~ dirname = os . path . dirname ( obj . key ) 
if dirname == '' : 
~~~ dirname = obj . key + '/' 
~~ rel_dirname = os . path . relpath ( dirname , start = prefix ) 
dir_parts = rel_dirname . split ( '/' ) 
if len ( dir_parts ) == 1 : 
~~~ dirnames . append ( dir_parts [ 0 ] ) 
~~ ~~ dirnames = list ( set ( dirnames ) ) 
for filtered_dir in ( '.' , '..' ) : 
~~~ if filtered_dir in dirnames : 
~~~ dirnames . remove ( filtered_dir ) 
~~ ~~ return dirnames 
~~ def _create_prefix ( self , dirname ) : 
if dirname in ( '.' , '/' ) : 
~~~ dirname = '' 
~~ prefix = os . path . join ( self . _bucket_root , dirname ) 
prefix = prefix . rstrip ( '/' ) 
return prefix 
~~ def delete_file ( self , filename ) : 
key = os . path . join ( self . _bucket_root , filename ) 
objects = list ( self . _bucket . objects . filter ( Prefix = key ) ) 
for obj in objects : 
~~~ obj . delete ( ) 
~~ ~~ def delete_directory ( self , dirname ) : 
key = os . path . join ( self . _bucket_root , dirname ) 
if not key . endswith ( '/' ) : 
~~~ key += '/' 
~~ key_objects = [ { 'Key' : obj . key } 
for obj in self . _bucket . objects . filter ( Prefix = key ) ] 
if len ( key_objects ) == 0 : 
~~ delete_keys = { 'Objects' : key_objects } 
s3 = self . _session . resource ( 's3' ) 
r = s3 . meta . client . delete_objects ( Bucket = self . _bucket . name , 
Delete = delete_keys ) 
self . _logger . debug ( r ) 
if 'Errors' in r : 
~~ ~~ def ensure_login ( ctx ) : 
if ctx . obj [ 'token' ] is None : 
~~~ if ctx . obj [ 'username' ] is None or ctx . obj [ 'password' ] is None : 
~~~ raise click . UsageError ( 
sys . exit ( 1 ) 
~~ logger . debug ( 
ctx . obj [ 'username' ] , 
ctx . obj [ 'keeper_hostname' ] ) 
token = get_keeper_token ( 
ctx . obj [ 'keeper_hostname' ] , 
ctx . obj [ 'password' ] ) 
ctx . obj [ 'token' ] = token 
~~~ logger . debug ( 
~~ ~~ def delete_dir ( bucket_name , root_path , 
client = s3 . meta . client 
if not root_path . endswith ( '/' ) : 
~~~ root_path . rstrip ( '/' ) 
~~ paginator = client . get_paginator ( 'list_objects_v2' ) 
pages = paginator . paginate ( Bucket = bucket_name , Prefix = root_path ) 
keys = dict ( Objects = [ ] ) 
for item in pages . search ( 'Contents' ) : 
~~~ keys [ 'Objects' ] . append ( { 'Key' : item [ 'Key' ] } ) 
~~ if len ( keys [ 'Objects' ] ) >= 1000 : 
~~~ client . delete_objects ( Bucket = bucket_name , Delete = keys ) 
~~ except Exception : 
logger . exception ( message ) 
raise S3Error ( message ) 
~~ keys = dict ( Objects = [ ] ) 
~~ ~~ if len ( keys [ 'Objects' ] ) > 0 : 
~~ ~~ ~~ def home_url ( ) : 
~~~ return reverse ( home_namespace ) 
~~~ url = home_namespace 
~~~ validate_url = URLValidator ( ) 
if '://' not in url : 
~~~ url = 'http://' + url 
~~ validate_url ( url ) 
return ( url ) 
~~ except ValidationError : 
~~ ~~ ~~ def silence_without_namespace ( f ) : 
@ wraps ( f ) 
def wrapped ( label = None ) : 
~~~ if not home_namespace : 
~~~ return '' 
~~ if label : 
~~~ return f ( label ) 
~~~ return f ( home_label ) 
~~ ~~ return wrapped 
~~ def project_home_breadcrumb_bs3 ( label ) : 
url = home_url ( ) 
if url : 
~~~ return format_html ( 
\ , url , label ) 
~~~ return format_html ( '<li>{}</li>' , label ) 
~~ ~~ def project_home_breadcrumb_bs4 ( label ) : 
\ , 
url , label ) 
label ) 
~~ ~~ def pm ( client , event , channel , nick , rest ) : 
if rest : 
~~~ rest = rest . strip ( ) 
Karma . store . change ( rest , 2 ) 
rcpt = rest 
~~~ rcpt = channel 
~~ if random . random ( ) > 0.95 : 
~~ def lm ( client , event , channel , nick , rest ) : 
~~ def fm ( client , event , channel , nick , rest ) : 
~~ def zorsupas ( client , event , channel , nick , rest ) : 
Karma . store . change ( rest , 1 ) 
~~ def danke ( client , event , channel , nick , rest ) : 
~~ def schneier ( client , event , channel , nick , rest ) : 
~~~ \ 
rcpt = rest . strip ( ) or channel 
if rest . strip ( ) : 
~~~ Karma . store . change ( rcpt , 2 ) 
~~ url = 'https://www.schneierfacts.com/' 
d = requests . get ( url ) . text 
start_tag = re . escape ( \ ) 
end_tag = re . escape ( '</p>' ) 
p = re . compile ( 
start_tag + '(.*?)' + end_tag , 
flags = re . DOTALL | re . MULTILINE ) 
match = p . search ( d ) 
if not match : 
if rcpt != channel : 
phrase = re . sub ( 'Bruce' , rcpt , phrase , flags = re . I ) 
~~ h = html . parser . HTMLParser ( ) 
phrase = h . unescape ( phrase ) 
phrase = ftfy . fix_encoding ( phrase ) 
return phrase 
~~ def get_interaction_energy ( ampal_objs , ff = None , assign_ff = True ) : 
if ff is None : 
~~~ ff = FORCE_FIELDS [ 'bude_2016v1' ] 
~~ if assign_ff : 
~~~ for ampal_obj in ampal_objs : 
~~~ assign_force_field ( ampal_obj , ff ) 
~~ ~~ interactions = find_inter_ampal ( ampal_objs , ff . distance_cutoff ) 
buff_score = score_interactions ( interactions , ff ) 
return buff_score 
~~ def get_internal_energy ( ampal_obj , ff = None , assign_ff = True ) : 
~~ interactions = find_intra_ampal ( ampal_obj , ff . distance_cutoff ) 
~~ def rooted_samples_by_file ( self ) : 
rooted_leaf_samples , _ = self . live_data_copy ( ) 
rooted_file_samples = { } 
for root , counts in rooted_leaf_samples . items ( ) : 
~~~ cur = { } 
for key , count in counts . items ( ) : 
~~~ code , lineno = key 
cur . setdefault ( code . co_filename , 0 ) 
cur [ code . co_filename ] += count 
~~ rooted_file_samples [ root ] = cur 
~~ return rooted_file_samples 
~~ def rooted_samples_by_line ( self , filename ) : 
rooted_line_samples = { } 
if code . co_filename != filename : 
~~ cur [ lineno ] = count 
~~ rooted_line_samples [ root ] = cur 
~~ return rooted_line_samples 
~~ def hotspots ( self ) : 
line_samples = { } 
for _ , counts in rooted_leaf_samples . items ( ) : 
~~~ for key , count in counts . items ( ) : 
~~~ line_samples . setdefault ( key , 0 ) 
line_samples [ key ] += count 
~~ ~~ return sorted ( 
line_samples . items ( ) , key = lambda v : v [ 1 ] , reverse = True ) 
~~ def flame_map ( self ) : 
flame_map = { } 
_ , stack_counts = self . live_data_copy ( ) 
for stack , count in stack_counts . items ( ) : 
~~~ root = stack [ - 2 ] . co_name 
stack_elements = [ ] 
for i in range ( len ( stack ) ) : 
~~~ if type ( stack [ i ] ) in ( int , long ) : 
~~ code = stack [ i ] 
stack_elements . append ( "{0}`{1}`{2}" . format ( 
root , code . co_filename , code . co_name ) ) 
~~ flame_key = ';' . join ( stack_elements ) 
flame_map . setdefault ( flame_key , 0 ) 
flame_map [ flame_key ] += count 
~~ return flame_map 
~~ def get_keeper_token ( host , username , password ) : 
token_endpoint = urljoin ( host , '/token' ) 
r = requests . get ( token_endpoint , auth = ( username , password ) ) 
if r . status_code != 200 : 
format ( host , r . status_code , r . json ( ) ) ) 
~~ return r . json ( ) [ 'token' ] 
~~ def upload ( ctx , product , git_ref , dirname , aws_id , aws_secret , ci_env , 
on_travis_push , on_travis_pr , on_travis_api , on_travis_cron , 
skip_upload ) : 
if skip_upload : 
sys . exit ( 0 ) 
on_travis_push , on_travis_pr , on_travis_api , on_travis_cron ) 
if ci_env == 'travis' and _should_skip_travis_event ( 
on_travis_push , on_travis_pr , on_travis_api , on_travis_cron ) : 
~~~ sys . exit ( 0 ) 
~~ ensure_login ( ctx ) 
git_refs = _get_git_refs ( ci_env , git_ref ) 
build_resource = register_build ( 
ctx . obj [ 'token' ] , 
product , 
git_refs 
upload_dir ( 
build_resource [ 'bucket_name' ] , 
build_resource [ 'bucket_root_dir' ] , 
dirname , 
aws_access_key_id = aws_id , 
aws_secret_access_key = aws_secret , 
surrogate_key = build_resource [ 'surrogate_key' ] , 
cache_control = 'max-age=31536000' , 
upload_dir_redirect_objects = True ) 
confirm_build ( 
build_resource [ 'self_url' ] , 
ctx . obj [ 'token' ] 
~~ def _should_skip_travis_event ( on_travis_push , on_travis_pr , on_travis_api , 
on_travis_cron ) : 
travis_event = os . getenv ( 'TRAVIS_EVENT_TYPE' ) 
if travis_event is None : 
~~ if travis_event == 'push' and on_travis_push is False : 
return True 
~~ elif travis_event == 'pull_request' and on_travis_pr is False : 
~~ elif travis_event == 'api' and on_travis_api is False : 
~~ elif travis_event == 'cron' and on_travis_cron is False : 
~~ ~~ def purge_key ( surrogate_key , service_id , api_key ) : 
api_root = 'https://api.fastly.com' 
path = '/service/{service}/purge/{surrogate_key}' . format ( 
service = service_id , 
surrogate_key = surrogate_key ) 
r = requests . post ( api_root + path , 
headers = { 'Fastly-Key' : api_key , 
'Accept' : 'application/json' } ) 
~~~ raise FastlyError ( r . json ) 
~~ ~~ def register_build ( host , keeper_token , product , git_refs ) : 
data = { 
'git_refs' : git_refs 
endpoint_url = uritemplate . expand ( 
urljoin ( host , '/products/{p}/builds/' ) , 
p = product ) 
r = requests . post ( 
endpoint_url , 
auth = ( keeper_token , '' ) , 
json = data ) 
if r . status_code != 201 : 
~~~ raise KeeperError ( r . json ( ) ) 
~~ build_info = r . json ( ) 
return build_info 
~~ def confirm_build ( build_url , keeper_token ) : 
'uploaded' : True 
r = requests . patch ( 
build_url , 
~~~ raise KeeperError ( r ) 
~~ ~~ def deep_update ( d , u ) : 
for k , v in u . items ( ) : 
~~~ if isinstance ( v , Mapping ) : 
~~~ d [ k ] = deep_update ( d . get ( k , { } ) , v ) 
~~ elif isinstance ( v , list ) : 
~~~ existing_elements = d . get ( k , [ ] ) 
d [ k ] = existing_elements + [ ele for ele in v if ele not in existing_elements ] 
~~~ d [ k ] = v 
~~ ~~ return d 
~~ def main ( ctx , log_level , keeper_hostname , username , password ) : 
ch = logging . StreamHandler ( ) 
formatter = logging . Formatter ( 
ch . setFormatter ( formatter ) 
logger = logging . getLogger ( 'ltdconveyor' ) 
logger . addHandler ( ch ) 
logger . setLevel ( log_level . upper ( ) ) 
ctx . obj = { 
'keeper_hostname' : keeper_hostname , 
'username' : username , 
'password' : password , 
'token' : None 
~~ def part_edit_cmd ( ) : 
parser = argparse . ArgumentParser ( description = inspect . getdoc ( part_edit_cmd ) ) 
parser . add_argument ( 
'path' , 
'--reformat-xml' , 
action = 'store_true' , 
help = ( 
) , 
args = parser . parse_args ( ) 
part_edit ( args . path , args . reformat_xml ) 
~~ def pack_dir_cmd ( ) : 
for item , is_file in sorted ( list_contents ( args . path ) ) : 
msg = prefix + item 
print ( msg ) 
~~ ~~ def split_all ( path ) : 
drive , path = os . path . splitdrive ( path ) 
head , tail = os . path . split ( path ) 
terminators = [ os . path . sep , os . path . altsep , '' ] 
parts = split_all ( head ) if head not in terminators else [ head ] 
return [ drive ] + parts + [ tail ] 
~~ def find_file ( path ) : 
path_components = split_all ( path ) 
def get_assemblies ( ) : 
for n in range ( len ( path_components ) , 0 , - 1 ) : 
~~~ file_c = path_components [ : n ] 
part_c = path_components [ n : ] or [ '' ] 
yield ( os . path . join ( * file_c ) , posixpath . join ( * part_c ) ) 
~~ ~~ for file_path , part_path in get_assemblies ( ) : 
~~~ if os . path . isfile ( file_path ) : 
~~~ return file_path , part_path 
~~ ~~ ~~ def get_editor ( filepath ) : 
default_editor = [ 'edit' , 'notepad' ] [ sys . platform . startswith ( 'win32' ) ] 
return os . environ . get ( 
'XML_EDITOR' , 
os . environ . get ( 'EDITOR' , default_editor ) , 
~~ def process_module ( self , node ) : 
if self . config . file_header : 
~~~ if sys . version_info [ 0 ] < 3 : 
~~~ pattern = re . compile ( 
'\\A' + self . config . file_header , re . LOCALE | re . MULTILINE ) 
'\\A' + self . config . file_header , re . MULTILINE ) 
~~ content = None 
with node . stream ( ) as stream : 
~~~ content = stream . read ( ) . decode ( 'utf-8' ) 
~~ matches = pattern . findall ( content ) 
if len ( matches ) != 1 : 
~~~ self . add_message ( 'invalid-file-header' , 1 , 
args = self . config . file_header ) 
~~ ~~ ~~ def gen ( self , slug , name , dataobj , xfield , yfield , time_unit = None , 
chart_type = "line" , width = 800 , 
height = 300 , color = Color ( ) , size = Size ( ) , 
scale = Scale ( zero = False ) , shape = Shape ( ) , filepath = None , 
html_before = "" , html_after = "" ) : 
chart_obj = self . serialize ( dataobj , xfield , yfield , time_unit , 
chart_type , width , height , color , size , scale , shape ) 
html = self . html ( slug , name , chart_obj , filepath , 
html_before , html_after ) 
return html 
~~ def html ( self , slug , name , chart_obj , filepath = None , 
~~~ html = "" 
if name : 
~~~ html = "<h3>" + name + "</h3>" 
~~ json_data = chart_obj . to_json ( ) 
json_data = self . _patch_json ( json_data ) 
html = html_before + html + self . _json_to_html ( slug , json_data ) + html_after 
~~ except Exception as e : 
~~~ tr . new ( e ) 
tr . check ( ) 
~~ if filepath is not None : 
~~~ self . _write_file ( slug , filepath , html ) 
~~~ return html 
~~ ~~ def serialize ( self , dataobj , xfield , yfield , time_unit = None , 
height = 300 , color = None , size = None , 
scale = Scale ( zero = False ) , shape = None , options = { } ) : 
dataset = dataobj 
if self . _is_dict ( dataobj ) is True : 
~~~ dataset = self . _dict_to_df ( dataobj , xfield , yfield ) 
~~ elif isinstance ( dataobj , list ) : 
~~~ dataset = Data ( values = dataobj ) 
~~ xencode , yencode = self . _encode_fields ( 
xfield , yfield , time_unit ) 
opts = dict ( x = xencode , y = yencode ) 
if color is not None : 
~~~ opts [ "color" ] = color 
~~ if size is not None : 
~~~ opts [ "size" ] = size 
~~ if shape is not None : 
~~~ opts [ "shape" ] = shape 
~~ chart = self . _chart_class ( dataset , chart_type , ** options ) . encode ( 
** opts 
) . configure_cell ( 
width = width , 
height = height , 
return chart 
~~ def _patch_json ( self , json_data ) : 
json_data = json . loads ( json_data ) 
json_data [ "$schema" ] = "https://vega.github.io/schema/vega-lite/2.0.0-beta.15.json" 
json_data [ "width" ] = json_data [ "config" ] [ "cell" ] [ "width" ] 
json_data [ "height" ] = json_data [ "config" ] [ "cell" ] [ "height" ] 
del ( json_data [ "config" ] [ "cell" ] ) 
return json . dumps ( json_data ) 
~~ def _json_to_html ( self , slug , json_data ) : 
html = \ + slug + \ 
html += '<script>' 
html += \ + slug + \ + slug + ');' 
html += '</script>' 
~~ def _dict_to_df ( self , dictobj , xfield , yfield ) : 
x = [ ] 
y = [ ] 
for datapoint in dictobj : 
~~~ x . append ( datapoint ) 
y . append ( dictobj [ datapoint ] ) 
~~ df = pd . DataFrame ( { xfield [ 0 ] : x , yfield [ 0 ] : y } ) 
return df 
~~ def _write_file ( self , slug , folderpath , html ) : 
if not os . path . isdir ( folderpath ) : 
~~~ os . makedirs ( folderpath ) 
~~~ tr . err ( e ) 
~~ ~~ filepath = folderpath + "/" + slug + ".html" 
~~~ filex = open ( filepath , "w" ) 
filex . write ( html ) 
filex . close ( ) 
~~ ~~ def _chart_class ( self , df , chart_type , ** kwargs ) : 
if chart_type == "bar" : 
~~~ return Chart ( df ) . mark_bar ( ** kwargs ) 
~~ elif chart_type == "circle" : 
~~~ return Chart ( df ) . mark_circle ( ** kwargs ) 
~~ elif chart_type == "line" : 
~~~ return Chart ( df ) . mark_line ( ** kwargs ) 
~~ elif chart_type == "point" : 
~~~ return Chart ( df ) . mark_point ( ** kwargs ) 
~~ elif chart_type == "area" : 
~~~ return Chart ( df ) . mark_area ( ** kwargs ) 
~~ elif chart_type == "tick" : 
~~~ return Chart ( df ) . mark_tick ( ** kwargs ) 
~~ elif chart_type == "text" : 
~~~ return Chart ( df ) . mark_text ( ** kwargs ) 
~~ elif chart_type == "square" : 
~~~ return Chart ( df ) . mark_square ( ** kwargs ) 
~~ elif chart_type == "rule" : 
~~~ return Chart ( df ) . mark_rule ( ** kwargs ) 
~~ return None 
~~ def _encode_fields ( self , xfield , yfield , time_unit = None , 
scale = Scale ( zero = False ) ) : 
if scale is None : 
~~~ scale = Scale ( ) 
~~ xfieldtype = xfield [ 1 ] 
yfieldtype = yfield [ 1 ] 
x_options = None 
if len ( xfield ) > 2 : 
~~~ x_options = xfield [ 2 ] 
~~ y_options = None 
if len ( yfield ) > 2 : 
~~~ y_options = yfield [ 2 ] 
~~ if time_unit is not None : 
~~~ if x_options is None : 
~~~ xencode = X ( xfieldtype , timeUnit = time_unit ) 
~~~ xencode = X ( 
xfieldtype , 
axis = Axis ( ** x_options ) , 
timeUnit = time_unit , 
scale = scale 
~~~ xencode = X ( xfieldtype ) 
~~ ~~ if y_options is None : 
~~~ yencode = Y ( yfieldtype , scale = scale ) 
~~~ yencode = Y ( 
yfieldtype , 
axis = Axis ( ** y_options ) , 
~~ return xencode , yencode 
~~ def ghuser_role ( name , rawtext , text , lineno , inliner , options = { } , content = [ ] ) : 
ref = 'https://www.github.com/' + text 
node = nodes . reference ( rawtext , text , refuri = ref , ** options ) 
return [ node ] , [ ] 
~~ def iter_attribute ( iterable_name ) -> Union [ Iterable , Callable ] : 
def create_new_class ( decorated_class ) -> Union [ Iterable , Callable ] : 
decorated_class . iterator_attr_index = 0 
def __iter__ ( instance ) -> Iterable : 
return instance 
~~ def __next__ ( instance ) -> Any : 
ind = instance . iterator_attr_index 
while ind < len ( getattr ( instance , iterable_name ) ) : 
~~~ val = getattr ( instance , iterable_name ) [ ind ] 
instance . iterator_attr_index += 1 
~~ instance . iterator_attr_index = 0 
raise StopIteration 
~~ dct = dict ( decorated_class . __dict__ ) 
dct [ '__iter__' ] = __iter__ 
dct [ '__next__' ] = __next__ 
dct [ 'iterator_attr_index' ] = decorated_class . iterator_attr_index 
return type ( decorated_class . __name__ , ( collections . Iterable , ) , dct ) 
~~ return create_new_class 
~~ def text ( length , choices = string . ascii_letters ) : 
return '' . join ( choice ( choices ) for x in range ( length ) ) 
~~ def rtext ( maxlength , minlength = 1 , choices = string . ascii_letters ) : 
return '' . join ( choice ( choices ) for x in range ( randint ( minlength , maxlength ) ) ) 
~~ def binary ( length ) : 
num = randint ( 1 , 999999 ) 
mask = '0' * length 
return ( mask + '' . join ( [ str ( num >> i & 1 ) for i in range ( 7 , - 1 , - 1 ) ] ) ) [ - length : ] 
~~ def ipaddress ( not_valid = None ) : 
not_valid_class_A = not_valid or [ ] 
class_a = [ r for r in range ( 1 , 256 ) if r not in not_valid_class_A ] 
shuffle ( class_a ) 
first = class_a . pop ( ) 
return "." . join ( [ str ( first ) , str ( randrange ( 1 , 256 ) ) , 
str ( randrange ( 1 , 256 ) ) , str ( randrange ( 1 , 256 ) ) ] ) 
~~ def ip ( private = True , public = True , max_attempts = 10000 ) : 
if not ( private or public ) : 
~~ if private != public : 
~~~ if private : 
~~~ is_valid = lambda address : address . is_private ( ) 
not_valid = [ n for n in range ( 1 , 255 ) if n not in NOT_NET ] 
~~~ is_valid = lambda address : not address . is_private ( ) 
not_valid = NOT_NET 
~~ attempt = 0 
while attempt < max_attempts : 
~~~ attempt += 1 
ip = IPAddress ( ipaddress ( not_valid ) ) 
if is_valid ( ip ) : 
~~~ return ip 
~~~ return IPAddress ( ipaddress ( ) ) 
~~ ~~ def date ( start , end ) : 
stime = date_to_timestamp ( start ) 
etime = date_to_timestamp ( end ) 
ptime = stime + random . random ( ) * ( etime - stime ) 
return datetime . date . fromtimestamp ( ptime ) 
~~ def sequence ( prefix , cache = None ) : 
if cache is None : 
~~~ cache = _sequence_counters 
~~ if cache == - 1 : 
~~~ cache = { } 
~~ if prefix not in cache : 
~~~ cache [ prefix ] = infinite ( ) 
~~ while cache [ prefix ] : 
~~~ yield "{0}-{1}" . format ( prefix , next ( cache [ prefix ] ) ) 
~~ ~~ def _get_memoized_value ( func , args , kwargs ) : 
key = ( repr ( args ) , repr ( kwargs ) ) 
if not key in func . _cache_dict : 
~~~ ret = func ( * args , ** kwargs ) 
func . _cache_dict [ key ] = ret 
~~ return func . _cache_dict [ key ] 
func . _cache_dict = { } 
def _inner ( * args , ** kwargs ) : 
~~~ return _get_memoized_value ( func , args , kwargs ) 
~~ return _inner 
~~ def unique ( func , num_args = 0 , max_attempts = 100 , cache = None ) : 
~~~ cache = _cache_unique 
~~ @ wraps ( func ) 
def wrapper ( * args ) : 
~~~ key = "%s_%s" % ( str ( func . __name__ ) , str ( args [ : num_args ] ) ) 
attempt = 0 
drawn = cache . get ( key , [ ] ) 
result = func ( * args ) 
if result not in drawn : 
~~~ drawn . append ( result ) 
cache [ key ] = drawn 
~~ ~~ raise MaxAttemptException ( ) 
~~ return wrapper 
~~ def register_sub_commands ( self , parser ) : 
sub_commands = self . get_sub_commands ( ) 
if sub_commands : 
~~~ sub_parsers = parser . add_subparsers ( dest = self . sub_parser_dest_name ) 
for name , cls in sub_commands . items ( ) : 
~~~ cmd = cls ( name ) 
sub_parser = sub_parsers . add_parser ( name , help = name , description = cmd . get_help ( ) , formatter_class = cmd . get_formatter_class ( ) ) 
cmd . add_args ( sub_parser ) 
cmd . register_sub_commands ( sub_parser ) 
~~ ~~ ~~ def get_root_argparser ( self ) : 
return self . arg_parse_class ( description = self . get_help ( ) , formatter_class = self . get_formatter_class ( ) ) 
~~ def get_description ( self ) : 
if self . description : 
~~~ return self . description 
~~ elif self . __doc__ and self . __doc__ . strip ( ) : 
~~~ return self . __doc__ . strip ( ) . split ( '.' ) [ 0 ] + '.' 
~~ ~~ def get_help ( self ) : 
if self . help : 
~~~ return self . help 
~~~ return self . __doc__ . strip ( ) 
~~ ~~ def run ( self , args = None ) : 
args = args or self . parse_args ( ) 
sub_command_name = getattr ( args , self . sub_parser_dest_name , None ) 
if sub_command_name : 
~~~ sub_commands = self . get_sub_commands ( ) 
cmd_cls = sub_commands [ sub_command_name ] 
return cmd_cls ( sub_command_name ) . run ( args ) 
~~ return self . action ( args ) or 0 
~~ def encode ( self , * args , ** kwargs ) : 
if isinstance ( args [ 0 ] , str ) : 
~~~ return self . encode ( [ args [ 0 ] ] , ** kwargs ) 
~~ elif isinstance ( args [ 0 ] , int ) or isinstance ( args [ 0 ] , float ) : 
~~~ return self . encode ( [ [ args [ 0 ] ] ] , ** kwargs ) 
~~ if len ( args ) > 1 : 
~~~ dataset = args 
~~~ dataset = args [ 0 ] 
~~ typemap = list ( map ( type , dataset ) ) 
code = self . encoding [ 0 ] 
if type ( '' ) in typemap : 
~~~ data = ',' . join ( map ( str , dataset ) ) 
~~ elif type ( [ ] ) in typemap or type ( ( ) ) in typemap : 
~~~ data = self . codeset [ 'char' ] . join ( map ( self . encodedata , dataset ) ) 
~~ elif len ( dataset ) == 1 and hasattr ( dataset [ 0 ] , '__iter__' ) : 
~~~ data = self . encodedata ( dataset [ 0 ] ) 
~~~ data = self . encodedata ( dataset ) 
~~~ data = self . encodedata ( ',' . join ( map ( unicode , dataset ) ) ) 
~~ ~~ if not '.' in data and code == 't' : 
~~~ code = 'e' 
~~ return '%s%s:%s' % ( code , self . series , data ) 
~~ def get_athletes ( self ) : 
response = self . _get_request ( self . host ) 
response_buffer = StringIO ( response . text ) 
return pd . read_csv ( response_buffer ) 
~~ def get_last_activities ( self , n ) : 
filenames = self . get_activity_list ( ) . iloc [ - n : ] . filename . tolist ( ) 
last_activities = [ self . get_activity ( f ) for f in filenames ] 
return last_activities 
~~ def _request_activity_list ( self , athlete ) : 
response = self . _get_request ( self . _athlete_endpoint ( athlete ) ) 
activity_list = pd . read_csv ( 
filepath_or_buffer = response_buffer , 
parse_dates = { 'datetime' : [ 'date' , 'time' ] } , 
sep = ',\\s*' , 
engine = 'python' 
activity_list . rename ( columns = lambda x : x . lower ( ) , inplace = True ) 
activity_list . rename ( 
columns = lambda x : '_' + x if x [ 0 ] . isdigit ( ) else x , inplace = True ) 
activity_list [ 'has_hr' ] = activity_list . average_heart_rate . map ( bool ) 
activity_list [ 'has_spd' ] = activity_list . average_speed . map ( bool ) 
activity_list [ 'has_pwr' ] = activity_list . average_power . map ( bool ) 
activity_list [ 'has_cad' ] = activity_list . average_heart_rate . map ( bool ) 
activity_list [ 'data' ] = pd . Series ( dtype = np . dtype ( "object" ) ) 
return activity_list 
~~ def _request_activity_data ( self , athlete , filename ) : 
response = self . _get_request ( self . _activity_endpoint ( athlete , filename ) ) . json ( ) 
activity = pd . DataFrame ( response [ 'RIDE' ] [ 'SAMPLES' ] ) 
activity = activity . rename ( columns = ACTIVITY_COLUMN_TRANSLATION ) 
activity . index = pd . to_timedelta ( activity . time , unit = 's' ) 
activity . drop ( 'time' , axis = 1 , inplace = True ) 
return activity [ [ i for i in ACTIVITY_COLUMN_ORDER if i in activity . columns ] ] 
~~ def _athlete_endpoint ( self , athlete ) : 
return '{host}{athlete}' . format ( 
host = self . host , 
athlete = quote_plus ( athlete ) 
~~ def _activity_endpoint ( self , athlete , filename ) : 
return '{host}{athlete}/activity/{filename}' . format ( 
athlete = quote_plus ( athlete ) , 
filename = filename 
~~ def _get_request ( self , endpoint ) : 
~~~ response = requests . get ( endpoint ) 
~~ except requests . exceptions . RequestException : 
~~~ raise GoldenCheetahNotAvailable ( endpoint ) 
~~~ match = re . match ( 
string = response . text ) 
raise AthleteDoesNotExist ( 
athlete = match . groupdict ( ) [ 'athlete' ] ) 
pattern = '.+/activity/(?P<filename>.+)' , 
string = endpoint ) 
raise ActivityDoesNotExist ( 
filename = match . groupdict ( ) [ 'filename' ] ) 
~~ def get_version ( ) : 
with open ( os . path . join ( os . path . dirname ( __file__ ) , 'argparsetree' , '__init__.py' ) ) as init_py : 
~~~ return re . search ( \ , init_py . read ( ) ) . group ( 1 ) 
~~ ~~ def title ( languages = None , genders = None ) : 
languages = languages or [ 'en' ] 
genders = genders or ( GENDER_FEMALE , GENDER_MALE ) 
choices = _get_titles ( languages ) 
gender = { 'm' : 0 , 'f' : 1 } [ random . choice ( genders ) ] 
return random . choice ( choices ) [ gender ] 
~~ def person ( languages = None , genders = None ) : 
lang = random . choice ( languages ) 
g = random . choice ( genders ) 
t = title ( [ lang ] , [ g ] ) 
return first_name ( [ lang ] , [ g ] ) , last_name ( [ lang ] ) , t , g 
~~ def first_name ( languages = None , genders = None ) : 
choices = [ ] 
genders = genders or [ GENDER_MALE , GENDER_FEMALE ] 
for lang in languages : 
~~~ for gender in genders : 
~~~ samples = _get_firstnames ( lang , gender ) 
choices . extend ( samples ) 
~~ ~~ return random . choice ( choices ) . title ( ) 
~~ def last_name ( languages = None ) : 
~~~ samples = _get_lastnames ( lang ) 
~~ return random . choice ( choices ) . title ( ) 
~~ def lookup_color ( color ) : 
if color is None : return 
color = color . lower ( ) 
if color in COLOR_MAP : 
~~~ return COLOR_MAP [ color ] 
~~ return color 
~~ def color_args ( args , * indexes ) : 
for i , arg in enumerate ( args ) : 
~~~ if i in indexes : 
~~~ yield lookup_color ( arg ) 
~~~ yield arg 
~~ ~~ ~~ def tick ( self , index , length ) : 
self . data [ 'ticks' ] . append ( '%s,%d' % ( index , length ) ) 
return self . parent 
~~ def type ( self , atype ) : 
for char in atype : 
~~ if not ',' in atype : 
~~~ atype = ',' . join ( atype ) 
~~ self [ 'chxt' ] = atype 
~~ def label ( self , index , * args ) : 
self . data [ 'labels' ] . append ( 
str ( '%s:|%s' % ( index , '|' . join ( map ( str , args ) ) ) ) . replace ( 'None' , '' ) 
~~ def range ( self , index , * args ) : 
self . data [ 'ranges' ] . append ( '%s,%s' % ( index , 
',' . join ( map ( smart_str , args ) ) ) ) 
~~ def style ( self , index , * args ) : 
args = color_args ( args , 0 ) 
self . data [ 'styles' ] . append ( 
',' . join ( [ str ( index ) ] + list ( map ( str , args ) ) ) 
~~ def render ( self ) : 
for opt , values in self . data . items ( ) : 
~~~ if opt == 'ticks' : 
~~~ self [ 'chxtc' ] = '|' . join ( values ) 
~~~ self [ 'chx%s' % opt [ 0 ] ] = '|' . join ( values ) 
~~ def fromurl ( cls , qs ) : 
if isinstance ( qs , dict ) : 
~~~ return cls ( ** qs ) 
~~ return cls ( ** dict ( parse_qsl ( qs [ qs . index ( '?' ) + 1 : ] ) ) ) 
~~ def map ( self , geo , country_codes ) : 
self . _geo = geo 
self . _ld = country_codes 
return self 
~~ def scale ( self , * args ) : 
self . _scale = [ ',' . join ( map ( smart_str , args ) ) ] 
~~ def dataset ( self , data , series = '' ) : 
self . _dataset = data 
self . _series = series 
~~ def marker ( self , * args ) : 
if len ( args [ 0 ] ) == 1 : 
args = color_args ( args , 1 ) 
self . markers . append ( ',' . join ( map ( str , args ) ) ) 
~~ def margin ( self , left , right , top , bottom , lwidth = 0 , lheight = 0 ) : 
self [ 'chma' ] = '%d,%d,%d,%d' % ( left , right , top , bottom ) 
if lwidth or lheight : 
~~~ self [ 'chma' ] += '|%d,%d' % ( lwidth , lheight ) 
~~ return self 
~~ def line ( self , * args ) : 
self . lines . append ( ',' . join ( [ '%.1f' % x for x in map ( float , args ) ] ) ) 
~~ def fill ( self , * args ) : 
a , b = args [ : 2 ] 
if len ( args ) == 3 : 
~~~ args = color_args ( args , 2 ) 
~~~ args = color_args ( args , 3 , 5 ) 
~~ self . fills . append ( ',' . join ( map ( str , args ) ) ) 
~~ def grid ( self , * args ) : 
grids = map ( str , map ( float , args ) ) 
self [ 'chg' ] = ',' . join ( grids ) . replace ( 'None' , '' ) 
~~ def color ( self , * args ) : 
args = color_args ( args , * range ( len ( args ) ) ) 
self [ 'chco' ] = ',' . join ( args ) 
~~ def label ( self , * args ) : 
if self [ 'cht' ] == 'qr' : 
~~~ self [ 'chl' ] = '' . join ( map ( str , args ) ) 
~~~ self [ 'chl' ] = '|' . join ( map ( str , args ) ) 
~~ def legend_pos ( self , pos ) : 
self [ 'chdlp' ] = str ( pos ) 
~~ def title ( self , title , * args ) : 
self [ 'chtt' ] = title 
if args : 
~~~ args = color_args ( args , 0 ) 
self [ 'chts' ] = ',' . join ( map ( str , args ) ) 
~~ def size ( self , * args ) : 
if len ( args ) == 2 : 
~~~ x , y = map ( int , args ) 
~~~ x , y = map ( int , args [ 0 ] ) 
~~ self . check_size ( x , y ) 
self [ 'chs' ] = '%dx%d' % ( x , y ) 
self . update ( self . axes . render ( ) ) 
encoder = Encoder ( self . _encoding , None , self . _series ) 
if not 'chs' in self : 
~~~ self [ 'chs' ] = '300x150' 
~~~ size = self [ 'chs' ] . split ( 'x' ) 
self . check_size ( * map ( int , size ) ) 
self [ 'cht' ] = self . check_type ( self [ 'cht' ] ) 
if ( 'any' in dir ( self . _dataset ) and self . _dataset . any ( ) ) or self . _dataset : 
~~~ self [ 'chd' ] = encoder . encode ( self . _dataset ) 
~~ elif not 'choe' in self : 
~~ if self . _scale : 
self [ 'chds' ] = ',' . join ( self . _scale ) 
~~ if self . _geo and self . _ld : 
~~~ self [ 'chtm' ] = self . _geo 
self [ 'chld' ] = self . _ld 
~~ if self . lines : 
~~~ self [ 'chls' ] = '|' . join ( self . lines ) 
~~ if self . markers : 
~~~ self [ 'chm' ] = '|' . join ( self . markers ) 
~~ if self . fills : 
~~~ self [ 'chf' ] = '|' . join ( self . fills ) 
~~ ~~ def check_type ( self , type ) : 
if type in TYPES : 
~~~ return type 
~~ tdict = dict ( zip ( TYPES , TYPES ) ) 
tdict . update ( { 
'line' : 'lc' , 
'bar' : 'bvs' , 
'pie' : 'p' , 
'venn' : 'v' , 
'scater' : 's' , 
'radar' : 'r' , 
'meter' : 'gom' , 
return tdict [ type ] 
~~ def url ( self ) : 
self . render ( ) 
~~ def show ( self , * args , ** kwargs ) : 
from webbrowser import open as webopen 
return webopen ( str ( self ) , * args , ** kwargs ) 
~~ def save ( self , fname = None ) : 
if not fname : 
~~~ fname = self . getname ( ) 
if not fname . endswith ( '.png' ) : 
~~~ fname += '.png' 
~~~ urlretrieve ( self . url , fname ) 
~~ return fname 
~~ def img ( self , ** kwargs ) : 
safe = \ % self . url . replace ( '&' , '&amp;' ) . replace ( '<' , '&lt;' ) . replace ( '>' , '&gt;' ) . replace ( \ , '&quot;' ) . replace ( "\ , '&#39;' ) 
for item in kwargs . items ( ) : 
~~~ if not item [ 0 ] in IMGATTRS : 
~~ safe += \ % item 
~~ def urlopen ( self ) : 
req = Request ( str ( self ) ) 
~~~ return urlopen ( req ) 
~~ except HTTPError : 
~~ except URLError : 
~~ ~~ def image ( self ) : 
~~~ import Image 
~~ except ImportError : 
~~~ from PIL import Image 
~~ ~~ except ImportError : 
~~~ from cStringIO import StringIO 
~~~ from StringIO import StringIO 
~~ return Image . open ( StringIO ( self . urlopen ( ) . read ( ) ) ) 
~~ def write ( self , fp ) : 
urlfp = self . urlopen ( ) . fp 
while 1 : 
~~~ fp . write ( urlfp . next ( ) ) 
~~ ~~ ~~ def checksum ( self ) : 
return new_sha ( '' . join ( sorted ( self . _parts ( ) ) ) ) . hexdigest ( ) 
~~ def get_codes ( ) : 
cache_filename = os . path . join ( os . path . dirname ( __file__ ) , 'data' , 'countryInfo.txt' ) 
data = [ ] 
for line in open ( cache_filename , 'r' ) : 
~~~ if not line . startswith ( '#' ) : 
~~~ data . append ( line . split ( '\\t' ) ) 
~~ ~~ return data 
~~ def amount ( min = 1 , max = sys . maxsize , decimal_places = 2 ) : 
q = '.%s1' % '0' * ( decimal_places - 1 ) 
return decimal . Decimal ( uniform ( min , max ) ) . quantize ( decimal . Decimal ( q ) ) 
~~ def eval ( self , orig ) : 
_le = { } 
_err = [ ] 
for k , v in self . sup_items ( ) : 
~~~ if k in DoNotCompare : 
~~ if k in orig : 
~~~ if is_lesser ( orig [ k ] , v ) : 
~~~ _le [ k ] = orig [ k ] 
~~~ _err . append ( { 'claim' : k , 'policy' : orig [ k ] , 'err' : v , 
'signer' : self . iss } ) 
~~~ _le [ k ] = v 
~~ ~~ for k , v in orig . items ( ) : 
~~ if k not in _le : 
~~ ~~ self . le = _le 
self . err = _err 
~~ def unprotected_and_protected_claims ( self ) : 
if self . sup : 
~~~ res = { } 
for k , v in self . le . items ( ) : 
~~~ if k not in self . sup . le : 
~~~ res [ k ] = v 
~~~ res [ k ] = self . sup . le [ k ] 
~~ ~~ return res 
~~~ return self . le 
~~ ~~ def signing_keys_as_jwks ( self ) : 
_l = [ x . serialize ( ) for x in self . self_signer . keyjar . get_signing_key ( ) ] 
if not _l : 
~~~ _l = [ x . serialize ( ) for x in 
self . self_signer . keyjar . get_signing_key ( owner = self . iss ) ] 
~~ return { 'keys' : _l } 
~~ def _unpack ( self , ms_dict , keyjar , cls , jwt_ms = None , liss = None ) : 
if liss is None : 
~~~ liss = [ ] 
~~ _pr = ParseInfo ( ) 
_pr . input = ms_dict 
ms_flag = False 
if 'metadata_statements' in ms_dict : 
~~~ ms_flag = True 
for iss , _ms in ms_dict [ 'metadata_statements' ] . items ( ) : 
~~~ if liss and iss not in liss : 
~~ _pr = self . _ums ( _pr , _ms , keyjar ) 
~~ ~~ if 'metadata_statement_uris' in ms_dict : 
if self . httpcli : 
~~~ for iss , url in ms_dict [ 'metadata_statement_uris' ] . items ( ) : 
~~ rsp = self . httpcli ( method = 'GET' , url = url , 
verify = self . verify_ssl ) 
if rsp . status_code == 200 : 
~~~ _pr = self . _ums ( _pr , rsp . text , keyjar ) 
~~~ raise ParseError ( 
~~ ~~ ~~ ~~ for _ms in _pr . parsed_statement : 
~~~ loaded = False 
~~~ keyjar . import_jwks_as_json ( _ms [ 'signing_keys' ] , 
ms_dict [ 'iss' ] ) 
~~ except KeyError : 
~~~ keyjar . import_jwks ( _ms [ 'signing_keys' ] , ms_dict [ 'iss' ] ) 
~~ except Exception as err : 
~~~ logger . error ( err ) 
~~~ loaded = True 
~~ if loaded : 
'keyjar' . format ( ms_dict [ 'iss' ] ) ) 
~~ ~~ ~~ if ms_flag is True and not _pr . parsed_statement : 
~~~ return _pr 
~~ if jwt_ms : 
~~~ _pr . result = cls ( ) . from_jwt ( jwt_ms , keyjar = keyjar ) 
~~ except MissingSigningKey : 
~~~ if 'signing_keys' in ms_dict : 
~~~ _pr . result = self . self_signed ( ms_dict , jwt_ms , cls ) 
~~ except MissingSigningKey as err : 
_pr . error [ jwt_ms ] = err 
~~ ~~ ~~ except ( JWSException , BadSignature , KeyError ) as err : 
~~~ _pr . result = ms_dict 
~~ if _pr . result and _pr . parsed_statement : 
~~~ _prr = _pr . result 
_res = { } 
for x in _pr . parsed_statement : 
~~~ if x : 
~~~ _res [ get_fo ( x ) ] = x 
~~ ~~ _msg = Message ( ** _res ) 
_pr . result [ 'metadata_statements' ] = _msg 
~~ return _pr 
~~ def unpack_metadata_statement ( self , ms_dict = None , jwt_ms = '' , keyjar = None , 
cls = ClientMetadataStatement , liss = None ) : 
if not keyjar : 
~~~ if self . jwks_bundle : 
~~~ keyjar = self . jwks_bundle . as_keyjar ( ) 
~~~ keyjar = KeyJar ( ) 
~~ ~~ if jwt_ms : 
~~~ ms_dict = unfurl ( jwt_ms ) 
~~ except JWSException as err : 
~~ ~~ if ms_dict : 
~~~ return self . _unpack ( ms_dict , keyjar , cls , jwt_ms , liss ) 
~~ ~~ def pack_metadata_statement ( self , metadata , receiver = '' , iss = '' , lifetime = 0 , 
sign_alg = '' ) : 
return self . self_signer . sign ( metadata , receiver = receiver , iss = iss , 
lifetime = lifetime , sign_alg = sign_alg ) 
~~ def evaluate_metadata_statement ( self , metadata , keyjar = None ) : 
res = dict ( [ ( k , v ) for k , v in metadata . items ( ) if k not in IgnoreKeys ] ) 
les = [ ] 
if 'metadata_statements' in metadata : 
~~~ for fo , ms in metadata [ 'metadata_statements' ] . items ( ) : 
~~~ if isinstance ( ms , str ) : 
~~~ ms = json . loads ( ms ) 
~~ for _le in self . evaluate_metadata_statement ( ms ) : 
~~~ if isinstance ( ms , Message ) : 
~~~ le = LessOrEqual ( sup = _le , ** ms . to_dict ( ) ) 
~~~ le = LessOrEqual ( sup = _le , ** ms ) 
~~ if le . is_expired ( ) : 
~~~ logger . error ( 
~~ le . eval ( res ) 
les . append ( le ) 
~~ ~~ return les 
~~~ _iss = metadata [ 'iss' ] 
~~ except : 
~~~ le = LessOrEqual ( ) 
le . eval ( res ) 
~~~ le = LessOrEqual ( iss = _iss , exp = metadata [ 'exp' ] ) 
~~ les . append ( le ) 
return les 
~~ ~~ def correct_usage ( self , metadata , federation_usage ) : 
~~~ _msl = { } 
for fo , ms in metadata [ 'metadata_statements' ] . items ( ) : 
~~~ if not isinstance ( ms , Message ) : 
~~ if self . correct_usage ( ms , federation_usage = federation_usage ) : 
~~~ _msl [ fo ] = ms 
~~ ~~ if _msl : 
~~~ metadata [ 'metadata_statements' ] = Message ( ** _msl ) 
return metadata 
~~~ assert federation_usage == metadata [ 'federation_usage' ] 
~~ except AssertionError : 
~~ ~~ def extend_with_ms ( self , req , sms_dict ) : 
_ms_uri = { } 
_ms = { } 
for fo , sms in sms_dict . items ( ) : 
~~~ if sms . startswith ( 'http://' ) or sms . startswith ( 'https://' ) : 
~~~ _ms_uri [ fo ] = sms 
~~~ _ms [ fo ] = sms 
~~ ~~ if _ms : 
~~~ req [ 'metadata_statements' ] = Message ( ** _ms ) 
~~ if _ms_uri : 
~~~ req [ 'metadata_statement_uris' ] = Message ( ** _ms_uri ) 
~~ return req 
~~ def _aodata ( echo , columns , xnxq = None , final_exam = False ) : 
ao_data = [ { "name" : "sEcho" , "value" : echo } , 
{ "name" : "iColumns" , "value" : len ( columns ) } , 
{ "name" : "sColumns" , "value" : "" } , 
{ "name" : "iDisplayStart" , "value" : 0 } , 
{ "name" : "iDisplayLength" , "value" : - 1 } , 
if xnxq : 
~~~ if final_exam : 
~~~ ao_data . append ( 
{ "name" : "ksrwid" , "value" : "000000005bf6cb6f015bfac609410d4b" } ) 
~~ ao_data . append ( { "name" : "xnxq" , "value" : xnxq } ) 
~~ for index , value in enumerate ( columns ) : 
{ "name" : "mDataProp_{}" . format ( index ) , "value" : value } ) 
ao_data . append ( 
{ "name" : "bSortable_{}" . format ( index ) , "value" : False } ) 
~~ return urlencode ( { "aoData" : ao_data } ) 
~~ def login ( self ) : 
if not hasattr ( self , 'session' ) : 
~~~ self . last_connect = time . time ( ) 
s = requests . session ( ) 
s . get ( 'http://bkjws.sdu.edu.cn' ) 
'j_username' : self . student_id , 
'j_password' : self . password_md5 
r6 = s . post ( 'http://bkjws.sdu.edu.cn/b/ajaxLogin' , headers = { 
'user-agent' : self . _ua } , 
data = data ) 
if r6 . text == \ : 
~~~ return s 
~~~ s . close ( ) 
raise AuthFailure ( r6 . text ) 
~~ ~~ ~~ def get_lesson ( self ) : 
html = self . _get ( 'http://bkjws.sdu.edu.cn/f/xk/xs/bxqkb' ) 
soup = BeautifulSoup ( html , "html.parser" ) 
s = soup . find ( 'table' , 
"id" : "ysjddDataTableId" } ) 
tr_box = s . find_all ( 'tr' ) 
c = list ( ) 
for les in tr_box [ 1 : ] : 
~~~ td_box = les . find_all ( 'td' ) 
c . append ( { "lesson_num_long" : td_box [ 1 ] . text , 
"lesson_name" : td_box [ 2 ] . text , 
"lesson_num_short" : td_box [ 3 ] . text , 
"credit" : td_box [ 4 ] . text , 
"school" : td_box [ 6 ] . text , 
"teacher" : td_box [ 7 ] . text , 
"weeks" : td_box [ 8 ] . text , 
"days" : td_box [ 9 ] . text , 
"times" : td_box [ 10 ] . text , 
"place" : td_box [ 11 ] . text } ) 
~~ self . _lessons = c 
return c 
~~ def lessons ( self ) : 
if hasattr ( self , '_lessons' ) : 
~~~ return self . _lessons 
~~~ self . get_lesson ( ) 
return self . _lessons 
~~ ~~ def detail ( self ) : 
if hasattr ( self , '_detail' ) : 
~~~ return self . _detail 
~~~ self . get_detail ( ) 
return self . _detail 
~~ ~~ def get_detail ( self ) : 
response = self . _post ( "http://bkjws.sdu.edu.cn/b/grxx/xs/xjxx/detail" , 
data = None ) 
if response [ 'result' ] == 'success' : 
~~~ self . _detail = response [ 'object' ] 
~~~ self . _unexpected ( response ) 
~~ ~~ def get_raw_past_score ( self ) : 
echo = self . _echo 
response = self . _post ( "http://bkjws.sdu.edu.cn/b/cj/cjcx/xs/lscx" , 
data = self . _aodata ( echo , 
columns = [ "xnxq" , "kcm" , "kxh" , "xf" , "kssj" , 
"kscjView" , "wfzjd" , 
"wfzdj" , 
"kcsx" ] ) ) 
if self . _check_response ( response , echo ) : 
~~~ self . _raw_past_score = response 
return self . _raw_past_score 
response = self . _post ( 'http://bkjws.sdu.edu.cn/b/pg/xs/list' , 
data = self . _aodata ( echo , [ 'kch' , 'kcm' , 'jsm' , 'function' , 'function' ] ) , ) 
if self . _check_response ( response , echo = echo ) : 
~~~ return response [ 'object' ] [ 'aaData' ] 
~~ ~~ def get_exam_time ( self , xnxq ) : 
response = self . _post ( 'http://bkjws.sdu.edu.cn/b/ksap/xs/vksapxs/pageList' , 
data = self . _aodata ( echo , xnxq = xnxq , 
columns = [ "function" , 'ksmc' , 'kcm' , 'kch' , 'xqmc' , 
'jxljs' , 'sjsj' , "ksfsmc" , 
"ksffmc" , "ksbz" ] ) ) 
~~ ~~ def _letter_map ( word ) : 
lmap = { } 
for letter in word : 
~~~ lmap [ letter ] += 1 
~~~ lmap [ letter ] = 1 
~~ ~~ return lmap 
~~ def anagrams_in_word ( word , sowpods = False , start = "" , end = "" ) : 
input_letters , blanks , questions = blank_tiles ( word ) 
for tile in start + end : 
~~~ input_letters . append ( tile ) 
~~ for word in word_list ( sowpods , start , end ) : 
~~~ lmap = _letter_map ( input_letters ) 
used_blanks = 0 
~~~ if letter in lmap : 
~~~ lmap [ letter ] -= 1 
if lmap [ letter ] < 0 : 
~~~ used_blanks += 1 
if used_blanks > ( blanks + questions ) : 
~~~ yield ( word , word_score ( word , input_letters , questions ) ) 
~~ ~~ ~~ def transform_timeseries_data ( timeseries , start , end = None ) : 
include = False 
for metric , points in timeseries . items ( ) : 
~~~ for point in points : 
~~~ if point [ 'x' ] == start : 
~~~ include = True 
~~ if include : 
~~~ data . append ( point [ 'y' ] ) 
~~ if end is not None and point [ 'x' ] == end : 
~~~ return data 
~~ ~~ ~~ return data 
~~ def get_last_value_from_timeseries ( timeseries ) : 
if not timeseries : 
~~~ return 0 
~~ for metric , points in timeseries . items ( ) : 
~~~ return next ( ( p [ 'y' ] for p in reversed ( points ) if p [ 'y' ] > 0 ) , 0 ) 
~~ ~~ def validate_page_number ( number ) : 
~~~ number = int ( number ) 
~~ if number < 1 : 
~~ return number 
~~ def get_page_of_iterator ( iterator , page_size , page_number ) : 
~~~ page_number = validate_page_number ( page_number ) 
~~ except ( PageNotAnInteger , EmptyPage ) : 
~~~ page_number = 1 
~~ start = ( page_number - 1 ) * page_size 
end = ( page_number * page_size ) + 1 
skipped_items = list ( islice ( iterator , start ) ) 
items = list ( islice ( iterator , end ) ) 
if len ( items ) == 0 and page_number != 1 : 
~~~ items = skipped_items 
page_number = 1 
~~ has_next = len ( items ) > page_size 
items = items [ : page_size ] 
return NoCountPage ( items , page_number , page_size , has_next ) 
~~ def make_internal_signing_service ( config , entity_id ) : 
_args = dict ( [ ( k , v ) for k , v in config . items ( ) if k in KJ_SPECS ] ) 
_kj = init_key_jar ( ** _args ) 
return InternalSigningService ( entity_id , _kj ) 
~~ def make_signing_service ( config , entity_id ) : 
if config [ 'type' ] == 'internal' : 
~~~ signer = InternalSigningService ( entity_id , _kj ) 
~~ elif config [ 'type' ] == 'web' : 
~~~ _kj . issuer_keys [ config [ 'iss' ] ] = _kj . issuer_keys [ '' ] 
del _kj . issuer_keys [ '' ] 
signer = WebSigningServiceClient ( config [ 'iss' ] , config [ 'url' ] , 
entity_id , _kj ) 
~~ return signer 
~~ def sign ( self , req , receiver = '' , iss = '' , lifetime = 0 , sign_alg = '' , aud = None ) : 
if not sign_alg : 
~~~ for key_type , s_alg in [ ( 'RSA' , 'RS256' ) , ( 'EC' , 'ES256' ) ] : 
~~~ if self . keyjar . get_signing_key ( key_type = key_type ) : 
~~~ sign_alg = s_alg 
~~ ~~ ~~ if not sign_alg : 
~~ return self . pack ( req = req , receiver = receiver , iss = iss , lifetime = lifetime , 
sign = True , encrypt = False , sign_alg = sign_alg ) 
~~ def create ( self , req , ** kwargs ) : 
response = requests . post ( self . url , json = req , ** self . req_args ( ) ) 
return self . parse_response ( response ) 
~~ def update_metadata_statement ( self , location , req ) : 
response = requests . put ( location , json = req , ** self . req_args ( ) ) 
~~ def update_signature ( self , location ) : 
response = requests . get ( location , ** self . req_args ( ) ) 
~~ def find_version ( filename ) : 
with io . open ( filename , encoding = "utf-8" ) as version_file : 
~~~ version_match = re . search ( r\ , 
version_file . read ( ) , re . M ) 
~~ if version_match : 
~~~ return version_match . group ( 1 ) 
~~ return "0.0-version-unknown" 
~~ def get_modules ( self ) : 
if not self . project_abspath : 
~~ packages_abspath = self . get_package_abspath ( ) 
for package_abspath in packages_abspath : 
~~~ self . get_module_name ( package_abspath ) 
~~ return self . _modules 
~~ def import_modules ( self ) : 
modules = self . get_modules ( ) 
~~~ for module in modules : 
~~~ __import__ ( module ) 
~~ ~~ except ImportError as error : 
~~~ raise ImportModulesError ( error . msg ) 
~~ ~~ def select_fields ( doc , field_list ) : 
if field_list is None or len ( field_list ) == 0 : 
~~~ return doc 
~~ newDoc = Nested_Dict ( { } ) 
oldDoc = Nested_Dict ( doc ) 
for i in field_list : 
~~~ if oldDoc . has_key ( i ) : 
~~~ newDoc . set_value ( i , oldDoc . get_value ( i ) ) 
~~ ~~ return newDoc . dict_value ( ) 
~~ def date_map ( doc , datemap_list , time_format = None ) : 
if datemap_list : 
~~~ for i in datemap_list : 
~~~ if isinstance ( i , datetime ) : 
~~~ doc = CursorFormatter . date_map_field ( doc , i , time_format = time_format ) 
~~ ~~ ~~ return doc 
~~ def printCursor ( self , fieldnames = None , datemap = None , time_format = None ) : 
if self . _format == 'csv' : 
~~~ count = self . printCSVCursor ( fieldnames , datemap , time_format ) 
~~~ count = self . printJSONCursor ( fieldnames , datemap , time_format ) 
~~ return count 
~~ def output ( self , fieldNames = None , datemap = None , time_format = None ) : 
count = self . printCursor ( self . _cursor , fieldNames , datemap , time_format ) 
~~ def add_default_deps ( project ) : 
for name , short , order , af in DEFAULT_DEPARTMENTS : 
~~~ dep , created = Department . objects . get_or_create ( name = name , short = short , ordervalue = order , assetflag = af ) 
dep . projects . add ( project ) 
dep . full_clean ( ) 
dep . save ( ) 
~~ ~~ def add_default_atypes ( project ) : 
for name , desc in DEFAULT_ASSETTYPES : 
~~~ at , created = Atype . objects . get_or_create ( name = name , defaults = { 'description' : desc } ) 
at . projects . add ( project ) 
at . full_clean ( ) 
at . save ( ) 
~~ ~~ def add_default_sequences ( project ) : 
for name , desc in seqs : 
~~~ seq , created = Sequence . objects . get_or_create ( name = name , project = project , defaults = { 'description' : desc } ) 
~~ ~~ def add_userrnd_shot ( project ) : 
rndseq = project . sequence_set . get ( name = RNDSEQ_NAME ) 
users = [ u for u in project . users . all ( ) ] 
for user in users : 
~~~ shot , created = Shot . objects . get_or_create ( name = user . username , 
project = project , 
sequence = rndseq , 
for t in shot . tasks . all ( ) : 
~~~ t . users . add ( user ) 
t . full_clean ( ) 
t . save ( ) 
~~ ~~ ~~ def prj_post_save_handler ( sender , ** kwargs ) : 
prj = kwargs [ 'instance' ] 
if not kwargs [ 'created' ] : 
~~~ add_userrnd_shot ( prj ) 
~~ add_default_deps ( prj ) 
add_default_atypes ( prj ) 
add_default_sequences ( prj ) 
~~ def seq_post_save_handler ( sender , ** kwargs ) : 
~~ seq = kwargs [ 'instance' ] 
if seq . name == RNDSEQ_NAME : 
~~ prj = seq . project 
name = GLOBAL_NAME 
Shot . objects . create ( name = name , project = prj , sequence = seq , description = desc ) 
~~ def create_all_tasks ( element ) : 
prj = element . project 
if isinstance ( element , Asset ) : 
~~~ flag = True 
~~ deps = prj . department_set . filter ( assetflag = flag ) 
for d in deps : 
~~~ t = Task ( project = prj , department = d , element = element ) 
~~ ~~ def path ( self ) : 
p = os . path . normpath ( self . _path ) 
if p . endswith ( ':' ) : 
~~~ p = p + os . path . sep 
~~ return p 
~~ def path ( self , value ) : 
prepval = value . replace ( '\\\\' , '/' ) 
self . _path = posixpath . normpath ( prepval ) 
if self . _path . endswith ( ':' ) : 
~~~ self . _path = self . _path + posixpath . sep 
~~ ~~ def clean ( self , ) : 
if self . startframe > self . endframe : 
~~ ~~ def path ( self , value ) : 
~~ def register_type ( self , typename ) : 
self . _dummy_protocol . register_type ( typename ) 
self . _typenames . add ( typename ) 
~~ def open ( self , packet_received ) : 
def port_open ( listeningport ) : 
~~~ self . _listeningport = listeningport 
self . ownid = self . ownid_factory ( listeningport ) 
self . packet_received = packet_received 
d = self . stream_server_endpoint . listen ( PoolFactory ( self , self . _typenames ) ) 
d . addCallback ( port_open ) 
return d 
~~ def pre_connect ( self , peer ) : 
if peer in self . _connections : 
~~~ return defer . succeed ( peer ) 
~~~ d = self . _connect ( peer , exact_peer = False ) 
def connected ( p ) : 
~~~ return p . peer 
~~ d . addCallback ( connected ) 
~~ ~~ def send ( self , peer , typename , data ) : 
def attempt_to_send ( _ ) : 
~~~ if peer not in self . _connections : 
~~~ d = self . _connect ( peer ) 
d . addCallback ( attempt_to_send ) 
~~~ conn = self . _connections [ peer ] [ 0 ] 
conn . send_packet ( typename , data ) 
return defer . succeed ( None ) 
~~ ~~ d = attempt_to_send ( None ) 
self . _ongoing_sends . add ( d ) 
def send_completed ( result ) : 
~~~ if d in self . _ongoing_sends : 
~~~ self . _ongoing_sends . remove ( d ) 
~~ d . addBoth ( send_completed ) 
def cancel_sends ( _ ) : 
while self . _ongoing_sends : 
~~~ d = self . _ongoing_sends . pop ( ) 
d . cancel ( ) 
~~ ~~ def close_connections ( _ ) : 
~~~ all_connections = [ c for conns in self . _connections . itervalues ( ) for c in conns ] 
for c in all_connections : 
~~~ c . transport . loseConnection ( ) 
~~ ds = [ c . wait_for_close ( ) for c in all_connections ] 
d = defer . DeferredList ( ds , fireOnOneErrback = True ) 
def allclosed ( _ ) : 
~~ d . addCallback ( allclosed ) 
d = defer . maybeDeferred ( self . _listeningport . stopListening ) 
d . addCallback ( cancel_sends ) 
d . addCallback ( close_connections ) 
~~ def get_config_value ( self , section , key , return_type : type ) : 
~~~ value = self . method_mapping [ return_type ] ( section , key ) 
~~ except NoSectionError as e : 
~~~ raise ConfigError ( e . message ) 
~~ except NoOptionError as e : 
~~ return value 
~~ def nova ( * arg ) : 
check_event_type ( Openstack . Nova , * arg ) 
event_type = arg [ 0 ] 
def decorator ( func ) : 
~~~ if event_type . find ( "*" ) != - 1 : 
~~~ event_type_pattern = pre_compile ( event_type ) 
nova_customer_process_wildcard [ event_type_pattern ] = func 
~~~ nova_customer_process [ event_type ] = func 
@ functools . wraps ( func ) 
def wrapper ( * args , ** kwargs ) : 
~~~ func ( * args , ** kwargs ) 
~~ return decorator 
~~ def cinder ( * arg ) : 
check_event_type ( Openstack . Cinder , * arg ) 
cinder_customer_process_wildcard [ event_type_pattern ] = func 
~~~ cinder_customer_process [ event_type ] = func 
~~ def neutron ( * arg ) : 
check_event_type ( Openstack . Neutron , * arg ) 
neutron_customer_process_wildcard [ event_type_pattern ] = func 
~~~ neutron_customer_process [ event_type ] = func 
~~ def glance ( * arg ) : 
check_event_type ( Openstack . Glance , * arg ) 
glance_customer_process_wildcard [ event_type_pattern ] = func 
~~~ glance_customer_process [ event_type ] = func 
~~ def swift ( * arg ) : 
check_event_type ( Openstack . Swift , * arg ) 
swift_customer_process_wildcard [ event_type_pattern ] = func 
~~~ swift_customer_process [ event_type ] = func 
~~ def keystone ( * arg ) : 
check_event_type ( Openstack . Keystone , * arg ) 
keystone_customer_process_wildcard [ event_type_pattern ] = func 
~~~ keystone_customer_process [ event_type ] = func 
~~ def heat ( * arg ) : 
check_event_type ( Openstack . Heat , * arg ) 
heat_customer_process_wildcard [ event_type_pattern ] = func 
~~~ heat_customer_process [ event_type ] = func 
~~ def enqueue ( self , s ) : 
self . _parts . append ( s ) 
self . _len += len ( s ) 
~~ def dequeue ( self , n ) : 
if self . _len < n : 
~~ self . _len -= n 
def part_generator ( n ) : 
remaining = n 
while remaining : 
~~~ part = self . _parts . popleft ( ) 
if len ( part ) <= remaining : 
~~~ yield part 
remaining -= len ( part ) 
~~~ yield part [ : remaining ] 
self . _parts . appendleft ( part [ remaining : ] ) 
remaining = 0 
~~ ~~ ~~ return "" . join ( part_generator ( n ) ) 
~~ def drop ( self , n ) : 
~~~ remaining -= len ( part ) 
~~~ self . _parts . appendleft ( part [ remaining : ] ) 
~~ ~~ ~~ def peek ( self , n ) : 
~~ def part_generator ( n ) : 
for part in self . _parts : 
~~~ if len ( part ) <= remaining : 
~~ if remaining == 0 : 
~~ def make_federation_entity ( config , eid = '' , httpcli = None , verify_ssl = True ) : 
if not eid : 
~~~ eid = config [ 'entity_id' ] 
~~ ~~ if 'self_signer' in config : 
~~~ self_signer = make_internal_signing_service ( config [ 'self_signer' ] , 
eid ) 
args [ 'self_signer' ] = self_signer 
~~~ bundle_cnf = config [ 'fo_bundle' ] 
~~~ _args = dict ( [ ( k , v ) for k , v in bundle_cnf . items ( ) if k in KJ_SPECS ] ) 
if _args : 
~~~ _kj = init_key_jar ( ** _args ) 
~~~ _kj = None 
~~ if 'dir' in bundle_cnf : 
~~~ jb = FSJWKSBundle ( eid , _kj , bundle_cnf [ 'dir' ] , 
key_conv = { 'to' : quote_plus , 'from' : unquote_plus } ) 
~~~ jb = JWKSBundle ( eid , _kj ) 
~~ args [ 'fo_bundle' ] = jb 
~~ for item in [ 'context' , 'entity_id' , 'fo_priority' , 'mds_owner' ] : 
~~~ args [ item ] = config [ item ] 
~~ ~~ if 'entity_id' not in args : 
~~~ args [ 'entity_id' ] = eid 
~~ if 'sms_dir' in config : 
~~~ args [ 'sms_dir' ] = config [ 'sms_dir' ] 
return FederationEntityOOB ( httpcli , iss = eid , ** args ) 
~~ elif 'mds_service' in config : 
~~~ args [ 'verify_ssl' ] = verify_ssl 
args [ 'mds_service' ] = config [ 'mds_service' ] 
return FederationEntityAMS ( httpcli , iss = eid , ** args ) 
~~ elif 'mdss_endpoint' in config : 
for key in [ 'mdss_endpoint' , 'mdss_owner' , 'mdss_keys' ] : 
~~~ args [ key ] = config [ key ] 
~~ return FederationEntitySwamid ( httpcli , iss = eid , ** args ) 
~~ ~~ def pick_signed_metadata_statements_regex ( self , pattern , context ) : 
comp_pat = re . compile ( pattern ) 
sms_dict = self . signer . metadata_statements [ context ] 
res = [ ] 
for iss , vals in sms_dict . items ( ) : 
~~~ if comp_pat . search ( iss ) : 
~~~ res . extend ( ( iss , vals ) ) 
~~ def pick_signed_metadata_statements ( self , fo , context ) : 
~~~ if iss == fo : 
~~ def get_metadata_statement ( self , input , cls = MetadataStatement , 
context = '' ) : 
if isinstance ( input , dict ) : 
~~~ data = input 
~~~ if isinstance ( input , Message ) : 
~~~ data = input . to_dict ( ) 
~~~ data = json . loads ( input ) 
~~ ~~ _pi = self . unpack_metadata_statement ( ms_dict = data , cls = cls ) 
if not _pi . result : 
~~~ return [ ] 
~~~ _cms = self . correct_usage ( _pi . result , context ) 
~~~ _cms = _pi . result 
if _cms : 
~~~ return self . evaluate_metadata_statement ( _cms ) 
~~ ~~ def self_sign ( self , req , receiver = '' , aud = None ) : 
if self . entity_id : 
~~~ _iss = self . entity_id 
~~~ _iss = self . iss 
~~ creq = req . copy ( ) 
if not 'metadata_statement_uris' in creq and not 'metadata_statements' in creq : 
~~~ _copy = creq . copy ( ) 
_jws = self . self_signer . sign ( _copy , receiver = receiver , iss = _iss , 
aud = aud ) 
sms_spec = { 'metadata_statements' : { self . iss : _jws } } 
~~~ for ref in [ 'metadata_statement_uris' , 'metadata_statements' ] : 
~~~ del creq [ ref ] 
~~ ~~ sms_spec = { 'metadata_statements' : Message ( ) } 
for ref in [ 'metadata_statement_uris' , 'metadata_statements' ] : 
~~~ if ref not in req : 
~~ for foid , value in req [ ref ] . items ( ) : 
_copy [ ref ] = Message ( ) 
_copy [ ref ] [ foid ] = value 
_jws = self . self_signer . sign ( _copy , receiver = receiver , 
iss = _iss , aud = aud ) 
sms_spec [ 'metadata_statements' ] [ foid ] = _jws 
~~ ~~ ~~ creq . update ( sms_spec ) 
return creq 
~~ def update_metadata_statement ( self , metadata_statement , receiver = '' , 
federation = None , context = '' ) : 
self . add_sms_spec_to_request ( metadata_statement , federation = federation , 
context = context ) 
self . add_signing_keys ( metadata_statement ) 
metadata_statement = self . self_sign ( metadata_statement , receiver ) 
del metadata_statement [ 'signing_keys' ] 
return metadata_statement 
~~ def add_sms_spec_to_request ( self , req , federation = '' , loes = None , 
~~~ if isinstance ( federation , list ) : 
~~~ req . update ( self . gather_metadata_statements ( federation , 
context = context ) ) 
~~~ req . update ( self . gather_metadata_statements ( [ federation ] , 
~~~ if loes : 
~~~ _fos = list ( [ r . fo for r in loes ] ) 
req . update ( self . gather_metadata_statements ( _fos , 
~~~ req . update ( self . gather_metadata_statements ( context = context ) ) 
~~ ~~ return req 
~~ def gather_metadata_statements ( self , fos = None , context = '' ) : 
if not context : 
~~~ context = self . context 
~~ _res = { } 
if self . metadata_statements : 
~~~ cms = self . metadata_statements [ context ] 
~~~ if self . metadata_statements == { 
'register' : { } , 
'discovery' : { } , 
'response' : { } 
} : 
context ) ) 
raise ValueError ( \ . format ( context ) ) 
~~~ if cms != { } : 
~~~ if fos is None : 
~~~ fos = list ( cms . keys ( ) ) 
~~ for f in fos : 
~~~ val = cms [ f ] 
~~ if val . startswith ( 'http' ) : 
~~~ value_type = 'metadata_statement_uris' 
~~~ value_type = 'metadata_statements' 
~~~ _res [ value_type ] [ f ] = val 
~~~ _res [ value_type ] = Message ( ) 
_res [ value_type ] [ f ] = val 
~~ ~~ ~~ ~~ ~~ return _res 
context = '' , url = '' ) : 
if federation : 
~~~ if not isinstance ( federation , list ) : 
~~~ federation = [ federation ] 
~~ ~~ if not url : 
~~~ url = "{}/getms/{}/{}" . format ( self . mds_service , context , 
quote_plus ( self . entity_id ) ) 
~~ http_resp = self . httpcli ( method = 'GET' , url = url , verify = self . verify_ssl ) 
if http_resp . status_code >= 400 : 
~~ msg = JsonWebToken ( ) . from_jwt ( http_resp . text , 
keyjar = self . jwks_bundle [ self . mds_owner ] ) 
if msg [ 'iss' ] != self . mds_owner : 
~~ if federation : 
~~~ _ms = dict ( 
[ ( fo , _ms ) for fo , _ms in msg . items ( ) if fo in federation ] ) 
~~~ _ms = msg . extra ( ) 
~~~ del _ms [ 'kid' ] 
~~ ~~ _sms = { } 
_smsu = { } 
for fo , item in _ms . items ( ) : 
~~~ if item . startswith ( 'https://' ) or item . startswith ( 'http://' ) : 
~~~ _smsu [ fo ] = item 
~~~ _sms [ fo ] = item 
~~ ~~ if _sms : 
~~~ req . update ( { 'metadata_statements' : _sms } ) 
~~ if _smsu : 
~~~ req . update ( { 'metadata_statement_uris' : _smsu } ) 
~~~ url = "{}/getsmscol/{}/{}" . format ( self . mdss_endpoint , context , 
~~ msg = JsonWebToken ( ) . from_jwt ( http_resp . text , keyjar = self . mdss_keys ) 
if msg [ 'iss' ] != self . mdss_owner : 
~~~ _sms = dict ( 
~~~ _sms = msg . extra ( ) 
~~~ del _sms [ 'kid' ] 
~~ ~~ req . update ( { 'metadata_statement_uris' : _sms } ) 
return req 
~~ def pretty_print ( input_word , anagrams , by_length = False ) : 
scores = { } 
if by_length : 
~~~ noun = "tiles" 
for word , score in anagrams : 
~~~ noun = "points" 
~~~ scores [ score ] . append ( word ) 
~~~ scores [ score ] = [ word ] 
if not valid_scrabble_word ( input_word ) : 
~~ for key , value in sorted ( scores . items ( ) , reverse = True ) : 
~~ ~~ def argument_parser ( args ) : 
parser = argparse . ArgumentParser ( 
prog = "nagaram" , 
formatter_class = argparse . RawDescriptionHelpFormatter , 
add_help = False , 
"-h" , "--help" , 
dest = "help" , 
action = "store_true" , 
default = False , 
"--sowpods" , 
dest = "sowpods" , 
"--length" , 
"-l" , 
dest = "length" , 
"--starts-with" , 
"-s" , 
dest = "starts_with" , 
metavar = "chars" , 
default = "" , 
nargs = 1 , 
type = str , 
"--ends-with" , 
"-e" , 
dest = "ends_with" , 
"--version" , 
"-v" , 
action = "version" , 
nagaram . __version__ , 
nagaram . __release_date__ , 
dest = "wordlist" , 
nargs = argparse . REMAINDER , 
settings = parser . parse_args ( args ) 
if settings . help : 
~~~ raise SystemExit ( nagaram . __doc__ . strip ( ) ) 
~~ if not settings . wordlist : 
~~~ raise SystemExit ( parser . print_usage ( ) ) 
~~ if settings . starts_with : 
~~~ settings . starts_with = settings . starts_with [ 0 ] 
~~ if settings . ends_with : 
~~~ settings . ends_with = settings . ends_with [ 0 ] 
~~ return ( settings . wordlist , settings . sowpods , settings . length , 
settings . starts_with , settings . ends_with ) 
~~ def main ( arguments = None ) : 
if not arguments : 
~~~ arguments = sys . argv [ 1 : ] 
~~ wordlist , sowpods , by_length , start , end = argument_parser ( arguments ) 
for word in wordlist : 
~~~ pretty_print ( 
word , 
anagrams_in_word ( word , sowpods , start , end ) , 
by_length , 
~~ ~~ def register_type ( self , typename ) : 
typekey = typehash ( typename ) 
if typekey in self . _type_register : 
~~ self . _type_register [ typekey ] = typename 
~~ def dataReceived ( self , data ) : 
self . _unprocessed_data . enqueue ( data ) 
~~~ if len ( self . _unprocessed_data ) < self . _header . size : 
~~ hdr_data = self . _unprocessed_data . peek ( self . _header . size ) 
packet_length , typekey = self . _header . unpack ( hdr_data ) 
total_length = self . _header . size + packet_length 
if len ( self . _unprocessed_data ) < total_length : 
~~ self . _unprocessed_data . drop ( self . _header . size ) 
packet = self . _unprocessed_data . dequeue ( packet_length ) 
self . _start_receive = None 
typename = self . _type_register . get ( typekey , None ) 
if typename is None : 
~~~ self . on_unregistered_type ( typekey , packet ) 
~~~ self . packet_received ( typename , packet ) 
~~ ~~ ~~ def send_packet ( self , typename , packet ) : 
if typename != self . _type_register . get ( typekey , None ) : 
~~ hdr = self . _header . pack ( len ( packet ) , typekey ) 
self . transport . writeSequence ( [ hdr , packet ] ) 
~~ def on_unregistered_type ( self , typekey , packet ) : 
self . transport . loseConnection ( ) 
~~ def create_tcp_rpc_system ( hostname = None , port_range = ( 0 , ) , ping_interval = 1 , ping_timeout = 0.5 ) : 
def ownid_factory ( listeningport ) : 
~~~ port = listeningport . getHost ( ) . port 
return "%s:%s" % ( hostname , port ) 
~~ def make_client_endpoint ( peer ) : 
~~~ host , port = peer . split ( ":" ) 
if host == socket . getfqdn ( ) : 
~~~ host = "localhost" 
~~ return endpoints . TCP4ClientEndpoint ( reactor , host , int ( port ) , timeout = 5 ) 
~~ if hostname is None : 
~~~ hostname = socket . getfqdn ( ) 
~~ server_endpointA = TCP4ServerRangeEndpoint ( reactor , port_range ) 
pool = connectionpool . ConnectionPool ( server_endpointA , make_client_endpoint , ownid_factory ) 
return RPCSystem ( pool , ping_interval = ping_interval , ping_timeout = ping_timeout ) 
~~ def open ( self ) : 
d = self . _connectionpool . open ( self . _packet_received ) 
def opened ( _ ) : 
self . _opened = True 
self . _ping_loop . start ( self . _ping_interval , now = False ) 
~~ d . addCallback ( opened ) 
self . _ping_loop . stop ( ) 
if self . _ping_current_iteration : 
~~~ self . _ping_current_iteration . cancel ( ) 
~~ return self . _connectionpool . close ( ) 
~~ def get_function_url ( self , function ) : 
logging . debug ( "get_function_url(%s)" % repr ( function ) ) 
if function in ~ self . _functions : 
~~~ functionid = self . _functions [ : function ] 
~~~ functionid = uuid . uuid1 ( ) 
self . _functions [ functionid ] = function 
~~ return "anycall://%s/functions/%s" % ( self . _connectionpool . ownid , functionid . hex ) 
~~ def create_function_stub ( self , url ) : 
logging . debug ( "create_function_stub(%s)" % repr ( url ) ) 
parseresult = urlparse . urlparse ( url ) 
scheme = parseresult . scheme 
path = parseresult . path . split ( "/" ) 
if scheme != "anycall" : 
~~ if len ( path ) != 3 or path [ 0 ] != "" or path [ 1 ] != "functions" : 
~~~ functionid = uuid . UUID ( path [ 2 ] ) 
~~ return _RPCFunctionStub ( parseresult . netloc , functionid , self ) 
~~ def _ping_loop_iteration ( self ) : 
deferredList = [ ] 
for peerid , callid in list ( self . _local_to_remote ) : 
~~~ if ( peerid , callid ) not in self . _local_to_remote : 
d = self . _invoke_function ( peerid , self . _PING , ( self . _connectionpool . ownid , callid ) , { } ) 
def failed ( failure ) : 
~~~ if ( peerid , callid ) in self . _local_to_remote : 
~~~ d = self . _local_to_remote . pop ( ( peerid , callid ) ) 
d . errback ( failure ) 
~~ ~~ def success ( value ) : 
return value 
~~ d . addCallbacks ( success , failed ) 
deferredList . append ( d ) 
~~ d = defer . DeferredList ( deferredList ) 
def done ( _ ) : 
~~~ self . _ping_current_iteration = None 
~~ self . _ping_current_iteration = d 
d . addBoth ( done ) 
~~ def _ping ( self , peerid , callid ) : 
if not ( peerid , callid ) in self . _remote_to_local : 
~~ ~~ def unfurl ( jwt ) : 
_rp_jwt = factory ( jwt ) 
return json . loads ( _rp_jwt . jwt . part [ 1 ] . decode ( 'utf8' ) ) 
~~ def keyjar_from_metadata_statements ( iss , msl ) : 
keyjar = KeyJar ( ) 
for ms in msl : 
~~~ keyjar . import_jwks ( ms [ 'signing_keys' ] , iss ) 
~~ return keyjar 
~~ def read_jwks_file ( jwks_file ) : 
_jwks = open ( jwks_file , 'r' ) . read ( ) 
_kj = KeyJar ( ) 
_kj . import_jwks ( json . loads ( _jwks ) , '' ) 
return _kj 
~~ def is_lesser ( a , b ) : 
if type ( a ) != type ( b ) : 
~~ if isinstance ( a , str ) and isinstance ( b , str ) : 
~~~ return a == b 
~~ elif isinstance ( a , bool ) and isinstance ( b , bool ) : 
~~ elif isinstance ( a , list ) and isinstance ( b , list ) : 
~~~ for element in a : 
~~~ flag = 0 
for e in b : 
~~~ if is_lesser ( element , e ) : 
~~~ flag = 1 
~~ ~~ if not flag : 
~~ elif isinstance ( a , dict ) and isinstance ( b , dict ) : 
~~~ if is_lesser ( list ( a . keys ( ) ) , list ( b . keys ( ) ) ) : 
~~~ for key , val in a . items ( ) : 
~~~ if not is_lesser ( val , b [ key ] ) : 
~~ elif isinstance ( a , int ) and isinstance ( b , int ) : 
~~~ return a <= b 
~~ elif isinstance ( a , float ) and isinstance ( b , float ) : 
~~ def verify ( self , ** kwargs ) : 
super ( MetadataStatement , self ) . verify ( ** kwargs ) 
if "signing_keys" in self : 
~~~ if 'signing_keys_uri' in self : 
~~~ raise VerificationError ( 
\ 
\ ) 
~~~ kj = KeyJar ( ) 
~~~ kj . import_jwks ( self [ 'signing_keys' ] , '' ) 
~~~ raise VerificationError ( \ ) 
~~ ~~ ~~ if "metadata_statements" in self and "metadata_statement_uris" in self : 
~~~ s = set ( self [ 'metadata_statements' ] . keys ( ) ) 
t = set ( self [ 'metadata_statement_uris' ] . keys ( ) ) 
if s . intersection ( t ) : 
~~ def _parse_remote_response ( self , response ) : 
~~~ if response . headers [ "Content-Type" ] == 'application/json' : 
~~~ return json . loads ( response . text ) 
~~ ~~ elif response . headers [ "Content-Type" ] == 'application/jwt' : 
_jws = factory ( response . text ) 
_resp = _jws . verify_compact ( 
response . text , keys = self . verify_keys . get_signing_key ( ) ) 
return _resp 
response . headers [ 'Content-Type' ] ) ) 
~~ ~~ except KeyError : 
~~ ~~ def login_required ( function = None , redirect_field_name = REDIRECT_FIELD_NAME , 
login_url = None ) : 
actual_decorator = request_passes_test ( 
lambda r : r . session . get ( 'user_token' ) , 
login_url = login_url , 
redirect_field_name = redirect_field_name 
if function : 
~~~ return actual_decorator ( function ) 
~~ return actual_decorator 
~~ def permission_required ( function = None , permission = None , object_id = None , 
redirect_field_name = REDIRECT_FIELD_NAME , 
~~ def tokens_required ( service_list ) : 
~~~ @ wraps ( func ) 
def inner ( request , * args , ** kwargs ) : 
~~~ for service in service_list : 
~~~ if service not in request . session [ "user_tokens" ] : 
~~~ return redirect ( 'denied' ) 
~~ ~~ return func ( request , * args , ** kwargs ) 
~~ def login ( request , template_name = 'ci/login.html' , 
authentication_form = AuthenticationForm ) : 
redirect_to = request . POST . get ( redirect_field_name , 
request . GET . get ( redirect_field_name , '' ) ) 
if request . method == "POST" : 
~~~ form = authentication_form ( request , data = request . POST ) 
if form . is_valid ( ) : 
~~~ if not is_safe_url ( url = redirect_to , host = request . get_host ( ) ) : 
~~~ redirect_to = resolve_url ( settings . LOGIN_REDIRECT_URL ) 
~~ user = form . get_user ( ) 
request . session [ 'user_token' ] = user [ "token" ] 
request . session [ 'user_email' ] = user [ "email" ] 
request . session [ 'user_permissions' ] = user [ "permissions" ] 
request . session [ 'user_id' ] = user [ "id" ] 
request . session [ 'user_list' ] = user [ "user_list" ] 
if not settings . HIDE_DASHBOARDS : 
~~~ dashboards = ciApi . get_user_dashboards ( user [ "id" ] ) 
dashboard_list = list ( dashboards [ 'results' ] ) 
if len ( dashboard_list ) > 0 : 
~~~ request . session [ 'user_dashboards' ] = dashboard_list [ 0 ] [ "dashboards" ] 
request . session [ 'user_default_dashboard' ] = dashboard_list [ 0 ] [ "default_dashboard" ] [ "id" ] 
~~~ request . session [ 'user_dashboards' ] = [ ] 
request . session [ 'user_default_dashboard' ] = None 
~~ ~~ tokens = ciApi . get_user_service_tokens ( 
params = { "user_id" : user [ "id" ] } ) 
token_list = list ( tokens [ 'results' ] ) 
user_tokens = { } 
if len ( token_list ) > 0 : 
~~~ for token in token_list : 
~~~ user_tokens [ token [ "service" ] [ "name" ] ] = { 
"token" : token [ "token" ] , 
"url" : token [ "service" ] [ "url" ] + "/api/v1" 
~~ ~~ request . session [ 'user_tokens' ] = user_tokens 
return HttpResponseRedirect ( redirect_to ) 
~~~ form = authentication_form ( request ) 
~~ current_site = get_current_site ( request ) 
context = { 
'form' : form , 
redirect_field_name : redirect_to , 
'site' : current_site , 
'site_name' : current_site . name , 
return TemplateResponse ( request , template_name , context ) 
~~ def build ( cli , path , package ) : 
for _ , name , ispkg in iter_modules ( path ) : 
~~~ module = import_module ( f'.{name}' , package ) 
if ispkg : 
~~~ build ( cli . group ( name ) ( module . group ) , 
module . __path__ , 
module . __package__ ) 
~~~ cli . command ( name ) ( module . command ) 
~~ ~~ ~~ def self_sign_jwks ( keyjar , iss , kid = '' , lifetime = 3600 ) : 
_jwt = JWT ( keyjar , iss = iss , lifetime = lifetime ) 
jwks = keyjar . export_jwks ( issuer = iss ) 
return _jwt . pack ( payload = { 'jwks' : jwks } , owner = iss , kid = kid ) 
~~ def verify_self_signed_jwks ( sjwt ) : 
_jws = factory ( sjwt ) 
_json = _jws . jwt . part [ 1 ] 
_body = json . loads ( as_unicode ( _json ) ) 
iss = _body [ 'iss' ] 
_jwks = _body [ 'jwks' ] 
_kj = jwks_to_keyjar ( _jwks , iss ) 
~~~ _kid = _jws . jwt . headers [ 'kid' ] 
~~~ _keys = _kj . get_signing_key ( owner = iss ) 
~~~ _keys = _kj . get_signing_key ( owner = iss , kid = _kid ) 
~~ _ver = _jws . verify_compact ( sjwt , _keys ) 
return { 'jwks' : _ver [ 'jwks' ] , 'iss' : iss } 
~~ def request_signed_by_signing_keys ( keyjar , msreq , iss , lifetime , kid = '' ) : 
~~~ jwks_to_keyjar ( msreq [ 'signing_keys' ] , iss ) 
~~~ jwks = keyjar . export_jwks ( issuer = iss ) 
msreq [ 'signing_keys' ] = jwks 
~~ _jwt = JWT ( keyjar , iss = iss , lifetime = lifetime ) 
return _jwt . pack ( owner = iss , kid = kid , payload = msreq . to_dict ( ) ) 
~~ def verify_request_signed_by_signing_keys ( smsreq ) : 
_jws = factory ( smsreq ) 
_jwks = _body [ 'signing_keys' ] 
~~ _ver = _jws . verify_compact ( smsreq , _keys ) 
for k in JsonWebToken . c_param . keys ( ) : 
~~~ del _ver [ k ] 
~~~ del _ver [ 'kid' ] 
~~ return { 'ms' : MetadataStatement ( ** _ver ) , 'iss' : iss } 
~~ def letter_score ( letter ) : 
score_map = { 
1 : [ "a" , "e" , "i" , "o" , "u" , "l" , "n" , "r" , "s" , "t" ] , 
2 : [ "d" , "g" ] , 
3 : [ "b" , "c" , "m" , "p" ] , 
4 : [ "f" , "h" , "v" , "w" , "y" ] , 
5 : [ "k" ] , 
8 : [ "j" , "x" ] , 
10 : [ "q" , "z" ] , 
for score , letters in score_map . items ( ) : 
~~~ if letter . lower ( ) in letters : 
~~~ return score 
~~ ~~ def word_score ( word , input_letters , questions = 0 ) : 
score = 0 
bingo = 0 
filled_by_blanks = [ ] 
~~~ if letter in rack : 
~~~ bingo += 1 
score += letter_score ( letter ) 
rack . remove ( letter ) 
~~~ filled_by_blanks . append ( letter_score ( letter ) ) 
~~ ~~ for blank_score in sorted ( filled_by_blanks , reverse = True ) : 
~~~ if questions > 0 : 
~~~ score += blank_score 
questions -= 1 
~~ ~~ if bingo > 6 : 
~~~ score += 50 
~~ return score 
~~ def blank_tiles ( input_word ) : 
blanks = 0 
questions = 0 
input_letters = [ ] 
for letter in input_word : 
~~~ if letter == "_" : 
~~~ blanks += 1 
~~ elif letter == "?" : 
~~~ questions += 1 
~~~ input_letters . append ( letter ) 
~~ ~~ return input_letters , blanks , questions 
~~ def word_list ( sowpods = False , start = "" , end = "" ) : 
location = os . path . join ( 
os . path . dirname ( os . path . realpath ( __file__ ) ) , 
"wordlists" , 
if sowpods : 
~~~ filename = "sowpods.txt" 
~~~ filename = "twl.txt" 
~~ filepath = os . path . join ( location , filename ) 
with open ( filepath ) as wordfile : 
~~~ for word in wordfile . readlines ( ) : 
~~~ word = word . strip ( ) 
if start and end and word . startswith ( start ) and word . endswith ( end ) : 
~~~ yield word 
~~ elif start and word . startswith ( start ) and not end : 
~~ elif end and word . endswith ( end ) and not start : 
~~ elif not start and not end : 
~~ ~~ ~~ ~~ def valid_scrabble_word ( word ) : 
letters_in_bag = { 
"a" : 9 , 
"b" : 2 , 
"c" : 2 , 
"d" : 4 , 
"e" : 12 , 
"f" : 2 , 
"g" : 3 , 
"h" : 2 , 
"i" : 9 , 
"j" : 1 , 
"k" : 1 , 
"l" : 4 , 
"m" : 2 , 
"n" : 6 , 
"o" : 8 , 
"p" : 2 , 
"q" : 1 , 
"r" : 6 , 
"s" : 4 , 
"t" : 6 , 
"u" : 4 , 
"v" : 2 , 
"w" : 2 , 
"x" : 1 , 
"y" : 2 , 
"z" : 1 , 
"_" : 2 , 
~~~ if letter == "?" : 
~~~ letters_in_bag [ letter ] -= 1 
~~ if letters_in_bag [ letter ] < 0 : 
~~~ letters_in_bag [ "_" ] -= 1 
if letters_in_bag [ "_" ] < 0 : 
~~ ~~ ~~ return True 
~~ def get_bundle ( iss , ver_keys , bundle_file ) : 
fp = open ( bundle_file , 'r' ) 
signed_bundle = fp . read ( ) 
fp . close ( ) 
return JWKSBundle ( iss , None ) . upload_signed_bundle ( signed_bundle , ver_keys ) 
~~ def get_signing_keys ( eid , keydef , key_file ) : 
if os . path . isfile ( key_file ) : 
kj . import_jwks ( json . loads ( open ( key_file , 'r' ) . read ( ) ) , eid ) 
~~~ kj = build_keyjar ( keydef ) [ 1 ] 
fp = open ( key_file , 'w' ) 
fp . write ( json . dumps ( kj . export_jwks ( ) ) ) 
kj . issuer_keys [ eid ] = kj . issuer_keys [ '' ] 
~~ return kj 
~~ def jwks_to_keyjar ( jwks , iss = '' ) : 
if not isinstance ( jwks , dict ) : 
~~~ jwks = json . loads ( jwks ) 
~~ except json . JSONDecodeError : 
~~ ~~ kj = KeyJar ( ) 
kj . import_jwks ( jwks , issuer = iss ) 
return kj 
~~ def create_signed_bundle ( self , sign_alg = 'RS256' , iss_list = None ) : 
data = self . dict ( iss_list ) 
_jwt = JWT ( self . sign_keys , iss = self . iss , sign_alg = sign_alg ) 
return _jwt . pack ( { 'bundle' : data } ) 
~~ def loads ( self , jstr ) : 
if isinstance ( jstr , dict ) : 
~~~ _info = jstr 
~~~ _info = json . loads ( jstr ) 
~~ for iss , jwks in _info . items ( ) : 
if isinstance ( jwks , dict ) : 
~~~ kj . import_jwks ( jwks , issuer = iss ) 
~~~ kj . import_jwks_as_json ( jwks , issuer = iss ) 
~~ self . bundle [ iss ] = kj 
~~ def dict ( self , iss_list = None ) : 
_int = { } 
for iss , kj in self . bundle . items ( ) : 
~~~ if iss_list is None or iss in iss_list : 
~~~ _int [ iss ] = kj . export_jwks_as_json ( issuer = iss ) 
~~~ _int [ iss ] = kj . export_jwks_as_json ( ) 
~~ ~~ ~~ return _int 
~~ def upload_signed_bundle ( self , sign_bundle , ver_keys ) : 
jwt = verify_signed_bundle ( sign_bundle , ver_keys ) 
self . loads ( jwt [ 'bundle' ] ) 
~~ def as_keyjar ( self ) : 
kj = KeyJar ( ) 
for iss , k in self . bundle . items ( ) : 
~~~ kj . issuer_keys [ iss ] = k . issuer_keys [ iss ] 
~~~ kj . issuer_keys [ iss ] = k . issuer_keys [ '' ] 
~~ ~~ return kj 
~~ def nova_process ( body , message ) : 
event_type = body [ 'event_type' ] 
process = nova_customer_process . get ( event_type ) 
if process is not None : 
~~~ process ( body , message ) 
~~~ matched = False 
process_wildcard = None 
for pattern in nova_customer_process_wildcard . keys ( ) : 
~~~ if pattern . match ( event_type ) : 
~~~ process_wildcard = nova_customer_process_wildcard . get ( pattern ) 
matched = True 
~~ ~~ if matched : 
~~~ process_wildcard ( body , message ) 
~~~ default_process ( body , message ) 
~~ ~~ message . ack ( ) 
~~ def cinder_process ( body , message ) : 
process = cinder_customer_process . get ( event_type ) 
for pattern in cinder_customer_process_wildcard . keys ( ) : 
~~~ process_wildcard = cinder_customer_process_wildcard . get ( pattern ) 
~~ def neutron_process ( body , message ) : 
process = neutron_customer_process . get ( event_type ) 
for pattern in neutron_customer_process_wildcard . keys ( ) : 
~~~ process_wildcard = neutron_customer_process_wildcard . get ( pattern ) 
~~ def glance_process ( body , message ) : 
process = glance_customer_process . get ( event_type ) 
for pattern in glance_customer_process_wildcard . keys ( ) : 
~~~ process_wildcard = glance_customer_process_wildcard . get ( pattern ) 
~~ def swift_process ( body , message ) : 
process = swift_customer_process . get ( event_type ) 
for pattern in swift_customer_process_wildcard . keys ( ) : 
~~~ process_wildcard = swift_customer_process_wildcard . get ( pattern ) 
~~ def keystone_process ( body , message ) : 
process = keystone_customer_process . get ( event_type ) 
for pattern in keystone_customer_process_wildcard . keys ( ) : 
~~~ process_wildcard = keystone_customer_process_wildcard . get ( pattern ) 
~~ def heat_process ( body , message ) : 
process = heat_customer_process . get ( event_type ) 
for pattern in heat_customer_process_wildcard . keys ( ) : 
~~~ process_wildcard = heat_customer_process_wildcard . get ( pattern ) 
~~ def serve ( self , server = None ) : 
if server is None : 
~~~ from wsgiref . simple_server import make_server 
server = lambda app : make_server ( '' , 8000 , app ) . serve_forever ( ) 
~~~ server ( self ) 
~~~ server . socket . close ( ) 
~~ ~~ def work ( self ) : 
self . init_modules ( ) 
connection = self . init_mq ( ) 
TernyaConnection ( self , connection ) . connect ( ) 
~~ def init_mq ( self ) : 
mq = self . init_connection ( ) 
self . init_consumer ( mq ) 
return mq . connection 
~~ def init_modules ( self ) : 
if not self . config : 
modules = ServiceModules ( self . config ) 
modules . import_modules ( ) 
~~ def init_nova_consumer ( self , mq ) : 
if not self . enable_component_notification ( Openstack . Nova ) : 
~~ for i in range ( self . config . nova_mq_consumer_count ) : 
~~~ mq . create_consumer ( self . config . nova_mq_exchange , 
self . config . nova_mq_queue , 
ProcessFactory . process ( Openstack . Nova ) ) 
~~ def init_cinder_consumer ( self , mq ) : 
if not self . enable_component_notification ( Openstack . Cinder ) : 
~~ for i in range ( self . config . cinder_mq_consumer_count ) : 
~~~ mq . create_consumer ( self . config . cinder_mq_exchange , 
self . config . cinder_mq_queue , 
ProcessFactory . process ( Openstack . Cinder ) ) 
~~ def init_neutron_consumer ( self , mq ) : 
if not self . enable_component_notification ( Openstack . Neutron ) : 
~~ for i in range ( self . config . neutron_mq_consumer_count ) : 
~~~ mq . create_consumer ( self . config . neutron_mq_exchange , 
self . config . neutron_mq_queue , 
ProcessFactory . process ( Openstack . Neutron ) ) 
~~ def init_glance_consumer ( self , mq ) : 
if not self . enable_component_notification ( Openstack . Glance ) : 
~~ for i in range ( self . config . glance_mq_consumer_count ) : 
~~~ mq . create_consumer ( self . config . glance_mq_exchange , 
self . config . glance_mq_queue , 
ProcessFactory . process ( Openstack . Glance ) ) 
~~ def init_swift_consumer ( self , mq ) : 
if not self . enable_component_notification ( Openstack . Swift ) : 
~~ for i in range ( self . config . swift_mq_consumer_count ) : 
~~~ mq . create_consumer ( self . config . swift_mq_exchange , 
self . config . swift_mq_queue , 
ProcessFactory . process ( Openstack . Swift ) ) 
~~ def init_keystone_consumer ( self , mq ) : 
if not self . enable_component_notification ( Openstack . Keystone ) : 
~~ for i in range ( self . config . keystone_mq_consumer_count ) : 
~~~ mq . create_consumer ( self . config . keystone_mq_exchange , 
self . config . keystone_mq_queue , 
ProcessFactory . process ( Openstack . Keystone ) ) 
~~ def init_heat_consumer ( self , mq ) : 
if not self . enable_component_notification ( Openstack . Heat ) : 
~~ for i in range ( self . config . heat_mq_consumer_count ) : 
~~~ mq . create_consumer ( self . config . heat_mq_exchange , 
self . config . heat_mq_queue , 
ProcessFactory . process ( Openstack . Heat ) ) 
~~ def enable_component_notification ( self , openstack_component ) : 
openstack_component_mapping = { 
Openstack . Nova : self . config . listen_nova_notification , 
Openstack . Cinder : self . config . listen_cinder_notification , 
Openstack . Neutron : self . config . listen_neutron_notification , 
Openstack . Glance : self . config . listen_glance_notification , 
Openstack . Swift : self . config . listen_swift_notification , 
Openstack . Keystone : self . config . listen_keystone_notification , 
Openstack . Heat : self . config . listen_heat_notification 
return openstack_component_mapping [ openstack_component ] 
~~ def execute ( self , globals_ = None , _locals = None ) : 
if globals_ is None : 
~~~ globals_ = globals ( ) 
~~ if _locals is None : 
~~~ self . _locals = globals_ 
~~~ self . _locals = _locals 
~~ self . globals_ = globals_ 
if self . contains_op ( "YIELD_VALUE" ) : 
~~~ return self . iterate_instructions ( ) 
~~~ return self . execute_instructions ( ) 
~~ ~~ def load_name ( self , name ) : 
if name in self . globals_ : 
~~~ return self . globals_ [ name ] 
~~ b = self . globals_ [ '__builtins__' ] 
if isinstance ( b , dict ) : 
~~~ return b [ name ] 
~~~ return getattr ( b , name ) 
~~ ~~ def pop ( self , n ) : 
poped = self . __stack [ len ( self . __stack ) - n : ] 
del self . __stack [ len ( self . __stack ) - n : ] 
return poped 
~~ def build_class ( self , callable_ , args ) : 
self . _print ( 'build_class' ) 
self . _print ( callable_ ) 
self . _print ( 'args=' , args ) 
if isinstance ( args [ 0 ] , FunctionType ) : 
~~~ c = args [ 0 ] . get_code ( ) 
~~~ c = args [ 0 ] . __closure__ [ 0 ] . cell_contents . __code__ 
machine = MachineClassSource ( c , self . verbose ) 
l = dict ( ) 
machine . execute ( self . globals_ , l ) 
a = Assembler ( ) 
for name , value in l . items ( ) : 
~~~ a . load_const ( value ) 
a . store_name ( name ) 
~~ a . load_const ( None ) 
a . return_value ( ) 
dis . dis ( a . code ( ) ) 
f = types . FunctionType ( a . code ( ) , self . globals_ , args [ 1 ] ) 
args = ( f , * args [ 1 : ] ) 
self . call_callbacks ( 'CALL_FUNCTION' , callable_ , * args ) 
return callable_ ( * args ) 
~~ def call_function ( self , c , i ) : 
callable_ = self . __stack [ - 1 - i . arg ] 
args = tuple ( self . __stack [ len ( self . __stack ) - i . arg : ] ) 
if isinstance ( callable_ , FunctionType ) : 
~~~ ret = callable_ ( * args ) 
~~ elif callable_ is builtins . __build_class__ : 
~~~ ret = self . build_class ( callable_ , args ) 
~~ elif callable_ is builtins . globals : 
~~~ ret = self . builtins_globals ( ) 
~~ self . pop ( 1 + i . arg ) 
self . __stack . append ( ret ) 
~~ def keys ( self ) : 
self . sync ( ) 
for k in self . db . keys ( ) : 
~~~ yield self . key_conv [ 'from' ] ( k ) 
~~~ yield k 
~~ ~~ ~~ def get_mtime ( fname ) : 
~~~ mtime = os . stat ( fname ) . st_mtime_ns 
~~ except OSError : 
~~~ time . sleep ( 1 ) 
mtime = os . stat ( fname ) . st_mtime_ns 
~~ return mtime 
~~ def is_changed ( self , item ) : 
fname = os . path . join ( self . fdir , item ) 
if os . path . isfile ( fname ) : 
~~~ mtime = self . get_mtime ( fname ) 
~~~ _ftime = self . fmtime [ item ] 
~~~ self . fmtime [ item ] = mtime 
raise KeyError ( item ) 
~~ ~~ def sync ( self ) : 
if not os . path . isdir ( self . fdir ) : 
~~~ os . makedirs ( self . fdir ) 
~~ for f in os . listdir ( self . fdir ) : 
~~~ fname = os . path . join ( self . fdir , f ) 
if not os . path . isfile ( fname ) : 
~~ if f in self . fmtime : 
~~~ if self . is_changed ( f ) : 
~~~ self . db [ f ] = self . _read_info ( fname ) 
self . db [ f ] = self . _read_info ( fname ) 
self . fmtime [ f ] = mtime 
~~ ~~ ~~ def items ( self ) : 
for k , v in self . db . items ( ) : 
~~~ yield self . key_conv [ 'from' ] ( k ) , v 
~~~ yield k , v 
~~ ~~ ~~ def clear ( self ) : 
~~~ os . makedirs ( self . fdir , exist_ok = True ) 
~~~ del self [ f ] 
~~ ~~ def update ( self , ava ) : 
for key , val in ava . items ( ) : 
~~~ self [ key ] = val 
~~ ~~ async def api_bikes ( request ) : 
postcode : Optional [ str ] = request . match_info . get ( 'postcode' , None ) 
~~~ radius = int ( request . match_info . get ( 'radius' , 10 ) ) 
~~~ postcode = ( await get_postcode_random ( ) ) if postcode == "random" else postcode 
bikes = await get_bikes ( postcode , radius ) 
~~ except CachingError as e : 
~~~ raise web . HTTPInternalServerError ( text = e . status ) 
~~~ if bikes is None : 
~~~ return str_json_response ( [ bike . serialize ( ) for bike in bikes ] ) 
~~ ~~ ~~ def fancy_tag_compiler ( params , defaults , takes_var_args , takes_var_kwargs , takes_context , name , node_class , parser , token ) : 
bits = token . split_contents ( ) [ 1 : ] 
if takes_context : 
~~~ if 'context' in params [ : 1 ] : 
~~~ params = params [ 1 : ] 
~~~ raise TemplateSyntaxError ( 
~~ ~~ args = [ ] 
kwarg_found = False 
unhandled_params = list ( params ) 
handled_params = [ ] 
if len ( bits ) > 1 and bits [ - 2 ] == 'as' : 
~~~ output_var = bits [ - 1 ] 
if len ( set ( output_var ) - set ( ALLOWED_VARIABLE_CHARS ) ) > 0 : 
~~ bits = bits [ : - 2 ] 
~~~ output_var = None 
~~ for bit in bits : 
~~~ kwarg_match = kwarg_re . match ( bit ) 
if kwarg_match : 
~~~ kw , var = kwarg_match . groups ( ) 
if kw not in params and not takes_var_kwargs : 
~~ elif kw in handled_params : 
~~~ kwargs [ str ( kw ) ] = var 
kwarg_found = True 
handled_params . append ( kw ) 
~~~ if kwarg_found : 
~~~ args . append ( bit ) 
~~~ handled_params . append ( unhandled_params . pop ( 0 ) ) 
~~~ if not takes_var_args : 
~~ ~~ ~~ ~~ ~~ if defaults is not None : 
~~~ unhandled_params = unhandled_params [ : - len ( defaults ) ] 
~~ if len ( unhandled_params ) == 1 : 
~~ elif len ( unhandled_params ) > 1 : 
~~ return node_class ( args , kwargs , output_var , takes_context ) 
~~ def setup_logging ( log_filename = None , log_level = "DEBUG" , str_format = None , 
date_format = None , log_file_level = "DEBUG" , 
log_stdout_level = None , log_restart = False , log_history = False , 
formatter = None , silence_modules = None , log_filter = None ) : 
setup_log_level ( log_level ) 
if log_filename : 
~~~ setup_file_logging ( log_filename = log_filename , str_format = str_format , 
log_history = log_history , formatter = formatter , 
log_file_level = log_file_level , 
log_restart = log_restart , date_format = date_format , 
log_filter = log_filter ) 
~~ if log_stdout_level is not None : 
~~~ setup_stdout_logging ( log_level = log_level , 
log_stdout_level = log_stdout_level , 
str_format = str_format , 
date_format = date_format , 
formatter = formatter , 
~~ silence_module_logging ( silence_modules ) 
~~ def setup_stdout_logging ( log_level = "DEBUG" , log_stdout_level = "DEBUG" , 
str_format = None , date_format = None , formatter = None , 
silence_modules = None , log_filter = None ) : 
formatter = formatter or SeabornFormatter ( str_format , date_format ) 
if log_stdout_level != 'ERROR' : 
~~~ stdout_handler = logging . StreamHandler ( sys . __stdout__ ) 
add_handler ( log_stdout_level , stdout_handler , 
formatter , NoErrorFilter ( ) ) 
~~ stderr_handler = logging . StreamHandler ( sys . __stderr__ ) 
add_handler ( "ERROR" , stderr_handler , formatter , log_filter ) 
silence_module_logging ( silence_modules ) 
~~ def setup_file_logging ( log_filename , log_file_level = "DEBUG" , str_format = None , 
date_format = None , log_restart = False , log_history = False , 
from seaborn_timestamp . timestamp import datetime_to_str 
if os . path . exists ( log_filename ) and log_restart : 
~~~ os . remove ( log_filename ) 
~~ add_file_handler ( log_file_level , log_filename , str_format = str_format , 
date_format = date_format , formatter = formatter , 
if log_history : 
~~~ base_name = os . path . basename ( log_filename ) . split ( '.' ) [ 0 ] + '_%s' % datetime_to_str ( str_format = '%Y-%m-%d_%H-%M-%S' ) 
history_log = os . path . join ( os . path . dirname ( log_filename ) , 
'history' , base_name + '.log' ) 
add_file_handler ( log_file_level , history_log , str_format = str_format , 
date_format = date_format , log_filter = log_filter ) 
~~ def add_file_handler ( log_file_level , log_filename , str_format = None , 
date_format = None , formatter = None , log_filter = None ) : 
formatter = formatter or SeabornFormatter ( str_format = str_format , 
date_format = date_format ) 
mkdir_for_file ( log_filename ) 
handler = logging . FileHandler ( log_filename ) 
add_handler ( log_file_level , handler , formatter , log_filter = log_filter ) 
~~ def add_handler ( log_handler_level , handler , formatter = None , log_filter = None ) : 
handler . setLevel ( log_handler_level ) 
if formatter is not None : 
~~~ handler . setFormatter ( formatter ) 
~~ if log_filter is not None : 
~~~ handler . addFilter ( log_filter ) 
~~ log . addHandler ( handler ) 
~~ def set_module_log_level ( modules = None , log_level = logging . WARNING ) : 
modules = modules or [ ] 
if not isinstance ( modules , list ) : 
~~~ modules = [ modules ] 
~~ for module in modules : 
~~~ logging . getLogger ( module ) . setLevel ( logging . WARNING ) 
~~ ~~ def findCaller ( self , stack_info = False ) : 
f = logging . currentframe ( ) 
if f is not None : 
~~~ f = f . f_back 
while hasattr ( f , "f_code" ) : 
~~~ co = f . f_code 
filename = os . path . normcase ( co . co_filename ) 
if filename == logging . _srcfile or filename == self . _srcfile : 
~~ rv = ( co . co_filename , f . f_lineno , co . co_name ) 
if stack_info : 
~~~ sio = io . StringIO ( ) 
traceback . print_stack ( f , file = sio ) 
sinfo = sio . getvalue ( ) 
if sinfo [ - 1 ] == '\\n' : 
~~~ sinfo = sinfo [ : - 1 ] 
~~ sio . close ( ) 
~~ break 
~~ return rv 
~~ def default_malformed_message_handler ( worker , exc_info , message_parts ) : 
exc_type , exc , tb = exc_info 
exc_strs = traceback . format_exception_only ( exc_type , exc ) 
exc_str = exc_strs [ 0 ] . strip ( ) 
if len ( exc_strs ) > 1 : 
~~~ exc_str += '...' 
~~ def work ( self , socket , call , args , kwargs , topics = ( ) ) : 
task_id = uuid4_bytes ( ) 
reply_socket , topics = self . replier ( socket , topics , call . reply_to ) 
if reply_socket : 
~~~ channel = ( call . call_id , task_id , topics ) 
~~~ channel = ( None , None , None ) 
~~ f , rpc_spec = self . find_call_target ( call ) 
if rpc_spec . reject_if . __get__ ( self . app ) ( call , topics ) : 
~~~ reply_socket and self . reject ( reply_socket , call . call_id , topics ) 
~~ reply_socket and self . accept ( reply_socket , channel ) 
success = False 
with self . catch_exceptions ( ) : 
~~~ val = self . call ( call , args , kwargs , f , rpc_spec ) 
~~~ exc_info = sys . exc_info ( ) 
self . raise_ ( reply_socket , channel , exc_info ) 
reraise ( * exc_info ) 
~~ success = True 
~~ if not success : 
~~ if isinstance ( val , Iterator ) : 
~~~ vals = val 
~~~ val = next ( vals ) 
~~~ self . send_reply ( reply_socket , YIELD , val , * channel ) 
for val in vals : 
~~ ~~ self . send_reply ( reply_socket , BREAK , None , * channel ) 
~~~ self . send_reply ( reply_socket , RETURN , val , * channel ) 
~~ ~~ def accept ( self , reply_socket , channel ) : 
info = self . info or b'' 
self . send_raw ( reply_socket , ACCEPT , info , * channel ) 
~~ def reject ( self , reply_socket , call_id , topics = ( ) ) : 
self . send_raw ( reply_socket , REJECT , info , call_id , b'' , topics ) 
~~ def raise_ ( self , reply_socket , channel , exc_info = None ) : 
if not reply_socket : 
~~ if exc_info is None : 
~~ exc_type , exc , tb = exc_info 
while tb . tb_next is not None : 
~~~ tb = tb . tb_next 
~~ if issubclass ( exc_type , RemoteException ) : 
~~~ exc_type = exc_type . exc_type 
~~ filename , lineno = tb . tb_frame . f_code . co_filename , tb . tb_lineno 
val = ( exc_type , str ( exc ) , filename , lineno ) 
~~~ state = exc . __getstate__ ( ) 
~~~ val += ( state , ) 
~~ self . send_reply ( reply_socket , RAISE , val , * channel ) 
~~ def _call_wait ( self , hints , name , args , kwargs , topics = ( ) , raw = False , 
limit = None , retry = False , max_retries = None ) : 
col = self . collector 
if not col . is_running ( ) : 
~~~ col . start ( ) 
~~ call_id = uuid4_bytes ( ) 
reply_to = ( DUPLEX if self . socket is col . socket else col . topic ) 
header = self . _make_header ( name , call_id , reply_to , hints ) 
payload = self . _pack ( args , kwargs , raw ) 
def send_call ( ) : 
~~~ safe ( send , self . socket , header , payload , topics , zmq . NOBLOCK ) 
~~ except zmq . Again : 
~~ ~~ col . prepare ( call_id , self , name , args , kwargs ) 
send_call ( ) 
return col . establish ( call_id , self . timeout , limit , 
send_call if retry else None , 
max_retries = max_retries ) 
~~ def establish ( self , call_id , timeout , limit = None , 
retry = None , max_retries = None ) : 
rejected = 0 
retried = 0 
results = [ ] 
result_queue = self . result_queues [ call_id ] 
~~~ with Timeout ( timeout , False ) : 
~~~ while True : 
~~~ result = result_queue . get ( ) 
if result is None : 
~~~ rejected += 1 
if retry is not None : 
~~~ if retried == max_retries : 
~~ retry ( ) 
retried += 1 
~~ continue 
~~ results . append ( result ) 
if len ( results ) == limit : 
~~ ~~ ~~ ~~ finally : 
~~~ del result_queue 
self . remove_result_queue ( call_id ) 
~~ if not results : 
~~~ if rejected : 
if rejected != 1 else 
~~ ~~ return results 
~~ def dispatch_reply ( self , reply , value ) : 
method = reply . method 
call_id = reply . call_id 
task_id = reply . task_id 
if method & ACK : 
~~~ result_queue = self . result_queues [ call_id ] 
~~ if method == ACCEPT : 
~~~ worker_info = value 
result = RemoteResult ( self , call_id , task_id , worker_info ) 
self . results [ call_id ] [ task_id ] = result 
result_queue . put_nowait ( result ) 
~~ elif method == REJECT : 
~~~ result_queue . put_nowait ( None ) 
~~~ result = self . results [ call_id ] [ task_id ] 
result . set_reply ( reply . method , value ) 
for file_name in self . files : 
~~~ if "~" in file_name : 
~~~ file_name = os . path . expanduser ( file_name ) 
~~ if not os . path . isfile ( file_name ) : 
~~~ print ( 
"Don\ 
+ Back . BLACK 
+ Fore . YELLOW 
+ file_name 
~~ with open ( os . path . expanduser ( file_name ) , "r" ) as file : 
~~~ for line in file : 
~~~ if line and "=" in line : 
if len ( possible ) > 4 and possible not in self . false_positives : 
~~~ self . secrets . append ( possible ) 
count = 0 
here = os . path . abspath ( self . source ) 
matches = [ ] 
for root , dirnames , filenames in os . walk ( here + "/" ) : 
~~~ for filename in filenames : 
~~~ matches . append ( os . path . join ( root , filename ) ) 
~~ ~~ for file in matches : 
~~~ if os . path . isdir ( file ) : 
~~ with open ( file ) as f : 
~~~ contents = f . read ( ) 
~~~ print ( e ) 
print ( file ) 
~~ ~~ for secret in self . secrets : 
~~~ if secret in contents : 
~~~ for line in contents . split ( "\\n" ) : 
~~~ if secret in line : 
~~~ self . found . setdefault ( file , [ ] ) . append ( 
secret , 
line . replace ( 
Fore . RED 
+ Back . YELLOW 
+ secret 
+ Style . RESET_ALL , 
count += 1 
logger . debug ( arguments ) 
if arguments [ "here" ] : 
~~~ go ( ) 
~~~ files = arguments [ "--secrets" ] 
searcher = Searcher ( source = arguments [ "--source" ] , files = files ) 
searcher . go ( ) 
~~ ~~ async def api_postcode ( request ) : 
~~~ coroutine = get_postcode_random ( ) if postcode == "random" else get_postcode ( postcode ) 
postcode : Optional [ Postcode ] = await coroutine 
~~~ return web . HTTPInternalServerError ( body = e . status ) 
~~ except CircuitBreakerError as e : 
~~~ if postcode is not None : 
~~~ return str_json_response ( postcode . serialize ( ) ) 
~~ ~~ ~~ async def api_nearby ( request ) : 
~~~ limit = int ( request . match_info . get ( 'limit' , 10 ) ) 
~~~ raise web . HTTPInternalServerError ( body = e . status ) 
~~ if postcode is None : 
~~~ nearby_items = await fetch_nearby ( postcode . lat , postcode . long , limit ) 
~~ except ApiError : 
~~ if nearby_items is None : 
~~~ return str_json_response ( nearby_items ) 
~~ ~~ def skip_module ( * modules ) : 
modules = ( modules and isinstance ( modules [ 0 ] , list ) ) and modules [ 0 ] or modules 
for module in modules : 
~~~ if not module in SKIPPED_MODULES : 
~~~ SKIPPED_MODULES . append ( module ) 
~~ ~~ traceback . extract_tb = _new_extract_tb 
~~ def only_module ( * modules ) : 
~~~ if not module in ONLY_MODULES : 
~~~ ONLY_MODULES . append ( module ) 
~~ def skip_path ( * paths ) : 
paths = ( paths and isinstance ( paths [ 0 ] , list ) ) and paths [ 0 ] or paths 
for path in paths : 
~~~ if not path in SKIPPED_PATHS : 
~~~ SKIPPED_PATHS . append ( path ) 
~~ async def fetch_twitter ( handle : str ) -> List : 
async with ClientSession ( ) as session : 
~~~ async with session . get ( f"http://twitrss.me/twitter_user_to_rss/?user={handle}" ) as request : 
~~~ text = await request . text ( ) 
~~ ~~ except ClientConnectionError as con_err : 
~~~ feed = parse ( text ) 
for x in feed . entries : 
~~~ x [ "image" ] = feed . feed [ "image" ] [ "href" ] 
~~ return feed . entries 
~~ ~~ ~~ async def fetch_nearby ( lat : float , long : float , limit : int = 10 ) -> Optional [ List [ Dict ] ] : 
request_url = f"https://en.wikipedia.org/w/api.php?action=query" f"&list=geosearch" f"&gscoord={lat}%7C{long}" f"&gsradius=10000" f"&gslimit={limit}" f"&format=json" 
~~~ async with session . get ( request_url ) as request : 
~~~ if request . status == 404 : 
~~ data = ( await request . json ( ) ) [ "query" ] [ "geosearch" ] 
~~ except JSONDecodeError as dec_err : 
~~~ for location in data : 
~~~ location . pop ( "ns" ) 
location . pop ( "primary" ) 
~~ return data 
~~~ completed = subprocess . run ( 
command , 
check = True , 
shell = True , 
stdout = subprocess . PIPE , 
stderr = subprocess . PIPE 
~~ except subprocess . CalledProcessError as err : 
~~~ print ( completed . stdout . decode ( 'utf-8' ) + str ( ":" ) + completed . stderr . decode ( "utf-8" ) ) 
return completed . stdout . decode ( 'utf-8' ) + completed . stderr . decode ( "utf-8" ) 
~~ ~~ def has_source_code_tree_changed ( self ) : 
global CURRENT_HASH 
directory = self . where 
CURRENT_HASH = dirhash ( directory , 'md5' , ignore_hidden = True , 
excluded_files = [ ".coverage" , "lint.txt" ] , 
excluded_extensions = [ ".pyc" ] 
if os . path . isfile ( self . state_file_name ) : 
~~~ with open ( self . state_file_name , "r+" ) as file : 
~~~ last_hash = file . read ( ) 
if last_hash != CURRENT_HASH : 
~~~ file . seek ( 0 ) 
file . write ( CURRENT_HASH ) 
file . truncate ( ) 
~~ ~~ ~~ with open ( self . state_file_name , "w" ) as file : 
~~~ file . write ( CURRENT_HASH ) 
~~ ~~ def check_pypi_name ( pypi_package_name , pypi_registry_host = None ) : 
if pypi_registry_host is None : 
~~~ pypi_registry_host = 'pypi.python.org' 
~~ receive_buffer = bytearray ( b'------------' ) 
context = ssl . create_default_context ( ) 
ssl_http_socket = context . wrap_socket ( socket . socket ( socket . AF_INET ) , server_hostname = pypi_registry_host ) 
ssl_http_socket . connect ( ( pypi_registry_host , 443 ) ) 
ssl_http_socket . send ( b'' . join ( [ 
b"\\r\\n\\r\\n" 
] ) ) 
ssl_http_socket . recv_into ( receive_buffer ) 
~~~ ssl_http_socket . shutdown ( 1 ) 
ssl_http_socket . close ( ) 
return False 
~~ remaining_bytes = ssl_http_socket . recv ( 2048 ) 
redirect_path_location_start = remaining_bytes . find ( b'Location:' ) + 10 
redirect_path_location_end = remaining_bytes . find ( b'\\r\\n' , redirect_path_location_start ) 
redirect_path = remaining_bytes [ redirect_path_location_start : redirect_path_location_end ] + b'/' 
ssl_http_socket . shutdown ( 1 ) 
b"\\r\\n\\r\\n" ] ) ) 
~~~ return True 
~~ ~~ async def fetch_postcodes_from_coordinates ( lat : float , long : float ) -> Optional [ List [ Postcode ] ] : 
postcode_lookup = f"/postcodes?lat={lat}&lon={long}" 
return await _get_postcode_from_url ( postcode_lookup ) 
~~ async def fetch_bikes ( ) -> List [ dict ] : 
~~~ async with session . get ( 'https://www.bikeregister.com/stolen-bikes' ) as request : 
~~~ document = document_fromstring ( await request . text ( ) ) 
~~ token = document . xpath ( "//input[@name=\ ) 
if len ( token ) != 1 : 
~~~ raise ApiError ( f"Couldn\ ) 
~~~ token = token [ 0 ] . value 
~~ xsrf_token = request . cookies [ "XSRF-TOKEN" ] 
laravel_session = request . cookies [ "laravel_session" ] 
headers = { 
'origin' : 'https://www.bikeregister.com' , 
'accept-language' : 'en-GB,en-US;q=0.9,en;q=0.8' , 
'accept' : '*/*' , 
'referer' : 'https://www.bikeregister.com/stolen-bikes' , 
'authority' : 'www.bikeregister.com' , 
'x-requested-with' : 'XMLHttpRequest' , 
data = [ 
( '_token' , token ) , 
( 'make' , '' ) , 
( 'model' , '' ) , 
( 'colour' , '' ) , 
( 'reporting_period' , '1' ) , 
~~~ async with session . post ( 'https://www.bikeregister.com/stolen-bikes' , headers = headers , data = data ) as request : 
~~~ bikes = json . loads ( await request . text ( ) ) 
~~ except json . JSONDecodeError as dec_err : 
~~ return bikes 
~~ def wrap ( cls , message_parts ) : 
~~ except BaseException as exception : 
~~~ __ , __ , tb = sys . exc_info ( ) 
reraise ( cls , cls ( exception , message_parts ) , tb ) 
~~ ~~ async def api_crime ( request ) : 
~~~ return web . Response ( body = e . status , status = 500 ) 
~~~ crime = await fetch_crime ( postcode . lat , postcode . long ) 
~~ except ( ApiError , CircuitBreakerError ) : 
~~ if crime is None : 
~~~ return str_json_response ( crime ) 
~~ ~~ async def api_neighbourhood ( request ) : 
neighbourhood = await get_neighbourhood ( postcode ) 
~~ if neighbourhood is None : 
~~~ return str_json_response ( neighbourhood . serialize ( ) ) 
~~ ~~ def set_remote_exception ( self , remote_exc_info ) : 
exc_type , exc_str , filename , lineno = remote_exc_info [ : 4 ] 
exc_type = RemoteException . compose ( exc_type ) 
exc = exc_type ( exc_str , filename , lineno , self . worker_info ) 
if len ( remote_exc_info ) > 4 : 
~~~ state = remote_exc_info [ 4 ] 
exc . __setstate__ ( state ) 
~~ self . set_exception ( exc ) 
~~ def get_most_recent_bike ( ) -> Optional [ 'Bike' ] : 
~~~ return Bike . select ( ) . order_by ( Bike . cached_date . desc ( ) ) . get ( ) 
~~ except pw . DoesNotExist : 
~~ ~~ def save ( variable , filename ) : 
fileObj = open ( filename , 'wb' ) 
pickle . dump ( variable , fileObj ) 
fileObj . close ( ) 
~~ def load ( filename ) : 
fileObj = open ( filename , 'rb' ) 
variable = pickle . load ( fileObj ) 
return variable 
~~ def main ( ) : 
if args . verbose : 
~~~ verbose = 2 
pprint ( args ) 
~~~ verbose = 0 
~~ query = args . query [ 0 ] 
for arg in args . query [ 1 : ] : 
files = Files ( path = args . path , filetype = args . filetype [ 0 ] , exclude = [ ] , update = args . update , verbose = verbose ) 
index = Index ( files , slb = slb , verbose = verbose ) 
results = index . search ( query , verbose = verbose ) 
Handler ( results , results_number = int ( args . results ) ) 
~~ def search ( self , query , verbose = 0 ) : 
if verbose > 0 : 
~~ query = query . lower ( ) 
qgram = ng ( query , self . slb ) 
qocument = set ( ) 
for q in qgram : 
~~~ if q in self . ngrams . keys ( ) : 
~~~ for i in self . ngrams [ q ] : 
~~~ qocument . add ( i ) 
~~ ~~ ~~ self . qocument = qocument 
results = { } 
for i in qocument : 
~~~ for j in self . D [ i ] . keys ( ) : 
~~~ if not j in results . keys ( ) : 
~~~ results [ j ] = 0 
~~ results [ j ] = results [ j ] + self . D [ i ] [ j ] 
~~ ~~ sorted_results = sorted ( results . items ( ) , key = operator . itemgetter ( 1 ) , reverse = True ) 
return [ self . elements [ f [ 0 ] ] for f in sorted_results ] 
~~ def partition ( condition , collection ) -> Tuple [ List , List ] : 
succeed , fail = [ ] , [ ] 
for x in collection : 
~~~ if condition ( x ) : 
~~~ succeed . append ( x ) 
~~~ fail . append ( x ) 
~~ ~~ return succeed , fail 
~~ async def cli ( location_strings : Tuple [ str ] , random_postcodes_count : int , * , 
bikes : bool = False , crime : bool = False , 
nearby : bool = False , as_json : bool = False ) : 
def match_getter ( location ) -> Optional [ PostcodeGetter ] : 
~~~ for getter in getters : 
~~~ if getter . can_provide ( location ) : 
~~~ return getter ( location ) 
~~ ~~ async def handle_getter ( exception_list , getter ) : 
~~~ return await getter . get_postcodes ( ) 
~~ except ( CachingError , ApiError ) : 
~~ ~~ async def handle_datas ( exception_list , postcode ) : 
~~~ postcode_data , new_exceptions = await get_postcode_data ( postcode , bikes , crime , nearby ) 
exception_list += new_exceptions 
return postcode_data 
~~ exception_list : List [ Exception ] = [ ] 
handle_getter = partial ( handle_getter , exception_list ) 
handle_datas = partial ( handle_datas , exception_list ) 
postcode_getters = { location : match_getter ( location ) for location in 
set ( location_strings ) | ( { random_postcodes_count } if random_postcodes_count > 0 else set ( ) ) } 
matched , unmatched = partition ( lambda k_v : k_v [ 1 ] is not None , postcode_getters . items ( ) ) 
for location , getter in unmatched : 
~~ postcodes_collection = [ await handle_getter ( getter ) for location , getter in matched ] 
if len ( exception_list ) > 0 : 
~~~ for f in exception_list : 
~~~ echo ( str ( f ) ) 
~~ return 1 
~~ postcode_datas = [ await handle_datas ( postcode ) for entry in postcodes_collection for postcode in entry ] 
serializer = ( PostcodeSerializerJSON if as_json else PostcodeSerializerHuman ) ( postcode_datas ) 
echo ( serializer . serialize ( ) ) 
~~ async def fetch_neighbourhood ( lat : float , long : float ) -> Optional [ dict ] : 
lookup_url = f"https://data.police.uk/api/locate-neighbourhood?q={lat},{long}" 
~~~ async with session . get ( lookup_url ) as request : 
~~ neighbourhood = await request . json ( ) 
~~ neighbourhood_url = f"https://data.police.uk/api/{neighbourhood[\ 
~~~ async with session . get ( neighbourhood_url ) as request : 
~~~ neighbourhood_data = await request . json ( ) 
~~ ~~ except ConnectionError as con_err : 
~~ return neighbourhood_data 
~~ ~~ async def fetch_crime ( lat : float , long : float ) -> List [ Dict ] : 
crime_lookup = f"https://data.police.uk/api/crimes-street/all-crime?lat={lat}&lng={long}" 
~~~ async with session . get ( crime_lookup ) as request : 
~~~ crime_request = await request . json ( ) 
~~~ return crime_request 
~~ ~~ ~~ def run ( locations , random , bikes , crime , nearby , json , update_bikes , api_server , cross_origin , host , port , db_path , 
verbose ) : 
log_levels = [ logging . WARNING , logging . INFO , logging . DEBUG ] 
logging . basicConfig ( level = log_levels [ min ( verbose , 2 ) ] ) 
initialize_database ( db_path ) 
loop = get_event_loop ( ) 
if update_bikes : 
loop . run_until_complete ( util . update_bikes ( ) ) 
~~ if api_server : 
~~~ if cross_origin : 
~~~ enable_cross_origin ( app ) 
~~~ web . run_app ( app , host = host , port = port ) 
~~ except CancelledError as e : 
~~~ if e . __context__ is not None : 
~~~ click . echo ( Fore . RED + ( 
exit ( 1 ) 
~~~ click . echo ( "Goodbye!" ) 
~~ ~~ ~~ elif len ( locations ) > 0 or random > 0 : 
~~~ exit ( loop . run_until_complete ( cli ( locations , random , bikes = bikes , crime = crime , nearby = nearby , as_json = json ) ) ) 
~~ ~~ async def api_twitter ( request ) : 
handle = request . match_info . get ( 'handle' , None ) 
if handle is None : 
~~~ posts = await fetch_twitter ( handle ) 
~~ except ApiError as e : 
~~ return str_json_response ( posts ) 
~~ def send ( socket , header , payload , topics = ( ) , flags = 0 ) : 
msgs = [ ] 
msgs . extend ( topics ) 
msgs . append ( SEAM ) 
msgs . extend ( header ) 
msgs . append ( payload ) 
return eintr_retry_zmq ( socket . send_multipart , msgs , flags ) 
~~ def recv ( socket , flags = 0 , capture = ( lambda msgs : None ) ) : 
msgs = eintr_retry_zmq ( socket . recv_multipart , flags ) 
capture ( msgs ) 
return parse ( msgs ) 
~~ def dead_code ( ) : 
with safe_cd ( SRC ) : 
~~~ if IS_TRAVIS : 
~~ output_file_name = "dead_code.txt" 
with open ( output_file_name , "w" ) as outfile : 
~~~ env = config_pythonpath ( ) 
subprocess . call ( command , stdout = outfile , env = env ) 
~~ cutoff = 20 
num_lines = sum ( 1 for line in open ( output_file_name ) if line ) 
if num_lines > cutoff : 
exit ( - 1 ) 
~~ ~~ ~~ def rpc ( f = None , ** kwargs ) : 
~~~ if isinstance ( f , six . string_types ) : 
~~~ if 'name' in kwargs : 
~~ kwargs [ 'name' ] = f 
~~~ return rpc ( ** kwargs ) ( f ) 
~~ ~~ return functools . partial ( _rpc , ** kwargs ) 
~~ def rpc_spec_table ( app ) : 
table = { } 
for attr , value in inspect . getmembers ( app ) : 
~~~ rpc_spec = get_rpc_spec ( value , default = None ) 
if rpc_spec is None : 
~~ table [ rpc_spec . name ] = ( value , rpc_spec ) 
~~ return table 
~~ async def normalize_postcode_middleware ( request , handler ) : 
if postcode is None or postcode == "random" : 
~~~ return await handler ( request ) 
~~ elif not is_uk_postcode ( postcode ) : 
if postcode_processed == postcode : 
~~~ url_name = request . match_info . route . name 
url = request . app . router [ url_name ] 
params = dict ( request . match_info ) 
params [ 'postcode' ] = postcode_processed 
raise web . HTTPMovedPermanently ( str ( url . url_for ( ** params ) ) ) 
~~ ~~ def make_repr ( obj , params = None , keywords = None , data = None , name = None , 
reprs = None ) : 
opts = [ ] 
if params is not None : 
_repr_attr ( obj , attr , data , reprs ) for attr in params ) ) 
~~ if keywords is not None : 
'%s=%s' % ( attr , _repr_attr ( obj , attr , data , reprs ) ) 
for attr in keywords ) ) 
~~ if name is None : 
~~~ name = class_name ( obj ) 
~~ def eintr_retry ( exc_type , f , * args , ** kwargs ) : 
~~~ return f ( * args , ** kwargs ) 
~~ except exc_type as exc : 
~~~ if exc . errno != EINTR : 
~~ ~~ ~~ def eintr_retry_zmq ( f , * args , ** kwargs ) : 
return eintr_retry ( zmq . ZMQError , f , * args , ** kwargs ) 
~~ async def update_bikes ( delta : Optional [ timedelta ] = None ) : 
async def update ( delta : timedelta ) : 
if await should_update_bikes ( delta ) : 
~~~ bike_data = await fetch_bikes ( ) 
~~ except CircuitBreakerError : 
~~~ most_recent_bike = Bike . get_most_recent_bike ( ) 
new_bikes = ( 
Bike . from_dict ( bike ) for index , bike in enumerate ( bike_data ) 
if index > ( most_recent_bike . id if most_recent_bike is not None else - 1 ) 
counter = 0 
with Bike . _meta . database . atomic ( ) : 
~~~ for bike in new_bikes : 
~~~ bike . save ( ) 
counter += 1 
~~ ~~ if delta is None : 
~~~ await update ( timedelta ( days = 1000 ) ) 
~~~ await update ( delta ) 
await asyncio . sleep ( delta . total_seconds ( ) ) 
~~ ~~ ~~ async def should_update_bikes ( delta : timedelta ) : 
bike = Bike . get_most_recent_bike ( ) 
if bike is not None : 
~~~ return bike . cached_date < datetime . now ( ) - delta 
~~ ~~ async def get_bikes ( postcode : PostCodeLike , kilometers = 1 ) -> Optional [ List [ Bike ] ] : 
~~~ postcode_opt = await get_postcode ( postcode ) 
~~ if postcode_opt is None : 
~~~ postcode = postcode_opt 
~~ center = Point ( postcode . lat , postcode . long ) 
distance = geodesic ( kilometers = kilometers ) 
lat_end = distance . destination ( point = center , bearing = 0 ) . latitude 
lat_start = distance . destination ( point = center , bearing = 180 ) . latitude 
long_start = distance . destination ( point = center , bearing = 270 ) . longitude 
long_end = distance . destination ( point = center , bearing = 90 ) . longitude 
bikes_in_area = Bike . select ( ) . where ( 
lat_start <= Bike . latitude , 
Bike . latitude <= lat_end , 
long_start <= Bike . longitude , 
Bike . longitude <= long_end 
bike for bike in bikes_in_area 
if geodesic ( Point ( bike . latitude , bike . longitude ) , center ) . kilometers < kilometers 
~~ async def get_postcode_random ( ) -> Postcode : 
~~~ postcode = await fetch_postcode_random ( ) 
~~ if postcode is not None : 
~~~ postcode . save ( ) 
~~ return postcode 
~~ async def get_postcode ( postcode_like : PostCodeLike ) -> Optional [ Postcode ] : 
if isinstance ( postcode_like , Postcode ) : 
~~~ return postcode_like 
~~~ postcode = Postcode . get ( Postcode . postcode == postcode_like ) 
~~ except DoesNotExist : 
~~~ postcode = await fetch_postcode_from_string ( postcode_like ) 
~~ ~~ return postcode 
~~ async def get_neighbourhood ( postcode_like : PostCodeLike ) -> Optional [ Neighbourhood ] : 
~~~ postcode = await get_postcode ( postcode_like ) 
~~~ if postcode is None : 
~~ elif postcode . neighbourhood is not None : 
~~~ return postcode . neighbourhood 
~~~ data = await fetch_neighbourhood ( postcode . lat , postcode . long ) 
~~ if data is not None : 
~~~ neighbourhood = Neighbourhood . from_dict ( data ) 
locations = [ Location . from_dict ( neighbourhood , postcode , location ) for location in data [ "locations" ] ] 
links = [ Link . from_dict ( neighbourhood , link ) for link in data [ "links" ] ] 
with Neighbourhood . _meta . database . atomic ( ) : 
~~~ neighbourhood . save ( ) 
postcode . neighbourhood = neighbourhood 
postcode . save ( ) 
for location in locations : 
~~~ location . save ( ) 
~~ for link in links : 
~~~ link . save ( ) 
~~~ neighbourhood = None 
~~ return neighbourhood 
~~ def t_NAME ( t ) : 
~~~ r'[A-Za-z_][A-Za-z0-9_]*' 
if t . value . upper ( ) in reserved : 
~~~ t . type = t . value . upper ( ) 
t . value = t . value . upper ( ) 
~~ return t 
~~ def t_STRING ( t ) : 
~~~ r"\ 
t . value = t . value . replace ( r'\\\\' , chr ( 92 ) ) . replace ( r"\\\ , r"\ ) [ 1 : - 1 ] 
return t 
~~ def p_postpositions ( p ) : 
if len ( p ) > 2 : 
~~~ if p [ 1 ] == "LIMIT" : 
~~~ postposition = { 
"limit" : p [ 2 ] 
rest = p [ 3 ] if p [ 3 ] else { } 
~~ elif p [ 1 : 3 ] == [ "ORDER" , "BY" ] : 
rest = p [ 4 ] if p [ 4 ] else { } 
~~~ breakpoint ( ) 
~~ p [ 0 ] = { ** postposition , ** rest } 
~~~ p [ 0 ] = { } 
~~ ~~ def p_colspec ( p ) : 
rest = p [ 3 ] if len ( p ) > 3 else [ ] 
if p [ 1 ] == "*" : 
~~~ p [ 0 ] = [ { "type" : "star" } ] 
~~ elif isinstance ( p [ 1 ] , dict ) and p [ 1 ] . get ( "type" ) == "function" : 
~~~ p [ 0 ] = [ p [ 1 ] , * rest ] 
~~ elif p [ 1 ] : 
~~~ p [ 0 ] = [ 
{ 
"type" : "name" , 
"value" : p [ 1 ] , 
} , 
* rest , 
~~~ p [ 0 ] = [ ] 
~~ ~~ def p_expression ( p ) : 
if len ( p ) < 3 : 
~~~ p [ 0 ] = p [ 1 ] 
~~~ p [ 0 ] = { 
"op" : "not" , 
"args" : [ p [ 2 ] ] , 
~~ elif p [ 1 ] == "(" : 
~~~ p [ 0 ] = p [ 2 ] 
"op" : p [ 2 ] . lower ( ) , 
"args" : [ p [ 1 ] , p [ 3 ] ] , 
~~ ~~ def parse ( self , config_file = None , specs = None , default_file = None ) : 
self . _config_exists ( config_file ) 
self . _specs_exists ( specs ) 
self . loaded_config = anyconfig . load ( self . config_file , ac_parser = 'yaml' ) 
if default_file is not None : 
~~~ self . _merge_default ( default_file ) 
~~ if self . specs is None : 
~~~ return self . loaded_config 
~~ self . _validate ( ) 
return self . loaded_config 
~~ def table ( rows ) : 
output = '<table>' 
for row in rows : 
~~~ output += '<tr>' 
for column in row : 
~~~ output += '<td>{s}</td>' . format ( s = column ) 
~~ output += '</tr>' 
~~ output += '</table>' 
return output 
~~ def link ( url , text = '' , classes = '' , target = '' , get = "" , ** kwargs ) : 
if not ( url . startswith ( 'http' ) or url . startswith ( '/' ) ) : 
~~~ urlargs = { } 
for arg , val in kwargs . items ( ) : 
~~~ if arg [ : 4 ] == "url_" : 
~~~ urlargs [ arg [ 4 : ] ] = val 
~~ ~~ url = reverse ( url , kwargs = urlargs ) 
if get : 
~~~ url += '?' + get 
~~ ~~ return html . tag ( 'a' , text or url , { 
'class' : classes , 'target' : target , 'href' : url } ) 
~~ def jsfile ( url ) : 
if not url . startswith ( 'http://' ) and not url [ : 1 ] == '/' : 
~~~ url = settings . STATIC_URL + url 
~~ return \ . format ( 
src = url ) 
~~ def cssfile ( url ) : 
~~ return \ . format ( src = url ) 
~~ def img ( url , alt = '' , classes = '' , style = '' ) : 
~~ attr = { 
'class' : classes , 
'alt' : alt , 
'style' : style , 
'src' : url 
return html . tag ( 'img' , '' , attr ) 
~~ def sub ( value , arg ) : 
~~~ return valid_numeric ( value ) - valid_numeric ( arg ) 
~~ except ( ValueError , TypeError ) : 
~~~ return value - arg 
~~ ~~ ~~ def mul ( value , arg ) : 
~~~ return valid_numeric ( value ) * valid_numeric ( arg ) 
~~~ return value * arg 
~~ ~~ ~~ def div ( value , arg ) : 
~~~ return valid_numeric ( value ) / valid_numeric ( arg ) 
~~~ return value / arg 
~~ ~~ ~~ def mod ( value , arg ) : 
~~~ return valid_numeric ( value ) % valid_numeric ( arg ) 
~~~ return value % arg 
~~ ~~ ~~ def model_verbose ( obj , capitalize = True ) : 
if isinstance ( obj , ModelForm ) : 
~~~ name = obj . _meta . model . _meta . verbose_name 
~~ elif isinstance ( obj , Model ) : 
~~~ name = obj . _meta . verbose_name 
~~ return name . capitalize ( ) if capitalize else name 
~~ def prepare_message ( self , data = None ) : 
message = { 
'protocol' : self . protocol , 
'node' : self . _node , 
'chip_id' : self . _chip_id , 
'event' : '' , 
'parameters' : { } , 
'response' : '' , 
'targets' : [ 
'ALL' 
if type ( data ) is dict : 
~~~ for k , v in data . items ( ) : 
~~~ if k in message : 
~~~ message [ k ] = v 
~~ ~~ ~~ return message 
~~ def decode_message ( self , message ) : 
~~~ message = json . loads ( message ) 
if not self . _validate_message ( message ) : 
~~~ message = None 
~~ ~~ except ValueError : 
~~ return message 
~~ def _validate_message ( self , message ) : 
if 'protocol' not in message or 'targets' not in message or type ( message [ 'targets' ] ) is not list : 
~~ if message [ 'protocol' ] != self . protocol : 
~~ if self . node not in message [ 'targets' ] and 'ALL' not in message [ 'targets' ] : 
~~ def patterns ( prefix , * args ) : 
pattern_list = [ ] 
for t in args : 
~~~ if isinstance ( t , ( list , tuple ) ) : 
~~~ t = url ( prefix = prefix , * t ) 
~~ elif isinstance ( t , RegexURLPattern ) : 
~~~ t . add_prefix ( prefix ) 
~~ pattern_list . append ( t ) 
~~ return pattern_list 
~~ def url ( regex , view , kwargs = None , name = None , prefix = '' ) : 
if isinstance ( view , ( list , tuple ) ) : 
~~~ urlconf_module , app_name , namespace = view 
return URLResolver ( regex , urlconf_module , kwargs , app_name = app_name , namespace = namespace ) 
~~~ if isinstance ( view , six . string_types ) : 
~~~ if not view : 
~~ if prefix : 
~~~ view = prefix + '.' + view 
~~ view = get_callable ( view ) 
~~ return CBVRegexURLPattern ( regex , view , kwargs , name ) 
~~ ~~ def run_from_argv ( self , argv ) : 
parser = self . create_parser ( argv [ 0 ] , argv [ 1 ] ) 
self . arguments = parser . parse_args ( argv [ 2 : ] ) 
handle_default_options ( self . arguments ) 
options = vars ( self . arguments ) 
self . execute ( ** options ) 
~~ def create_parser ( self , prog_name , subcommand ) : 
parser = ArgumentParser ( 
description = self . description , 
epilog = self . epilog , 
add_help = self . add_help , 
prog = self . prog , 
usage = self . get_usage ( subcommand ) , 
parser . add_argument ( '--version' , action = 'version' , version = self . get_version ( ) ) 
self . add_arguments ( parser ) 
return parser 
~~ def get_member ( thing_obj , member_string ) : 
mems = { x [ 0 ] : x [ 1 ] for x in inspect . getmembers ( thing_obj ) } 
if member_string in mems : 
~~~ return mems [ member_string ] 
~~ ~~ def func_from_string ( callable_str ) : 
components = callable_str . split ( '.' ) 
func = None 
if len ( components ) < 2 : 
~~ elif len ( components ) == 2 : 
~~~ mod_name = components [ 0 ] 
func_name = components [ 1 ] 
~~~ mod = import_module ( mod_name ) 
~~ except ModuleNotFoundError : 
~~ func = get_member ( mod , func_name ) 
if func is None : 
~~~ mod_name = '.' . join ( components [ : - 1 ] ) 
func_name = components [ - 1 ] 
~~~ mod_name = '.' . join ( components [ : - 2 ] ) 
class_name = components [ - 2 ] 
~~ klass = get_member ( mod , class_name ) 
if klass is None : 
~~ func = get_member ( klass , func_name ) 
~~ ~~ if func is None : 
~~~ func = get_member ( mod , func_name ) 
~~ ~~ ~~ if inspect . ismodule ( func ) : 
~~ if inspect . isclass ( func ) : 
~~~ sig = [ x for x in inspect . signature ( func ) . parameters ] 
~~ if len ( sig ) == 1 : 
~~~ if sig [ 0 ] == 'message' : 
~~~ return func 
~~ ~~ elif len ( sig ) == 2 and sig [ 0 ] == 'self' and sig [ 1 ] == 'message' : 
~~~ raise ValueError ( "Can\ ) 
~~ ~~ def task_with_callable ( the_callable , label = None , schedule = DEFAULT_SCHEDULE , userdata = None , pk_override = None ) : 
task = Task ( ) 
if isinstance ( the_callable , str ) : 
~~~ if pk_override is not None : 
~~~ components = the_callable . split ( '.' ) 
info = dict ( 
func_type = 'instancemethod' , 
module_name = '.' . join ( components [ : - 2 ] ) , 
class_name = components [ - 2 ] , 
class_path = '.' . join ( components [ : - 1 ] ) , 
model_pk = pk_override , 
func_name = components [ - 1 ] , 
func_path = the_callable , 
task . funcinfo = info 
~~~ task . funcinfo = get_func_info ( func_from_string ( the_callable ) ) 
~~~ task . funcinfo = get_func_info ( the_callable ) 
~~ if label is None : 
~~~ task . label = task . funcinfo [ 'func_path' ] 
~~~ task . label = label 
~~ task . schedule = schedule 
if not croniter . is_valid ( task . schedule ) : 
~~ if userdata is None : 
~~~ task . userdata = dict ( ) 
~~~ if isinstance ( userdata , dict ) : 
~~~ task . userdata = userdata 
~~ ~~ return task 
~~ def taskinfo_with_label ( label ) : 
task = Task . objects . get ( label = label ) 
info = json . loads ( task . _func_info ) 
return info 
~~ def func_from_info ( self ) : 
info = self . funcinfo 
functype = info [ 'func_type' ] 
if functype in [ 'instancemethod' , 'classmethod' , 'staticmethod' ] : 
~~~ the_modelclass = get_module_member_by_dottedpath ( info [ 'class_path' ] ) 
if functype == 'instancemethod' : 
~~~ the_modelobject = the_modelclass . objects . get ( pk = info [ 'model_pk' ] ) 
the_callable = get_member ( the_modelobject , info [ 'func_name' ] ) 
~~~ the_callable = get_member ( the_modelclass , info [ 'func_name' ] ) 
~~ return the_callable 
~~ elif functype == 'function' : 
~~~ mod = import_module ( info [ 'module_name' ] ) 
the_callable = get_member ( mod , info [ 'func_name' ] ) 
return the_callable 
~~ ~~ def run_tasks ( cls ) : 
now = timezone . now ( ) 
tasks = cls . objects . filter ( enabled = True ) 
for task in tasks : 
~~~ if task . next_run == HAS_NOT_RUN : 
~~~ task . calc_next_run ( ) 
~~ if task . next_run < now : 
~~~ if ( task . start_running < now ) : 
~~~ if ( task . end_running > now ) : 
~~~ task . run_asap ( ) 
~~~ task . enabled = False 
task . save ( ) 
Channel ( KILL_TASK_CHANNEL ) . send ( { 'id' : task . pk } ) 
~~ ~~ ~~ ~~ ~~ def calc_next_run ( self ) : 
base_time = self . last_run 
if self . last_run == HAS_NOT_RUN : 
~~~ if self . wait_for_schedule is False : 
~~~ self . next_run = timezone . now ( ) 
self . save ( ) 
~~~ base_time = timezone . now ( ) 
~~ ~~ self . next_run = croniter ( self . schedule , base_time ) . get_next ( datetime ) 
~~ def submit ( self , timestamp ) : 
Channel ( RUN_TASK_CHANNEL ) . send ( { 'id' : self . pk , 'ts' : timestamp . timestamp ( ) } ) 
~~ def run ( self , message ) : 
the_callable = self . func_from_info ( ) 
~~~ task_message = dict ( 
task = self , 
channel_message = message , 
the_callable ( task_message ) 
~~~ if self . end_running < self . next_run : 
~~~ self . enabled = False 
Channel ( KILL_TASK_CHANNEL ) . send ( { 'id' : self . pk } ) 
~~ if self . iterations == 0 : 
~~~ self . iterations -= 1 
if self . iterations == 0 : 
~~ self . save ( ) 
~~ ~~ ~~ def run_asap ( self ) : 
self . last_run = now 
self . calc_next_run ( ) 
self . submit ( now ) 
task = task_with_callable ( the_callable , label = label , schedule = schedule , userdata = userdata ) 
task . iterations = iterations 
if delay_until is not None : 
~~~ if isinstance ( delay_until , datetime ) : 
~~~ if delay_until > timezone . now ( ) : 
~~~ task . start_running = delay_until 
~~ ~~ if run_immediately : 
~~~ task . next_run = timezone . now ( ) 
~~ task . save ( ) 
~~ def run_once ( cls , the_callable , userdata = None , delay_until = None ) : 
cls . run_iterations ( the_callable , userdata = userdata , run_immediately = True , delay_until = delay_until ) 
~~ def get_object_or_none ( qs , * args , ** kwargs ) : 
~~~ return qs . get ( * args , ** kwargs ) 
~~ except models . ObjectDoesNotExist : 
~~ ~~ def slices ( self , size , n = 3 ) : 
if n <= 0 : 
~~ if size < n : 
~~ slice_size = size // n 
last_slice_size = size - ( n - 1 ) * slice_size 
t = [ self ( c , slice_size ) for c in range ( 0 , ( n - 1 ) * slice_size , slice_size ) ] 
t . append ( self ( ( n - 1 ) * slice_size , last_slice_size ) ) 
~~ def seek ( self , offset , whence = 0 ) : 
if self . closed : 
~~ if whence == SEEK_SET : 
~~~ pos = offset 
~~ elif whence == SEEK_CUR : 
~~~ pos = self . pos + offset 
~~ elif whence == SEEK_END : 
~~~ pos = self . size + offset 
~~ if not 0 <= pos <= self . size : 
~~ self . pos = pos 
return self . pos 
~~ def read ( self , size = - 1 ) : 
~~ if size == - 1 or size > self . size - self . pos : 
~~~ size = self . size - self . pos 
~~ with self . lock : 
~~~ self . f . seek ( self . start + self . pos ) 
result = self . f . read ( size ) 
self . seek ( self . pos + size ) 
~~ def write ( self , b ) : 
~~ if self . pos + len ( b ) > self . size : 
result = self . f . write ( b ) 
self . seek ( self . pos + len ( b ) ) 
~~ def writelines ( self , lines ) : 
lines = b'' . join ( lines ) 
self . write ( lines ) 
~~ def user_has_group ( user , group , superuser_skip = True ) : 
if user . is_superuser and superuser_skip : 
~~ return user . groups . filter ( name = group ) . exists ( ) 
~~ def resolve_class ( class_path ) : 
modulepath , classname = class_path . rsplit ( '.' , 1 ) 
module = __import__ ( modulepath , fromlist = [ classname ] ) 
return getattr ( module , classname ) 
~~ def grad ( f , delta = DELTA ) : 
def grad_f ( * args , ** kwargs ) : 
~~~ if len ( args ) == 1 : 
~~~ x , = args 
gradf_x = ( 
f ( x + delta / 2 ) - f ( x - delta / 2 ) 
) / delta 
return gradf_x 
~~ elif len ( args ) == 2 : 
~~~ x , y = args 
if type ( x ) in [ float , int ] and type ( y ) in [ float , int ] : 
~~~ gradf_x = ( f ( x + delta / 2 , y ) - f ( x - delta / 2 , y ) ) / delta 
gradf_y = ( f ( x , y + delta / 2 ) - f ( x , y - delta / 2 ) ) / delta 
return gradf_x , gradf_y 
~~ ~~ ~~ return grad_f 
~~ def hessian ( f , delta = DELTA ) : 
def hessian_f ( * args , ** kwargs ) : 
hessianf_x = ( 
f ( x + delta ) + f ( x - delta ) - 2 * f ( x ) 
) / delta ** 2 
return hessianf_x 
~~~ hess_xx = ( 
f ( x + delta , y ) + f ( x - delta , y ) - 2 * f ( x , y ) 
hess_yy = ( 
f ( x , y + delta ) + f ( x , y - delta ) - 2 * f ( x , y ) 
hess_xy = ( 
+ f ( x + delta / 2 , y + delta / 2 ) 
+ f ( x - delta / 2 , y - delta / 2 ) 
- f ( x + delta / 2 , y - delta / 2 ) 
- f ( x - delta / 2 , y + delta / 2 ) 
return hess_xx , hess_xy , hess_yy 
~~ ~~ ~~ return hessian_f 
~~ def ndgrad ( f , delta = DELTA ) : 
~~~ x = args [ 0 ] 
grad_val = numpy . zeros ( x . shape ) 
it = numpy . nditer ( x , op_flags = [ 'readwrite' ] , flags = [ 'multi_index' ] ) 
for xi in it : 
~~~ i = it . multi_index 
xi += delta / 2 
fp = f ( * args , ** kwargs ) 
xi -= delta 
fm = f ( * args , ** kwargs ) 
grad_val [ i ] = ( fp - fm ) / delta 
~~ return grad_val 
~~ return grad_f 
~~ def ndhess ( f , delta = DELTA ) : 
def hess_f ( * args , ** kwargs ) : 
hess_val = numpy . zeros ( x . shape + x . shape ) 
jt = numpy . nditer ( x , op_flags = [ 'readwrite' ] , flags = [ 'multi_index' ] ) 
for xj in jt : 
~~~ j = jt . multi_index 
xj += delta / 2 
fpp = f ( x ) 
xj -= delta 
fpm = f ( x ) 
fmm = f ( x ) 
xj += delta 
fmp = f ( x ) 
xj -= delta / 2 
hess_val [ i + j ] = ( fpp + fmm - fpm - fmp ) / delta ** 2 
~~ ~~ return hess_val 
~~ return hess_f 
~~ def clgrad ( obj , exe , arg , delta = DELTA ) : 
f , x = get_method_and_copy_of_attribute ( obj , exe , arg ) 
~~~ grad_val = numpy . zeros ( x . shape ) 
~~ def clmixhess ( obj , exe , arg1 , arg2 , delta = DELTA ) : 
f , x = get_method_and_copy_of_attribute ( obj , exe , arg1 ) 
_ , y = get_method_and_copy_of_attribute ( obj , exe , arg2 ) 
~~~ hess_val = numpy . zeros ( x . shape + y . shape ) 
jt = numpy . nditer ( y , op_flags = [ 'readwrite' ] , flags = [ 'multi_index' ] ) 
for yj in jt : 
yj += delta / 2 
fpp = f ( * args , ** kwargs ) 
yj -= delta 
fpm = f ( * args , ** kwargs ) 
fmm = f ( * args , ** kwargs ) 
yj += delta 
fmp = f ( * args , ** kwargs ) 
yj -= delta / 2 
~~ def group_required ( group , 
login_url = None , 
skip_superuser = True ) : 
def decorator ( view_func ) : 
~~~ @ login_required ( redirect_field_name = redirect_field_name , 
login_url = login_url ) 
~~~ if not ( request . user . is_superuser and skip_superuser ) : 
~~~ if request . user . groups . filter ( name = group ) . count ( ) == 0 : 
~~~ raise PermissionDenied 
~~ ~~ return view_func ( request , * args , ** kwargs ) 
~~ return _wrapped_view 
~~ def render_template ( tpl , context ) : 
templates = [ tpl ] if type ( tpl ) != list else tpl 
tpl_instance = None 
for tpl in templates : 
~~~ tpl_instance = template . loader . get_template ( tpl ) 
~~ except template . TemplateDoesNotExist : 
~~ ~~ if not tpl_instance : 
~~ return tpl_instance . render ( template . Context ( context ) ) 
~~ def run_heartbeat ( message ) : 
then = arrow . get ( message [ 'time' ] ) 
now = arrow . get ( ) 
if ( now - then ) > timezone . timedelta ( seconds = ( TICK_FREQ + 1 ) ) : 
~~~ Task . run_tasks ( ) 
~~ ~~ def run_task ( message ) : 
task = Task . objects . get ( pk = message [ 'id' ] ) 
if task . allow_overlap : 
~~~ task . run ( message ) 
~~~ if not task . running : 
~~~ task . running = True 
~~~ task . running = False 
~~ ~~ ~~ ~~ def remove_task ( message ) : 
task . delete ( ) 
~~ def patch_protocol_for_agent ( protocol ) : 
old_makeConnection = protocol . makeConnection 
old_connectionLost = protocol . connectionLost 
def new_makeConnection ( transport ) : 
~~~ patch_transport_fake_push_producer ( transport ) 
patch_transport_abortConnection ( transport , protocol ) 
return old_makeConnection ( transport ) 
~~ def new_connectionLost ( reason ) : 
~~~ if protocol . _fake_connection_aborted and reason . check ( ConnectionDone ) : 
~~~ reason = Failure ( ConnectionAborted ( ) ) 
~~ return old_connectionLost ( reason ) 
~~ protocol . makeConnection = new_makeConnection 
protocol . connectionLost = new_connectionLost 
protocol . _fake_connection_aborted = False 
~~ def patch_if_missing ( obj , name , method ) : 
setattr ( obj , name , getattr ( obj , name , method ) ) 
~~ def patch_transport_fake_push_producer ( transport ) : 
patch_if_missing ( transport , 'pauseProducing' , lambda : None ) 
patch_if_missing ( transport , 'resumeProducing' , lambda : None ) 
patch_if_missing ( transport , 'stopProducing' , transport . loseConnection ) 
~~ def patch_transport_abortConnection ( transport , protocol ) : 
def abortConnection ( ) : 
~~~ protocol . _fake_connection_aborted = True 
transport . loseConnection ( ) 
~~ patch_if_missing ( transport , 'abortConnection' , abortConnection ) 
~~ def accept_connection ( self ) : 
self . server_protocol = self . server . server_factory . buildProtocol ( None ) 
self . _accept_d . callback ( 
FakeServerProtocolWrapper ( self , self . server_protocol ) ) 
return self . await_connected ( ) 
~~ def reject_connection ( self , reason = None ) : 
if reason is None : 
~~~ reason = ConnectionRefusedError ( ) 
~~ self . _accept_d . errback ( reason ) 
~~ def get_agent ( self , reactor = None , contextFactory = None ) : 
return ProxyAgentWithContext ( 
self . endpoint , reactor = reactor , contextFactory = contextFactory ) 
~~ def form_valid ( self , form ) : 
self . object = form . save ( commit = False ) 
response = self . pre_save ( self . object ) 
if response : 
~~~ return response 
~~ self . object . save ( ) 
form . save_m2m ( ) 
self . post_save ( self . object ) 
return HttpResponseRedirect ( self . get_success_url ( ) ) 
~~ def delete ( self , request , * args , ** kwargs ) : 
self . object = self . get_object ( ) 
success_url = self . get_success_url ( ) 
self . pre_delete ( self . object ) 
self . object . delete ( ) 
self . post_delete ( self . object ) 
return HttpResponseRedirect ( success_url ) 
~~ def get_initial ( self ) : 
data = super ( UserViewMixin , self ) . get_initial ( ) 
for k in self . user_field : 
~~~ data [ k ] = self . request . user 
~~ def pre_save ( self , instance ) : 
~~~ super ( UserViewMixin , self ) . pre_save ( instance ) 
if self . request . user . is_authenticated ( ) : 
~~~ for field in self . user_field : 
~~~ setattr ( instance , field , self . request . user ) 
~~ ~~ ~~ async def request ( self , command : str , ** kwargs ) -> dict : 
kwargs . update ( dict ( apikey = self . api_key , command = command , response = 'json' ) ) 
~~~ kwargs . update ( dict ( pagesize = self . max_page_size , page = 1 ) ) 
~~ final_data = dict ( ) 
~~~ async with self . client_session . get ( self . end_point , params = self . _sign ( kwargs ) ) as response : 
~~~ data = await self . _handle_response ( response = response , 
await_final_result = 'queryasyncjobresult' not in command . lower ( ) ) 
~~~ count = data . pop ( 'count' ) 
~~~ if not data : 
~~~ return final_data 
~~~ for key , value in data . items ( ) : 
~~~ final_data . setdefault ( key , [ ] ) . extend ( value ) 
~~ final_data [ 'count' ] = final_data . setdefault ( 'count' , 0 ) + count 
kwargs [ 'page' ] += 1 
~~ ~~ ~~ ~~ ~~ async def _handle_response ( self , response : aiohttp . client_reqrep . ClientResponse , await_final_result : bool ) -> dict : 
~~~ data = await response . json ( ) 
~~ except aiohttp . client_exceptions . ContentTypeError : 
~~~ text = await response . text ( ) 
logging . debug ( \ . format ( text ) ) 
~~~ data = self . _transform_data ( data ) 
if response . status != 200 : 
error_code = data . get ( "errorcode" , response . status ) , 
error_text = data . get ( "errortext" ) , 
response = data ) 
~~ ~~ while await_final_result and ( 'jobid' in data ) : 
~~~ await asyncio . sleep ( self . async_poll_latency ) 
data = await self . queryAsyncJobResult ( jobid = data [ 'jobid' ] ) 
~~~ return data [ 'jobresult' ] 
error_code = data . get ( "errorcode" ) , 
~~ def _sign ( self , url_parameters : dict ) -> dict : 
if url_parameters : 
request_string = urlencode ( sorted ( url_parameters . items ( ) ) , safe = '.-*_' , quote_via = quote ) . lower ( ) 
digest = hmac . new ( self . api_secret . encode ( 'utf-8' ) , request_string . encode ( 'utf-8' ) , hashlib . sha1 ) . digest ( ) 
url_parameters [ 'signature' ] = base64 . b64encode ( digest ) . decode ( 'utf-8' ) . strip ( ) 
~~ return url_parameters 
~~ def _transform_data ( data : dict ) -> dict : 
for key in data . keys ( ) : 
~~~ return_value = data [ key ] 
if isinstance ( return_value , dict ) : 
~~~ return return_value 
~~ def handle ( self , integers , ** options ) : 
print integers , options 
return super ( Command , self ) . handle ( integers , ** options ) 
~~ def read ( * paths ) : 
with open ( os . path . join ( * paths ) , 'r' ) as file_handler : 
~~~ return file_handler . read ( ) 
~~ ~~ def generate ( secret , age , ** payload ) : 
if not payload : 
~~~ payload = { } 
~~ payload [ 'exp' ] = int ( time . time ( ) + age ) 
payload [ 'jti' ] = jti 
return jwt . encode ( payload , decode_secret ( secret ) ) 
~~ def mutex ( func ) : 
lock = args [ 0 ] . lock 
lock . acquire ( True ) 
~~~ return func ( * args , ** kwargs ) 
~~~ lock . release ( ) 
~~ ~~ return wrapper 
~~ def _clean ( self ) : 
now = time . time ( ) 
for jwt in self . jwts . keys ( ) : 
~~~ if ( now - self . jwts [ jwt ] ) > ( self . age * 2 ) : 
~~~ del self . jwts [ jwt ] 
~~ ~~ ~~ def already_used ( self , tok ) : 
if tok in self . jwts : 
~~ self . jwts [ tok ] = time . time ( ) 
~~ def valid ( self , token ) : 
~~~ token = token [ 7 : ] 
~~ data = None 
for secret in self . secrets : 
~~~ data = jwt . decode ( token , secret ) 
~~ except jwt . DecodeError : 
~~ except jwt . ExpiredSignatureError : 
~~ ~~ if not data : 
~~ exp = data . get ( 'exp' ) 
if not exp : 
~~ if now - exp > self . age : 
~~ jti = data . get ( 'jti' ) 
if not jti : 
~~ if self . already_used ( jti ) : 
~~ def handle ( self , * args , ** options ) : 
~~~ Channel ( HEARTBEAT_CHANNEL ) . send ( { 'time' : time . time ( ) } ) 
time . sleep ( HEARTBEAT_FREQUENCY ) 
~~ ~~ except KeyboardInterrupt : 
~~ ~~ def countdown ( name , date , description = '' , id = '' , granularity = 'sec' , start = None , 
progressbar = False , progressbar_inversed = False , showpct = False ) : 
end_date = dateparse . parse_datetime ( date ) 
end = dateformat . format ( end_date , 'U' ) 
content = \ + name + '</div>' 
content += \ + description + '</div>' 
if progressbar : 
parsed_date = datetime . datetime . combine ( 
dateparse . parse_date ( start ) , datetime . time ( ) ) 
start_date = dateparse . parse_datetime ( start ) or parsed_date 
now = datetime . datetime . now ( ) 
pct = ( now - start_date ) . total_seconds ( ) / ( end_date - start_date ) . total_seconds ( ) 
pct = int ( pct * 100 ) 
if progressbar_inversed : 
~~~ pct = 100 - pct 
~~ bar = \ 
bar += \ 
bar += '</div>' 
if showpct : 
~~~ bar += \ 
~~ bar = bar . format ( pct = pct ) 
content += bar 
~~ content += \ 
attr = { 
'class' : 'countdownbox' , 
'data-datetime' : end , 
'data-granularity' : granularity 
if id : 
~~~ attr [ 'id' ] = id 
~~ return html . tag ( 'div' , content , attr ) 
~~ def cd ( self , newdir ) : 
prevdir = os . getcwd ( ) 
os . chdir ( newdir ) 
~~~ os . chdir ( prevdir ) 
~~ ~~ def decode_cmd_out ( self , completed_cmd ) : 
~~~ stdout = completed_cmd . stdout . encode ( 'utf-8' ) . decode ( ) 
~~~ stdout = str ( bytes ( completed_cmd . stdout ) , 'big5' ) . strip ( ) 
~~~ stdout = str ( bytes ( completed_cmd . stdout ) . decode ( 'utf-8' ) ) . strip ( ) 
~~~ stderr = completed_cmd . stderr . encode ( 'utf-8' ) . decode ( ) 
~~~ stderr = str ( bytes ( completed_cmd . stderr ) , 'big5' ) . strip ( ) 
~~~ stderr = str ( bytes ( completed_cmd . stderr ) . decode ( 'utf-8' ) ) . strip ( ) 
~~ ~~ return ParsedCompletedCommand ( 
completed_cmd . returncode , 
completed_cmd . args , 
stdout , 
stderr 
~~ def run_command_under_r_root ( self , cmd , catched = True ) : 
RPATH = self . path 
with self . cd ( newdir = RPATH ) : 
~~~ if catched : 
~~~ process = sp . run ( cmd , stdout = sp . PIPE , stderr = sp . PIPE ) 
~~~ process = sp . run ( cmd ) 
~~ return process 
~~ ~~ def execute ( self ) : 
rprocess = OrderedDict ( ) 
commands = OrderedDict ( [ 
( self . file , [ 'Rscript' , self . file ] + self . cmd ) , 
] ) 
for cmd_name , cmd in commands . items ( ) : 
~~~ rprocess [ cmd_name ] = self . run_command_under_r_root ( cmd ) 
~~ return self . decode_cmd_out ( completed_cmd = rprocess [ self . file ] ) 
~~ def find_best_string ( query , 
corpus , 
step = 4 , 
flex = 3 , 
case_sensitive = False ) : 
def ratio ( a , b ) : 
return SequenceMatcher ( None , a , b ) . ratio ( ) 
~~ def scan_corpus ( step ) : 
match_values = [ ] 
m = 0 
while m + qlen - step <= len ( corpus ) : 
~~~ match_values . append ( ratio ( query , corpus [ m : m - 1 + qlen ] ) ) 
m += step 
~~ return match_values 
~~ def index_max ( v ) : 
return max ( range ( len ( v ) ) , key = v . __getitem__ ) 
~~ def adjust_left_right_positions ( ) : 
p_l , bp_l = [ pos ] * 2 
p_r , bp_r = [ pos + qlen ] * 2 
bmv_l = match_values [ round_decimal ( p_l / step ) ] 
bmv_r = match_values [ round_decimal ( p_r / step ) ] 
for f in range ( flex ) : 
~~~ ll = ratio ( query , corpus [ p_l - f : p_r ] ) 
if ll > bmv_l : 
~~~ bmv_l = ll 
bp_l = p_l - f 
~~ lr = ratio ( query , corpus [ p_l + f : p_r ] ) 
if lr > bmv_l : 
~~~ bmv_l = lr 
bp_l = p_l + f 
~~ rl = ratio ( query , corpus [ p_l : p_r - f ] ) 
if rl > bmv_r : 
~~~ bmv_r = rl 
bp_r = p_r - f 
~~ rr = ratio ( query , corpus [ p_l : p_r + f ] ) 
if rr > bmv_r : 
~~~ bmv_r = rr 
bp_r = p_r + f 
~~ ~~ return bp_l , bp_r , ratio ( query , corpus [ bp_l : bp_r ] ) 
~~ if not case_sensitive : 
~~~ query = query . lower ( ) 
corpus = corpus . lower ( ) 
~~ qlen = len ( query ) 
if flex >= qlen / 2 : 
flex = 3 
~~ match_values = scan_corpus ( step ) 
pos = index_max ( match_values ) * step 
pos_left , pos_right , match_value = adjust_left_right_positions ( ) 
return corpus [ pos_left : pos_right ] . strip ( ) , match_value 
~~ def load_all_modules_in_packages ( package_or_set_of_packages ) : 
if isinstance ( package_or_set_of_packages , types . ModuleType ) : 
~~~ packages = [ package_or_set_of_packages ] 
~~ elif isinstance ( package_or_set_of_packages , Iterable ) and not isinstance ( package_or_set_of_packages , ( dict , str ) ) : 
~~~ packages = package_or_set_of_packages 
~~ imported = packages . copy ( ) 
for package in packages : 
~~~ if not hasattr ( package , '__path__' ) : 
~~~ raise Exception ( 
~~ for module_finder , name , ispkg in pkgutil . walk_packages ( package . __path__ ) : 
~~~ module_name = '{}.{}' . format ( package . __name__ , name ) 
current_module = importlib . import_module ( module_name ) 
imported . append ( current_module ) 
~~~ imported += load_all_modules_in_packages ( current_module ) 
~~ ~~ ~~ for module in imported : 
~~~ dir ( module ) 
~~ return list ( 
module . __name__ : module 
for module in imported 
} . values ( ) 
~~ def learn ( env , 
network , 
seed = None , 
lr = 5e-4 , 
total_timesteps = 100000 , 
buffer_size = 50000 , 
exploration_fraction = 0.1 , 
exploration_final_eps = 0.02 , 
train_freq = 1 , 
batch_size = 32 , 
print_freq = 100 , 
checkpoint_freq = 10000 , 
checkpoint_path = None , 
learning_starts = 1000 , 
gamma = 1.0 , 
target_network_update_freq = 500 , 
prioritized_replay = False , 
prioritized_replay_alpha = 0.6 , 
prioritized_replay_beta0 = 0.4 , 
prioritized_replay_beta_iters = None , 
prioritized_replay_eps = 1e-6 , 
param_noise = False , 
callback = None , 
load_path = None , 
** network_kwargs 
sess = get_session ( ) 
set_global_seeds ( seed ) 
q_func = build_q_func ( network , ** network_kwargs ) 
observation_space = env . observation_space 
def make_obs_ph ( name ) : 
~~~ return ObservationInput ( observation_space , name = name ) 
~~ act , train , update_target , debug = deepq . build_train ( 
make_obs_ph = make_obs_ph , 
q_func = q_func , 
num_actions = env . action_space . n , 
optimizer = tf . train . AdamOptimizer ( learning_rate = lr ) , 
gamma = gamma , 
grad_norm_clipping = 10 , 
param_noise = param_noise 
act_params = { 
'make_obs_ph' : make_obs_ph , 
'q_func' : q_func , 
'num_actions' : env . action_space . n , 
act = ActWrapper ( act , act_params ) 
if prioritized_replay : 
~~~ replay_buffer = PrioritizedReplayBuffer ( buffer_size , alpha = prioritized_replay_alpha ) 
if prioritized_replay_beta_iters is None : 
~~~ prioritized_replay_beta_iters = total_timesteps 
~~ beta_schedule = LinearSchedule ( prioritized_replay_beta_iters , 
initial_p = prioritized_replay_beta0 , 
final_p = 1.0 ) 
~~~ replay_buffer = ReplayBuffer ( buffer_size ) 
beta_schedule = None 
~~ exploration = LinearSchedule ( schedule_timesteps = int ( exploration_fraction * total_timesteps ) , 
initial_p = 1.0 , 
final_p = exploration_final_eps ) 
U . initialize ( ) 
update_target ( ) 
episode_rewards = [ 0.0 ] 
saved_mean_reward = None 
obs = env . reset ( ) 
reset = True 
with tempfile . TemporaryDirectory ( ) as td : 
~~~ td = checkpoint_path or td 
model_file = os . path . join ( td , "model" ) 
model_saved = False 
if tf . train . latest_checkpoint ( td ) is not None : 
~~~ load_variables ( model_file ) 
model_saved = True 
~~ elif load_path is not None : 
~~~ load_variables ( load_path ) 
~~ for t in range ( total_timesteps ) : 
~~~ if callback is not None : 
~~~ if callback ( locals ( ) , globals ( ) ) : 
~~ ~~ kwargs = { } 
if not param_noise : 
~~~ update_eps = exploration . value ( t ) 
update_param_noise_threshold = 0. 
~~~ update_eps = 0. 
update_param_noise_threshold = - np . log ( 1. - exploration . value ( t ) + exploration . value ( t ) / float ( env . action_space . n ) ) 
kwargs [ 'reset' ] = reset 
kwargs [ 'update_param_noise_threshold' ] = update_param_noise_threshold 
kwargs [ 'update_param_noise_scale' ] = True 
~~ action = act ( np . array ( obs ) [ None ] , update_eps = update_eps , ** kwargs ) [ 0 ] 
env_action = action 
reset = False 
new_obs , rew , done , _ = env . step ( env_action ) 
replay_buffer . add ( obs , action , rew , new_obs , float ( done ) ) 
obs = new_obs 
episode_rewards [ - 1 ] += rew 
if done : 
~~~ obs = env . reset ( ) 
episode_rewards . append ( 0.0 ) 
~~ if t > learning_starts and t % train_freq == 0 : 
~~~ if prioritized_replay : 
~~~ experience = replay_buffer . sample ( batch_size , beta = beta_schedule . value ( t ) ) 
( obses_t , actions , rewards , obses_tp1 , dones , weights , batch_idxes ) = experience 
~~~ obses_t , actions , rewards , obses_tp1 , dones = replay_buffer . sample ( batch_size ) 
weights , batch_idxes = np . ones_like ( rewards ) , None 
~~ td_errors = train ( obses_t , actions , rewards , obses_tp1 , dones , weights ) 
~~~ new_priorities = np . abs ( td_errors ) + prioritized_replay_eps 
replay_buffer . update_priorities ( batch_idxes , new_priorities ) 
~~ ~~ if t > learning_starts and t % target_network_update_freq == 0 : 
~~~ update_target ( ) 
~~ mean_100ep_reward = round ( np . mean ( episode_rewards [ - 101 : - 1 ] ) , 1 ) 
num_episodes = len ( episode_rewards ) 
if done and print_freq is not None and len ( episode_rewards ) % print_freq == 0 : 
~~~ logger . record_tabular ( "steps" , t ) 
logger . record_tabular ( "episodes" , num_episodes ) 
logger . dump_tabular ( ) 
~~ if ( checkpoint_freq is not None and t > learning_starts and 
num_episodes > 100 and t % checkpoint_freq == 0 ) : 
~~~ if saved_mean_reward is None or mean_100ep_reward > saved_mean_reward : 
~~~ if print_freq is not None : 
saved_mean_reward , mean_100ep_reward ) ) 
~~ save_variables ( model_file ) 
saved_mean_reward = mean_100ep_reward 
~~ ~~ ~~ if model_saved : 
~~ load_variables ( model_file ) 
~~ ~~ return act 
~~ def save_act ( self , path = None ) : 
if path is None : 
~~~ path = os . path . join ( logger . get_dir ( ) , "model.pkl" ) 
~~ with tempfile . TemporaryDirectory ( ) as td : 
~~~ save_variables ( os . path . join ( td , "model" ) ) 
arc_name = os . path . join ( td , "packed.zip" ) 
with zipfile . ZipFile ( arc_name , 'w' ) as zipf : 
~~~ for root , dirs , files in os . walk ( td ) : 
~~~ for fname in files : 
~~~ file_path = os . path . join ( root , fname ) 
if file_path != arc_name : 
~~~ zipf . write ( file_path , os . path . relpath ( file_path , td ) ) 
~~ ~~ ~~ ~~ with open ( arc_name , "rb" ) as f : 
~~~ model_data = f . read ( ) 
~~ ~~ with open ( path , "wb" ) as f : 
~~~ cloudpickle . dump ( ( model_data , self . _act_params ) , f ) 
~~ ~~ def nature_cnn ( unscaled_images , ** conv_kwargs ) : 
scaled_images = tf . cast ( unscaled_images , tf . float32 ) / 255. 
activ = tf . nn . relu 
h = activ ( conv ( scaled_images , 'c1' , nf = 32 , rf = 8 , stride = 4 , init_scale = np . sqrt ( 2 ) , 
** conv_kwargs ) ) 
h2 = activ ( conv ( h , 'c2' , nf = 64 , rf = 4 , stride = 2 , init_scale = np . sqrt ( 2 ) , ** conv_kwargs ) ) 
h3 = activ ( conv ( h2 , 'c3' , nf = 64 , rf = 3 , stride = 1 , init_scale = np . sqrt ( 2 ) , ** conv_kwargs ) ) 
h3 = conv_to_fc ( h3 ) 
return activ ( fc ( h3 , 'fc1' , nh = 512 , init_scale = np . sqrt ( 2 ) ) ) 
~~ def mlp ( num_layers = 2 , num_hidden = 64 , activation = tf . tanh , layer_norm = False ) : 
def network_fn ( X ) : 
~~~ h = tf . layers . flatten ( X ) 
for i in range ( num_layers ) : 
~~~ h = fc ( h , 'mlp_fc{}' . format ( i ) , nh = num_hidden , init_scale = np . sqrt ( 2 ) ) 
if layer_norm : 
~~~ h = tf . contrib . layers . layer_norm ( h , center = True , scale = True ) 
~~ h = activation ( h ) 
~~ return h 
~~ return network_fn 
~~ def lstm ( nlstm = 128 , layer_norm = False ) : 
def network_fn ( X , nenv = 1 ) : 
~~~ nbatch = X . shape [ 0 ] 
nsteps = nbatch // nenv 
h = tf . layers . flatten ( X ) 
S = tf . placeholder ( tf . float32 , [ nenv , 2 * nlstm ] ) #states 
xs = batch_to_seq ( h , nenv , nsteps ) 
ms = batch_to_seq ( M , nenv , nsteps ) 
~~~ h5 , snew = utils . lnlstm ( xs , ms , S , scope = 'lnlstm' , nh = nlstm ) 
~~~ h5 , snew = utils . lstm ( xs , ms , S , scope = 'lstm' , nh = nlstm ) 
~~ h = seq_to_batch ( h5 ) 
initial_state = np . zeros ( S . shape . as_list ( ) , dtype = float ) 
return h , { 'S' : S , 'M' : M , 'state' : snew , 'initial_state' : initial_state } 
~~ def conv_only ( convs = [ ( 32 , 8 , 4 ) , ( 64 , 4 , 2 ) , ( 64 , 3 , 1 ) ] , ** conv_kwargs ) : 
~~~ out = tf . cast ( X , tf . float32 ) / 255. 
with tf . variable_scope ( "convnet" ) : 
~~~ for num_outputs , kernel_size , stride in convs : 
~~~ out = layers . convolution2d ( out , 
num_outputs = num_outputs , 
kernel_size = kernel_size , 
stride = stride , 
activation_fn = tf . nn . relu , 
** conv_kwargs ) 
~~ ~~ return out 
~~ def get_network_builder ( name ) : 
if callable ( name ) : 
~~~ return name 
~~ elif name in mapping : 
~~~ return mapping [ name ] 
~~ ~~ def mlp ( hiddens = [ ] , layer_norm = False ) : 
return lambda * args , ** kwargs : _mlp ( hiddens , layer_norm = layer_norm , * args , ** kwargs ) 
~~ def cnn_to_mlp ( convs , hiddens , dueling = False , layer_norm = False ) : 
return lambda * args , ** kwargs : _cnn_to_mlp ( convs , hiddens , dueling , layer_norm = layer_norm , * args , ** kwargs ) 
~~ def make_vec_env ( env_id , env_type , num_env , seed , 
wrapper_kwargs = None , 
start_index = 0 , 
reward_scale = 1.0 , 
flatten_dict_observations = True , 
gamestate = None ) : 
wrapper_kwargs = wrapper_kwargs or { } 
mpi_rank = MPI . COMM_WORLD . Get_rank ( ) if MPI else 0 
seed = seed + 10000 * mpi_rank if seed is not None else None 
logger_dir = logger . get_dir ( ) 
def make_thunk ( rank ) : 
~~~ return lambda : make_env ( 
env_id = env_id , 
env_type = env_type , 
mpi_rank = mpi_rank , 
subrank = rank , 
seed = seed , 
reward_scale = reward_scale , 
gamestate = gamestate , 
flatten_dict_observations = flatten_dict_observations , 
wrapper_kwargs = wrapper_kwargs , 
logger_dir = logger_dir 
~~ set_global_seeds ( seed ) 
if num_env > 1 : 
~~~ return SubprocVecEnv ( [ make_thunk ( i + start_index ) for i in range ( num_env ) ] ) 
~~~ return DummyVecEnv ( [ make_thunk ( start_index ) ] ) 
~~ ~~ def make_mujoco_env ( env_id , seed , reward_scale = 1.0 ) : 
rank = MPI . COMM_WORLD . Get_rank ( ) 
myseed = seed + 1000 * rank if seed is not None else None 
set_global_seeds ( myseed ) 
env = gym . make ( env_id ) 
logger_path = None if logger . get_dir ( ) is None else os . path . join ( logger . get_dir ( ) , str ( rank ) ) 
env = Monitor ( env , logger_path , allow_early_resets = True ) 
env . seed ( seed ) 
if reward_scale != 1.0 : 
~~~ from baselines . common . retro_wrappers import RewardScaler 
env = RewardScaler ( env , reward_scale ) 
~~ return env 
~~ def make_robotics_env ( env_id , seed , rank = 0 ) : 
env = FlattenDictWrapper ( env , [ 'observation' , 'desired_goal' ] ) 
env = Monitor ( 
env , logger . get_dir ( ) and os . path . join ( logger . get_dir ( ) , str ( rank ) ) , 
info_keywords = ( 'is_success' , ) ) 
return env 
~~ def common_arg_parser ( ) : 
parser = arg_parser ( ) 
parser . add_argument ( '--alg' , help = 'Algorithm' , type = str , default = 'ppo2' ) 
parser . add_argument ( '--num_timesteps' , type = float , default = 1e6 ) , 
parser . add_argument ( '--play' , default = False , action = 'store_true' ) 
~~ def robotics_arg_parser ( ) : 
parser . add_argument ( '--num-timesteps' , type = int , default = int ( 1e6 ) ) 
~~ def parse_unknown_args ( args ) : 
retval = { } 
preceded_by_key = False 
for arg in args : 
~~~ if arg . startswith ( '--' ) : 
~~~ if '=' in arg : 
~~~ key = arg . split ( '=' ) [ 0 ] [ 2 : ] 
value = arg . split ( '=' ) [ 1 ] 
retval [ key ] = value 
~~~ key = arg [ 2 : ] 
preceded_by_key = True 
~~ ~~ elif preceded_by_key : 
~~~ retval [ key ] = arg 
~~ ~~ return retval 
~~ def clear_mpi_env_vars ( ) : 
removed_environment = { } 
for k , v in list ( os . environ . items ( ) ) : 
~~~ for prefix in [ 'OMPI_' , 'PMI_' ] : 
~~~ if k . startswith ( prefix ) : 
~~~ removed_environment [ k ] = v 
del os . environ [ k ] 
~~ ~~ ~~ try : 
~~~ os . environ . update ( removed_environment ) 
~~ ~~ def learn ( * , network , env , total_timesteps , eval_env = None , seed = None , nsteps = 2048 , ent_coef = 0.0 , lr = 3e-4 , 
vf_coef = 0.5 , max_grad_norm = 0.5 , gamma = 0.99 , lam = 0.95 , 
log_interval = 10 , nminibatches = 4 , noptepochs = 4 , cliprange = 0.2 , 
save_interval = 0 , load_path = None , model_fn = None , ** network_kwargs ) : 
if isinstance ( lr , float ) : lr = constfn ( lr ) 
else : assert callable ( lr ) 
if isinstance ( cliprange , float ) : cliprange = constfn ( cliprange ) 
else : assert callable ( cliprange ) 
total_timesteps = int ( total_timesteps ) 
policy = build_policy ( env , network , ** network_kwargs ) 
nenvs = env . num_envs 
ob_space = env . observation_space 
ac_space = env . action_space 
nbatch = nenvs * nsteps 
nbatch_train = nbatch // nminibatches 
if model_fn is None : 
~~~ from baselines . ppo2 . model import Model 
model_fn = Model 
~~ model = model_fn ( policy = policy , ob_space = ob_space , ac_space = ac_space , nbatch_act = nenvs , nbatch_train = nbatch_train , 
nsteps = nsteps , ent_coef = ent_coef , vf_coef = vf_coef , 
max_grad_norm = max_grad_norm ) 
if load_path is not None : 
~~~ model . load ( load_path ) 
~~ runner = Runner ( env = env , model = model , nsteps = nsteps , gamma = gamma , lam = lam ) 
if eval_env is not None : 
~~~ eval_runner = Runner ( env = eval_env , model = model , nsteps = nsteps , gamma = gamma , lam = lam ) 
~~ epinfobuf = deque ( maxlen = 100 ) 
~~~ eval_epinfobuf = deque ( maxlen = 100 ) 
~~ tfirststart = time . perf_counter ( ) 
nupdates = total_timesteps // nbatch 
for update in range ( 1 , nupdates + 1 ) : 
~~~ assert nbatch % nminibatches == 0 
tstart = time . perf_counter ( ) 
frac = 1.0 - ( update - 1.0 ) / nupdates 
lrnow = lr ( frac ) 
cliprangenow = cliprange ( frac ) 
~~ epinfobuf . extend ( epinfos ) 
~~~ eval_epinfobuf . extend ( eval_epinfos ) 
~~ mblossvals = [ ] 
~~~ inds = np . arange ( nbatch ) 
for _ in range ( noptepochs ) : 
~~~ np . random . shuffle ( inds ) 
for start in range ( 0 , nbatch , nbatch_train ) : 
~~~ end = start + nbatch_train 
mbinds = inds [ start : end ] 
slices = ( arr [ mbinds ] for arr in ( obs , returns , masks , actions , values , neglogpacs ) ) 
mblossvals . append ( model . train ( lrnow , cliprangenow , * slices ) ) 
~~~ assert nenvs % nminibatches == 0 
envsperbatch = nenvs // nminibatches 
envinds = np . arange ( nenvs ) 
flatinds = np . arange ( nenvs * nsteps ) . reshape ( nenvs , nsteps ) 
~~~ np . random . shuffle ( envinds ) 
for start in range ( 0 , nenvs , envsperbatch ) : 
~~~ end = start + envsperbatch 
mbenvinds = envinds [ start : end ] 
mbflatinds = flatinds [ mbenvinds ] . ravel ( ) 
slices = ( arr [ mbflatinds ] for arr in ( obs , returns , masks , actions , values , neglogpacs ) ) 
mbstates = states [ mbenvinds ] 
mblossvals . append ( model . train ( lrnow , cliprangenow , * slices , mbstates ) ) 
~~ ~~ ~~ lossvals = np . mean ( mblossvals , axis = 0 ) 
tnow = time . perf_counter ( ) 
fps = int ( nbatch / ( tnow - tstart ) ) 
if update % log_interval == 0 or update == 1 : 
~~~ ev = explained_variance ( values , returns ) 
logger . logkv ( "serial_timesteps" , update * nsteps ) 
logger . logkv ( "nupdates" , update ) 
logger . logkv ( "total_timesteps" , update * nbatch ) 
logger . logkv ( "fps" , fps ) 
logger . logkv ( "explained_variance" , float ( ev ) ) 
logger . logkv ( 'eprewmean' , safemean ( [ epinfo [ 'r' ] for epinfo in epinfobuf ] ) ) 
logger . logkv ( 'eplenmean' , safemean ( [ epinfo [ 'l' ] for epinfo in epinfobuf ] ) ) 
~~~ logger . logkv ( 'eval_eprewmean' , safemean ( [ epinfo [ 'r' ] for epinfo in eval_epinfobuf ] ) ) 
logger . logkv ( 'eval_eplenmean' , safemean ( [ epinfo [ 'l' ] for epinfo in eval_epinfobuf ] ) ) 
~~ logger . logkv ( 'time_elapsed' , tnow - tfirststart ) 
for ( lossval , lossname ) in zip ( lossvals , model . loss_names ) : 
~~~ logger . logkv ( lossname , lossval ) 
~~ if MPI is None or MPI . COMM_WORLD . Get_rank ( ) == 0 : 
~~~ logger . dumpkvs ( ) 
~~ ~~ if save_interval and ( update % save_interval == 0 or update == 1 ) and logger . get_dir ( ) and ( MPI is None or MPI . COMM_WORLD . Get_rank ( ) == 0 ) : 
~~~ checkdir = osp . join ( logger . get_dir ( ) , 'checkpoints' ) 
os . makedirs ( checkdir , exist_ok = True ) 
savepath = osp . join ( checkdir , '%.5i' % update ) 
model . save ( savepath ) 
~~ ~~ return model 
~~ def cg ( f_Ax , b , cg_iters = 10 , callback = None , verbose = False , residual_tol = 1e-10 ) : 
p = b . copy ( ) 
r = b . copy ( ) 
x = np . zeros_like ( b ) 
rdotr = r . dot ( r ) 
for i in range ( cg_iters ) : 
~~~ callback ( x ) 
~~ if verbose : print ( fmtstr % ( i , rdotr , np . linalg . norm ( x ) ) ) 
z = f_Ax ( p ) 
v = rdotr / p . dot ( z ) 
x += v * p 
r -= v * z 
newrdotr = r . dot ( r ) 
mu = newrdotr / rdotr 
p = r + mu * p 
rdotr = newrdotr 
if rdotr < residual_tol : 
~~ ~~ if callback is not None : 
return x 
~~ def observation_placeholder ( ob_space , batch_size = None , name = 'Ob' ) : 
dtype = ob_space . dtype 
if dtype == np . int8 : 
~~~ dtype = np . uint8 
~~ return tf . placeholder ( shape = ( batch_size , ) + ob_space . shape , dtype = dtype , name = name ) 
~~ def observation_input ( ob_space , batch_size = None , name = 'Ob' ) : 
placeholder = observation_placeholder ( ob_space , batch_size , name ) 
return placeholder , encode_observation ( ob_space , placeholder ) 
~~ def encode_observation ( ob_space , placeholder ) : 
if isinstance ( ob_space , Discrete ) : 
~~~ return tf . to_float ( tf . one_hot ( placeholder , ob_space . n ) ) 
~~ elif isinstance ( ob_space , Box ) : 
~~~ return tf . to_float ( placeholder ) 
~~ elif isinstance ( ob_space , MultiDiscrete ) : 
~~~ placeholder = tf . cast ( placeholder , tf . int32 ) 
one_hots = [ tf . to_float ( tf . one_hot ( placeholder [ ... , i ] , ob_space . nvec [ i ] ) ) for i in range ( placeholder . shape [ - 1 ] ) ] 
return tf . concat ( one_hots , axis = - 1 ) 
~~~ raise NotImplementedError 
~~ ~~ def generate_rollouts ( self ) : 
self . reset_all_rollouts ( ) 
o [ : ] = self . initial_o 
ag [ : ] = self . initial_ag 
obs , achieved_goals , acts , goals , successes = [ ] , [ ] , [ ] , [ ] , [ ] 
dones = [ ] 
info_values = [ np . empty ( ( self . T - 1 , self . rollout_batch_size , self . dims [ 'info_' + key ] ) , np . float32 ) for key in self . info_keys ] 
Qs = [ ] 
for t in range ( self . T ) : 
~~~ policy_output = self . policy . get_actions ( 
o , ag , self . g , 
compute_Q = self . compute_Q , 
noise_eps = self . noise_eps if not self . exploit else 0. , 
random_eps = self . random_eps if not self . exploit else 0. , 
use_target_net = self . use_target_net ) 
if self . compute_Q : 
~~~ u , Q = policy_output 
Qs . append ( Q ) 
~~~ u = policy_output 
~~ if u . ndim == 1 : 
~~~ u = u . reshape ( 1 , - 1 ) 
~~ o_new = np . empty ( ( self . rollout_batch_size , self . dims [ 'o' ] ) ) 
ag_new = np . empty ( ( self . rollout_batch_size , self . dims [ 'g' ] ) ) 
success = np . zeros ( self . rollout_batch_size ) 
obs_dict_new , _ , done , info = self . venv . step ( u ) 
o_new = obs_dict_new [ 'observation' ] 
ag_new = obs_dict_new [ 'achieved_goal' ] 
success = np . array ( [ i . get ( 'is_success' , 0.0 ) for i in info ] ) 
if any ( done ) : 
~~ for i , info_dict in enumerate ( info ) : 
~~~ for idx , key in enumerate ( self . info_keys ) : 
~~~ info_values [ idx ] [ t , i ] = info [ i ] [ key ] 
~~ ~~ if np . isnan ( o_new ) . any ( ) : 
return self . generate_rollouts ( ) 
~~ dones . append ( done ) 
obs . append ( o . copy ( ) ) 
achieved_goals . append ( ag . copy ( ) ) 
successes . append ( success . copy ( ) ) 
acts . append ( u . copy ( ) ) 
goals . append ( self . g . copy ( ) ) 
o [ ... ] = o_new 
ag [ ... ] = ag_new 
~~ obs . append ( o . copy ( ) ) 
episode = dict ( o = obs , 
u = acts , 
g = goals , 
ag = achieved_goals ) 
for key , value in zip ( self . info_keys , info_values ) : 
~~~ episode [ 'info_{}' . format ( key ) ] = value 
~~ successful = np . array ( successes ) [ - 1 , : ] 
assert successful . shape == ( self . rollout_batch_size , ) 
success_rate = np . mean ( successful ) 
self . success_history . append ( success_rate ) 
~~~ self . Q_history . append ( np . mean ( Qs ) ) 
~~ self . n_episodes += self . rollout_batch_size 
return convert_episode_to_batch_major ( episode ) 
~~ def save_policy ( self , path ) : 
with open ( path , 'wb' ) as f : 
~~~ pickle . dump ( self . policy , f ) 
~~ ~~ def logs ( self , prefix = 'worker' ) : 
logs = [ ] 
logs += [ ( 'success_rate' , np . mean ( self . success_history ) ) ] 
~~~ logs += [ ( 'mean_Q' , np . mean ( self . Q_history ) ) ] 
~~ logs += [ ( 'episode' , self . n_episodes ) ] 
if prefix != '' and not prefix . endswith ( '/' ) : 
~~~ return [ ( prefix + '/' + key , val ) for key , val in logs ] 
~~~ return logs 
~~ ~~ def smooth ( y , radius , mode = 'two_sided' , valid_only = False ) : 
assert mode in ( 'two_sided' , 'causal' ) 
if len ( y ) < 2 * radius + 1 : 
~~~ return np . ones_like ( y ) * y . mean ( ) 
~~ elif mode == 'two_sided' : 
~~~ convkernel = np . ones ( 2 * radius + 1 ) 
out = np . convolve ( y , convkernel , mode = 'same' ) / np . convolve ( np . ones_like ( y ) , convkernel , mode = 'same' ) 
if valid_only : 
~~~ out [ : radius ] = out [ - radius : ] = np . nan 
~~ ~~ elif mode == 'causal' : 
~~~ convkernel = np . ones ( radius ) 
out = np . convolve ( y , convkernel , mode = 'full' ) / np . convolve ( np . ones_like ( y ) , convkernel , mode = 'full' ) 
out = out [ : - radius + 1 ] 
~~~ out [ : radius ] = np . nan 
~~ def one_sided_ema ( xolds , yolds , low = None , high = None , n = 512 , decay_steps = 1. , low_counts_threshold = 1e-8 ) : 
low = xolds [ 0 ] if low is None else low 
high = xolds [ - 1 ] if high is None else high 
xolds = xolds . astype ( 'float64' ) 
yolds = yolds . astype ( 'float64' ) 
sum_y = 0. 
count_y = 0. 
xnews = np . linspace ( low , high , n ) 
decay_period = ( high - low ) / ( n - 1 ) * decay_steps 
interstep_decay = np . exp ( - 1. / decay_steps ) 
sum_ys = np . zeros_like ( xnews ) 
count_ys = np . zeros_like ( xnews ) 
for i in range ( n ) : 
~~~ xnew = xnews [ i ] 
sum_y *= interstep_decay 
count_y *= interstep_decay 
~~~ xold = xolds [ luoi ] 
if xold <= xnew : 
~~~ decay = np . exp ( - ( xnew - xold ) / decay_period ) 
sum_y += decay * yolds [ luoi ] 
count_y += decay 
luoi += 1 
~~ if luoi >= len ( xolds ) : 
~~ ~~ sum_ys [ i ] = sum_y 
count_ys [ i ] = count_y 
~~ ys = sum_ys / count_ys 
ys [ count_ys < low_counts_threshold ] = np . nan 
return xnews , ys , count_ys 
~~ def symmetric_ema ( xolds , yolds , low = None , high = None , n = 512 , decay_steps = 1. , low_counts_threshold = 1e-8 ) : 
xs , ys1 , count_ys1 = one_sided_ema ( xolds , yolds , low , high , n , decay_steps , low_counts_threshold = 0 ) 
_ , ys2 , count_ys2 = one_sided_ema ( - xolds [ : : - 1 ] , yolds [ : : - 1 ] , - high , - low , n , decay_steps , low_counts_threshold = 0 ) 
ys2 = ys2 [ : : - 1 ] 
count_ys2 = count_ys2 [ : : - 1 ] 
count_ys = count_ys1 + count_ys2 
ys = ( ys1 * count_ys1 + ys2 * count_ys2 ) / count_ys 
return xs , ys , count_ys 
~~ def load_results ( root_dir_or_dirs , enable_progress = True , enable_monitor = True , verbose = False ) : 
import re 
if isinstance ( root_dir_or_dirs , str ) : 
~~~ rootdirs = [ osp . expanduser ( root_dir_or_dirs ) ] 
~~~ rootdirs = [ osp . expanduser ( d ) for d in root_dir_or_dirs ] 
~~ allresults = [ ] 
for rootdir in rootdirs : 
for dirname , dirs , files in os . walk ( rootdir ) : 
~~~ if '-proc' in dirname : 
~~~ files [ : ] = [ ] 
~~ monitor_re = re . compile ( r'(\\d+\\.)?(\\d+\\.)?monitor\\.csv' ) 
~~~ result = { 'dirname' : dirname } 
if "metadata.json" in files : 
~~~ with open ( osp . join ( dirname , "metadata.json" ) , "r" ) as fh : 
~~~ result [ 'metadata' ] = json . load ( fh ) 
~~ ~~ progjson = osp . join ( dirname , "progress.json" ) 
progcsv = osp . join ( dirname , "progress.csv" ) 
if enable_progress : 
~~~ if osp . exists ( progjson ) : 
~~~ result [ 'progress' ] = pandas . DataFrame ( read_json ( progjson ) ) 
~~ elif osp . exists ( progcsv ) : 
~~~ result [ 'progress' ] = read_csv ( progcsv ) 
~~ except pandas . errors . EmptyDataError : 
~~ ~~ if enable_monitor : 
~~~ result [ 'monitor' ] = pandas . DataFrame ( monitor . load_results ( dirname ) ) 
~~ except monitor . LoadMonitorResultsError : 
~~ ~~ if result . get ( 'monitor' ) is not None or result . get ( 'progress' ) is not None : 
~~~ allresults . append ( Result ( ** result ) ) 
if verbose : 
return allresults 
~~ def plot_results ( 
allresults , * , 
xy_fn = default_xy_fn , 
split_fn = default_split_fn , 
group_fn = default_split_fn , 
average_group = False , 
shaded_std = True , 
shaded_err = True , 
figsize = None , 
legend_outside = False , 
resample = 0 , 
smooth_step = 1.0 
if split_fn is None : split_fn = lambda _ : '' 
if group_fn is None : group_fn = lambda _ : '' 
for result in allresults : 
~~~ splitkey = split_fn ( result ) 
sk2r [ splitkey ] . append ( result ) 
~~ assert len ( sk2r ) > 0 
nrows = len ( sk2r ) 
ncols = 1 
figsize = figsize or ( 6 , 6 * nrows ) 
f , axarr = plt . subplots ( nrows , ncols , sharex = False , squeeze = False , figsize = figsize ) 
groups = list ( set ( group_fn ( result ) for result in allresults ) ) 
default_samples = 512 
if average_group : 
~~~ resample = resample or default_samples 
~~ for ( isplit , sk ) in enumerate ( sorted ( sk2r . keys ( ) ) ) : 
~~~ g2l = { } 
g2c = defaultdict ( int ) 
sresults = sk2r [ sk ] 
gresults = defaultdict ( list ) 
ax = axarr [ isplit ] [ 0 ] 
for result in sresults : 
~~~ group = group_fn ( result ) 
g2c [ group ] += 1 
x , y = xy_fn ( result ) 
if x is None : x = np . arange ( len ( y ) ) 
x , y = map ( np . asarray , ( x , y ) ) 
~~~ gresults [ group ] . append ( ( x , y ) ) 
~~~ if resample : 
~~~ x , y , counts = symmetric_ema ( x , y , x [ 0 ] , x [ - 1 ] , resample , decay_steps = smooth_step ) 
~~ l , = ax . plot ( x , y , color = COLORS [ groups . index ( group ) % len ( COLORS ) ] ) 
g2l [ group ] = l 
~~ ~~ if average_group : 
~~~ for group in sorted ( groups ) : 
~~~ xys = gresults [ group ] 
if not any ( xys ) : 
~~ color = COLORS [ groups . index ( group ) % len ( COLORS ) ] 
origxs = [ xy [ 0 ] for xy in xys ] 
minxlen = min ( map ( len , origxs ) ) 
def allequal ( qs ) : 
~~~ return all ( ( q == qs [ 0 ] ) . all ( ) for q in qs [ 1 : ] ) 
~~ if resample : 
~~~ low = max ( x [ 0 ] for x in origxs ) 
high = min ( x [ - 1 ] for x in origxs ) 
usex = np . linspace ( low , high , resample ) 
ys = [ ] 
for ( x , y ) in xys : 
~~~ ys . append ( symmetric_ema ( x , y , low , high , resample , decay_steps = smooth_step ) [ 1 ] ) 
usex = origxs [ 0 ] 
ys = [ xy [ 1 ] [ : minxlen ] for xy in xys ] 
~~ ymean = np . mean ( ys , axis = 0 ) 
ystd = np . std ( ys , axis = 0 ) 
ystderr = ystd / np . sqrt ( len ( ys ) ) 
l , = axarr [ isplit ] [ 0 ] . plot ( usex , ymean , color = color ) 
if shaded_err : 
~~~ ax . fill_between ( usex , ymean - ystderr , ymean + ystderr , color = color , alpha = .4 ) 
~~ if shaded_std : 
~~~ ax . fill_between ( usex , ymean - ystd , ymean + ystd , color = color , alpha = .2 ) 
~~ ~~ ~~ plt . tight_layout ( ) 
if any ( g2l . keys ( ) ) : 
~~~ ax . legend ( 
g2l . values ( ) , 
loc = 2 if legend_outside else None , 
bbox_to_anchor = ( 1 , 1 ) if legend_outside else None ) 
~~ ax . set_title ( sk ) 
~~ return f , axarr 
~~ def check_synced ( localval , comm = None ) : 
comm = comm or MPI . COMM_WORLD 
vals = comm . gather ( localval ) 
if comm . rank == 0 : 
~~~ assert all ( val == vals [ 0 ] for val in vals [ 1 : ] ) 
~~ ~~ def copy_obs_dict ( obs ) : 
return { k : np . copy ( v ) for k , v in obs . items ( ) } 
~~ def obs_space_info ( obs_space ) : 
if isinstance ( obs_space , gym . spaces . Dict ) : 
~~~ assert isinstance ( obs_space . spaces , OrderedDict ) 
subspaces = obs_space . spaces 
~~~ subspaces = { None : obs_space } 
~~ keys = [ ] 
shapes = { } 
dtypes = { } 
for key , box in subspaces . items ( ) : 
~~~ keys . append ( key ) 
shapes [ key ] = box . shape 
dtypes [ key ] = box . dtype 
~~ return keys , shapes , dtypes 
~~ def q_retrace ( R , D , q_i , v , rho_i , nenvs , nsteps , gamma ) : 
q_is = batch_to_seq ( q_i , nenvs , nsteps , True ) 
vs = batch_to_seq ( v , nenvs , nsteps + 1 , True ) 
v_final = vs [ - 1 ] 
qret = v_final 
qrets = [ ] 
for i in range ( nsteps - 1 , - 1 , - 1 ) : 
~~~ check_shape ( [ qret , ds [ i ] , rs [ i ] , rho_bar [ i ] , q_is [ i ] , vs [ i ] ] , [ [ nenvs ] ] * 6 ) 
qret = rs [ i ] + gamma * qret * ( 1.0 - ds [ i ] ) 
qrets . append ( qret ) 
qret = ( rho_bar [ i ] * ( qret - q_is [ i ] ) ) + vs [ i ] 
~~ qrets = qrets [ : : - 1 ] 
qret = seq_to_batch ( qrets , flat = True ) 
return qret 
~~ def learn ( network , env , seed = None , nsteps = 20 , total_timesteps = int ( 80e6 ) , q_coef = 0.5 , ent_coef = 0.01 , 
max_grad_norm = 10 , lr = 7e-4 , lrschedule = 'linear' , rprop_epsilon = 1e-5 , rprop_alpha = 0.99 , gamma = 0.99 , 
log_interval = 100 , buffer_size = 50000 , replay_ratio = 4 , replay_start = 10000 , c = 10.0 , 
trust_region = True , alpha = 0.99 , delta = 1 , load_path = None , ** network_kwargs ) : 
print ( locals ( ) ) 
if not isinstance ( env , VecFrameStack ) : 
~~~ env = VecFrameStack ( env , 1 ) 
~~ policy = build_policy ( env , network , estimate_q = True , ** network_kwargs ) 
nstack = env . nstack 
model = Model ( policy = policy , ob_space = ob_space , ac_space = ac_space , nenvs = nenvs , nsteps = nsteps , 
ent_coef = ent_coef , q_coef = q_coef , gamma = gamma , 
max_grad_norm = max_grad_norm , lr = lr , rprop_alpha = rprop_alpha , rprop_epsilon = rprop_epsilon , 
total_timesteps = total_timesteps , lrschedule = lrschedule , c = c , 
trust_region = trust_region , alpha = alpha , delta = delta ) 
runner = Runner ( env = env , model = model , nsteps = nsteps ) 
if replay_ratio > 0 : 
~~~ buffer = Buffer ( env = env , nsteps = nsteps , size = buffer_size ) 
~~~ buffer = None 
~~ nbatch = nenvs * nsteps 
acer = Acer ( runner , model , buffer , log_interval ) 
acer . tstart = time . time ( ) 
~~~ acer . call ( on_policy = True ) 
if replay_ratio > 0 and buffer . has_atleast ( replay_start ) : 
~~~ n = np . random . poisson ( replay_ratio ) 
for _ in range ( n ) : 
~~ ~~ ~~ return model 
~~ def apply_stats ( self , statsUpdates ) : 
def updateAccumStats ( ) : 
~~~ if self . _full_stats_init : 
~~~ return tf . cond ( tf . greater ( self . sgd_step , self . _cold_iter ) , lambda : tf . group ( * self . _apply_stats ( statsUpdates , accumulate = True , accumulateCoeff = 1. / self . _stats_accum_iter ) ) , tf . no_op ) 
~~~ return tf . group ( * self . _apply_stats ( statsUpdates , accumulate = True , accumulateCoeff = 1. / self . _stats_accum_iter ) ) 
~~ ~~ def updateRunningAvgStats ( statsUpdates , fac_iter = 1 ) : 
~~~ return tf . group ( * self . _apply_stats ( statsUpdates ) ) 
~~ if self . _async_stats : 
~~~ update_stats = self . _apply_stats ( statsUpdates ) 
queue = tf . FIFOQueue ( 1 , [ item . dtype for item in update_stats ] , shapes = [ 
item . get_shape ( ) for item in update_stats ] ) 
enqueue_op = queue . enqueue ( update_stats ) 
def dequeue_stats_op ( ) : 
~~~ return queue . dequeue ( ) 
~~ self . qr_stats = tf . train . QueueRunner ( queue , [ enqueue_op ] ) 
update_stats_op = tf . cond ( tf . equal ( queue . size ( ) , tf . convert_to_tensor ( 
0 ) ) , tf . no_op , lambda : tf . group ( * [ dequeue_stats_op ( ) , ] ) ) 
~~~ update_stats_op = tf . cond ( tf . greater_equal ( 
self . stats_step , self . _stats_accum_iter ) , lambda : updateRunningAvgStats ( statsUpdates ) , updateAccumStats ) 
~~ self . _update_stats_op = update_stats_op 
return update_stats_op 
~~ def tile_images ( img_nhwc ) : 
img_nhwc = np . asarray ( img_nhwc ) 
N , h , w , c = img_nhwc . shape 
H = int ( np . ceil ( np . sqrt ( N ) ) ) 
W = int ( np . ceil ( float ( N ) / H ) ) 
img_nhwc = np . array ( list ( img_nhwc ) + [ img_nhwc [ 0 ] * 0 for _ in range ( N , H * W ) ] ) 
img_HWhwc = img_nhwc . reshape ( H , W , h , w , c ) 
img_HhWwc = img_HWhwc . transpose ( 0 , 2 , 1 , 3 , 4 ) 
img_Hh_Ww_c = img_HhWwc . reshape ( H * h , W * w , c ) 
return img_Hh_Ww_c 
~~ def sum ( self , start = 0 , end = None ) : 
return super ( SumSegmentTree , self ) . reduce ( start , end ) 
~~ def find_prefixsum_idx ( self , prefixsum ) : 
assert 0 <= prefixsum <= self . sum ( ) + 1e-5 
idx = 1 
~~~ if self . _value [ 2 * idx ] > prefixsum : 
~~~ idx = 2 * idx 
~~~ prefixsum -= self . _value [ 2 * idx ] 
idx = 2 * idx + 1 
~~ ~~ return idx - self . _capacity 
~~ def min ( self , start = 0 , end = None ) : 
return super ( MinSegmentTree , self ) . reduce ( start , end ) 
~~ def value ( self , t ) : 
for ( l_t , l ) , ( r_t , r ) in zip ( self . _endpoints [ : - 1 ] , self . _endpoints [ 1 : ] ) : 
~~~ if l_t <= t and t < r_t : 
~~~ alpha = float ( t - l_t ) / ( r_t - l_t ) 
return self . _interpolation ( l , r , alpha ) 
~~ ~~ assert self . _outside_value is not None 
return self . _outside_value 
~~ def _subproc_worker ( pipe , parent_pipe , env_fn_wrapper , obs_bufs , obs_shapes , obs_dtypes , keys ) : 
def _write_obs ( maybe_dict_obs ) : 
~~~ flatdict = obs_to_dict ( maybe_dict_obs ) 
for k in keys : 
~~~ dst = obs_bufs [ k ] . get_obj ( ) 
np . copyto ( dst_np , flatdict [ k ] ) 
~~ ~~ env = env_fn_wrapper . x ( ) 
parent_pipe . close ( ) 
~~~ cmd , data = pipe . recv ( ) 
if cmd == 'reset' : 
~~~ pipe . send ( _write_obs ( env . reset ( ) ) ) 
~~ elif cmd == 'step' : 
~~~ obs , reward , done , info = env . step ( data ) 
~~ pipe . send ( ( _write_obs ( obs ) , reward , done , info ) ) 
~~ elif cmd == 'render' : 
~~~ pipe . send ( env . render ( mode = 'rgb_array' ) ) 
~~ elif cmd == 'close' : 
~~~ pipe . send ( None ) 
~~ ~~ ~~ except KeyboardInterrupt : 
~~~ env . close ( ) 
~~ ~~ def learn ( 
env , 
nsteps = 5 , 
total_timesteps = int ( 80e6 ) , 
vf_coef = 0.5 , 
ent_coef = 0.01 , 
max_grad_norm = 0.5 , 
lr = 7e-4 , 
lrschedule = 'linear' , 
epsilon = 1e-5 , 
alpha = 0.99 , 
gamma = 0.99 , 
log_interval = 100 , 
** network_kwargs ) : 
model = Model ( policy = policy , env = env , nsteps = nsteps , ent_coef = ent_coef , vf_coef = vf_coef , 
max_grad_norm = max_grad_norm , lr = lr , alpha = alpha , epsilon = epsilon , total_timesteps = total_timesteps , lrschedule = lrschedule ) 
~~ runner = Runner ( env , model , nsteps = nsteps , gamma = gamma ) 
epinfobuf = deque ( maxlen = 100 ) 
tstart = time . time ( ) 
for update in range ( 1 , total_timesteps // nbatch + 1 ) : 
~~~ obs , states , rewards , masks , actions , values , epinfos = runner . run ( ) 
epinfobuf . extend ( epinfos ) 
policy_loss , value_loss , policy_entropy = model . train ( obs , states , rewards , masks , actions , values ) 
nseconds = time . time ( ) - tstart 
fps = int ( ( update * nbatch ) / nseconds ) 
~~~ ev = explained_variance ( values , rewards ) 
logger . record_tabular ( "nupdates" , update ) 
logger . record_tabular ( "total_timesteps" , update * nbatch ) 
logger . record_tabular ( "fps" , fps ) 
logger . record_tabular ( "policy_entropy" , float ( policy_entropy ) ) 
logger . record_tabular ( "value_loss" , float ( value_loss ) ) 
logger . record_tabular ( "explained_variance" , float ( ev ) ) 
logger . record_tabular ( "eprewmean" , safemean ( [ epinfo [ 'r' ] for epinfo in epinfobuf ] ) ) 
logger . record_tabular ( "eplenmean" , safemean ( [ epinfo [ 'l' ] for epinfo in epinfobuf ] ) ) 
~~ def sf01 ( arr ) : 
s = arr . shape 
return arr . swapaxes ( 0 , 1 ) . reshape ( s [ 0 ] * s [ 1 ] , * s [ 2 : ] ) 
~~ def step ( self , observation , ** extra_feed ) : 
a , v , state , neglogp = self . _evaluate ( [ self . action , self . vf , self . state , self . neglogp ] , observation , ** extra_feed ) 
if state . size == 0 : 
~~~ state = None 
~~ return a , v , state , neglogp 
~~ def value ( self , ob , * args , ** kwargs ) : 
return self . _evaluate ( self . vf , ob , * args , ** kwargs ) 
~~ def pretty_eta ( seconds_left ) : 
minutes_left = seconds_left // 60 
seconds_left %= 60 
hours_left = minutes_left // 60 
minutes_left %= 60 
days_left = hours_left // 24 
hours_left %= 24 
def helper ( cnt , name ) : 
~~ if days_left > 0 : 
~~~ msg = helper ( days_left , 'day' ) 
if hours_left > 0 : 
~~ return msg 
~~ if hours_left > 0 : 
~~~ msg = helper ( hours_left , 'hour' ) 
if minutes_left > 0 : 
~~ if minutes_left > 0 : 
~~~ return helper ( minutes_left , 'minute' ) 
~~ def boolean_flag ( parser , name , default = False , help = None ) : 
dest = name . replace ( '-' , '_' ) 
parser . add_argument ( "--" + name , action = "store_true" , default = default , dest = dest , help = help ) 
parser . add_argument ( "--no-" + name , action = "store_false" , dest = dest ) 
~~ def get_wrapper_by_name ( env , classname ) : 
currentenv = env 
~~~ if classname == currentenv . class_name ( ) : 
~~~ return currentenv 
~~ elif isinstance ( currentenv , gym . Wrapper ) : 
~~~ currentenv = currentenv . env 
~~~ raise ValueError ( "Couldn\ % classname ) 
~~ ~~ ~~ def relatively_safe_pickle_dump ( obj , path , compression = False ) : 
temp_storage = path + ".relatively_safe" 
if compression : 
~~~ with tempfile . NamedTemporaryFile ( ) as uncompressed_file : 
~~~ pickle . dump ( obj , uncompressed_file ) 
uncompressed_file . file . flush ( ) 
with zipfile . ZipFile ( temp_storage , "w" , compression = zipfile . ZIP_DEFLATED ) as myzip : 
~~~ myzip . write ( uncompressed_file . name , "data" ) 
~~~ with open ( temp_storage , "wb" ) as f : 
~~~ pickle . dump ( obj , f ) 
~~ ~~ os . rename ( temp_storage , path ) 
~~ def pickle_load ( path , compression = False ) : 
~~~ with zipfile . ZipFile ( path , "r" , compression = zipfile . ZIP_DEFLATED ) as myzip : 
~~~ with myzip . open ( "data" ) as f : 
~~~ return pickle . load ( f ) 
~~~ with open ( path , "rb" ) as f : 
~~ ~~ ~~ def update ( self , new_val ) : 
if self . _value is None : 
~~~ self . _value = new_val 
~~~ self . _value = self . _gamma * self . _value + ( 1.0 - self . _gamma ) * new_val 
~~ ~~ def store_args ( method ) : 
argspec = inspect . getfullargspec ( method ) 
defaults = { } 
if argspec . defaults is not None : 
~~~ defaults = dict ( 
zip ( argspec . args [ - len ( argspec . defaults ) : ] , argspec . defaults ) ) 
~~ if argspec . kwonlydefaults is not None : 
~~~ defaults . update ( argspec . kwonlydefaults ) 
~~ arg_names = argspec . args [ 1 : ] 
@ functools . wraps ( method ) 
def wrapper ( * positional_args , ** keyword_args ) : 
~~~ self = positional_args [ 0 ] 
args = defaults . copy ( ) 
for name , value in zip ( arg_names , positional_args [ 1 : ] ) : 
~~~ args [ name ] = value 
~~ args . update ( keyword_args ) 
self . __dict__ . update ( args ) 
return method ( * positional_args , ** keyword_args ) 
~~ def import_function ( spec ) : 
mod_name , fn_name = spec . split ( ':' ) 
module = importlib . import_module ( mod_name ) 
fn = getattr ( module , fn_name ) 
return fn 
~~ def flatten_grads ( var_list , grads ) : 
return tf . concat ( [ tf . reshape ( grad , [ U . numel ( v ) ] ) 
for ( v , grad ) in zip ( var_list , grads ) ] , 0 ) 
~~ def nn ( input , layers_sizes , reuse = None , flatten = False , name = "" ) : 
for i , size in enumerate ( layers_sizes ) : 
~~~ activation = tf . nn . relu if i < len ( layers_sizes ) - 1 else None 
input = tf . layers . dense ( inputs = input , 
units = size , 
kernel_initializer = tf . contrib . layers . xavier_initializer ( ) , 
reuse = reuse , 
name = name + '_' + str ( i ) ) 
if activation : 
~~~ input = activation ( input ) 
~~ ~~ if flatten : 
~~~ assert layers_sizes [ - 1 ] == 1 
input = tf . reshape ( input , [ - 1 ] ) 
~~ return input 
~~ def mpi_fork ( n , extra_mpi_args = [ ] ) : 
if n <= 1 : 
~~~ return "child" 
~~ if os . getenv ( "IN_MPI" ) is None : 
~~~ env = os . environ . copy ( ) 
env . update ( 
MKL_NUM_THREADS = "1" , 
OMP_NUM_THREADS = "1" , 
IN_MPI = "1" 
args = [ "mpirun" , "-np" , str ( n ) ] + extra_mpi_args + [ sys . executable ] 
args += sys . argv 
subprocess . check_call ( args , env = env ) 
return "parent" 
~~~ install_mpi_excepthook ( ) 
return "child" 
~~ ~~ def convert_episode_to_batch_major ( episode ) : 
episode_batch = { } 
for key in episode . keys ( ) : 
~~~ val = np . array ( episode [ key ] ) . copy ( ) 
episode_batch [ key ] = val . swapaxes ( 0 , 1 ) 
~~ return episode_batch 
~~ def reshape_for_broadcasting ( source , target ) : 
dim = len ( target . get_shape ( ) ) 
shape = ( [ 1 ] * ( dim - 1 ) ) + [ - 1 ] 
return tf . reshape ( tf . cast ( source , target . dtype ) , shape ) 
~~ def add_vtarg_and_adv ( seg , gamma , lam ) : 
vpred = np . append ( seg [ "vpred" ] , seg [ "nextvpred" ] ) 
T = len ( seg [ "rew" ] ) 
seg [ "adv" ] = gaelam = np . empty ( T , 'float32' ) 
rew = seg [ "rew" ] 
lastgaelam = 0 
for t in reversed ( range ( T ) ) : 
~~~ nonterminal = 1 - new [ t + 1 ] 
delta = rew [ t ] + gamma * vpred [ t + 1 ] * nonterminal - vpred [ t ] 
gaelam [ t ] = lastgaelam = delta + gamma * lam * nonterminal * lastgaelam 
~~ seg [ "tdlamret" ] = seg [ "adv" ] + seg [ "vpred" ] 
~~ def switch ( condition , then_expression , else_expression ) : 
x_shape = copy . copy ( then_expression . get_shape ( ) ) 
x = tf . cond ( tf . cast ( condition , 'bool' ) , 
lambda : then_expression , 
lambda : else_expression ) 
x . set_shape ( x_shape ) 
~~ def huber_loss ( x , delta = 1.0 ) : 
return tf . where ( 
tf . abs ( x ) < delta , 
tf . square ( x ) * 0.5 , 
delta * ( tf . abs ( x ) - 0.5 * delta ) 
~~ def get_session ( config = None ) : 
sess = tf . get_default_session ( ) 
if sess is None : 
~~~ sess = make_session ( config = config , make_default = True ) 
~~ return sess 
~~ def make_session ( config = None , num_cpu = None , make_default = False , graph = None ) : 
if num_cpu is None : 
~~~ num_cpu = int ( os . getenv ( 'RCALL_NUM_CPU' , multiprocessing . cpu_count ( ) ) ) 
~~ if config is None : 
~~~ config = tf . ConfigProto ( 
allow_soft_placement = True , 
inter_op_parallelism_threads = num_cpu , 
intra_op_parallelism_threads = num_cpu ) 
config . gpu_options . allow_growth = True 
~~ if make_default : 
~~~ return tf . InteractiveSession ( config = config , graph = graph ) 
~~~ return tf . Session ( config = config , graph = graph ) 
~~ ~~ def initialize ( ) : 
new_variables = set ( tf . global_variables ( ) ) - ALREADY_INITIALIZED 
get_session ( ) . run ( tf . variables_initializer ( new_variables ) ) 
ALREADY_INITIALIZED . update ( new_variables ) 
~~ def function ( inputs , outputs , updates = None , givens = None ) : 
if isinstance ( outputs , list ) : 
~~~ return _Function ( inputs , outputs , updates , givens = givens ) 
~~ elif isinstance ( outputs , ( dict , collections . OrderedDict ) ) : 
~~~ f = _Function ( inputs , outputs . values ( ) , updates , givens = givens ) 
return lambda * args , ** kwargs : type ( outputs ) ( zip ( outputs . keys ( ) , f ( * args , ** kwargs ) ) ) 
~~~ f = _Function ( inputs , [ outputs ] , updates , givens = givens ) 
return lambda * args , ** kwargs : f ( * args , ** kwargs ) [ 0 ] 
~~ ~~ def adjust_shape ( placeholder , data ) : 
if not isinstance ( data , np . ndarray ) and not isinstance ( data , list ) : 
~~ if isinstance ( data , list ) : 
~~~ data = np . array ( data ) 
~~ placeholder_shape = [ x or - 1 for x in placeholder . shape . as_list ( ) ] 
return np . reshape ( data , placeholder_shape ) 
~~ def _check_shape ( placeholder_shape , data_shape ) : 
squeezed_placeholder_shape = _squeeze_shape ( placeholder_shape ) 
squeezed_data_shape = _squeeze_shape ( data_shape ) 
for i , s_data in enumerate ( squeezed_data_shape ) : 
~~~ s_placeholder = squeezed_placeholder_shape [ i ] 
if s_placeholder != - 1 and s_data != s_placeholder : 
~~ def profile ( n ) : 
def decorator_with_name ( func ) : 
~~~ def func_wrapper ( * args , ** kwargs ) : 
~~~ with profile_kv ( n ) : 
~~ ~~ return func_wrapper 
~~ return decorator_with_name 
~~ def wrap_deepmind ( env , episode_life = True , clip_rewards = True , frame_stack = False , scale = False ) : 
if episode_life : 
~~~ env = EpisodicLifeEnv ( env ) 
~~ if 'FIRE' in env . unwrapped . get_action_meanings ( ) : 
~~~ env = FireResetEnv ( env ) 
~~ env = WarpFrame ( env ) 
if scale : 
~~~ env = ScaledFloatFrame ( env ) 
~~ if clip_rewards : 
~~~ env = ClipRewardEnv ( env ) 
~~ if frame_stack : 
~~~ env = FrameStack ( env , 4 ) 
~~ def reset ( self , ** kwargs ) : 
if self . was_real_done : 
~~~ obs = self . env . reset ( ** kwargs ) 
~~~ obs , _ , _ , _ = self . env . step ( 0 ) 
~~ self . lives = self . env . unwrapped . ale . lives ( ) 
return obs 
~~ def sync_from_root ( sess , variables , comm = None ) : 
if comm is None : comm = MPI . COMM_WORLD 
import tensorflow as tf 
values = comm . bcast ( sess . run ( variables ) ) 
sess . run ( [ tf . assign ( var , val ) 
for ( var , val ) in zip ( variables , values ) ] ) 
~~ def gpu_count ( ) : 
if shutil . which ( 'nvidia-smi' ) is None : 
~~ output = subprocess . check_output ( [ 'nvidia-smi' , '--query-gpu=gpu_name' , '--format=csv' ] ) 
return max ( 0 , len ( output . split ( b'\\n' ) ) - 2 ) 
~~ def setup_mpi_gpus ( ) : 
if 'CUDA_VISIBLE_DEVICES' not in os . environ : 
~~~ lrank , _lsize = get_local_rank_size ( MPI . COMM_WORLD ) 
ids = [ lrank ] 
~~ os . environ [ "CUDA_VISIBLE_DEVICES" ] = "," . join ( map ( str , ids ) ) 
~~ ~~ def get_local_rank_size ( comm ) : 
this_node = platform . node ( ) 
ranks_nodes = comm . allgather ( ( comm . Get_rank ( ) , this_node ) ) 
node2rankssofar = defaultdict ( int ) 
local_rank = None 
for ( rank , node ) in ranks_nodes : 
~~~ if rank == comm . Get_rank ( ) : 
~~~ local_rank = node2rankssofar [ node ] 
~~ node2rankssofar [ node ] += 1 
~~ assert local_rank is not None 
return local_rank , node2rankssofar [ this_node ] 
~~ def share_file ( comm , path ) : 
localrank , _ = get_local_rank_size ( comm ) 
if comm . Get_rank ( ) == 0 : 
~~~ with open ( path , 'rb' ) as fh : 
~~~ data = fh . read ( ) 
~~ comm . bcast ( data ) 
~~~ data = comm . bcast ( None ) 
if localrank == 0 : 
~~~ os . makedirs ( os . path . dirname ( path ) , exist_ok = True ) 
with open ( path , 'wb' ) as fh : 
~~~ fh . write ( data ) 
~~ ~~ ~~ comm . Barrier ( ) 
~~ def dict_gather ( comm , d , op = 'mean' , assert_all_have_data = True ) : 
if comm is None : return d 
alldicts = comm . allgather ( d ) 
size = comm . size 
k2li = defaultdict ( list ) 
for d in alldicts : 
~~~ for ( k , v ) in d . items ( ) : 
~~~ k2li [ k ] . append ( v ) 
~~ ~~ result = { } 
for ( k , li ) in k2li . items ( ) : 
~~~ if assert_all_have_data : 
~~ if op == 'mean' : 
~~~ result [ k ] = np . mean ( li , axis = 0 ) 
~~ elif op == 'sum' : 
~~~ result [ k ] = np . sum ( li , axis = 0 ) 
~~~ assert 0 , op 
~~ ~~ return result 
~~ def mpi_weighted_mean ( comm , local_name2valcount ) : 
all_name2valcount = comm . gather ( local_name2valcount ) 
~~~ name2sum = defaultdict ( float ) 
name2count = defaultdict ( float ) 
for n2vc in all_name2valcount : 
~~~ for ( name , ( val , count ) ) in n2vc . items ( ) : 
~~~ val = float ( val ) 
~~~ if comm . rank == 0 : 
~~~ name2sum [ name ] += val * count 
name2count [ name ] += count 
~~ ~~ ~~ return { name : name2sum [ name ] / name2count [ name ] for name in name2sum } 
~~ ~~ def learn ( * , 
total_timesteps , 
max_kl = 0.001 , 
cg_iters = 10 , 
ent_coef = 0.0 , 
cg_damping = 1e-2 , 
vf_stepsize = 3e-4 , 
vf_iters = 3 , 
if MPI is not None : 
~~~ nworkers = MPI . COMM_WORLD . Get_size ( ) 
~~~ nworkers = 1 
rank = 0 
~~ cpus_per_worker = 1 
U . get_session ( config = tf . ConfigProto ( 
inter_op_parallelism_threads = cpus_per_worker , 
intra_op_parallelism_threads = cpus_per_worker 
policy = build_policy ( env , network , value_network = 'copy' , ** network_kwargs ) 
np . set_printoptions ( precision = 3 ) 
ob = observation_placeholder ( ob_space ) 
with tf . variable_scope ( "pi" ) : 
~~~ pi = policy ( observ_placeholder = ob ) 
~~ with tf . variable_scope ( "oldpi" ) : 
~~~ oldpi = policy ( observ_placeholder = ob ) 
ac = pi . pdtype . sample_placeholder ( [ None ] ) 
kloldnew = oldpi . pd . kl ( pi . pd ) 
ent = pi . pd . entropy ( ) 
meankl = tf . reduce_mean ( kloldnew ) 
meanent = tf . reduce_mean ( ent ) 
entbonus = ent_coef * meanent 
vferr = tf . reduce_mean ( tf . square ( pi . vf - ret ) ) 
surrgain = tf . reduce_mean ( ratio * atarg ) 
optimgain = surrgain + entbonus 
losses = [ optimgain , meankl , entbonus , surrgain , meanent ] 
loss_names = [ "optimgain" , "meankl" , "entloss" , "surrgain" , "entropy" ] 
dist = meankl 
all_var_list = get_trainable_variables ( "pi" ) 
var_list = get_pi_trainable_variables ( "pi" ) 
vf_var_list = get_vf_trainable_variables ( "pi" ) 
vfadam = MpiAdam ( vf_var_list ) 
get_flat = U . GetFlat ( var_list ) 
set_from_flat = U . SetFromFlat ( var_list ) 
klgrads = tf . gradients ( dist , var_list ) 
flat_tangent = tf . placeholder ( dtype = tf . float32 , shape = [ None ] , name = "flat_tan" ) 
shapes = [ var . get_shape ( ) . as_list ( ) for var in var_list ] 
start = 0 
tangents = [ ] 
for shape in shapes : 
~~~ sz = U . intprod ( shape ) 
tangents . append ( tf . reshape ( flat_tangent [ start : start + sz ] , shape ) ) 
start += sz 
fvp = U . flatgrad ( gvp , var_list ) 
assign_old_eq_new = U . function ( [ ] , [ ] , updates = [ tf . assign ( oldv , newv ) 
for ( oldv , newv ) in zipsame ( get_variables ( "oldpi" ) , get_variables ( "pi" ) ) ] ) 
compute_losses = U . function ( [ ob , ac , atarg ] , losses ) 
compute_lossandgrad = U . function ( [ ob , ac , atarg ] , losses + [ U . flatgrad ( optimgain , var_list ) ] ) 
compute_fvp = U . function ( [ flat_tangent , ob , ac , atarg ] , fvp ) 
compute_vflossandgrad = U . function ( [ ob , ret ] , U . flatgrad ( vferr , vf_var_list ) ) 
@ contextmanager 
def timed ( msg ) : 
~~~ if rank == 0 : 
~~~ print ( colorize ( msg , color = 'magenta' ) ) 
yield 
~~ ~~ def allmean ( x ) : 
~~~ assert isinstance ( x , np . ndarray ) 
~~~ out = np . empty_like ( x ) 
MPI . COMM_WORLD . Allreduce ( x , out , op = MPI . SUM ) 
out /= nworkers 
~~~ out = np . copy ( x ) 
~~ U . initialize ( ) 
~~~ pi . load ( load_path ) 
~~ th_init = get_flat ( ) 
~~~ MPI . COMM_WORLD . Bcast ( th_init , root = 0 ) 
~~ set_from_flat ( th_init ) 
vfadam . sync ( ) 
seg_gen = traj_segment_generator ( pi , env , timesteps_per_batch , stochastic = True ) 
episodes_so_far = 0 
timesteps_so_far = 0 
iters_so_far = 0 
if sum ( [ max_iters > 0 , total_timesteps > 0 , max_episodes > 0 ] ) == 0 : 
~~~ return pi 
~~~ if callback : callback ( locals ( ) , globals ( ) ) 
if total_timesteps and timesteps_so_far >= total_timesteps : 
~~ elif max_episodes and episodes_so_far >= max_episodes : 
~~ elif max_iters and iters_so_far >= max_iters : 
with timed ( "sampling" ) : 
~~~ seg = seg_gen . __next__ ( ) 
~~ add_vtarg_and_adv ( seg , gamma , lam ) 
ob , ac , atarg , tdlamret = seg [ "ob" ] , seg [ "ac" ] , seg [ "adv" ] , seg [ "tdlamret" ] 
if hasattr ( pi , "ret_rms" ) : pi . ret_rms . update ( tdlamret ) 
args = seg [ "ob" ] , seg [ "ac" ] , atarg 
fvpargs = [ arr [ : : 5 ] for arr in args ] 
def fisher_vector_product ( p ) : 
~~~ return allmean ( compute_fvp ( p , * fvpargs ) ) + cg_damping * p 
with timed ( "computegrad" ) : 
~~~ * lossbefore , g = compute_lossandgrad ( * args ) 
~~ lossbefore = allmean ( np . array ( lossbefore ) ) 
g = allmean ( g ) 
if np . allclose ( g , 0 ) : 
~~~ with timed ( "cg" ) : 
~~~ stepdir = cg ( fisher_vector_product , g , cg_iters = cg_iters , verbose = rank == 0 ) 
~~ assert np . isfinite ( stepdir ) . all ( ) 
shs = .5 * stepdir . dot ( fisher_vector_product ( stepdir ) ) 
lm = np . sqrt ( shs / max_kl ) 
fullstep = stepdir / lm 
expectedimprove = g . dot ( fullstep ) 
surrbefore = lossbefore [ 0 ] 
stepsize = 1.0 
thbefore = get_flat ( ) 
for _ in range ( 10 ) : 
~~~ thnew = thbefore + fullstep * stepsize 
set_from_flat ( thnew ) 
meanlosses = surr , kl , * _ = allmean ( np . array ( compute_losses ( * args ) ) ) 
improve = surr - surrbefore 
if not np . isfinite ( meanlosses ) . all ( ) : 
~~ elif kl > max_kl * 1.5 : 
~~ elif improve < 0 : 
~~ stepsize *= .5 
~~~ logger . log ( "couldn\ ) 
set_from_flat ( thbefore ) 
~~ if nworkers > 1 and iters_so_far % 20 == 0 : 
assert all ( np . allclose ( ps , paramsums [ 0 ] ) for ps in paramsums [ 1 : ] ) 
~~ ~~ for ( lossname , lossval ) in zip ( loss_names , meanlosses ) : 
~~~ logger . record_tabular ( lossname , lossval ) 
~~ with timed ( "vf" ) : 
~~~ for _ in range ( vf_iters ) : 
~~~ for ( mbob , mbret ) in dataset . iterbatches ( ( seg [ "ob" ] , seg [ "tdlamret" ] ) , 
include_final_partial_batch = False , batch_size = 64 ) : 
~~~ g = allmean ( compute_vflossandgrad ( mbob , mbret ) ) 
vfadam . update ( g , vf_stepsize ) 
~~ ~~ ~~ logger . record_tabular ( "ev_tdlam_before" , explained_variance ( vpredbefore , tdlamret ) ) 
~~~ listoflrpairs = [ lrlocal ] 
~~ lens , rews = map ( flatten_lists , zip ( * listoflrpairs ) ) 
lenbuffer . extend ( lens ) 
rewbuffer . extend ( rews ) 
logger . record_tabular ( "EpLenMean" , np . mean ( lenbuffer ) ) 
logger . record_tabular ( "EpRewMean" , np . mean ( rewbuffer ) ) 
logger . record_tabular ( "EpThisIter" , len ( lens ) ) 
episodes_so_far += len ( lens ) 
timesteps_so_far += sum ( lens ) 
iters_so_far += 1 
logger . record_tabular ( "EpisodesSoFar" , episodes_so_far ) 
logger . record_tabular ( "TimestepsSoFar" , timesteps_so_far ) 
logger . record_tabular ( "TimeElapsed" , time . time ( ) - tstart ) 
if rank == 0 : 
~~~ logger . dump_tabular ( ) 
~~ ~~ return pi 
~~ def discount ( x , gamma ) : 
assert x . ndim >= 1 
return scipy . signal . lfilter ( [ 1 ] , [ 1 , - gamma ] , x [ : : - 1 ] , axis = 0 ) [ : : - 1 ] 
~~ def explained_variance ( ypred , y ) : 
assert y . ndim == 1 and ypred . ndim == 1 
vary = np . var ( y ) 
return np . nan if vary == 0 else 1 - np . var ( y - ypred ) / vary 
~~ def discount_with_boundaries ( X , New , gamma ) : 
Y = np . zeros_like ( X ) 
T = X . shape [ 0 ] 
Y [ T - 1 ] = X [ T - 1 ] 
for t in range ( T - 2 , - 1 , - 1 ) : 
~~~ Y [ t ] = X [ t ] + gamma * Y [ t + 1 ] * ( 1 - New [ t + 1 ] ) 
~~ return Y 
~~ def sample ( self , batch_size ) : 
idxes = [ random . randint ( 0 , len ( self . _storage ) - 1 ) for _ in range ( batch_size ) ] 
return self . _encode_sample ( idxes ) 
~~ def add ( self , * args , ** kwargs ) : 
idx = self . _next_idx 
super ( ) . add ( * args , ** kwargs ) 
self . _it_sum [ idx ] = self . _max_priority ** self . _alpha 
self . _it_min [ idx ] = self . _max_priority ** self . _alpha 
~~ def sample ( self , batch_size , beta ) : 
assert beta > 0 
idxes = self . _sample_proportional ( batch_size ) 
weights = [ ] 
p_min = self . _it_min . min ( ) / self . _it_sum . sum ( ) 
max_weight = ( p_min * len ( self . _storage ) ) ** ( - beta ) 
for idx in idxes : 
~~~ p_sample = self . _it_sum [ idx ] / self . _it_sum . sum ( ) 
weight = ( p_sample * len ( self . _storage ) ) ** ( - beta ) 
weights . append ( weight / max_weight ) 
~~ weights = np . array ( weights ) 
encoded_sample = self . _encode_sample ( idxes ) 
return tuple ( list ( encoded_sample ) + [ weights , idxes ] ) 
~~ def update_priorities ( self , idxes , priorities ) : 
assert len ( idxes ) == len ( priorities ) 
for idx , priority in zip ( idxes , priorities ) : 
~~~ assert priority > 0 
assert 0 <= idx < len ( self . _storage ) 
self . _it_sum [ idx ] = priority ** self . _alpha 
self . _it_min [ idx ] = priority ** self . _alpha 
self . _max_priority = max ( self . _max_priority , priority ) 
~~ ~~ def wrap_deepmind_retro ( env , scale = True , frame_stack = 4 ) : 
env = WarpFrame ( env ) 
env = ClipRewardEnv ( env ) 
if frame_stack > 1 : 
~~~ env = FrameStack ( env , frame_stack ) 
~~ if scale : 
~~ def scope_vars ( scope , trainable_only = False ) : 
return tf . get_collection ( 
tf . GraphKeys . TRAINABLE_VARIABLES if trainable_only else tf . GraphKeys . GLOBAL_VARIABLES , 
scope = scope if isinstance ( scope , str ) else scope . name 
~~ def build_act ( make_obs_ph , q_func , num_actions , scope = "deepq" , reuse = None ) : 
with tf . variable_scope ( scope , reuse = reuse ) : 
~~~ observations_ph = make_obs_ph ( "observation" ) 
stochastic_ph = tf . placeholder ( tf . bool , ( ) , name = "stochastic" ) 
update_eps_ph = tf . placeholder ( tf . float32 , ( ) , name = "update_eps" ) 
eps = tf . get_variable ( "eps" , ( ) , initializer = tf . constant_initializer ( 0 ) ) 
q_values = q_func ( observations_ph . get ( ) , num_actions , scope = "q_func" ) 
deterministic_actions = tf . argmax ( q_values , axis = 1 ) 
batch_size = tf . shape ( observations_ph . get ( ) ) [ 0 ] 
random_actions = tf . random_uniform ( tf . stack ( [ batch_size ] ) , minval = 0 , maxval = num_actions , dtype = tf . int64 ) 
chose_random = tf . random_uniform ( tf . stack ( [ batch_size ] ) , minval = 0 , maxval = 1 , dtype = tf . float32 ) < eps 
stochastic_actions = tf . where ( chose_random , random_actions , deterministic_actions ) 
output_actions = tf . cond ( stochastic_ph , lambda : stochastic_actions , lambda : deterministic_actions ) 
update_eps_expr = eps . assign ( tf . cond ( update_eps_ph >= 0 , lambda : update_eps_ph , lambda : eps ) ) 
_act = U . function ( inputs = [ observations_ph , stochastic_ph , update_eps_ph ] , 
outputs = output_actions , 
givens = { update_eps_ph : - 1.0 , stochastic_ph : True } , 
updates = [ update_eps_expr ] ) 
def act ( ob , stochastic = True , update_eps = - 1 ) : 
~~~ return _act ( ob , stochastic , update_eps ) 
~~ return act 
~~ ~~ def build_act_with_param_noise ( make_obs_ph , q_func , num_actions , scope = "deepq" , reuse = None , param_noise_filter_func = None ) : 
if param_noise_filter_func is None : 
~~~ param_noise_filter_func = default_param_noise_filter 
~~ with tf . variable_scope ( scope , reuse = reuse ) : 
update_param_noise_threshold_ph = tf . placeholder ( tf . float32 , ( ) , name = "update_param_noise_threshold" ) 
update_param_noise_scale_ph = tf . placeholder ( tf . bool , ( ) , name = "update_param_noise_scale" ) 
reset_ph = tf . placeholder ( tf . bool , ( ) , name = "reset" ) 
param_noise_scale = tf . get_variable ( "param_noise_scale" , ( ) , initializer = tf . constant_initializer ( 0.01 ) , trainable = False ) 
param_noise_threshold = tf . get_variable ( "param_noise_threshold" , ( ) , initializer = tf . constant_initializer ( 0.05 ) , trainable = False ) 
q_values_perturbed = q_func ( observations_ph . get ( ) , num_actions , scope = "perturbed_q_func" ) 
def perturb_vars ( original_scope , perturbed_scope ) : 
~~~ all_vars = scope_vars ( absolute_scope_name ( original_scope ) ) 
all_perturbed_vars = scope_vars ( absolute_scope_name ( perturbed_scope ) ) 
assert len ( all_vars ) == len ( all_perturbed_vars ) 
perturb_ops = [ ] 
for var , perturbed_var in zip ( all_vars , all_perturbed_vars ) : 
~~~ if param_noise_filter_func ( perturbed_var ) : 
~~~ op = tf . assign ( perturbed_var , var + tf . random_normal ( shape = tf . shape ( var ) , mean = 0. , stddev = param_noise_scale ) ) 
~~~ op = tf . assign ( perturbed_var , var ) 
~~ perturb_ops . append ( op ) 
~~ assert len ( perturb_ops ) == len ( all_vars ) 
return tf . group ( * perturb_ops ) 
~~ q_values_adaptive = q_func ( observations_ph . get ( ) , num_actions , scope = "adaptive_q_func" ) 
perturb_for_adaption = perturb_vars ( original_scope = "q_func" , perturbed_scope = "adaptive_q_func" ) 
kl = tf . reduce_sum ( tf . nn . softmax ( q_values ) * ( tf . log ( tf . nn . softmax ( q_values ) ) - tf . log ( tf . nn . softmax ( q_values_adaptive ) ) ) , axis = - 1 ) 
mean_kl = tf . reduce_mean ( kl ) 
def update_scale ( ) : 
~~~ with tf . control_dependencies ( [ perturb_for_adaption ] ) : 
~~~ update_scale_expr = tf . cond ( mean_kl < param_noise_threshold , 
lambda : param_noise_scale . assign ( param_noise_scale * 1.01 ) , 
lambda : param_noise_scale . assign ( param_noise_scale / 1.01 ) , 
~~ return update_scale_expr 
~~ update_param_noise_threshold_expr = param_noise_threshold . assign ( tf . cond ( update_param_noise_threshold_ph >= 0 , 
lambda : update_param_noise_threshold_ph , lambda : param_noise_threshold ) ) 
deterministic_actions = tf . argmax ( q_values_perturbed , axis = 1 ) 
updates = [ 
update_eps_expr , 
tf . cond ( reset_ph , lambda : perturb_vars ( original_scope = "q_func" , perturbed_scope = "perturbed_q_func" ) , lambda : tf . group ( * [ ] ) ) , 
tf . cond ( update_param_noise_scale_ph , lambda : update_scale ( ) , lambda : tf . Variable ( 0. , trainable = False ) ) , 
update_param_noise_threshold_expr , 
_act = U . function ( inputs = [ observations_ph , stochastic_ph , update_eps_ph , reset_ph , update_param_noise_threshold_ph , update_param_noise_scale_ph ] , 
givens = { update_eps_ph : - 1.0 , stochastic_ph : True , reset_ph : False , update_param_noise_threshold_ph : False , update_param_noise_scale_ph : False } , 
updates = updates ) 
def act ( ob , reset = False , update_param_noise_threshold = False , update_param_noise_scale = False , stochastic = True , update_eps = - 1 ) : 
~~~ return _act ( ob , stochastic , update_eps , reset , update_param_noise_threshold , update_param_noise_scale ) 
~~ ~~ def build_train ( make_obs_ph , q_func , num_actions , optimizer , grad_norm_clipping = None , gamma = 1.0 , 
double_q = True , scope = "deepq" , reuse = None , param_noise = False , param_noise_filter_func = None ) : 
if param_noise : 
~~~ act_f = build_act_with_param_noise ( make_obs_ph , q_func , num_actions , scope = scope , reuse = reuse , 
param_noise_filter_func = param_noise_filter_func ) 
~~~ act_f = build_act ( make_obs_ph , q_func , num_actions , scope = scope , reuse = reuse ) 
~~~ obs_t_input = make_obs_ph ( "obs_t" ) 
act_t_ph = tf . placeholder ( tf . int32 , [ None ] , name = "action" ) 
rew_t_ph = tf . placeholder ( tf . float32 , [ None ] , name = "reward" ) 
obs_tp1_input = make_obs_ph ( "obs_tp1" ) 
done_mask_ph = tf . placeholder ( tf . float32 , [ None ] , name = "done" ) 
importance_weights_ph = tf . placeholder ( tf . float32 , [ None ] , name = "weight" ) 
q_func_vars = tf . get_collection ( tf . GraphKeys . GLOBAL_VARIABLES , scope = tf . get_variable_scope ( ) . name + "/q_func" ) 
q_tp1 = q_func ( obs_tp1_input . get ( ) , num_actions , scope = "target_q_func" ) 
target_q_func_vars = tf . get_collection ( tf . GraphKeys . GLOBAL_VARIABLES , scope = tf . get_variable_scope ( ) . name + "/target_q_func" ) 
q_t_selected = tf . reduce_sum ( q_t * tf . one_hot ( act_t_ph , num_actions ) , 1 ) 
if double_q : 
~~~ q_tp1_using_online_net = q_func ( obs_tp1_input . get ( ) , num_actions , scope = "q_func" , reuse = True ) 
q_tp1_best_using_online_net = tf . argmax ( q_tp1_using_online_net , 1 ) 
q_tp1_best = tf . reduce_sum ( q_tp1 * tf . one_hot ( q_tp1_best_using_online_net , num_actions ) , 1 ) 
~~~ q_tp1_best = tf . reduce_max ( q_tp1 , 1 ) 
~~ q_tp1_best_masked = ( 1.0 - done_mask_ph ) * q_tp1_best 
q_t_selected_target = rew_t_ph + gamma * q_tp1_best_masked 
td_error = q_t_selected - tf . stop_gradient ( q_t_selected_target ) 
errors = U . huber_loss ( td_error ) 
weighted_error = tf . reduce_mean ( importance_weights_ph * errors ) 
if grad_norm_clipping is not None : 
~~~ gradients = optimizer . compute_gradients ( weighted_error , var_list = q_func_vars ) 
for i , ( grad , var ) in enumerate ( gradients ) : 
~~~ if grad is not None : 
~~~ gradients [ i ] = ( tf . clip_by_norm ( grad , grad_norm_clipping ) , var ) 
~~ ~~ optimize_expr = optimizer . apply_gradients ( gradients ) 
~~~ optimize_expr = optimizer . minimize ( weighted_error , var_list = q_func_vars ) 
~~ update_target_expr = [ ] 
for var , var_target in zip ( sorted ( q_func_vars , key = lambda v : v . name ) , 
sorted ( target_q_func_vars , key = lambda v : v . name ) ) : 
~~~ update_target_expr . append ( var_target . assign ( var ) ) 
~~ update_target_expr = tf . group ( * update_target_expr ) 
train = U . function ( 
inputs = [ 
obs_t_input , 
act_t_ph , 
rew_t_ph , 
obs_tp1_input , 
done_mask_ph , 
importance_weights_ph 
] , 
outputs = td_error , 
updates = [ optimize_expr ] 
update_target = U . function ( [ ] , [ ] , updates = [ update_target_expr ] ) 
q_values = U . function ( [ obs_t_input ] , q_t ) 
return act_f , train , update_target , { 'q_values' : q_values } 
~~ ~~ def profile_tf_runningmeanstd ( ) : 
~~~ import time 
from baselines . common import tf_util 
tf_util . get_session ( config = tf . ConfigProto ( 
inter_op_parallelism_threads = 1 , 
intra_op_parallelism_threads = 1 , 
allow_soft_placement = True 
x = np . random . random ( ( 376 , ) ) 
n_trials = 10000 
rms = RunningMeanStd ( ) 
tfrms = TfRunningMeanStd ( ) 
tic1 = time . time ( ) 
for _ in range ( n_trials ) : 
~~~ rms . update ( x ) 
~~ tic2 = time . time ( ) 
~~~ tfrms . update ( x ) 
~~ tic3 = time . time ( ) 
~~~ z1 = rms . mean 
~~~ z2 = tfrms . mean 
~~ assert z1 == z2 
tic3 = time . time ( ) 
~~ def make_sample_her_transitions ( replay_strategy , replay_k , reward_fun ) : 
if replay_strategy == 'future' : 
~~~ future_p = 1 - ( 1. / ( 1 + replay_k ) ) 
~~~ future_p = 0 
~~ def _sample_her_transitions ( episode_batch , batch_size_in_transitions ) : 
T = episode_batch [ 'u' ] . shape [ 1 ] 
rollout_batch_size = episode_batch [ 'u' ] . shape [ 0 ] 
batch_size = batch_size_in_transitions 
episode_idxs = np . random . randint ( 0 , rollout_batch_size , batch_size ) 
t_samples = np . random . randint ( T , size = batch_size ) 
transitions = { key : episode_batch [ key ] [ episode_idxs , t_samples ] . copy ( ) 
for key in episode_batch . keys ( ) } 
her_indexes = np . where ( np . random . uniform ( size = batch_size ) < future_p ) 
future_offset = np . random . uniform ( size = batch_size ) * ( T - t_samples ) 
future_offset = future_offset . astype ( int ) 
future_t = ( t_samples + 1 + future_offset ) [ her_indexes ] 
future_ag = episode_batch [ 'ag' ] [ episode_idxs [ her_indexes ] , future_t ] 
transitions [ 'g' ] [ her_indexes ] = future_ag 
info = { } 
for key , value in transitions . items ( ) : 
~~~ if key . startswith ( 'info_' ) : 
~~~ info [ key . replace ( 'info_' , '' ) ] = value 
~~ ~~ reward_params = { k : transitions [ k ] for k in [ 'ag_2' , 'g' ] } 
reward_params [ 'info' ] = info 
transitions [ 'r' ] = reward_fun ( ** reward_params ) 
transitions = { k : transitions [ k ] . reshape ( batch_size , * transitions [ k ] . shape [ 1 : ] ) 
for k in transitions . keys ( ) } 
assert ( transitions [ 'u' ] . shape [ 0 ] == batch_size_in_transitions ) 
return transitions 
~~ return _sample_her_transitions 
~~ def model ( inpt , num_actions , scope , reuse = False ) : 
~~~ out = inpt 
out = layers . fully_connected ( out , num_outputs = 64 , activation_fn = tf . nn . tanh ) 
out = layers . fully_connected ( out , num_outputs = num_actions , activation_fn = None ) 
return out 
~~ ~~ def sample ( self , batch_size ) : 
buffers = { } 
with self . lock : 
~~~ assert self . current_size > 0 
for key in self . buffers . keys ( ) : 
~~~ buffers [ key ] = self . buffers [ key ] [ : self . current_size ] 
~~ ~~ buffers [ 'o_2' ] = buffers [ 'o' ] [ : , 1 : , : ] 
buffers [ 'ag_2' ] = buffers [ 'ag' ] [ : , 1 : , : ] 
transitions = self . sample_transitions ( buffers , batch_size ) 
for key in ( [ 'r' , 'o_2' , 'ag_2' ] + list ( self . buffers . keys ( ) ) ) : 
~~ return transitions 
~~ def store_episode ( self , episode_batch ) : 
batch_sizes = [ len ( episode_batch [ key ] ) for key in episode_batch . keys ( ) ] 
assert np . all ( np . array ( batch_sizes ) == batch_sizes [ 0 ] ) 
batch_size = batch_sizes [ 0 ] 
~~~ idxs = self . _get_storage_idx ( batch_size ) 
~~~ self . buffers [ key ] [ idxs ] = episode_batch [ key ] 
~~ self . n_transitions_stored += batch_size * self . T 
~~ ~~ def store_episode ( self , episode_batch , update_stats = True ) : 
self . buffer . store_episode ( episode_batch ) 
if update_stats : 
~~~ episode_batch [ 'o_2' ] = episode_batch [ 'o' ] [ : , 1 : , : ] 
episode_batch [ 'ag_2' ] = episode_batch [ 'ag' ] [ : , 1 : , : ] 
num_normalizing_transitions = transitions_in_episode_batch ( episode_batch ) 
transitions = self . sample_transitions ( episode_batch , num_normalizing_transitions ) 
o , g , ag = transitions [ 'o' ] , transitions [ 'g' ] , transitions [ 'ag' ] 
transitions [ 'o' ] , transitions [ 'g' ] = self . _preprocess_og ( o , ag , g ) 
self . o_stats . update ( transitions [ 'o' ] ) 
self . g_stats . update ( transitions [ 'g' ] ) 
self . o_stats . recompute_stats ( ) 
self . g_stats . recompute_stats ( ) 
~~ ~~ def parse_cmdline_kwargs ( args ) : 
def parse ( v ) : 
~~~ assert isinstance ( v , str ) 
~~~ return eval ( v ) 
~~ except ( NameError , SyntaxError ) : 
~~~ return v 
~~ ~~ return { k : parse ( v ) for k , v in parse_unknown_args ( args ) . items ( ) } 
~~ def cached_make_env ( make_env ) : 
if make_env not in CACHED_ENVS : 
~~~ env = make_env ( ) 
CACHED_ENVS [ make_env ] = env 
~~ return CACHED_ENVS [ make_env ] 
~~ def setdefault ( self , key : str , value : str ) -> str : 
set_key = key . lower ( ) . encode ( "latin-1" ) 
set_value = value . encode ( "latin-1" ) 
for idx , ( item_key , item_value ) in enumerate ( self . _list ) : 
~~~ if item_key == set_key : 
~~~ return item_value . decode ( "latin-1" ) 
~~ ~~ self . _list . append ( ( set_key , set_value ) ) 
~~ def append ( self , key : str , value : str ) -> None : 
append_key = key . lower ( ) . encode ( "latin-1" ) 
append_value = value . encode ( "latin-1" ) 
self . _list . append ( ( append_key , append_value ) ) 
~~ def request_response ( func : typing . Callable ) -> ASGIApp : 
is_coroutine = asyncio . iscoroutinefunction ( func ) 
async def app ( scope : Scope , receive : Receive , send : Send ) -> None : 
~~~ request = Request ( scope , receive = receive ) 
if is_coroutine : 
~~~ response = await func ( request ) 
~~~ response = await run_in_threadpool ( func , request ) 
~~ await response ( scope , receive , send ) 
~~ return app 
~~ def websocket_session ( func : typing . Callable ) -> ASGIApp : 
~~~ session = WebSocket ( scope , receive = receive , send = send ) 
await func ( session ) 
~~ def compile_path ( 
path : str 
) -> typing . Tuple [ typing . Pattern , str , typing . Dict [ str , Convertor ] ] : 
path_regex = "^" 
path_format = "" 
idx = 0 
param_convertors = { } 
for match in PARAM_REGEX . finditer ( path ) : 
~~~ param_name , convertor_type = match . groups ( "str" ) 
convertor_type = convertor_type . lstrip ( ":" ) 
assert ( 
convertor_type in CONVERTOR_TYPES 
convertor = CONVERTOR_TYPES [ convertor_type ] 
path_regex += path [ idx : match . start ( ) ] 
path_regex += f"(?P<{param_name}>{convertor.regex})" 
path_format += path [ idx : match . start ( ) ] 
path_format += "{%s}" % param_name 
param_convertors [ param_name ] = convertor 
idx = match . end ( ) 
~~ path_regex += path [ idx : ] + "$" 
path_format += path [ idx : ] 
return re . compile ( path_regex ) , path_format , param_convertors 
~~ def get_endpoints ( 
self , routes : typing . List [ BaseRoute ] 
) -> typing . List [ EndpointInfo ] : 
endpoints_info : list = [ ] 
for route in routes : 
~~~ if isinstance ( route , Mount ) : 
~~~ routes = route . routes or [ ] 
sub_endpoints = [ 
EndpointInfo ( 
path = "" . join ( ( route . path , sub_endpoint . path ) ) , 
http_method = sub_endpoint . http_method , 
func = sub_endpoint . func , 
for sub_endpoint in self . get_endpoints ( routes ) 
endpoints_info . extend ( sub_endpoints ) 
~~ elif not isinstance ( route , Route ) or not route . include_in_schema : 
~~ elif inspect . isfunction ( route . endpoint ) or inspect . ismethod ( route . endpoint ) : 
~~~ for method in route . methods or [ "GET" ] : 
~~~ if method == "HEAD" : 
~~ endpoints_info . append ( 
EndpointInfo ( route . path , method . lower ( ) , route . endpoint ) 
~~~ for method in [ "get" , "post" , "put" , "patch" , "delete" , "options" ] : 
~~~ if not hasattr ( route . endpoint , method ) : 
~~ func = getattr ( route . endpoint , method ) 
endpoints_info . append ( 
EndpointInfo ( route . path , method . lower ( ) , func ) 
~~ ~~ ~~ return endpoints_info 
~~ def parse_docstring ( self , func_or_method : typing . Callable ) -> dict : 
docstring = func_or_method . __doc__ 
if not docstring : 
~~ docstring = docstring . split ( "---" ) [ - 1 ] 
parsed = yaml . safe_load ( docstring ) 
if not isinstance ( parsed , dict ) : 
~~ return parsed 
~~ def get_directories ( 
self , directory : str = None , packages : typing . List [ str ] = None 
) -> typing . List [ str ] : 
directories = [ ] 
if directory is not None : 
~~~ directories . append ( directory ) 
~~ for package in packages or [ ] : 
~~~ spec = importlib . util . find_spec ( package ) 
spec . origin is not None 
directory = os . path . normpath ( os . path . join ( spec . origin , ".." , "statics" ) ) 
assert os . path . isdir ( 
directory 
directories . append ( directory ) 
~~ return directories 
~~ def get_path ( self , scope : Scope ) -> str : 
return os . path . normpath ( os . path . join ( * scope [ "path" ] . split ( "/" ) ) ) 
~~ async def get_response ( self , path : str , scope : Scope ) -> Response : 
if scope [ "method" ] not in ( "GET" , "HEAD" ) : 
~~ if path . startswith ( ".." ) : 
~~ full_path , stat_result = await self . lookup_path ( path ) 
if stat_result and stat . S_ISREG ( stat_result . st_mode ) : 
~~~ return self . file_response ( full_path , stat_result , scope ) 
~~ elif stat_result and stat . S_ISDIR ( stat_result . st_mode ) and self . html : 
~~~ index_path = os . path . join ( path , "index.html" ) 
full_path , stat_result = await self . lookup_path ( index_path ) 
if stat_result is not None and stat . S_ISREG ( stat_result . st_mode ) : 
~~~ if not scope [ "path" ] . endswith ( "/" ) : 
~~~ url = URL ( scope = scope ) 
url = url . replace ( path = url . path + "/" ) 
return RedirectResponse ( url = url ) 
~~ return self . file_response ( full_path , stat_result , scope ) 
~~ ~~ if self . html : 
~~~ full_path , stat_result = await self . lookup_path ( "404.html" ) 
~~~ return self . file_response ( 
full_path , stat_result , scope , status_code = 404 
~~ async def check_config ( self ) -> None : 
if self . directory is None : 
~~~ stat_result = await aio_stat ( self . directory ) 
~~ except FileNotFoundError : 
~~ if not ( stat . S_ISDIR ( stat_result . st_mode ) or stat . S_ISLNK ( stat_result . st_mode ) ) : 
~~ ~~ def is_not_modified ( 
self , response_headers : Headers , request_headers : Headers 
) -> bool : 
~~~ if_none_match = request_headers [ "if-none-match" ] 
etag = response_headers [ "etag" ] 
if if_none_match == etag : 
~~~ if_modified_since = parsedate ( request_headers [ "if-modified-since" ] ) 
last_modified = parsedate ( response_headers [ "last-modified" ] ) 
if ( 
if_modified_since is not None 
and last_modified is not None 
and if_modified_since >= last_modified 
~~ def build_environ ( scope : Scope , body : bytes ) -> dict : 
environ = { 
"REQUEST_METHOD" : scope [ "method" ] , 
"SCRIPT_NAME" : scope . get ( "root_path" , "" ) , 
"PATH_INFO" : scope [ "path" ] , 
"QUERY_STRING" : scope [ "query_string" ] . decode ( "ascii" ) , 
"SERVER_PROTOCOL" : f"HTTP/{scope[\ , 
"wsgi.version" : ( 1 , 0 ) , 
"wsgi.url_scheme" : scope . get ( "scheme" , "http" ) , 
"wsgi.input" : io . BytesIO ( body ) , 
"wsgi.errors" : sys . stdout , 
"wsgi.multithread" : True , 
"wsgi.multiprocess" : True , 
"wsgi.run_once" : False , 
server = scope . get ( "server" ) or ( "localhost" , 80 ) 
environ [ "SERVER_NAME" ] = server [ 0 ] 
environ [ "SERVER_PORT" ] = server [ 1 ] 
if scope . get ( "client" ) : 
~~~ environ [ "REMOTE_ADDR" ] = scope [ "client" ] [ 0 ] 
~~ for name , value in scope . get ( "headers" , [ ] ) : 
~~~ name = name . decode ( "latin1" ) 
if name == "content-length" : 
~~~ corrected_name = "CONTENT_LENGTH" 
~~ elif name == "content-type" : 
~~~ corrected_name = "CONTENT_TYPE" 
~~~ corrected_name = f"HTTP_{name}" . upper ( ) . replace ( "-" , "_" ) 
~~ value = value . decode ( "latin1" ) 
if corrected_name in environ : 
~~~ value = environ [ corrected_name ] + "," + value 
~~ environ [ corrected_name ] = value 
~~ return environ 
~~ async def receive ( self ) -> Message : 
if self . client_state == WebSocketState . CONNECTING : 
~~~ message = await self . _receive ( ) 
message_type = message [ "type" ] 
assert message_type == "websocket.connect" 
self . client_state = WebSocketState . CONNECTED 
return message 
~~ elif self . client_state == WebSocketState . CONNECTED : 
assert message_type in { "websocket.receive" , "websocket.disconnect" } 
if message_type == "websocket.disconnect" : 
~~~ self . client_state = WebSocketState . DISCONNECTED 
~~ ~~ async def send ( self , message : Message ) -> None : 
if self . application_state == WebSocketState . CONNECTING : 
~~~ message_type = message [ "type" ] 
assert message_type in { "websocket.accept" , "websocket.close" } 
if message_type == "websocket.close" : 
~~~ self . application_state = WebSocketState . DISCONNECTED 
~~~ self . application_state = WebSocketState . CONNECTED 
~~ await self . _send ( message ) 
~~ elif self . application_state == WebSocketState . CONNECTED : 
assert message_type in { "websocket.send" , "websocket.close" } 
~~~ raise RuntimeError ( \ ) 
~~ ~~ def load_voc_dataset ( path = 'data' , dataset = '2012' , contain_classes_in_person = False ) : 
path = os . path . join ( path , 'VOC' ) 
def _recursive_parse_xml_to_dict ( xml ) : 
if xml is not None : 
~~~ return { xml . tag : xml . text } 
~~ result = { } 
for child in xml : 
~~~ child_result = _recursive_parse_xml_to_dict ( child ) 
if child . tag != 'object' : 
~~~ result [ child . tag ] = child_result [ child . tag ] 
~~~ if child . tag not in result : 
~~~ result [ child . tag ] = [ ] 
~~ result [ child . tag ] . append ( child_result [ child . tag ] ) 
~~ ~~ return { xml . tag : result } 
~~ import xml . etree . ElementTree as ET 
if dataset == "2012" : 
~~~ url = "http://pjreddie.com/media/files/" 
tar_filename = "VOCtrainval_11-May-2012.tar" 
extracted_filename = "VOC2012" #"VOCdevkit/VOC2012" 
~~ elif dataset == "2012test" : 
~~~ extracted_filename = "VOC2012test" #"VOCdevkit/VOC2012" 
logging . info ( 
import time 
time . sleep ( 3 ) 
if os . path . isdir ( os . path . join ( path , extracted_filename ) ) is False : 
exit ( ) 
~~ ~~ elif dataset == "2007" : 
tar_filename = "VOCtrainval_06-Nov-2007.tar" 
extracted_filename = "VOC2007" 
~~ elif dataset == "2007test" : 
tar_filename = "VOCtest_06-Nov-2007.tar" 
extracted_filename = "VOC2007test" 
~~ if dataset != "2012test" : 
~~~ from sys import platform as _platform 
if folder_exists ( os . path . join ( path , extracted_filename ) ) is False : 
maybe_download_and_extract ( tar_filename , path , url , extract = True ) 
del_file ( os . path . join ( path , tar_filename ) ) 
~~~ if _platform == "win32" : 
~~ ~~ elif dataset == "2007test" : 
~~ ~~ del_folder ( os . path . join ( path , 'VOCdevkit' ) ) 
~~ ~~ classes = [ 
"aeroplane" , "bicycle" , "bird" , "boat" , "bottle" , "bus" , "car" , "cat" , "chair" , "cow" , "diningtable" , "dog" , 
"horse" , "motorbike" , "person" , "pottedplant" , "sheep" , "sofa" , "train" , "tvmonitor" 
if contain_classes_in_person : 
~~~ classes_in_person = [ "head" , "hand" , "foot" ] 
~~~ classes_in_person = [ ] 
classes_dict = utils . list_string_to_dict ( classes ) 
folder_imgs = os . path . join ( path , extracted_filename , "JPEGImages" ) 
imgs_file_list = load_file_list ( path = folder_imgs , regx = '\\\\.jpg' , printable = False ) 
imgs_file_list . sort ( 
imgs_file_list = [ os . path . join ( folder_imgs , s ) for s in imgs_file_list ] 
if dataset != "2012test" : 
~~~ folder_semseg = os . path . join ( path , extracted_filename , "SegmentationClass" ) 
imgs_semseg_file_list = load_file_list ( path = folder_semseg , regx = '\\\\.png' , printable = False ) 
imgs_semseg_file_list . sort ( 
imgs_semseg_file_list = [ os . path . join ( folder_semseg , s ) for s in imgs_semseg_file_list ] 
folder_insseg = os . path . join ( path , extracted_filename , "SegmentationObject" ) 
imgs_insseg_file_list = load_file_list ( path = folder_insseg , regx = '\\\\.png' , printable = False ) 
imgs_insseg_file_list . sort ( 
imgs_insseg_file_list = [ os . path . join ( folder_insseg , s ) for s in imgs_insseg_file_list ] 
~~~ imgs_semseg_file_list = [ ] 
imgs_insseg_file_list = [ ] 
~~ folder_ann = os . path . join ( path , extracted_filename , "Annotations" ) 
imgs_ann_file_list = load_file_list ( path = folder_ann , regx = '\\\\.xml' , printable = False ) 
imgs_ann_file_list . sort ( 
imgs_ann_file_list = [ os . path . join ( folder_ann , s ) for s in imgs_ann_file_list ] 
~~~ imgs_file_list_new = [ ] 
for ann in imgs_ann_file_list : 
~~~ ann = os . path . split ( ann ) [ - 1 ] . split ( '.' ) [ 0 ] 
for im in imgs_file_list : 
~~~ if ann in im : 
~~~ imgs_file_list_new . append ( im ) 
~~ ~~ ~~ imgs_file_list = imgs_file_list_new 
~~ def convert ( size , box ) : 
~~~ dw = 1. / size [ 0 ] 
dh = 1. / size [ 1 ] 
x = ( box [ 0 ] + box [ 1 ] ) / 2.0 
y = ( box [ 2 ] + box [ 3 ] ) / 2.0 
w = box [ 1 ] - box [ 0 ] 
h = box [ 3 ] - box [ 2 ] 
x = x * dw 
w = w * dw 
y = y * dh 
h = h * dh 
return x , y , w , h 
~~ def convert_annotation ( file_name ) : 
in_file = open ( file_name ) 
out_file = "" 
tree = ET . parse ( in_file ) 
root = tree . getroot ( ) 
size = root . find ( 'size' ) 
w = int ( size . find ( 'width' ) . text ) 
h = int ( size . find ( 'height' ) . text ) 
n_objs = 0 
for obj in root . iter ( 'object' ) : 
~~~ if dataset != "2012test" : 
~~~ difficult = obj . find ( 'difficult' ) . text 
cls = obj . find ( 'name' ) . text 
if cls not in classes or int ( difficult ) == 1 : 
~~~ cls = obj . find ( 'name' ) . text 
if cls not in classes : 
~~ ~~ cls_id = classes . index ( cls ) 
xmlbox = obj . find ( 'bndbox' ) 
b = ( 
float ( xmlbox . find ( 'xmin' ) . text ) , float ( xmlbox . find ( 'xmax' ) . text ) , float ( xmlbox . find ( 'ymin' ) . text ) , 
float ( xmlbox . find ( 'ymax' ) . text ) 
bb = convert ( ( w , h ) , b ) 
n_objs += 1 
if cls in "person" : 
~~~ for part in obj . iter ( 'part' ) : 
~~~ cls = part . find ( 'name' ) . text 
if cls not in classes_in_person : 
~~ cls_id = classes . index ( cls ) 
xmlbox = part . find ( 'bndbox' ) 
float ( xmlbox . find ( 'xmin' ) . text ) , float ( xmlbox . find ( 'xmax' ) . text ) , 
float ( xmlbox . find ( 'ymin' ) . text ) , float ( xmlbox . find ( 'ymax' ) . text ) 
~~ ~~ ~~ in_file . close ( ) 
return n_objs , out_file 
n_objs_list = [ ] 
objs_info_dicts = { } 
for idx , ann_file in enumerate ( imgs_ann_file_list ) : 
~~~ n_objs , objs_info = convert_annotation ( ann_file ) 
n_objs_list . append ( n_objs ) 
objs_info_list . append ( objs_info ) 
with tf . gfile . GFile ( ann_file , 'r' ) as fid : 
~~~ xml_str = fid . read ( ) 
~~ xml = etree . fromstring ( xml_str ) 
data = _recursive_parse_xml_to_dict ( xml ) [ 'annotation' ] 
objs_info_dicts . update ( { imgs_file_list [ idx ] : data } ) 
~~ return imgs_file_list , imgs_semseg_file_list , imgs_insseg_file_list , imgs_ann_file_list , classes , classes_in_person , classes_dict , n_objs_list , objs_info_list , objs_info_dicts 
~~ def main ( _ ) : 
if FLAGS . model == "small" : 
~~~ init_scale = 0.1 
learning_rate = 1.0 
max_grad_norm = 5 
num_steps = 20 
hidden_size = 200 
max_epoch = 4 
max_max_epoch = 13 
keep_prob = 1.0 
lr_decay = 0.5 
batch_size = 20 
vocab_size = 10000 
~~ elif FLAGS . model == "medium" : 
~~~ init_scale = 0.05 
num_steps = 35 
hidden_size = 650 
max_epoch = 6 
max_max_epoch = 39 
keep_prob = 0.5 
lr_decay = 0.8 
~~ elif FLAGS . model == "large" : 
~~~ init_scale = 0.04 
max_grad_norm = 10 
hidden_size = 1500 
max_epoch = 14 
max_max_epoch = 55 
keep_prob = 0.35 
lr_decay = 1 / 1.15 
~~ train_data , valid_data , test_data , vocab_size = tl . files . load_ptb_dataset ( ) 
sess = tf . InteractiveSession ( ) 
input_data = tf . placeholder ( tf . int32 , [ batch_size , num_steps ] ) 
targets = tf . placeholder ( tf . int32 , [ batch_size , num_steps ] ) 
input_data_test = tf . placeholder ( tf . int32 , [ 1 , 1 ] ) 
targets_test = tf . placeholder ( tf . int32 , [ 1 , 1 ] ) 
def inference ( x , is_training , num_steps , reuse = None ) : 
init = tf . random_uniform_initializer ( - init_scale , init_scale ) 
with tf . variable_scope ( "model" , reuse = reuse ) : 
~~~ net = tl . layers . EmbeddingInputlayer ( x , vocab_size , hidden_size , init , name = 'embedding' ) 
net = tl . layers . DropoutLayer ( net , keep = keep_prob , is_fix = True , is_train = is_training , name = 'drop1' ) 
net = tl . layers . RNNLayer ( 
net , 
n_hidden = hidden_size , 
initializer = init , 
n_steps = num_steps , 
return_last = False , 
name = 'basic_lstm_layer1' 
lstm1 = net 
net = tl . layers . DropoutLayer ( net , keep = keep_prob , is_fix = True , is_train = is_training , name = 'drop2' ) 
return_seq_2d = True , 
name = 'basic_lstm_layer2' 
lstm2 = net 
net = tl . layers . DropoutLayer ( net , keep = keep_prob , is_fix = True , is_train = is_training , name = 'drop3' ) 
net = tl . layers . DenseLayer ( net , vocab_size , W_init = init , b_init = init , act = None , name = 'output' ) 
~~ return net , lstm1 , lstm2 
~~ net , lstm1 , lstm2 = inference ( input_data , is_training = True , num_steps = num_steps , reuse = None ) 
net_val , lstm1_val , lstm2_val = inference ( input_data , is_training = False , num_steps = num_steps , reuse = True ) 
net_test , lstm1_test , lstm2_test = inference ( input_data_test , is_training = False , num_steps = 1 , reuse = True ) 
sess . run ( tf . global_variables_initializer ( ) ) 
~~~ loss = tf . contrib . legacy_seq2seq . sequence_loss_by_example ( 
[ outputs ] , [ tf . reshape ( targets , [ - 1 ] ) ] , [ tf . ones_like ( tf . reshape ( targets , [ - 1 ] ) , dtype = tf . float32 ) ] 
cost = tf . reduce_sum ( loss ) / batch_size 
return cost 
with tf . variable_scope ( 'learning_rate' ) : 
~~~ lr = tf . Variable ( 0.0 , trainable = False ) 
~~ tvars = tf . trainable_variables ( ) 
grads , _ = tf . clip_by_global_norm ( tf . gradients ( cost , tvars ) , max_grad_norm ) 
optimizer = tf . train . GradientDescentOptimizer ( lr ) 
train_op = optimizer . apply_gradients ( zip ( grads , tvars ) ) 
net . print_params ( ) 
net . print_layers ( ) 
tl . layers . print_all_variables ( ) 
for i in range ( max_max_epoch ) : 
~~~ new_lr_decay = lr_decay ** max ( i - max_epoch , 0.0 ) 
sess . run ( tf . assign ( lr , learning_rate * new_lr_decay ) ) 
epoch_size = ( ( len ( train_data ) // batch_size ) - 1 ) // num_steps 
start_time = time . time ( ) 
costs = 0.0 
iters = 0 
state1 = tl . layers . initialize_rnn_state ( lstm1 . initial_state ) 
state2 = tl . layers . initialize_rnn_state ( lstm2 . initial_state ) 
for step , ( x , y ) in enumerate ( tl . iterate . ptb_iterator ( train_data , batch_size , num_steps ) ) : 
~~~ feed_dict = { 
input_data : x , 
targets : y , 
lstm1 . initial_state : state1 , 
lstm2 . initial_state : state2 , 
feed_dict . update ( net . all_drop ) 
_cost , state1 , state2 , _ = sess . run ( 
[ cost , lstm1 . final_state , lstm2 . final_state , train_op ] , feed_dict = feed_dict 
costs += _cost 
iters += num_steps 
if step % ( epoch_size // 10 ) == 10 : 
( step * 1.0 / epoch_size , np . exp ( costs / iters ) , iters * batch_size / ( time . time ( ) - start_time ) ) 
~~ ~~ train_perplexity = np . exp ( costs / iters ) 
state1 = tl . layers . initialize_rnn_state ( lstm1_val . initial_state ) 
state2 = tl . layers . initialize_rnn_state ( lstm2_val . initial_state ) 
for step , ( x , y ) in enumerate ( tl . iterate . ptb_iterator ( valid_data , batch_size , num_steps ) ) : 
lstm1_val . initial_state : state1 , 
lstm2_val . initial_state : state2 , 
[ cost_val , lstm1_val . final_state , lstm2_val . final_state , 
tf . no_op ( ) ] , feed_dict = feed_dict 
~~ valid_perplexity = np . exp ( costs / iters ) 
~~ print ( "Evaluation" ) 
state1 = tl . layers . initialize_rnn_state ( lstm1_test . initial_state ) 
state2 = tl . layers . initialize_rnn_state ( lstm2_test . initial_state ) 
for step , ( x , y ) in enumerate ( tl . iterate . ptb_iterator ( test_data , batch_size = 1 , num_steps = 1 ) ) : 
input_data_test : x , 
targets_test : y , 
lstm1_test . initial_state : state1 , 
lstm2_test . initial_state : state2 , 
_cost , state1 , state2 = sess . run ( 
[ cost_test , lstm1_test . final_state , lstm2_test . final_state ] , feed_dict = feed_dict 
iters += 1 
~~ test_perplexity = np . exp ( costs / iters ) 
print ( 
~~ def private_method ( func ) : 
def func_wrapper ( * args , ** kwargs ) : 
outer_frame = inspect . stack ( ) [ 1 ] [ 0 ] 
if 'self' not in outer_frame . f_locals or outer_frame . f_locals [ 'self' ] is not args [ 0 ] : 
~~ return func ( * args , ** kwargs ) 
~~ return func_wrapper 
~~ def protected_method ( func ) : 
caller = inspect . getmro ( outer_frame . f_locals [ 'self' ] . __class__ ) [ : - 1 ] 
target = inspect . getmro ( args [ 0 ] . __class__ ) [ : - 1 ] 
share_subsclass = False 
for cls_ in target : 
~~~ if issubclass ( caller [ 0 ] , cls_ ) or caller [ 0 ] is cls_ : 
~~~ share_subsclass = True 
~~ ~~ if ( 'self' not in outer_frame . f_locals or 
outer_frame . f_locals [ 'self' ] is not args [ 0 ] ) and ( not share_subsclass ) : 
~~ def atrous_conv1d ( 
prev_layer , 
n_filter = 32 , 
filter_size = 2 , 
stride = 1 , 
dilation = 1 , 
act = None , 
padding = 'SAME' , 
data_format = 'NWC' , 
W_init = tf . truncated_normal_initializer ( stddev = 0.02 ) , 
b_init = tf . constant_initializer ( value = 0.0 ) , 
W_init_args = None , 
b_init_args = None , 
name = 'atrous_1d' , 
return Conv1dLayer ( 
prev_layer = prev_layer , 
act = act , 
shape = ( filter_size , int ( prev_layer . outputs . get_shape ( ) [ - 1 ] ) , n_filter ) , 
padding = padding , 
dilation_rate = dilation , 
data_format = data_format , 
W_init = W_init , 
b_init = b_init , 
W_init_args = W_init_args , 
b_init_args = b_init_args , 
name = name , 
~~ def _GetNextLogCountPerToken ( token ) : 
_log_counter_per_token [ token ] = 1 + _log_counter_per_token . get ( token , - 1 ) 
return _log_counter_per_token [ token ] 
~~ def log_every_n ( level , msg , n , * args ) : 
count = _GetNextLogCountPerToken ( _GetFileAndLine ( ) ) 
log_if ( level , msg , not ( count % n ) , * args ) 
~~ def log_if ( level , msg , condition , * args ) : 
if condition : 
~~~ vlog ( level , msg , * args ) 
~~ ~~ def _GetFileAndLine ( ) : 
f = _sys . _getframe ( ) 
our_file = f . f_code . co_filename 
f = f . f_back 
while f : 
~~~ code = f . f_code 
if code . co_filename != our_file : 
~~~ return ( code . co_filename , f . f_lineno ) 
~~ f = f . f_back 
~~ return ( '<unknown>' , 0 ) 
~~ def google2_log_prefix ( level , timestamp = None , file_and_line = None ) : 
global _level_names 
now = timestamp or _time . time ( ) 
now_tuple = _time . localtime ( now ) 
now_microsecond = int ( 1e6 * ( now % 1.0 ) ) 
( filename , line ) = file_and_line or _GetFileAndLine ( ) 
basename = _os . path . basename ( filename ) 
severity = 'I' 
if level in _level_names : 
~~~ severity = _level_names [ level ] [ 0 ] 
severity , 
now_microsecond , 
_get_thread_id ( ) , 
basename , 
line 
return s 
~~ def load_mpii_pose_dataset ( path = 'data' , is_16_pos_only = False ) : 
path = os . path . join ( path , 'mpii_human_pose' ) 
url = "http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/" 
tar_filename = "mpii_human_pose_v1_u12_2.zip" 
extracted_filename = "mpii_human_pose_v1_u12_2" 
~~ url = "http://datasets.d2.mpi-inf.mpg.de/andriluka14cvpr/" 
tar_filename = "mpii_human_pose_v1.tar.gz" 
extracted_filename2 = "images" 
if folder_exists ( os . path . join ( path , extracted_filename2 ) ) is False : 
~~ import scipy . io as sio 
ann_train_list = [ ] 
ann_test_list = [ ] 
img_train_list = [ ] 
img_test_list = [ ] 
def save_joints ( ) : 
~~~ mat = sio . loadmat ( os . path . join ( path , extracted_filename , "mpii_human_pose_v1_u12_1.mat" ) ) 
zip ( mat [ 'RELEASE' ] [ 'annolist' ] [ 0 , 0 ] [ 0 ] , mat [ 'RELEASE' ] [ 'img_train' ] [ 0 , 0 ] [ 0 ] ) ) : 
~~~ img_fn = anno [ 'image' ] [ 'name' ] [ 0 , 0 ] [ 0 ] 
train_flag = int ( train_flag ) 
if train_flag : 
~~~ img_train_list . append ( img_fn ) 
ann_train_list . append ( [ ] ) 
~~~ img_test_list . append ( img_fn ) 
ann_test_list . append ( [ ] ) 
~~ head_rect = [ ] 
if 'x1' in str ( anno [ 'annorect' ] . dtype ) : 
~~~ head_rect = zip ( 
[ x1 [ 0 , 0 ] for x1 in anno [ 'annorect' ] [ 'x1' ] [ 0 ] ] , [ y1 [ 0 , 0 ] for y1 in anno [ 'annorect' ] [ 'y1' ] [ 0 ] ] , 
[ x2 [ 0 , 0 ] for x2 in anno [ 'annorect' ] [ 'x2' ] [ 0 ] ] , [ y2 [ 0 , 0 ] for y2 in anno [ 'annorect' ] [ 'y2' ] [ 0 ] ] 
~~ if 'annopoints' in str ( anno [ 'annorect' ] . dtype ) : 
~~~ annopoints = anno [ 'annorect' ] [ 'annopoints' ] [ 0 ] 
head_x1s = anno [ 'annorect' ] [ 'x1' ] [ 0 ] 
head_y1s = anno [ 'annorect' ] [ 'y1' ] [ 0 ] 
head_x2s = anno [ 'annorect' ] [ 'x2' ] [ 0 ] 
head_y2s = anno [ 'annorect' ] [ 'y2' ] [ 0 ] 
for annopoint , head_x1 , head_y1 , head_x2 , head_y2 in zip ( annopoints , head_x1s , head_y1s , head_x2s , 
head_y2s ) : 
~~~ if annopoint . size : 
~~~ head_rect = [ 
float ( head_x1 [ 0 , 0 ] ) , 
float ( head_y1 [ 0 , 0 ] ) , 
float ( head_x2 [ 0 , 0 ] ) , 
float ( head_y2 [ 0 , 0 ] ) 
annopoint = annopoint [ 'point' ] [ 0 , 0 ] 
j_id = [ str ( j_i [ 0 , 0 ] ) for j_i in annopoint [ 'id' ] [ 0 ] ] 
x = [ x [ 0 , 0 ] for x in annopoint [ 'x' ] [ 0 ] ] 
y = [ y [ 0 , 0 ] for y in annopoint [ 'y' ] [ 0 ] ] 
joint_pos = { } 
for _j_id , ( _x , _y ) in zip ( j_id , zip ( x , y ) ) : 
~~~ joint_pos [ int ( _j_id ) ] = [ float ( _x ) , float ( _y ) ] 
~~ if 'is_visible' in str ( annopoint . dtype ) : 
~~~ vis = [ v [ 0 ] if v . size > 0 else [ 0 ] for v in annopoint [ 'is_visible' ] [ 0 ] ] 
vis = dict ( [ ( k , int ( v [ 0 ] ) ) if len ( v ) > 0 else v for k , v in zip ( j_id , vis ) ] ) 
~~~ vis = None 
~~ if ( ( is_16_pos_only == True ) and ( len ( joint_pos ) == 16 ) ) or ( is_16_pos_only == False ) : 
~~~ data = { 
'filename' : img_fn , 
'train' : train_flag , 
'head_rect' : head_rect , 
'is_visible' : vis , 
'joint_pos' : joint_pos 
~~~ ann_train_list [ - 1 ] . append ( data ) 
~~~ ann_test_list [ - 1 ] . append ( data ) 
# 
~~ ~~ ~~ ~~ ~~ ~~ ~~ save_joints ( ) 
img_dir = os . path . join ( path , extracted_filename2 ) 
_img_list = load_file_list ( path = os . path . join ( path , extracted_filename2 ) , regx = '\\\\.jpg' , printable = False ) 
for i , im in enumerate ( img_train_list ) : 
~~~ if im not in _img_list : 
del img_train_list [ i ] 
del ann_train_list [ i ] 
~~ ~~ for i , im in enumerate ( img_test_list ) : 
~~ ~~ n_train_images = len ( img_train_list ) 
n_test_images = len ( img_test_list ) 
n_images = n_train_images + n_test_images 
n_train_ann = len ( ann_train_list ) 
n_test_ann = len ( ann_test_list ) 
n_ann = n_train_ann + n_test_ann 
n_train_people = len ( sum ( ann_train_list , [ ] ) ) 
n_test_people = len ( sum ( ann_test_list , [ ] ) ) 
n_people = n_train_people + n_test_people 
for i , value in enumerate ( img_train_list ) : 
~~~ img_train_list [ i ] = os . path . join ( img_dir , value ) 
~~ for i , value in enumerate ( img_test_list ) : 
~~~ img_test_list [ i ] = os . path . join ( img_dir , value ) 
~~ return img_train_list , ann_train_list , img_test_list , ann_test_list 
~~ def transformer ( U , theta , out_size , name = 'SpatialTransformer2dAffine' ) : 
def _repeat ( x , n_repeats ) : 
~~~ with tf . variable_scope ( '_repeat' ) : 
~~~ rep = tf . transpose ( tf . expand_dims ( tf . ones ( shape = tf . stack ( [ 
n_repeats , 
] ) ) , 1 ) , [ 1 , 0 ] ) 
rep = tf . cast ( rep , 'int32' ) 
x = tf . matmul ( tf . reshape ( x , ( - 1 , 1 ) ) , rep ) 
return tf . reshape ( x , [ - 1 ] ) 
~~ ~~ def _interpolate ( im , x , y , out_size ) : 
~~~ with tf . variable_scope ( '_interpolate' ) : 
~~~ num_batch = tf . shape ( im ) [ 0 ] 
height = tf . shape ( im ) [ 1 ] 
width = tf . shape ( im ) [ 2 ] 
channels = tf . shape ( im ) [ 3 ] 
x = tf . cast ( x , 'float32' ) 
y = tf . cast ( y , 'float32' ) 
height_f = tf . cast ( height , 'float32' ) 
width_f = tf . cast ( width , 'float32' ) 
out_height = out_size [ 0 ] 
out_width = out_size [ 1 ] 
zero = tf . zeros ( [ ] , dtype = 'int32' ) 
max_y = tf . cast ( tf . shape ( im ) [ 1 ] - 1 , 'int32' ) 
max_x = tf . cast ( tf . shape ( im ) [ 2 ] - 1 , 'int32' ) 
x = ( x + 1.0 ) * ( width_f ) / 2.0 
y = ( y + 1.0 ) * ( height_f ) / 2.0 
x0 = tf . cast ( tf . floor ( x ) , 'int32' ) 
x1 = x0 + 1 
y0 = tf . cast ( tf . floor ( y ) , 'int32' ) 
y1 = y0 + 1 
x0 = tf . clip_by_value ( x0 , zero , max_x ) 
x1 = tf . clip_by_value ( x1 , zero , max_x ) 
y0 = tf . clip_by_value ( y0 , zero , max_y ) 
y1 = tf . clip_by_value ( y1 , zero , max_y ) 
dim2 = width 
dim1 = width * height 
base = _repeat ( tf . range ( num_batch ) * dim1 , out_height * out_width ) 
base_y0 = base + y0 * dim2 
base_y1 = base + y1 * dim2 
idx_a = base_y0 + x0 
idx_b = base_y1 + x0 
idx_c = base_y0 + x1 
idx_d = base_y1 + x1 
im_flat = tf . reshape ( im , tf . stack ( [ - 1 , channels ] ) ) 
im_flat = tf . cast ( im_flat , 'float32' ) 
Ia = tf . gather ( im_flat , idx_a ) 
Ib = tf . gather ( im_flat , idx_b ) 
Ic = tf . gather ( im_flat , idx_c ) 
Id = tf . gather ( im_flat , idx_d ) 
x0_f = tf . cast ( x0 , 'float32' ) 
x1_f = tf . cast ( x1 , 'float32' ) 
y0_f = tf . cast ( y0 , 'float32' ) 
y1_f = tf . cast ( y1 , 'float32' ) 
wa = tf . expand_dims ( ( ( x1_f - x ) * ( y1_f - y ) ) , 1 ) 
wb = tf . expand_dims ( ( ( x1_f - x ) * ( y - y0_f ) ) , 1 ) 
wc = tf . expand_dims ( ( ( x - x0_f ) * ( y1_f - y ) ) , 1 ) 
wd = tf . expand_dims ( ( ( x - x0_f ) * ( y - y0_f ) ) , 1 ) 
output = tf . add_n ( [ wa * Ia , wb * Ib , wc * Ic , wd * Id ] ) 
~~ ~~ def _meshgrid ( height , width ) : 
~~~ with tf . variable_scope ( '_meshgrid' ) : 
~~~ x_t = tf . matmul ( 
tf . ones ( shape = tf . stack ( [ height , 1 ] ) ) , 
tf . transpose ( tf . expand_dims ( tf . linspace ( - 1.0 , 1.0 , width ) , 1 ) , [ 1 , 0 ] ) 
y_t = tf . matmul ( tf . expand_dims ( tf . linspace ( - 1.0 , 1.0 , height ) , 1 ) , tf . ones ( shape = tf . stack ( [ 1 , width ] ) ) ) 
x_t_flat = tf . reshape ( x_t , ( 1 , - 1 ) ) 
y_t_flat = tf . reshape ( y_t , ( 1 , - 1 ) ) 
ones = tf . ones_like ( x_t_flat ) 
grid = tf . concat ( axis = 0 , values = [ x_t_flat , y_t_flat , ones ] ) 
return grid 
~~ ~~ def _transform ( theta , input_dim , out_size ) : 
~~~ with tf . variable_scope ( '_transform' ) : 
~~~ num_batch = tf . shape ( input_dim ) [ 0 ] 
num_channels = tf . shape ( input_dim ) [ 3 ] 
theta = tf . reshape ( theta , ( - 1 , 2 , 3 ) ) 
theta = tf . cast ( theta , 'float32' ) 
grid = _meshgrid ( out_height , out_width ) 
grid = tf . expand_dims ( grid , 0 ) 
grid = tf . reshape ( grid , [ - 1 ] ) 
grid = tf . tile ( grid , tf . stack ( [ num_batch ] ) ) 
grid = tf . reshape ( grid , tf . stack ( [ num_batch , 3 , - 1 ] ) ) 
T_g = tf . matmul ( theta , grid ) 
x_s = tf . slice ( T_g , [ 0 , 0 , 0 ] , [ - 1 , 1 , - 1 ] ) 
y_s = tf . slice ( T_g , [ 0 , 1 , 0 ] , [ - 1 , 1 , - 1 ] ) 
x_s_flat = tf . reshape ( x_s , [ - 1 ] ) 
y_s_flat = tf . reshape ( y_s , [ - 1 ] ) 
input_transformed = _interpolate ( input_dim , x_s_flat , y_s_flat , out_size ) 
output = tf . reshape ( input_transformed , tf . stack ( [ num_batch , out_height , out_width , num_channels ] ) ) 
~~ ~~ with tf . variable_scope ( name ) : 
~~~ output = _transform ( theta , U , out_size ) 
~~ ~~ def batch_transformer ( U , thetas , out_size , name = 'BatchSpatialTransformer2dAffine' ) : 
with tf . variable_scope ( name ) : 
~~~ num_batch , num_transforms = map ( int , thetas . get_shape ( ) . as_list ( ) [ : 2 ] ) 
indices = [ [ i ] * num_transforms for i in xrange ( num_batch ) ] 
input_repeated = tf . gather ( U , tf . reshape ( indices , [ - 1 ] ) ) 
return transformer ( input_repeated , thetas , out_size ) 
~~ ~~ def create_task_spec_def ( ) : 
if 'TF_CONFIG' in os . environ : 
~~~ env = json . loads ( os . environ . get ( 'TF_CONFIG' , '{}' ) ) 
task_data = env . get ( 'task' , None ) or { 'type' : 'master' , 'index' : 0 } 
cluster_data = env . get ( 'cluster' , None ) or { 'ps' : None , 'worker' : None , 'master' : None } 
return TaskSpecDef ( 
task_type = task_data [ 'type' ] , index = task_data [ 'index' ] , 
trial = task_data [ 'trial' ] if 'trial' in task_data else None , ps_hosts = cluster_data [ 'ps' ] , 
worker_hosts = cluster_data [ 'worker' ] , master = cluster_data [ 'master' ] if 'master' in cluster_data else None 
~~ elif 'JOB_NAME' in os . environ : 
~~~ return TaskSpecDef ( 
task_type = os . environ [ 'JOB_NAME' ] , index = os . environ [ 'TASK_INDEX' ] , ps_hosts = os . environ . get ( 'PS_HOSTS' , None ) , 
worker_hosts = os . environ . get ( 'WORKER_HOSTS' , None ) , master = os . environ . get ( 'MASTER_HOST' , None ) 
~~ ~~ def create_distributed_session ( 
task_spec = None , checkpoint_dir = None , scaffold = None , hooks = None , chief_only_hooks = None , save_checkpoint_secs = 600 , 
save_summaries_steps = object ( ) , save_summaries_secs = object ( ) , config = None , stop_grace_period_secs = 120 , 
log_step_count_steps = 100 
target = task_spec . target ( ) if task_spec is not None else None 
is_chief = task_spec . is_master ( ) if task_spec is not None else True 
return tf . train . MonitoredTrainingSession ( 
master = target , is_chief = is_chief , checkpoint_dir = checkpoint_dir , scaffold = scaffold , 
save_checkpoint_secs = save_checkpoint_secs , save_summaries_steps = save_summaries_steps , 
save_summaries_secs = save_summaries_secs , log_step_count_steps = log_step_count_steps , 
stop_grace_period_secs = stop_grace_period_secs , config = config , hooks = hooks , chief_only_hooks = chief_only_hooks 
~~ def validation_metrics ( self ) : 
if ( self . _validation_iterator is None ) or ( self . _validation_metrics is None ) : 
~~ n = 0.0 
metric_sums = [ 0.0 ] * len ( self . _validation_metrics ) 
self . _sess . run ( self . _validation_iterator . initializer ) 
~~~ metrics = self . _sess . run ( self . _validation_metrics ) 
for i , m in enumerate ( metrics ) : 
~~~ metric_sums [ i ] += m 
~~ n += 1.0 
~~ except tf . errors . OutOfRangeError : 
~~ ~~ for i , m in enumerate ( metric_sums ) : 
~~~ metric_sums [ i ] = metric_sums [ i ] / n 
~~ return zip ( self . _validation_metrics , metric_sums ) 
~~ def train_and_validate_to_end ( self , validate_step_size = 50 ) : 
while not self . _sess . should_stop ( ) : 
if self . global_step % validate_step_size == 0 : 
for n , m in self . validation_metrics : 
~~ logging . info ( log_str ) 
~~ ~~ ~~ def _load_mnist_dataset ( shape , path , name = 'mnist' , url = 'http://yann.lecun.com/exdb/mnist/' ) : 
path = os . path . join ( path , name ) 
def load_mnist_images ( path , filename ) : 
~~~ filepath = maybe_download_and_extract ( filename , path , url ) 
logging . info ( filepath ) 
with gzip . open ( filepath , 'rb' ) as f : 
~~~ data = np . frombuffer ( f . read ( ) , np . uint8 , offset = 16 ) 
~~ data = data . reshape ( shape ) 
return data / np . float32 ( 256 ) 
~~ def load_mnist_labels ( path , filename ) : 
~~~ data = np . frombuffer ( f . read ( ) , np . uint8 , offset = 8 ) 
X_train = load_mnist_images ( path , 'train-images-idx3-ubyte.gz' ) 
y_train = load_mnist_labels ( path , 'train-labels-idx1-ubyte.gz' ) 
X_test = load_mnist_images ( path , 't10k-images-idx3-ubyte.gz' ) 
y_test = load_mnist_labels ( path , 't10k-labels-idx1-ubyte.gz' ) 
X_train , X_val = X_train [ : - 10000 ] , X_train [ - 10000 : ] 
y_train , y_val = y_train [ : - 10000 ] , y_train [ - 10000 : ] 
X_train = np . asarray ( X_train , dtype = np . float32 ) 
y_train = np . asarray ( y_train , dtype = np . int32 ) 
X_val = np . asarray ( X_val , dtype = np . float32 ) 
y_val = np . asarray ( y_val , dtype = np . int32 ) 
X_test = np . asarray ( X_test , dtype = np . float32 ) 
y_test = np . asarray ( y_test , dtype = np . int32 ) 
return X_train , y_train , X_val , y_val , X_test , y_test 
~~ def load_cifar10_dataset ( shape = ( - 1 , 32 , 32 , 3 ) , path = 'data' , plotable = False ) : 
path = os . path . join ( path , 'cifar10' ) 
def unpickle ( file ) : 
~~~ fp = open ( file , 'rb' ) 
if sys . version_info . major == 2 : 
~~~ data = pickle . load ( fp ) 
~~ elif sys . version_info . major == 3 : 
~~~ data = pickle . load ( fp , encoding = 'latin-1' ) 
~~ fp . close ( ) 
return data 
~~ filename = 'cifar-10-python.tar.gz' 
url = 'https://www.cs.toronto.edu/~kriz/' 
maybe_download_and_extract ( filename , path , url , extract = True ) 
X_train = None 
y_train = [ ] 
for i in range ( 1 , 6 ) : 
~~~ data_dic = unpickle ( os . path . join ( path , 'cifar-10-batches-py/' , "data_batch_{}" . format ( i ) ) ) 
if i == 1 : 
~~~ X_train = data_dic [ 'data' ] 
~~~ X_train = np . vstack ( ( X_train , data_dic [ 'data' ] ) ) 
~~ y_train += data_dic [ 'labels' ] 
~~ test_data_dic = unpickle ( os . path . join ( path , 'cifar-10-batches-py/' , "test_batch" ) ) 
X_test = test_data_dic [ 'data' ] 
y_test = np . array ( test_data_dic [ 'labels' ] ) 
if shape == ( - 1 , 3 , 32 , 32 ) : 
~~~ X_test = X_test . reshape ( shape ) 
X_train = X_train . reshape ( shape ) 
~~ elif shape == ( - 1 , 32 , 32 , 3 ) : 
~~~ X_test = X_test . reshape ( shape , order = 'F' ) 
X_train = X_train . reshape ( shape , order = 'F' ) 
X_test = np . transpose ( X_test , ( 0 , 2 , 1 , 3 ) ) 
X_train = np . transpose ( X_train , ( 0 , 2 , 1 , 3 ) ) 
~~ y_train = np . array ( y_train ) 
if plotable : 
~~~ logging . info ( '\\nCIFAR-10' ) 
fig = plt . figure ( 1 ) 
count = 1 
~~~ _ = fig . add_subplot ( 10 , 10 , count ) 
~~~ plt . imshow ( np . transpose ( X_train [ count - 1 ] , ( 1 , 2 , 0 ) ) , interpolation = 'nearest' ) 
~~~ plt . imshow ( X_train [ count - 1 ] , interpolation = 'nearest' ) 
plt . gca ( ) . yaxis . set_major_locator ( plt . NullLocator ( ) ) 
count = count + 1 
~~ X_train = np . asarray ( X_train , dtype = np . float32 ) 
return X_train , y_train , X_test , y_test 
~~ def load_cropped_svhn ( path = 'data' , include_extra = True ) : 
path = os . path . join ( path , 'cropped_svhn' ) 
url = "http://ufldl.stanford.edu/housenumbers/" 
np_file = os . path . join ( path , "train_32x32.npz" ) 
if file_exists ( np_file ) is False : 
~~~ filename = "train_32x32.mat" 
filepath = maybe_download_and_extract ( filename , path , url ) 
mat = sio . loadmat ( filepath ) 
X_train = np . transpose ( X_train , ( 3 , 0 , 1 , 2 ) ) 
y_train = np . squeeze ( mat [ 'y' ] , axis = 1 ) 
np . savez ( np_file , X = X_train , y = y_train ) 
del_file ( filepath ) 
~~~ v = np . load ( np_file ) 
X_train = v [ 'X' ] 
y_train = v [ 'y' ] 
np_file = os . path . join ( path , "test_32x32.npz" ) 
~~~ filename = "test_32x32.mat" 
X_test = mat [ 'X' ] / 255.0 
X_test = np . transpose ( X_test , ( 3 , 0 , 1 , 2 ) ) 
y_test = np . squeeze ( mat [ 'y' ] , axis = 1 ) 
y_test [ y_test == 10 ] = 0 
np . savez ( np_file , X = X_test , y = y_test ) 
X_test = v [ 'X' ] 
y_test = v [ 'y' ] 
if include_extra : 
np_file = os . path . join ( path , "extra_32x32.npz" ) 
filename = "extra_32x32.mat" 
X_extra = mat [ 'X' ] / 255.0 
X_extra = np . transpose ( X_extra , ( 3 , 0 , 1 , 2 ) ) 
y_extra = np . squeeze ( mat [ 'y' ] , axis = 1 ) 
y_extra [ y_extra == 10 ] = 0 
np . savez ( np_file , X = X_extra , y = y_extra ) 
X_extra = v [ 'X' ] 
y_extra = v [ 'y' ] 
t = time . time ( ) 
X_train = np . concatenate ( ( X_train , X_extra ) , 0 ) 
y_train = np . concatenate ( ( y_train , y_extra ) , 0 ) 
~~ def load_ptb_dataset ( path = 'data' ) : 
path = os . path . join ( path , 'ptb' ) 
filename = 'simple-examples.tgz' 
url = 'http://www.fit.vutbr.cz/~imikolov/rnnlm/' 
data_path = os . path . join ( path , 'simple-examples' , 'data' ) 
train_path = os . path . join ( data_path , "ptb.train.txt" ) 
valid_path = os . path . join ( data_path , "ptb.valid.txt" ) 
test_path = os . path . join ( data_path , "ptb.test.txt" ) 
word_to_id = nlp . build_vocab ( nlp . read_words ( train_path ) ) 
train_data = nlp . words_to_word_ids ( nlp . read_words ( train_path ) , word_to_id ) 
valid_data = nlp . words_to_word_ids ( nlp . read_words ( valid_path ) , word_to_id ) 
test_data = nlp . words_to_word_ids ( nlp . read_words ( test_path ) , word_to_id ) 
vocab_size = len ( word_to_id ) 
return train_data , valid_data , test_data , vocab_size 
~~ def load_matt_mahoney_text8_dataset ( path = 'data' ) : 
path = os . path . join ( path , 'mm_test8' ) 
filename = 'text8.zip' 
url = 'http://mattmahoney.net/dc/' 
maybe_download_and_extract ( filename , path , url , expected_bytes = 31344016 ) 
with zipfile . ZipFile ( os . path . join ( path , filename ) ) as f : 
~~~ word_list = f . read ( f . namelist ( ) [ 0 ] ) . split ( ) 
for idx , _ in enumerate ( word_list ) : 
~~~ word_list [ idx ] = word_list [ idx ] . decode ( ) 
~~ ~~ return word_list 
~~ def load_imdb_dataset ( 
path = 'data' , nb_words = None , skip_top = 0 , maxlen = None , test_split = 0.2 , seed = 113 , start_char = 1 , oov_char = 2 , 
index_from = 3 
path = os . path . join ( path , 'imdb' ) 
filename = "imdb.pkl" 
url = 'https://s3.amazonaws.com/text-datasets/' 
maybe_download_and_extract ( filename , path , url ) 
if filename . endswith ( ".gz" ) : 
~~~ f = gzip . open ( os . path . join ( path , filename ) , 'rb' ) 
~~~ f = open ( os . path . join ( path , filename ) , 'rb' ) 
~~ X , labels = cPickle . load ( f ) 
f . close ( ) 
np . random . seed ( seed ) 
np . random . shuffle ( X ) 
np . random . shuffle ( labels ) 
if start_char is not None : 
~~~ X = [ [ start_char ] + [ w + index_from for w in x ] for x in X ] 
~~ elif index_from : 
~~~ X = [ [ w + index_from for w in x ] for x in X ] 
~~ if maxlen : 
~~~ new_X = [ ] 
new_labels = [ ] 
for x , y in zip ( X , labels ) : 
~~~ if len ( x ) < maxlen : 
~~~ new_X . append ( x ) 
new_labels . append ( y ) 
~~ ~~ X = new_X 
labels = new_labels 
~~ if not X : 
~~ if not nb_words : 
~~~ nb_words = max ( [ max ( x ) for x in X ] ) 
~~ if oov_char is not None : 
~~~ X = [ [ oov_char if ( w >= nb_words or w < skip_top ) else w for w in x ] for x in X ] 
~~~ nX = [ ] 
for x in X : 
~~~ nx = [ ] 
for w in x : 
~~~ if ( w >= nb_words or w < skip_top ) : 
~~~ nx . append ( w ) 
~~ ~~ nX . append ( nx ) 
~~ X = nX 
~~ X_train = np . array ( X [ : int ( len ( X ) * ( 1 - test_split ) ) ] ) 
y_train = np . array ( labels [ : int ( len ( X ) * ( 1 - test_split ) ) ] ) 
X_test = np . array ( X [ int ( len ( X ) * ( 1 - test_split ) ) : ] ) 
y_test = np . array ( labels [ int ( len ( X ) * ( 1 - test_split ) ) : ] ) 
~~ def load_nietzsche_dataset ( path = 'data' ) : 
path = os . path . join ( path , 'nietzsche' ) 
filename = "nietzsche.txt" 
with open ( filepath , "r" ) as f : 
~~~ words = f . read ( ) 
return words 
~~ ~~ def load_wmt_en_fr_dataset ( path = 'data' ) : 
path = os . path . join ( path , 'wmt_en_fr' ) 
_WMT_ENFR_TRAIN_URL = "http://www.statmt.org/wmt10/" 
_WMT_ENFR_DEV_URL = "http://www.statmt.org/wmt15/" 
def gunzip_file ( gz_path , new_path ) : 
with gzip . open ( gz_path , "rb" ) as gz_file : 
~~~ with open ( new_path , "wb" ) as new_file : 
~~~ for line in gz_file : 
~~~ new_file . write ( line ) 
~~ ~~ ~~ ~~ def get_wmt_enfr_train_set ( path ) : 
filename = "training-giga-fren.tar" 
maybe_download_and_extract ( filename , path , _WMT_ENFR_TRAIN_URL , extract = True ) 
train_path = os . path . join ( path , "giga-fren.release2.fixed" ) 
gunzip_file ( train_path + ".fr.gz" , train_path + ".fr" ) 
gunzip_file ( train_path + ".en.gz" , train_path + ".en" ) 
return train_path 
~~ def get_wmt_enfr_dev_set ( path ) : 
filename = "dev-v2.tgz" 
dev_file = maybe_download_and_extract ( filename , path , _WMT_ENFR_DEV_URL , extract = False ) 
dev_name = "newstest2013" 
dev_path = os . path . join ( path , "newstest2013" ) 
if not ( gfile . Exists ( dev_path + ".fr" ) and gfile . Exists ( dev_path + ".en" ) ) : 
with tarfile . open ( dev_file , "r:gz" ) as dev_tar : 
~~~ fr_dev_file = dev_tar . getmember ( "dev/" + dev_name + ".fr" ) 
en_dev_file = dev_tar . getmember ( "dev/" + dev_name + ".en" ) 
en_dev_file . name = dev_name + ".en" 
dev_tar . extract ( fr_dev_file , path ) 
dev_tar . extract ( en_dev_file , path ) 
~~ ~~ return dev_path 
train_path = get_wmt_enfr_train_set ( path ) 
dev_path = get_wmt_enfr_dev_set ( path ) 
return train_path , dev_path 
~~ def load_flickr25k_dataset ( tag = 'sky' , path = "data" , n_threads = 50 , printable = False ) : 
path = os . path . join ( path , 'flickr25k' ) 
filename = 'mirflickr25k.zip' 
url = 'http://press.liacs.nl/mirflickr/mirflickr25k/' 
if folder_exists ( os . path . join ( path , "mirflickr" ) ) is False : 
del_file ( os . path . join ( path , filename ) ) 
~~ folder_imgs = os . path . join ( path , "mirflickr" ) 
path_imgs = load_file_list ( path = folder_imgs , regx = '\\\\.jpg' , printable = False ) 
path_imgs . sort ( key = natural_keys ) 
folder_tags = os . path . join ( path , "mirflickr" , "meta" , "tags" ) 
path_tags = load_file_list ( path = folder_tags , regx = '\\\\.txt' , printable = False ) 
path_tags . sort ( key = natural_keys ) 
if tag is None : 
~~ images_list = [ ] 
for idx , _v in enumerate ( path_tags ) : 
~~~ tags = read_file ( os . path . join ( folder_tags , path_tags [ idx ] ) ) . split ( '\\n' ) 
if tag is None or tag in tags : 
~~~ images_list . append ( path_imgs [ idx ] ) 
~~ ~~ images = visualize . read_images ( images_list , folder_imgs , n_threads = n_threads , printable = printable ) 
return images 
~~ def load_flickr1M_dataset ( tag = 'sky' , size = 10 , path = "data" , n_threads = 50 , printable = False ) : 
path = os . path . join ( path , 'flickr1M' ) 
images_zip = [ 
'images0.zip' , 'images1.zip' , 'images2.zip' , 'images3.zip' , 'images4.zip' , 'images5.zip' , 'images6.zip' , 
'images7.zip' , 'images8.zip' , 'images9.zip' 
tag_zip = 'tags.zip' 
url = 'http://press.liacs.nl/mirflickr/mirflickr1m/' 
for image_zip in images_zip [ 0 : size ] : 
~~~ image_folder = image_zip . split ( "." ) [ 0 ] 
if folder_exists ( os . path . join ( path , image_folder ) ) is False : 
maybe_download_and_extract ( image_zip , path , url , extract = True ) 
del_file ( os . path . join ( path , image_zip ) ) 
shutil . move ( os . path . join ( path , 'images' ) , os . path . join ( path , image_folder ) ) 
~~ ~~ if folder_exists ( os . path . join ( path , "tags" ) ) is False : 
maybe_download_and_extract ( tag_zip , path , url , extract = True ) 
del_file ( os . path . join ( path , tag_zip ) ) 
images_folder_list = [ ] 
for i in range ( 0 , size ) : 
~~~ images_folder_list += load_folder_list ( path = os . path . join ( path , 'images%d' % i ) ) 
for folder in images_folder_list [ 0 : size * 10 ] : 
~~~ tmp = load_file_list ( path = folder , regx = '\\\\.jpg' , printable = False ) 
images_list . extend ( [ os . path . join ( folder , x ) for x in tmp ] ) 
~~ tag_list = [ ] 
tag_folder_list = load_folder_list ( os . path . join ( path , "tags" ) ) 
tag_folder_list . sort ( key = lambda s : int ( os . path . basename ( s ) ) ) 
for folder in tag_folder_list [ 0 : size * 10 ] : 
~~~ tmp = load_file_list ( path = folder , regx = '\\\\.txt' , printable = False ) 
tmp = [ os . path . join ( folder , s ) for s in tmp ] 
tag_list += tmp 
select_images_list = [ ] 
for idx , _val in enumerate ( tag_list ) : 
~~~ tags = read_file ( tag_list [ idx ] ) . split ( '\\n' ) 
if tag in tags : 
~~~ select_images_list . append ( images_list [ idx ] ) 
images = visualize . read_images ( select_images_list , '' , n_threads = n_threads , printable = printable ) 
~~ def load_cyclegan_dataset ( filename = 'summer2winter_yosemite' , path = 'data' ) : 
path = os . path . join ( path , 'cyclegan' ) 
url = 'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/' 
if folder_exists ( os . path . join ( path , filename ) ) is False : 
maybe_download_and_extract ( filename + '.zip' , path , url , extract = True ) 
del_file ( os . path . join ( path , filename + '.zip' ) ) 
~~ def load_image_from_folder ( path ) : 
~~~ path_imgs = load_file_list ( path = path , regx = '\\\\.jpg' , printable = False ) 
return visualize . read_images ( path_imgs , path = path , n_threads = 10 , printable = False ) 
~~ im_train_A = load_image_from_folder ( os . path . join ( path , filename , "trainA" ) ) 
im_train_B = load_image_from_folder ( os . path . join ( path , filename , "trainB" ) ) 
im_test_A = load_image_from_folder ( os . path . join ( path , filename , "testA" ) ) 
im_test_B = load_image_from_folder ( os . path . join ( path , filename , "testB" ) ) 
~~~ for i , _v in enumerate ( images ) : 
~~~ if len ( images [ i ] . shape ) == 2 : 
~~~ images [ i ] = images [ i ] [ : , : , np . newaxis ] 
images [ i ] = np . tile ( images [ i ] , ( 1 , 1 , 3 ) ) 
~~ ~~ return images 
~~ im_train_A = if_2d_to_3d ( im_train_A ) 
im_train_B = if_2d_to_3d ( im_train_B ) 
im_test_A = if_2d_to_3d ( im_test_A ) 
im_test_B = if_2d_to_3d ( im_test_B ) 
return im_train_A , im_train_B , im_test_A , im_test_B 
~~ def download_file_from_google_drive ( ID , destination ) : 
def save_response_content ( response , destination , chunk_size = 32 * 1024 ) : 
~~~ total_size = int ( response . headers . get ( 'content-length' , 0 ) ) 
with open ( destination , "wb" ) as f : 
~~~ for chunk in tqdm ( response . iter_content ( chunk_size ) , total = total_size , unit = 'B' , unit_scale = True , 
desc = destination ) : 
~~~ f . write ( chunk ) 
~~ ~~ ~~ ~~ def get_confirm_token ( response ) : 
~~~ for key , value in response . cookies . items ( ) : 
~~~ if key . startswith ( 'download_warning' ) : 
~~ URL = "https://docs.google.com/uc?export=download" 
session = requests . Session ( ) 
response = session . get ( URL , params = { 'id' : ID } , stream = True ) 
token = get_confirm_token ( response ) 
if token : 
~~~ params = { 'id' : ID , 'confirm' : token } 
response = session . get ( URL , params = params , stream = True ) 
~~ save_response_content ( response , destination ) 
~~ def load_celebA_dataset ( path = 'data' ) : 
data_dir = 'celebA' 
filename , drive_id = "img_align_celeba.zip" , "0B7EVK8r0v71pZjFTYXZWM3FlRnM" 
save_path = os . path . join ( path , filename ) 
image_path = os . path . join ( path , data_dir ) 
if os . path . exists ( image_path ) : 
~~~ exists_or_mkdir ( path ) 
download_file_from_google_drive ( drive_id , save_path ) 
zip_dir = '' 
with zipfile . ZipFile ( save_path ) as zf : 
~~~ zip_dir = zf . namelist ( ) [ 0 ] 
zf . extractall ( path ) 
~~ os . remove ( save_path ) 
os . rename ( os . path . join ( path , zip_dir ) , image_path ) 
~~ data_files = load_file_list ( path = image_path , regx = '\\\\.jpg' , printable = False ) 
for i , _v in enumerate ( data_files ) : 
~~~ data_files [ i ] = os . path . join ( image_path , data_files [ i ] ) 
~~ return data_files 
~~ def save_npz ( save_list = None , name = 'model.npz' , sess = None ) : 
if save_list is None : 
~~~ save_list = [ ] 
~~ save_list_var = [ ] 
if sess : 
~~~ save_list_var = sess . run ( save_list ) 
~~~ save_list_var . extend ( [ v . eval ( ) for v in save_list ] ) 
~~~ logging . info ( 
~~ ~~ np . savez ( name , params = save_list_var ) 
save_list_var = None 
del save_list_var 
~~ def load_npz ( path = '' , name = 'model.npz' ) : 
d = np . load ( os . path . join ( path , name ) ) 
return d [ 'params' ] 
~~ def assign_params ( sess , params , network ) : 
ops = [ ] 
for idx , param in enumerate ( params ) : 
~~~ ops . append ( network . all_params [ idx ] . assign ( param ) ) 
~~ if sess is not None : 
~~~ sess . run ( ops ) 
~~ return ops 
~~ def load_and_assign_npz ( sess = None , name = None , network = None ) : 
if network is None : 
~~ if sess is None : 
~~ if not os . path . exists ( name ) : 
~~~ params = load_npz ( name = name ) 
assign_params ( sess , params , network ) 
return network 
~~ ~~ def save_npz_dict ( save_list = None , name = 'model.npz' , sess = None ) : 
~~ if save_list is None : 
~~ save_list_names = [ tensor . name for tensor in save_list ] 
save_list_var = sess . run ( save_list ) 
save_var_dict = { save_list_names [ idx ] : val for idx , val in enumerate ( save_list_var ) } 
np . savez ( name , ** save_var_dict ) 
save_var_dict = None 
del save_var_dict 
~~ def load_and_assign_npz_dict ( name = 'model.npz' , sess = None ) : 
~~ params = np . load ( name ) 
if len ( params . keys ( ) ) != len ( set ( params . keys ( ) ) ) : 
~~ ops = list ( ) 
for key in params . keys ( ) : 
~~~ varlist = tf . get_collection ( tf . GraphKeys . GLOBAL_VARIABLES , scope = key ) 
if len ( varlist ) > 1 : 
~~ elif len ( varlist ) == 0 : 
~~~ raise KeyError 
~~~ ops . append ( varlist [ 0 ] . assign ( params [ key ] ) ) 
~~ ~~ sess . run ( ops ) 
~~ def save_ckpt ( 
sess = None , mode_name = 'model.ckpt' , save_dir = 'checkpoint' , var_list = None , global_step = None , printable = False 
~~ if var_list is None : 
~~~ var_list = [ ] 
~~ ckpt_file = os . path . join ( save_dir , mode_name ) 
if var_list == [ ] : 
~~~ var_list = tf . global_variables ( ) 
if printable : 
~~~ for idx , v in enumerate ( var_list ) : 
~~ ~~ saver = tf . train . Saver ( var_list ) 
saver . save ( sess , ckpt_file , global_step = global_step ) 
~~ def load_ckpt ( sess = None , mode_name = 'model.ckpt' , save_dir = 'checkpoint' , var_list = None , is_latest = True , printable = False ) : 
~~ if is_latest : 
~~~ ckpt_file = tf . train . latest_checkpoint ( save_dir ) 
~~~ ckpt_file = os . path . join ( save_dir , mode_name ) 
~~ if not var_list : 
~~~ saver = tf . train . Saver ( var_list ) 
saver . restore ( sess , ckpt_file ) 
~~~ logging . info ( e ) 
~~ ~~ def load_npy_to_any ( path = '' , name = 'file.npy' ) : 
file_path = os . path . join ( path , name ) 
~~~ return np . load ( file_path ) . item ( ) 
~~~ return np . load ( file_path ) 
~~ def load_file_list ( path = None , regx = '\\.jpg' , printable = True , keep_prefix = False ) : 
~~~ path = os . getcwd ( ) 
~~ file_list = os . listdir ( path ) 
return_list = [ ] 
for _ , f in enumerate ( file_list ) : 
~~~ if re . search ( regx , f ) : 
~~~ return_list . append ( f ) 
~~ ~~ if keep_prefix : 
~~~ for i , f in enumerate ( return_list ) : 
~~~ return_list [ i ] = os . path . join ( path , f ) 
~~ ~~ if printable : 
~~ return return_list 
~~ def load_folder_list ( path = "" ) : 
return [ os . path . join ( path , o ) for o in os . listdir ( path ) if os . path . isdir ( os . path . join ( path , o ) ) ] 
~~ def exists_or_mkdir ( path , verbose = True ) : 
if not os . path . exists ( path ) : 
~~~ if verbose : 
~~ os . makedirs ( path ) 
~~ ~~ def maybe_download_and_extract ( filename , working_directory , url_source , extract = False , expected_bytes = None ) : 
def _download ( filename , working_directory , url_source ) : 
~~~ progress_bar = progressbar . ProgressBar ( ) 
def _dlProgress ( count , blockSize , totalSize , pbar = progress_bar ) : 
~~~ if ( totalSize != 0 ) : 
~~~ if not pbar . max_value : 
~~~ totalBlocks = math . ceil ( float ( totalSize ) / float ( blockSize ) ) 
pbar . max_value = int ( totalBlocks ) 
~~ pbar . update ( count , force = True ) 
~~ ~~ filepath = os . path . join ( working_directory , filename ) 
urlretrieve ( url_source + filename , filepath , reporthook = _dlProgress ) 
~~ exists_or_mkdir ( working_directory , verbose = False ) 
filepath = os . path . join ( working_directory , filename ) 
if not os . path . exists ( filepath ) : 
~~~ _download ( filename , working_directory , url_source ) 
statinfo = os . stat ( filepath ) 
if ( not ( expected_bytes is None ) and ( expected_bytes != statinfo . st_size ) ) : 
~~ if ( extract ) : 
~~~ if tarfile . is_tarfile ( filepath ) : 
tarfile . open ( filepath , 'r' ) . extractall ( working_directory ) 
~~ elif zipfile . is_zipfile ( filepath ) : 
with zipfile . ZipFile ( filepath ) as zf : 
~~~ zf . extractall ( working_directory ) 
~~ ~~ ~~ return filepath 
~~ def natural_keys ( text ) : 
def atoi ( text ) : 
~~~ return int ( text ) if text . isdigit ( ) else text 
~~ return [ atoi ( c ) for c in re . split ( '(\\d+)' , text ) ] 
~~ def npz_to_W_pdf ( path = None , regx = 'w1pre_[0-9]+\\.(npz)' ) : 
file_list = load_file_list ( path = path , regx = regx ) 
for f in file_list : 
~~~ W = load_npz ( path , f ) [ 0 ] 
visualize . draw_weights ( W , second = 10 , saveable = True , name = f . split ( '.' ) [ 0 ] , fig_idx = 2012 ) 
~~ ~~ def threading_data ( data = None , fn = None , thread_count = None , ** kwargs ) : 
def apply_fn ( results , i , data , kwargs ) : 
~~~ results [ i ] = fn ( data , ** kwargs ) 
~~ if thread_count is None : 
~~~ results = [ None ] * len ( data ) 
threads = [ ] 
for i , d in enumerate ( data ) : 
~~~ t = threading . Thread ( name = 'threading_and_return' , target = apply_fn , args = ( results , i , d , kwargs ) ) 
t . start ( ) 
threads . append ( t ) 
~~~ divs = np . linspace ( 0 , len ( data ) , thread_count + 1 ) 
divs = np . round ( divs ) . astype ( int ) 
results = [ None ] * thread_count 
for i in range ( thread_count ) : 
~~~ t = threading . Thread ( 
name = 'threading_and_return' , target = apply_fn , args = ( results , i , data [ divs [ i ] : divs [ i + 1 ] ] , kwargs ) 
~~ ~~ for t in threads : 
~~~ t . join ( ) 
~~~ return np . asarray ( results ) 
~~~ return results 
~~~ return np . concatenate ( results ) 
~~ ~~ def affine_rotation_matrix ( angle = ( - 20 , 20 ) ) : 
if isinstance ( angle , tuple ) : 
~~~ theta = np . pi / 180 * np . random . uniform ( angle [ 0 ] , angle [ 1 ] ) 
~~~ theta = np . pi / 180 * angle 
~~ rotation_matrix = np . array ( [ [ np . cos ( theta ) , np . sin ( theta ) , 0 ] , [ - np . sin ( theta ) , np . cos ( theta ) , 0 ] , [ 0 , 0 , 1 ] ] ) 
return rotation_matrix 
~~ def affine_horizontal_flip_matrix ( prob = 0.5 ) : 
factor = np . random . uniform ( 0 , 1 ) 
if prob >= factor : 
~~~ filp_matrix = np . array ( [ [ - 1. , 0. , 0. ] , [ 0. , 1. , 0. ] , [ 0. , 0. , 1. ] ] ) 
return filp_matrix 
~~~ filp_matrix = np . array ( [ [ 1. , 0. , 0. ] , [ 0. , 1. , 0. ] , [ 0. , 0. , 1. ] ] ) 
~~ ~~ def affine_vertical_flip_matrix ( prob = 0.5 ) : 
~~~ filp_matrix = np . array ( [ [ 1. , 0. , 0. ] , [ 0. , - 1. , 0. ] , [ 0. , 0. , 1. ] ] ) 
~~ ~~ def affine_shift_matrix ( wrg = ( - 0.1 , 0.1 ) , hrg = ( - 0.1 , 0.1 ) , w = 200 , h = 200 ) : 
if isinstance ( wrg , tuple ) : 
~~~ tx = np . random . uniform ( wrg [ 0 ] , wrg [ 1 ] ) * w 
~~~ tx = wrg * w 
~~ if isinstance ( hrg , tuple ) : 
~~~ ty = np . random . uniform ( hrg [ 0 ] , hrg [ 1 ] ) * h 
~~~ ty = hrg * h 
~~ shift_matrix = np . array ( [ [ 1 , 0 , tx ] , [ 0 , 1 , ty ] , [ 0 , 0 , 1 ] ] ) 
return shift_matrix 
~~ def affine_shear_matrix ( x_shear = ( - 0.1 , 0.1 ) , y_shear = ( - 0.1 , 0.1 ) ) : 
if isinstance ( x_shear , tuple ) : 
~~~ x_shear = np . random . uniform ( x_shear [ 0 ] , x_shear [ 1 ] ) 
~~ if isinstance ( y_shear , tuple ) : 
~~~ y_shear = np . random . uniform ( y_shear [ 0 ] , y_shear [ 1 ] ) 
~~ shear_matrix = np . array ( [ [ 1 , x_shear , 0 ] , [ y_shear , 1 , 0 ] , [ 0 , 0 , 1 ] ] ) 
return shear_matrix 
~~ def affine_zoom_matrix ( zoom_range = ( 0.8 , 1.1 ) ) : 
if isinstance ( zoom_range , ( float , int ) ) : 
~~~ scale = zoom_range 
~~ elif isinstance ( zoom_range , tuple ) : 
~~~ scale = np . random . uniform ( zoom_range [ 0 ] , zoom_range [ 1 ] ) 
~~ zoom_matrix = np . array ( [ [ scale , 0 , 0 ] , [ 0 , scale , 0 ] , [ 0 , 0 , 1 ] ] ) 
return zoom_matrix 
~~ def affine_respective_zoom_matrix ( w_range = 0.8 , h_range = 1.1 ) : 
if isinstance ( h_range , ( float , int ) ) : 
~~~ zy = h_range 
~~ elif isinstance ( h_range , tuple ) : 
~~~ zy = np . random . uniform ( h_range [ 0 ] , h_range [ 1 ] ) 
~~ if isinstance ( w_range , ( float , int ) ) : 
~~~ zx = w_range 
~~ elif isinstance ( w_range , tuple ) : 
~~~ zx = np . random . uniform ( w_range [ 0 ] , w_range [ 1 ] ) 
~~ zoom_matrix = np . array ( [ [ zx , 0 , 0 ] , [ 0 , zy , 0 ] , [ 0 , 0 , 1 ] ] ) 
~~ def transform_matrix_offset_center ( matrix , y , x ) : 
o_x = ( x - 1 ) / 2.0 
o_y = ( y - 1 ) / 2.0 
offset_matrix = np . array ( [ [ 1 , 0 , o_x ] , [ 0 , 1 , o_y ] , [ 0 , 0 , 1 ] ] ) 
reset_matrix = np . array ( [ [ 1 , 0 , - o_x ] , [ 0 , 1 , - o_y ] , [ 0 , 0 , 1 ] ] ) 
transform_matrix = np . dot ( np . dot ( offset_matrix , matrix ) , reset_matrix ) 
return transform_matrix 
~~ def affine_transform ( x , transform_matrix , channel_index = 2 , fill_mode = 'nearest' , cval = 0. , order = 1 ) : 
x = np . rollaxis ( x , channel_index , 0 ) 
final_affine_matrix = transform_matrix [ : 2 , : 2 ] 
final_offset = transform_matrix [ : 2 , 2 ] 
channel_images = [ 
ndi . interpolation . 
affine_transform ( x_channel , final_affine_matrix , final_offset , order = order , mode = fill_mode , cval = cval ) 
for x_channel in x 
x = np . stack ( channel_images , axis = 0 ) 
x = np . rollaxis ( x , 0 , channel_index + 1 ) 
~~ def affine_transform_cv2 ( x , transform_matrix , flags = None , border_mode = 'constant' ) : 
rows , cols = x . shape [ 0 ] , x . shape [ 1 ] 
if flags is None : 
~~~ flags = cv2 . INTER_AREA 
~~ if border_mode is 'constant' : 
~~~ border_mode = cv2 . BORDER_CONSTANT 
~~ elif border_mode is 'replicate' : 
~~~ border_mode = cv2 . BORDER_REPLICATE 
~~ return cv2 . warpAffine ( x , transform_matrix [ 0 : 2 , : ] , ( cols , rows ) , flags = flags , borderMode = border_mode ) 
~~ def affine_transform_keypoints ( coords_list , transform_matrix ) : 
coords_result_list = [ ] 
for coords in coords_list : 
~~~ coords = np . asarray ( coords ) 
coords = coords . transpose ( [ 1 , 0 ] ) 
coords = np . insert ( coords , 2 , 1 , axis = 0 ) 
coords_result = np . matmul ( transform_matrix , coords ) 
coords_result = coords_result [ 0 : 2 , : ] . transpose ( [ 1 , 0 ] ) 
coords_result_list . append ( coords_result ) 
~~ return coords_result_list 
~~ def projective_transform_by_points ( 
x , src , dst , map_args = None , output_shape = None , order = 1 , mode = 'constant' , cval = 0.0 , clip = True , 
preserve_range = False 
if map_args is None : 
~~~ map_args = { } 
~~~ src = np . array ( src ) 
~~ if isinstance ( dst , list ) : 
~~~ dst = np . array ( dst ) 
~~~ x = x / 255 
~~ m = transform . ProjectiveTransform ( ) 
m . estimate ( dst , src ) 
warped = transform . warp ( 
x , m , map_args = map_args , output_shape = output_shape , order = order , mode = mode , cval = cval , clip = clip , 
preserve_range = preserve_range 
return warped 
~~ def rotation ( 
x , rg = 20 , is_random = False , row_index = 0 , col_index = 1 , channel_index = 2 , fill_mode = 'nearest' , cval = 0. , order = 1 
if is_random : 
~~~ theta = np . pi / 180 * np . random . uniform ( - rg , rg ) 
~~~ theta = np . pi / 180 * rg 
~~ rotation_matrix = np . array ( [ [ np . cos ( theta ) , - np . sin ( theta ) , 0 ] , [ np . sin ( theta ) , np . cos ( theta ) , 0 ] , [ 0 , 0 , 1 ] ] ) 
h , w = x . shape [ row_index ] , x . shape [ col_index ] 
transform_matrix = transform_matrix_offset_center ( rotation_matrix , h , w ) 
x = affine_transform ( x , transform_matrix , channel_index , fill_mode , cval , order ) 
~~ def crop ( x , wrg , hrg , is_random = False , row_index = 0 , col_index = 1 ) : 
if ( h < hrg ) or ( w < wrg ) : 
~~ if is_random : 
~~~ h_offset = int ( np . random . uniform ( 0 , h - hrg ) ) 
w_offset = int ( np . random . uniform ( 0 , w - wrg ) ) 
return x [ h_offset : hrg + h_offset , w_offset : wrg + w_offset ] 
~~~ h_offset = int ( np . floor ( ( h - hrg ) / 2. ) ) 
w_offset = int ( np . floor ( ( w - wrg ) / 2. ) ) 
h_end = h_offset + hrg 
w_end = w_offset + wrg 
return x [ h_offset : h_end , w_offset : w_end ] 
~~ ~~ def crop_multi ( x , wrg , hrg , is_random = False , row_index = 0 , col_index = 1 ) : 
h , w = x [ 0 ] . shape [ row_index ] , x [ 0 ] . shape [ col_index ] 
for data in x : 
~~~ results . append ( data [ h_offset : hrg + h_offset , w_offset : wrg + w_offset ] ) 
~~ return np . asarray ( results ) 
~~~ h_offset = ( h - hrg ) / 2 
w_offset = ( w - wrg ) / 2 
~~~ results . append ( data [ h_offset : h - h_offset , w_offset : w - w_offset ] ) 
~~ ~~ def flip_axis ( x , axis = 1 , is_random = False ) : 
~~~ factor = np . random . uniform ( - 1 , 1 ) 
if factor > 0 : 
~~~ x = np . asarray ( x ) . swapaxes ( axis , 0 ) 
x = x [ : : - 1 , ... ] 
x = x . swapaxes ( 0 , axis ) 
~~~ return x 
~~ ~~ def flip_axis_multi ( x , axis , is_random = False ) : 
~~~ results = [ ] 
~~~ data = np . asarray ( data ) . swapaxes ( axis , 0 ) 
data = data [ : : - 1 , ... ] 
data = data . swapaxes ( 0 , axis ) 
results . append ( data ) 
~~~ return np . asarray ( x ) 
~~ ~~ def shift ( 
x , wrg = 0.1 , hrg = 0.1 , is_random = False , row_index = 0 , col_index = 1 , channel_index = 2 , fill_mode = 'nearest' , cval = 0. , 
order = 1 
~~~ tx = np . random . uniform ( - hrg , hrg ) * h 
ty = np . random . uniform ( - wrg , wrg ) * w 
~~~ tx , ty = hrg * h , wrg * w 
~~ translation_matrix = np . array ( [ [ 1 , 0 , tx ] , [ 0 , 1 , ty ] , [ 0 , 0 , 1 ] ] ) 
~~ def shift_multi ( 
~~~ results . append ( affine_transform ( data , transform_matrix , channel_index , fill_mode , cval , order ) ) 
~~ def shear ( 
x , intensity = 0.1 , is_random = False , row_index = 0 , col_index = 1 , channel_index = 2 , fill_mode = 'nearest' , cval = 0. , 
~~~ shear = np . random . uniform ( - intensity , intensity ) 
~~~ shear = intensity 
~~ shear_matrix = np . array ( [ [ 1 , - np . sin ( shear ) , 0 ] , [ 0 , np . cos ( shear ) , 0 ] , [ 0 , 0 , 1 ] ] ) 
transform_matrix = transform_matrix_offset_center ( shear_matrix , h , w ) 
~~ def shear2 ( 
x , shear = ( 0.1 , 0.1 ) , is_random = False , row_index = 0 , col_index = 1 , channel_index = 2 , fill_mode = 'nearest' , cval = 0. , 
if len ( shear ) != 2 : 
~~~ raise AssertionError ( 
~~ if isinstance ( shear , tuple ) : 
~~~ shear = list ( shear ) 
~~~ shear [ 0 ] = np . random . uniform ( - shear [ 0 ] , shear [ 0 ] ) 
shear [ 1 ] = np . random . uniform ( - shear [ 1 ] , shear [ 1 ] ) 
~~ shear_matrix = np . array ( [ [ 1 , shear [ 0 ] , 0 ] , [ shear [ 1 ] , 1 , 0 ] , [ 0 , 0 , 1 ] ] ) 
~~ def swirl ( 
x , center = None , strength = 1 , radius = 100 , rotation = 0 , output_shape = None , order = 1 , mode = 'constant' , cval = 0 , 
clip = True , preserve_range = False , is_random = False 
if radius == 0 : 
~~ rotation = np . pi / 180 * rotation 
~~~ center_h = int ( np . random . uniform ( 0 , x . shape [ 0 ] ) ) 
center_w = int ( np . random . uniform ( 0 , x . shape [ 1 ] ) ) 
center = ( center_h , center_w ) 
strength = np . random . uniform ( 0 , strength ) 
radius = np . random . uniform ( 1e-10 , radius ) 
rotation = np . random . uniform ( - rotation , rotation ) 
~~ max_v = np . max ( x ) 
~~~ x = x / max_v 
~~ swirled = skimage . transform . swirl ( 
x , center = center , strength = strength , radius = radius , rotation = rotation , output_shape = output_shape , order = order , 
mode = mode , cval = cval , clip = clip , preserve_range = preserve_range 
if max_v > 1 : 
~~~ swirled = swirled * max_v 
~~ return swirled 
~~ def elastic_transform ( x , alpha , sigma , mode = "constant" , cval = 0 , is_random = False ) : 
if is_random is False : 
~~~ random_state = np . random . RandomState ( None ) 
~~~ random_state = np . random . RandomState ( int ( time . time ( ) ) ) 
~~ is_3d = False 
if len ( x . shape ) == 3 and x . shape [ - 1 ] == 1 : 
~~~ x = x [ : , : , 0 ] 
is_3d = True 
~~ elif len ( x . shape ) == 3 and x . shape [ - 1 ] != 1 : 
~~ if len ( x . shape ) != 2 : 
~~ shape = x . shape 
dx = gaussian_filter ( ( random_state . rand ( * shape ) * 2 - 1 ) , sigma , mode = mode , cval = cval ) * alpha 
dy = gaussian_filter ( ( random_state . rand ( * shape ) * 2 - 1 ) , sigma , mode = mode , cval = cval ) * alpha 
x_ , y_ = np . meshgrid ( np . arange ( shape [ 0 ] ) , np . arange ( shape [ 1 ] ) , indexing = 'ij' ) 
indices = np . reshape ( x_ + dx , ( - 1 , 1 ) ) , np . reshape ( y_ + dy , ( - 1 , 1 ) ) 
if is_3d : 
~~~ return map_coordinates ( x , indices , order = 1 ) . reshape ( ( shape [ 0 ] , shape [ 1 ] , 1 ) ) 
~~~ return map_coordinates ( x , indices , order = 1 ) . reshape ( shape ) 
~~ ~~ def zoom ( x , zoom_range = ( 0.9 , 1.1 ) , flags = None , border_mode = 'constant' ) : 
zoom_matrix = affine_zoom_matrix ( zoom_range = zoom_range ) 
h , w = x . shape [ 0 ] , x . shape [ 1 ] 
transform_matrix = transform_matrix_offset_center ( zoom_matrix , h , w ) 
x = affine_transform_cv2 ( x , transform_matrix , flags = flags , border_mode = border_mode ) 
~~ def respective_zoom ( x , h_range = ( 0.9 , 1.1 ) , w_range = ( 0.9 , 1.1 ) , flags = None , border_mode = 'constant' ) : 
zoom_matrix = affine_respective_zoom_matrix ( h_range = h_range , w_range = w_range ) 
x = affine_transform_cv2 ( 
x , transform_matrix , flags = flags , border_mode = border_mode 
~~ def zoom_multi ( x , zoom_range = ( 0.9 , 1.1 ) , flags = None , border_mode = 'constant' ) : 
for img in x : 
~~~ h , w = x . shape [ 0 ] , x . shape [ 1 ] 
results . append ( affine_transform_cv2 ( x , transform_matrix , flags = flags , border_mode = border_mode ) ) 
~~ def brightness ( x , gamma = 1 , gain = 1 , is_random = False ) : 
~~~ gamma = np . random . uniform ( 1 - gamma , 1 + gamma ) 
~~ x = exposure . adjust_gamma ( x , gamma , gain ) 
~~ def brightness_multi ( x , gamma = 1 , gain = 1 , is_random = False ) : 
~~ results = [ ] 
~~~ results . append ( exposure . adjust_gamma ( data , gamma , gain ) ) 
~~ def illumination ( x , gamma = 1. , contrast = 1. , saturation = 1. , is_random = False ) : 
~~~ if not ( len ( gamma ) == len ( contrast ) == len ( saturation ) == 2 ) : 
~~~ gamma = 1 
~~ im_ = brightness ( x , gamma = gamma , gain = 1 , is_random = False ) 
contrast_adjust = PIL . ImageEnhance . Contrast ( image ) 
image = contrast_adjust . enhance ( np . random . uniform ( contrast [ 0 ] , contrast [ 1 ] ) ) #0.3,0.9)) 
saturation_adjust = PIL . ImageEnhance . Color ( image ) 
~~~ im_ = brightness ( x , gamma = gamma , gain = 1 , is_random = False ) 
image = contrast_adjust . enhance ( contrast ) 
image = saturation_adjust . enhance ( saturation ) 
~~ return np . asarray ( im_ ) 
~~ def rgb_to_hsv ( rgb ) : 
rgb = rgb . astype ( 'float' ) 
hsv = np . zeros_like ( rgb ) 
hsv [ ... , 3 : ] = rgb [ ... , 3 : ] 
r , g , b = rgb [ ... , 0 ] , rgb [ ... , 1 ] , rgb [ ... , 2 ] 
maxc = np . max ( rgb [ ... , : 3 ] , axis = - 1 ) 
minc = np . min ( rgb [ ... , : 3 ] , axis = - 1 ) 
hsv [ ... , 2 ] = maxc 
mask = maxc != minc 
hsv [ mask , 1 ] = ( maxc - minc ) [ mask ] / maxc [ mask ] 
rc = np . zeros_like ( r ) 
gc = np . zeros_like ( g ) 
bc = np . zeros_like ( b ) 
rc [ mask ] = ( maxc - r ) [ mask ] / ( maxc - minc ) [ mask ] 
gc [ mask ] = ( maxc - g ) [ mask ] / ( maxc - minc ) [ mask ] 
bc [ mask ] = ( maxc - b ) [ mask ] / ( maxc - minc ) [ mask ] 
hsv [ ... , 0 ] = np . select ( [ r == maxc , g == maxc ] , [ bc - gc , 2.0 + rc - bc ] , default = 4.0 + gc - rc ) 
hsv [ ... , 0 ] = ( hsv [ ... , 0 ] / 6.0 ) % 1.0 
return hsv 
~~ def hsv_to_rgb ( hsv ) : 
rgb = np . empty_like ( hsv ) 
rgb [ ... , 3 : ] = hsv [ ... , 3 : ] 
h , s , v = hsv [ ... , 0 ] , hsv [ ... , 1 ] , hsv [ ... , 2 ] 
i = ( h * 6.0 ) . astype ( 'uint8' ) 
f = ( h * 6.0 ) - i 
p = v * ( 1.0 - s ) 
q = v * ( 1.0 - s * f ) 
t = v * ( 1.0 - s * ( 1.0 - f ) ) 
i = i % 6 
conditions = [ s == 0.0 , i == 1 , i == 2 , i == 3 , i == 4 , i == 5 ] 
rgb [ ... , 0 ] = np . select ( conditions , [ v , q , p , p , t , v ] , default = v ) 
rgb [ ... , 1 ] = np . select ( conditions , [ v , v , v , q , p , p ] , default = t ) 
rgb [ ... , 2 ] = np . select ( conditions , [ v , p , t , v , v , q ] , default = p ) 
return rgb . astype ( 'uint8' ) 
~~ def adjust_hue ( im , hout = 0.66 , is_offset = True , is_clip = True , is_random = False ) : 
hsv = rgb_to_hsv ( im ) 
~~~ hout = np . random . uniform ( - hout , hout ) 
~~ if is_offset : 
~~~ hsv [ ... , 0 ] += hout 
~~~ hsv [ ... , 0 ] = hout 
~~ if is_clip : 
~~ rgb = hsv_to_rgb ( hsv ) 
return rgb 
~~ def imresize ( x , size = None , interp = 'bicubic' , mode = None ) : 
if size is None : 
~~~ size = [ 100 , 100 ] 
~~ if x . shape [ - 1 ] == 1 : 
~~~ x = scipy . misc . imresize ( x [ : , : , 0 ] , size , interp = interp , mode = mode ) 
return x [ : , : , np . newaxis ] 
~~~ return scipy . misc . imresize ( x , size , interp = interp , mode = mode ) 
~~ ~~ def pixel_value_scale ( im , val = 0.9 , clip = None , is_random = False ) : 
clip = clip if clip is not None else ( - np . inf , np . inf ) 
~~~ scale = 1 + np . random . uniform ( - val , val ) 
im = im * scale 
~~~ im = im * val 
~~ if len ( clip ) == 2 : 
~~~ im = np . clip ( im , clip [ 0 ] , clip [ 1 ] ) 
~~ return im 
~~ def samplewise_norm ( 
x , rescale = None , samplewise_center = False , samplewise_std_normalization = False , channel_index = 2 , epsilon = 1e-7 
if rescale : 
~~~ x *= rescale 
~~ if x . shape [ channel_index ] == 1 : 
~~~ if samplewise_center : 
~~~ x = x - np . mean ( x ) 
~~ if samplewise_std_normalization : 
~~~ x = x / np . std ( x ) 
~~ return x 
~~ elif x . shape [ channel_index ] == 3 : 
~~~ x = x - np . mean ( x , axis = channel_index , keepdims = True ) 
~~~ x = x / ( np . std ( x , axis = channel_index , keepdims = True ) + epsilon ) 
~~ ~~ def featurewise_norm ( x , mean = None , std = None , epsilon = 1e-7 ) : 
if mean : 
~~~ x = x - mean 
~~ if std : 
~~~ x = x / ( std + epsilon ) 
~~ def get_zca_whitening_principal_components_img ( X ) : 
flatX = np . reshape ( X , ( X . shape [ 0 ] , X . shape [ 1 ] * X . shape [ 2 ] * X . shape [ 3 ] ) ) 
sigma = np . dot ( flatX . T , flatX ) / flatX . shape [ 0 ] 
principal_components = np . dot ( np . dot ( U , np . diag ( 1. / np . sqrt ( S + 10e-7 ) ) ) , U . T ) 
return principal_components 
~~ def zca_whitening ( x , principal_components ) : 
flatx = np . reshape ( x , ( x . size ) ) 
whitex = np . dot ( flatx , principal_components ) 
x = np . reshape ( whitex , ( x . shape [ 0 ] , x . shape [ 1 ] , x . shape [ 2 ] ) ) 
~~ def channel_shift ( x , intensity , is_random = False , channel_index = 2 ) : 
~~~ factor = np . random . uniform ( - intensity , intensity ) 
~~~ factor = intensity 
~~ x = np . rollaxis ( x , channel_index , 0 ) 
min_x , max_x = np . min ( x ) , np . max ( x ) 
channel_images = [ np . clip ( x_channel + factor , min_x , max_x ) for x_channel in x ] 
~~ def channel_shift_multi ( x , intensity , is_random = False , channel_index = 2 ) : 
~~~ data = np . rollaxis ( data , channel_index , 0 ) 
min_x , max_x = np . min ( data ) , np . max ( data ) 
data = np . stack ( channel_images , axis = 0 ) 
data = np . rollaxis ( x , 0 , channel_index + 1 ) 
~~ def drop ( x , keep = 0.5 ) : 
if len ( x . shape ) == 3 : 
~~~ img_size = x . shape 
mask = np . random . binomial ( n = 1 , p = keep , size = x . shape [ : - 1 ] ) 
for i in range ( 3 ) : 
~~~ x [ : , : , i ] = np . multiply ( x [ : , : , i ] , mask ) 
x = np . multiply ( x , np . random . binomial ( n = 1 , p = keep , size = img_size ) ) 
~~ def array_to_img ( x , dim_ordering = ( 0 , 1 , 2 ) , scale = True ) : 
x = x . transpose ( dim_ordering ) 
~~~ x += max ( - np . min ( x ) , 0 ) 
x_max = np . max ( x ) 
if x_max != 0 : 
~~~ x = x / x_max 
~~ x *= 255 
~~ if x . shape [ 2 ] == 3 : 
~~~ return PIL . Image . fromarray ( x . astype ( 'uint8' ) , 'RGB' ) 
~~ elif x . shape [ 2 ] == 1 : 
~~~ return PIL . Image . fromarray ( x [ : , : , 0 ] . astype ( 'uint8' ) , 'L' ) 
~~ ~~ def find_contours ( x , level = 0.8 , fully_connected = 'low' , positive_orientation = 'low' ) : 
return skimage . measure . find_contours ( 
x , level , fully_connected = fully_connected , positive_orientation = positive_orientation 
~~ def pt2map ( list_points = None , size = ( 100 , 100 ) , val = 1 ) : 
if list_points is None : 
~~ i_m = np . zeros ( size ) 
if len ( list_points ) == 0 : 
~~~ return i_m 
~~ for xx in list_points : 
~~~ for x in xx : 
~~~ i_m [ int ( np . round ( x [ 0 ] ) ) ] [ int ( np . round ( x [ 1 ] ) ) ] = val 
~~ ~~ return i_m 
~~ def binary_dilation ( x , radius = 3 ) : 
mask = disk ( radius ) 
x = _binary_dilation ( x , selem = mask ) 
~~ def dilation ( x , radius = 3 ) : 
x = dilation ( x , selem = mask ) 
~~ def binary_erosion ( x , radius = 3 ) : 
x = _binary_erosion ( x , selem = mask ) 
~~ def erosion ( x , radius = 3 ) : 
x = _erosion ( x , selem = mask ) 
~~ def obj_box_coords_rescale ( coords = None , shape = None ) : 
if coords is None : 
~~~ coords = [ ] 
~~ if shape is None : 
~~~ shape = [ 100 , 200 ] 
~~ imh , imw = shape [ 0 ] , shape [ 1 ] 
imw = imw * 1.0 
coords_new = list ( ) 
for coord in coords : 
~~~ if len ( coord ) != 4 : 
~~ x = coord [ 0 ] / imw 
y = coord [ 1 ] / imh 
w = coord [ 2 ] / imw 
h = coord [ 3 ] / imh 
coords_new . append ( [ x , y , w , h ] ) 
~~ return coords_new 
~~ def obj_box_coord_rescale ( coord = None , shape = None ) : 
if coord is None : 
~~~ coord = [ ] 
~~ return obj_box_coords_rescale ( coords = [ coord ] , shape = shape ) [ 0 ] 
~~ def obj_box_coord_scale_to_pixelunit ( coord , shape = None ) : 
if shape is None : 
~~~ shape = [ 100 , 100 ] 
~~ imh , imw = shape [ 0 : 2 ] 
x = int ( coord [ 0 ] * imw ) 
x2 = int ( coord [ 2 ] * imw ) 
y = int ( coord [ 1 ] * imh ) 
y2 = int ( coord [ 3 ] * imh ) 
return [ x , y , x2 , y2 ] 
~~ def obj_box_coord_centroid_to_upleft_butright ( coord , to_int = False ) : 
if len ( coord ) != 4 : 
~~ x_center , y_center , w , h = coord 
x = x_center - w / 2. 
y = y_center - h / 2. 
x2 = x + w 
y2 = y + h 
if to_int : 
~~~ return [ int ( x ) , int ( y ) , int ( x2 ) , int ( y2 ) ] 
~~~ return [ x , y , x2 , y2 ] 
~~ ~~ def obj_box_coord_upleft_butright_to_centroid ( coord ) : 
~~ x1 , y1 , x2 , y2 = coord 
w = x2 - x1 
h = y2 - y1 
x_c = x1 + w / 2. 
y_c = y1 + h / 2. 
return [ x_c , y_c , w , h ] 
~~ def obj_box_coord_centroid_to_upleft ( coord ) : 
return [ x , y , w , h ] 
~~ def parse_darknet_ann_str_to_list ( annotations ) : 
annotations = annotations . split ( "\\n" ) 
ann = [ ] 
for a in annotations : 
~~~ a = a . split ( ) 
if len ( a ) == 5 : 
~~~ for i , _v in enumerate ( a ) : 
~~~ if i == 0 : 
~~~ a [ i ] = int ( a [ i ] ) 
~~~ a [ i ] = float ( a [ i ] ) 
~~ ~~ ann . append ( a ) 
~~ ~~ return ann 
~~ def parse_darknet_ann_list_to_cls_box ( annotations ) : 
class_list = [ ] 
bbox_list = [ ] 
for ann in annotations : 
~~~ class_list . append ( ann [ 0 ] ) 
bbox_list . append ( ann [ 1 : ] ) 
~~ return class_list , bbox_list 
~~ def obj_box_horizontal_flip ( im , coords = None , is_rescale = False , is_center = False , is_random = False ) : 
~~ def _flip ( im , coords ) : 
~~~ im = flip_axis ( im , axis = 1 , is_random = False ) 
~~ if is_rescale : 
~~~ if is_center : 
~~~ x = 1. - coord [ 0 ] 
~~~ x = 1. - coord [ 0 ] - coord [ 2 ] 
~~~ x = im . shape [ 1 ] - coord [ 0 ] 
~~~ x = im . shape [ 1 ] - coord [ 0 ] - coord [ 2 ] 
~~ ~~ coords_new . append ( [ x , coord [ 1 ] , coord [ 2 ] , coord [ 3 ] ] ) 
~~ return im , coords_new 
~~~ return _flip ( im , coords ) 
~~~ return im , coords 
~~ ~~ def obj_box_imresize ( im , coords = None , size = None , interp = 'bicubic' , mode = None , is_rescale = False ) : 
~~ if size is None : 
~~ imh , imw = im . shape [ 0 : 2 ] 
im = imresize ( im , size = size , interp = interp , mode = mode ) 
if is_rescale is False : 
~~~ coords_new = list ( ) 
~~ x = int ( coord [ 0 ] * ( size [ 1 ] / imw ) ) 
y = int ( coord [ 1 ] * ( size [ 0 ] / imh ) ) 
w = int ( coord [ 2 ] * ( size [ 1 ] / imw ) ) 
h = int ( coord [ 3 ] * ( size [ 0 ] / imh ) ) 
~~ ~~ def obj_box_crop ( 
im , classes = None , coords = None , wrg = 100 , hrg = 100 , is_rescale = False , is_center = False , is_random = False , 
thresh_wh = 0.02 , thresh_wh2 = 12. 
if classes is None : 
~~~ classes = [ ] 
~~ if coords is None : 
~~ h , w = im . shape [ 0 ] , im . shape [ 1 ] 
if ( h <= hrg ) or ( w <= wrg ) : 
~~~ h_offset = int ( np . random . uniform ( 0 , h - hrg ) - 1 ) 
w_offset = int ( np . random . uniform ( 0 , w - wrg ) - 1 ) 
h_end = hrg + h_offset 
w_end = wrg + w_offset 
im_new = im [ h_offset : h_end , w_offset : w_end ] 
~~ def _get_coord ( coord ) : 
if is_center : 
~~~ coord = obj_box_coord_centroid_to_upleft ( coord ) 
~~ x = coord [ 0 ] - w_offset 
y = coord [ 1 ] - h_offset 
w = coord [ 2 ] 
h = coord [ 3 ] 
if x < 0 : 
~~~ if x + w <= 0 : 
~~ w = w + x 
x = 0 
~~ if y < 0 : 
~~~ if y + h <= 0 : 
~~ h = h + y 
y = 0 
~~~ w = im_new . shape [ 1 ] - x 
~~~ h = im_new . shape [ 0 ] - y 
~~ if ( w / ( im_new . shape [ 1 ] * 1. ) < thresh_wh ) or ( h / ( im_new . shape [ 0 ] * 1. ) < 
~~ coord = [ x , y , w , h ] 
~~~ coord = obj_box_coord_upleft_to_centroid ( coord ) 
~~ return coord 
~~ coords_new = list ( ) 
classes_new = list ( ) 
for i , _ in enumerate ( coords ) : 
~~~ coord = coords [ i ] 
~~~ coord = obj_box_coord_scale_to_pixelunit ( coord , im . shape ) 
coord = _get_coord ( coord ) 
if coord is not None : 
~~~ coord = obj_box_coord_rescale ( coord , im_new . shape ) 
coords_new . append ( coord ) 
classes_new . append ( classes [ i ] ) 
~~~ coord = _get_coord ( coord ) 
~~~ coords_new . append ( coord ) 
~~ ~~ ~~ return im_new , classes_new , coords_new 
~~ def obj_box_shift ( 
im , classes = None , coords = None , wrg = 0.1 , hrg = 0.1 , row_index = 0 , col_index = 1 , channel_index = 2 , fill_mode = 'nearest' , 
cval = 0. , order = 1 , is_rescale = False , is_center = False , is_random = False , thresh_wh = 0.02 , thresh_wh2 = 12. 
~~ imh , imw = im . shape [ row_index ] , im . shape [ col_index ] 
if ( hrg >= 1.0 ) and ( hrg <= 0. ) and ( wrg >= 1.0 ) and ( wrg <= 0. ) : 
~~~ tx = np . random . uniform ( - hrg , hrg ) * imh 
ty = np . random . uniform ( - wrg , wrg ) * imw 
~~~ tx , ty = hrg * imh , wrg * imw 
im_new = affine_transform ( im , transform_matrix , channel_index , fill_mode , cval , order ) 
def _get_coord ( coord ) : 
~~ def obj_box_zoom ( 
im , classes = None , coords = None , zoom_range = ( 0.9 , 
1.1 ) , row_index = 0 , col_index = 1 , channel_index = 2 , fill_mode = 'nearest' , 
~~ if len ( zoom_range ) != 2 : 
~~~ if zoom_range [ 0 ] == 1 and zoom_range [ 1 ] == 1 : 
~~~ zx , zy = 1 , 1 
~~~ zx , zy = np . random . uniform ( zoom_range [ 0 ] , zoom_range [ 1 ] , 2 ) 
~~~ zx , zy = zoom_range 
h , w = im . shape [ row_index ] , im . shape [ col_index ] 
~~ def pad_sequences ( sequences , maxlen = None , dtype = 'int32' , padding = 'post' , truncating = 'pre' , value = 0. ) : 
lengths = [ len ( s ) for s in sequences ] 
nb_samples = len ( sequences ) 
if maxlen is None : 
~~~ maxlen = np . max ( lengths ) 
~~ sample_shape = tuple ( ) 
for s in sequences : 
~~~ if len ( s ) > 0 : 
~~~ sample_shape = np . asarray ( s ) . shape [ 1 : ] 
~~ ~~ x = ( np . ones ( ( nb_samples , maxlen ) + sample_shape ) * value ) . astype ( dtype ) 
for idx , s in enumerate ( sequences ) : 
~~~ if len ( s ) == 0 : 
~~ if truncating == 'pre' : 
~~~ trunc = s [ - maxlen : ] 
~~ elif truncating == 'post' : 
~~~ trunc = s [ : maxlen ] 
~~~ raise ValueError ( \ % truncating ) 
~~ trunc = np . asarray ( trunc , dtype = dtype ) 
if trunc . shape [ 1 : ] != sample_shape : 
( trunc . shape [ 1 : ] , idx , sample_shape ) 
~~ if padding == 'post' : 
~~~ x [ idx , : len ( trunc ) ] = trunc 
~~ elif padding == 'pre' : 
~~~ x [ idx , - len ( trunc ) : ] = trunc 
~~~ raise ValueError ( \ % padding ) 
~~ ~~ return x . tolist ( ) 
~~ def remove_pad_sequences ( sequences , pad_id = 0 ) : 
sequences_out = copy . deepcopy ( sequences ) 
for i , _ in enumerate ( sequences ) : 
~~~ for j in range ( 1 , len ( sequences [ i ] ) ) : 
~~~ if sequences [ i ] [ - j ] != pad_id : 
~~~ sequences_out [ i ] = sequences_out [ i ] [ 0 : - j + 1 ] 
~~ ~~ ~~ return sequences_out 
~~ def process_sequences ( sequences , end_id = 0 , pad_val = 0 , is_shorten = True , remain_end_id = False ) : 
max_length = 0 
for _ , seq in enumerate ( sequences ) : 
~~~ is_end = False 
for i_w , n in enumerate ( seq ) : 
~~~ is_end = True 
if max_length < i_w : 
~~~ max_length = i_w 
~~ if remain_end_id is False : 
~~ ~~ elif is_end == True : 
~~~ seq [ i_w ] = pad_val 
~~ ~~ ~~ if remain_end_id is True : 
~~~ max_length += 1 
~~ if is_shorten : 
~~~ for i , seq in enumerate ( sequences ) : 
~~~ sequences [ i ] = seq [ : max_length ] 
~~ ~~ return sequences 
~~ def sequences_add_start_id ( sequences , start_id = 0 , remove_last = False ) : 
~~~ if remove_last : 
~~~ sequences_out [ i ] = [ start_id ] + sequences [ i ] [ : - 1 ] 
~~~ sequences_out [ i ] = [ start_id ] + sequences [ i ] 
~~ ~~ return sequences_out 
~~ def sequences_add_end_id ( sequences , end_id = 888 ) : 
~~~ sequences_out [ i ] = sequences [ i ] + [ end_id ] 
~~ return sequences_out 
~~ def sequences_add_end_id_after_pad ( sequences , end_id = 888 , pad_id = 0 ) : 
for i , v in enumerate ( sequences ) : 
~~~ for j , _v2 in enumerate ( v ) : 
~~~ if sequences [ i ] [ j ] == pad_id : 
~~~ sequences_out [ i ] [ j ] = end_id 
~~ def sequences_get_mask ( sequences , pad_val = 0 ) : 
mask = np . ones_like ( sequences ) 
for i , seq in enumerate ( sequences ) : 
~~~ for i_w in reversed ( range ( len ( seq ) ) ) : 
~~~ if seq [ i_w ] == pad_val : 
~~~ mask [ i , i_w ] = 0 
~~ ~~ ~~ return mask 
~~ def keypoint_random_crop ( image , annos , mask = None , size = ( 368 , 368 ) ) : 
_target_height = size [ 0 ] 
_target_width = size [ 1 ] 
target_size = ( _target_width , _target_height ) 
if len ( np . shape ( image ) ) == 2 : 
~~~ image = cv2 . cvtColor ( image , cv2 . COLOR_GRAY2RGB ) 
~~ height , width , _ = np . shape ( image ) 
for _ in range ( 50 ) : 
~~~ x = random . randrange ( 0 , width - target_size [ 0 ] ) if width > target_size [ 0 ] else 0 
y = random . randrange ( 0 , height - target_size [ 1 ] ) if height > target_size [ 1 ] else 0 
for joint in annos : 
~~~ if x <= joint [ 0 ] [ 0 ] < x + target_size [ 0 ] and y <= joint [ 0 ] [ 1 ] < y + target_size [ 1 ] : 
~~~ target_size = ( w , h ) 
img = image 
resized = img [ y : y + target_size [ 1 ] , x : x + target_size [ 0 ] , : ] 
resized_mask = mask [ y : y + target_size [ 1 ] , x : x + target_size [ 0 ] ] 
adjust_joint_list = [ ] 
~~~ adjust_joint = [ ] 
for point in joint : 
~~~ if point [ 0 ] < - 10 or point [ 1 ] < - 10 : 
~~~ adjust_joint . append ( ( - 1000 , - 1000 ) ) 
~~ new_x , new_y = point [ 0 ] - x , point [ 1 ] - y 
if new_x > w - 1 or new_y > h - 1 : 
~~ adjust_joint . append ( ( new_x , new_y ) ) 
~~ adjust_joint_list . append ( adjust_joint ) 
~~ return resized , adjust_joint_list , resized_mask 
~~ return pose_crop ( image , annos , mask , x , y , target_size [ 0 ] , target_size [ 1 ] ) 
~~ def keypoint_resize_random_crop ( image , annos , mask = None , size = ( 368 , 368 ) ) : 
~~ def resize_image ( image , annos , mask , target_width , target_height ) : 
y , x , _ = np . shape ( image ) 
ratio_y = target_height / y 
ratio_x = target_width / x 
new_joints = [ ] 
for people in annos : 
~~~ new_keypoints = [ ] 
for keypoints in people : 
~~~ if keypoints [ 0 ] < 0 or keypoints [ 1 ] < 0 : 
~~~ new_keypoints . append ( ( - 1000 , - 1000 ) ) 
~~ pts = ( int ( keypoints [ 0 ] * ratio_x + 0.5 ) , int ( keypoints [ 1 ] * ratio_y + 0.5 ) ) 
if pts [ 0 ] > target_width - 1 or pts [ 1 ] > target_height - 1 : 
~~ new_keypoints . append ( pts ) 
~~ new_joints . append ( new_keypoints ) 
~~ annos = new_joints 
new_image = cv2 . resize ( image , ( target_width , target_height ) , interpolation = cv2 . INTER_AREA ) 
if mask is not None : 
~~~ new_mask = cv2 . resize ( mask , ( target_width , target_height ) , interpolation = cv2 . INTER_AREA ) 
return new_image , annos , new_mask 
~~~ return new_image , annos , None 
~~ ~~ _target_height = size [ 0 ] 
if height <= width : 
~~~ ratio = _target_height / height 
new_width = int ( ratio * width ) 
if height == width : 
~~~ new_width = _target_height 
~~ image , annos , mask = resize_image ( image , annos , mask , new_width , _target_height ) 
if new_width > _target_width : 
~~~ crop_range_x = np . random . randint ( 0 , new_width - _target_width ) 
~~~ crop_range_x = 0 
~~ image = image [ : , crop_range_x : crop_range_x + _target_width , : ] 
~~~ mask = mask [ : , crop_range_x : crop_range_x + _target_width ] 
~~ new_joints = [ ] 
~~~ if keypoints [ 0 ] < - 10 or keypoints [ 1 ] < - 10 : 
~~ top = crop_range_x + _target_width - 1 
if keypoints [ 0 ] >= crop_range_x and keypoints [ 0 ] <= top : 
~~~ pts = ( int ( keypoints [ 0 ] - crop_range_x ) , int ( keypoints [ 1 ] ) ) 
~~~ pts = ( - 1000 , - 1000 ) 
~~ if height > width : 
~~~ ratio = _target_width / width 
new_height = int ( ratio * height ) 
image , annos , mask = resize_image ( image , annos , mask , _target_width , new_height ) 
if new_height > _target_height : 
~~~ crop_range_y = np . random . randint ( 0 , new_height - _target_height ) 
~~~ crop_range_y = 0 
~~ image = image [ crop_range_y : crop_range_y + _target_width , : , : ] 
~~~ mask = mask [ crop_range_y : crop_range_y + _target_width , : ] 
~~ bot = crop_range_y + _target_height - 1 
if keypoints [ 1 ] >= crop_range_y and keypoints [ 1 ] <= bot : 
~~~ pts = ( int ( keypoints [ 0 ] ) , int ( keypoints [ 1 ] - crop_range_y ) ) 
~~ if mask is not None : 
~~~ return image , annos , mask 
~~~ return image , annos , None 
~~ ~~ def keypoint_random_rotate ( image , annos , mask = None , rg = 15. ) : 
def _rotate_coord ( shape , newxy , point , angle ) : 
~~~ angle = - 1 * angle / 180.0 * math . pi 
ox , oy = shape 
px , py = point 
ox /= 2 
oy /= 2 
qx = math . cos ( angle ) * ( px - ox ) - math . sin ( angle ) * ( py - oy ) 
qy = math . sin ( angle ) * ( px - ox ) + math . cos ( angle ) * ( py - oy ) 
new_x , new_y = newxy 
qx += ox - new_x 
qy += oy - new_y 
return int ( qx + 0.5 ) , int ( qy + 0.5 ) 
~~ def _largest_rotated_rect ( w , h , angle ) : 
angle = angle / 180.0 * math . pi 
if w <= 0 or h <= 0 : 
~~~ return 0 , 0 
~~ width_is_longer = w >= h 
side_long , side_short = ( w , h ) if width_is_longer else ( h , w ) 
sin_a , cos_a = abs ( math . sin ( angle ) ) , abs ( math . cos ( angle ) ) 
if side_short <= 2. * sin_a * cos_a * side_long : 
~~~ x = 0.5 * side_short 
wr , hr = ( x / sin_a , x / cos_a ) if width_is_longer else ( x / cos_a , x / sin_a ) 
~~~ cos_2a = cos_a * cos_a - sin_a * sin_a 
wr , hr = ( w * cos_a - h * sin_a ) / cos_2a , ( h * cos_a - w * sin_a ) / cos_2a 
~~ return int ( np . round ( wr ) ) , int ( np . round ( hr ) ) 
~~ img_shape = np . shape ( image ) 
height = img_shape [ 0 ] 
width = img_shape [ 1 ] 
deg = np . random . uniform ( - rg , rg ) 
rot_m = cv2 . getRotationMatrix2D ( ( int ( center [ 0 ] ) , int ( center [ 1 ] ) ) , deg , 1 ) 
ret = cv2 . warpAffine ( img , rot_m , img . shape [ 1 : : - 1 ] , flags = cv2 . INTER_AREA , borderMode = cv2 . BORDER_CONSTANT ) 
if img . ndim == 3 and ret . ndim == 2 : 
~~~ ret = ret [ : , : , np . newaxis ] 
~~ neww , newh = _largest_rotated_rect ( ret . shape [ 1 ] , ret . shape [ 0 ] , deg ) 
neww = min ( neww , ret . shape [ 1 ] ) 
newh = min ( newh , ret . shape [ 0 ] ) 
newx = int ( center [ 0 ] - neww * 0.5 ) 
newy = int ( center [ 1 ] - newh * 0.5 ) 
img = ret [ newy : newy + newh , newx : newx + neww ] 
~~~ if point [ 0 ] < - 100 or point [ 1 ] < - 100 : 
~~ x , y = _rotate_coord ( ( width , height ) , ( newx , newy ) , point , deg ) 
if x > neww - 1 or y > newh - 1 : 
~~ if x < 0 or y < 0 : 
~~ adjust_joint . append ( ( x , y ) ) 
~~ joint_list = adjust_joint_list 
~~~ msk = mask 
ret = cv2 . warpAffine ( msk , rot_m , msk . shape [ 1 : : - 1 ] , flags = cv2 . INTER_AREA , borderMode = cv2 . BORDER_CONSTANT ) 
if msk . ndim == 3 and msk . ndim == 2 : 
msk = ret [ newy : newy + newh , newx : newx + neww ] 
return img , joint_list , msk 
~~~ return img , joint_list , None 
~~ ~~ def keypoint_random_flip ( 
image , annos , mask = None , prob = 0.5 , flip_list = ( 0 , 1 , 5 , 6 , 7 , 2 , 3 , 4 , 11 , 12 , 13 , 8 , 9 , 10 , 15 , 14 , 17 , 16 , 18 ) 
_prob = np . random . uniform ( 0 , 1.0 ) 
if _prob < prob : 
~~ _ , width , _ = np . shape ( image ) 
image = cv2 . flip ( image , 1 ) 
mask = cv2 . flip ( mask , 1 ) 
for k in flip_list : 
~~~ point = people [ k ] 
if point [ 0 ] < 0 or point [ 1 ] < 0 : 
~~ if point [ 0 ] > image . shape [ 1 ] - 1 or point [ 1 ] > image . shape [ 0 ] - 1 : 
~~ if ( width - point [ 0 ] ) > image . shape [ 1 ] - 1 : 
~~ new_keypoints . append ( ( width - point [ 0 ] , point [ 1 ] ) ) 
return image , annos , mask 
~~ def keypoint_random_resize ( image , annos , mask = None , zoom_range = ( 0.8 , 1.2 ) ) : 
height = image . shape [ 0 ] 
width = image . shape [ 1 ] 
_min , _max = zoom_range 
scalew = np . random . uniform ( _min , _max ) 
scaleh = np . random . uniform ( _min , _max ) 
neww = int ( width * scalew ) 
newh = int ( height * scaleh ) 
dst = cv2 . resize ( image , ( neww , newh ) , interpolation = cv2 . INTER_AREA ) 
~~~ mask = cv2 . resize ( mask , ( neww , newh ) , interpolation = cv2 . INTER_AREA ) 
~~ adjust_joint_list = [ ] 
~~ adjust_joint . append ( ( int ( point [ 0 ] * scalew + 0.5 ) , int ( point [ 1 ] * scaleh + 0.5 ) ) ) 
~~~ return dst , adjust_joint_list , mask 
~~~ return dst , adjust_joint_list , None 
~~ ~~ def Vgg19 ( rgb ) : 
rgb_scaled = rgb * 255.0 
red , green , blue = tf . split ( rgb_scaled , 3 , 3 ) 
if red . get_shape ( ) . as_list ( ) [ 1 : ] != [ 224 , 224 , 1 ] : 
~~ if green . get_shape ( ) . as_list ( ) [ 1 : ] != [ 224 , 224 , 1 ] : 
~~ if blue . get_shape ( ) . as_list ( ) [ 1 : ] != [ 224 , 224 , 1 ] : 
~~ bgr = tf . concat ( [ 
blue - VGG_MEAN [ 0 ] , 
green - VGG_MEAN [ 1 ] , 
red - VGG_MEAN [ 2 ] , 
] , axis = 3 ) 
if bgr . get_shape ( ) . as_list ( ) [ 1 : ] != [ 224 , 224 , 3 ] : 
~~ net_in = InputLayer ( bgr , name = 'input' ) 
net = Conv2dLayer ( net_in , act = tf . nn . relu , shape = [ 3 , 3 , 3 , 64 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv1_1' ) 
net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 64 , 64 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv1_2' ) 
net = PoolLayer ( net , ksize = [ 1 , 2 , 2 , 1 ] , strides = [ 1 , 2 , 2 , 1 ] , padding = 'SAME' , pool = tf . nn . max_pool , name = 'pool1' ) 
net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 64 , 128 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv2_1' ) 
net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 128 , 128 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv2_2' ) 
net = PoolLayer ( net , ksize = [ 1 , 2 , 2 , 1 ] , strides = [ 1 , 2 , 2 , 1 ] , padding = 'SAME' , pool = tf . nn . max_pool , name = 'pool2' ) 
net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 128 , 256 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv3_1' ) 
net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 256 , 256 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv3_2' ) 
net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 256 , 256 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv3_3' ) 
net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 256 , 256 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv3_4' ) 
net = PoolLayer ( net , ksize = [ 1 , 2 , 2 , 1 ] , strides = [ 1 , 2 , 2 , 1 ] , padding = 'SAME' , pool = tf . nn . max_pool , name = 'pool3' ) 
net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 256 , 512 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv4_1' ) 
net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 512 , 512 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv4_2' ) 
net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 512 , 512 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv4_3' ) 
net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 512 , 512 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv4_4' ) 
net = PoolLayer ( net , ksize = [ 1 , 2 , 2 , 1 ] , strides = [ 1 , 2 , 2 , 1 ] , padding = 'SAME' , pool = tf . nn . max_pool , name = 'pool4' ) 
net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 512 , 512 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv5_1' ) 
net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 512 , 512 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv5_2' ) 
net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 512 , 512 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv5_3' ) 
net = Conv2dLayer ( net , act = tf . nn . relu , shape = [ 3 , 3 , 512 , 512 ] , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' , name = 'conv5_4' ) 
net = PoolLayer ( net , ksize = [ 1 , 2 , 2 , 1 ] , strides = [ 1 , 2 , 2 , 1 ] , padding = 'SAME' , pool = tf . nn . max_pool , name = 'pool5' ) 
net = FlattenLayer ( net , name = 'flatten' ) 
net = DenseLayer ( net , n_units = 4096 , act = tf . nn . relu , name = 'fc6' ) 
net = DenseLayer ( net , n_units = 4096 , act = tf . nn . relu , name = 'fc7' ) 
net = DenseLayer ( net , n_units = 1000 , act = None , name = 'fc8' ) 
return net 
~~ def Vgg19_simple_api ( rgb ) : 
net = Conv2d ( net_in , 64 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv1_1' ) 
net = Conv2d ( net , n_filter = 64 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv1_2' ) 
net = MaxPool2d ( net , filter_size = ( 2 , 2 ) , strides = ( 2 , 2 ) , padding = 'SAME' , name = 'pool1' ) 
net = Conv2d ( net , n_filter = 128 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv2_1' ) 
net = Conv2d ( net , n_filter = 128 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv2_2' ) 
net = MaxPool2d ( net , filter_size = ( 2 , 2 ) , strides = ( 2 , 2 ) , padding = 'SAME' , name = 'pool2' ) 
net = Conv2d ( net , n_filter = 256 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv3_1' ) 
net = Conv2d ( net , n_filter = 256 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv3_2' ) 
net = Conv2d ( net , n_filter = 256 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv3_3' ) 
net = Conv2d ( net , n_filter = 256 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv3_4' ) 
net = MaxPool2d ( net , filter_size = ( 2 , 2 ) , strides = ( 2 , 2 ) , padding = 'SAME' , name = 'pool3' ) 
net = Conv2d ( net , n_filter = 512 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv4_1' ) 
net = Conv2d ( net , n_filter = 512 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv4_2' ) 
net = Conv2d ( net , n_filter = 512 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv4_3' ) 
net = Conv2d ( net , n_filter = 512 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv4_4' ) 
net = MaxPool2d ( net , filter_size = ( 2 , 2 ) , strides = ( 2 , 2 ) , padding = 'SAME' , name = 'pool4' ) 
net = Conv2d ( net , n_filter = 512 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv5_1' ) 
net = Conv2d ( net , n_filter = 512 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv5_2' ) 
net = Conv2d ( net , n_filter = 512 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv5_3' ) 
net = Conv2d ( net , n_filter = 512 , filter_size = ( 3 , 3 ) , strides = ( 1 , 1 ) , act = tf . nn . relu , padding = 'SAME' , name = 'conv5_4' ) 
net = MaxPool2d ( net , filter_size = ( 2 , 2 ) , strides = ( 2 , 2 ) , padding = 'SAME' , name = 'pool5' ) 
~~ def prepro ( I ) : 
I = I [ 35 : 195 ] 
I = I [ : : 2 , : : 2 , 0 ] 
I [ I == 144 ] = 0 
I [ I == 109 ] = 0 
I [ I != 0 ] = 1 
return I . astype ( np . float ) . ravel ( ) 
~~ def discount_episode_rewards ( rewards = None , gamma = 0.99 , mode = 0 ) : 
if rewards is None : 
~~ discounted_r = np . zeros_like ( rewards , dtype = np . float32 ) 
running_add = 0 
for t in reversed ( xrange ( 0 , rewards . size ) ) : 
~~~ if mode == 0 : 
~~~ if rewards [ t ] != 0 : running_add = 0 
~~ running_add = running_add * gamma + rewards [ t ] 
discounted_r [ t ] = running_add 
~~ return discounted_r 
~~ def cross_entropy_reward_loss ( logits , actions , rewards , name = None ) : 
cross_entropy = tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = actions , logits = logits , name = name ) 
return tf . reduce_sum ( tf . multiply ( cross_entropy , rewards ) ) 
~~ def log_weight ( probs , weights , name = 'log_weight' ) : 
~~~ exp_v = tf . reduce_mean ( tf . log ( probs ) * weights ) 
return exp_v 
~~ ~~ def choice_action_by_probs ( probs = ( 0.5 , 0.5 ) , action_list = None ) : 
if action_list is None : 
~~~ n_action = len ( probs ) 
action_list = np . arange ( n_action ) 
~~~ if len ( action_list ) != len ( probs ) : 
~~ ~~ return np . random . choice ( action_list , p = probs ) 
~~ def cross_entropy ( output , target , name = None ) : 
if name is None : 
~~ return tf . reduce_mean ( tf . nn . sparse_softmax_cross_entropy_with_logits ( labels = target , logits = output ) , name = name ) 
~~ def sigmoid_cross_entropy ( output , target , name = None ) : 
return tf . reduce_mean ( tf . nn . sigmoid_cross_entropy_with_logits ( labels = target , logits = output ) , name = name ) 
~~ def binary_cross_entropy ( output , target , epsilon = 1e-8 , name = 'bce_loss' ) : 
return tf . reduce_mean ( 
tf . reduce_sum ( - ( target * tf . log ( output + epsilon ) + ( 1. - target ) * tf . log ( 1. - output + epsilon ) ) , axis = 1 ) , 
name = name 
~~ def mean_squared_error ( output , target , is_mean = False , name = "mean_squared_error" ) : 
~~~ if is_mean : 
~~~ mse = tf . reduce_mean ( tf . reduce_mean ( tf . squared_difference ( output , target ) , 1 ) , name = name ) 
~~~ mse = tf . reduce_mean ( tf . reduce_sum ( tf . squared_difference ( output , target ) , 1 ) , name = name ) 
~~~ mse = tf . reduce_mean ( tf . reduce_mean ( tf . squared_difference ( output , target ) , [ 1 , 2 ] ) , name = name ) 
~~~ mse = tf . reduce_mean ( tf . reduce_sum ( tf . squared_difference ( output , target ) , [ 1 , 2 ] ) , name = name ) 
~~~ mse = tf . reduce_mean ( tf . reduce_mean ( tf . squared_difference ( output , target ) , [ 1 , 2 , 3 ] ) , name = name ) 
~~~ mse = tf . reduce_mean ( tf . reduce_sum ( tf . squared_difference ( output , target ) , [ 1 , 2 , 3 ] ) , name = name ) 
~~ return mse 
~~ def normalized_mean_square_error ( output , target , name = "normalized_mean_squared_error_loss" ) : 
~~~ nmse_a = tf . sqrt ( tf . reduce_sum ( tf . squared_difference ( output , target ) , axis = 1 ) ) 
nmse_b = tf . sqrt ( tf . reduce_sum ( tf . square ( target ) , axis = 1 ) ) 
~~~ nmse_a = tf . sqrt ( tf . reduce_sum ( tf . squared_difference ( output , target ) , axis = [ 1 , 2 ] ) ) 
nmse_b = tf . sqrt ( tf . reduce_sum ( tf . square ( target ) , axis = [ 1 , 2 ] ) ) 
~~~ nmse_a = tf . sqrt ( tf . reduce_sum ( tf . squared_difference ( output , target ) , axis = [ 1 , 2 , 3 ] ) ) 
nmse_b = tf . sqrt ( tf . reduce_sum ( tf . square ( target ) , axis = [ 1 , 2 , 3 ] ) ) 
~~ nmse = tf . reduce_mean ( nmse_a / nmse_b , name = name ) 
return nmse 
~~ def absolute_difference_error ( output , target , is_mean = False , name = "absolute_difference_error_loss" ) : 
~~~ loss = tf . reduce_mean ( tf . reduce_mean ( tf . abs ( output - target ) , 1 ) , name = name ) 
~~~ loss = tf . reduce_mean ( tf . reduce_sum ( tf . abs ( output - target ) , 1 ) , name = name ) 
~~~ loss = tf . reduce_mean ( tf . reduce_mean ( tf . abs ( output - target ) , [ 1 , 2 ] ) , name = name ) 
~~~ loss = tf . reduce_mean ( tf . reduce_sum ( tf . abs ( output - target ) , [ 1 , 2 ] ) , name = name ) 
~~~ loss = tf . reduce_mean ( tf . reduce_mean ( tf . abs ( output - target ) , [ 1 , 2 , 3 ] ) , name = name ) 
~~~ loss = tf . reduce_mean ( tf . reduce_sum ( tf . abs ( output - target ) , [ 1 , 2 , 3 ] ) , name = name ) 
~~ return loss 
~~ def dice_coe ( output , target , loss_type = 'jaccard' , axis = ( 1 , 2 , 3 ) , smooth = 1e-5 ) : 
inse = tf . reduce_sum ( output * target , axis = axis ) 
if loss_type == 'jaccard' : 
~~~ l = tf . reduce_sum ( output * output , axis = axis ) 
r = tf . reduce_sum ( target * target , axis = axis ) 
~~ elif loss_type == 'sorensen' : 
~~~ l = tf . reduce_sum ( output , axis = axis ) 
r = tf . reduce_sum ( target , axis = axis ) 
~~ dice = ( 2. * inse + smooth ) / ( l + r + smooth ) 
## 
dice = tf . reduce_mean ( dice , name = 'dice_coe' ) 
return dice 
~~ def dice_hard_coe ( output , target , threshold = 0.5 , axis = ( 1 , 2 , 3 ) , smooth = 1e-5 ) : 
output = tf . cast ( output > threshold , dtype = tf . float32 ) 
target = tf . cast ( target > threshold , dtype = tf . float32 ) 
inse = tf . reduce_sum ( tf . multiply ( output , target ) , axis = axis ) 
l = tf . reduce_sum ( output , axis = axis ) 
hard_dice = ( 2. * inse + smooth ) / ( l + r + smooth ) 
hard_dice = tf . reduce_mean ( hard_dice , name = 'hard_dice' ) 
return hard_dice 
~~ def iou_coe ( output , target , threshold = 0.5 , axis = ( 1 , 2 , 3 ) , smooth = 1e-5 ) : 
pre = tf . cast ( output > threshold , dtype = tf . float32 ) 
truth = tf . cast ( target > threshold , dtype = tf . float32 ) 
batch_iou = ( inse + smooth ) / ( union + smooth ) 
iou = tf . reduce_mean ( batch_iou , name = 'iou_coe' ) 
return iou 
sequence_loss_by_example_fn = tf . contrib . legacy_seq2seq . sequence_loss_by_example 
loss = sequence_loss_by_example_fn ( 
[ logits ] , [ tf . reshape ( target_seqs , [ - 1 ] ) ] , [ tf . ones_like ( tf . reshape ( target_seqs , [ - 1 ] ) , dtype = tf . float32 ) ] 
if batch_size is not None : 
~~~ cost = cost / batch_size 
~~ return cost 
~~ def cross_entropy_seq_with_mask ( logits , target_seqs , input_mask , return_details = False , name = None ) : 
losses = tf . nn . sparse_softmax_cross_entropy_with_logits ( logits = logits , labels = targets , name = name ) * weights 
loss = tf . divide ( 
tf . reduce_sum ( weights ) , 
name = "seq_loss_with_mask" 
if return_details : 
~~~ return loss , losses , weights , targets 
~~~ return loss 
~~ ~~ def cosine_similarity ( v1 , v2 ) : 
return tf . reduce_sum ( tf . multiply ( v1 , v2 ) , 1 ) / ( tf . sqrt ( tf . reduce_sum ( tf . multiply ( v1 , v1 ) , 1 ) ) * 
tf . sqrt ( tf . reduce_sum ( tf . multiply ( v2 , v2 ) , 1 ) ) ) 
~~ def li_regularizer ( scale , scope = None ) : 
if isinstance ( scale , numbers . Integral ) : 
~~ if isinstance ( scale , numbers . Real ) : 
~~~ if scale < 0. : 
~~ if scale >= 1. : 
~~ if scale == 0. : 
return lambda _ , name = None : None 
~~ ~~ def li ( weights ) : 
with tf . name_scope ( 'li_regularizer' ) as scope : 
~~~ my_scale = ops . convert_to_tensor ( scale , dtype = weights . dtype . base_dtype , name = 'scale' ) 
standard_ops_fn = standard_ops . multiply 
return standard_ops_fn ( 
my_scale , standard_ops . reduce_sum ( standard_ops . sqrt ( standard_ops . reduce_sum ( tf . square ( weights ) , 1 ) ) ) , 
name = scope 
~~ ~~ return li 
~~ def maxnorm_regularizer ( scale = 1.0 ) : 
~~ ~~ def mn ( weights , name = 'max_regularizer' ) : 
with tf . name_scope ( name ) as scope : 
return standard_ops_fn ( my_scale , standard_ops . reduce_max ( standard_ops . abs ( weights ) ) , name = scope ) 
~~ ~~ return mn 
~~ def maxnorm_o_regularizer ( scale ) : 
~~ ~~ def mn_o ( weights , name = 'maxnorm_o_regularizer' ) : 
if tf . __version__ <= '0.12' : 
~~~ standard_ops_fn = standard_ops . mul 
~~~ standard_ops_fn = standard_ops . multiply 
~~ return standard_ops_fn ( 
my_scale , standard_ops . reduce_sum ( standard_ops . reduce_max ( standard_ops . abs ( weights ) , 0 ) ) , name = scope 
~~ ~~ return mn_o 
~~ def ramp ( x , v_min = 0 , v_max = 1 , name = None ) : 
return tf . clip_by_value ( x , clip_value_min = v_min , clip_value_max = v_max , name = name ) 
~~ def leaky_relu ( x , alpha = 0.2 , name = "leaky_relu" ) : 
if not ( 0 < alpha <= 1 ) : 
~~ with tf . name_scope ( name , "leaky_relu" ) as name_scope : 
~~~ x = tf . convert_to_tensor ( x , name = "features" ) 
return tf . maximum ( x , alpha * x , name = name_scope ) 
~~ ~~ def leaky_relu6 ( x , alpha = 0.2 , name = "leaky_relu6" ) : 
if not isinstance ( alpha , tf . Tensor ) and not ( 0 < alpha <= 1 ) : 
~~ with tf . name_scope ( name , "leaky_relu6" ) as name_scope : 
return tf . minimum ( tf . maximum ( x , alpha * x ) , 6 , name = name_scope ) 
~~ ~~ def leaky_twice_relu6 ( x , alpha_low = 0.2 , alpha_high = 0.2 , name = "leaky_relu6" ) : 
if not isinstance ( alpha_high , tf . Tensor ) and not ( 0 < alpha_high <= 1 ) : 
~~ if not isinstance ( alpha_low , tf . Tensor ) and not ( 0 < alpha_low <= 1 ) : 
~~ with tf . name_scope ( name , "leaky_twice_relu6" ) as name_scope : 
x_is_above_0 = tf . minimum ( x , 6 * ( 1 - alpha_high ) + alpha_high * x ) 
x_is_below_0 = tf . minimum ( alpha_low * x , 0 ) 
return tf . maximum ( x_is_above_0 , x_is_below_0 , name = name_scope ) 
~~ ~~ def swish ( x , name = 'swish' ) : 
with tf . name_scope ( name ) : 
~~~ x = tf . nn . sigmoid ( x ) * x 
~~ def pixel_wise_softmax ( x , name = 'pixel_wise_softmax' ) : 
~~~ return tf . nn . softmax ( x ) 
~~ ~~ def _conv_linear ( args , filter_size , num_features , bias , bias_start = 0.0 , scope = None ) : 
total_arg_size_depth = 0 
shapes = [ a . get_shape ( ) . as_list ( ) for a in args ] 
~~~ if len ( shape ) != 4 : 
~~ if not shape [ 3 ] : 
~~~ total_arg_size_depth += shape [ 3 ] 
~~ ~~ dtype = [ a . dtype for a in args ] [ 0 ] 
with tf . variable_scope ( scope or "Conv" ) : 
~~~ matrix = tf . get_variable ( 
"Matrix" , [ filter_size [ 0 ] , filter_size [ 1 ] , total_arg_size_depth , num_features ] , dtype = dtype 
if len ( args ) == 1 : 
~~~ res = tf . nn . conv2d ( args [ 0 ] , matrix , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' ) 
~~~ res = tf . nn . conv2d ( tf . concat ( args , 3 ) , matrix , strides = [ 1 , 1 , 1 , 1 ] , padding = 'SAME' ) 
~~ if not bias : 
~~~ return res 
~~ bias_term = tf . get_variable ( 
"Bias" , [ num_features ] , dtype = dtype , initializer = tf . constant_initializer ( bias_start , dtype = dtype ) 
~~ return res + bias_term 
~~ def advanced_indexing_op ( inputs , index ) : 
batch_size = tf . shape ( inputs ) [ 0 ] 
dim_size = int ( inputs . get_shape ( ) [ 2 ] ) 
index = tf . range ( 0 , batch_size ) * max_length + ( index - 1 ) 
flat = tf . reshape ( inputs , [ - 1 , dim_size ] ) 
relevant = tf . gather ( flat , index ) 
return relevant 
~~ def retrieve_seq_length_op ( data ) : 
with tf . name_scope ( 'GetLength' ) : 
~~~ used = tf . sign ( tf . reduce_max ( tf . abs ( data ) , 2 ) ) 
length = tf . reduce_sum ( used , 1 ) 
return tf . cast ( length , tf . int32 ) 
~~ ~~ def retrieve_seq_length_op2 ( data ) : 
return tf . reduce_sum ( tf . cast ( tf . greater ( data , tf . zeros_like ( data ) ) , tf . int32 ) , 1 ) 
data_shape_size = data . get_shape ( ) . ndims 
if data_shape_size == 3 : 
~~~ return tf . reduce_sum ( tf . cast ( tf . reduce_any ( tf . not_equal ( data , pad_val ) , axis = 2 ) , dtype = tf . int32 ) , 1 ) 
~~ elif data_shape_size == 2 : 
~~~ return tf . reduce_sum ( tf . cast ( tf . not_equal ( data , pad_val ) , dtype = tf . int32 ) , 1 ) 
~~ elif data_shape_size == 1 : 
~~ ~~ def zero_state ( self , batch_size , dtype = LayersConfig . tf_dtype ) : 
shape = self . shape 
num_features = self . num_features 
zeros = tf . zeros ( [ batch_size , shape [ 0 ] , shape [ 1 ] , num_features * 2 ] , dtype = dtype ) 
return zeros 
~~ def state_size ( self ) : 
return ( LSTMStateTuple ( self . _num_units , self . _num_units ) if self . _state_is_tuple else 2 * self . _num_units ) 
~~ def _to_bc_h_w ( self , x , x_shape ) : 
x = tf . transpose ( x , [ 0 , 3 , 1 , 2 ] ) 
x = tf . reshape ( x , ( - 1 , x_shape [ 1 ] , x_shape [ 2 ] ) ) 
~~ def _to_b_h_w_n_c ( self , x , x_shape ) : 
x = tf . reshape ( x , ( - 1 , x_shape [ 4 ] , x_shape [ 1 ] , x_shape [ 2 ] , x_shape [ 3 ] ) ) 
x = tf . transpose ( x , [ 0 , 2 , 3 , 4 , 1 ] ) 
~~ def _tf_repeat ( self , a , repeats ) : 
if len ( a . get_shape ( ) ) != 1 : 
~~ a = tf . expand_dims ( a , - 1 ) 
a = tf . tile ( a , [ 1 , repeats ] ) 
a = self . tf_flatten ( a ) 
return a 
~~ def _tf_batch_map_coordinates ( self , inputs , coords ) : 
input_shape = inputs . get_shape ( ) 
coords_shape = coords . get_shape ( ) 
batch_channel = tf . shape ( inputs ) [ 0 ] 
input_h = int ( input_shape [ 1 ] ) 
input_w = int ( input_shape [ 2 ] ) 
kernel_n = int ( coords_shape [ 3 ] ) 
n_coords = input_h * input_w * kernel_n 
coords_lt = tf . cast ( tf . floor ( coords ) , 'int32' ) 
coords_rb = tf . cast ( tf . ceil ( coords ) , 'int32' ) 
coords_lb = tf . stack ( [ coords_lt [ : , : , : , : , 0 ] , coords_rb [ : , : , : , : , 1 ] ] , axis = - 1 ) 
coords_rt = tf . stack ( [ coords_rb [ : , : , : , : , 0 ] , coords_lt [ : , : , : , : , 1 ] ] , axis = - 1 ) 
idx = self . _tf_repeat ( tf . range ( batch_channel ) , n_coords ) 
vals_lt = self . _get_vals_by_coords ( inputs , coords_lt , idx , ( batch_channel , input_h , input_w , kernel_n ) ) 
vals_rb = self . _get_vals_by_coords ( inputs , coords_rb , idx , ( batch_channel , input_h , input_w , kernel_n ) ) 
vals_lb = self . _get_vals_by_coords ( inputs , coords_lb , idx , ( batch_channel , input_h , input_w , kernel_n ) ) 
vals_rt = self . _get_vals_by_coords ( inputs , coords_rt , idx , ( batch_channel , input_h , input_w , kernel_n ) ) 
coords_offset_lt = coords - tf . cast ( coords_lt , 'float32' ) 
vals_t = vals_lt + ( vals_rt - vals_lt ) * coords_offset_lt [ : , : , : , : , 0 ] 
vals_b = vals_lb + ( vals_rb - vals_lb ) * coords_offset_lt [ : , : , : , : , 0 ] 
mapped_vals = vals_t + ( vals_b - vals_t ) * coords_offset_lt [ : , : , : , : , 1 ] 
return mapped_vals 
~~ def _tf_batch_map_offsets ( self , inputs , offsets , grid_offset ) : 
kernel_n = int ( int ( offsets . get_shape ( ) [ 3 ] ) / 2 ) 
input_h = input_shape [ 1 ] 
input_w = input_shape [ 2 ] 
channel = input_shape [ 3 ] 
inputs = self . _to_bc_h_w ( inputs , input_shape ) 
offsets = tf . reshape ( offsets , ( batch_size , input_h , input_w , kernel_n , 2 ) ) 
coords = tf . stack ( 
[ 
tf . clip_by_value ( coords [ : , : , : , : , 0 ] , 0.0 , tf . cast ( input_h - 1 , 'float32' ) ) , 
tf . clip_by_value ( coords [ : , : , : , : , 1 ] , 0.0 , tf . cast ( input_w - 1 , 'float32' ) ) 
] , axis = - 1 
coords = tf . tile ( coords , [ channel , 1 , 1 , 1 , 1 ] ) 
mapped_vals = self . _tf_batch_map_coordinates ( inputs , coords ) 
mapped_vals = self . _to_b_h_w_n_c ( mapped_vals , [ batch_size , input_h , input_w , kernel_n , channel ] ) 
~~ def minibatches ( inputs = None , targets = None , batch_size = None , allow_dynamic_batch_size = False , shuffle = False ) : 
if len ( inputs ) != len ( targets ) : 
~~ if shuffle : 
~~~ indices = np . arange ( len ( inputs ) ) 
np . random . shuffle ( indices ) 
~~ for start_idx in range ( 0 , len ( inputs ) , batch_size ) : 
~~~ end_idx = start_idx + batch_size 
if end_idx > len ( inputs ) : 
~~~ if allow_dynamic_batch_size : 
~~~ end_idx = len ( inputs ) 
~~ ~~ if shuffle : 
~~~ excerpt = indices [ start_idx : end_idx ] 
~~~ excerpt = slice ( start_idx , end_idx ) 
~~ if ( isinstance ( inputs , list ) or isinstance ( targets , list ) ) and ( shuffle == True ) : 
~~~ yield [ inputs [ i ] for i in excerpt ] , [ targets [ i ] for i in excerpt ] 
~~~ yield inputs [ excerpt ] , targets [ excerpt ] 
~~ ~~ ~~ def seq_minibatches ( inputs , targets , batch_size , seq_length , stride = 1 ) : 
~~ n_loads = ( batch_size * stride ) + ( seq_length - stride ) 
for start_idx in range ( 0 , len ( inputs ) - n_loads + 1 , ( batch_size * stride ) ) : 
~~~ seq_inputs = np . zeros ( ( batch_size , seq_length ) + inputs . shape [ 1 : ] , dtype = inputs . dtype ) 
seq_targets = np . zeros ( ( batch_size , seq_length ) + targets . shape [ 1 : ] , dtype = targets . dtype ) 
for b_idx in xrange ( batch_size ) : 
~~~ start_seq_idx = start_idx + ( b_idx * stride ) 
end_seq_idx = start_seq_idx + seq_length 
seq_inputs [ b_idx ] = inputs [ start_seq_idx : end_seq_idx ] 
seq_targets [ b_idx ] = targets [ start_seq_idx : end_seq_idx ] 
~~ flatten_inputs = seq_inputs . reshape ( ( - 1 , ) + inputs . shape [ 1 : ] ) 
flatten_targets = seq_targets . reshape ( ( - 1 , ) + targets . shape [ 1 : ] ) 
yield flatten_inputs , flatten_targets 
~~ ~~ def seq_minibatches2 ( inputs , targets , batch_size , num_steps ) : 
~~ data_len = len ( inputs ) 
batch_len = data_len // batch_size 
data = np . zeros ( ( batch_size , batch_len ) + inputs . shape [ 1 : ] , dtype = inputs . dtype ) 
data2 = np . zeros ( [ batch_size , batch_len ] ) 
for i in range ( batch_size ) : 
~~~ data [ i ] = inputs [ batch_len * i : batch_len * ( i + 1 ) ] 
data2 [ i ] = targets [ batch_len * i : batch_len * ( i + 1 ) ] 
~~ epoch_size = ( batch_len - 1 ) // num_steps 
if epoch_size == 0 : 
~~ for i in range ( epoch_size ) : 
~~~ x = data [ : , i * num_steps : ( i + 1 ) * num_steps ] 
x2 = data2 [ : , i * num_steps : ( i + 1 ) * num_steps ] 
yield ( x , x2 ) 
~~ ~~ def ptb_iterator ( raw_data , batch_size , num_steps ) : 
raw_data = np . array ( raw_data , dtype = np . int32 ) 
data_len = len ( raw_data ) 
data = np . zeros ( [ batch_size , batch_len ] , dtype = np . int32 ) 
~~~ data [ i ] = raw_data [ batch_len * i : batch_len * ( i + 1 ) ] 
y = data [ : , i * num_steps + 1 : ( i + 1 ) * num_steps + 1 ] 
yield ( x , y ) 
~~ ~~ def deconv2d_bilinear_upsampling_initializer ( shape ) : 
if shape [ 0 ] != shape [ 1 ] : 
~~ if shape [ 3 ] < shape [ 2 ] : 
~~ filter_size = shape [ 0 ] 
num_out_channels = shape [ 2 ] 
num_in_channels = shape [ 3 ] 
bilinear_kernel = np . zeros ( [ filter_size , filter_size ] , dtype = np . float32 ) 
scale_factor = ( filter_size + 1 ) // 2 
if filter_size % 2 == 1 : 
~~~ center = scale_factor - 1 
~~~ center = scale_factor - 0.5 
~~ for x in range ( filter_size ) : 
~~~ for y in range ( filter_size ) : 
~~~ bilinear_kernel [ x , y ] = ( 1 - abs ( x - center ) / scale_factor ) * ( 1 - abs ( y - center ) / scale_factor ) 
~~ ~~ weights = np . zeros ( ( filter_size , filter_size , num_out_channels , num_in_channels ) ) 
for i in range ( num_out_channels ) : 
~~~ weights [ : , : , i , i ] = bilinear_kernel 
~~ return tf . constant_initializer ( value = weights , dtype = LayersConfig . tf_dtype ) 
~~ def save_model ( self , network = None , model_name = 'model' , ** kwargs ) : 
kwargs . update ( { 'model_name' : model_name } ) 
params = network . get_all_params ( ) 
s = time . time ( ) 
kwargs . update ( { 'architecture' : network . all_graphs , 'time' : datetime . utcnow ( ) } ) 
~~~ params_id = self . model_fs . put ( self . _serialization ( params ) ) 
kwargs . update ( { 'params_id' : params_id , 'time' : datetime . utcnow ( ) } ) 
self . db . Model . insert_one ( kwargs ) 
~~~ exc_type , exc_obj , exc_tb = sys . exc_info ( ) 
fname = os . path . split ( exc_tb . tb_frame . f_code . co_filename ) [ 1 ] 
~~ ~~ def find_top_model ( self , sess , sort = None , model_name = 'model' , ** kwargs ) : 
self . _fill_project_info ( kwargs ) 
d = self . db . Model . find_one ( filter = kwargs , sort = sort ) 
_temp_file_name = '_find_one_model_ztemp_file' 
if d is not None : 
~~~ params_id = d [ 'params_id' ] 
graphs = d [ 'architecture' ] 
_datetime = d [ 'time' ] 
exists_or_mkdir ( _temp_file_name , False ) 
with open ( os . path . join ( _temp_file_name , 'graph.pkl' ) , 'wb' ) as file : 
~~~ pickle . dump ( graphs , file , protocol = pickle . HIGHEST_PROTOCOL ) 
~~~ params = self . _deserialization ( self . model_fs . get ( params_id ) . read ( ) ) 
np . savez ( os . path . join ( _temp_file_name , 'params.npz' ) , params = params ) 
network = load_graph_and_params ( name = _temp_file_name , sess = sess ) 
del_folder ( _temp_file_name ) 
pc = self . db . Model . find ( kwargs ) 
format ( kwargs , sort , _datetime , round ( time . time ( ) - s , 2 ) ) 
for key in d : 
~~~ network . __dict__ . update ( { "_%s" % key : d [ key ] } ) 
~~ params_id_list = pc . distinct ( 'params_id' ) 
n_params = len ( params_id_list ) 
if n_params != 1 : 
~~ return network 
~~ ~~ def delete_model ( self , ** kwargs ) : 
self . db . Model . delete_many ( kwargs ) 
~~ def save_dataset ( self , dataset = None , dataset_name = None , ** kwargs ) : 
if dataset_name is None : 
~~ kwargs . update ( { 'dataset_name' : dataset_name } ) 
~~~ dataset_id = self . dataset_fs . put ( self . _serialization ( dataset ) ) 
kwargs . update ( { 'dataset_id' : dataset_id , 'time' : datetime . utcnow ( ) } ) 
self . db . Dataset . insert_one ( kwargs ) 
~~ ~~ def find_top_dataset ( self , dataset_name = None , sort = None , ** kwargs ) : 
d = self . db . Dataset . find_one ( filter = kwargs , sort = sort ) 
~~~ dataset_id = d [ 'dataset_id' ] 
~~~ dataset = self . _deserialization ( self . dataset_fs . get ( dataset_id ) . read ( ) ) 
pc = self . db . Dataset . find ( kwargs ) 
dataset_id_list = pc . distinct ( 'dataset_id' ) 
n_dataset = len ( dataset_id_list ) 
if n_dataset != 1 : 
~~ return dataset 
~~ ~~ def find_datasets ( self , dataset_name = None , ** kwargs ) : 
if pc is not None : 
~~~ dataset_id_list = pc . distinct ( 'dataset_id' ) 
dataset_list = [ ] 
~~~ tmp = self . dataset_fs . get ( dataset_id ) . read ( ) 
dataset_list . append ( self . _deserialization ( tmp ) ) 
return dataset_list 
~~ def delete_datasets ( self , ** kwargs ) : 
self . db . Dataset . delete_many ( kwargs ) 
~~ def save_training_log ( self , ** kwargs ) : 
kwargs . update ( { 'time' : datetime . utcnow ( ) } ) 
_result = self . db . TrainLog . insert_one ( kwargs ) 
_log = self . _print_dict ( kwargs ) 
~~ def save_validation_log ( self , ** kwargs ) : 
_result = self . db . ValidLog . insert_one ( kwargs ) 
~~ def delete_training_log ( self , ** kwargs ) : 
self . db . TrainLog . delete_many ( kwargs ) 
~~ def delete_validation_log ( self , ** kwargs ) : 
self . db . ValidLog . delete_many ( kwargs ) 
~~ def create_task ( self , task_name = None , script = None , hyper_parameters = None , saved_result_keys = None , ** kwargs ) : 
~~ if hyper_parameters is None : 
~~~ hyper_parameters = { } 
~~ if saved_result_keys is None : 
~~~ saved_result_keys = [ ] 
~~ self . _fill_project_info ( kwargs ) 
kwargs . update ( { 'hyper_parameters' : hyper_parameters } ) 
kwargs . update ( { 'saved_result_keys' : saved_result_keys } ) 
_script = open ( script , 'rb' ) . read ( ) 
kwargs . update ( { 'status' : 'pending' , 'script' : _script , 'result' : { } } ) 
self . db . Task . insert_one ( kwargs ) 
~~ def run_top_task ( self , task_name = None , sort = None , ** kwargs ) : 
kwargs . update ( { 'status' : 'pending' } ) 
task = self . db . Task . find_one_and_update ( kwargs , { '$set' : { 'status' : 'running' } } , sort = sort ) 
~~~ if task is None : 
~~ _datetime = task [ 'time' ] 
_script = task [ 'script' ] 
_id = task [ '_id' ] 
_hyper_parameters = task [ 'hyper_parameters' ] 
_saved_result_keys = task [ 'saved_result_keys' ] 
for key in _hyper_parameters : 
~~~ globals ( ) [ key ] = _hyper_parameters [ key ] 
~~ s = time . time ( ) 
_script = _script . decode ( 'utf-8' ) 
~~~ exec ( _script , globals ( ) ) 
~~ _ = self . db . Task . find_one_and_update ( { '_id' : _id } , { '$set' : { 'status' : 'finished' } } ) 
__result = { } 
for _key in _saved_result_keys : 
__result . update ( { "%s" % _key : globals ( ) [ _key ] } ) 
~~ _ = self . db . Task . find_one_and_update ( 
'_id' : _id 
} , { '$set' : { 
'result' : __result 
} } , return_document = pymongo . ReturnDocument . AFTER 
format ( task_name , sort , _datetime , 
time . time ( ) - s ) 
_ = self . db . Task . find_one_and_update ( { '_id' : _id } , { '$set' : { 'status' : 'pending' } } ) 
~~ ~~ def delete_tasks ( self , ** kwargs ) : 
self . db . Task . delete_many ( kwargs ) 
~~ def check_unfinished_task ( self , task_name = None , ** kwargs ) : 
kwargs . update ( { '$or' : [ { 'status' : 'pending' } , { 'status' : 'running' } ] } ) 
task = self . db . Task . find ( kwargs ) 
task_id_list = task . distinct ( '_id' ) 
n_task = len ( task_id_list ) 
if n_task == 0 : 
~~ ~~ def augment_with_ngrams ( unigrams , unigram_vocab_size , n_buckets , n = 2 ) : 
def get_ngrams ( n ) : 
~~~ return list ( zip ( * [ unigrams [ i : ] for i in range ( n ) ] ) ) 
~~ def hash_ngram ( ngram ) : 
~~~ bytes_ = array . array ( 'L' , ngram ) . tobytes ( ) 
hash_ = int ( hashlib . sha256 ( bytes_ ) . hexdigest ( ) , 16 ) 
return unigram_vocab_size + hash_ % n_buckets 
~~ return unigrams + [ hash_ngram ( ngram ) for i in range ( 2 , n + 1 ) for ngram in get_ngrams ( i ) ] 
~~ def load_and_preprocess_imdb_data ( n_gram = None ) : 
X_train , y_train , X_test , y_test = tl . files . load_imdb_dataset ( nb_words = VOCAB_SIZE ) 
if n_gram is not None : 
~~~ X_train = np . array ( [ augment_with_ngrams ( x , VOCAB_SIZE , N_BUCKETS , n = n_gram ) for x in X_train ] ) 
X_test = np . array ( [ augment_with_ngrams ( x , VOCAB_SIZE , N_BUCKETS , n = n_gram ) for x in X_test ] ) 
~~ return X_train , y_train , X_test , y_test 
~~ def read_image ( image , path = '' ) : 
return imageio . imread ( os . path . join ( path , image ) ) 
~~ def read_images ( img_list , path = '' , n_threads = 10 , printable = True ) : 
imgs = [ ] 
for idx in range ( 0 , len ( img_list ) , n_threads ) : 
~~~ b_imgs_list = img_list [ idx : idx + n_threads ] 
b_imgs = tl . prepro . threading_data ( b_imgs_list , fn = read_image , path = path ) 
imgs . extend ( b_imgs ) 
~~ ~~ return imgs 
~~ def save_image ( image , image_path = '_temp.png' ) : 
~~~ imageio . imwrite ( image_path , image ) 
~~~ imageio . imwrite ( image_path , image [ : , : , 0 ] ) 
~~ ~~ def save_images ( images , size , image_path = '_temp.png' ) : 
~~~ images = images [ : , : , : , np . newaxis ] 
~~ def merge ( images , size ) : 
~~~ h , w = images . shape [ 1 ] , images . shape [ 2 ] 
img = np . zeros ( ( h * size [ 0 ] , w * size [ 1 ] , 3 ) , dtype = images . dtype ) 
for idx , image in enumerate ( images ) : 
~~~ i = idx % size [ 1 ] 
j = idx // size [ 1 ] 
img [ j * h : j * h + h , i * w : i * w + w , : ] = image 
~~ return img 
~~ def imsave ( images , size , path ) : 
~~~ if np . max ( images ) <= 1 and ( - 1 <= np . min ( images ) < 0 ) : 
~~~ images = ( ( images + 1 ) * 127.5 ) . astype ( np . uint8 ) 
~~ elif np . max ( images ) <= 1 and np . min ( images ) >= 0 : 
~~~ images = ( images * 255 ) . astype ( np . uint8 ) 
~~ return imageio . imwrite ( path , merge ( images , size ) ) 
~~ if len ( images ) > size [ 0 ] * size [ 1 ] : 
~~ return imsave ( images , size , image_path ) 
~~ def draw_boxes_and_labels_to_image ( 
image , classes , coords , scores , classes_list , is_center = True , is_rescale = True , save_name = None 
if len ( coords ) != len ( classes ) : 
~~ if len ( scores ) > 0 and len ( scores ) != len ( classes ) : 
~~ image = image . copy ( ) 
imh , imw = image . shape [ 0 : 2 ] 
thick = int ( ( imh + imw ) // 430 ) 
for i , _v in enumerate ( coords ) : 
~~~ x , y , x2 , y2 = tl . prepro . obj_box_coord_centroid_to_upleft_butright ( coords [ i ] ) 
~~~ x , y , x2 , y2 = coords [ i ] 
~~~ x , y , x2 , y2 = tl . prepro . obj_box_coord_scale_to_pixelunit ( [ x , y , x2 , y2 ] , ( imh , imw ) ) 
~~ cv2 . rectangle ( 
image , 
( int ( x ) , int ( y ) ) , 
[ 0 , 255 , 0 ] , 
thick 
cv2 . putText ( 
0 , 
int ( thick / 2 ) + 1 
~~ if save_name is not None : 
~~~ save_image ( image , save_name ) 
~~ return image 
~~ def draw_mpii_pose_to_image ( image , poses , save_name = 'image.png' ) : 
image = image . copy ( ) 
radius = int ( thick * 1.5 ) 
if image . max ( ) < 1 : 
~~~ image = image * 255 
~~ for people in poses : 
~~~ joint_pos = people [ 'joint_pos' ] 
lines = [ 
[ ( 0 , 1 ) , [ 100 , 255 , 100 ] ] , 
[ ( 1 , 2 ) , [ 50 , 255 , 50 ] ] , 
[ ( 3 , 4 ) , [ 100 , 100 , 255 ] ] , 
[ ( 4 , 5 ) , [ 50 , 50 , 255 ] ] , 
[ ( 6 , 7 ) , [ 255 , 255 , 100 ] ] , 
[ ( 10 , 11 ) , [ 255 , 100 , 255 ] ] , 
[ ( 11 , 12 ) , [ 255 , 50 , 255 ] ] , 
[ ( 8 , 13 ) , [ 0 , 255 , 255 ] ] , 
[ ( 13 , 14 ) , [ 100 , 255 , 255 ] ] , 
for line in lines : 
~~~ start , end = line [ 0 ] 
if ( start in joint_pos ) and ( end in joint_pos ) : 
~~~ cv2 . line ( 
( int ( joint_pos [ start ] [ 0 ] ) , int ( joint_pos [ start ] [ 1 ] ) ) , 
line [ 1 ] , 
~~ ~~ for pos in joint_pos . items ( ) : 
pos_loc = ( int ( pos_loc [ 0 ] ) , int ( pos_loc [ 1 ] ) ) 
cv2 . circle ( image , center = pos_loc , radius = radius , color = ( 200 , 200 , 200 ) , thickness = - 1 ) 
~~ head_rect = people [ 'head_rect' ] 
~~~ cv2 . rectangle ( 
( int ( head_rect [ 0 ] ) , int ( head_rect [ 1 ] ) ) , 
[ 0 , 180 , 0 ] , 
~~ ~~ if save_name is not None : 
~~ def frame ( I = None , second = 5 , saveable = True , name = 'frame' , cmap = None , fig_idx = 12836 ) : 
import matplotlib . pyplot as plt 
if saveable is False : 
~~~ plt . ion ( ) 
~~~ I = I [ : , : , 0 ] 
~~ plt . imshow ( I , cmap ) 
plt . title ( name ) 
if saveable : 
~~~ plt . savefig ( name + '.pdf' , format = 'pdf' ) 
~~~ plt . draw ( ) 
plt . pause ( second ) 
~~ ~~ def CNN2d ( CNN = None , second = 10 , saveable = True , name = 'cnn' , fig_idx = 3119362 ) : 
n_mask = CNN . shape [ 3 ] 
n_row = CNN . shape [ 0 ] 
n_col = CNN . shape [ 1 ] 
n_color = CNN . shape [ 2 ] 
row = int ( np . sqrt ( n_mask ) ) 
col = int ( np . ceil ( n_mask / row ) ) 
fig = plt . figure ( fig_idx ) 
for _ir in range ( 1 , row + 1 ) : 
~~~ for _ic in range ( 1 , col + 1 ) : 
~~~ if count > n_mask : 
~~ fig . add_subplot ( col , row , count ) 
if n_color == 1 : 
~~~ plt . imshow ( np . reshape ( CNN [ : , : , : , count - 1 ] , ( n_row , n_col ) ) , cmap = 'gray' , interpolation = "nearest" ) 
~~ elif n_color == 3 : 
~~~ plt . imshow ( 
np . reshape ( CNN [ : , : , : , count - 1 ] , ( n_row , n_col , n_color ) ) , cmap = 'gray' , interpolation = "nearest" 
~~ ~~ if saveable : 
~~ ~~ def tsne_embedding ( embeddings , reverse_dictionary , plot_only = 500 , second = 5 , saveable = False , name = 'tsne' , fig_idx = 9862 ) : 
def plot_with_labels ( low_dim_embs , labels , figsize = ( 18 , 18 ) , second = 5 , saveable = True , name = 'tsne' , fig_idx = 9862 ) : 
~~~ if low_dim_embs . shape [ 0 ] < len ( labels ) : 
~~ if saveable is False : 
plt . figure ( fig_idx ) 
for i , label in enumerate ( labels ) : 
~~~ x , y = low_dim_embs [ i , : ] 
plt . scatter ( x , y ) 
~~ if saveable : 
~~~ from sklearn . manifold import TSNE 
from six . moves import xrange 
tsne = TSNE ( perplexity = 30 , n_components = 2 , init = 'pca' , n_iter = 5000 ) 
low_dim_embs = tsne . fit_transform ( embeddings [ : plot_only , : ] ) 
labels = [ reverse_dictionary [ i ] for i in xrange ( plot_only ) ] 
plot_with_labels ( low_dim_embs , labels , second = second , saveable = saveable , name = name , fig_idx = fig_idx ) 
tl . logging . error ( _err ) 
raise ImportError ( _err ) 
~~ ~~ def draw_weights ( W = None , second = 10 , saveable = True , shape = None , name = 'mnist' , fig_idx = 2396512 ) : 
~~~ shape = [ 28 , 28 ] 
~~ import matplotlib . pyplot as plt 
n_units = W . shape [ 1 ] 
num_c = int ( np . ceil ( n_units / num_r ) ) 
count = int ( 1 ) 
for _row in range ( 1 , num_r + 1 ) : 
~~~ for _col in range ( 1 , num_c + 1 ) : 
~~~ if count > n_units : 
~~ fig . add_subplot ( num_r , num_c , count ) 
feature = W [ : , count - 1 ] / np . sqrt ( ( W [ : , count - 1 ] ** 2 ) . sum ( ) ) 
plt . imshow ( 
np . reshape ( feature , ( shape [ 0 ] , shape [ 1 ] ) ) , cmap = 'gray' , interpolation = "nearest" 
~~ ~~ def data_to_tfrecord ( images , labels , filename ) : 
if os . path . isfile ( filename ) : 
writer = tf . python_io . TFRecordWriter ( filename ) 
for index , img in enumerate ( images ) : 
~~~ img_raw = img . tobytes ( ) 
label = int ( labels [ index ] ) 
example = tf . train . Example ( 
features = tf . train . Features ( 
feature = { 
"label" : tf . train . Feature ( int64_list = tf . train . Int64List ( value = [ label ] ) ) , 
'img_raw' : tf . train . Feature ( bytes_list = tf . train . BytesList ( value = [ img_raw ] ) ) , 
~~ writer . close ( ) 
~~ def read_and_decode ( filename , is_train = None ) : 
filename_queue = tf . train . string_input_producer ( [ filename ] ) 
reader = tf . TFRecordReader ( ) 
_ , serialized_example = reader . read ( filename_queue ) 
features = tf . parse_single_example ( 
serialized_example , features = { 
'label' : tf . FixedLenFeature ( [ ] , tf . int64 ) , 
'img_raw' : tf . FixedLenFeature ( [ ] , tf . string ) , 
img = tf . decode_raw ( features [ 'img_raw' ] , tf . float32 ) 
img = tf . reshape ( img , [ 32 , 32 , 3 ] ) 
if is_train == True : 
~~~ img = tf . random_crop ( img , [ 24 , 24 , 3 ] ) 
img = tf . image . random_flip_left_right ( img ) 
img = tf . image . random_brightness ( img , max_delta = 63 ) 
img = tf . image . random_contrast ( img , lower = 0.2 , upper = 1.8 ) 
img = tf . image . per_image_standardization ( img ) 
~~ elif is_train == False : 
~~~ img = tf . image . resize_image_with_crop_or_pad ( img , 24 , 24 ) 
~~ elif is_train == None : 
~~~ img = img 
~~ label = tf . cast ( features [ 'label' ] , tf . int32 ) 
return img , label 
~~ def print_params ( self , details = True , session = None ) : 
for i , p in enumerate ( self . all_params ) : 
~~~ if details : 
~~~ val = p . eval ( session = session ) 
format ( i , p . name , str ( val . shape ) , p . dtype . name , val . mean ( ) , np . median ( val ) , val . std ( ) ) 
~~~ logging . info ( str ( e ) ) 
raise Exception ( 
~~ def print_layers ( self ) : 
for i , layer in enumerate ( self . all_layers ) : 
~~ ~~ def count_params ( self ) : 
n_params = 0 
for _i , p in enumerate ( self . all_params ) : 
~~~ n = 1 
for s in p . get_shape ( ) : 
~~~ s = int ( s ) 
~~~ s = 1 
~~ if s : 
~~~ n = n * s 
~~ ~~ n_params = n_params + n 
~~ return n_params 
~~ def get_all_params ( self , session = None ) : 
_params = [ ] 
for p in self . all_params : 
~~~ if session is None : 
~~~ _params . append ( p . eval ( ) ) 
~~~ _params . append ( session . run ( p ) ) 
~~ ~~ return _params 
~~ def _get_init_args ( self , skip = 4 ) : 
stack = inspect . stack ( ) 
if len ( stack ) < skip + 1 : 
~~ args , _ , _ , values = inspect . getargvalues ( stack [ skip ] [ 0 ] ) 
params = { } 
~~~ if values [ arg ] is not None and arg not in [ 'self' , 'prev_layer' , 'inputs' ] : 
~~~ val = values [ arg ] 
if inspect . isfunction ( val ) : 
~~~ params [ arg ] = { "module_path" : val . __module__ , "func_name" : val . __name__ } 
~~ elif arg . endswith ( 'init' ) : 
~~~ params [ arg ] = val 
~~ ~~ ~~ return params 
~~ def roi_pooling ( input , rois , pool_height , pool_width ) : 
out = roi_pooling_module . roi_pooling ( input , rois , pool_height = pool_height , pool_width = pool_width ) 
output , argmax_output = out [ 0 ] , out [ 1 ] 
~~ def _int64_feature ( value ) : 
return tf . train . Feature ( int64_list = tf . train . Int64List ( value = [ value ] ) ) 
~~ def _bytes_feature ( value ) : 
return tf . train . Feature ( bytes_list = tf . train . BytesList ( value = [ value ] ) ) 
~~ def _int64_feature_list ( values ) : 
return tf . train . FeatureList ( feature = [ _int64_feature ( v ) for v in values ] ) 
~~ def _bytes_feature_list ( values ) : 
return tf . train . FeatureList ( feature = [ _bytes_feature ( v ) for v in values ] ) 
~~ def distort_image ( image , thread_id ) : 
~~~ image = tf . image . random_flip_left_right ( image ) 
~~ color_ordering = thread_id % 2 
~~~ if color_ordering == 0 : 
~~~ image = tf . image . random_brightness ( image , max_delta = 32. / 255. ) 
image = tf . image . random_saturation ( image , lower = 0.5 , upper = 1.5 ) 
image = tf . image . random_hue ( image , max_delta = 0.032 ) 
image = tf . image . random_contrast ( image , lower = 0.5 , upper = 1.5 ) 
~~ elif color_ordering == 1 : 
~~ image = tf . clip_by_value ( image , 0.0 , 1.0 ) 
~~ def prefetch_input_data ( 
reader , file_pattern , is_training , batch_size , values_per_shard , input_queue_capacity_factor = 16 , 
num_reader_threads = 1 , shard_queue_name = "filename_queue" , value_queue_name = "input_queue" 
data_files = [ ] 
for pattern in file_pattern . split ( "," ) : 
~~~ data_files . extend ( tf . gfile . Glob ( pattern ) ) 
~~ if not data_files : 
~~ if is_training : 
filename_queue = tf . train . string_input_producer ( data_files , shuffle = True , capacity = 16 , name = shard_queue_name ) 
min_queue_examples = values_per_shard * input_queue_capacity_factor 
capacity = min_queue_examples + 100 * batch_size 
values_queue = tf . RandomShuffleQueue ( 
capacity = capacity , min_after_dequeue = min_queue_examples , dtypes = [ tf . string ] , 
name = "random_" + value_queue_name 
filename_queue = tf . train . string_input_producer ( data_files , shuffle = False , capacity = 1 , name = shard_queue_name ) 
capacity = values_per_shard + 3 * batch_size 
values_queue = tf . FIFOQueue ( capacity = capacity , dtypes = [ tf . string ] , name = "fifo_" + value_queue_name ) 
~~ enqueue_ops = [ ] 
for _ in range ( num_reader_threads ) : 
~~~ _ , value = reader . read ( filename_queue ) 
enqueue_ops . append ( values_queue . enqueue ( [ value ] ) ) 
~~ tf . train . queue_runner . add_queue_runner ( tf . train . queue_runner . QueueRunner ( values_queue , enqueue_ops ) ) 
tf . summary . scalar ( 
"queue/%s/fraction_of_%d_full" % ( values_queue . name , capacity ) , 
tf . cast ( values_queue . size ( ) , tf . float32 ) * ( 1. / capacity ) 
return values_queue 
~~ def batch_with_dynamic_pad ( images_and_captions , batch_size , queue_capacity , add_summaries = True ) : 
enqueue_list = [ ] 
for image , caption in images_and_captions : 
~~~ caption_length = tf . shape ( caption ) [ 0 ] 
input_length = tf . expand_dims ( tf . subtract ( caption_length , 1 ) , 0 ) 
input_seq = tf . slice ( caption , [ 0 ] , input_length ) 
target_seq = tf . slice ( caption , [ 1 ] , input_length ) 
indicator = tf . ones ( input_length , dtype = tf . int32 ) 
enqueue_list . append ( [ image , input_seq , target_seq , indicator ] ) 
~~ images , input_seqs , target_seqs , mask = tf . train . batch_join ( 
enqueue_list , batch_size = batch_size , capacity = queue_capacity , dynamic_pad = True , name = "batch_and_pad" 
if add_summaries : 
~~~ lengths = tf . add ( tf . reduce_sum ( mask , 1 ) , 1 ) 
tf . summary . scalar ( "caption_length/batch_min" , tf . reduce_min ( lengths ) ) 
tf . summary . scalar ( "caption_length/batch_max" , tf . reduce_max ( lengths ) ) 
tf . summary . scalar ( "caption_length/batch_mean" , tf . reduce_mean ( lengths ) ) 
~~ return images , input_seqs , target_seqs , mask 
~~ def _to_channel_first_bias ( b ) : 
channel_size = int ( b . shape [ 0 ] ) 
new_shape = ( channel_size , 1 , 1 ) 
return tf . reshape ( b , new_shape ) 
~~ def _bias_scale ( x , b , data_format ) : 
if data_format == 'NHWC' : 
~~~ return x * b 
~~ elif data_format == 'NCHW' : 
~~~ return x * _to_channel_first_bias ( b ) 
~~ ~~ def _bias_add ( x , b , data_format ) : 
~~~ return tf . add ( x , b ) 
~~~ return tf . add ( x , _to_channel_first_bias ( b ) ) 
~~ ~~ def batch_normalization ( x , mean , variance , offset , scale , variance_epsilon , data_format , name = None ) : 
with ops . name_scope ( name , 'batchnorm' , [ x , mean , variance , scale , offset ] ) : 
~~~ inv = math_ops . rsqrt ( variance + variance_epsilon ) 
if scale is not None : 
~~~ inv *= scale 
~~ a = math_ops . cast ( inv , x . dtype ) 
b = math_ops . cast ( offset - mean * inv if offset is not None else - mean * inv , x . dtype ) 
df = { 'channels_first' : 'NCHW' , 'channels_last' : 'NHWC' } 
return _bias_add ( _bias_scale ( x , a , df [ data_format ] ) , b , df [ data_format ] ) 
~~ ~~ def compute_alpha ( x ) : 
threshold = _compute_threshold ( x ) 
alpha1_temp1 = tf . where ( tf . greater ( x , threshold ) , x , tf . zeros_like ( x , tf . float32 ) ) 
alpha1_temp2 = tf . where ( tf . less ( x , - threshold ) , x , tf . zeros_like ( x , tf . float32 ) ) 
alpha_array = tf . add ( alpha1_temp1 , alpha1_temp2 , name = None ) 
alpha_array_abs = tf . abs ( alpha_array ) 
alpha_array_abs1 = tf . where ( 
tf . greater ( alpha_array_abs , 0 ) , tf . ones_like ( alpha_array_abs , tf . float32 ) , 
tf . zeros_like ( alpha_array_abs , tf . float32 ) 
alpha_sum = tf . reduce_sum ( alpha_array_abs ) 
n = tf . reduce_sum ( alpha_array_abs1 ) 
alpha = tf . div ( alpha_sum , n ) 
return alpha 
~~ def flatten_reshape ( variable , name = 'flatten' ) : 
dim = 1 
for d in variable . get_shape ( ) [ 1 : ] . as_list ( ) : 
~~~ dim *= d 
~~ return tf . reshape ( variable , shape = [ - 1 , dim ] , name = name ) 
~~ def get_layers_with_name ( net , name = "" , verbose = False ) : 
layers = [ ] 
i = 0 
for layer in net . all_layers : 
~~~ if name in layer . name : 
~~~ layers . append ( layer ) 
i = i + 1 
~~ ~~ ~~ return layers 
~~ def get_variables_with_name ( name = None , train_only = True , verbose = False ) : 
if train_only : 
~~~ t_vars = tf . trainable_variables ( ) 
~~~ t_vars = tf . global_variables ( ) 
~~ d_vars = [ var for var in t_vars if name in var . name ] 
~~~ for idx , v in enumerate ( d_vars ) : 
~~ ~~ return d_vars 
~~ def initialize_rnn_state ( state , feed_dict = None ) : 
if isinstance ( state , LSTMStateTuple ) : 
~~~ c = state . c . eval ( feed_dict = feed_dict ) 
h = state . h . eval ( feed_dict = feed_dict ) 
return c , h 
~~~ new_state = state . eval ( feed_dict = feed_dict ) 
return new_state 
~~ ~~ def list_remove_repeat ( x ) : 
for i in x : 
~~~ if i not in y : 
~~~ y . append ( i ) 
~~ ~~ return y 
~~ def merge_networks ( layers = None ) : 
if layers is None : 
~~ layer = layers [ 0 ] 
all_params = [ ] 
all_layers = [ ] 
all_drop = { } 
for l in layers : 
~~~ all_params . extend ( l . all_params ) 
all_layers . extend ( l . all_layers ) 
all_drop . update ( l . all_drop ) 
~~ layer . all_params = list ( all_params ) 
layer . all_layers = list ( all_layers ) 
layer . all_drop = dict ( all_drop ) 
layer . all_layers = list_remove_repeat ( layer . all_layers ) 
layer . all_params = list_remove_repeat ( layer . all_params ) 
return layer 
~~ def print_all_variables ( train_only = False ) : 
~~ for idx , v in enumerate ( t_vars ) : 
~~ ~~ def ternary_operation ( x ) : 
g = tf . get_default_graph ( ) 
with g . gradient_override_map ( { "Sign" : "Identity" } ) : 
~~~ threshold = _compute_threshold ( x ) 
x = tf . sign ( tf . add ( tf . sign ( tf . add ( x , threshold ) ) , tf . sign ( tf . add ( x , - threshold ) ) ) ) 
~~ ~~ def _compute_threshold ( x ) : 
x_sum = tf . reduce_sum ( tf . abs ( x ) , reduction_indices = None , keepdims = False , name = None ) 
threshold = tf . div ( x_sum , tf . cast ( tf . size ( x ) , tf . float32 ) , name = None ) 
threshold = tf . multiply ( 0.7 , threshold , name = None ) 
return threshold 
~~ def freeze_graph ( graph_path , checkpoint_path , output_path , end_node_names , is_binary_graph ) : 
_freeze_graph ( 
input_graph = graph_path , input_saver = '' , input_binary = is_binary_graph , input_checkpoint = checkpoint_path , 
output_graph = output_path , output_node_names = end_node_names , restore_op_name = 'save/restore_all' , 
filename_tensor_name = 'save/Const:0' , clear_devices = True , initializer_nodes = None 
~~ def convert_model_to_onnx ( frozen_graph_path , end_node_names , onnx_output_path ) : 
with tf . gfile . GFile ( frozen_graph_path , "rb" ) as f : 
~~~ graph_def = tf . GraphDef ( ) 
graph_def . ParseFromString ( f . read ( ) ) 
onnx_model = tensorflow_graph_to_onnx_model ( graph_def , end_node_names , opset = 6 ) 
file = open ( onnx_output_path , "wb" ) 
file . write ( onnx_model . SerializeToString ( ) ) 
file . close ( ) 
~~ ~~ def convert_onnx_to_model ( onnx_input_path ) : 
model = onnx . load ( onnx_input_path ) 
tf_rep = prepare ( model ) 
img = np . load ( "./assets/image.npz" ) 
output = tf_rep . run ( img . reshape ( [ 1 , 784 ] ) ) 
~~ def _add_deprecated_function_notice_to_docstring ( doc , date , instructions ) : 
if instructions : 
~~ main_text = [ deprecation_message ] 
~~ def _add_notice_to_docstring ( doc , no_doc_str , notice ) : 
if not doc : 
~~~ lines = [ no_doc_str ] 
~~~ lines = _normalize_docstring ( doc ) . splitlines ( ) 
~~ notice = [ '' ] + notice 
if len ( lines ) > 1 : 
~~~ if lines [ 1 ] . strip ( ) : 
~~~ notice . append ( '' ) 
~~ lines [ 1 : 1 ] = notice 
~~~ lines += notice 
~~ return '\\n' . join ( lines ) 
~~ def alphas ( shape , alpha_value , name = None ) : 
with ops . name_scope ( name , "alphas" , [ shape ] ) as name : 
~~~ alpha_tensor = convert_to_tensor ( alpha_value ) 
alpha_dtype = dtypes . as_dtype ( alpha_tensor . dtype ) . base_dtype 
if not isinstance ( shape , ops . Tensor ) : 
~~~ shape = constant_op . _tensor_shape_tensor_conversion_function ( tensor_shape . TensorShape ( shape ) ) 
~~~ shape = ops . convert_to_tensor ( shape , dtype = dtypes . int32 ) 
~~ ~~ if not shape . _shape_tuple ( ) : 
~~~ output = constant ( alpha_value , shape = shape , dtype = alpha_dtype , name = name ) 
~~~ output = fill ( shape , constant ( alpha_value , dtype = alpha_dtype ) , name = name ) 
~~ if output . dtype . base_dtype != alpha_dtype : 
~~ return output 
~~ ~~ def alphas_like ( tensor , alpha_value , name = None , optimize = True ) : 
with ops . name_scope ( name , "alphas_like" , [ tensor ] ) as name : 
~~~ tensor = ops . convert_to_tensor ( tensor , name = "tensor" ) 
~~~ ret = alphas ( shape_internal ( tensor , optimize = optimize ) , alpha_value = alpha_value , name = name ) 
~~~ if ( optimize and tensor . shape . is_fully_defined ( ) ) : 
~~~ ret = alphas ( tensor . shape , alpha_value = alpha_value , name = name ) 
~~ ret . set_shape ( tensor . get_shape ( ) ) 
~~ ~~ def example1 ( ) : 
st = time . time ( ) 
~~~ xx = tl . prepro . rotation ( image , rg = - 20 , is_random = False ) 
xx = tl . prepro . flip_axis ( xx , axis = 1 , is_random = False ) 
xx = tl . prepro . shear2 ( xx , shear = ( 0. , - 0.2 ) , is_random = False ) 
xx = tl . prepro . zoom ( xx , zoom_range = 1 / 0.8 ) 
xx = tl . prepro . shift ( xx , wrg = - 0.1 , hrg = 0 , is_random = False ) 
tl . vis . save_image ( xx , '_result_slow.png' ) 
~~ def example2 ( ) : 
~~~ transform_matrix = create_transformation_matrix ( ) 
tl . vis . save_image ( result , '_result_fast.png' ) 
~~ def example3 ( ) : 
n_data = 100 
imgs_file_list = [ 'tiger.jpeg' ] * n_data 
train_targets = [ np . ones ( 1 ) ] * n_data 
def generator ( ) : 
~~~ if len ( imgs_file_list ) != len ( train_targets ) : 
~~ for _input , _target in zip ( imgs_file_list , train_targets ) : 
~~~ yield _input , _target 
~~ ~~ def _data_aug_fn ( image ) : 
~~ def _map_fn ( image_path , target ) : 
~~~ image = tf . read_file ( image_path ) 
image = tf . image . convert_image_dtype ( image , dtype = tf . float32 ) 
image = tf . py_func ( _data_aug_fn , [ image ] , [ tf . float32 ] ) 
target = tf . reshape ( target , ( ) ) 
return image , target 
~~ n_epoch = 10 
batch_size = 5 
dataset = tf . data . Dataset ( ) . from_generator ( generator , output_types = ( tf . string , tf . int64 ) ) 
dataset = dataset . repeat ( n_epoch ) 
dataset = dataset . map ( _map_fn , num_parallel_calls = multiprocessing . cpu_count ( ) ) 
iterator = dataset . make_one_shot_iterator ( ) 
one_element = iterator . get_next ( ) 
sess = tf . Session ( ) 
n_step = round ( n_epoch * n_data / batch_size ) 
for _ in range ( n_step ) : 
~~~ _images , _targets = sess . run ( one_element ) 
~~ def example4 ( ) : 
transform_matrix = create_transformation_matrix ( ) 
coords = [ [ ( 50 , 100 ) , ( 100 , 100 ) , ( 100 , 50 ) , ( 200 , 200 ) ] , [ ( 250 , 50 ) , ( 200 , 50 ) , ( 200 , 100 ) ] ] 
coords_result = tl . prepro . affine_transform_keypoints ( coords , transform_matrix ) 
def imwrite ( image , coords_list , name ) : 
~~~ coords_list_ = [ ] 
~~~ coords = np . array ( coords , np . int32 ) 
coords = coords . reshape ( ( - 1 , 1 , 2 ) ) 
coords_list_ . append ( coords ) 
~~ image = cv2 . polylines ( image , coords_list_ , True , ( 0 , 255 , 255 ) , 3 ) 
cv2 . imwrite ( name , image [ ... , : : - 1 ] ) 
~~ imwrite ( image , coords , '_with_keypoints_origin.png' ) 
imwrite ( result , coords_result , '_with_keypoints_result.png' ) 
~~ def distort_fn ( x , is_train = False ) : 
x = tl . prepro . crop ( x , 24 , 24 , is_random = is_train ) 
if is_train : 
~~~ x = tl . prepro . flip_axis ( x , axis = 1 , is_random = True ) 
x = tl . prepro . brightness ( x , gamma = 0.1 , gain = 1 , is_random = True ) 
~~ def fit ( 
sess , network , train_op , cost , X_train , y_train , x , y_ , acc = None , batch_size = 100 , n_epoch = 100 , print_freq = 5 , 
X_val = None , y_val = None , eval_train = True , tensorboard_dir = None , tensorboard_epoch_freq = 5 , 
tensorboard_weight_histograms = True , tensorboard_graph_vis = True 
if X_train . shape [ 0 ] < batch_size : 
~~ if tensorboard_dir is not None : 
tl . files . exists_or_mkdir ( tensorboard_dir ) 
if hasattr ( tf , 'summary' ) and hasattr ( tf . summary , 'FileWriter' ) : 
~~~ if tensorboard_graph_vis : 
~~~ train_writer = tf . summary . FileWriter ( tensorboard_dir + '/train' , sess . graph ) 
val_writer = tf . summary . FileWriter ( tensorboard_dir + '/validation' , sess . graph ) 
~~~ train_writer = tf . summary . FileWriter ( tensorboard_dir + '/train' ) 
val_writer = tf . summary . FileWriter ( tensorboard_dir + '/validation' ) 
~~ ~~ if ( tensorboard_weight_histograms ) : 
~~~ for param in network . all_params : 
~~~ if hasattr ( tf , 'summary' ) and hasattr ( tf . summary , 'histogram' ) : 
tf . summary . histogram ( param . name , param ) 
~~ ~~ ~~ if hasattr ( tf , 'summary' ) and hasattr ( tf . summary , 'histogram' ) : 
~~~ tf . summary . scalar ( 'cost' , cost ) 
~~ merged = tf . summary . merge_all ( ) 
tl . layers . initialize_global_variables ( sess ) 
start_time_begin = time . time ( ) 
tensorboard_train_index , tensorboard_val_index = 0 , 0 
for epoch in range ( n_epoch ) : 
~~~ start_time = time . time ( ) 
loss_ep = 0 
n_step = 0 
for X_train_a , y_train_a in tl . iterate . minibatches ( X_train , y_train , batch_size , shuffle = True ) : 
~~~ feed_dict = { x : X_train_a , y_ : y_train_a } 
loss , _ = sess . run ( [ cost , train_op ] , feed_dict = feed_dict ) 
loss_ep += loss 
n_step += 1 
~~ loss_ep = loss_ep / n_step 
if tensorboard_dir is not None and hasattr ( tf , 'summary' ) : 
~~~ if epoch + 1 == 1 or ( epoch + 1 ) % tensorboard_epoch_freq == 0 : 
~~~ for X_train_a , y_train_a in tl . iterate . minibatches ( X_train , y_train , batch_size , shuffle = True ) : 
feed_dict = { x : X_train_a , y_ : y_train_a } 
feed_dict . update ( dp_dict ) 
result = sess . run ( merged , feed_dict = feed_dict ) 
train_writer . add_summary ( result , tensorboard_train_index ) 
tensorboard_train_index += 1 
~~ if ( X_val is not None ) and ( y_val is not None ) : 
~~~ for X_val_a , y_val_a in tl . iterate . minibatches ( X_val , y_val , batch_size , shuffle = True ) : 
feed_dict = { x : X_val_a , y_ : y_val_a } 
val_writer . add_summary ( result , tensorboard_val_index ) 
tensorboard_val_index += 1 
~~ ~~ ~~ ~~ if epoch + 1 == 1 or ( epoch + 1 ) % print_freq == 0 : 
~~~ if ( X_val is not None ) and ( y_val is not None ) : 
if eval_train is True : 
~~~ train_loss , train_acc , n_batch = 0 , 0 , 0 
if acc is not None : 
~~~ err , ac = sess . run ( [ cost , acc ] , feed_dict = feed_dict ) 
train_acc += ac 
~~~ err = sess . run ( cost , feed_dict = feed_dict ) 
~~ train_loss += err 
n_batch += 1 
~~ ~~ val_loss , val_acc , n_batch = 0 , 0 , 0 
for X_val_a , y_val_a in tl . iterate . minibatches ( X_val , y_val , batch_size , shuffle = True ) : 
val_acc += ac 
~~ val_loss += err 
~~~ tl . logging . info ( 
~~ def predict ( sess , network , X , x , y_op , batch_size = None ) : 
if batch_size is None : 
feed_dict = { 
x : X , 
return sess . run ( y_op , feed_dict = feed_dict ) 
~~~ result = None 
for X_a , _ in tl . iterate . minibatches ( X , X , batch_size , shuffle = False ) : 
~~~ dp_dict = dict_to_one ( network . all_drop ) 
x : X_a , 
result_a = sess . run ( y_op , feed_dict = feed_dict ) 
~~~ result = result_a 
~~~ result = np . concatenate ( ( result , result_a ) ) 
~~ ~~ if result is None : 
~~~ if len ( X ) % batch_size != 0 : 
x : X [ - ( len ( X ) % batch_size ) : , : ] , 
result = result_a 
~~~ if len ( X ) != len ( result ) and len ( X ) % batch_size != 0 : 
result = np . concatenate ( ( result , result_a ) ) 
~~ ~~ def evaluation ( y_test = None , y_predict = None , n_classes = None ) : 
c_mat = confusion_matrix ( y_test , y_predict , labels = [ x for x in range ( n_classes ) ] ) 
f1 = f1_score ( y_test , y_predict , average = None , labels = [ x for x in range ( n_classes ) ] ) 
f1_macro = f1_score ( y_test , y_predict , average = 'macro' ) 
acc = accuracy_score ( y_test , y_predict ) 
return c_mat , f1 , acc , f1_macro 
~~ def class_balancing_oversample ( X_train = None , y_train = None , printable = True ) : 
~~ c = Counter ( y_train ) 
~~ most_num = c . most_common ( 1 ) [ 0 ] [ 1 ] 
~~ locations = { } 
number = { } 
~~~ number [ lab ] = num 
locations [ lab ] = np . where ( np . array ( y_train ) == lab ) [ 0 ] 
~~ if printable : 
for lab , num in number . items ( ) : 
~~~ X [ lab ] = X_train [ locations [ lab ] ] 
~~ for key in X : 
~~~ temp = X [ key ] 
~~~ if len ( X [ key ] ) >= most_num : 
~~ X [ key ] = np . vstack ( ( X [ key ] , temp ) ) 
~~~ X [ key ] = X [ key ] [ 0 : most_num , : ] 
~~ y_train = [ ] 
X_train = np . empty ( shape = ( 0 , len ( X [ 0 ] [ 0 ] ) ) ) 
for key in X : 
~~~ X_train = np . vstack ( ( X_train , X [ key ] ) ) 
y_train . extend ( [ key for i in range ( len ( X [ key ] ) ) ] ) 
~~ return X_train , y_train 
~~ def get_random_int ( min_v = 0 , max_v = 10 , number = 5 , seed = None ) : 
rnd = random . Random ( ) 
if seed : 
~~~ rnd = random . Random ( seed ) 
~~ return [ rnd . randint ( min_v , max_v ) for p in range ( 0 , number ) ] 
~~ def list_string_to_dict ( string ) : 
dictionary = { } 
for idx , c in enumerate ( string ) : 
~~~ dictionary . update ( { c : idx } ) 
~~ return dictionary 
~~ def exit_tensorflow ( sess = None , port = 6006 ) : 
if sess is not None : 
~~~ sess . close ( ) 
~~ if _platform == "linux" or _platform == "linux2" : 
os . system ( 'nvidia-smi' ) 
_exit ( ) 
~~ elif _platform == "darwin" : 
subprocess . Popen ( 
~~ elif _platform == "win32" : 
~~~ tl . logging . info ( text2 + _platform ) 
~~ ~~ def open_tensorboard ( log_dir = '/tmp/tensorflow' , port = 6006 ) : 
if not tl . files . exists_or_mkdir ( log_dir , verbose = False ) : 
~~~ raise NotImplementedError ( ) 
~~~ tl . logging . info ( _platform + text2 ) 
~~ ~~ def clear_all_placeholder_variables ( printable = True ) : 
gl = globals ( ) . copy ( ) 
for var in gl : 
~~~ if var [ 0 ] == '_' : continue 
if 'func' in str ( globals ( ) [ var ] ) : continue 
if 'module' in str ( globals ( ) [ var ] ) : continue 
if 'class' in str ( globals ( ) [ var ] ) : continue 
~~ del globals ( ) [ var ] 
~~ ~~ def set_gpu_fraction ( gpu_fraction = 0.3 ) : 
gpu_options = tf . GPUOptions ( per_process_gpu_memory_fraction = gpu_fraction ) 
sess = tf . Session ( config = tf . ConfigProto ( gpu_options = gpu_options ) ) 
return sess 
~~ def generate_skip_gram_batch ( data , batch_size , num_skips , skip_window , data_index = 0 ) : 
if batch_size % num_skips != 0 : 
~~ if num_skips > 2 * skip_window : 
~~ batch = np . ndarray ( shape = ( batch_size ) , dtype = np . int32 ) 
labels = np . ndarray ( shape = ( batch_size , 1 ) , dtype = np . int32 ) 
buffer = collections . deque ( maxlen = span ) 
for _ in range ( span ) : 
~~~ buffer . append ( data [ data_index ] ) 
data_index = ( data_index + 1 ) % len ( data ) 
~~ for i in range ( batch_size // num_skips ) : 
targets_to_avoid = [ skip_window ] 
for j in range ( num_skips ) : 
~~~ while target in targets_to_avoid : 
~~~ target = random . randint ( 0 , span - 1 ) 
~~ targets_to_avoid . append ( target ) 
batch [ i * num_skips + j ] = buffer [ skip_window ] 
labels [ i * num_skips + j , 0 ] = buffer [ target ] 
~~ buffer . append ( data [ data_index ] ) 
~~ return batch , labels , data_index 
~~ def sample ( a = None , temperature = 1.0 ) : 
if a is None : 
~~ b = np . copy ( a ) 
~~~ if temperature == 1 : 
~~~ return np . argmax ( np . random . multinomial ( 1 , a , 1 ) ) 
~~ if temperature is None : 
~~~ return np . argmax ( a ) 
~~~ a = np . log ( a ) / temperature 
a = np . exp ( a ) / np . sum ( np . exp ( a ) ) 
return np . argmax ( np . random . multinomial ( 1 , a , 1 ) ) 
~~ ~~ except Exception : 
warnings . warn ( message , Warning ) 
return np . argmax ( np . random . multinomial ( 1 , b , 1 ) ) 
~~ ~~ def sample_top ( a = None , top_k = 10 ) : 
~~~ a = [ ] 
~~ idx = np . argpartition ( a , - top_k ) [ - top_k : ] 
probs = a [ idx ] 
probs = probs / np . sum ( probs ) 
choice = np . random . choice ( idx , p = probs ) 
return choice 
~~ def process_sentence ( sentence , start_word = "<S>" , end_word = "</S>" ) : 
if start_word is not None : 
~~~ process_sentence = [ start_word ] 
~~~ process_sentence = [ ] 
~~ process_sentence . extend ( nltk . tokenize . word_tokenize ( sentence . lower ( ) ) ) 
if end_word is not None : 
~~~ process_sentence . append ( end_word ) 
~~ return process_sentence 
~~ def create_vocab ( sentences , word_counts_output_file , min_word_count = 1 ) : 
counter = Counter ( ) 
for c in sentences : 
~~~ counter . update ( c ) 
word_counts = [ x for x in counter . items ( ) if x [ 1 ] >= min_word_count ] 
word_counts . sort ( key = lambda x : x [ 1 ] , reverse = True ) 
with tf . gfile . FastGFile ( word_counts_output_file , "w" ) as f : 
reverse_vocab = [ x [ 0 ] for x in word_counts ] 
unk_id = len ( reverse_vocab ) 
vocab_dict = dict ( [ ( x , y ) for ( y , x ) in enumerate ( reverse_vocab ) ] ) 
vocab = SimpleVocabulary ( vocab_dict , unk_id ) 
return vocab 
~~ def read_words ( filename = "nietzsche.txt" , replace = None ) : 
if replace is None : 
~~~ replace = [ '\\n' , '<eos>' ] 
~~ with tf . gfile . GFile ( filename , "r" ) as f : 
~~~ context_list = f . read ( ) . replace ( * replace ) . split ( ) 
~~~ f . seek ( 0 ) 
replace = [ x . encode ( 'utf-8' ) for x in replace ] 
context_list = f . read ( ) . replace ( * replace ) . split ( ) 
~~ return context_list 
~~ ~~ def read_analogies_file ( eval_file = 'questions-words.txt' , word2id = None ) : 
if word2id is None : 
~~~ word2id = { } 
~~ questions = [ ] 
questions_skipped = 0 
with open ( eval_file , "rb" ) as analogy_f : 
~~~ for line in analogy_f : 
ids = [ word2id . get ( w . strip ( ) ) for w in words ] 
if None in ids or len ( ids ) != 4 : 
~~~ questions_skipped += 1 
~~~ questions . append ( np . array ( ids ) ) 
analogy_questions = np . array ( questions , dtype = np . int32 ) 
return analogy_questions 
~~ def build_reverse_dictionary ( word_to_id ) : 
reverse_dictionary = dict ( zip ( word_to_id . values ( ) , word_to_id . keys ( ) ) ) 
return reverse_dictionary 
~~ def build_words_dataset ( words = None , vocabulary_size = 50000 , printable = True , unk_key = 'UNK' ) : 
if words is None : 
~~ count = [ [ unk_key , - 1 ] ] 
count . extend ( collections . Counter ( words ) . most_common ( vocabulary_size - 1 ) ) 
dictionary = dict ( ) 
for word , _ in count : 
~~~ dictionary [ word ] = len ( dictionary ) 
~~ data = list ( ) 
unk_count = 0 
for word in words : 
~~~ if word in dictionary : 
~~~ index = dictionary [ word ] 
unk_count += 1 
~~ data . append ( index ) 
~~ count [ 0 ] [ 1 ] = unk_count 
reverse_dictionary = dict ( zip ( dictionary . values ( ) , dictionary . keys ( ) ) ) 
~~ if len ( collections . Counter ( words ) . keys ( ) ) < vocabulary_size : 
~~ return data , count , dictionary , reverse_dictionary 
~~ def words_to_word_ids ( data = None , word_to_id = None , unk_key = 'UNK' ) : 
if data is None : 
~~ if word_to_id is None : 
~~ word_ids = [ ] 
for word in data : 
~~~ if word_to_id . get ( word ) is not None : 
~~~ word_ids . append ( word_to_id [ word ] ) 
~~~ word_ids . append ( word_to_id [ unk_key ] ) 
~~ ~~ return word_ids 
~~ def save_vocab ( count = None , name = 'vocab.txt' ) : 
if count is None : 
~~~ count = [ ] 
~~ pwd = os . getcwd ( ) 
vocabulary_size = len ( count ) 
with open ( os . path . join ( pwd , name ) , "w" ) as f : 
~~~ for i in xrange ( vocabulary_size ) : 
~~ def basic_tokenizer ( sentence , _WORD_SPLIT = re . compile ( b"([.,!?\\"\ ) ) : 
words = [ ] 
sentence = tf . compat . as_bytes ( sentence ) 
for space_separated_fragment in sentence . strip ( ) . split ( ) : 
~~~ words . extend ( re . split ( _WORD_SPLIT , space_separated_fragment ) ) 
~~ return [ w for w in words if w ] 
~~ def create_vocabulary ( 
vocabulary_path , data_path , max_vocabulary_size , tokenizer = None , normalize_digits = True , 
_DIGIT_RE = re . compile ( br"\\d" ) , _START_VOCAB = None 
if _START_VOCAB is None : 
~~~ _START_VOCAB = [ b"_PAD" , b"_GO" , b"_EOS" , b"_UNK" ] 
~~ if not gfile . Exists ( vocabulary_path ) : 
vocab = { } 
with gfile . GFile ( data_path , mode = "rb" ) as f : 
~~~ counter = 0 
for line in f : 
~~~ counter += 1 
if counter % 100000 == 0 : 
~~ tokens = tokenizer ( line ) if tokenizer else basic_tokenizer ( line ) 
for w in tokens : 
~~~ word = re . sub ( _DIGIT_RE , b"0" , w ) if normalize_digits else w 
if word in vocab : 
~~~ vocab [ word ] += 1 
~~~ vocab [ word ] = 1 
~~ ~~ ~~ vocab_list = _START_VOCAB + sorted ( vocab , key = vocab . get , reverse = True ) 
if len ( vocab_list ) > max_vocabulary_size : 
~~~ vocab_list = vocab_list [ : max_vocabulary_size ] 
~~ with gfile . GFile ( vocabulary_path , mode = "wb" ) as vocab_file : 
~~~ for w in vocab_list : 
~~~ vocab_file . write ( w + b"\\n" ) 
~~ ~~ ~~ ~~ else : 
~~ ~~ def initialize_vocabulary ( vocabulary_path ) : 
if gfile . Exists ( vocabulary_path ) : 
~~~ rev_vocab = [ ] 
with gfile . GFile ( vocabulary_path , mode = "rb" ) as f : 
~~~ rev_vocab . extend ( f . readlines ( ) ) 
~~ rev_vocab = [ tf . compat . as_bytes ( line . strip ( ) ) for line in rev_vocab ] 
vocab = dict ( [ ( x , y ) for ( y , x ) in enumerate ( rev_vocab ) ] ) 
return vocab , rev_vocab 
~~ ~~ def sentence_to_token_ids ( 
sentence , vocabulary , tokenizer = None , normalize_digits = True , UNK_ID = 3 , _DIGIT_RE = re . compile ( br"\\d" ) 
if tokenizer : 
~~~ words = tokenizer ( sentence ) 
~~~ words = basic_tokenizer ( sentence ) 
~~ if not normalize_digits : 
~~~ return [ vocabulary . get ( w , UNK_ID ) for w in words ] 
~~ return [ vocabulary . get ( re . sub ( _DIGIT_RE , b"0" , w ) , UNK_ID ) for w in words ] 
~~ def data_to_token_ids ( 
data_path , target_path , vocabulary_path , tokenizer = None , normalize_digits = True , UNK_ID = 3 , 
_DIGIT_RE = re . compile ( br"\\d" ) 
if not gfile . Exists ( target_path ) : 
vocab , _ = initialize_vocabulary ( vocabulary_path ) 
with gfile . GFile ( data_path , mode = "rb" ) as data_file : 
~~~ with gfile . GFile ( target_path , mode = "w" ) as tokens_file : 
for line in data_file : 
~~ token_ids = sentence_to_token_ids ( 
line , vocab , tokenizer , normalize_digits , UNK_ID = UNK_ID , _DIGIT_RE = _DIGIT_RE 
~~ ~~ def moses_multi_bleu ( hypotheses , references , lowercase = False ) : 
if np . size ( hypotheses ) == 0 : 
~~~ return np . float32 ( 0.0 ) 
~~~ multi_bleu_path , _ = urllib . request . urlretrieve ( 
"https://raw.githubusercontent.com/moses-smt/mosesdecoder/" 
"master/scripts/generic/multi-bleu.perl" 
os . chmod ( multi_bleu_path , 0o755 ) 
metrics_dir = os . path . dirname ( os . path . realpath ( __file__ ) ) 
bin_dir = os . path . abspath ( os . path . join ( metrics_dir , ".." , ".." , "bin" ) ) 
multi_bleu_path = os . path . join ( bin_dir , "tools/multi-bleu.perl" ) 
~~ hypothesis_file = tempfile . NamedTemporaryFile ( ) 
hypothesis_file . write ( "\\n" . join ( hypotheses ) . encode ( "utf-8" ) ) 
hypothesis_file . write ( b"\\n" ) 
hypothesis_file . flush ( ) 
reference_file = tempfile . NamedTemporaryFile ( ) 
reference_file . write ( "\\n" . join ( references ) . encode ( "utf-8" ) ) 
reference_file . write ( b"\\n" ) 
reference_file . flush ( ) 
with open ( hypothesis_file . name , "r" ) as read_pred : 
~~~ bleu_cmd = [ multi_bleu_path ] 
if lowercase : 
~~~ bleu_cmd += [ "-lc" ] 
~~ bleu_cmd += [ reference_file . name ] 
~~~ bleu_out = subprocess . check_output ( bleu_cmd , stdin = read_pred , stderr = subprocess . STDOUT ) 
bleu_out = bleu_out . decode ( "utf-8" ) 
bleu_score = float ( bleu_score ) 
~~ except subprocess . CalledProcessError as error : 
~~~ if error . output is not None : 
tl . logging . warning ( error . output ) 
~~ bleu_score = np . float32 ( 0.0 ) 
~~ ~~ hypothesis_file . close ( ) 
reference_file . close ( ) 
return np . float32 ( bleu_score ) 
~~ def word_to_id ( self , word ) : 
if word in self . _vocab : 
~~~ return self . _vocab [ word ] 
~~~ return self . _unk_id 
~~ ~~ def word_to_id ( self , word ) : 
if word in self . vocab : 
~~~ return self . vocab [ word ] 
~~~ return self . unk_id 
~~ ~~ def id_to_word ( self , word_id ) : 
if word_id >= len ( self . reverse_vocab ) : 
~~~ return self . reverse_vocab [ self . unk_id ] 
~~~ return self . reverse_vocab [ word_id ] 
~~ ~~ def basic_clean_str ( string ) : 
string = re . sub ( r"\\’d" , "" , string ) 
return string . strip ( ) . lower ( ) 
~~ def main_restore_embedding_layer ( ) : 
vocabulary_size = 50000 
embedding_size = 128 
model_file_name = "model_word2vec_50k_128" 
batch_size = None 
all_var = tl . files . load_npy_to_any ( name = model_file_name + '.npy' ) 
data = all_var [ 'data' ] 
count = all_var [ 'count' ] 
dictionary = all_var [ 'dictionary' ] 
reverse_dictionary = all_var [ 'reverse_dictionary' ] 
tl . nlp . save_vocab ( count , name = 'vocab_' + model_file_name + '.txt' ) 
del all_var , data , count 
load_params = tl . files . load_npz ( name = model_file_name + '.npz' ) 
x = tf . placeholder ( tf . int32 , shape = [ batch_size ] ) 
emb_net = tl . layers . EmbeddingInputlayer ( x , vocabulary_size , embedding_size , name = 'emb' ) 
tl . files . assign_params ( sess , [ load_params [ 0 ] ] , emb_net ) 
emb_net . print_params ( ) 
emb_net . print_layers ( ) 
word = b'hello' 
word_id = dictionary [ word ] 
print ( 'word_id:' , word_id ) 
words = [ b'i' , b'am' , b'tensor' , b'layer' ] 
word_ids = tl . nlp . words_to_word_ids ( words , dictionary , _UNK ) 
context = tl . nlp . word_ids_to_words ( word_ids , reverse_dictionary ) 
print ( 'word_ids:' , word_ids ) 
print ( 'context:' , context ) 
vector = sess . run ( emb_net . outputs , feed_dict = { x : [ word_id ] } ) 
print ( 'vector:' , vector . shape ) 
vectors = sess . run ( emb_net . outputs , feed_dict = { x : word_ids } ) 
print ( 'vectors:' , vectors . shape ) 
~~ def main_lstm_generate_text ( ) : 
init_scale = 0.1 
sequence_length = 20 
max_max_epoch = 100 
lr_decay = 0.9 
top_k_list = [ 1 , 3 , 5 , 10 ] 
print_length = 30 
model_file_name = "model_generate_text.npz" 
words = customized_read_words ( input_fpath = "data/trump/trump_text.txt" ) 
vocab = tl . nlp . create_vocab ( [ words ] , word_counts_output_file = 'vocab.txt' , min_word_count = 1 ) 
vocab = tl . nlp . Vocabulary ( 'vocab.txt' , unk_word = "<UNK>" ) 
vocab_size = vocab . unk_id + 1 
train_data = [ vocab . word_to_id ( word ) for word in words ] 
seed = nltk . tokenize . word_tokenize ( seed ) 
input_data = tf . placeholder ( tf . int32 , [ batch_size , sequence_length ] ) 
targets = tf . placeholder ( tf . int32 , [ batch_size , sequence_length ] ) 
def inference ( x , is_train , sequence_length , reuse = None ) : 
rnn_init = tf . random_uniform_initializer ( - init_scale , init_scale ) 
~~~ network = EmbeddingInputlayer ( x , vocab_size , hidden_size , rnn_init , name = 'embedding' ) 
network = RNNLayer ( 
network , cell_fn = tf . contrib . rnn . BasicLSTMCell , cell_init_args = { 
'forget_bias' : 0.0 , 
'state_is_tuple' : True 
} , n_hidden = hidden_size , initializer = rnn_init , n_steps = sequence_length , return_last = False , 
return_seq_2d = True , name = 'lstm1' 
lstm1 = network 
network = DenseLayer ( network , vocab_size , W_init = rnn_init , b_init = rnn_init , act = None , name = 'output' ) 
~~ return network , lstm1 
~~ network , lstm1 = inference ( input_data , is_train = True , sequence_length = sequence_length , reuse = None ) 
network_test , lstm1_test = inference ( input_data_test , is_train = False , sequence_length = 1 , reuse = True ) 
y_linear = network_test . outputs 
y_soft = tf . nn . softmax ( y_linear ) 
def loss_fn ( outputs , targets , batch_size , sequence_length ) : 
[ outputs ] , [ tf . reshape ( targets , [ - 1 ] ) ] , [ tf . ones ( [ batch_size * sequence_length ] ) ] 
~~ cost = loss_fn ( network . outputs , targets , batch_size , sequence_length ) 
~~ tvars = network . all_params 
epoch_size = ( ( len ( train_data ) // batch_size ) - 1 ) // sequence_length 
for step , ( x , y ) in enumerate ( tl . iterate . ptb_iterator ( train_data , batch_size , sequence_length ) ) : 
~~~ _cost , state1 , _ = sess . run ( 
[ cost , lstm1 . final_state , train_op ] , feed_dict = { 
lstm1 . initial_state : state1 
iters += sequence_length 
if step % ( epoch_size // 10 ) == 1 : 
for top_k in top_k_list : 
~~~ state1 = tl . layers . initialize_rnn_state ( lstm1_test . initial_state ) 
outs_id = [ vocab . word_to_id ( w ) for w in seed ] 
for ids in outs_id [ : - 1 ] : 
~~~ a_id = np . asarray ( ids ) . reshape ( 1 , 1 ) 
state1 = sess . run ( 
[ lstm1_test . final_state ] , feed_dict = { 
input_data_test : a_id , 
lstm1_test . initial_state : state1 
~~ a_id = outs_id [ - 1 ] 
for _ in range ( print_length ) : 
~~~ a_id = np . asarray ( a_id ) . reshape ( 1 , 1 ) 
out , state1 = sess . run ( 
[ y_soft , lstm1_test . final_state ] , feed_dict = { 
a_id = tl . nlp . sample_top ( out [ 0 ] , top_k = top_k ) 
outs_id . append ( a_id ) 
~~ sentence = [ vocab . id_to_word ( w ) for w in outs_id ] 
print ( top_k , ':' , sentence ) 
tl . files . save_npz ( network_test . all_params , name = model_file_name ) 
~~ def createAndStartSwarm ( client , clientInfo = "" , clientKey = "" , params = "" , 
minimumWorkers = None , maximumWorkers = None , 
alreadyRunning = False ) : 
if minimumWorkers is None : 
~~~ minimumWorkers = Configuration . getInt ( 
"nupic.hypersearch.minWorkersPerSwarm" ) 
~~ if maximumWorkers is None : 
~~~ maximumWorkers = Configuration . getInt ( 
"nupic.hypersearch.maxWorkersPerSwarm" ) 
~~ return ClientJobsDAO . get ( ) . jobInsert ( 
client = client , 
cmdLine = "$HYPERSEARCH" , 
clientInfo = clientInfo , 
clientKey = clientKey , 
alreadyRunning = alreadyRunning , 
params = params , 
minimumWorkers = minimumWorkers , 
maximumWorkers = maximumWorkers , 
jobType = ClientJobsDAO . JOB_TYPE_HS ) 
~~ def getSwarmModelParams ( modelID ) : 
cjDAO = ClientJobsDAO . get ( ) 
( jobID , description ) = cjDAO . modelsGetFields ( 
modelID , 
[ "jobId" , "genDescription" ] ) 
( baseDescription , ) = cjDAO . jobGetFields ( jobID , [ "genBaseDescription" ] ) 
descriptionDirectory = tempfile . mkdtemp ( ) 
~~~ baseDescriptionFilePath = os . path . join ( descriptionDirectory , "base.py" ) 
with open ( baseDescriptionFilePath , mode = "wb" ) as f : 
~~~ f . write ( baseDescription ) 
~~ descriptionFilePath = os . path . join ( descriptionDirectory , "description.py" ) 
with open ( descriptionFilePath , mode = "wb" ) as f : 
~~~ f . write ( description ) 
~~ expIface = helpers . getExperimentDescriptionInterfaceFromModule ( 
helpers . loadExperimentDescriptionScriptFromDir ( descriptionDirectory ) ) 
return json . dumps ( 
dict ( 
modelConfig = expIface . getModelDescription ( ) , 
inferenceArgs = expIface . getModelControl ( ) . get ( "inferenceArgs" , None ) ) ) 
~~~ shutil . rmtree ( descriptionDirectory , ignore_errors = True ) 
~~ ~~ def enableConcurrencyChecks ( maxConcurrency , raiseException = True ) : 
global g_max_concurrency , g_max_concurrency_raise_exception 
assert maxConcurrency >= 0 
g_max_concurrency = maxConcurrency 
g_max_concurrency_raise_exception = raiseException 
~~ def _getCommonSteadyDBArgsDict ( ) : 
return dict ( 
creator = pymysql , 
host = Configuration . get ( 'nupic.cluster.database.host' ) , 
port = int ( Configuration . get ( 'nupic.cluster.database.port' ) ) , 
user = Configuration . get ( 'nupic.cluster.database.user' ) , 
passwd = Configuration . get ( 'nupic.cluster.database.passwd' ) , 
charset = 'utf8' , 
use_unicode = True , 
~~ def _getLogger ( cls , logLevel = None ) : 
logger = logging . getLogger ( 
"." . join ( [ 'com.numenta' , _MODULE_NAME , cls . __name__ ] ) ) 
if logLevel is not None : 
~~~ logger . setLevel ( logLevel ) 
~~ return logger 
~~ def get ( cls ) : 
if cls . _connectionPolicy is None : 
~~~ logger = _getLogger ( cls ) 
cls . _connectionPolicyInstanceProvider ) 
cls . _connectionPolicy = cls . _connectionPolicyInstanceProvider ( ) 
~~ return cls . _connectionPolicy . acquireConnection ( ) 
~~ def _createDefaultPolicy ( cls ) : 
logger = _getLogger ( cls ) 
platform . system ( ) , pymysql . VERSION ) 
if platform . system ( ) == "Java" : 
~~~ policy = SingleSharedConnectionPolicy ( ) 
~~~ policy = PooledConnectionPolicy ( ) 
~~ return policy 
~~ def release ( self ) : 
if self . _addedToInstanceSet : 
~~~ self . _clsOutstandingInstances . remove ( self ) 
~~~ self . _logger . exception ( 
~~ ~~ self . _releaser ( dbConn = self . dbConn , cursor = self . cursor ) 
self . __class__ . _clsNumOutstanding -= 1 
assert self . _clsNumOutstanding >= 0 , "_clsNumOutstanding=%r" % ( self . _clsNumOutstanding , ) 
self . _releaser = None 
self . cursor = None 
self . dbConn = None 
self . _creationTracebackString = None 
self . _addedToInstanceSet = False 
self . _logger = None 
~~ def _trackInstanceAndCheckForConcurrencyViolation ( self ) : 
assert g_max_concurrency is not None 
assert self not in self . _clsOutstandingInstances , repr ( self ) 
self . _creationTracebackString = traceback . format_stack ( ) 
if self . _clsNumOutstanding >= g_max_concurrency : 
self . _clsNumOutstanding , g_max_concurrency , self , 
len ( self . _clsOutstandingInstances ) , self . _clsOutstandingInstances , ) 
self . _logger . error ( errorMsg ) 
if g_max_concurrency_raise_exception : 
~~~ raise ConcurrencyExceededError ( errorMsg ) 
~~ ~~ self . _clsOutstandingInstances . add ( self ) 
self . _addedToInstanceSet = True 
self . _logger . info ( "Closing" ) 
if self . _conn is not None : 
~~~ self . _conn . close ( ) 
self . _conn = None 
~~~ self . _logger . warning ( 
~~ def acquireConnection ( self ) : 
self . _conn . _ping_check ( ) 
connWrap = ConnectionWrapper ( dbConn = self . _conn , 
cursor = self . _conn . cursor ( ) , 
releaser = self . _releaseConnection , 
logger = self . _logger ) 
return connWrap 
if self . _pool is not None : 
~~~ self . _pool . close ( ) 
self . _pool = None 
dbConn = self . _pool . connection ( shareable = False ) 
connWrap = ConnectionWrapper ( dbConn = dbConn , 
cursor = dbConn . cursor ( ) , 
if self . _opened : 
~~~ self . _opened = False 
dbConn = SteadyDB . connect ( ** _getCommonSteadyDBArgsDict ( ) ) 
~~ def _releaseConnection ( self , dbConn , cursor ) : 
cursor . close ( ) 
dbConn . close ( ) 
~~ def getSpec ( cls ) : 
ns = dict ( 
description = KNNAnomalyClassifierRegion . __doc__ , 
singleNodeOnly = True , 
inputs = dict ( 
spBottomUpOut = dict ( 
dataType = 'Real32' , 
count = 0 , 
required = True , 
regionLevel = False , 
isDefaultInput = True , 
requireSplitterMap = False ) , 
tpTopDownOut = dict ( 
tpLrnActiveStateT = dict ( 
sequenceIdIn = dict ( 
dataType = 'UInt64' , 
count = 1 , 
required = False , 
regionLevel = True , 
isDefaultInput = False , 
outputs = dict ( 
parameters = dict ( 
trainRecords = dict ( 
dataType = 'UInt32' , 
constraints = '' , 
defaultValue = 0 , 
accessMode = 'Create' ) , 
anomalyThreshold = dict ( 
cacheSize = dict ( 
classificationVectorType = dict ( 
defaultValue = 1 , 
accessMode = 'ReadWrite' ) , 
activeColumnCount = dict ( 
defaultValue = 40 , 
classificationMaxDist = dict ( 
defaultValue = 0.65 , 
accessMode = 'Create' 
commands = dict ( 
getLabels = dict ( description = 
addLabel = dict ( description = 
removeLabels = dict ( description = 
ns [ 'parameters' ] . update ( KNNClassifierRegion . getSpec ( ) [ 'parameters' ] ) 
return ns 
~~ def getParameter ( self , name , index = - 1 ) : 
if name == "trainRecords" : 
~~~ return self . trainRecords 
~~ elif name == "anomalyThreshold" : 
~~~ return self . anomalyThreshold 
~~ elif name == "activeColumnCount" : 
~~~ return self . _activeColumnCount 
~~ elif name == "classificationMaxDist" : 
~~~ return self . _classificationMaxDist 
~~~ return PyRegion . getParameter ( self , name , index ) 
~~ ~~ def setParameter ( self , name , index , value ) : 
~~~ if not ( isinstance ( value , float ) or isinstance ( value , int ) ) : 
~~ if len ( self . _recordsCache ) > 0 and value < self . _recordsCache [ 0 ] . ROWID : 
~~ self . trainRecords = value 
self . _deleteRangeFromKNN ( 0 , self . _recordsCache [ 0 ] . ROWID ) 
self . _classifyStates ( ) 
~~ self . anomalyThreshold = value 
~~ self . _classificationMaxDist = value 
~~~ self . _activeColumnCount = value 
~~~ return PyRegion . setParameter ( self , name , index , value ) 
~~ ~~ def compute ( self , inputs , outputs ) : 
record = self . _constructClassificationRecord ( inputs ) 
if record . ROWID >= self . getParameter ( 'trainRecords' ) : 
~~~ self . _classifyState ( record ) 
~~ self . _recordsCache . append ( record ) 
while len ( self . _recordsCache ) > self . cacheSize : 
~~~ self . _recordsCache . pop ( 0 ) 
~~ self . labelResults = record . anomalyLabel 
self . _iteration += 1 
~~ def _classifyState ( self , state ) : 
if state . ROWID < self . getParameter ( 'trainRecords' ) : 
~~~ if not state . setByUser : 
~~~ state . anomalyLabel = [ ] 
self . _deleteRecordsFromKNN ( [ state ] ) 
~~ label = KNNAnomalyClassifierRegion . AUTO_THRESHOLD_CLASSIFIED_LABEL 
autoLabel = label + KNNAnomalyClassifierRegion . AUTO_TAG 
newCategory = self . _recomputeRecordFromKNN ( state ) 
labelList = self . _categoryToLabelList ( newCategory ) 
if state . setByUser : 
~~~ if label in state . anomalyLabel : 
~~~ state . anomalyLabel . remove ( label ) 
~~ if autoLabel in state . anomalyLabel : 
~~~ state . anomalyLabel . remove ( autoLabel ) 
~~ labelList . extend ( state . anomalyLabel ) 
~~ if state . anomalyScore >= self . getParameter ( 'anomalyThreshold' ) : 
~~~ labelList . append ( label ) 
~~ elif label in labelList : 
~~~ ind = labelList . index ( label ) 
labelList [ ind ] = autoLabel 
~~ labelList = list ( set ( labelList ) ) 
if label in labelList and autoLabel in labelList : 
~~~ labelList . remove ( autoLabel ) 
~~ if state . anomalyLabel == labelList : 
~~ state . anomalyLabel = labelList 
if state . anomalyLabel == [ ] : 
~~~ self . _deleteRecordsFromKNN ( [ state ] ) 
~~~ self . _addRecordToKNN ( state ) 
~~ ~~ def _constructClassificationRecord ( self , inputs ) : 
allSPColumns = inputs [ "spBottomUpOut" ] 
activeSPColumns = allSPColumns . nonzero ( ) [ 0 ] 
score = anomaly . computeRawAnomalyScore ( activeSPColumns , 
self . _prevPredictedColumns ) 
spSize = len ( allSPColumns ) 
allTPCells = inputs [ 'tpTopDownOut' ] 
tpSize = len ( inputs [ 'tpLrnActiveStateT' ] ) 
classificationVector = numpy . array ( [ ] ) 
if self . classificationVectorType == 1 : 
~~~ classificationVector = numpy . zeros ( tpSize ) 
activeCellMatrix = inputs [ "tpLrnActiveStateT" ] . reshape ( tpSize , 1 ) 
activeCellIdx = numpy . where ( activeCellMatrix > 0 ) [ 0 ] 
if activeCellIdx . shape [ 0 ] > 0 : 
~~~ classificationVector [ numpy . array ( activeCellIdx , dtype = numpy . uint16 ) ] = 1 
~~ ~~ elif self . classificationVectorType == 2 : 
~~~ classificationVector = numpy . zeros ( spSize + spSize ) 
if activeSPColumns . shape [ 0 ] > 0 : 
~~~ classificationVector [ activeSPColumns ] = 1.0 
~~ errorColumns = numpy . setdiff1d ( self . _prevPredictedColumns , 
activeSPColumns ) 
if errorColumns . shape [ 0 ] > 0 : 
~~~ errorColumnIndexes = ( numpy . array ( errorColumns , dtype = numpy . uint16 ) + 
spSize ) 
classificationVector [ errorColumnIndexes ] = 1.0 
~~ numPredictedCols = len ( self . _prevPredictedColumns ) 
predictedColumns = allTPCells . nonzero ( ) [ 0 ] 
self . _prevPredictedColumns = copy . deepcopy ( predictedColumns ) 
if self . _anomalyVectorLength is None : 
~~~ self . _anomalyVectorLength = len ( classificationVector ) 
~~ result = _CLAClassificationRecord ( 
anomalyScore = score , 
anomalyVector = classificationVector . nonzero ( ) [ 0 ] . tolist ( ) , 
anomalyLabel = [ ] 
~~ def _addRecordToKNN ( self , record ) : 
knn = self . _knnclassifier . _knn 
prototype_idx = self . _knnclassifier . getParameter ( 'categoryRecencyList' ) 
category = self . _labelListToCategoryNumber ( record . anomalyLabel ) 
if record . ROWID in prototype_idx : 
~~~ knn . prototypeSetCategory ( record . ROWID , category ) 
~~ pattern = self . _getStateAnomalyVector ( record ) 
rowID = record . ROWID 
knn . learn ( pattern , category , rowID = rowID ) 
~~ def _deleteRecordsFromKNN ( self , recordsToDelete ) : 
idsToDelete = ( [ r . ROWID for r in recordsToDelete if 
not r . setByUser and r . ROWID in prototype_idx ] ) 
nProtos = self . _knnclassifier . _knn . _numPatterns 
self . _knnclassifier . _knn . removeIds ( idsToDelete ) 
assert self . _knnclassifier . _knn . _numPatterns == nProtos - len ( idsToDelete ) 
~~ def _deleteRangeFromKNN ( self , start = 0 , end = None ) : 
prototype_idx = numpy . array ( 
self . _knnclassifier . getParameter ( 'categoryRecencyList' ) ) 
if end is None : 
~~~ end = prototype_idx . max ( ) + 1 
~~ idsIdxToDelete = numpy . logical_and ( prototype_idx >= start , 
prototype_idx < end ) 
idsToDelete = prototype_idx [ idsIdxToDelete ] 
self . _knnclassifier . _knn . removeIds ( idsToDelete . tolist ( ) ) 
~~ def _recomputeRecordFromKNN ( self , record ) : 
inputs = { 
"categoryIn" : [ None ] , 
"bottomUpIn" : self . _getStateAnomalyVector ( record ) , 
outputs = { "categoriesOut" : numpy . zeros ( ( 1 , ) ) , 
"bestPrototypeIndices" : numpy . zeros ( ( 1 , ) ) , 
"categoryProbabilitiesOut" : numpy . zeros ( ( 1 , ) ) } 
classifier_indexes = numpy . array ( 
valid_idx = numpy . where ( 
( classifier_indexes >= self . getParameter ( 'trainRecords' ) ) & 
( classifier_indexes < record . ROWID ) 
) [ 0 ] . tolist ( ) 
if len ( valid_idx ) == 0 : 
~~ self . _knnclassifier . setParameter ( 'inferenceMode' , None , True ) 
self . _knnclassifier . setParameter ( 'learningMode' , None , False ) 
self . _knnclassifier . compute ( inputs , outputs ) 
self . _knnclassifier . setParameter ( 'learningMode' , None , True ) 
classifier_distances = self . _knnclassifier . getLatestDistances ( ) 
valid_distances = classifier_distances [ valid_idx ] 
if valid_distances . min ( ) <= self . _classificationMaxDist : 
~~~ classifier_indexes_prev = classifier_indexes [ valid_idx ] 
rowID = classifier_indexes_prev [ valid_distances . argmin ( ) ] 
indexID = numpy . where ( classifier_indexes == rowID ) [ 0 ] [ 0 ] 
category = self . _knnclassifier . getCategoryList ( ) [ indexID ] 
return category 
~~ def _labelToCategoryNumber ( self , label ) : 
if label not in self . saved_categories : 
~~~ self . saved_categories . append ( label ) 
~~ return pow ( 2 , self . saved_categories . index ( label ) ) 
~~ def _labelListToCategoryNumber ( self , labelList ) : 
categoryNumber = 0 
for label in labelList : 
~~~ categoryNumber += self . _labelToCategoryNumber ( label ) 
~~ return categoryNumber 
~~ def _categoryToLabelList ( self , category ) : 
if category is None : 
~~ labelList = [ ] 
labelNum = 0 
while category > 0 : 
~~~ if category % 2 == 1 : 
~~~ labelList . append ( self . saved_categories [ labelNum ] ) 
~~ labelNum += 1 
category = category >> 1 
~~ return labelList 
~~ def _getStateAnomalyVector ( self , state ) : 
vector = numpy . zeros ( self . _anomalyVectorLength ) 
vector [ state . anomalyVector ] = 1 
return vector 
~~ def getLabels ( self , start = None , end = None ) : 
if len ( self . _recordsCache ) == 0 : 
'isProcessing' : False , 
'recordLabels' : [ ] 
~~~ start = int ( start ) 
~~~ start = 0 
~~~ end = int ( end ) 
~~~ end = self . _recordsCache [ - 1 ] . ROWID 
~~ if end <= start : 
debugInfo = { 
'requestRange' : { 
'startRecordID' : start , 
'endRecordID' : end 
'numRecordsStored' : len ( self . _recordsCache ) 
~~ results = { 
ROWIDX = numpy . array ( 
validIdx = numpy . where ( ( ROWIDX >= start ) & ( ROWIDX < end ) ) [ 0 ] . tolist ( ) 
categories = self . _knnclassifier . getCategoryList ( ) 
for idx in validIdx : 
~~~ row = dict ( 
ROWID = int ( ROWIDX [ idx ] ) , 
labels = self . _categoryToLabelList ( categories [ idx ] ) ) 
results [ 'recordLabels' ] . append ( row ) 
~~ return results 
~~ def addLabel ( self , start , end , labelName ) : 
~~~ end = int ( self . _recordsCache [ - 1 ] . ROWID ) 
~~ startID = self . _recordsCache [ 0 ] . ROWID 
clippedStart = max ( 0 , start - startID ) 
clippedEnd = max ( 0 , min ( len ( self . _recordsCache ) , end - startID ) ) 
if clippedEnd <= clippedStart : 
'clippedRequestRange' : { 
'startRecordID' : clippedStart , 
'endRecordID' : clippedEnd 
'validRange' : { 
'startRecordID' : startID , 
'endRecordID' : self . _recordsCache [ len ( self . _recordsCache ) - 1 ] . ROWID 
~~ for state in self . _recordsCache [ clippedStart : clippedEnd ] : 
~~~ if labelName not in state . anomalyLabel : 
~~~ state . anomalyLabel . append ( labelName ) 
state . setByUser = True 
self . _addRecordToKNN ( state ) 
~~ ~~ assert len ( self . saved_categories ) > 0 
for state in self . _recordsCache [ clippedEnd : ] : 
~~~ self . _classifyState ( state ) 
~~ ~~ def removeLabels ( self , start = None , end = None , labelFilter = None ) : 
"\ ) 
clippedStart = 0 if start is None else max ( 0 , start - startID ) 
clippedEnd = len ( self . _recordsCache ) if end is None else max ( 0 , min ( len ( self . _recordsCache ) , end - startID ) ) 
"\ , debugInfo = { 
~~ recordsToDelete = [ ] 
for state in self . _recordsCache [ clippedStart : clippedEnd ] : 
~~~ if labelFilter is not None : 
~~~ if labelFilter in state . anomalyLabel : 
~~~ state . anomalyLabel . remove ( labelFilter ) 
~~ state . setByUser = False 
recordsToDelete . append ( state ) 
~~ self . _deleteRecordsFromKNN ( recordsToDelete ) 
self . _deleteRangeFromKNN ( start , end ) 
~~ ~~ def match ( self , record ) : 
for field , meta in self . filterDict . iteritems ( ) : 
~~~ index = meta [ 'index' ] 
categories = meta [ 'categories' ] 
for category in categories : 
~~~ if not record : 
~~ if record [ index ] . find ( category ) != - 1 : 
~~ ~~ ~~ return False 
~~ def replace ( self , columnIndex , bitmap ) : 
return super ( _SparseMatrixCorticalColumnAdapter , self ) . replaceSparseRow ( 
columnIndex , bitmap 
~~ def update ( self , columnIndex , vector ) : 
return super ( _SparseMatrixCorticalColumnAdapter , self ) . setRowFromDense ( 
columnIndex , vector 
~~ def setLocalAreaDensity ( self , localAreaDensity ) : 
assert ( localAreaDensity > 0 and localAreaDensity <= 1 ) 
self . _localAreaDensity = localAreaDensity 
self . _numActiveColumnsPerInhArea = 0 
~~ def getPotential ( self , columnIndex , potential ) : 
assert ( columnIndex < self . _numColumns ) 
potential [ : ] = self . _potentialPools [ columnIndex ] 
~~ def setPotential ( self , columnIndex , potential ) : 
potentialSparse = numpy . where ( potential > 0 ) [ 0 ] 
if len ( potentialSparse ) < self . _stimulusThreshold : 
~~ self . _potentialPools . replace ( columnIndex , potentialSparse ) 
~~ def getPermanence ( self , columnIndex , permanence ) : 
permanence [ : ] = self . _permanences [ columnIndex ] 
~~ def setPermanence ( self , columnIndex , permanence ) : 
self . _updatePermanencesForColumn ( permanence , columnIndex , raisePerm = False ) 
~~ def getConnectedSynapses ( self , columnIndex , connectedSynapses ) : 
connectedSynapses [ : ] = self . _connectedSynapses [ columnIndex ] 
~~ def stripUnlearnedColumns ( self , activeArray ) : 
neverLearned = numpy . where ( self . _activeDutyCycles == 0 ) [ 0 ] 
activeArray [ neverLearned ] = 0 
~~ def _updateMinDutyCycles ( self ) : 
if self . _globalInhibition or self . _inhibitionRadius > self . _numInputs : 
~~~ self . _updateMinDutyCyclesGlobal ( ) 
~~~ self . _updateMinDutyCyclesLocal ( ) 
~~ ~~ def _updateMinDutyCyclesGlobal ( self ) : 
self . _minOverlapDutyCycles . fill ( 
self . _minPctOverlapDutyCycles * self . _overlapDutyCycles . max ( ) 
~~ def _updateMinDutyCyclesLocal ( self ) : 
for column in xrange ( self . _numColumns ) : 
~~~ neighborhood = self . _getColumnNeighborhood ( column ) 
maxActiveDuty = self . _activeDutyCycles [ neighborhood ] . max ( ) 
maxOverlapDuty = self . _overlapDutyCycles [ neighborhood ] . max ( ) 
self . _minOverlapDutyCycles [ column ] = ( maxOverlapDuty * 
self . _minPctOverlapDutyCycles ) 
~~ ~~ def _updateDutyCycles ( self , overlaps , activeColumns ) : 
overlapArray = numpy . zeros ( self . _numColumns , dtype = realDType ) 
activeArray = numpy . zeros ( self . _numColumns , dtype = realDType ) 
overlapArray [ overlaps > 0 ] = 1 
activeArray [ activeColumns ] = 1 
period = self . _dutyCyclePeriod 
if ( period > self . _iterationNum ) : 
~~~ period = self . _iterationNum 
~~ self . _overlapDutyCycles = self . _updateDutyCyclesHelper ( 
self . _overlapDutyCycles , 
overlapArray , 
period 
self . _activeDutyCycles = self . _updateDutyCyclesHelper ( 
self . _activeDutyCycles , 
activeArray , 
~~ def _updateInhibitionRadius ( self ) : 
if self . _globalInhibition : 
~~~ self . _inhibitionRadius = int ( self . _columnDimensions . max ( ) ) 
~~ avgConnectedSpan = numpy . average ( 
[ self . _avgConnectedSpanForColumnND ( i ) 
for i in xrange ( self . _numColumns ) ] 
columnsPerInput = self . _avgColumnsPerInput ( ) 
diameter = avgConnectedSpan * columnsPerInput 
radius = ( diameter - 1 ) / 2.0 
radius = max ( 1.0 , radius ) 
self . _inhibitionRadius = int ( radius + 0.5 ) 
~~ def _avgColumnsPerInput ( self ) : 
numDim = max ( self . _columnDimensions . size , self . _inputDimensions . size ) 
colDim = numpy . ones ( numDim ) 
colDim [ : self . _columnDimensions . size ] = self . _columnDimensions 
inputDim = numpy . ones ( numDim ) 
inputDim [ : self . _inputDimensions . size ] = self . _inputDimensions 
columnsPerInput = colDim . astype ( realDType ) / inputDim 
return numpy . average ( columnsPerInput ) 
~~ def _avgConnectedSpanForColumn1D ( self , columnIndex ) : 
assert ( self . _inputDimensions . size == 1 ) 
connected = self . _connectedSynapses [ columnIndex ] . nonzero ( ) [ 0 ] 
if connected . size == 0 : 
~~~ return max ( connected ) - min ( connected ) + 1 
~~ ~~ def _avgConnectedSpanForColumn2D ( self , columnIndex ) : 
assert ( self . _inputDimensions . size == 2 ) 
connected = self . _connectedSynapses [ columnIndex ] 
( rows , cols ) = connected . reshape ( self . _inputDimensions ) . nonzero ( ) 
if rows . size == 0 and cols . size == 0 : 
~~ rowSpan = rows . max ( ) - rows . min ( ) + 1 
colSpan = cols . max ( ) - cols . min ( ) + 1 
return numpy . average ( [ rowSpan , colSpan ] ) 
~~ def _bumpUpWeakColumns ( self ) : 
weakColumns = numpy . where ( self . _overlapDutyCycles 
< self . _minOverlapDutyCycles ) [ 0 ] 
for columnIndex in weakColumns : 
~~~ perm = self . _permanences [ columnIndex ] . astype ( realDType ) 
maskPotential = numpy . where ( self . _potentialPools [ columnIndex ] > 0 ) [ 0 ] 
perm [ maskPotential ] += self . _synPermBelowStimulusInc 
self . _updatePermanencesForColumn ( perm , columnIndex , raisePerm = False ) 
~~ ~~ def _raisePermanenceToThreshold ( self , perm , mask ) : 
if len ( mask ) < self . _stimulusThreshold : 
~~ numpy . clip ( perm , self . _synPermMin , self . _synPermMax , out = perm ) 
~~~ numConnected = numpy . nonzero ( 
perm > self . _synPermConnected - PERMANENCE_EPSILON ) [ 0 ] . size 
if numConnected >= self . _stimulusThreshold : 
~~ perm [ mask ] += self . _synPermBelowStimulusInc 
~~ ~~ def _updatePermanencesForColumn ( self , perm , columnIndex , raisePerm = True ) : 
if raisePerm : 
~~~ self . _raisePermanenceToThreshold ( perm , maskPotential ) 
~~ perm [ perm < self . _synPermTrimThreshold ] = 0 
numpy . clip ( perm , self . _synPermMin , self . _synPermMax , out = perm ) 
newConnected = numpy . where ( perm >= 
self . _synPermConnected - PERMANENCE_EPSILON ) [ 0 ] 
self . _permanences . update ( columnIndex , perm ) 
self . _connectedSynapses . replace ( columnIndex , newConnected ) 
self . _connectedCounts [ columnIndex ] = newConnected . size 
~~ def _initPermConnected ( self ) : 
p = self . _synPermConnected + ( 
self . _synPermMax - self . _synPermConnected ) * self . _random . getReal64 ( ) 
p = int ( p * 100000 ) / 100000.0 
return p 
~~ def _initPermNonConnected ( self ) : 
p = self . _synPermConnected * self . _random . getReal64 ( ) 
~~ def _initPermanence ( self , potential , connectedPct ) : 
perm = numpy . zeros ( self . _numInputs , dtype = realDType ) 
for i in xrange ( self . _numInputs ) : 
~~~ if ( potential [ i ] < 1 ) : 
~~ if ( self . _random . getReal64 ( ) <= connectedPct ) : 
~~~ perm [ i ] = self . _initPermConnected ( ) 
~~~ perm [ i ] = self . _initPermNonConnected ( ) 
~~ ~~ perm [ perm < self . _synPermTrimThreshold ] = 0 
return perm 
~~ def _mapColumn ( self , index ) : 
columnCoords = numpy . unravel_index ( index , self . _columnDimensions ) 
columnCoords = numpy . array ( columnCoords , dtype = realDType ) 
ratios = columnCoords / self . _columnDimensions 
inputCoords = self . _inputDimensions * ratios 
inputCoords += 0.5 * self . _inputDimensions / self . _columnDimensions 
inputCoords = inputCoords . astype ( int ) 
inputIndex = numpy . ravel_multi_index ( inputCoords , self . _inputDimensions ) 
return inputIndex 
~~ def _mapPotential ( self , index ) : 
centerInput = self . _mapColumn ( index ) 
columnInputs = self . _getInputNeighborhood ( centerInput ) . astype ( uintType ) 
numPotential = int ( columnInputs . size * self . _potentialPct + 0.5 ) 
selectedInputs = numpy . empty ( numPotential , dtype = uintType ) 
self . _random . sample ( columnInputs , selectedInputs ) 
potential = numpy . zeros ( self . _numInputs , dtype = uintType ) 
potential [ selectedInputs ] = 1 
return potential 
~~ def _updateBoostFactorsGlobal ( self ) : 
if ( self . _localAreaDensity > 0 ) : 
~~~ targetDensity = self . _localAreaDensity 
~~~ inhibitionArea = ( ( 2 * self . _inhibitionRadius + 1 ) 
** self . _columnDimensions . size ) 
inhibitionArea = min ( self . _numColumns , inhibitionArea ) 
targetDensity = float ( self . _numActiveColumnsPerInhArea ) / inhibitionArea 
targetDensity = min ( targetDensity , 0.5 ) 
~~ self . _boostFactors = numpy . exp ( 
( targetDensity - self . _activeDutyCycles ) * self . _boostStrength ) 
~~ def _updateBoostFactorsLocal ( self ) : 
targetDensity = numpy . zeros ( self . _numColumns , dtype = realDType ) 
for i in xrange ( self . _numColumns ) : 
~~~ maskNeighbors = self . _getColumnNeighborhood ( i ) 
targetDensity [ i ] = numpy . mean ( self . _activeDutyCycles [ maskNeighbors ] ) 
~~ def _calculateOverlap ( self , inputVector ) : 
overlaps = numpy . zeros ( self . _numColumns , dtype = realDType ) 
self . _connectedSynapses . rightVecSumAtNZ_fast ( inputVector . astype ( realDType ) , 
overlaps ) 
return overlaps 
~~ def _inhibitColumns ( self , overlaps ) : 
~~~ density = self . _localAreaDensity 
density = float ( self . _numActiveColumnsPerInhArea ) / inhibitionArea 
density = min ( density , 0.5 ) 
~~ if self . _globalInhibition or self . _inhibitionRadius > max ( self . _columnDimensions ) : 
~~~ return self . _inhibitColumnsGlobal ( overlaps , density ) 
~~~ return self . _inhibitColumnsLocal ( overlaps , density ) 
~~ ~~ def _inhibitColumnsGlobal ( self , overlaps , density ) : 
numActive = int ( density * self . _numColumns ) 
sortedWinnerIndices = numpy . argsort ( overlaps , kind = 'mergesort' ) 
start = len ( sortedWinnerIndices ) - numActive 
while start < len ( sortedWinnerIndices ) : 
~~~ i = sortedWinnerIndices [ start ] 
if overlaps [ i ] >= self . _stimulusThreshold : 
~~~ start += 1 
~~ ~~ return sortedWinnerIndices [ start : ] [ : : - 1 ] 
~~ def _inhibitColumnsLocal ( self , overlaps , density ) : 
activeArray = numpy . zeros ( self . _numColumns , dtype = "bool" ) 
for column , overlap in enumerate ( overlaps ) : 
~~~ if overlap >= self . _stimulusThreshold : 
neighborhoodOverlaps = overlaps [ neighborhood ] 
numBigger = numpy . count_nonzero ( neighborhoodOverlaps > overlap ) 
ties = numpy . where ( neighborhoodOverlaps == overlap ) 
tiedNeighbors = neighborhood [ ties ] 
numTiesLost = numpy . count_nonzero ( activeArray [ tiedNeighbors ] ) 
numActive = int ( 0.5 + density * len ( neighborhood ) ) 
if numBigger + numTiesLost < numActive : 
~~~ activeArray [ column ] = True 
~~ ~~ ~~ return activeArray . nonzero ( ) [ 0 ] 
~~ def _getColumnNeighborhood ( self , centerColumn ) : 
if self . _wrapAround : 
~~~ return topology . wrappingNeighborhood ( centerColumn , 
self . _inhibitionRadius , 
self . _columnDimensions ) 
~~~ return topology . neighborhood ( centerColumn , 
~~ ~~ def _getInputNeighborhood ( self , centerInput ) : 
~~~ return topology . wrappingNeighborhood ( centerInput , 
self . _potentialRadius , 
self . _inputDimensions ) 
~~~ return topology . neighborhood ( centerInput , 
~~ ~~ def _seed ( self , seed = - 1 ) : 
if seed != - 1 : 
~~~ self . _random = NupicRandom ( seed ) 
~~~ self . _random = NupicRandom ( ) 
~~ ~~ def handleGetValue ( self , topContainer ) : 
value = self . __referenceDict if self . __referenceDict is not None else topContainer 
for key in self . __dictKeyChain : 
~~~ value = value [ key ] 
~~ def Array ( dtype , size = None , ref = False ) : 
def getArrayType ( self ) : 
return self . _dtype 
~~ if ref : 
~~~ assert size is None 
~~ index = basicTypes . index ( dtype ) 
if index == - 1 : 
~~ if size and size <= 0 : 
~~ suffix = 'ArrayRef' if ref else 'Array' 
arrayFactory = getattr ( engine_internal , dtype + suffix ) 
arrayFactory . getType = getArrayType 
if size : 
~~~ a = arrayFactory ( size ) 
~~~ a = arrayFactory ( ) 
~~ a . _dtype = basicTypes [ index ] 
~~ def getInputNames ( self ) : 
inputs = self . getSpec ( ) . inputs 
return [ inputs . getByIndex ( i ) [ 0 ] for i in xrange ( inputs . getCount ( ) ) ] 
~~ def getOutputNames ( self ) : 
outputs = self . getSpec ( ) . outputs 
return [ outputs . getByIndex ( i ) [ 0 ] for i in xrange ( outputs . getCount ( ) ) ] 
~~ def _getParameterMethods ( self , paramName ) : 
if paramName in self . _paramTypeCache : 
~~~ return self . _paramTypeCache [ paramName ] 
~~~ paramSpec = self . getSpec ( ) . parameters . getByName ( paramName ) 
~~~ return ( None , None ) 
~~ dataType = paramSpec . dataType 
dataTypeName = basicTypes [ dataType ] 
count = paramSpec . count 
if count == 1 : 
~~~ x = 'etParameter' + dataTypeName 
dataTypeName ) 
~~ info = ( s , g ) 
~~~ if dataTypeName == "Byte" : 
~~~ info = ( self . setParameterString , self . getParameterString ) 
~~~ helper = _ArrayParameterHelper ( self , dataType ) 
info = ( self . setParameterArray , helper . getParameterArray ) 
~~ ~~ self . _paramTypeCache [ paramName ] = info 
~~ def getParameter ( self , paramName ) : 
( setter , getter ) = self . _getParameterMethods ( paramName ) 
if getter is None : 
~~~ import exceptions 
raise exceptions . Exception ( 
% ( paramName , self . name , self . type ) ) 
~~ return getter ( paramName ) 
~~ def setParameter ( self , paramName , value ) : 
if setter is None : 
~~ setter ( paramName , value ) 
~~ def _getRegions ( self ) : 
def makeRegion ( name , r ) : 
r = Region ( r , self ) 
~~ regions = CollectionWrapper ( engine_internal . Network . getRegions ( self ) , makeRegion ) 
return regions 
~~ def getRegionsByType ( self , regionClass ) : 
regions = [ ] 
for region in self . regions . values ( ) : 
~~~ if type ( region . getSelf ( ) ) is regionClass : 
~~~ regions . append ( region ) 
~~ ~~ return regions 
description = SDRClassifierRegion . __doc__ , 
actValueIn = dict ( 
dataType = "Real32" , 
bucketIdxIn = dict ( 
dataType = "UInt64" , 
categoryIn = dict ( 
bottomUpIn = dict ( 
predictedActiveCells = dict ( 
categoriesOut = dict ( 
isDefaultOutput = False , 
actualValues = dict ( 
probabilities = dict ( 
learningMode = dict ( 
constraints = 'bool' , 
inferenceMode = dict ( 
maxCategoryCount = dict ( 
defaultValue = 2000 , 
steps = dict ( 
dataType = "Byte" , 
defaultValue = '0' , 
alpha = dict ( 
'learning' , 
defaultValue = 0.001 , 
implementation = dict ( 
accessMode = 'ReadWrite' , 
dataType = 'Byte' , 
verbosity = dict ( 
commands = dict ( ) 
~~ def initialize ( self ) : 
if self . _sdrClassifier is None : 
~~~ self . _sdrClassifier = SDRClassifierFactory . create ( 
steps = self . stepsList , 
alpha = self . alpha , 
verbosity = self . verbosity , 
implementation = self . implementation , 
if name == "learningMode" : 
~~~ self . learningMode = bool ( int ( value ) ) 
~~ elif name == "inferenceMode" : 
~~~ self . inferenceMode = bool ( int ( value ) ) 
~~ ~~ def writeToProto ( self , proto ) : 
proto . implementation = self . implementation 
proto . steps = self . steps 
proto . alpha = self . alpha 
proto . verbosity = self . verbosity 
proto . maxCategoryCount = self . maxCategoryCount 
proto . learningMode = self . learningMode 
proto . inferenceMode = self . inferenceMode 
proto . recordNum = self . recordNum 
self . _sdrClassifier . write ( proto . sdrClassifier ) 
~~ def readFromProto ( cls , proto ) : 
instance = cls ( ) 
instance . implementation = proto . implementation 
instance . steps = proto . steps 
instance . stepsList = [ int ( i ) for i in proto . steps . split ( "," ) ] 
instance . alpha = proto . alpha 
instance . verbosity = proto . verbosity 
instance . maxCategoryCount = proto . maxCategoryCount 
instance . _sdrClassifier = SDRClassifierFactory . read ( proto ) 
instance . learningMode = proto . learningMode 
instance . inferenceMode = proto . inferenceMode 
instance . recordNum = proto . recordNum 
~~ def compute ( self , inputs , outputs ) : 
self . _computeFlag = True 
patternNZ = inputs [ "bottomUpIn" ] . nonzero ( ) [ 0 ] 
if self . learningMode : 
~~~ categories = [ category for category in inputs [ "categoryIn" ] 
if category >= 0 ] 
if len ( categories ) > 0 : 
~~~ bucketIdxList = [ ] 
actValueList = [ ] 
~~~ bucketIdxList . append ( int ( category ) ) 
if "actValueIn" not in inputs : 
~~~ actValueList . append ( int ( category ) ) 
~~~ actValueList . append ( float ( inputs [ "actValueIn" ] ) ) 
~~ ~~ classificationIn = { "bucketIdx" : bucketIdxList , 
"actValue" : actValueList } 
~~~ if "bucketIdxIn" not in inputs : 
~~ if "actValueIn" not in inputs : 
~~ classificationIn = { "bucketIdx" : int ( inputs [ "bucketIdxIn" ] ) , 
"actValue" : float ( inputs [ "actValueIn" ] ) } 
~~~ classificationIn = { "actValue" : 0 , "bucketIdx" : 0 } 
~~ clResults = self . _sdrClassifier . compute ( recordNum = self . recordNum , 
patternNZ = patternNZ , 
classification = classificationIn , 
learn = self . learningMode , 
infer = self . inferenceMode ) 
if clResults is not None and len ( clResults ) > 0 : 
~~~ outputs [ 'actualValues' ] [ : len ( clResults [ "actualValues" ] ) ] = clResults [ "actualValues" ] 
for step in self . stepsList : 
~~~ stepIndex = self . stepsList . index ( step ) 
categoryOut = clResults [ "actualValues" ] [ clResults [ step ] . argmax ( ) ] 
outputs [ 'categoriesOut' ] [ stepIndex ] = categoryOut 
stepProbabilities = clResults [ step ] 
for categoryIndex in xrange ( self . maxCategoryCount ) : 
~~~ flatIndex = categoryIndex + stepIndex * self . maxCategoryCount 
if categoryIndex < len ( stepProbabilities ) : 
~~~ outputs [ 'probabilities' ] [ flatIndex ] = stepProbabilities [ categoryIndex ] 
~~~ outputs [ 'probabilities' ] [ flatIndex ] = 0.0 
~~ ~~ ~~ ~~ self . recordNum += 1 
~~ def customCompute ( self , recordNum , patternNZ , classification ) : 
if not hasattr ( self , "_computeFlag" ) : 
~~~ self . _computeFlag = False 
~~ if self . _computeFlag : 
~~~ warnings . simplefilter ( 'error' , DeprecationWarning ) 
DeprecationWarning ) 
~~ return self . _sdrClassifier . compute ( recordNum , 
patternNZ , 
classification , 
self . learningMode , 
self . inferenceMode ) 
~~ def getOutputElementCount ( self , outputName ) : 
if outputName == "categoriesOut" : 
~~~ return len ( self . stepsList ) 
~~ elif outputName == "probabilities" : 
~~~ return len ( self . stepsList ) * self . maxCategoryCount 
~~ elif outputName == "actualValues" : 
~~~ return self . maxCategoryCount 
~~ ~~ def run ( self ) : 
descriptionPyModule = helpers . loadExperimentDescriptionScriptFromDir ( 
self . _experimentDir ) 
expIface = helpers . getExperimentDescriptionInterfaceFromModule ( 
descriptionPyModule ) 
expIface . normalizeStreamSources ( ) 
modelDescription = expIface . getModelDescription ( ) 
self . _modelControl = expIface . getModelControl ( ) 
streamDef = self . _modelControl [ 'dataset' ] 
from nupic . data . stream_reader import StreamReader 
readTimeout = 0 
self . _inputSource = StreamReader ( streamDef , isBlocking = False , 
maxTimeout = readTimeout ) 
fieldStats = self . _getFieldStats ( ) 
self . _model = ModelFactory . create ( modelDescription ) 
self . _model . setFieldStatistics ( fieldStats ) 
self . _model . enableLearning ( ) 
self . _model . enableInference ( self . _modelControl . get ( "inferenceArgs" , None ) ) 
self . __metricMgr = MetricsManager ( self . _modelControl . get ( 'metrics' , None ) , 
self . _model . getFieldInfo ( ) , 
self . _model . getInferenceType ( ) ) 
self . __loggedMetricPatterns = self . _modelControl . get ( "loggedMetrics" , [ ] ) 
self . _optimizedMetricLabel = self . __getOptimizedMetricLabel ( ) 
self . _reportMetricLabels = matchPatterns ( self . _reportKeyPatterns , 
self . _getMetricLabels ( ) ) 
self . _periodic = self . _initPeriodicActivities ( ) 
numIters = self . _modelControl . get ( 'iterationCount' , - 1 ) 
learningOffAt = None 
iterationCountInferOnly = self . _modelControl . get ( 'iterationCountInferOnly' , 0 ) 
if iterationCountInferOnly == - 1 : 
~~~ self . _model . disableLearning ( ) 
~~ elif iterationCountInferOnly > 0 : 
learningOffAt = numIters - iterationCountInferOnly 
~~ self . __runTaskMainLoop ( numIters , learningOffAt = learningOffAt ) 
self . _finalize ( ) 
return ( self . _cmpReason , None ) 
~~ def __runTaskMainLoop ( self , numIters , learningOffAt = None ) : 
self . _model . resetSequenceStates ( ) 
self . _currentRecordIndex = - 1 
~~~ if self . _isKilled : 
~~ if self . _isCanceled : 
~~ if self . _isInterrupted . isSet ( ) : 
~~~ self . __setAsOrphaned ( ) 
~~ if self . _isMature : 
~~~ if not self . _isBestModel : 
~~~ self . _cmpReason = self . _jobsDAO . CMPL_REASON_STOPPED 
~~~ self . _cmpReason = self . _jobsDAO . CMPL_REASON_EOF 
~~ if learningOffAt is not None and self . _currentRecordIndex == learningOffAt : 
~~~ inputRecord = self . _inputSource . getNextRecordDict ( ) 
if self . _currentRecordIndex < 0 : 
~~~ self . _inputSource . setTimeout ( 10 ) 
~~ ~~ except Exception , e : 
~~~ raise utils . JobFailException ( ErrorCodes . streamReading , str ( e . args ) , 
traceback . format_exc ( ) ) 
~~ if inputRecord is None : 
~~ if inputRecord : 
~~~ self . _currentRecordIndex += 1 
result = self . _model . run ( inputRecord = inputRecord ) 
result . metrics = self . __metricMgr . update ( result ) 
if not result . metrics : 
~~~ result . metrics = self . __metricMgr . getMetrics ( ) 
~~ if InferenceElement . encodings in result . inferences : 
~~~ result . inferences . pop ( InferenceElement . encodings ) 
~~ result . sensorInput . dataEncodings = None 
self . _writePrediction ( result ) 
self . _periodic . tick ( ) 
if numIters >= 0 and self . _currentRecordIndex >= numIters - 1 : 
inputRecord ) 
~~ ~~ ~~ def _finalize ( self ) : 
self . _logger . info ( 
self . _modelID , self . _currentRecordIndex + 1 ) 
self . _updateModelDBResults ( ) 
if not self . _isKilled : 
~~~ self . __updateJobResults ( ) 
~~~ self . __deleteOutputCache ( self . _modelID ) 
~~ if self . _predictionLogger : 
~~~ self . _predictionLogger . close ( ) 
~~ if self . _inputSource : 
~~~ self . _inputSource . close ( ) 
~~ ~~ def __createModelCheckpoint ( self ) : 
if self . _model is None or self . _modelCheckpointGUID is None : 
~~ if self . _predictionLogger is None : 
~~~ self . _createPredictionLogger ( ) 
~~ predictions = StringIO . StringIO ( ) 
self . _predictionLogger . checkpoint ( 
checkpointSink = predictions , 
maxRows = int ( Configuration . get ( 'nupic.model.checkpoint.maxPredictionRows' ) ) ) 
self . _model . save ( os . path . join ( self . _experimentDir , str ( self . _modelCheckpointGUID ) ) ) 
self . _jobsDAO . modelSetFields ( modelID , 
{ 'modelCheckpointId' : str ( self . _modelCheckpointGUID ) } , 
ignoreUnchanged = True ) 
~~ def __deleteModelCheckpoint ( self , modelID ) : 
checkpointID = self . _jobsDAO . modelsGetFields ( modelID , [ 'modelCheckpointId' ] ) [ 0 ] 
if checkpointID is None : 
~~~ shutil . rmtree ( os . path . join ( self . _experimentDir , str ( self . _modelCheckpointGUID ) ) ) 
checkpointID ) 
~~ self . _jobsDAO . modelSetFields ( modelID , 
{ 'modelCheckpointId' : None } , 
~~ def _createPredictionLogger ( self ) : 
self . _predictionLogger = BasicPredictionLogger ( 
fields = self . _model . getFieldInfo ( ) , 
experimentDir = self . _experimentDir , 
label = "hypersearch-worker" , 
inferenceType = self . _model . getInferenceType ( ) ) 
if self . __loggedMetricPatterns : 
~~~ metricLabels = self . __metricMgr . getMetricLabels ( ) 
loggedMetrics = matchPatterns ( self . __loggedMetricPatterns , metricLabels ) 
self . _predictionLogger . setLoggedMetrics ( loggedMetrics ) 
~~ ~~ def __getOptimizedMetricLabel ( self ) : 
matchingKeys = matchPatterns ( [ self . _optimizeKeyPattern ] , 
if len ( matchingKeys ) == 0 : 
~~ elif len ( matchingKeys ) > 1 : 
~~ return matchingKeys [ 0 ] 
~~ def _getFieldStats ( self ) : 
fieldStats = dict ( ) 
fieldNames = self . _inputSource . getFieldNames ( ) 
for field in fieldNames : 
~~~ curStats = dict ( ) 
curStats [ 'min' ] = self . _inputSource . getFieldMin ( field ) 
curStats [ 'max' ] = self . _inputSource . getFieldMax ( field ) 
fieldStats [ field ] = curStats 
~~ return fieldStats 
~~ def _updateModelDBResults ( self ) : 
metrics = self . _getMetrics ( ) 
reportDict = dict ( [ ( k , metrics [ k ] ) for k in self . _reportMetricLabels ] ) 
optimizeDict = dict ( ) 
if self . _optimizeKeyPattern is not None : 
~~~ optimizeDict [ self . _optimizedMetricLabel ] = metrics [ self . _optimizedMetricLabel ] 
~~ results = json . dumps ( ( metrics , optimizeDict ) ) 
self . _jobsDAO . modelUpdateResults ( self . _modelID , results = results , 
metricValue = optimizeDict . values ( ) [ 0 ] , 
numRecords = ( self . _currentRecordIndex + 1 ) ) 
self . _logger . debug ( 
~~ def __updateJobResultsPeriodic ( self ) : 
if self . _isBestModelStored and not self . _isBestModel : 
~~~ jobResultsStr = self . _jobsDAO . jobGetFields ( self . _jobID , [ 'results' ] ) [ 0 ] 
if jobResultsStr is None : 
~~~ jobResults = { } 
~~~ self . _isBestModelStored = True 
if not self . _isBestModel : 
~~ jobResults = json . loads ( jobResultsStr ) 
~~ bestModel = jobResults . get ( 'bestModel' , None ) 
bestMetric = jobResults . get ( 'bestValue' , None ) 
isSaved = jobResults . get ( 'saved' , False ) 
if ( bestModel is not None ) and ( self . _modelID != bestModel ) : 
~~~ self . _isBestModel = False 
~~ self . __flushPredictionCache ( ) 
self . _jobsDAO . modelUpdateTimestamp ( self . _modelID ) 
jobResults [ 'bestModel' ] = self . _modelID 
jobResults [ 'bestValue' ] = metrics [ self . _optimizedMetricLabel ] 
jobResults [ 'metrics' ] = metrics 
jobResults [ 'saved' ] = False 
newResults = json . dumps ( jobResults ) 
isUpdated = self . _jobsDAO . jobSetFieldIfEqual ( self . _jobID , 
fieldName = 'results' , 
curValue = jobResultsStr , 
newValue = newResults ) 
if isUpdated or ( not isUpdated and newResults == jobResultsStr ) : 
~~~ self . _isBestModel = True 
~~ ~~ ~~ def __checkIfBestCompletedModel ( self ) : 
jobResultsStr = self . _jobsDAO . jobGetFields ( self . _jobID , [ 'results' ] ) [ 0 ] 
~~~ jobResults = json . loads ( jobResultsStr ) 
~~ isSaved = jobResults . get ( 'saved' , False ) 
currentMetric = self . _getMetrics ( ) [ self . _optimizedMetricLabel ] 
self . _isBestModel = ( not isSaved ) or ( currentMetric < bestMetric ) 
return self . _isBestModel , jobResults , jobResultsStr 
~~ def __updateJobResults ( self ) : 
isSaved = False 
~~~ self . _isBestModel , jobResults , jobResultsStr = self . __checkIfBestCompletedModel ( ) 
if self . _isBestModel : 
~~~ if not isSaved : 
~~~ self . __flushPredictionCache ( ) 
self . __createModelCheckpoint ( ) 
isSaved = True 
~~ prevBest = jobResults . get ( 'bestModel' , None ) 
prevWasSaved = jobResults . get ( 'saved' , False ) 
if prevBest == self . _modelID : 
~~~ assert not prevWasSaved 
~~ metrics = self . _getMetrics ( ) 
jobResults [ 'saved' ] = True 
newValue = json . dumps ( jobResults ) ) 
if isUpdated : 
~~~ if prevWasSaved : 
~~~ self . __deleteOutputCache ( prevBest ) 
self . __deleteModelCheckpoint ( prevBest ) 
self . __deleteModelCheckpoint ( self . _modelID ) 
~~ ~~ ~~ def _writePrediction ( self , result ) : 
self . __predictionCache . append ( result ) 
~~ ~~ def __flushPredictionCache ( self ) : 
if not self . __predictionCache : 
~~ startTime = time . time ( ) 
self . _predictionLogger . writeRecords ( self . __predictionCache , 
progressCB = self . __writeRecordsCallback ) 
len ( self . __predictionCache ) , time . time ( ) - startTime ) 
self . __predictionCache . clear ( ) 
~~ def __deleteOutputCache ( self , modelID ) : 
if modelID == self . _modelID and self . _predictionLogger is not None : 
del self . __predictionCache 
self . _predictionLogger = None 
self . __predictionCache = None 
~~ ~~ def _initPeriodicActivities ( self ) : 
updateModelDBResults = PeriodicActivityRequest ( repeating = True , 
period = 100 , 
cb = self . _updateModelDBResults ) 
updateJobResults = PeriodicActivityRequest ( repeating = True , 
cb = self . __updateJobResultsPeriodic ) 
checkCancelation = PeriodicActivityRequest ( repeating = True , 
period = 50 , 
cb = self . __checkCancelation ) 
checkMaturity = PeriodicActivityRequest ( repeating = True , 
period = 10 , 
cb = self . __checkMaturity ) 
updateJobResultsFirst = PeriodicActivityRequest ( repeating = False , 
period = 2 , 
periodicActivities = [ updateModelDBResults , 
updateJobResultsFirst , 
updateJobResults , 
checkCancelation ] 
if self . _isMaturityEnabled : 
~~~ periodicActivities . append ( checkMaturity ) 
~~ return PeriodicActivityMgr ( requestedActivities = periodicActivities ) 
~~ def __checkCancelation ( self ) : 
print >> sys . stderr , "reporter:counter:HypersearchWorker,numRecords,50" 
jobCancel = self . _jobsDAO . jobGetFields ( self . _jobID , [ 'cancel' ] ) [ 0 ] 
if jobCancel : 
~~~ self . _cmpReason = ClientJobsDAO . CMPL_REASON_KILLED 
self . _isCanceled = True 
self . _modelID , self . _jobID ) 
~~~ stopReason = self . _jobsDAO . modelsGetFields ( self . _modelID , [ 'engStop' ] ) [ 0 ] 
if stopReason is None : 
~~ elif stopReason == ClientJobsDAO . STOP_REASON_KILLED : 
self . _isKilled = True 
self . _modelID ) 
~~ elif stopReason == ClientJobsDAO . STOP_REASON_STOPPED : 
~~~ self . _cmpReason = ClientJobsDAO . CMPL_REASON_STOPPED 
~~ ~~ ~~ def __checkMaturity ( self ) : 
if self . _currentRecordIndex + 1 < self . _MIN_RECORDS_TO_BE_BEST : 
~~ metric = self . _getMetrics ( ) [ self . _optimizedMetricLabel ] 
self . _metricRegression . addPoint ( x = self . _currentRecordIndex , y = metric ) 
pctChange , absPctChange = self . _metricRegression . getPctChanges ( ) 
if pctChange is not None and absPctChange <= self . _MATURITY_MAX_CHANGE : 
~~~ self . _jobsDAO . modelSetFields ( self . _modelID , 
{ 'engMatured' : True } ) 
self . _cmpReason = ClientJobsDAO . CMPL_REASON_STOPPED 
self . _isMature = True 
self . _MATURITY_NUM_POINTS , 
self . _metricRegression . _window ) 
~~ ~~ def __setAsOrphaned ( self ) : 
cmplReason = ClientJobsDAO . CMPL_REASON_ORPHAN 
self . _jobsDAO . modelSetCompleted ( self . _modelID , cmplReason , cmplMessage ) 
~~ def readStateFromDB ( self ) : 
self . _priorStateJSON = self . _hsObj . _cjDAO . jobGetFields ( self . _hsObj . _jobID , 
[ 'engWorkerState' ] ) [ 0 ] 
if self . _priorStateJSON is None : 
~~~ swarms = dict ( ) 
if self . _hsObj . _fixedFields is not None : 
~~~ print self . _hsObj . _fixedFields 
encoderSet = [ ] 
for field in self . _hsObj . _fixedFields : 
~~~ if field == '_classifierInput' : 
~~ encoderName = self . getEncoderKeyFromName ( field ) 
encoderSet . append ( encoderName ) 
~~ encoderSet . sort ( ) 
swarms [ '.' . join ( encoderSet ) ] = { 
'status' : 'active' , 
'bestModelId' : None , 
'bestErrScore' : None , 
'sprintIdx' : 0 , 
~~ elif self . _hsObj . _searchType == HsSearchType . temporal : 
~~~ for encoderName in self . _hsObj . _encoderNames : 
~~~ swarms [ encoderName ] = { 
~~ ~~ elif self . _hsObj . _searchType == HsSearchType . classification : 
~~~ if encoderName == self . _hsObj . _predictedFieldEncoder : 
~~ swarms [ encoderName ] = { 
~~ ~~ elif self . _hsObj . _searchType == HsSearchType . legacyTemporal : 
~~~ swarms [ self . _hsObj . _predictedFieldEncoder ] = { 
~~ self . _state = dict ( 
lastUpdateTime = time . time ( ) , 
lastGoodSprint = None , 
searchOver = False , 
activeSwarms = swarms . keys ( ) , 
swarms = swarms , 
sprints = [ { 'status' : 'active' , 
'bestErrScore' : None } ] , 
blackListedEncoders = [ ] , 
self . _hsObj . _cjDAO . jobSetFieldIfEqual ( 
self . _hsObj . _jobID , 'engWorkerState' , json . dumps ( self . _state ) , None ) 
self . _priorStateJSON = self . _hsObj . _cjDAO . jobGetFields ( 
self . _hsObj . _jobID , [ 'engWorkerState' ] ) [ 0 ] 
assert ( self . _priorStateJSON is not None ) 
~~ self . _state = json . loads ( self . _priorStateJSON ) 
self . _dirty = False 
~~ def writeStateToDB ( self ) : 
if not self . _dirty : 
~~ self . _state [ 'lastUpdateTime' ] = time . time ( ) 
newStateJSON = json . dumps ( self . _state ) 
success = self . _hsObj . _cjDAO . jobSetFieldIfEqual ( self . _hsObj . _jobID , 
'engWorkerState' , str ( newStateJSON ) , str ( self . _priorStateJSON ) ) 
self . _priorStateJSON = newStateJSON 
self . _state = json . loads ( self . _priorStateJSON ) 
~~ return success 
~~ def getFieldContributions ( self ) : 
~~~ return dict ( ) , dict ( ) 
~~ predictedEncoderName = self . _hsObj . _predictedFieldEncoder 
fieldScores = [ ] 
for swarmId , info in self . _state [ 'swarms' ] . iteritems ( ) : 
~~~ encodersUsed = swarmId . split ( '.' ) 
if len ( encodersUsed ) != 1 : 
~~ field = self . getEncoderNameFromKey ( encodersUsed [ 0 ] ) 
bestScore = info [ 'bestErrScore' ] 
if bestScore is None : 
~~~ ( _modelId , bestScore ) = self . _hsObj . _resultsDB . bestModelIdAndErrScore ( swarmId ) 
~~ fieldScores . append ( ( bestScore , field ) ) 
~~ if self . _hsObj . _searchType == HsSearchType . legacyTemporal : 
~~~ assert ( len ( fieldScores ) == 1 ) 
( baseErrScore , baseField ) = fieldScores [ 0 ] 
if len ( encodersUsed ) != 2 : 
~~ fields = [ self . getEncoderNameFromKey ( name ) for name in encodersUsed ] 
fields . remove ( baseField ) 
fieldScores . append ( ( info [ 'bestErrScore' ] , fields [ 0 ] ) ) 
~~~ fieldScores . sort ( reverse = True ) 
if self . _hsObj . _maxBranching > 0 and len ( fieldScores ) > self . _hsObj . _maxBranching : 
~~~ baseErrScore = fieldScores [ - self . _hsObj . _maxBranching - 1 ] [ 0 ] 
~~~ baseErrScore = fieldScores [ 0 ] [ 0 ] 
~~ ~~ pctFieldContributionsDict = dict ( ) 
absFieldContributionsDict = dict ( ) 
if baseErrScore is not None : 
~~~ if abs ( baseErrScore ) < 0.00001 : 
~~~ baseErrScore = 0.00001 
~~ for ( errScore , field ) in fieldScores : 
~~~ if errScore is not None : 
~~~ pctBetter = ( baseErrScore - errScore ) * 100.0 / baseErrScore 
~~~ pctBetter = 0.0 
~~ pctFieldContributionsDict [ field ] = pctBetter 
absFieldContributionsDict [ field ] = baseErrScore - errScore 
return pctFieldContributionsDict , absFieldContributionsDict 
~~ def getAllSwarms ( self , sprintIdx ) : 
swarmIds = [ ] 
~~~ if info [ 'sprintIdx' ] == sprintIdx : 
~~~ swarmIds . append ( swarmId ) 
~~ ~~ return swarmIds 
~~ def getCompletedSwarms ( self ) : 
~~~ if info [ 'status' ] == 'completed' : 
~~ def getCompletingSwarms ( self ) : 
~~~ if info [ 'status' ] == 'completing' : 
~~ def bestModelInSprint ( self , sprintIdx ) : 
swarms = self . getAllSwarms ( sprintIdx ) 
bestModelId = None 
bestErrScore = numpy . inf 
for swarmId in swarms : 
~~~ ( modelId , errScore ) = self . _hsObj . _resultsDB . bestModelIdAndErrScore ( swarmId ) 
if errScore < bestErrScore : 
~~~ bestModelId = modelId 
bestErrScore = errScore 
~~ ~~ return ( bestModelId , bestErrScore ) 
~~ def setSwarmState ( self , swarmId , newStatus ) : 
assert ( newStatus in [ 'active' , 'completing' , 'completed' , 'killed' ] ) 
swarmInfo = self . _state [ 'swarms' ] [ swarmId ] 
if swarmInfo [ 'status' ] == newStatus : 
~~ if swarmInfo [ 'status' ] == 'completed' and newStatus == 'completing' : 
~~ self . _dirty = True 
swarmInfo [ 'status' ] = newStatus 
if newStatus == 'completed' : 
swarmInfo [ 'bestModelId' ] = modelId 
swarmInfo [ 'bestErrScore' ] = errScore 
~~ if newStatus != 'active' and swarmId in self . _state [ 'activeSwarms' ] : 
~~~ self . _state [ 'activeSwarms' ] . remove ( swarmId ) 
~~ if newStatus == 'killed' : 
~~~ self . _hsObj . killSwarmParticles ( swarmId ) 
~~ sprintIdx = swarmInfo [ 'sprintIdx' ] 
self . isSprintActive ( sprintIdx ) 
sprintInfo = self . _state [ 'sprints' ] [ sprintIdx ] 
statusCounts = dict ( active = 0 , completing = 0 , completed = 0 , killed = 0 ) 
bestModelIds = [ ] 
bestErrScores = [ ] 
for info in self . _state [ 'swarms' ] . itervalues ( ) : 
~~~ if info [ 'sprintIdx' ] != sprintIdx : 
~~ statusCounts [ info [ 'status' ] ] += 1 
if info [ 'status' ] == 'completed' : 
~~~ bestModelIds . append ( info [ 'bestModelId' ] ) 
bestErrScores . append ( info [ 'bestErrScore' ] ) 
~~ ~~ if statusCounts [ 'active' ] > 0 : 
~~~ sprintStatus = 'active' 
~~ elif statusCounts [ 'completing' ] > 0 : 
~~~ sprintStatus = 'completing' 
~~~ sprintStatus = 'completed' 
~~ sprintInfo [ 'status' ] = sprintStatus 
if sprintStatus == 'completed' : 
~~~ if len ( bestErrScores ) > 0 : 
~~~ whichIdx = numpy . array ( bestErrScores ) . argmin ( ) 
sprintInfo [ 'bestModelId' ] = bestModelIds [ whichIdx ] 
sprintInfo [ 'bestErrScore' ] = bestErrScores [ whichIdx ] 
~~~ sprintInfo [ 'bestModelId' ] = 0 
sprintInfo [ 'bestErrScore' ] = numpy . inf 
~~ bestPrior = numpy . inf 
for idx in range ( sprintIdx ) : 
~~~ if self . _state [ 'sprints' ] [ idx ] [ 'status' ] == 'completed' : 
~~~ ( _ , errScore ) = self . bestModelInCompletedSprint ( idx ) 
if errScore is None : 
~~~ errScore = numpy . inf 
~~ if errScore < bestPrior : 
~~~ bestPrior = errScore 
~~ ~~ if sprintInfo [ 'bestErrScore' ] >= bestPrior : 
~~~ self . _state [ 'lastGoodSprint' ] = sprintIdx - 1 
~~ if self . _state [ 'lastGoodSprint' ] is not None and not self . anyGoodSprintsActive ( ) : 
~~~ self . _state [ 'searchOver' ] = True 
~~ ~~ ~~ def anyGoodSprintsActive ( self ) : 
if self . _state [ 'lastGoodSprint' ] is not None : 
~~~ goodSprints = self . _state [ 'sprints' ] [ 0 : self . _state [ 'lastGoodSprint' ] + 1 ] 
~~~ goodSprints = self . _state [ 'sprints' ] 
~~ for sprint in goodSprints : 
~~~ if sprint [ 'status' ] == 'active' : 
~~~ anyActiveSprints = True 
~~~ anyActiveSprints = False 
~~ return anyActiveSprints 
~~ def isSprintCompleted ( self , sprintIdx ) : 
numExistingSprints = len ( self . _state [ 'sprints' ] ) 
if sprintIdx >= numExistingSprints : 
~~ return ( self . _state [ 'sprints' ] [ sprintIdx ] [ 'status' ] == 'completed' ) 
~~ def killUselessSwarms ( self ) : 
if self . _hsObj . _searchType == HsSearchType . legacyTemporal : 
~~~ if numExistingSprints <= 2 : 
~~~ if numExistingSprints <= 1 : 
~~ ~~ completedSwarms = self . getCompletedSwarms ( ) 
completedSwarms = [ ( swarm , self . _state [ "swarms" ] [ swarm ] , 
self . _state [ "swarms" ] [ swarm ] [ "bestErrScore" ] ) for swarm in completedSwarms ] 
completedMatrix = [ [ ] for i in range ( numExistingSprints ) ] 
for swarm in completedSwarms : 
~~~ completedMatrix [ swarm [ 1 ] [ "sprintIdx" ] ] . append ( swarm ) 
~~ for sprint in completedMatrix : 
~~~ sprint . sort ( key = itemgetter ( 2 ) ) 
~~ activeSwarms = self . getActiveSwarms ( ) 
activeSwarms . extend ( self . getCompletingSwarms ( ) ) 
activeSwarms = [ ( swarm , self . _state [ "swarms" ] [ swarm ] , 
self . _state [ "swarms" ] [ swarm ] [ "bestErrScore" ] ) for swarm in activeSwarms ] 
activeMatrix = [ [ ] for i in range ( numExistingSprints ) ] 
for swarm in activeSwarms : 
~~~ activeMatrix [ swarm [ 1 ] [ "sprintIdx" ] ] . append ( swarm ) 
~~ for sprint in activeMatrix : 
~~ toKill = [ ] 
for i in range ( 1 , numExistingSprints ) : 
~~~ for swarm in activeMatrix [ i ] : 
~~~ curSwarmEncoders = swarm [ 0 ] . split ( "." ) 
if ( len ( activeMatrix [ i - 1 ] ) == 0 ) : 
~~~ if i == 2 and ( self . _hsObj . _tryAll3FieldCombinations or self . _hsObj . _tryAll3FieldCombinationsWTimestamps ) : 
~~~ bestInPrevious = completedMatrix [ i - 1 ] [ 0 ] 
bestEncoders = bestInPrevious [ 0 ] . split ( '.' ) 
for encoder in bestEncoders : 
~~~ if not encoder in curSwarmEncoders : 
~~~ toKill . append ( swarm ) 
#elif(len(completedMatrix[i-1])>1): 
~~ ~~ ~~ ~~ ~~ ~~ if len ( toKill ) > 0 : 
~~ for swarm in toKill : 
~~~ self . setSwarmState ( swarm [ 0 ] , "killed" ) 
~~ def isSprintActive ( self , sprintIdx ) : 
~~~ numExistingSprints = len ( self . _state [ 'sprints' ] ) 
if sprintIdx <= numExistingSprints - 1 : 
~~~ if not self . _hsObj . _speculativeParticles : 
~~~ active = ( self . _state [ 'sprints' ] [ sprintIdx ] [ 'status' ] == 'active' ) 
return ( active , False ) 
if not active : 
~~~ return ( active , False ) 
~~ activeSwarmIds = self . getActiveSwarms ( sprintIdx ) 
swarmSizes = [ self . _hsObj . _resultsDB . getParticleInfos ( swarmId , 
matured = False ) [ 0 ] for swarmId in activeSwarmIds ] 
notFullSwarms = [ len ( swarm ) for swarm in swarmSizes if len ( swarm ) < self . _hsObj . _minParticlesPerSwarm ] 
if len ( notFullSwarms ) > 0 : 
~~~ return ( True , False ) 
~~ ~~ ~~ if self . _state [ 'lastGoodSprint' ] is not None : 
~~~ return ( False , True ) 
~~ if self . _hsObj . _fixedFields is not None : 
~~ if sprintIdx > 0 and self . _state [ 'sprints' ] [ sprintIdx - 1 ] [ 'status' ] == 'completed' : 
~~~ ( bestModelId , _ ) = self . bestModelInCompletedSprint ( sprintIdx - 1 ) 
( particleState , _ , _ , _ , _ ) = self . _hsObj . _resultsDB . getParticleInfo ( 
bestModelId ) 
bestSwarmId = particleState [ 'swarmId' ] 
baseEncoderSets = [ bestSwarmId . split ( '.' ) ] 
~~~ bestSwarmId = None 
particleState = None 
baseEncoderSets = [ ] 
for swarmId in self . getNonKilledSwarms ( sprintIdx - 1 ) : 
~~~ baseEncoderSets . append ( swarmId . split ( '.' ) ) 
~~ ~~ encoderAddSet = [ ] 
limitFields = False 
if self . _hsObj . _maxBranching > 0 or self . _hsObj . _minFieldContribution >= 0 : 
~~~ if self . _hsObj . _searchType == HsSearchType . temporal or self . _hsObj . _searchType == HsSearchType . classification : 
~~~ if sprintIdx >= 1 : 
~~~ limitFields = True 
baseSprintIdx = 0 
~~~ if sprintIdx >= 2 : 
baseSprintIdx = 1 
~~ ~~ if limitFields : 
~~~ pctFieldContributions , absFieldContributions = self . getFieldContributions ( ) 
toRemove = [ ] 
for fieldname in pctFieldContributions : 
~~~ if pctFieldContributions [ fieldname ] < self . _hsObj . _minFieldContribution : 
toRemove . append ( self . getEncoderKeyFromName ( fieldname ) ) 
~~ ~~ swarms = self . _state [ "swarms" ] 
sprintSwarms = [ ( swarm , swarms [ swarm ] [ "bestErrScore" ] ) for swarm in swarms if swarms [ swarm ] [ "sprintIdx" ] == baseSprintIdx ] 
sprintSwarms = sorted ( sprintSwarms , key = itemgetter ( 1 ) ) 
if self . _hsObj . _maxBranching > 0 : 
~~~ sprintSwarms = sprintSwarms [ 0 : self . _hsObj . _maxBranching ] 
~~ for swarm in sprintSwarms : 
~~~ swarmEncoders = swarm [ 0 ] . split ( "." ) 
for encoder in swarmEncoders : 
~~~ if not encoder in encoderAddSet : 
~~~ encoderAddSet . append ( encoder ) 
~~ ~~ ~~ encoderAddSet = [ encoder for encoder in encoderAddSet if not str ( encoder ) in toRemove ] 
~~~ encoderAddSet = self . _hsObj . _encoderNames 
~~ newSwarmIds = set ( ) 
if ( self . _hsObj . _searchType == HsSearchType . temporal or self . _hsObj . _searchType == HsSearchType . legacyTemporal ) and sprintIdx == 2 and ( self . _hsObj . _tryAll3FieldCombinations or self . _hsObj . _tryAll3FieldCombinationsWTimestamps ) : 
~~~ if self . _hsObj . _tryAll3FieldCombinations : 
~~~ newEncoders = set ( self . _hsObj . _encoderNames ) 
if self . _hsObj . _predictedFieldEncoder in newEncoders : 
~~~ newEncoders . remove ( self . _hsObj . _predictedFieldEncoder ) 
~~~ newEncoders = set ( encoderAddSet ) 
~~ for encoder in self . _hsObj . _encoderNames : 
~~~ if encoder . endswith ( '_timeOfDay' ) or encoder . endswith ( '_weekend' ) or encoder . endswith ( '_dayOfWeek' ) : 
~~~ newEncoders . add ( encoder ) 
~~ ~~ ~~ allCombos = list ( itertools . combinations ( newEncoders , 2 ) ) 
for combo in allCombos : 
~~~ newSet = list ( combo ) 
newSet . append ( self . _hsObj . _predictedFieldEncoder ) 
newSet . sort ( ) 
newSwarmId = '.' . join ( newSet ) 
if newSwarmId not in self . _state [ 'swarms' ] : 
~~~ newSwarmIds . add ( newSwarmId ) 
if ( len ( self . getActiveSwarms ( sprintIdx - 1 ) ) > 0 ) : 
~~~ for baseEncoderSet in baseEncoderSets : 
~~~ for encoder in encoderAddSet : 
~~~ if encoder not in self . _state [ 'blackListedEncoders' ] and encoder not in baseEncoderSet : 
~~~ newSet = list ( baseEncoderSet ) 
newSet . append ( encoder ) 
~~ ~~ ~~ ~~ ~~ ~~ newSwarmIds = sorted ( newSwarmIds ) 
if len ( newSwarmIds ) == 0 : 
~~~ if len ( self . getAllSwarms ( sprintIdx ) ) > 0 : 
~~ ~~ self . _dirty = True 
if len ( self . _state [ "sprints" ] ) == sprintIdx : 
~~~ self . _state [ 'sprints' ] . append ( { 'status' : 'active' , 
'bestErrScore' : None } ) 
~~ for swarmId in newSwarmIds : 
~~~ self . _state [ 'swarms' ] [ swarmId ] = { 'status' : 'active' , 
'sprintIdx' : sprintIdx } 
~~ self . _state [ 'activeSwarms' ] = self . getActiveSwarms ( ) 
success = self . writeStateToDB ( ) 
~~ ~~ ~~ def addEncoder ( self , name , encoder ) : 
self . encoders . append ( ( name , encoder , self . width ) ) 
for d in encoder . getDescription ( ) : 
~~~ self . description . append ( ( d [ 0 ] , d [ 1 ] + self . width ) ) 
~~ self . width += encoder . getWidth ( ) 
~~ def invariant ( self ) : 
assert isinstance ( self . description , str ) 
assert isinstance ( self . singleNodeOnly , bool ) 
assert isinstance ( self . inputs , dict ) 
assert isinstance ( self . outputs , dict ) 
assert isinstance ( self . parameters , dict ) 
assert isinstance ( self . commands , dict ) 
hasDefaultInput = False 
for k , v in self . inputs . items ( ) : 
~~~ assert isinstance ( k , str ) 
assert isinstance ( v , InputSpec ) 
v . invariant ( ) 
if v . isDefaultInput : 
~~~ assert not hasDefaultInput 
hasDefaultInput = True 
~~ ~~ hasDefaultOutput = False 
for k , v in self . outputs . items ( ) : 
assert isinstance ( v , OutputSpec ) 
if v . isDefaultOutput : 
~~~ assert not hasDefaultOutput 
hasDefaultOutput = True 
~~ ~~ for k , v in self . parameters . items ( ) : 
assert isinstance ( v , ParameterSpec ) 
~~ for k , v in self . commands . items ( ) : 
assert isinstance ( v , CommandSpec ) 
~~ ~~ def toDict ( self ) : 
def items2dict ( items ) : 
d = { } 
for k , v in items . items ( ) : 
~~~ d [ k ] = v . __dict__ 
~~ return d 
~~ self . invariant ( ) 
return dict ( description = self . description , 
singleNodeOnly = self . singleNodeOnly , 
inputs = items2dict ( self . inputs ) , 
outputs = items2dict ( self . outputs ) , 
parameters = items2dict ( self . parameters ) , 
commands = items2dict ( self . commands ) ) 
~~ def updateResultsForJob ( self , forceUpdate = True ) : 
updateInterval = time . time ( ) - self . _lastUpdateAttemptTime 
if updateInterval < self . _MIN_UPDATE_INTERVAL and not forceUpdate : 
time . time ( ) , 
self . _lastUpdateAttemptTime ) ) 
timestampUpdated = self . _cjDB . jobUpdateSelectionSweep ( self . _jobID , 
self . _MIN_UPDATE_INTERVAL ) 
if not timestampUpdated : 
if not forceUpdate : 
~~ ~~ self . _lastUpdateAttemptTime = time . time ( ) 
minUpdateRecords = self . _MIN_UPDATE_THRESHOLD 
jobResults = self . _getJobResults ( ) 
if forceUpdate or jobResults is None : 
~~~ minUpdateRecords = 0 
~~ candidateIDs , bestMetric = self . _cjDB . modelsGetCandidates ( self . _jobID , minUpdateRecords ) 
if len ( candidateIDs ) == 0 : 
~~ self . _jobUpdateCandidate ( candidateIDs [ 0 ] , bestMetric , results = jobResults ) 
~~ def createEncoder ( ) : 
consumption_encoder = ScalarEncoder ( 21 , 0.0 , 100.0 , n = 50 , name = "consumption" , 
clipInput = True ) 
time_encoder = DateEncoder ( timeOfDay = ( 21 , 9.5 ) , name = "timestamp_timeOfDay" ) 
encoder = MultiEncoder ( ) 
encoder . addEncoder ( "consumption" , consumption_encoder ) 
encoder . addEncoder ( "timestamp" , time_encoder ) 
return encoder 
~~ def createNetwork ( dataSource ) : 
network = Network ( ) 
network . addRegion ( "sensor" , "py.RecordSensor" , 
json . dumps ( { "verbosity" : _VERBOSITY } ) ) 
sensor = network . regions [ "sensor" ] . getSelf ( ) 
sensor . encoder = createEncoder ( ) 
sensor . dataSource = dataSource 
SP_PARAMS [ "inputWidth" ] = sensor . encoder . getWidth ( ) 
network . addRegion ( "spatialPoolerRegion" , "py.SPRegion" , json . dumps ( SP_PARAMS ) ) 
network . link ( "sensor" , "spatialPoolerRegion" , "UniformLink" , "" ) 
network . link ( "sensor" , "spatialPoolerRegion" , "UniformLink" , "" , 
srcOutput = "resetOut" , destInput = "resetIn" ) 
network . link ( "spatialPoolerRegion" , "sensor" , "UniformLink" , "" , 
srcOutput = "spatialTopDownOut" , destInput = "spatialTopDownIn" ) 
srcOutput = "temporalTopDownOut" , destInput = "temporalTopDownIn" ) 
network . addRegion ( "temporalPoolerRegion" , "py.TMRegion" , 
json . dumps ( TM_PARAMS ) ) 
network . link ( "spatialPoolerRegion" , "temporalPoolerRegion" , "UniformLink" , "" ) 
network . link ( "temporalPoolerRegion" , "spatialPoolerRegion" , "UniformLink" , "" , 
srcOutput = "topDownOut" , destInput = "topDownIn" ) 
network . addRegion ( "anomalyLikelihoodRegion" , "py.AnomalyLikelihoodRegion" , 
json . dumps ( { } ) ) 
network . link ( "temporalPoolerRegion" , "anomalyLikelihoodRegion" , "UniformLink" , 
"" , srcOutput = "anomalyScore" , destInput = "rawAnomalyScore" ) 
network . link ( "sensor" , "anomalyLikelihoodRegion" , "UniformLink" , "" , 
srcOutput = "sourceOut" , destInput = "metricValue" ) 
spatialPoolerRegion = network . regions [ "spatialPoolerRegion" ] 
spatialPoolerRegion . setParameter ( "learningMode" , True ) 
spatialPoolerRegion . setParameter ( "anomalyMode" , False ) 
temporalPoolerRegion = network . regions [ "temporalPoolerRegion" ] 
temporalPoolerRegion . setParameter ( "topDownMode" , True ) 
temporalPoolerRegion . setParameter ( "learningMode" , True ) 
temporalPoolerRegion . setParameter ( "inferenceMode" , True ) 
temporalPoolerRegion . setParameter ( "anomalyMode" , True ) 
~~ def runNetwork ( network , writer ) : 
sensorRegion = network . regions [ "sensor" ] 
anomalyLikelihoodRegion = network . regions [ "anomalyLikelihoodRegion" ] 
prevPredictedColumns = [ ] 
for i in xrange ( _NUM_RECORDS ) : 
~~~ network . run ( 1 ) 
consumption = sensorRegion . getOutputData ( "sourceOut" ) [ 0 ] 
anomalyScore = temporalPoolerRegion . getOutputData ( "anomalyScore" ) [ 0 ] 
anomalyLikelihood = anomalyLikelihoodRegion . getOutputData ( "anomalyLikelihood" ) [ 0 ] 
writer . writerow ( ( i , consumption , anomalyScore , anomalyLikelihood ) ) 
~~ ~~ def __validateExperimentControl ( self , control ) : 
taskList = control . get ( 'tasks' , None ) 
if taskList is not None : 
~~~ taskLabelsList = [ ] 
for task in taskList : 
~~~ validateOpfJsonValue ( task , "opfTaskSchema.json" ) 
validateOpfJsonValue ( task [ 'taskControl' ] , "opfTaskControlSchema.json" ) 
taskLabel = task [ 'taskLabel' ] 
taskLabelsList . append ( taskLabel . lower ( ) ) 
~~ taskLabelDuplicates = filter ( lambda x : taskLabelsList . count ( x ) > 1 , 
taskLabelsList ) 
~~ def normalizeStreamSource ( self , stream ) : 
source = stream [ 'source' ] [ len ( FILE_SCHEME ) : ] 
if os . path . isabs ( source ) : 
~~~ sourcePath = source 
~~~ sourcePath = resource_filename ( "nupic.datafiles" , source ) 
if not os . path . exists ( sourcePath ) : 
~~~ sourcePath = os . path . join ( os . getcwd ( ) , source ) 
~~ ~~ stream [ 'source' ] = FILE_SCHEME + sourcePath 
~~ def normalizeStreamSources ( self ) : 
task = dict ( self . __control ) 
if 'dataset' in task : 
~~~ for stream in task [ 'dataset' ] [ 'streams' ] : 
~~~ self . normalizeStreamSource ( stream ) 
~~~ for subtask in task [ 'tasks' ] : 
~~~ for stream in subtask [ 'dataset' ] [ 'streams' ] : 
~~ ~~ ~~ ~~ def convertNupicEnvToOPF ( self ) : 
task . pop ( 'environment' ) 
inferenceArgs = task . pop ( 'inferenceArgs' ) 
task [ 'taskLabel' ] = 'DefaultTask' 
iterationCount = task . get ( 'iterationCount' , - 1 ) 
iterationCountInferOnly = task . pop ( 'iterationCountInferOnly' , 0 ) 
~~~ iterationCycle = [ IterationPhaseSpecInferOnly ( 1000 , inferenceArgs = inferenceArgs ) ] 
iterationCycle = [ IterationPhaseSpecLearnAndInfer ( iterationCount 
- iterationCountInferOnly , inferenceArgs = inferenceArgs ) , 
IterationPhaseSpecInferOnly ( iterationCountInferOnly , inferenceArgs = inferenceArgs ) ] 
~~~ iterationCycle = [ IterationPhaseSpecLearnAndInfer ( 1000 , inferenceArgs = inferenceArgs ) ] 
~~ taskControl = dict ( metrics = task . pop ( 'metrics' ) , 
loggedMetrics = task . pop ( 'loggedMetrics' ) , 
iterationCycle = iterationCycle ) 
task [ 'taskControl' ] = taskControl 
self . __control = dict ( environment = OpfEnvironment . Nupic , 
tasks = [ task ] ) 
sys . path . append ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) 
from custom_region . identity_region import IdentityRegion 
Network . registerRegion ( IdentityRegion ) 
network . addRegion ( "identityRegion" , "py.IdentityRegion" , 
json . dumps ( { 
"dataWidth" : sensor . encoder . getWidth ( ) , 
} ) ) 
network . link ( "sensor" , "identityRegion" , "UniformLink" , "" ) 
network . initialize ( ) 
identityRegion = network . regions [ "identityRegion" ] 
encoding = identityRegion . getOutputData ( "out" ) 
writer . writerow ( ( i , encoding ) ) 
~~ ~~ def _appendReportKeys ( keys , prefix , results ) : 
allKeys = results . keys ( ) 
allKeys . sort ( ) 
for key in allKeys : 
~~~ if hasattr ( results [ key ] , 'keys' ) : 
~~~ _appendReportKeys ( keys , "%s%s:" % ( prefix , key ) , results [ key ] ) 
~~~ keys . add ( "%s%s" % ( prefix , key ) ) 
~~ ~~ ~~ def _matchReportKeys ( reportKeyREs = [ ] , allReportKeys = [ ] ) : 
matchingReportKeys = [ ] 
for keyRE in reportKeyREs : 
~~~ matchObj = re . compile ( keyRE ) 
found = False 
for keyName in allReportKeys : 
~~~ match = matchObj . match ( keyName ) 
if match and match . end ( ) == len ( keyName ) : 
~~~ matchingReportKeys . append ( keyName ) 
found = True 
~~ ~~ if not found : 
~~~ raise _BadKeyError ( keyRE ) 
~~ ~~ return matchingReportKeys 
~~ def _getReportItem ( itemName , results ) : 
subKeys = itemName . split ( ':' ) 
subResults = results 
for subKey in subKeys : 
~~~ subResults = subResults [ subKey ] 
~~ return subResults 
~~ def filterResults ( allResults , reportKeys , optimizeKey = None ) : 
allReportKeys = set ( ) 
_appendReportKeys ( keys = allReportKeys , prefix = '' , results = allResults ) 
#---------------------------------------------------------------------------- 
matchingKeys = _matchReportKeys ( reportKeys , allReportKeys ) 
reportDict = dict ( ) 
for keyName in matchingKeys : 
~~~ value = _getReportItem ( keyName , allResults ) 
reportDict [ keyName ] = value 
~~ if optimizeKey is not None : 
~~~ matchingKeys = _matchReportKeys ( [ optimizeKey ] , allReportKeys ) 
~~~ raise _BadKeyError ( optimizeKey ) 
~~~ raise _BadOptimizeKeyError ( optimizeKey , matchingKeys ) 
~~ optimizeKeyFullName = matchingKeys [ 0 ] 
value = _getReportItem ( optimizeKeyFullName , allResults ) 
optimizeDict [ optimizeKeyFullName ] = value 
reportDict [ optimizeKeyFullName ] = value 
~~ return ( reportDict , optimizeDict ) 
~~ def _handleModelRunnerException ( jobID , modelID , jobsDAO , experimentDir , logger , 
e ) : 
msg = StringIO . StringIO ( ) 
modelID , e , type ( e ) ) 
traceback . print_exc ( None , msg ) 
completionReason = jobsDAO . CMPL_REASON_ERROR 
completionMsg = msg . getvalue ( ) 
logger . error ( completionMsg ) 
if type ( e ) is not InvalidConnectionException : 
~~~ jobsDAO . modelUpdateResults ( modelID , results = None , numRecords = 0 ) 
~~ if type ( e ) == JobFailException : 
~~~ workerCmpReason = jobsDAO . jobGetFields ( jobID , 
[ 'workerCompletionReason' ] ) [ 0 ] 
if workerCmpReason == ClientJobsDAO . CMPL_REASON_SUCCESS : 
~~~ jobsDAO . jobSetFields ( jobID , fields = dict ( 
cancel = True , 
workerCompletionReason = ClientJobsDAO . CMPL_REASON_ERROR , 
useConnectionID = False , 
~~ ~~ return ( completionReason , completionMsg ) 
~~ def runModelGivenBaseAndParams ( modelID , jobID , baseDescription , params , 
predictedField , reportKeys , optimizeKey , jobsDAO , 
modelCheckpointGUID , logLevel = None , predictionCacheMaxRecords = None ) : 
from nupic . swarming . ModelRunner import OPFModelRunner 
logger = logging . getLogger ( 'com.numenta.nupic.hypersearch.utils' ) 
experimentDir = tempfile . mkdtemp ( ) 
paramsFilePath = os . path . join ( experimentDir , 'description.py' ) 
paramsFile = open ( paramsFilePath , 'wb' ) 
paramsFile . write ( _paramsFileHead ( ) ) 
items = params . items ( ) 
items . sort ( ) 
for ( key , value ) in items : 
~~~ quotedKey = _quoteAndEscape ( key ) 
if isinstance ( value , basestring ) : 
~~ ~~ paramsFile . write ( _paramsFileTail ( ) ) 
paramsFile . close ( ) 
baseParamsFile = open ( os . path . join ( experimentDir , 'base.py' ) , 'wb' ) 
baseParamsFile . write ( baseDescription ) 
baseParamsFile . close ( ) 
fd = open ( paramsFilePath ) 
expDescription = fd . read ( ) 
jobsDAO . modelSetFields ( modelID , { 'genDescription' : expDescription } ) 
~~~ runner = OPFModelRunner ( 
modelID = modelID , 
jobID = jobID , 
predictedField = predictedField , 
experimentDir = experimentDir , 
reportKeyPatterns = reportKeys , 
optimizeKeyPattern = optimizeKey , 
jobsDAO = jobsDAO , 
modelCheckpointGUID = modelCheckpointGUID , 
logLevel = logLevel , 
predictionCacheMaxRecords = predictionCacheMaxRecords ) 
signal . signal ( signal . SIGINT , runner . handleWarningSignal ) 
( completionReason , completionMsg ) = runner . run ( ) 
~~ except InvalidConnectionException : 
~~ except Exception , e : 
~~~ ( completionReason , completionMsg ) = _handleModelRunnerException ( jobID , 
modelID , jobsDAO , experimentDir , logger , e ) 
~~ ~~ finally : 
~~~ shutil . rmtree ( experimentDir ) 
signal . signal ( signal . SIGINT , signal . default_int_handler ) 
~~ return ( completionReason , completionMsg ) 
~~ def rCopy ( d , f = identityConversion , discardNoneKeys = True , deepCopy = True ) : 
if deepCopy : 
~~~ d = copy . deepcopy ( d ) 
~~ newDict = { } 
toCopy = [ ( k , v , newDict , ( ) ) for k , v in d . iteritems ( ) ] 
while len ( toCopy ) > 0 : 
~~~ k , v , d , prevKeys = toCopy . pop ( ) 
prevKeys = prevKeys + ( k , ) 
if isinstance ( v , dict ) : 
~~~ d [ k ] = dict ( ) 
toCopy [ 0 : 0 ] = [ ( innerK , innerV , d [ k ] , prevKeys ) 
for innerK , innerV in v . iteritems ( ) ] 
~~~ newV = f ( v , prevKeys ) 
if not discardNoneKeys or newV is not None : 
~~~ d [ k ] = newV 
~~ ~~ ~~ return newDict 
~~ def rApply ( d , f ) : 
remainingDicts = [ ( d , ( ) ) ] 
while len ( remainingDicts ) > 0 : 
~~~ current , prevKeys = remainingDicts . pop ( ) 
for k , v in current . iteritems ( ) : 
~~~ keys = prevKeys + ( k , ) 
~~~ remainingDicts . insert ( 0 , ( v , keys ) ) 
~~~ f ( v , keys ) 
~~ ~~ ~~ ~~ def clippedObj ( obj , maxElementSize = 64 ) : 
if hasattr ( obj , '_asdict' ) : 
~~~ obj = obj . _asdict ( ) 
~~ if isinstance ( obj , dict ) : 
~~~ objOut = dict ( ) 
for key , val in obj . iteritems ( ) : 
~~~ objOut [ key ] = clippedObj ( val ) 
~~ ~~ elif hasattr ( obj , '__iter__' ) : 
~~~ objOut = [ ] 
for val in obj : 
~~~ objOut . append ( clippedObj ( val ) ) 
~~~ objOut = str ( obj ) 
if len ( objOut ) > maxElementSize : 
~~~ objOut = objOut [ 0 : maxElementSize ] + '...' 
~~ ~~ return objOut 
~~ def validate ( value , ** kwds ) : 
assert len ( kwds . keys ( ) ) >= 1 
assert 'schemaPath' in kwds or 'schemaDict' in kwds 
schemaDict = None 
if 'schemaPath' in kwds : 
~~~ schemaPath = kwds . pop ( 'schemaPath' ) 
schemaDict = loadJsonValueFromFile ( schemaPath ) 
~~ elif 'schemaDict' in kwds : 
~~~ schemaDict = kwds . pop ( 'schemaDict' ) 
~~~ validictory . validate ( value , schemaDict , ** kwds ) 
~~ except validictory . ValidationError as e : 
~~~ raise ValidationError ( e ) 
~~ ~~ def loadJsonValueFromFile ( inputFilePath ) : 
with open ( inputFilePath ) as fileObj : 
~~~ value = json . load ( fileObj ) 
~~ def sortedJSONDumpS ( obj ) : 
itemStrs = [ ] 
~~~ items = obj . items ( ) 
for key , value in items : 
~~ elif hasattr ( obj , '__iter__' ) : 
~~~ for val in obj : 
~~~ itemStrs . append ( sortedJSONDumpS ( val ) ) 
~~~ return json . dumps ( obj ) 
~~ ~~ def tick ( self ) : 
for act in self . __activities : 
~~~ if not act . iteratorHolder [ 0 ] : 
~~~ next ( act . iteratorHolder [ 0 ] ) 
~~~ act . cb ( ) 
if act . repeating : 
~~~ act . iteratorHolder [ 0 ] = iter ( xrange ( act . period ) ) 
~~~ act . iteratorHolder [ 0 ] = None 
~~ def rUpdate ( original , updates ) : 
dictPairs = [ ( original , updates ) ] 
while len ( dictPairs ) > 0 : 
~~~ original , updates = dictPairs . pop ( ) 
for k , v in updates . iteritems ( ) : 
~~~ if k in original and isinstance ( original [ k ] , dict ) and isinstance ( v , dict ) : 
~~~ dictPairs . append ( ( original [ k ] , v ) ) 
~~~ original [ k ] = v 
~~ ~~ ~~ ~~ def dictDiffAndReport ( da , db ) : 
differences = dictDiff ( da , db ) 
if not differences : 
~~~ return differences 
~~ if differences [ 'inAButNotInB' ] : 
~~ if differences [ 'inBButNotInA' ] : 
~~ for key in differences [ 'differentValues' ] : 
~~ return differences 
~~ def dictDiff ( da , db ) : 
different = False 
resultDict = dict ( ) 
resultDict [ 'inAButNotInB' ] = set ( da ) - set ( db ) 
if resultDict [ 'inAButNotInB' ] : 
~~~ different = True 
~~ resultDict [ 'inBButNotInA' ] = set ( db ) - set ( da ) 
if resultDict [ 'inBButNotInA' ] : 
~~ resultDict [ 'differentValues' ] = [ ] 
for key in ( set ( da ) - resultDict [ 'inAButNotInB' ] ) : 
~~~ comparisonResult = da [ key ] == db [ key ] 
if isinstance ( comparisonResult , bool ) : 
~~~ isEqual = comparisonResult 
~~~ isEqual = comparisonResult . all ( ) 
~~ if not isEqual : 
~~~ resultDict [ 'differentValues' ] . append ( key ) 
different = True 
~~ ~~ assert ( ( ( resultDict [ 'inAButNotInB' ] or resultDict [ 'inBButNotInA' ] or 
resultDict [ 'differentValues' ] ) and different ) or not different ) 
return resultDict if different else None 
~~ def _seed ( self , seed = - 1 ) : 
~~~ self . random = NupicRandom ( seed ) 
~~~ self . random = NupicRandom ( ) 
~~ ~~ def _newRep ( self ) : 
maxAttempts = 1000 
for _ in xrange ( maxAttempts ) : 
~~~ foundUnique = True 
population = numpy . arange ( self . n , dtype = numpy . uint32 ) 
choices = numpy . arange ( self . w , dtype = numpy . uint32 ) 
oneBits = sorted ( self . random . sample ( population , choices ) ) 
sdr = numpy . zeros ( self . n , dtype = 'uint8' ) 
sdr [ oneBits ] = 1 
for i in xrange ( self . ncategories ) : 
~~~ if ( sdr == self . sdrs [ i ] ) . all ( ) : 
~~~ foundUnique = False 
~~ ~~ if foundUnique : 
~~~ break ; 
~~ ~~ if not foundUnique : 
~~ return sdr 
~~ def getScalars ( self , input ) : 
if input == SENTINEL_VALUE_FOR_MISSING_DATA : 
~~~ return numpy . array ( [ 0 ] ) 
~~ index = self . categoryToIndex . get ( input , None ) 
if index is None : 
~~~ if self . _learningEnabled : 
~~~ self . _addCategory ( input ) 
index = self . ncategories - 1 
~~ ~~ return numpy . array ( [ index ] ) 
~~ def decode ( self , encoded , parentFieldName = '' ) : 
assert ( encoded [ 0 : self . n ] <= 1.0 ) . all ( ) 
resultString = "" 
resultRanges = [ ] 
overlaps = ( self . sdrs * encoded [ 0 : self . n ] ) . sum ( axis = 1 ) 
if self . verbosity >= 2 : 
for i in xrange ( 0 , self . ncategories ) : 
~~ ~~ matchingCategories = ( overlaps > self . thresholdOverlap ) . nonzero ( ) [ 0 ] 
for index in matchingCategories : 
~~~ if resultString != "" : 
~~ resultString += str ( self . categories [ index ] ) 
resultRanges . append ( [ int ( index ) , int ( index ) ] ) 
~~ if parentFieldName != '' : 
~~~ fieldName = "%s.%s" % ( parentFieldName , self . name ) 
~~~ fieldName = self . name 
~~ return ( { fieldName : ( resultRanges , resultString ) } , [ fieldName ] ) 
~~ def _getTopDownMapping ( self ) : 
if self . _topDownMappingM is None : 
~~~ self . _topDownMappingM = SM32 ( self . ncategories , self . n ) 
outputSpace = numpy . zeros ( self . n , dtype = GetNTAReal ( ) ) 
~~~ self . encodeIntoArray ( self . categories [ i ] , outputSpace ) 
self . _topDownMappingM . setRowFromDense ( i , outputSpace ) 
~~ ~~ return self . _topDownMappingM 
~~ def getBucketInfo ( self , buckets ) : 
if self . ncategories == 0 : 
~~ topDownMappingM = self . _getTopDownMapping ( ) 
categoryIndex = buckets [ 0 ] 
category = self . categories [ categoryIndex ] 
encoding = topDownMappingM . getRow ( categoryIndex ) 
return [ EncoderResult ( value = category , scalar = categoryIndex , 
encoding = encoding ) ] 
~~ def topDownCompute ( self , encoded ) : 
categoryIndex = topDownMappingM . rightVecProd ( encoded ) . argmax ( ) 
return EncoderResult ( value = category , scalar = categoryIndex , encoding = encoding ) 
~~ def getScalarNames ( self , parentFieldName = '' ) : 
names = [ ] 
def _formFieldName ( encoder ) : 
~~~ if parentFieldName == '' : 
~~~ return encoder . name 
~~~ return '%s.%s' % ( parentFieldName , encoder . name ) 
~~ ~~ if self . seasonEncoder is not None : 
~~~ names . append ( _formFieldName ( self . seasonEncoder ) ) 
~~ if self . dayOfWeekEncoder is not None : 
~~~ names . append ( _formFieldName ( self . dayOfWeekEncoder ) ) 
~~ if self . customDaysEncoder is not None : 
~~~ names . append ( _formFieldName ( self . customDaysEncoder ) ) 
~~ if self . weekendEncoder is not None : 
~~~ names . append ( _formFieldName ( self . weekendEncoder ) ) 
~~ if self . holidayEncoder is not None : 
~~~ names . append ( _formFieldName ( self . holidayEncoder ) ) 
~~ if self . timeOfDayEncoder is not None : 
~~~ names . append ( _formFieldName ( self . timeOfDayEncoder ) ) 
~~ return names 
~~ def getEncodedValues ( self , input ) : 
~~~ return numpy . array ( [ None ] ) 
~~ assert isinstance ( input , datetime . datetime ) 
values = [ ] 
timetuple = input . timetuple ( ) 
timeOfDay = timetuple . tm_hour + float ( timetuple . tm_min ) / 60.0 
if self . seasonEncoder is not None : 
~~~ dayOfYear = timetuple . tm_yday 
values . append ( dayOfYear - 1 ) 
~~~ dayOfWeek = timetuple . tm_wday + timeOfDay / 24.0 
values . append ( dayOfWeek ) 
~~~ if timetuple . tm_wday == 6 or timetuple . tm_wday == 5 or ( timetuple . tm_wday == 4 and timeOfDay > 18 ) : 
~~~ weekend = 1 
~~~ weekend = 0 
~~ values . append ( weekend ) 
~~~ if timetuple . tm_wday in self . customDays : 
~~~ customDay = 1 
~~~ customDay = 0 
~~ values . append ( customDay ) 
~~~ if len ( self . holidays ) == 0 : 
~~~ holidays = [ ( 12 , 25 ) ] 
~~~ holidays = self . holidays 
~~ val = 0 
for h in holidays : 
~~~ if len ( h ) == 3 : 
~~~ hdate = datetime . datetime ( h [ 0 ] , h [ 1 ] , h [ 2 ] , 0 , 0 , 0 ) 
~~~ hdate = datetime . datetime ( timetuple . tm_year , h [ 0 ] , h [ 1 ] , 0 , 0 , 0 ) 
~~ if input > hdate : 
~~~ diff = input - hdate 
if diff . days == 0 : 
~~~ val = 1 
~~ elif diff . days == 1 : 
~~~ val = 1.0 - ( float ( diff . seconds ) / 86400 ) 
~~~ diff = hdate - input 
~~ ~~ ~~ values . append ( val ) 
~~~ values . append ( timeOfDay ) 
~~ return values 
~~ def getBucketIndices ( self , input ) : 
~~~ return [ None ] * len ( self . encoders ) 
~~~ assert isinstance ( input , datetime . datetime ) 
scalars = self . getScalars ( input ) 
for i in xrange ( len ( self . encoders ) ) : 
~~~ ( name , encoder , offset ) = self . encoders [ i ] 
result . extend ( encoder . getBucketIndices ( scalars [ i ] ) ) 
~~ ~~ def encodeIntoArray ( self , input , output ) : 
~~~ output [ 0 : ] = 0 
~~~ if not isinstance ( input , datetime . datetime ) : 
type ( input ) , str ( input ) ) ) 
~~ scalars = self . getScalars ( input ) 
encoder . encodeIntoArray ( scalars [ i ] , output [ offset : ] ) 
~~ ~~ ~~ def getSpec ( cls ) : 
spec = { 
"description" : IdentityRegion . __doc__ , 
"singleNodeOnly" : True , 
"inputs" : { 
"in" : { 
"dataType" : "Real32" , 
"count" : 0 , 
"required" : True , 
"regionLevel" : False , 
"isDefaultInput" : True , 
"requireSplitterMap" : False } , 
"outputs" : { 
"out" : { 
"regionLevel" : True , 
"isDefaultOutput" : True } , 
"parameters" : { 
"dataWidth" : { 
"accessMode" : "Read" , 
"dataType" : "UInt32" , 
"count" : 1 , 
"constraints" : "" } , 
return spec 
~~ def _setRandomEncoderResolution ( minResolution = 0.001 ) : 
encoder = ( 
model_params . MODEL_PARAMS [ "modelParams" ] [ "sensorParams" ] [ "encoders" ] [ "value" ] 
if encoder [ "type" ] == "RandomDistributedScalarEncoder" : 
~~~ rangePadding = abs ( _INPUT_MAX - _INPUT_MIN ) * 0.2 
minValue = _INPUT_MIN - rangePadding 
maxValue = _INPUT_MAX + rangePadding 
resolution = max ( minResolution , 
( maxValue - minValue ) / encoder . pop ( "numBuckets" ) 
encoder [ "resolution" ] = resolution 
~~ ~~ def addLabel ( self , start , end , labelName ) : 
if len ( self . saved_states ) == 0 : 
~~ startID = self . saved_states [ 0 ] . ROWID 
clippedEnd = max ( 0 , min ( len ( self . saved_states ) , end - startID ) ) 
'endRecordID' : self . saved_states [ len ( self . saved_states ) - 1 ] . ROWID 
'numRecordsStored' : len ( self . saved_states ) 
~~ for state in self . saved_states [ clippedStart : clippedEnd ] : 
for state in self . saved_states [ clippedEnd : ] : 
~~~ self . _updateState ( state ) 
clippedEnd = len ( self . saved_states ) if end is None else max ( 0 , min ( len ( self . saved_states ) , end - startID ) ) 
for state in self . saved_states [ clippedStart : clippedEnd ] : 
~~ return { 'status' : 'success' } 
classifier = self . htm_prediction_model . _getAnomalyClassifier ( ) 
knn = classifier . getSelf ( ) . _knn 
prototype_idx = classifier . getSelf ( ) . getParameter ( 'categoryRecencyList' ) 
idsToDelete = [ r . ROWID for r in recordsToDelete if not r . setByUser and r . ROWID in prototype_idx ] 
nProtos = knn . _numPatterns 
knn . removeIds ( idsToDelete ) 
assert knn . _numPatterns == nProtos - len ( idsToDelete ) 
classifier . getSelf ( ) . getParameter ( 'categoryRecencyList' ) ) 
knn . removeIds ( idsToDelete . tolist ( ) ) 
classifier_indexes = numpy . array ( classifier . getSelf ( ) . getParameter ( 'categoryRecencyList' ) ) 
( classifier_indexes >= self . _autoDetectWaitRecords ) & 
~~ classifier . setParameter ( 'inferenceMode' , True ) 
classifier . setParameter ( 'learningMode' , False ) 
classifier . getSelf ( ) . compute ( inputs , outputs ) 
classifier . setParameter ( 'learningMode' , True ) 
classifier_distances = classifier . getSelf ( ) . getLatestDistances ( ) 
category = classifier . getSelf ( ) . getCategoryList ( ) [ indexID ] 
~~ def _constructClassificationRecord ( self ) : 
model = self . htm_prediction_model 
sp = model . _getSPRegion ( ) 
tm = model . _getTPRegion ( ) 
tpImp = tm . getSelf ( ) . _tfdr 
activeColumns = sp . getOutputData ( "bottomUpOut" ) . nonzero ( ) [ 0 ] 
score = numpy . in1d ( activeColumns , self . _prevPredictedColumns ) . sum ( ) 
score = ( self . _activeColumnCount - score ) / float ( self . _activeColumnCount ) 
spSize = sp . getParameter ( 'activeOutputCount' ) 
tpSize = tm . getParameter ( 'cellsPerColumn' ) * tm . getParameter ( 'columnCount' ) 
if self . _vectorType == 'tpc' : 
activeCellMatrix = tpImp . getLearnActiveStateT ( ) . reshape ( tpSize , 1 ) 
~~ ~~ elif self . _vectorType == 'sp_tpe' : 
if activeColumns . shape [ 0 ] > 0 : 
~~~ classificationVector [ activeColumns ] = 1.0 
~~ errorColumns = numpy . setdiff1d ( self . _prevPredictedColumns , activeColumns ) 
predictedColumns = tm . getOutputData ( "topDownOut" ) . nonzero ( ) [ 0 ] 
~~ def compute ( self ) : 
result = self . _constructClassificationRecord ( ) 
if result . ROWID >= self . _autoDetectWaitRecords : 
~~~ self . _updateState ( result ) 
~~ self . saved_states . append ( result ) 
if len ( self . saved_states ) > self . _history_length : 
~~~ self . saved_states . pop ( 0 ) 
~~ def setAutoDetectWaitRecords ( self , waitRecords ) : 
if not isinstance ( waitRecords , int ) : 
~~ if len ( self . saved_states ) > 0 and waitRecords < self . saved_states [ 0 ] . ROWID : 
~~ self . _autoDetectWaitRecords = waitRecords 
for state in self . saved_states : 
~~ ~~ def setAutoDetectThreshold ( self , threshold ) : 
if not ( isinstance ( threshold , float ) or isinstance ( threshold , int ) ) : 
~~ self . _autoDetectThreshold = threshold 
~~ ~~ def _getAdditionalSpecs ( spatialImp , kwargs = { } ) : 
typeNames = { int : 'UInt32' , float : 'Real32' , str : 'Byte' , bool : 'bool' , tuple : 'tuple' } 
def getArgType ( arg ) : 
~~~ t = typeNames . get ( type ( arg ) , 'Byte' ) 
count = 0 if t == 'Byte' else 1 
if t == 'tuple' : 
~~~ t = typeNames . get ( type ( arg [ 0 ] ) , 'Byte' ) 
count = len ( arg ) 
~~ if t == 'bool' : 
~~~ t = 'UInt32' 
~~ return ( t , count ) 
~~ def getConstraints ( arg ) : 
if t == 'Byte' : 
~~~ return 'multiple' 
~~ elif t == 'bool' : 
~~~ return 'bool' 
~~ ~~ SpatialClass = getSPClass ( spatialImp ) 
sArgTuples = _buildArgs ( SpatialClass . __init__ ) 
spatialSpec = { } 
for argTuple in sArgTuples : 
~~~ d = dict ( 
description = argTuple [ 1 ] , 
dataType = getArgType ( argTuple [ 2 ] ) [ 0 ] , 
count = getArgType ( argTuple [ 2 ] ) [ 1 ] , 
constraints = getConstraints ( argTuple [ 2 ] ) ) 
spatialSpec [ argTuple [ 0 ] ] = d 
~~ spatialSpec . update ( dict ( 
columnCount = dict ( 
accessMode = 'Read' , 
constraints = '' ) , 
inputWidth = dict ( 
spInputNonZeros = dict ( 
spOutputNonZeros = dict ( 
spOverlapDistribution = dict ( 
sparseCoincidenceMatrix = dict ( 
denseOutput = dict ( 
spLearningStatsStr = dict ( 
constraints = 'handle' ) , 
spatialImp = dict ( 
otherSpec = dict ( 
constraints = 'bool' ) , 
anomalyMode = dict ( 
topDownMode = dict ( 
activeOutputCount = dict ( 
logPathInput = dict ( 
logPathOutput = dict ( 
logPathOutputDense = dict ( 
return spatialSpec , otherSpec 
~~ def _initializeEphemeralMembers ( self ) : 
for attrName in self . _getEphemeralMembersBase ( ) : 
~~~ if attrName != "_loaded" : 
~~~ if hasattr ( self , attrName ) : 
~~~ if self . _loaded : 
~~ ~~ ~~ ~~ if not self . _loaded : 
~~~ for attrName in self . _getEphemeralMembersBase ( ) : 
~~~ assert not hasattr ( self , attrName ) 
~~~ assert hasattr ( self , attrName ) 
~~ ~~ ~~ self . _profileObj = None 
self . _iterations = 0 
self . _initEphemerals ( ) 
self . _checkEphemeralMembers ( ) 
self . _spatialPoolerOutput = numpy . zeros ( self . columnCount , 
dtype = GetNTAReal ( ) ) 
self . _spatialPoolerInput = numpy . zeros ( ( 1 , self . inputWidth ) , 
self . _allocateSpatialFDR ( None ) 
~~ def _allocateSpatialFDR ( self , rfInput ) : 
if self . _sfdr : 
~~ autoArgs = dict ( ( name , getattr ( self , name ) ) 
for name in self . _spatialArgNames ) 
if ( ( self . SpatialClass == CPPSpatialPooler ) or 
( self . SpatialClass == PYSpatialPooler ) ) : 
~~~ autoArgs [ 'columnDimensions' ] = [ self . columnCount ] 
autoArgs [ 'inputDimensions' ] = [ self . inputWidth ] 
autoArgs [ 'potentialRadius' ] = self . inputWidth 
self . _sfdr = self . SpatialClass ( 
** autoArgs 
if False and self . learningMode and self . _iterations > 0 and self . _iterations <= 10 : 
~~~ import hotshot 
if self . _iterations == 10 : 
stats = hotshot . stats . load ( "hotshot.stats" ) 
stats . strip_dirs ( ) 
stats . sort_stats ( 'time' , 'calls' ) 
stats . print_stats ( ) 
~~ if self . _profileObj is None : 
if os . path . exists ( 'hotshot.stats' ) : 
~~~ os . remove ( 'hotshot.stats' ) 
~~ self . _profileObj = hotshot . Profile ( "hotshot.stats" , 1 , 1 ) 
~~ self . _profileObj . runcall ( self . _compute , * [ inputs , outputs ] ) 
~~~ self . _compute ( inputs , outputs ) 
~~ ~~ def _compute ( self , inputs , outputs ) : 
if self . _sfdr is None : 
~~ if not self . topDownMode : 
~~~ self . _iterations += 1 
buInputVector = inputs [ 'bottomUpIn' ] 
resetSignal = False 
if 'resetIn' in inputs : 
~~~ assert len ( inputs [ 'resetIn' ] ) == 1 
resetSignal = inputs [ 'resetIn' ] [ 0 ] != 0 
~~ rfOutput = self . _doBottomUpCompute ( 
rfInput = buInputVector . reshape ( ( 1 , buInputVector . size ) ) , 
resetSignal = resetSignal 
outputs [ 'bottomUpOut' ] [ : ] = rfOutput . flat 
~~~ topDownIn = inputs . get ( 'topDownIn' , None ) 
spatialTopDownOut , temporalTopDownOut = self . _doTopDownInfer ( topDownIn ) 
outputs [ 'spatialTopDownOut' ] [ : ] = spatialTopDownOut 
if temporalTopDownOut is not None : 
~~~ outputs [ 'temporalTopDownOut' ] [ : ] = temporalTopDownOut 
~~ ~~ outputs [ 'anomalyScore' ] [ : ] = 0 
~~ def _doBottomUpCompute ( self , rfInput , resetSignal ) : 
self . _conditionalBreak ( ) 
self . _spatialPoolerInput = rfInput . reshape ( - 1 ) 
assert ( rfInput . shape [ 0 ] == 1 ) 
inputVector = numpy . array ( rfInput [ 0 ] ) . astype ( 'uint32' ) 
outputVector = numpy . zeros ( self . _sfdr . getNumColumns ( ) ) . astype ( 'uint32' ) 
self . _sfdr . compute ( inputVector , self . learningMode , outputVector ) 
self . _spatialPoolerOutput [ : ] = outputVector [ : ] 
if self . _fpLogSP : 
~~~ output = self . _spatialPoolerOutput . reshape ( - 1 ) 
outputNZ = output . nonzero ( ) [ 0 ] 
print >> self . _fpLogSP , output . size , outStr 
~~ if self . _fpLogSPInput : 
~~~ output = rfInput . reshape ( - 1 ) 
print >> self . _fpLogSPInput , output . size , outStr 
~~ return self . _spatialPoolerOutput 
~~ def getBaseSpec ( cls ) : 
spec = dict ( 
description = SPRegion . __doc__ , 
resetIn = dict ( 
topDownIn = dict ( 
bottomUpOut = dict ( 
isDefaultOutput = True ) , 
topDownOut = dict ( 
isDefaultOutput = False ) , 
spatialTopDownOut = dict ( 
temporalTopDownOut = dict ( 
anomalyScore = dict ( 
breakPdb = dict ( 
breakKomodo = dict ( 
spec = cls . getBaseSpec ( ) 
s , o = _getAdditionalSpecs ( spatialImp = getDefaultSPImp ( ) ) 
spec [ 'parameters' ] . update ( s ) 
spec [ 'parameters' ] . update ( o ) 
~~ def getParameter ( self , parameterName , index = - 1 ) : 
if parameterName == 'activeOutputCount' : 
~~~ return self . columnCount 
~~ elif parameterName == 'spatialPoolerInput' : 
~~~ return list ( self . _spatialPoolerInput . reshape ( - 1 ) ) 
~~ elif parameterName == 'spatialPoolerOutput' : 
~~~ return list ( self . _spatialPoolerOutput ) 
~~ elif parameterName == 'spNumActiveOutputs' : 
~~~ return len ( self . _spatialPoolerOutput . nonzero ( ) [ 0 ] ) 
~~ elif parameterName == 'spOutputNonZeros' : 
~~~ return [ len ( self . _spatialPoolerOutput ) ] + list ( self . _spatialPoolerOutput . nonzero ( ) [ 0 ] ) 
~~ elif parameterName == 'spInputNonZeros' : 
~~~ import pdb ; pdb . set_trace ( ) 
return [ len ( self . _spatialPoolerInput ) ] + list ( self . _spatialPoolerInput . nonzero ( ) [ 0 ] ) 
~~ elif parameterName == 'spLearningStatsStr' : 
~~~ return str ( self . _sfdr . getLearningStats ( ) ) 
~~~ return str ( dict ( ) ) 
~~~ return PyRegion . getParameter ( self , parameterName , index ) 
~~ ~~ def setParameter ( self , parameterName , index , parameterValue ) : 
if parameterName in self . _spatialArgNames : 
~~~ setattr ( self . _sfdr , parameterName , parameterValue ) 
~~ elif parameterName == "logPathInput" : 
~~~ self . logPathInput = parameterValue 
if self . _fpLogSPInput : 
~~~ self . _fpLogSPInput . close ( ) 
self . _fpLogSPInput = None 
~~ if parameterValue : 
~~~ self . _fpLogSPInput = open ( self . logPathInput , 'w' ) 
~~ ~~ elif parameterName == "logPathOutput" : 
~~~ self . logPathOutput = parameterValue 
~~~ self . _fpLogSP . close ( ) 
self . _fpLogSP = None 
~~~ self . _fpLogSP = open ( self . logPathOutput , 'w' ) 
~~ ~~ elif parameterName == "logPathOutputDense" : 
~~~ self . logPathOutputDense = parameterValue 
if self . _fpLogSPDense : 
~~~ self . _fpLogSPDense . close ( ) 
self . _fpLogSPDense = None 
~~~ self . _fpLogSPDense = open ( self . logPathOutputDense , 'w' ) 
~~ ~~ elif hasattr ( self , parameterName ) : 
~~~ setattr ( self , parameterName , parameterValue ) 
proto . spatialImp = self . spatialImp 
proto . columnCount = self . columnCount 
proto . inputWidth = self . inputWidth 
proto . learningMode = 1 if self . learningMode else 0 
proto . inferenceMode = 1 if self . inferenceMode else 0 
proto . anomalyMode = 1 if self . anomalyMode else 0 
proto . topDownMode = 1 if self . topDownMode else 0 
self . _sfdr . write ( proto . spatialPooler ) 
instance = cls ( proto . columnCount , proto . inputWidth ) 
instance . spatialImp = proto . spatialImp 
instance . anomalyMode = proto . anomalyMode 
instance . topDownMode = proto . topDownMode 
spatialImp = proto . spatialImp 
instance . _sfdr = getSPClass ( spatialImp ) . read ( proto . spatialPooler ) 
~~ def _initEphemerals ( self ) : 
if hasattr ( self , '_sfdr' ) and self . _sfdr : 
~~~ self . _spatialPoolerOutput = numpy . zeros ( self . columnCount , 
~~ self . _fpLogSPInput = None 
self . logPathInput = "" 
self . logPathOutput = "" 
self . logPathOutputDense = "" 
~~ def getParameterArrayCount ( self , name , index ) : 
p = self . getParameter ( name ) 
if ( not hasattr ( p , '__len__' ) ) : 
~~ return len ( p ) 
~~ def getParameterArray ( self , name , index , a ) : 
~~ if len ( p ) > 0 : 
~~~ a [ : ] = p [ : ] 
~~ ~~ def _cacheSequenceInfoType ( self ) : 
hasReset = self . resetFieldName is not None 
hasSequenceId = self . sequenceIdFieldName is not None 
if hasReset and not hasSequenceId : 
~~~ self . _sequenceInfoType = self . SEQUENCEINFO_RESET_ONLY 
self . _prevSequenceId = 0 
~~ elif not hasReset and hasSequenceId : 
~~~ self . _sequenceInfoType = self . SEQUENCEINFO_SEQUENCEID_ONLY 
self . _prevSequenceId = None 
~~ elif hasReset and hasSequenceId : 
~~~ self . _sequenceInfoType = self . SEQUENCEINFO_BOTH 
~~~ self . _sequenceInfoType = self . SEQUENCEINFO_NONE 
~~ ~~ def _getTPClass ( temporalImp ) : 
if temporalImp == 'py' : 
~~~ return backtracking_tm . BacktrackingTM 
~~ elif temporalImp == 'cpp' : 
~~~ return backtracking_tm_cpp . BacktrackingTMCPP 
~~ elif temporalImp == 'tm_py' : 
~~~ return backtracking_tm_shim . TMShim 
~~ elif temporalImp == 'tm_cpp' : 
~~~ return backtracking_tm_shim . TMCPPShim 
~~ elif temporalImp == 'monitored_tm_py' : 
~~~ return backtracking_tm_shim . MonitoredTMShim 
"\ % ( temporalImp ) ) 
~~ ~~ def _buildArgs ( f , self = None , kwargs = { } ) : 
argTuples = getArgumentDescriptions ( f ) 
init = TMRegion . __init__ 
ourArgNames = [ t [ 0 ] for t in getArgumentDescriptions ( init ) ] 
ourArgNames += [ 
for argTuple in argTuples [ : ] : 
~~~ if argTuple [ 0 ] in ourArgNames : 
~~~ argTuples . remove ( argTuple ) 
~~ ~~ if self : 
~~~ for argTuple in argTuples : 
~~~ argName = argTuple [ 0 ] 
if argName in kwargs : 
~~~ argValue = kwargs . pop ( argName ) 
~~~ if len ( argTuple ) == 2 : 
~~ argValue = argTuple [ 2 ] 
~~ setattr ( self , argName , argValue ) 
~~ ~~ return argTuples 
~~ def _getAdditionalSpecs ( temporalImp , kwargs = { } ) : 
~~ ~~ TemporalClass = _getTPClass ( temporalImp ) 
tArgTuples = _buildArgs ( TemporalClass . __init__ ) 
temporalSpec = { } 
for argTuple in tArgTuples : 
temporalSpec [ argTuple [ 0 ] ] = d 
~~ temporalSpec . update ( dict ( 
cellsPerColumn = dict ( 
predictedSegmentDecrement = dict ( 
dataType = 'Real' , 
orColumnOutputs = dict ( 
dataType = 'Bool' , 
cellsSavePath = dict ( 
temporalImp = dict ( 
defaultValue = True , 
defaultValue = False , 
computePredictedActiveCellIndices = dict ( 
accessMode = 'Create' , 
storeDenseOutput = dict ( 
return temporalSpec , otherSpec 
autoArgs = dict ( ( name , getattr ( self , name ) ) 
for name in self . _temporalArgNames ) 
if self . _tfdr is None : 
~~~ tpClass = _getTPClass ( self . temporalImp ) 
if self . temporalImp in [ 'py' , 'cpp' , 'r' , 
'tm_py' , 'tm_cpp' , 
'monitored_tm_py' , ] : 
~~~ self . _tfdr = tpClass ( 
numberOfCols = self . columnCount , 
cellsPerColumn = self . cellsPerColumn , 
** autoArgs ) 
~~ ~~ ~~ def _compute ( self , inputs , outputs ) : 
~~ self . _conditionalBreak ( ) 
self . _iterations += 1 
if inputs [ 'resetIn' ] [ 0 ] != 0 : 
~~~ self . _tfdr . reset ( ) 
~~ ~~ if self . computePredictedActiveCellIndices : 
~~~ prevPredictedState = self . _tfdr . getPredictedState ( ) . reshape ( - 1 ) . astype ( 'float32' ) 
~~ if self . anomalyMode : 
~~~ prevPredictedColumns = self . _tfdr . topDownCompute ( ) . copy ( ) . nonzero ( ) [ 0 ] 
~~ tpOutput = self . _tfdr . compute ( buInputVector , self . learningMode , self . inferenceMode ) 
self . _sequencePos += 1 
if self . orColumnOutputs : 
~~~ tpOutput = tpOutput . reshape ( self . columnCount , 
self . cellsPerColumn ) . max ( axis = 1 ) 
~~ if self . _fpLogTPOutput : 
~~~ output = tpOutput . reshape ( - 1 ) 
outputNZ = tpOutput . nonzero ( ) [ 0 ] 
print >> self . _fpLogTPOutput , output . size , outStr 
~~ outputs [ 'bottomUpOut' ] [ : ] = tpOutput . flat 
if self . topDownMode : 
~~~ outputs [ 'topDownOut' ] [ : ] = self . _tfdr . topDownCompute ( ) . copy ( ) 
~~~ activeLearnCells = self . _tfdr . getLearnActiveStateT ( ) 
size = activeLearnCells . shape [ 0 ] * activeLearnCells . shape [ 1 ] 
outputs [ 'lrnActiveStateT' ] [ : ] = activeLearnCells . reshape ( size ) 
activeColumns = buInputVector . nonzero ( ) [ 0 ] 
outputs [ 'anomalyScore' ] [ : ] = anomaly . computeRawAnomalyScore ( 
activeColumns , prevPredictedColumns ) 
~~ if self . computePredictedActiveCellIndices : 
~~~ activeState = self . _tfdr . _getActiveState ( ) . reshape ( - 1 ) . astype ( 'float32' ) 
activeIndices = numpy . where ( activeState != 0 ) [ 0 ] 
predictedIndices = numpy . where ( prevPredictedState != 0 ) [ 0 ] 
predictedActiveIndices = numpy . intersect1d ( activeIndices , predictedIndices ) 
outputs [ "activeCells" ] . fill ( 0 ) 
outputs [ "activeCells" ] [ activeIndices ] = 1 
outputs [ "predictedActiveCells" ] . fill ( 0 ) 
outputs [ "predictedActiveCells" ] [ predictedActiveIndices ] = 1 
~~ ~~ def getBaseSpec ( cls ) : 
description = TMRegion . __doc__ , 
activeCells = dict ( 
lrnActiveStateT = dict ( 
commands = { } 
t , o = _getAdditionalSpecs ( temporalImp = gDefaultTemporalImp ) 
spec [ 'parameters' ] . update ( t ) 
if parameterName in self . _temporalArgNames : 
~~~ return getattr ( self . _tfdr , parameterName ) 
~~~ setattr ( self . _tfdr , parameterName , parameterValue ) 
~~ elif parameterName == "logPathOutput" : 
if self . _fpLogTPOutput is not None : 
~~~ self . _fpLogTPOutput . close ( ) 
self . _fpLogTPOutput = None 
~~~ self . _fpLogTPOutput = open ( self . logPathOutput , 'w' ) 
~~ ~~ def finishLearning ( self ) : 
~~ if hasattr ( self . _tfdr , 'finishLearning' ) : 
~~~ self . resetSequenceStates ( ) 
self . _tfdr . finishLearning ( ) 
proto . temporalImp = self . temporalImp 
proto . cellsPerColumn = self . cellsPerColumn 
proto . anomalyMode = self . anomalyMode 
proto . topDownMode = self . topDownMode 
proto . computePredictedActiveCellIndices = ( 
self . computePredictedActiveCellIndices ) 
proto . orColumnOutputs = self . orColumnOutputs 
if self . temporalImp == "py" : 
~~~ tmProto = proto . init ( "backtrackingTM" ) 
~~ elif self . temporalImp == "cpp" : 
~~~ tmProto = proto . init ( "backtrackingTMCpp" ) 
~~ elif self . temporalImp == "tm_py" : 
~~~ tmProto = proto . init ( "temporalMemory" ) 
~~ elif self . temporalImp == "tm_cpp" : 
~~~ raise TypeError ( 
self . temporalImp ) ) 
~~ self . _tfdr . write ( tmProto ) 
instance = cls ( proto . columnCount , proto . inputWidth , proto . cellsPerColumn ) 
instance . temporalImp = proto . temporalImp 
instance . computePredictedActiveCellIndices = ( 
proto . computePredictedActiveCellIndices ) 
instance . orColumnOutputs = proto . orColumnOutputs 
if instance . temporalImp == "py" : 
~~~ tmProto = proto . backtrackingTM 
~~ elif instance . temporalImp == "cpp" : 
~~~ tmProto = proto . backtrackingTMCpp 
~~ elif instance . temporalImp == "tm_py" : 
~~~ tmProto = proto . temporalMemory 
~~ elif instance . temporalImp == "tm_cpp" : 
instance . temporalImp ) ) 
~~ instance . _tfdr = _getTPClass ( proto . temporalImp ) . read ( tmProto ) 
~~ def getOutputElementCount ( self , name ) : 
if name == 'bottomUpOut' : 
~~~ return self . outputWidth 
~~ elif name == 'topDownOut' : 
~~ elif name == 'lrnActiveStateT' : 
~~ elif name == "activeCells" : 
~~ elif name == "predictedActiveCells" : 
~~ ~~ def computeRawAnomalyScore ( activeColumns , prevPredictedColumns ) : 
nActiveColumns = len ( activeColumns ) 
if nActiveColumns > 0 : 
~~~ score = numpy . in1d ( activeColumns , prevPredictedColumns ) . sum ( ) 
score = ( nActiveColumns - score ) / float ( nActiveColumns ) 
~~~ score = 0.0 
~~ def compute ( self , activeColumns , predictedColumns , 
inputValue = None , timestamp = None ) : 
anomalyScore = computeRawAnomalyScore ( activeColumns , predictedColumns ) 
if self . _mode == Anomaly . MODE_PURE : 
~~~ score = anomalyScore 
~~ elif self . _mode == Anomaly . MODE_LIKELIHOOD : 
~~~ if inputValue is None : 
~~ probability = self . _likelihood . anomalyProbability ( 
inputValue , anomalyScore , timestamp ) 
score = 1 - probability 
~~ elif self . _mode == Anomaly . MODE_WEIGHTED : 
~~~ probability = self . _likelihood . anomalyProbability ( 
score = anomalyScore * ( 1 - probability ) 
~~ if self . _movingAverage is not None : 
~~~ score = self . _movingAverage . next ( score ) 
~~ if self . _binaryThreshold is not None : 
~~~ if score >= self . _binaryThreshold : 
~~~ score = 1.0 
~~ ~~ return score 
~~ def addGraph ( self , data , position = 111 , xlabel = None , ylabel = None ) : 
ax = self . _addBase ( position , xlabel = xlabel , ylabel = ylabel ) 
ax . plot ( data ) 
plt . draw ( ) 
~~ def addHistogram ( self , data , position = 111 , xlabel = None , ylabel = None , 
bins = None ) : 
ax . hist ( data , bins = bins , color = "green" , alpha = 0.8 ) 
~~ def add2DArray ( self , data , position = 111 , xlabel = None , ylabel = None , cmap = None , 
aspect = "auto" , interpolation = "nearest" , name = None ) : 
if cmap is None : 
~~~ cmap = cm . Greys 
~~ ax = self . _addBase ( position , xlabel = xlabel , ylabel = ylabel ) 
ax . imshow ( data , cmap = cmap , aspect = aspect , interpolation = interpolation ) 
if self . _show : 
~~ if name is not None : 
~~~ if not os . path . exists ( "log" ) : 
~~~ os . mkdir ( "log" ) 
~~ plt . savefig ( "log/{name}.png" . format ( name = name ) , bbox_inches = "tight" , 
figsize = ( 8 , 6 ) , dpi = 400 ) 
~~ ~~ def _addBase ( self , position , xlabel = None , ylabel = None ) : 
ax = self . _fig . add_subplot ( position ) 
ax . set_xlabel ( xlabel ) 
ax . set_ylabel ( ylabel ) 
return ax 
~~ def _generateOverlapping ( filename = "overlap.csv" , numSequences = 2 , elementsPerSeq = 3 , 
numRepeats = 10 , hub = [ 0 , 1 ] , hubOffset = 1 , resets = False ) : 
assert ( hubOffset + len ( hub ) <= elementsPerSeq ) 
scriptDir = os . path . dirname ( __file__ ) 
pathname = os . path . join ( scriptDir , 'datasets' , filename ) 
fields = [ ( 'reset' , 'int' , 'R' ) , 
( 'field1' , 'string' , '' ) , 
( 'field2' , 'float' , '' ) ] 
outFile = FileRecordStream ( pathname , write = True , fields = fields ) 
sequences = [ ] 
nextElemIdx = max ( hub ) + 1 
for _ in range ( numSequences ) : 
~~~ seq = [ ] 
for j in range ( hubOffset ) : 
~~~ seq . append ( nextElemIdx ) 
nextElemIdx += 1 
~~ for j in hub : 
~~~ seq . append ( j ) 
~~ j = hubOffset + len ( hub ) 
while j < elementsPerSeq : 
j += 1 
~~ sequences . append ( seq ) 
~~ seqIdxs = [ ] 
for _ in range ( numRepeats ) : 
~~~ seqIdxs += range ( numSequences ) 
~~ random . shuffle ( seqIdxs ) 
for seqIdx in seqIdxs : 
~~~ reset = int ( resets ) 
seq = sequences [ seqIdx ] 
for ( x ) in seq : 
~~~ outFile . appendRecord ( [ reset , str ( x ) , x ] ) 
reset = 0 
~~ ~~ outFile . close ( ) 
~~ def _generateFirstOrder0 ( ) : 
numCategories = 5 
initProb = numpy . zeros ( numCategories ) 
initProb [ 0 ] = 1.0 
firstOrder = dict ( ) 
firstOrder [ '0' ] = numpy . array ( [ 0 , 0.1 , 0 , 0 , 0.9 ] ) 
firstOrder [ '1' ] = numpy . array ( [ 0 , 0 , 0.75 , 0.25 , 0 ] ) 
firstOrder [ '2' ] = numpy . array ( [ 1.0 , 0 , 0 , 0 , 0 ] ) 
firstOrder [ '3' ] = numpy . array ( [ 1.0 , 0 , 0 , 0 , 0 ] ) 
firstOrder [ '4' ] = numpy . array ( [ 0 , 0 , 0.5 , 0.5 , 0 ] ) 
secondOrder = None 
categoryList = [ '%d' % x for x in range ( 5 ) ] 
return ( initProb , firstOrder , secondOrder , 3 , categoryList ) 
~~ def _generateFileFromProb ( filename , numRecords , categoryList , initProb , 
firstOrderProb , secondOrderProb , seqLen , numNoise = 0 , resetsEvery = None ) : 
initCumProb = initProb . cumsum ( ) 
firstOrderCumProb = dict ( ) 
for ( key , value ) in firstOrderProb . iteritems ( ) : 
~~~ firstOrderCumProb [ key ] = value . cumsum ( ) 
~~ if secondOrderProb is not None : 
~~~ secondOrderCumProb = dict ( ) 
for ( key , value ) in secondOrderProb . iteritems ( ) : 
~~~ secondOrderCumProb [ key ] = value . cumsum ( ) 
~~~ secondOrderCumProb = None 
~~ elementsInSeq = [ ] 
numElementsSinceReset = 0 
maxCatIdx = len ( categoryList ) - 1 
for _ in xrange ( numRecords ) : 
~~~ if numElementsSinceReset == 0 : 
~~~ reset = 1 
~~~ reset = 0 
~~ rand = numpy . random . rand ( ) 
if secondOrderCumProb is None : 
~~~ if len ( elementsInSeq ) == 0 : 
~~~ catIdx = numpy . searchsorted ( initCumProb , rand ) 
~~ elif len ( elementsInSeq ) >= 1 and ( seqLen is None or len ( elementsInSeq ) < seqLen - numNoise ) : 
~~~ catIdx = numpy . searchsorted ( firstOrderCumProb [ str ( elementsInSeq [ - 1 ] ) ] , 
rand ) 
~~~ catIdx = numpy . random . randint ( len ( categoryList ) ) 
~~ elif len ( elementsInSeq ) == 1 : 
~~~ catIdx = numpy . searchsorted ( firstOrderCumProb [ str ( elementsInSeq ) ] , rand ) 
~~ elif ( len ( elementsInSeq ) >= 2 ) and ( seqLen is None or len ( elementsInSeq ) < seqLen - numNoise ) : 
~~~ catIdx = numpy . searchsorted ( secondOrderCumProb [ str ( elementsInSeq [ - 2 : ] ) ] , rand ) 
~~ ~~ catIdx = min ( maxCatIdx , catIdx ) 
outFile . appendRecord ( [ reset , categoryList [ catIdx ] , catIdx ] ) 
elementsInSeq . append ( catIdx ) 
numElementsSinceReset += 1 
if resetsEvery is not None and numElementsSinceReset == resetsEvery : 
~~~ numElementsSinceReset = 0 
elementsInSeq = [ ] 
~~ if seqLen is not None and ( len ( elementsInSeq ) == seqLen + numNoise ) : 
~~~ elementsInSeq = [ ] 
~~ def getVersion ( ) : 
with open ( os . path . join ( REPO_DIR , "VERSION" ) , "r" ) as versionFile : 
~~~ return versionFile . read ( ) . strip ( ) 
~~ ~~ def nupicBindingsPrereleaseInstalled ( ) : 
~~~ nupicDistribution = pkg_resources . get_distribution ( "nupic.bindings" ) 
if pkg_resources . parse_version ( nupicDistribution . version ) . is_prerelease : 
~~ ~~ except pkg_resources . DistributionNotFound : 
~~ def findRequirements ( ) : 
requirementsPath = os . path . join ( REPO_DIR , "requirements.txt" ) 
requirements = parse_file ( requirementsPath ) 
if nupicBindingsPrereleaseInstalled ( ) : 
~~~ requirements = [ req for req in requirements if "nupic.bindings" not in req ] 
~~ return requirements 
~~ def _handleDescriptionOption ( cmdArgStr , outDir , usageStr , hsVersion , 
claDescriptionTemplateFile ) : 
~~~ args = json . loads ( cmdArgStr ) 
~~~ raise _InvalidCommandArgException ( 
_makeUsageErrorStr ( 
~~ filesDescription = _generateExperiment ( args , outDir , hsVersion = hsVersion , 
claDescriptionTemplateFile = claDescriptionTemplateFile ) 
pprint . pprint ( filesDescription ) 
~~ def _handleDescriptionFromFileOption ( filename , outDir , usageStr , hsVersion , 
~~~ fileHandle = open ( filename , 'r' ) 
JSONStringFromFile = fileHandle . read ( ) . splitlines ( ) 
JSONStringFromFile = '' . join ( JSONStringFromFile ) 
~~ _handleDescriptionOption ( JSONStringFromFile , outDir , usageStr , 
hsVersion = hsVersion , 
~~ def _isInt ( x , precision = 0.0001 ) : 
xInt = int ( round ( x ) ) 
return ( abs ( x - xInt ) < precision * x , xInt ) 
~~ def _indentLines ( str , indentLevels = 1 , indentFirstLine = True ) : 
indent = _ONE_INDENT * indentLevels 
lines = str . splitlines ( True ) 
result = '' 
if len ( lines ) > 0 and not indentFirstLine : 
~~~ first = 1 
result += lines [ 0 ] 
~~~ first = 0 
~~ for line in lines [ first : ] : 
~~~ result += indent + line 
~~ def _generateMetricSpecString ( inferenceElement , metric , 
params = None , field = None , 
returnLabel = False ) : 
metricSpecArgs = dict ( metric = metric , 
field = field , 
inferenceElement = inferenceElement ) 
for item in metricSpecArgs . iteritems ( ) ] ) 
if not returnLabel : 
~~~ return metricSpecAsString 
~~ spec = MetricSpec ( ** metricSpecArgs ) 
metricLabel = spec . getLabel ( ) 
return metricSpecAsString , metricLabel 
~~ def _generateFileFromTemplates ( templateFileNames , outputFilePath , 
replacementDict ) : 
installPath = os . path . dirname ( __file__ ) 
outputFile = open ( outputFilePath , "w" ) 
outputLines = [ ] 
inputLines = [ ] 
firstFile = True 
for templateFileName in templateFileNames : 
~~~ if not firstFile : 
~~~ inputLines . extend ( [ os . linesep ] * 2 ) 
~~ firstFile = False 
inputFilePath = os . path . join ( installPath , templateFileName ) 
inputFile = open ( inputFilePath ) 
inputLines . extend ( inputFile . readlines ( ) ) 
inputFile . close ( ) 
for line in inputLines : 
~~~ tempLine = line 
for k , v in replacementDict . iteritems ( ) : 
~~~ if v is None : 
~~~ v = "None" 
~~ tempLine = re . sub ( k , v , tempLine ) 
~~ outputFile . write ( tempLine ) 
~~ outputFile . close ( ) 
~~ def _generateEncoderChoicesV1 ( fieldInfo ) : 
width = 7 
fieldName = fieldInfo [ 'fieldName' ] 
fieldType = fieldInfo [ 'fieldType' ] 
encoderChoicesList = [ ] 
if fieldType in [ 'float' , 'int' ] : 
~~~ aggFunction = 'mean' 
encoders = [ None ] 
for n in ( 13 , 50 , 150 , 500 ) : 
~~~ encoder = dict ( type = 'ScalarSpaceEncoder' , name = fieldName , fieldname = fieldName , 
n = n , w = width , clipInput = True , space = "absolute" ) 
if 'minValue' in fieldInfo : 
~~~ encoder [ 'minval' ] = fieldInfo [ 'minValue' ] 
~~ if 'maxValue' in fieldInfo : 
~~~ encoder [ 'maxval' ] = fieldInfo [ 'maxValue' ] 
~~ encoders . append ( encoder ) 
~~ encoderChoicesList . append ( encoders ) 
~~ elif fieldType == 'string' : 
~~~ aggFunction = 'first' 
encoder = dict ( type = 'SDRCategoryEncoder' , name = fieldName , 
fieldname = fieldName , n = 100 , w = width ) 
encoders . append ( encoder ) 
encoderChoicesList . append ( encoders ) 
~~ elif fieldType == 'datetime' : 
for radius in ( 1 , 8 ) : 
~~~ encoder = dict ( type = 'DateEncoder' , name = '%s_timeOfDay' % ( fieldName ) , 
fieldname = fieldName , timeOfDay = ( width , radius ) ) 
for radius in ( 1 , 3 ) : 
~~~ encoder = dict ( type = 'DateEncoder' , name = '%s_dayOfWeek' % ( fieldName ) , 
fieldname = fieldName , dayOfWeek = ( width , radius ) ) 
~~ return ( encoderChoicesList , aggFunction ) 
~~ def _generateEncoderStringsV1 ( includedFields ) : 
for fieldInfo in includedFields : 
~~~ fieldName = fieldInfo [ 'fieldName' ] 
( choicesList , aggFunction ) = _generateEncoderChoicesV1 ( fieldInfo ) 
encoderChoicesList . extend ( choicesList ) 
~~ encoderSpecsList = [ ] 
for encoderChoices in encoderChoicesList : 
~~~ encoder = encoderChoices [ - 1 ] 
for c in _ILLEGAL_FIELDNAME_CHARACTERS : 
~~~ if encoder [ 'name' ] . find ( c ) >= 0 : 
c , encoder [ 'name' ] ) ) 
_quoteAndEscape ( encoder [ 'name' ] ) , 
2 * _ONE_INDENT , 
pprint . pformat ( encoder , indent = 2 * _INDENT_STEP ) ) ) 
permEncoderChoicesList = [ ] 
_quoteAndEscape ( encoderChoices [ - 1 ] [ 'name' ] ) , 
pprint . pformat ( encoderChoices , indent = 2 * _INDENT_STEP ) ) ) 
~~ permEncoderChoicesStr = '\\n' . join ( permEncoderChoicesList ) 
permEncoderChoicesStr = _indentLines ( permEncoderChoicesStr , 1 , 
indentFirstLine = False ) 
return ( encoderSpecsStr , permEncoderChoicesStr ) 
~~ def _generatePermEncoderStr ( options , encoderDict ) : 
permStr = "" 
if encoderDict . get ( 'classifierOnly' , False ) : 
~~~ permStr = "dict(" 
for key , value in encoderDict . items ( ) : 
~~~ if key == "name" : 
~~ if key == 'n' and encoderDict [ 'type' ] != 'SDRCategoryEncoder' : 
encoderDict [ "w" ] + 500 ) 
~~~ if issubclass ( type ( value ) , basestring ) : 
~~~ permStr += "%s=\ % ( key , value ) 
~~ ~~ ~~ permStr += ")" 
~~~ if encoderDict [ "type" ] in [ "ScalarSpaceEncoder" , "AdaptiveScalarEncoder" , 
"ScalarEncoder" , "LogEncoder" ] : 
~~~ permStr = "PermuteEncoder(" 
~~~ if key == "fieldname" : 
~~~ key = "fieldName" 
~~ elif key == "type" : 
~~~ key = "encoderClass" 
~~ elif key == "name" : 
~~ if key == "n" : 
~~ elif key == "runDelta" : 
~~~ if value and not "space" in encoderDict : 
~~ encoderDict . pop ( "runDelta" ) 
~~ elif encoderDict [ "type" ] in [ "SDRCategoryEncoder" ] : 
~~ if issubclass ( type ( value ) , basestring ) : 
~~ ~~ permStr += ")" 
~~ elif encoderDict [ "type" ] in [ "DateEncoder" ] : 
~~ if key == "timeOfDay" : 
~~~ permStr += "encoderClass=\ % ( encoderDict [ "type" ] ) 
~~ elif key == "dayOfWeek" : 
~~ elif key == "weekend" : 
~~ ~~ return permStr 
~~ def _generateEncoderStringsV2 ( includedFields , options ) : 
width = 21 
encoderDictsList = [ ] 
if options [ 'inferenceType' ] in [ "NontemporalClassification" , 
"NontemporalMultiStep" , 
"TemporalMultiStep" , 
"MultiStep" ] : 
~~~ classifierOnlyField = options [ 'inferenceArgs' ] [ 'predictedField' ] 
~~~ classifierOnlyField = None 
~~ for fieldInfo in includedFields : 
~~~ runDelta = fieldInfo . get ( "runDelta" , False ) 
if runDelta or "space" in fieldInfo : 
~~~ encoderDict = dict ( type = 'ScalarSpaceEncoder' , name = fieldName , 
fieldname = fieldName , n = 100 , w = width , clipInput = True ) 
if runDelta : 
~~~ encoderDict [ "runDelta" ] = True 
~~~ encoderDict = dict ( type = 'AdaptiveScalarEncoder' , name = fieldName , 
~~ if 'minValue' in fieldInfo : 
~~~ encoderDict [ 'minval' ] = fieldInfo [ 'minValue' ] 
~~~ encoderDict [ 'maxval' ] = fieldInfo [ 'maxValue' ] 
~~ if ( 'minValue' in fieldInfo and 'maxValue' in fieldInfo ) and ( encoderDict [ 'type' ] == 'AdaptiveScalarEncoder' ) : 
~~~ encoderDict [ 'type' ] = 'ScalarEncoder' 
~~ if 'encoderType' in fieldInfo : 
~~~ encoderDict [ 'type' ] = fieldInfo [ 'encoderType' ] 
~~ if 'space' in fieldInfo : 
~~~ encoderDict [ 'space' ] = fieldInfo [ 'space' ] 
~~ encoderDictsList . append ( encoderDict ) 
~~~ encoderDict = dict ( type = 'SDRCategoryEncoder' , name = fieldName , 
fieldname = fieldName , n = 100 + width , w = width ) 
if 'encoderType' in fieldInfo : 
~~~ encoderDict = dict ( type = 'DateEncoder' , name = '%s_timeOfDay' % ( fieldName ) , 
fieldname = fieldName , timeOfDay = ( width , 1 ) ) 
encoderDict = dict ( type = 'DateEncoder' , name = '%s_dayOfWeek' % ( fieldName ) , 
fieldname = fieldName , dayOfWeek = ( width , 1 ) ) 
encoderDict = dict ( type = 'DateEncoder' , name = '%s_weekend' % ( fieldName ) , 
fieldname = fieldName , weekend = ( width ) ) 
~~ if fieldName == classifierOnlyField : 
~~~ clEncoderDict = dict ( encoderDict ) 
clEncoderDict [ 'classifierOnly' ] = True 
clEncoderDict [ 'name' ] = '_classifierInput' 
encoderDictsList . append ( clEncoderDict ) 
if options [ "inferenceArgs" ] [ "inputPredictedField" ] == "no" : 
~~~ encoderDictsList . remove ( encoderDict ) 
~~ ~~ ~~ if options . get ( 'fixedFields' ) is not None : 
~~~ tempList = [ ] 
for encoderDict in encoderDictsList : 
~~~ if encoderDict [ 'name' ] in options [ 'fixedFields' ] : 
~~~ tempList . append ( encoderDict ) 
~~ ~~ encoderDictsList = tempList 
~~~ if encoderDict [ 'name' ] . find ( '\\\\' ) >= 0 : 
~~ for c in _ILLEGAL_FIELDNAME_CHARACTERS : 
~~~ if encoderDict [ 'name' ] . find ( c ) >= 0 : 
~~ ~~ constructorStr = _generatePermEncoderStr ( options , encoderDict ) 
encoderKey = _quoteAndEscape ( encoderDict [ 'name' ] ) 
encoderKey , 
pprint . pformat ( encoderDict , indent = 2 * _INDENT_STEP ) ) ) 
permEncoderChoicesStr = '\\n' . join ( permEncoderChoicesList ) 
indentFirstLine = True ) 
~~ def _handleJAVAParameters ( options ) : 
if 'inferenceType' not in options : 
~~~ prediction = options . get ( 'prediction' , { InferenceType . TemporalNextStep : 
{ 'optimize' : True } } ) 
inferenceType = None 
for infType , value in prediction . iteritems ( ) : 
~~~ if value [ 'optimize' ] : 
~~~ inferenceType = infType 
~~ ~~ if inferenceType == 'temporal' : 
~~~ inferenceType = InferenceType . TemporalNextStep 
~~ if inferenceType != InferenceType . TemporalNextStep : 
~~ options [ 'inferenceType' ] = inferenceType 
~~ if 'predictionField' in options : 
~~~ if 'inferenceArgs' not in options : 
~~~ options [ 'inferenceArgs' ] = { 'predictedField' : options [ 'predictionField' ] } 
~~ elif 'predictedField' not in options [ 'inferenceArgs' ] : 
~~~ options [ 'inferenceArgs' ] [ 'predictedField' ] = options [ 'predictionField' ] 
~~ ~~ ~~ def _getPropertyValue ( schema , propertyName , options ) : 
if propertyName not in options : 
~~~ paramsSchema = schema [ 'properties' ] [ propertyName ] 
if 'default' in paramsSchema : 
~~~ options [ propertyName ] = paramsSchema [ 'default' ] 
~~~ options [ propertyName ] = None 
~~ ~~ ~~ def _getExperimentDescriptionSchema ( ) : 
installPath = os . path . dirname ( os . path . abspath ( __file__ ) ) 
schemaFilePath = os . path . join ( installPath , "experimentDescriptionSchema.json" ) 
return json . loads ( open ( schemaFilePath , 'r' ) . read ( ) ) 
~~ def _generateExperiment ( options , outputDirPath , hsVersion , 
_gExperimentDescriptionSchema = _getExperimentDescriptionSchema ( ) 
~~~ validictory . validate ( options , _gExperimentDescriptionSchema ) 
~~ streamSchema = json . load ( resource_stream ( jsonschema . __name__ , 
'stream_def.json' ) ) 
~~~ validictory . validate ( options [ 'streamDef' ] , streamSchema ) 
~~ _handleJAVAParameters ( options ) 
for propertyName in _gExperimentDescriptionSchema [ 'properties' ] : 
~~~ _getPropertyValue ( _gExperimentDescriptionSchema , propertyName , options ) 
~~ if options [ 'inferenceArgs' ] is not None : 
~~~ infArgs = _gExperimentDescriptionSchema [ 'properties' ] [ 'inferenceArgs' ] 
for schema in infArgs [ 'type' ] : 
~~~ if isinstance ( schema , dict ) : 
~~~ for propertyName in schema [ 'properties' ] : 
~~~ _getPropertyValue ( schema , propertyName , options [ 'inferenceArgs' ] ) 
~~ ~~ ~~ ~~ if options [ 'anomalyParams' ] is not None : 
~~~ anomalyArgs = _gExperimentDescriptionSchema [ 'properties' ] [ 'anomalyParams' ] 
for schema in anomalyArgs [ 'type' ] : 
~~~ _getPropertyValue ( schema , propertyName , options [ 'anomalyParams' ] ) 
~~ ~~ ~~ ~~ predictionSteps = options [ 'inferenceArgs' ] . get ( 'predictionSteps' , None ) 
if options [ 'inferenceType' ] == InferenceType . NontemporalClassification : 
~~~ if predictionSteps is not None and predictionSteps != [ 0 ] : 
~~ ~~ if predictionSteps == [ 0 ] and options [ 'inferenceType' ] in [ 'NontemporalMultiStep' , 
'TemporalMultiStep' , 
'MultiStep' ] : 
~~~ options [ 'inferenceType' ] = InferenceType . NontemporalClassification 
~~ if options [ "inferenceType" ] == InferenceType . NontemporalClassification : 
~~~ if options [ "inferenceArgs" ] [ "inputPredictedField" ] == "yes" or options [ "inferenceArgs" ] [ "inputPredictedField" ] == "auto" : 
~~ options [ "inferenceArgs" ] [ "inputPredictedField" ] = "no" 
~~ swarmSize = options [ 'swarmSize' ] 
if swarmSize is None : 
~~~ if options [ "inferenceArgs" ] [ "inputPredictedField" ] is None : 
~~~ options [ "inferenceArgs" ] [ "inputPredictedField" ] = "auto" 
~~ ~~ elif swarmSize == 'small' : 
~~~ if options [ 'minParticlesPerSwarm' ] is None : 
~~~ options [ 'minParticlesPerSwarm' ] = 3 
~~ if options [ 'iterationCount' ] is None : 
~~~ options [ 'iterationCount' ] = 100 
~~ if options [ 'maxModels' ] is None : 
~~~ options [ 'maxModels' ] = 1 
~~ if options [ "inferenceArgs" ] [ "inputPredictedField" ] is None : 
~~~ options [ "inferenceArgs" ] [ "inputPredictedField" ] = "yes" 
~~ ~~ elif swarmSize == 'medium' : 
~~~ options [ 'minParticlesPerSwarm' ] = 5 
~~~ options [ 'iterationCount' ] = 4000 
~~~ options [ 'maxModels' ] = 200 
~~ ~~ elif swarmSize == 'large' : 
~~~ options [ 'minParticlesPerSwarm' ] = 15 
~~ options [ 'tryAll3FieldCombinationsWTimestamps' ] = True 
if options [ "inferenceArgs" ] [ "inputPredictedField" ] is None : 
~~ tokenReplacements = dict ( ) 
#-------------------------------------------------------------------------- 
includedFields = options [ 'includedFields' ] 
if hsVersion == 'v1' : 
~~~ ( encoderSpecsStr , permEncoderChoicesStr ) = _generateEncoderStringsV1 ( includedFields ) 
~~ elif hsVersion in [ 'v2' , 'ensemble' ] : 
~~~ ( encoderSpecsStr , permEncoderChoicesStr ) = _generateEncoderStringsV2 ( includedFields , options ) 
~~ if options [ 'resetPeriod' ] is not None : 
~~~ sensorAutoResetStr = pprint . pformat ( options [ 'resetPeriod' ] , 
indent = 2 * _INDENT_STEP ) 
~~~ sensorAutoResetStr = 'None' 
~~ aggregationPeriod = { 
'days' : 0 , 
'hours' : 0 , 
'microseconds' : 0 , 
'milliseconds' : 0 , 
'minutes' : 0 , 
'months' : 0 , 
'seconds' : 0 , 
'weeks' : 0 , 
'years' : 0 , 
aggFunctionsDict = { } 
if 'aggregation' in options [ 'streamDef' ] : 
~~~ for key in aggregationPeriod . keys ( ) : 
~~~ if key in options [ 'streamDef' ] [ 'aggregation' ] : 
~~~ aggregationPeriod [ key ] = options [ 'streamDef' ] [ 'aggregation' ] [ key ] 
~~ ~~ if 'fields' in options [ 'streamDef' ] [ 'aggregation' ] : 
~~~ for ( fieldName , func ) in options [ 'streamDef' ] [ 'aggregation' ] [ 'fields' ] : 
~~~ aggFunctionsDict [ fieldName ] = str ( func ) 
~~ ~~ ~~ hasAggregation = False 
for v in aggregationPeriod . values ( ) : 
~~~ if v != 0 : 
~~~ hasAggregation = True 
~~ ~~ aggFunctionList = aggFunctionsDict . items ( ) 
aggregationInfo = dict ( aggregationPeriod ) 
aggregationInfo [ 'fields' ] = aggFunctionList 
aggregationInfoStr = "%s" % ( pprint . pformat ( aggregationInfo , 
indent = 2 * _INDENT_STEP ) ) 
datasetSpec = options [ 'streamDef' ] 
if 'aggregation' in datasetSpec : 
~~~ datasetSpec . pop ( 'aggregation' ) 
~~ if hasAggregation : 
~~~ datasetSpec [ 'aggregation' ] = '$SUBSTITUTE' 
~~ datasetSpecStr = pprint . pformat ( datasetSpec , indent = 2 * _INDENT_STEP ) 
datasetSpecStr = datasetSpecStr . replace ( 
"\ , "config[\ ) 
datasetSpecStr = _indentLines ( datasetSpecStr , 2 , indentFirstLine = False ) 
computeInterval = options [ 'computeInterval' ] 
if computeInterval is not None and options [ 'inferenceType' ] in [ 'NontemporalMultiStep' , 
~~~ predictionSteps = options [ 'inferenceArgs' ] . get ( 'predictionSteps' , [ 1 ] ) 
if len ( predictionSteps ) > 1 : 
~~ if max ( aggregationInfo . values ( ) ) == 0 : 
~~ numSteps = predictionSteps [ 0 ] 
predictAheadTime = dict ( aggregationPeriod ) 
for key in predictAheadTime . iterkeys ( ) : 
~~~ predictAheadTime [ key ] *= numSteps 
~~ predictAheadTimeStr = pprint . pformat ( predictAheadTime , 
options [ 'dynamicPredictionSteps' ] = True 
~~~ options [ 'dynamicPredictionSteps' ] = False 
predictAheadTimeStr = "None" 
~~ tokenReplacements [ '\\$EXP_GENERATOR_PROGRAM_PATH' ] = _quoteAndEscape ( os . path . abspath ( __file__ ) ) 
inferenceType = options [ 'inferenceType' ] 
if inferenceType == 'MultiStep' : 
~~~ inferenceType = InferenceType . TemporalMultiStep 
~~ tokenReplacements [ '\\$INFERENCE_TYPE' ] = "\ % inferenceType 
if inferenceType == InferenceType . NontemporalClassification : 
~~~ tokenReplacements [ '\\$SP_ENABLE' ] = "False" 
tokenReplacements [ '\\$TP_ENABLE' ] = "False" 
~~~ tokenReplacements [ '\\$SP_ENABLE' ] = "True" 
tokenReplacements [ '\\$TP_ENABLE' ] = "True" 
tokenReplacements [ '\\$CLA_CLASSIFIER_IMPL' ] = "" 
~~ tokenReplacements [ '\\$ANOMALY_PARAMS' ] = pprint . pformat ( 
options [ 'anomalyParams' ] , indent = 2 * _INDENT_STEP ) 
tokenReplacements [ '\\$ENCODER_SPECS' ] = encoderSpecsStr 
tokenReplacements [ '\\$SENSOR_AUTO_RESET' ] = sensorAutoResetStr 
tokenReplacements [ '\\$AGGREGATION_INFO' ] = aggregationInfoStr 
tokenReplacements [ '\\$DATASET_SPEC' ] = datasetSpecStr 
if options [ 'iterationCount' ] is None : 
~~~ options [ 'iterationCount' ] = - 1 
~~ tokenReplacements [ '\\$ITERATION_COUNT' ] = str ( options [ 'iterationCount' ] ) 
tokenReplacements [ '\\$SP_POOL_PCT' ] = str ( options [ 'spCoincInputPoolPct' ] ) 
tokenReplacements [ '\\$HS_MIN_PARTICLES' ] = str ( options [ 'minParticlesPerSwarm' ] ) 
tokenReplacements [ '\\$SP_PERM_CONNECTED' ] = str ( options [ 'spSynPermConnected' ] ) 
tokenReplacements [ '\\$FIELD_PERMUTATION_LIMIT' ] = str ( options [ 'fieldPermutationLimit' ] ) 
tokenReplacements [ '\\$PERM_ENCODER_CHOICES' ] = permEncoderChoicesStr 
predictionSteps = options [ 'inferenceArgs' ] . get ( 'predictionSteps' , [ 1 ] ) 
predictionStepsStr = ',' . join ( [ str ( x ) for x in predictionSteps ] ) 
tokenReplacements [ '\\$PREDICTION_STEPS' ] = "\ % ( predictionStepsStr ) 
tokenReplacements [ '\\$PREDICT_AHEAD_TIME' ] = predictAheadTimeStr 
tokenReplacements [ '\\$PERM_SP_CHOICES' ] = "" 
if options [ 'spPermuteDecrement' ] and options [ 'inferenceType' ] != 'NontemporalClassification' : 
~~~ tokenReplacements [ '\\$PERM_SP_CHOICES' ] = _ONE_INDENT + "\ 
~~ if options [ 'inferenceType' ] in [ 'NontemporalMultiStep' , 
'NontemporalClassification' ] : 
~~~ tokenReplacements [ '\\$PERM_TP_CHOICES' ] = "" 
~~ if options [ 'inferenceType' ] == 'MultiStep' : 
~~~ tokenReplacements [ '\\$PERM_INFERENCE_TYPE_CHOICES' ] = "" 
~~ if options [ 'inferenceType' ] in [ 'NontemporalMultiStep' , 'TemporalMultiStep' , 
'MultiStep' , 'TemporalAnomaly' , 
~~~ tokenReplacements [ '\\$PERM_CL_CHOICES' ] = "" 
if options . get ( 'minFieldContribution' , None ) is not None : 
~~~ tokenReplacements [ '\\$PERM_MIN_FIELD_CONTRIBUTION' ] = "" 
~~ if options . get ( 'killUselessSwarms' , None ) is not None : 
~~~ tokenReplacements [ '\\$PERM_KILL_USELESS_SWARMS' ] = "" 
~~ if options . get ( 'maxFieldBranching' , None ) is not None : 
~~~ tokenReplacements [ '\\$PERM_MAX_FIELD_BRANCHING' ] = "" 
~~ if options . get ( 'tryAll3FieldCombinations' , None ) is not None : 
~~~ tokenReplacements [ '\\$PERM_TRY_ALL_3_FIELD_COMBINATIONS' ] = "" 
~~ if options . get ( 'tryAll3FieldCombinationsWTimestamps' , None ) is not None : 
~~~ tokenReplacements [ '\\$PERM_TRY_ALL_3_FIELD_COMBINATIONS_W_TIMESTAMPS' ] = "" 
~~ if options . get ( 'fixedFields' , None ) is not None : 
~~~ tokenReplacements [ '\\$PERM_FIXED_FIELDS' ] = "" 
~~ if options . get ( 'fastSwarmModelParams' , None ) is not None : 
~~~ tokenReplacements [ '\\$PERM_FAST_SWARM_MODEL_PARAMS' ] = "" 
~~ if options . get ( 'maxModels' , None ) is not None : 
~~~ tokenReplacements [ '\\$PERM_MAX_MODELS' ] = "" 
~~ if options [ 'dynamicPredictionSteps' ] : 
~~~ debugAgg = True 
quotient = aggregationDivide ( computeInterval , aggregationPeriod ) 
( isInt , multiple ) = _isInt ( quotient ) 
if not isInt or multiple < 1 : 
~~ mTimesN = float ( predictionSteps [ 0 ] ) 
possibleNs = [ ] 
for n in xrange ( 1 , int ( mTimesN ) + 1 ) : 
~~~ m = mTimesN / n 
mInt = int ( round ( m ) ) 
if mInt < 1 : 
~~ if abs ( m - mInt ) > 0.0001 * m : 
~~ possibleNs . append ( n ) 
~~ if debugAgg : 
~~ aggChoices = [ ] 
for n in possibleNs : 
~~~ agg = dict ( aggregationPeriod ) 
for key in agg . iterkeys ( ) : 
~~~ agg [ key ] *= n 
~~ quotient = aggregationDivide ( computeInterval , agg ) 
~~ aggChoices . append ( agg ) 
~~ aggChoices = aggChoices [ - 5 : ] 
if debugAgg : 
for agg in aggChoices : 
~~ print 
~~ tokenReplacements [ '\\$PERM_AGGREGATION_CHOICES' ] = ( 
"PermuteChoices(%s)" % ( 
pprint . pformat ( aggChoices , indent = 2 * _INDENT_STEP ) ) ) 
~~~ tokenReplacements [ '\\$PERM_AGGREGATION_CHOICES' ] = aggregationInfoStr 
~~ _generateInferenceArgs ( options , tokenReplacements ) 
_generateMetricsSubstitutions ( options , tokenReplacements ) 
environment = options [ 'environment' ] 
if environment == OpfEnvironment . Nupic : 
~~~ tokenReplacements [ '\\$ENVIRONMENT' ] = "\ % OpfEnvironment . Nupic 
controlTemplate = "nupicEnvironmentTemplate.tpl" 
~~ elif environment == OpfEnvironment . Experiment : 
~~~ tokenReplacements [ '\\$ENVIRONMENT' ] = "\ % OpfEnvironment . Experiment 
controlTemplate = "opfExperimentTemplate.tpl" 
~~ if outputDirPath is None : 
~~~ outputDirPath = tempfile . mkdtemp ( ) 
~~ if not os . path . exists ( outputDirPath ) : 
~~~ os . makedirs ( outputDirPath ) 
descriptionPyPath = os . path . join ( outputDirPath , "description.py" ) 
_generateFileFromTemplates ( [ claDescriptionTemplateFile , controlTemplate ] , 
descriptionPyPath , 
tokenReplacements ) 
permutationsPyPath = os . path . join ( outputDirPath , "permutations.py" ) 
~~~ _generateFileFromTemplates ( [ 'permutationsTemplateV1.tpl' ] , permutationsPyPath , 
~~ elif hsVersion == 'ensemble' : 
~~~ _generateFileFromTemplates ( [ 'permutationsTemplateEnsemble.tpl' ] , permutationsPyPath , 
~~ elif hsVersion == 'v2' : 
~~~ _generateFileFromTemplates ( [ 'permutationsTemplateV2.tpl' ] , permutationsPyPath , 
hsVersion ) ) 
~~ print "done." 
~~ def _generateMetricsSubstitutions ( options , tokenReplacements ) : 
options [ 'loggedMetrics' ] = [ ".*" ] 
metricList , optimizeMetricLabel = _generateMetricSpecs ( options ) 
metricListString = ",\\n" . join ( metricList ) 
metricListString = _indentLines ( metricListString , 2 , indentFirstLine = False ) 
permOptimizeSettingStr = \ % optimizeMetricLabel 
for ptrn in options [ 'loggedMetrics' ] ] ) ) 
tokenReplacements [ '\\$LOGGED_METRICS' ] = loggedMetricsListAsStr 
tokenReplacements [ '\\$METRICS' ] = metricListString 
tokenReplacements [ '\\$PERM_OPTIMIZE_SETTING' ] = permOptimizeSettingStr 
~~ def _generateMetricSpecs ( options ) : 
inferenceArgs = options [ 'inferenceArgs' ] 
predictionSteps = inferenceArgs [ 'predictionSteps' ] 
metricWindow = options [ 'metricWindow' ] 
if metricWindow is None : 
~~~ metricWindow = int ( Configuration . get ( "nupic.opf.metricWindow" ) ) 
~~ metricSpecStrings = [ ] 
optimizeMetricLabel = "" 
metricSpecStrings . extend ( _generateExtraMetricSpecs ( options ) ) 
optimizeMetricSpec = None 
if options [ 'dynamicPredictionSteps' ] : 
~~~ assert len ( predictionSteps ) == 1 
predictionSteps = [ '$REPLACE_ME' ] 
~~ if inferenceType in ( InferenceType . TemporalNextStep , 
InferenceType . TemporalAnomaly , 
InferenceType . TemporalMultiStep , 
InferenceType . NontemporalMultiStep , 
InferenceType . NontemporalClassification , 
'MultiStep' ) : 
~~~ predictedFieldName , predictedFieldType = _getPredictedField ( options ) 
isCategory = _isCategory ( predictedFieldType ) 
metricNames = ( 'avg_err' , ) if isCategory else ( 'aae' , 'altMAPE' ) 
trivialErrorMetric = 'avg_err' if isCategory else 'altMAPE' 
oneGramErrorMetric = 'avg_err' if isCategory else 'altMAPE' 
movingAverageBaselineName = 'moving_mode' if isCategory else 'moving_mean' 
for metricName in metricNames : 
~~~ metricSpec , metricLabel = _generateMetricSpecString ( field = predictedFieldName , 
inferenceElement = InferenceElement . multiStepBestPredictions , 
metric = 'multiStep' , 
params = { 'errorMetric' : metricName , 
'window' : metricWindow , 
'steps' : predictionSteps } , 
returnLabel = True ) 
metricSpecStrings . append ( metricSpec ) 
~~ if options [ "customErrorMetric" ] is not None : 
~~~ metricParams = dict ( options [ "customErrorMetric" ] ) 
metricParams [ 'errorMetric' ] = 'custom_error_metric' 
metricParams [ 'steps' ] = predictionSteps 
if not "errorWindow" in metricParams : 
~~~ metricParams [ "errorWindow" ] = metricWindow 
~~ metricSpec , metricLabel = _generateMetricSpecString ( field = predictedFieldName , 
inferenceElement = InferenceElement . multiStepPredictions , 
metric = "multiStep" , 
params = metricParams , 
~~ optimizeMetricSpec = metricSpec 
metricLabel = metricLabel . replace ( '[' , '\\\\[' ) 
metricLabel = metricLabel . replace ( ']' , '\\\\]' ) 
optimizeMetricLabel = metricLabel 
if options [ "customErrorMetric" ] is not None : 
~~~ optimizeMetricLabel = ".*custom_error_metric.*" 
~~ if options [ "runBaselines" ] and inferenceType != InferenceType . NontemporalClassification : 
~~~ for steps in predictionSteps : 
~~~ metricSpecStrings . append ( 
_generateMetricSpecString ( field = predictedFieldName , 
inferenceElement = InferenceElement . prediction , 
metric = "trivial" , 
params = { 'window' : metricWindow , 
"errorMetric" : trivialErrorMetric , 
'steps' : steps } ) 
#metricSpecStrings.append( 
if isCategory : 
metric = movingAverageBaselineName , 
params = { 'window' : metricWindow 
, "errorMetric" : "avg_err" , 
"mode_window" : 200 , 
"steps" : steps } ) 
, "errorMetric" : "altMAPE" , 
"mean_window" : 200 , 
~~ ~~ ~~ ~~ elif inferenceType in ( InferenceType . TemporalClassification ) : 
~~~ metricName = 'avg_err' 
trivialErrorMetric = 'avg_err' 
oneGramErrorMetric = 'avg_err' 
movingAverageBaselineName = 'moving_mode' 
optimizeMetricSpec , optimizeMetricLabel = _generateMetricSpecString ( inferenceElement = InferenceElement . classification , 
metric = metricName , 
params = { 'window' : metricWindow } , 
metricSpecStrings . append ( optimizeMetricSpec ) 
if options [ "runBaselines" ] : 
~~~ if inferenceType == InferenceType . TemporalClassification : 
_generateMetricSpecString ( inferenceElement = InferenceElement . classification , 
"errorMetric" : trivialErrorMetric } ) 
metricSpecStrings . append ( 
metric = "two_gram" , 
"errorMetric" : oneGramErrorMetric } ) 
"mode_window" : 200 } ) 
~~ ~~ if not options [ "customErrorMetric" ] == None : 
~~~ if not "errorWindow" in options [ "customErrorMetric" ] : 
~~~ options [ "customErrorMetric" ] [ "errorWindow" ] = metricWindow 
~~ optimizeMetricSpec = _generateMetricSpecString ( 
inferenceElement = InferenceElement . classification , 
metric = "custom" , 
params = options [ "customErrorMetric" ] ) 
optimizeMetricLabel = ".*custom_error_metric.*" 
~~ ~~ if options [ 'dynamicPredictionSteps' ] : 
~~~ for i in range ( len ( metricSpecStrings ) ) : 
~~~ metricSpecStrings [ i ] = metricSpecStrings [ i ] . replace ( 
"\ , "predictionSteps" ) 
~~ optimizeMetricLabel = optimizeMetricLabel . replace ( 
"\ , ".*" ) 
~~ return metricSpecStrings , optimizeMetricLabel 
~~ def _generateExtraMetricSpecs ( options ) : 
_metricSpecSchema = { 'properties' : { } } 
for metric in options [ 'metrics' ] : 
~~~ for propertyName in _metricSpecSchema [ 'properties' ] . keys ( ) : 
~~~ _getPropertyValue ( _metricSpecSchema , propertyName , metric ) 
~~ specString , label = _generateMetricSpecString ( 
field = metric [ 'field' ] , 
metric = metric [ 'metric' ] , 
params = metric [ 'params' ] , 
inferenceElement = metric [ 'inferenceElement' ] , 
if metric [ 'logged' ] : 
~~~ options [ 'loggedMetrics' ] . append ( label ) 
~~ results . append ( specString ) 
~~ def _getPredictedField ( options ) : 
if not options [ 'inferenceArgs' ] or not options [ 'inferenceArgs' ] [ 'predictedField' ] : 
~~~ return None , None 
~~ predictedField = options [ 'inferenceArgs' ] [ 'predictedField' ] 
predictedFieldInfo = None 
for info in includedFields : 
~~~ if info [ 'fieldName' ] == predictedField : 
~~~ predictedFieldInfo = info 
~~ ~~ if predictedFieldInfo is None : 
~~ predictedFieldType = predictedFieldInfo [ 'fieldType' ] 
return predictedField , predictedFieldType 
~~ def _generateInferenceArgs ( options , tokenReplacements ) : 
optionInferenceArgs = options . get ( 'inferenceArgs' , None ) 
resultInferenceArgs = { } 
predictedField = _getPredictedField ( options ) [ 0 ] 
if inferenceType in ( InferenceType . TemporalNextStep , 
InferenceType . TemporalAnomaly ) : 
~~ if optionInferenceArgs : 
~~~ if options [ 'dynamicPredictionSteps' ] : 
~~~ altOptionInferenceArgs = copy . deepcopy ( optionInferenceArgs ) 
altOptionInferenceArgs [ 'predictionSteps' ] = '$REPLACE_ME' 
resultInferenceArgs = pprint . pformat ( altOptionInferenceArgs ) 
resultInferenceArgs = resultInferenceArgs . replace ( "\ , 
'[predictionSteps]' ) 
~~~ resultInferenceArgs = pprint . pformat ( optionInferenceArgs ) 
~~ ~~ tokenReplacements [ '\\$INFERENCE_ARGS' ] = resultInferenceArgs 
tokenReplacements [ '\\$PREDICTION_FIELD' ] = predictedField 
~~ def expGenerator ( args ) : 
parser = OptionParser ( ) 
parser . add_option ( "--description" , dest = "description" , 
parser . add_option ( "--descriptionFromFile" , dest = 'descriptionFromFile' , 
parser . add_option ( "--claDescriptionTemplateFile" , 
dest = 'claDescriptionTemplateFile' , 
default = 'claDescriptionTemplate.tpl' , 
parser . add_option ( "--showSchema" , 
action = "store_true" , dest = "showSchema" , 
parser . add_option ( "--version" , dest = 'version' , default = 'v2' , 
parser . add_option ( "--outDir" , 
dest = "outDir" , default = None , 
( options , remainingArgs ) = parser . parse_args ( args ) 
if len ( remainingArgs ) > 0 : 
~~ activeOptions = filter ( lambda x : getattr ( options , x ) != None , 
( 'description' , 'showSchema' ) ) 
if len ( activeOptions ) > 1 : 
parser . get_usage ( ) ) ) 
~~ if options . showSchema : 
~~~ _handleShowSchemaOption ( ) 
~~ elif options . description : 
~~~ _handleDescriptionOption ( options . description , options . outDir , 
parser . get_usage ( ) , hsVersion = options . version , 
claDescriptionTemplateFile = options . claDescriptionTemplateFile ) 
~~ elif options . descriptionFromFile : 
~~~ _handleDescriptionFromFileOption ( options . descriptionFromFile , 
options . outDir , parser . get_usage ( ) , hsVersion = options . version , 
"provided:\\n" , parser . get_usage ( ) ) ) 
~~ ~~ def parseTimestamp ( s ) : 
s = s . strip ( ) 
for pattern in DATETIME_FORMATS : 
~~~ return datetime . datetime . strptime ( s , pattern ) 
~~ def parseBool ( s ) : 
l = s . lower ( ) 
if l in ( "true" , "t" , "1" ) : 
~~ if l in ( "false" , "f" , "0" ) : 
~~ def escape ( s ) : 
if s is None : 
s = s . replace ( '\\\\' , '\\\\\\\\' ) 
s = s . replace ( '\\n' , '\\\\n' ) 
s = s . replace ( '\\t' , '\\\\t' ) 
s = s . replace ( ',' , '\\t' ) 
~~ def unescape ( s ) : 
assert isinstance ( s , basestring ) 
s = s . replace ( '\\t' , ',' ) 
s = s . replace ( '\\\\,' , ',' ) 
s = s . replace ( '\\\\n' , '\\n' ) 
s = s . replace ( '\\\\\\\\' , '\\\\' ) 
~~ def parseSdr ( s ) : 
sdr = [ int ( c ) for c in s if c in ( "0" , "1" ) ] 
if len ( sdr ) != len ( s ) : 
~~ def parseStringList ( s ) : 
return [ int ( i ) for i in s . split ( ) ] 
~~ def coordinatesFromIndex ( index , dimensions ) : 
coordinates = [ 0 ] * len ( dimensions ) 
shifted = index 
for i in xrange ( len ( dimensions ) - 1 , 0 , - 1 ) : 
~~~ coordinates [ i ] = shifted % dimensions [ i ] 
shifted = shifted / dimensions [ i ] 
~~ coordinates [ 0 ] = shifted 
return coordinates 
~~ def indexFromCoordinates ( coordinates , dimensions ) : 
index = 0 
for i , dimension in enumerate ( dimensions ) : 
~~~ index *= dimension 
index += coordinates [ i ] 
~~ return index 
~~ def neighborhood ( centerIndex , radius , dimensions ) : 
centerPosition = coordinatesFromIndex ( centerIndex , dimensions ) 
intervals = [ ] 
~~~ left = max ( 0 , centerPosition [ i ] - radius ) 
right = min ( dimension - 1 , centerPosition [ i ] + radius ) 
intervals . append ( xrange ( left , right + 1 ) ) 
~~ coords = numpy . array ( list ( itertools . product ( * intervals ) ) ) 
return numpy . ravel_multi_index ( coords . T , dimensions ) 
~~ def encodeIntoArray ( self , inputData , output ) : 
( coordinate , radius ) = inputData 
. format ( radius , type ( radius ) ) ) 
neighbors = self . _neighbors ( coordinate , radius ) 
winners = self . _topWCoordinates ( neighbors , self . w ) 
bitFn = lambda coordinate : self . _bitForCoordinate ( coordinate , self . n ) 
indices = numpy . array ( [ bitFn ( w ) for w in winners ] ) 
output [ : ] = 0 
output [ indices ] = 1 
~~ def _neighbors ( coordinate , radius ) : 
ranges = ( xrange ( n - radius , n + radius + 1 ) for n in coordinate . tolist ( ) ) 
return numpy . array ( list ( itertools . product ( * ranges ) ) ) 
~~ def _topWCoordinates ( cls , coordinates , w ) : 
orders = numpy . array ( [ cls . _orderForCoordinate ( c ) 
for c in coordinates . tolist ( ) ] ) 
indices = numpy . argsort ( orders ) [ - w : ] 
return coordinates [ indices ] 
~~ def _hashCoordinate ( coordinate ) : 
coordinateStr = "," . join ( str ( v ) for v in coordinate ) 
hash = int ( int ( hashlib . md5 ( coordinateStr ) . hexdigest ( ) , 16 ) % ( 2 ** 64 ) ) 
return hash 
~~ def _orderForCoordinate ( cls , coordinate ) : 
seed = cls . _hashCoordinate ( coordinate ) 
rng = Random ( seed ) 
return rng . getReal64 ( ) 
~~ def _bitForCoordinate ( cls , coordinate , n ) : 
return rng . getUInt32 ( n ) 
~~ def binSearch ( arr , val ) : 
i = bisect_left ( arr , val ) 
if i != len ( arr ) and arr [ i ] == val : 
~~~ return i 
~~ return - 1 
~~ def createSegment ( self , cell ) : 
cellData = self . _cells [ cell ] 
if len ( self . _freeFlatIdxs ) > 0 : 
~~~ flatIdx = self . _freeFlatIdxs . pop ( ) 
~~~ flatIdx = self . _nextFlatIdx 
self . _segmentForFlatIdx . append ( None ) 
self . _nextFlatIdx += 1 
~~ ordinal = self . _nextSegmentOrdinal 
self . _nextSegmentOrdinal += 1 
segment = Segment ( cell , flatIdx , ordinal ) 
cellData . _segments . append ( segment ) 
self . _segmentForFlatIdx [ flatIdx ] = segment 
return segment 
~~ def destroySegment ( self , segment ) : 
for synapse in segment . _synapses : 
~~~ self . _removeSynapseFromPresynapticMap ( synapse ) 
~~ self . _numSynapses -= len ( segment . _synapses ) 
segments = self . _cells [ segment . cell ] . _segments 
i = segments . index ( segment ) 
del segments [ i ] 
self . _freeFlatIdxs . append ( segment . flatIdx ) 
self . _segmentForFlatIdx [ segment . flatIdx ] = None 
~~ def createSynapse ( self , segment , presynapticCell , permanence ) : 
idx = len ( segment . _synapses ) 
synapse = Synapse ( segment , presynapticCell , permanence , 
self . _nextSynapseOrdinal ) 
self . _nextSynapseOrdinal += 1 
segment . _synapses . add ( synapse ) 
self . _synapsesForPresynapticCell [ presynapticCell ] . add ( synapse ) 
self . _numSynapses += 1 
return synapse 
~~ def destroySynapse ( self , synapse ) : 
self . _numSynapses -= 1 
self . _removeSynapseFromPresynapticMap ( synapse ) 
synapse . segment . _synapses . remove ( synapse ) 
~~ def computeActivity ( self , activePresynapticCells , connectedPermanence ) : 
numActiveConnectedSynapsesForSegment = [ 0 ] * self . _nextFlatIdx 
numActivePotentialSynapsesForSegment = [ 0 ] * self . _nextFlatIdx 
threshold = connectedPermanence - EPSILON 
for cell in activePresynapticCells : 
~~~ for synapse in self . _synapsesForPresynapticCell [ cell ] : 
~~~ flatIdx = synapse . segment . flatIdx 
numActivePotentialSynapsesForSegment [ flatIdx ] += 1 
if synapse . permanence > threshold : 
~~~ numActiveConnectedSynapsesForSegment [ flatIdx ] += 1 
~~ ~~ ~~ return ( numActiveConnectedSynapsesForSegment , 
numActivePotentialSynapsesForSegment ) 
~~ def numSegments ( self , cell = None ) : 
if cell is not None : 
~~~ return len ( self . _cells [ cell ] . _segments ) 
~~ return self . _nextFlatIdx - len ( self . _freeFlatIdxs ) 
~~ def segmentPositionSortKey ( self , segment ) : 
return segment . cell + ( segment . _ordinal / float ( self . _nextSegmentOrdinal ) ) 
~~ def write ( self , proto ) : 
protoCells = proto . init ( 'cells' , self . numCells ) 
for i in xrange ( self . numCells ) : 
~~~ segments = self . _cells [ i ] . _segments 
protoSegments = protoCells [ i ] . init ( 'segments' , len ( segments ) ) 
for j , segment in enumerate ( segments ) : 
~~~ synapses = segment . _synapses 
protoSynapses = protoSegments [ j ] . init ( 'synapses' , len ( synapses ) ) 
for k , synapse in enumerate ( sorted ( synapses , key = lambda s : s . _ordinal ) ) : 
~~~ protoSynapses [ k ] . presynapticCell = synapse . presynapticCell 
protoSynapses [ k ] . permanence = synapse . permanence 
~~ ~~ ~~ ~~ def read ( cls , proto ) : 
protoCells = proto . cells 
connections = cls ( len ( protoCells ) ) 
for cellIdx , protoCell in enumerate ( protoCells ) : 
~~~ protoCell = protoCells [ cellIdx ] 
protoSegments = protoCell . segments 
connections . _cells [ cellIdx ] = CellData ( ) 
segments = connections . _cells [ cellIdx ] . _segments 
for segmentIdx , protoSegment in enumerate ( protoSegments ) : 
~~~ segment = Segment ( cellIdx , connections . _nextFlatIdx , 
connections . _nextSegmentOrdinal ) 
segments . append ( segment ) 
connections . _segmentForFlatIdx . append ( segment ) 
connections . _nextFlatIdx += 1 
connections . _nextSegmentOrdinal += 1 
synapses = segment . _synapses 
protoSynapses = protoSegment . synapses 
for synapseIdx , protoSynapse in enumerate ( protoSynapses ) : 
~~~ presynapticCell = protoSynapse . presynapticCell 
synapse = Synapse ( segment , presynapticCell , protoSynapse . permanence , 
ordinal = connections . _nextSynapseOrdinal ) 
connections . _nextSynapseOrdinal += 1 
synapses . add ( synapse ) 
connections . _synapsesForPresynapticCell [ presynapticCell ] . add ( synapse ) 
connections . _numSynapses += 1 
~~ ~~ ~~ return connections 
~~ def getString ( cls , prop ) : 
if cls . _properties is None : 
~~~ cls . _readStdConfigFiles ( ) 
~~ envValue = os . environ . get ( "%s%s" % ( cls . envPropPrefix , 
prop . replace ( '.' , '_' ) ) , None ) 
if envValue is not None : 
~~~ return envValue 
~~ return cls . _properties [ prop ] 
~~ def getBool ( cls , prop ) : 
value = cls . getInt ( prop ) 
if value not in ( 0 , 1 ) : 
value , prop ) ) 
~~ return bool ( value ) 
~~ def set ( cls , prop , value ) : 
~~ cls . _properties [ prop ] = str ( value ) 
~~ def dict ( cls ) : 
~~ result = dict ( cls . _properties ) 
keys = os . environ . keys ( ) 
replaceKeys = filter ( lambda x : x . startswith ( cls . envPropPrefix ) , 
keys ) 
for envKey in replaceKeys : 
~~~ key = envKey [ len ( cls . envPropPrefix ) : ] 
key = key . replace ( '_' , '.' ) 
result [ key ] = os . environ [ envKey ] 
~~ def readConfigFile ( cls , filename , path = None ) : 
properties = cls . _readConfigFile ( filename , path ) 
~~~ cls . _properties = dict ( ) 
~~ for name in properties : 
~~~ if 'value' in properties [ name ] : 
~~~ cls . _properties [ name ] = properties [ name ] [ 'value' ] 
~~ ~~ ~~ def findConfigFile ( cls , filename ) : 
paths = cls . getConfigPaths ( ) 
for p in paths : 
~~~ testPath = os . path . join ( p , filename ) 
if os . path . isfile ( testPath ) : 
~~~ return os . path . join ( p , filename ) 
~~ ~~ ~~ def getConfigPaths ( cls ) : 
configPaths = [ ] 
if cls . _configPaths is not None : 
~~~ return cls . _configPaths 
~~~ if 'NTA_CONF_PATH' in os . environ : 
~~~ configVar = os . environ [ 'NTA_CONF_PATH' ] 
configPaths = configVar . split ( os . pathsep ) 
~~ return configPaths 
~~ ~~ def addNoise ( input , noise = 0.1 , doForeground = True , doBackground = True ) : 
if doForeground and doBackground : 
~~~ return numpy . abs ( input - ( numpy . random . random ( input . shape ) < noise ) ) 
~~~ if doForeground : 
~~~ return numpy . logical_and ( input , numpy . random . random ( input . shape ) > noise ) 
~~ if doBackground : 
~~~ return numpy . logical_or ( input , numpy . random . random ( input . shape ) < noise ) 
~~ ~~ return input 
~~ def generateCoincMatrix ( nCoinc = 10 , length = 500 , activity = 50 ) : 
coincMatrix0 = SM32 ( int ( nCoinc ) , int ( length ) ) 
theOnes = numpy . array ( [ 1.0 ] * activity , dtype = numpy . float32 ) 
for rowIdx in xrange ( nCoinc ) : 
~~~ coinc = numpy . array ( random . sample ( xrange ( length ) , 
activity ) , dtype = numpy . uint32 ) 
coinc . sort ( ) 
coincMatrix0 . setRowFromSparse ( rowIdx , coinc , theOnes ) 
~~ coincMatrix = SM32 ( int ( nCoinc ) , int ( length ) ) 
coincMatrix . initializeWithFixedNNZR ( activity ) 
return coincMatrix0 
~~ def generateVectors ( numVectors = 100 , length = 500 , activity = 50 ) : 
vectors = [ ] 
coinc = numpy . zeros ( length , dtype = 'int32' ) 
indexList = range ( length ) 
for i in xrange ( numVectors ) : 
~~~ coinc [ : ] = 0 
coinc [ random . sample ( indexList , activity ) ] = 1 
vectors . append ( coinc . copy ( ) ) 
~~ return vectors 
~~ def generateSimpleSequences ( nCoinc = 10 , seqLength = [ 5 , 6 , 7 ] , nSeq = 100 ) : 
coincList = range ( nCoinc ) 
seqList = [ ] 
for i in xrange ( nSeq ) : 
~~~ if max ( seqLength ) <= nCoinc : 
~~~ seqList . append ( random . sample ( coincList , random . choice ( seqLength ) ) ) 
~~~ len = random . choice ( seqLength ) 
seq = [ ] 
for x in xrange ( len ) : 
~~~ seq . append ( random . choice ( coincList ) ) 
~~ seqList . append ( seq ) 
~~ ~~ return seqList 
~~ def generateHubSequences ( nCoinc = 10 , hubs = [ 2 , 6 ] , seqLength = [ 5 , 6 , 7 ] , nSeq = 100 ) : 
for hub in hubs : 
~~~ coincList . remove ( hub ) 
~~ seqList = [ ] 
~~~ length = random . choice ( seqLength ) - 1 
seq = random . sample ( coincList , length ) 
seq . insert ( length // 2 , random . choice ( hubs ) ) 
seqList . append ( seq ) 
~~ return seqList 
~~ def generateSimpleCoincMatrix ( nCoinc = 10 , length = 500 , activity = 50 ) : 
assert nCoinc * activity <= length , "can\ 
coincMatrix = SM32 ( 0 , length ) 
for i in xrange ( nCoinc ) : 
coinc [ i * activity : ( i + 1 ) * activity ] = 1 
coincMatrix . addRow ( coinc ) 
~~ return coincMatrix 
~~ def generateSequences ( nPatterns = 10 , patternLen = 500 , patternActivity = 50 , 
hubs = [ 2 , 6 ] , seqLength = [ 5 , 6 , 7 ] , 
nSimpleSequences = 50 , nHubSequences = 50 ) : 
patterns = generateCoincMatrix ( nCoinc = nPatterns , length = patternLen , 
activity = patternActivity ) 
seqList = generateSimpleSequences ( nCoinc = nPatterns , seqLength = seqLength , 
nSeq = nSimpleSequences ) + generateHubSequences ( nCoinc = nPatterns , hubs = hubs , seqLength = seqLength , 
nSeq = nHubSequences ) 
return ( seqList , patterns ) 
~~ def generateL2Sequences ( nL1Patterns = 10 , l1Hubs = [ 2 , 6 ] , l1SeqLength = [ 5 , 6 , 7 ] , 
nL1SimpleSequences = 50 , nL1HubSequences = 50 , 
l1Pooling = 4 , perfectStability = False , spHysteresisFactor = 1.0 , 
patternLen = 500 , patternActivity = 50 ) : 
l1SeqList = generateSimpleSequences ( nCoinc = nL1Patterns , seqLength = l1SeqLength , 
nSeq = nL1SimpleSequences ) + generateHubSequences ( nCoinc = nL1Patterns , hubs = l1Hubs , 
seqLength = l1SeqLength , nSeq = nL1HubSequences ) 
spOutput = generateSlowSPOutput ( seqListBelow = l1SeqList , 
poolingTimeBelow = l1Pooling , outputWidth = patternLen , 
activity = patternActivity , perfectStability = perfectStability , 
spHysteresisFactor = spHysteresisFactor ) 
outSeq = None 
outSeqList = [ ] 
outPatterns = SM32 ( 0 , patternLen ) 
for pattern in spOutput : 
~~~ if pattern . sum ( ) == 0 : 
~~~ if outSeq is not None : 
~~~ outSeqList . append ( outSeq ) 
~~ outSeq = [ ] 
~~ patternIdx = None 
if outPatterns . nRows ( ) > 0 : 
~~~ matches = outPatterns . rightVecSumAtNZ ( pattern ) 
outCoinc = matches . argmax ( ) . astype ( 'uint32' ) 
numOnes = pattern . sum ( ) 
if matches [ outCoinc ] == numOnes and outPatterns . getRow ( int ( outCoinc ) ) . sum ( ) == numOnes : 
~~~ patternIdx = outCoinc 
~~ ~~ if patternIdx is None : 
~~~ outPatterns . addRow ( pattern ) 
patternIdx = outPatterns . nRows ( ) - 1 
~~ outSeq . append ( patternIdx ) 
~~ if outSeq is not None : 
~~ return ( outSeqList , outPatterns ) 
~~ def vectorsFromSeqList ( seqList , patternMatrix ) : 
totalLen = 0 
for seq in seqList : 
~~~ totalLen += len ( seq ) 
~~ vectors = numpy . zeros ( ( totalLen , patternMatrix . shape [ 1 ] ) , dtype = 'bool' ) 
vecOffset = 0 
~~~ seq = numpy . array ( seq , dtype = 'uint32' ) 
for idx , coinc in enumerate ( seq ) : 
~~~ vectors [ vecOffset ] = patternMatrix . getRow ( int ( coinc ) ) 
vecOffset += 1 
~~ ~~ return vectors 
~~ def sameTMParams ( tp1 , tp2 ) : 
for param in [ "numberOfCols" , "cellsPerColumn" , "initialPerm" , "connectedPerm" , 
"minThreshold" , "newSynapseCount" , "permanenceInc" , "permanenceDec" , 
"permanenceMax" , "globalDecay" , "activationThreshold" , 
"doPooling" , "segUpdateValidDuration" , 
"burnIn" , "pamLength" , "maxAge" ] : 
~~~ if getattr ( tp1 , param ) != getattr ( tp2 , param ) : 
print getattr ( tp1 , param ) , "vs" , getattr ( tp2 , param ) 
~~ def sameSynapse ( syn , synapses ) : 
for s in synapses : 
~~~ if ( s [ 0 ] == syn [ 0 ] ) and ( s [ 1 ] == syn [ 1 ] ) and ( abs ( s [ 2 ] - syn [ 2 ] ) <= 0.001 ) : 
~~ ~~ return False 
~~ def sameSegment ( seg1 , seg2 ) : 
for field in [ 1 , 2 , 3 , 4 , 5 , 6 ] : 
~~~ if abs ( seg1 [ 0 ] [ field ] - seg2 [ 0 ] [ field ] ) > 0.001 : 
~~~ result = False 
~~ ~~ if len ( seg1 [ 1 : ] ) != len ( seg2 [ 1 : ] ) : 
~~ for syn in seg2 [ 1 : ] : 
~~~ if syn [ 2 ] <= 0 : 
~~ ~~ if result == True : 
~~~ for syn in seg1 [ 1 : ] : 
~~ res = sameSynapse ( syn , seg2 [ 1 : ] ) 
if res == False : 
~~ def tmDiff ( tm1 , tm2 , verbosity = 0 , relaxSegmentTests = True ) : 
if sameTMParams ( tm1 , tm2 ) == False : 
~~ result = True 
if ( tm1 . activeState [ 't' ] != tm2 . activeState [ 't' ] ) . any ( ) : 
~~ if ( tm1 . predictedState [ 't' ] - tm2 . predictedState [ 't' ] ) . any ( ) : 
~~ if tm1 . getNumSegments ( ) != tm2 . getNumSegments ( ) : 
~~ if tm1 . getNumSynapses ( ) != tm2 . getNumSynapses ( ) : 
tm1 . printCells ( ) 
tm2 . printCells ( ) 
~~ for c in xrange ( tm1 . numberOfCols ) : 
~~~ for i in xrange ( tm2 . cellsPerColumn ) : 
~~~ if tm1 . getNumSegmentsInCell ( c , i ) != tm2 . getNumSegmentsInCell ( c , i ) : 
print tm1 . getNumSegmentsInCell ( c , i ) , tm2 . getNumSegmentsInCell ( c , i ) 
~~ ~~ ~~ if result == True and not relaxSegmentTests : 
~~~ for c in xrange ( tm1 . numberOfCols ) : 
~~~ nSegs = tm1 . getNumSegmentsInCell ( c , i ) 
for segIdx in xrange ( nSegs ) : 
~~~ tm1seg = tm1 . getSegmentOnCell ( c , i , segIdx ) 
res = False 
for tm2segIdx in xrange ( nSegs ) : 
~~~ tm2seg = tm2 . getSegmentOnCell ( c , i , tm2segIdx ) 
if sameSegment ( tm1seg , tm2seg ) == True : 
~~~ res = True 
~~ ~~ if res == False : 
if verbosity >= 1 : 
~~~ print "C++" 
tm1 . printCell ( c , i ) 
print "Py" 
tm2 . printCell ( c , i ) 
~~ result = False 
~~ ~~ ~~ ~~ ~~ if result == True and ( verbosity > 1 ) : 
~~~ print "TM\ 
~~ def tmDiff2 ( tm1 , tm2 , verbosity = 0 , relaxSegmentTests = True , 
checkLearn = True , checkStates = True ) : 
if checkStates : 
~~~ if ( tm1 . infActiveState [ 't' ] != tm2 . infActiveState [ 't' ] ) . any ( ) : 
~~ if ( tm1 . infPredictedState [ 't' ] - tm2 . infPredictedState [ 't' ] ) . any ( ) : 
~~ if checkLearn and ( tm1 . lrnActiveState [ 't' ] - tm2 . lrnActiveState [ 't' ] ) . any ( ) : 
~~ if checkLearn and ( tm1 . lrnPredictedState [ 't' ] - tm2 . lrnPredictedState [ 't' ] ) . any ( ) : 
~~ if checkLearn and abs ( tm1 . getAvgLearnedSeqLength ( ) - tm2 . getAvgLearnedSeqLength ( ) ) > 0.01 : 
~~ ~~ if tm1 . getNumSegments ( ) != tm2 . getNumSegments ( ) : 
if verbosity >= 3 : 
~~ ~~ for c in xrange ( tm1 . numberOfCols ) : 
~~ ~~ ~~ if result == True and not relaxSegmentTests and checkLearn : 
if verbosity >= 0 : 
~~ ~~ ~~ ~~ ~~ ~~ if result == True and ( verbosity > 1 ) : 
~~ def spDiff ( SP1 , SP2 ) : 
if ( len ( SP1 . _masterConnectedM ) != len ( SP2 . _masterConnectedM ) ) : 
~~ if ( len ( SP1 . _masterPotentialM ) != len ( SP2 . _masterPotentialM ) ) : 
~~ if ( len ( SP1 . _masterPermanenceM ) != len ( SP2 . _masterPermanenceM ) ) : 
~~ for i in range ( 0 , len ( SP1 . _masterConnectedM ) ) : 
~~~ connected1 = SP1 . _masterConnectedM [ i ] 
connected2 = SP2 . _masterConnectedM [ i ] 
if ( connected1 != connected2 ) : 
~~ permanences1 = SP1 . _masterPermanenceM [ i ] ; 
permanences2 = SP2 . _masterPermanenceM [ i ] ; 
if ( permanences1 != permanences2 ) : 
~~ potential1 = SP1 . _masterPotentialM [ i ] ; 
potential2 = SP2 . _masterPotentialM [ i ] ; 
if ( potential1 != potential2 ) : 
~~ ~~ if ( not numpy . array_equal ( SP1 . _firingBoostFactors , SP2 . _firingBoostFactors ) ) : 
~~ if ( not numpy . array_equal ( SP1 . _dutyCycleAfterInh , SP2 . _dutyCycleAfterInh ) ) : 
~~ if ( not numpy . array_equal ( SP1 . _dutyCycleBeforeInh , SP2 . _dutyCycleBeforeInh ) ) : 
~~ def removeSeqStarts ( vectors , resets , numSteps = 1 ) : 
if numSteps == 0 : 
~~~ return vectors 
~~ resetIndices = resets . nonzero ( ) [ 0 ] 
removeRows = resetIndices 
for i in range ( numSteps - 1 ) : 
~~~ removeRows = numpy . hstack ( ( removeRows , resetIndices + i + 1 ) ) 
~~ return numpy . delete ( vectors , removeRows , axis = 0 ) 
~~ def _accumulateFrequencyCounts ( values , freqCounts = None ) : 
values = numpy . array ( values ) 
numEntries = values . max ( ) + 1 
if freqCounts is not None : 
~~~ numEntries = max ( numEntries , freqCounts . size ) 
~~ if freqCounts is not None : 
~~~ if freqCounts . size != numEntries : 
~~~ newCounts = numpy . zeros ( numEntries , dtype = 'int32' ) 
newCounts [ 0 : freqCounts . size ] = freqCounts 
~~~ newCounts = freqCounts 
~~ for v in values : 
~~~ newCounts [ v ] += 1 
~~ return newCounts 
~~ def _listOfOnTimesInVec ( vector ) : 
durations = [ ] 
numOnTimes = 0 
totalOnTime = 0 
nonzeros = numpy . array ( vector ) . nonzero ( ) [ 0 ] 
if len ( nonzeros ) == 0 : 
~~~ return ( 0 , 0 , [ ] ) 
~~ if len ( nonzeros ) == 1 : 
~~~ return ( 1 , 1 , [ 1 ] ) 
~~ prev = nonzeros [ 0 ] 
onTime = 1 
endIdx = nonzeros [ - 1 ] 
for idx in nonzeros [ 1 : ] : 
~~~ if idx != prev + 1 : 
~~~ totalOnTime += onTime 
numOnTimes += 1 
durations . append ( onTime ) 
~~~ onTime += 1 
~~ prev = idx 
~~ totalOnTime += onTime 
return ( totalOnTime , numOnTimes , durations ) 
~~ def _fillInOnTimes ( vector , durations ) : 
~~~ durations [ nonzeros [ 0 ] ] = 1 
onStartIdx = prev 
~~~ durations [ onStartIdx : onStartIdx + onTime ] = range ( 1 , onTime + 1 ) 
onStartIdx = idx 
~~ durations [ onStartIdx : onStartIdx + onTime ] = range ( 1 , onTime + 1 ) 
~~ def averageOnTimePerTimestep ( vectors , numSamples = None ) : 
if vectors . ndim == 1 : 
~~~ vectors . shape = ( - 1 , 1 ) 
~~ numTimeSteps = len ( vectors ) 
numElements = len ( vectors [ 0 ] ) 
if numSamples is not None : 
countOn = numpy . random . randint ( 0 , numElements , numSamples ) 
vectors = vectors [ : , countOn ] 
~~ durations = numpy . zeros ( vectors . shape , dtype = 'int32' ) 
for col in xrange ( vectors . shape [ 1 ] ) : 
~~~ _fillInOnTimes ( vectors [ : , col ] , durations [ : , col ] ) 
~~ sums = vectors . sum ( axis = 1 ) 
sums . clip ( min = 1 , max = numpy . inf , out = sums ) 
avgDurations = durations . sum ( axis = 1 , dtype = 'float64' ) / sums 
avgOnTime = avgDurations . sum ( ) / ( avgDurations > 0 ) . sum ( ) 
freqCounts = _accumulateFrequencyCounts ( avgDurations ) 
return ( avgOnTime , freqCounts ) 
~~ def averageOnTime ( vectors , numSamples = None ) : 
if numSamples is None : 
~~~ numSamples = numElements 
countOn = range ( numElements ) 
~~~ countOn = numpy . random . randint ( 0 , numElements , numSamples ) 
~~ sumOfLengths = 0.0 
onTimeFreqCounts = None 
n = 0 
for i in countOn : 
~~~ ( onTime , segments , durations ) = _listOfOnTimesInVec ( vectors [ : , i ] ) 
if onTime != 0.0 : 
~~~ sumOfLengths += onTime 
n += segments 
onTimeFreqCounts = _accumulateFrequencyCounts ( durations , onTimeFreqCounts ) 
~~ ~~ if n > 0 : 
~~~ return ( sumOfLengths / n , onTimeFreqCounts ) 
~~~ return ( 0.0 , onTimeFreqCounts ) 
~~ ~~ def plotOutputsOverTime ( vectors , buVectors = None , title = 'On-times' ) : 
import pylab 
pylab . ion ( ) 
pylab . figure ( ) 
imData = vectors . transpose ( ) 
if buVectors is not None : 
~~~ assert ( buVectors . shape == vectors . shape ) 
imData = imData . copy ( ) 
imData [ buVectors . transpose ( ) . astype ( 'bool' ) ] = 2 
~~ pylab . imshow ( imData , aspect = 'auto' , cmap = pylab . cm . gray_r , 
interpolation = 'nearest' ) 
pylab . title ( title ) 
pylab . bar ( numpy . arange ( len ( freqCounts ) ) - 0.5 , freqCounts ) 
pylab . xlabel ( xLabel ) 
~~ def populationStability ( vectors , numSamples = None ) : 
numVectors = len ( vectors ) 
~~~ numSamples = numVectors - 1 
countOn = range ( numVectors - 1 ) 
~~~ countOn = numpy . random . randint ( 0 , numVectors - 1 , numSamples ) 
~~ sigmap = 0.0 
~~~ match = checkMatch ( vectors [ i ] , vectors [ i + 1 ] , sparse = False ) 
if match [ 1 ] != 0 : 
~~~ sigmap += float ( match [ 0 ] ) / match [ 1 ] 
~~ ~~ return sigmap / numSamples 
~~ def percentOutputsStableOverNTimeSteps ( vectors , numSamples = None ) : 
totalSamples = len ( vectors ) 
windowSize = numSamples 
numWindows = 0 
pctStable = 0 
for wStart in range ( 0 , totalSamples - windowSize + 1 ) : 
~~~ data = vectors [ wStart : wStart + windowSize ] 
outputSums = data . sum ( axis = 0 ) 
stableOutputs = ( outputSums == windowSize ) . sum ( ) 
samplePctStable = float ( stableOutputs ) / data [ 0 ] . sum ( ) 
print samplePctStable 
pctStable += samplePctStable 
numWindows += 1 
~~ return float ( pctStable ) / numWindows 
~~ def computeSaturationLevels ( outputs , outputsShape , sparseForm = False ) : 
if not sparseForm : 
~~~ outputs = outputs . reshape ( outputsShape ) 
spOut = SM32 ( outputs ) 
~~~ if len ( outputs ) > 0 : 
~~~ assert ( outputs . max ( ) < outputsShape [ 0 ] * outputsShape [ 1 ] ) 
~~ spOut = SM32 ( 1 , outputsShape [ 0 ] * outputsShape [ 1 ] ) 
spOut . setRowFromSparse ( 0 , outputs , [ 1 ] * len ( outputs ) ) 
spOut . reshape ( outputsShape [ 0 ] , outputsShape [ 1 ] ) 
~~ regionSize = 15 
rows = xrange ( regionSize + 1 , outputsShape [ 0 ] + 1 , regionSize ) 
cols = xrange ( regionSize + 1 , outputsShape [ 1 ] + 1 , regionSize ) 
regionSums = spOut . nNonZerosPerBox ( rows , cols ) 
( locations , values ) = regionSums . tolist ( ) 
values /= float ( regionSize * regionSize ) 
sat = list ( values ) 
innerSat = [ ] 
locationSet = set ( locations ) 
for ( location , value ) in itertools . izip ( locations , values ) : 
~~~ ( row , col ) = location 
if ( row - 1 , col ) in locationSet and ( row , col - 1 ) in locationSet and ( row + 1 , col ) in locationSet and ( row , col + 1 ) in locationSet : 
~~~ innerSat . append ( value ) 
~~ ~~ return ( sat , innerSat ) 
~~ def checkMatch ( input , prediction , sparse = True , verbosity = 0 ) : 
if sparse : 
~~~ activeElementsInInput = set ( input ) 
activeElementsInPrediction = set ( prediction ) 
~~~ activeElementsInInput = set ( input . nonzero ( ) [ 0 ] ) 
activeElementsInPrediction = set ( prediction . nonzero ( ) [ 0 ] ) 
~~ totalActiveInPrediction = len ( activeElementsInPrediction ) 
totalActiveInInput = len ( activeElementsInInput ) 
foundInInput = len ( activeElementsInPrediction . intersection ( activeElementsInInput ) ) 
missingFromInput = len ( activeElementsInPrediction . difference ( activeElementsInInput ) ) 
missingFromPrediction = len ( activeElementsInInput . difference ( activeElementsInPrediction ) ) 
~~ return ( foundInInput , totalActiveInInput , missingFromInput , 
totalActiveInPrediction ) 
~~ def predictionExtent ( inputs , resets , outputs , minOverlapPct = 100.0 ) : 
predCounts = None 
predTotal = 0 
nSamples = len ( outputs ) 
predTotalNotLimited = 0 
nSamplesNotLimited = 0 
nCols = len ( inputs [ 0 ] ) 
nCellsPerCol = len ( outputs [ 0 ] ) // nCols 
for idx in xrange ( nSamples ) : 
~~~ activeCols = outputs [ idx ] . reshape ( nCols , nCellsPerCol ) . max ( axis = 1 ) 
steps = 0 
while ( idx + steps + 1 < nSamples ) and ( resets [ idx + steps + 1 ] == 0 ) : 
~~~ overlap = numpy . logical_and ( inputs [ idx + steps + 1 ] , activeCols ) 
overlapPct = 100.0 * float ( overlap . sum ( ) ) / inputs [ idx + steps + 1 ] . sum ( ) 
if overlapPct >= minOverlapPct : 
~~~ steps += 1 
~~ ~~ predCounts = _accumulateFrequencyCounts ( [ steps ] , predCounts ) 
predTotal += steps 
if resets [ idx ] or ( ( idx + steps + 1 < nSamples ) and ( not resets [ idx + steps + 1 ] ) ) : 
~~~ predTotalNotLimited += steps 
nSamplesNotLimited += 1 
~~ ~~ return ( float ( predTotal ) / nSamples , 
float ( predTotalNotLimited ) / nSamplesNotLimited , 
predCounts ) 
~~ def getCentreAndSpreadOffsets ( spaceShape , 
spreadShape , 
stepSize = 1 ) : 
from nupic . math . cross import cross 
shape = spaceShape 
if shape [ 0 ] == 1 and shape [ 1 ] == 1 : 
~~~ centerOffsets = [ ( 0 , 0 ) ] 
~~~ xMin = - 1 * ( shape [ 1 ] // 2 ) 
xMax = xMin + shape [ 1 ] - 1 
xPositions = range ( stepSize * xMin , stepSize * xMax + 1 , stepSize ) 
yMin = - 1 * ( shape [ 0 ] // 2 ) 
yMax = yMin + shape [ 0 ] - 1 
yPositions = range ( stepSize * yMin , stepSize * yMax + 1 , stepSize ) 
centerOffsets = list ( cross ( yPositions , xPositions ) ) 
~~ numCenterOffsets = len ( centerOffsets ) 
print "centerOffsets:" , centerOffsets 
shape = spreadShape 
~~~ spreadOffsets = [ ( 0 , 0 ) ] 
spreadOffsets = list ( cross ( yPositions , xPositions ) ) 
spreadOffsets . remove ( ( 0 , 0 ) ) 
spreadOffsets . insert ( 0 , ( 0 , 0 ) ) 
~~ numSpreadOffsets = len ( spreadOffsets ) 
print "spreadOffsets:" , spreadOffsets 
return centerOffsets , spreadOffsets 
~~ def makeCloneMap ( columnsShape , outputCloningWidth , outputCloningHeight = - 1 ) : 
if outputCloningHeight < 0 : 
~~~ outputCloningHeight = outputCloningWidth 
~~ columnsHeight , columnsWidth = columnsShape 
numDistinctMasters = outputCloningWidth * outputCloningHeight 
a = numpy . empty ( ( columnsHeight , columnsWidth ) , 'uint32' ) 
for row in xrange ( columnsHeight ) : 
~~~ for col in xrange ( columnsWidth ) : 
~~~ a [ row , col ] = ( col % outputCloningWidth ) + ( row % outputCloningHeight ) * outputCloningWidth 
~~ ~~ return a , numDistinctMasters 
~~ def numpyStr ( array , format = '%f' , includeIndices = False , includeZeros = True ) : 
shape = array . shape 
assert ( len ( shape ) <= 2 ) 
items = [ '[' ] 
if len ( shape ) == 1 : 
~~~ if includeIndices : 
~~~ format = '%d:' + format 
if includeZeros : 
~~~ rowItems = [ format % ( c , x ) for ( c , x ) in enumerate ( array ) ] 
~~~ rowItems = [ format % ( c , x ) for ( c , x ) in enumerate ( array ) if x != 0 ] 
~~~ rowItems = [ format % ( x ) for x in array ] 
~~ items . extend ( rowItems ) 
~~~ ( rows , cols ) = shape 
if includeIndices : 
~~~ format = '%d,%d:' + format 
~~ for r in xrange ( rows ) : 
~~~ rowItems = [ format % ( r , c , x ) for c , x in enumerate ( array [ r ] ) ] 
~~~ rowItems = [ format % ( x ) for x in array [ r ] ] 
~~ if r > 0 : 
~~~ items . append ( '' ) 
~~ items . append ( '[' ) 
items . extend ( rowItems ) 
if r < rows - 1 : 
~~~ items . append ( ']\\n' ) 
~~~ items . append ( ']' ) 
~~ ~~ ~~ items . append ( ']' ) 
~~ def sample ( self , rgen ) : 
rf = rgen . uniform ( 0 , self . sum ) 
index = bisect . bisect ( self . cdf , rf ) 
return self . keys [ index ] , numpy . log ( self . pmf [ index ] ) 
~~ def logProbability ( self , distn ) : 
x = numpy . asarray ( distn ) 
n = x . sum ( ) 
return ( logFactorial ( n ) - numpy . sum ( [ logFactorial ( k ) for k in x ] ) + 
numpy . sum ( x * numpy . log ( self . dist . pmf ) ) ) 
x = rgen . poisson ( self . lambdaParameter ) 
return x , self . logDensity ( x ) 
~~ def createDataOutLink ( network , sensorRegionName , regionName ) : 
network . link ( sensorRegionName , regionName , "UniformLink" , "" , 
srcOutput = "dataOut" , destInput = "bottomUpIn" ) 
~~ def createFeedForwardLink ( network , regionName1 , regionName2 ) : 
network . link ( regionName1 , regionName2 , "UniformLink" , "" , 
srcOutput = "bottomUpOut" , destInput = "bottomUpIn" ) 
~~ def createResetLink ( network , sensorRegionName , regionName ) : 
~~ def createSensorToClassifierLinks ( network , sensorRegionName , 
classifierRegionName ) : 
network . link ( sensorRegionName , classifierRegionName , "UniformLink" , "" , 
srcOutput = "bucketIdxOut" , destInput = "bucketIdxIn" ) 
srcOutput = "actValueOut" , destInput = "actValueIn" ) 
srcOutput = "categoryOut" , destInput = "categoryIn" ) 
with open ( _PARAMS_PATH , "r" ) as f : 
~~~ modelParams = yaml . safe_load ( f ) [ "modelParams" ] 
~~ network = Network ( ) 
network . addRegion ( "sensor" , "py.RecordSensor" , '{}' ) 
sensorRegion = network . regions [ "sensor" ] . getSelf ( ) 
sensorRegion . encoder = createEncoder ( modelParams [ "sensorParams" ] [ "encoders" ] ) 
sensorRegion . dataSource = dataSource 
modelParams [ "spParams" ] [ "inputWidth" ] = sensorRegion . encoder . getWidth ( ) 
network . addRegion ( "SP" , "py.SPRegion" , json . dumps ( modelParams [ "spParams" ] ) ) 
network . addRegion ( "TM" , "py.TMRegion" , json . dumps ( modelParams [ "tmParams" ] ) ) 
clName = "py.%s" % modelParams [ "clParams" ] . pop ( "regionName" ) 
network . addRegion ( "classifier" , clName , json . dumps ( modelParams [ "clParams" ] ) ) 
createSensorToClassifierLinks ( network , "sensor" , "classifier" ) 
createDataOutLink ( network , "sensor" , "SP" ) 
createFeedForwardLink ( network , "SP" , "TM" ) 
createFeedForwardLink ( network , "TM" , "classifier" ) 
createResetLink ( network , "sensor" , "SP" ) 
createResetLink ( network , "sensor" , "TM" ) 
~~ def getPredictionResults ( network , clRegionName ) : 
classifierRegion = network . regions [ clRegionName ] 
actualValues = classifierRegion . getOutputData ( "actualValues" ) 
probabilities = classifierRegion . getOutputData ( "probabilities" ) 
steps = classifierRegion . getSelf ( ) . stepsList 
N = classifierRegion . getSelf ( ) . maxCategoryCount 
results = { step : { } for step in steps } 
for i in range ( len ( steps ) ) : 
~~~ stepProbabilities = probabilities [ i * N : ( i + 1 ) * N - 1 ] 
mostLikelyCategoryIdx = stepProbabilities . argmax ( ) 
predictedValue = actualValues [ mostLikelyCategoryIdx ] 
predictionConfidence = stepProbabilities [ mostLikelyCategoryIdx ] 
results [ steps [ i ] ] [ "predictedValue" ] = predictedValue 
results [ steps [ i ] ] [ "predictionConfidence" ] = predictionConfidence 
~~ def runHotgym ( numRecords ) : 
dataSource = FileRecordStream ( streamID = _INPUT_FILE_PATH ) 
numRecords = min ( numRecords , dataSource . getDataRowCount ( ) ) 
network = createNetwork ( dataSource ) 
network . regions [ "sensor" ] . setParameter ( "predictedField" , "consumption" ) 
network . regions [ "SP" ] . setParameter ( "learningMode" , 1 ) 
network . regions [ "TM" ] . setParameter ( "learningMode" , 1 ) 
network . regions [ "classifier" ] . setParameter ( "learningMode" , 1 ) 
network . regions [ "SP" ] . setParameter ( "inferenceMode" , 1 ) 
network . regions [ "TM" ] . setParameter ( "inferenceMode" , 1 ) 
network . regions [ "classifier" ] . setParameter ( "inferenceMode" , 1 ) 
for iteration in range ( 0 , numRecords , N ) : 
~~~ network . run ( N ) 
predictionResults = getPredictionResults ( network , "classifier" ) 
oneStep = predictionResults [ 1 ] [ "predictedValue" ] 
oneStepConfidence = predictionResults [ 1 ] [ "predictionConfidence" ] 
fiveStep = predictionResults [ 5 ] [ "predictedValue" ] 
fiveStepConfidence = predictionResults [ 5 ] [ "predictionConfidence" ] 
result = ( oneStep , oneStepConfidence * 100 , 
fiveStep , fiveStepConfidence * 100 ) 
results . append ( result ) 
~~ def _loadDummyModelParameters ( self , params ) : 
for key , value in params . iteritems ( ) : 
~~~ if type ( value ) == list : 
~~~ index = self . modelIndex % len ( params [ key ] ) 
self . _params [ key ] = params [ key ] [ index ] 
~~~ self . _params [ key ] = params [ key ] 
~~ ~~ ~~ def _computModelDelay ( self ) : 
if self . _params [ 'delay' ] is not None and self . _params [ 'sleepModelRange' ] is not None : 
~~ if self . _sleepModelRange is not None : 
~~~ range , delay = self . _sleepModelRange . split ( ':' ) 
delay = float ( delay ) 
range = map ( int , range . split ( ',' ) ) 
modelIDs = self . _jobsDAO . jobGetModelIDs ( self . _jobID ) 
modelIDs . sort ( ) 
range [ 1 ] = min ( range [ 1 ] , len ( modelIDs ) ) 
if self . _modelID in modelIDs [ range [ 0 ] : range [ 1 ] ] : 
~~~ self . _delay = delay 
~~~ self . _delay = self . _params [ 'delay' ] 
~~ ~~ def _getMetrics ( self ) : 
metric = None 
if self . metrics is not None : 
~~~ metric = self . metrics ( self . _currentRecordIndex + 1 ) 
~~ elif self . metricValue is not None : 
~~~ metric = self . metricValue 
~~ return { self . _optimizeKeyPattern : metric } 
~~ def run ( self ) : 
periodic = self . _initPeriodicActivities ( ) 
self . _optimizedMetricLabel = self . _optimizeKeyPattern 
self . _reportMetricLabels = [ self . _optimizeKeyPattern ] 
if self . _iterations >= 0 : 
~~~ iterTracker = iter ( xrange ( self . _iterations ) ) 
~~~ iterTracker = iter ( itertools . count ( ) ) 
~~ doSysExit = False 
if self . _sysExitModelRange is not None : 
~~~ modelAndCounters = self . _jobsDAO . modelsGetUpdateCounters ( self . _jobID ) 
modelIDs = [ x [ 0 ] for x in modelAndCounters ] 
( beg , end ) = self . _sysExitModelRange 
if self . _modelID in modelIDs [ int ( beg ) : int ( end ) ] : 
~~~ doSysExit = True 
~~ ~~ if self . _delayModelRange is not None : 
( beg , end ) = self . _delayModelRange 
~~~ time . sleep ( 10 ) 
~~ ~~ if self . _errModelRange is not None : 
( beg , end ) = self . _errModelRange 
~~ ~~ if self . _delay is not None : 
~~~ time . sleep ( self . _delay ) 
~~ self . _currentRecordIndex = 0 
~~~ self . _currentRecordIndex = next ( iterTracker ) 
~~ self . _writePrediction ( ModelResult ( None , None , None , None ) ) 
periodic . tick ( ) 
if self . __shouldSysExit ( self . _currentRecordIndex ) : 
~~~ sys . exit ( 1 ) 
~~ if self . _busyWaitTime is not None : 
~~~ time . sleep ( self . _busyWaitTime ) 
self . __computeWaitTime ( ) 
~~ if doSysExit : 
~~ if self . _jobFailErr : 
~~~ raise utils . JobFailException ( "E10000" , 
"dummyModel\ ) 
~~ ~~ if self . _doFinalize : 
~~~ if not self . _makeCheckpoint : 
~~~ self . _model = None 
~~ if self . _finalDelay is not None : 
~~~ time . sleep ( self . _finalDelay ) 
~~ self . _finalize ( ) 
class DummyLogger : 
~~~ def writeRecord ( self , record ) : pass 
def writeRecords ( self , records , progressCB ) : pass 
def close ( self ) : pass 
~~ self . _predictionLogger = DummyLogger ( ) 
~~ def __shouldSysExit ( self , iteration ) : 
if self . _exitAfter is None or iteration < self . _exitAfter : 
~~ results = self . _jobsDAO . modelsGetFieldsForJob ( self . _jobID , [ 'params' ] ) 
modelIDs = [ e [ 0 ] for e in results ] 
modelNums = [ json . loads ( e [ 1 ] [ 0 ] ) [ 'structuredParams' ] [ '__model_num' ] for e in results ] 
sameModelNumbers = filter ( lambda x : x [ 1 ] == self . modelIndex , 
zip ( modelIDs , modelNums ) ) 
firstModelID = min ( zip ( * sameModelNumbers ) [ 0 ] ) 
return firstModelID == self . _modelID 
~~ def getDescription ( self ) : 
return description 
~~ def setSeed ( self , seed ) : 
rand . seed ( seed ) 
~~ def addField ( self , name , fieldParams , encoderParams ) : 
assert fieldParams is not None and 'type' in fieldParams 
dataClassName = fieldParams . pop ( 'type' ) 
~~~ dataClass = eval ( dataClassName ) ( fieldParams ) 
~~ except TypeError , e : 
~~ encoderParams [ 'dataClass' ] = dataClass 
encoderParams [ 'dataClassName' ] = dataClassName 
fieldIndex = self . defineField ( name , encoderParams ) 
~~ def addMultipleFields ( self , fieldsInfo ) : 
assert all ( x in field for x in [ 'name' , 'fieldSpec' , 'encoderParams' ] for field in fieldsInfo ) 
for spec in fieldsInfo : 
~~~ self . addField ( spec . pop ( 'name' ) , spec . pop ( 'fieldSpec' ) , spec . pop ( 'encoderParams' ) ) 
~~ ~~ def defineField ( self , name , encoderParams = None ) : 
self . fields . append ( _field ( name , encoderParams ) ) 
return len ( self . fields ) - 1 
~~ def setFlag ( self , index , flag ) : 
assert len ( self . fields ) > index 
self . fields [ index ] . flag = flag 
~~ def generateRecord ( self , record ) : 
assert ( len ( record ) == len ( self . fields ) ) 
if record is not None : 
~~~ for x in range ( len ( self . fields ) ) : 
~~~ self . fields [ x ] . addValue ( record [ x ] ) 
~~~ for field in self . fields : 
~~~ field . addValue ( field . dataClass . getNext ( ) ) 
~~ ~~ ~~ def generateRecords ( self , records ) : 
if self . verbosity > 0 : print 'Generating' , len ( records ) , 'records...' 
for record in records : 
~~~ self . generateRecord ( record ) 
~~ ~~ def getRecord ( self , n = None ) : 
if n is None : 
~~~ assert len ( self . fields ) > 0 
n = self . fields [ 0 ] . numRecords - 1 
~~ assert ( all ( field . numRecords > n for field in self . fields ) ) 
record = [ field . values [ n ] for field in self . fields ] 
return record 
~~ def getAllRecords ( self ) : 
numRecords = self . fields [ 0 ] . numRecords 
assert ( all ( field . numRecords == numRecords for field in self . fields ) ) 
for x in range ( numRecords ) : 
~~~ values . append ( self . getRecord ( x ) ) 
~~ def encodeRecord ( self , record , toBeAdded = True ) : 
encoding = [ self . fields [ i ] . encodeValue ( record [ i ] , toBeAdded ) for i in xrange ( len ( self . fields ) ) ] 
return encoding 
~~ def encodeAllRecords ( self , records = None , toBeAdded = True ) : 
if records is None : 
~~~ records = self . getAllRecords ( ) 
~~ if self . verbosity > 0 : print 'Encoding' , len ( records ) , 'records.' 
encodings = [ self . encodeRecord ( record , toBeAdded ) for record in records ] 
return encodings 
~~ def addValueToField ( self , i , value = None ) : 
assert ( len ( self . fields ) > i ) 
if value is None : 
~~~ value = self . fields [ i ] . dataClass . getNext ( ) 
self . fields [ i ] . addValue ( value ) 
~~ else : self . fields [ i ] . addValue ( value ) 
~~ def addValuesToField ( self , i , numValues ) : 
values = [ self . addValueToField ( i ) for n in range ( numValues ) ] 
return values 
~~ def getSDRforValue ( self , i , j ) : 
assert len ( self . fields ) > i 
assert self . fields [ i ] . numRecords > j 
encoding = self . fields [ i ] . encodings [ j ] 
~~ def getZeroedOutEncoding ( self , n ) : 
assert all ( field . numRecords > n for field in self . fields ) 
encoding = np . concatenate ( [ field . encoder . encode ( SENTINEL_VALUE_FOR_MISSING_DATA ) if field . isPredictedField else field . encodings [ n ] for field in self . fields ] ) 
~~ def getTotaln ( self ) : 
n = sum ( [ field . n for field in self . fields ] ) 
return n 
~~ def getTotalw ( self ) : 
w = sum ( [ field . w for field in self . fields ] ) 
return w 
~~ def getEncoding ( self , n ) : 
assert ( all ( field . numEncodings > n for field in self . fields ) ) 
encoding = np . concatenate ( [ field . encodings [ n ] for field in self . fields ] ) 
~~ def getAllEncodings ( self ) : 
numEncodings = self . fields [ 0 ] . numEncodings 
assert ( all ( field . numEncodings == numEncodings for field in self . fields ) ) 
encodings = [ self . getEncoding ( index ) for index in range ( numEncodings ) ] 
~~ def saveRecords ( self , path = 'myOutput' ) : 
import csv 
with open ( path + '.csv' , 'wb' ) as f : 
~~~ writer = csv . writer ( f ) 
writer . writerow ( self . getAllFieldNames ( ) ) 
writer . writerow ( self . getAllDataTypes ( ) ) 
writer . writerow ( self . getAllFlags ( ) ) 
writer . writerows ( self . getAllRecords ( ) ) 
~~ if self . verbosity > 0 : 
~~ ~~ def removeAllRecords ( self ) : 
for field in self . fields : 
~~~ field . encodings , field . values = [ ] , [ ] 
field . numRecords , field . numEncodings = ( 0 , 0 ) 
~~ ~~ def encodeValue ( self , value , toBeAdded = True ) : 
encodedValue = np . array ( self . encoder . encode ( value ) , dtype = realDType ) 
if toBeAdded : 
~~~ self . encodings . append ( encodedValue ) 
self . numEncodings += 1 
~~ return encodedValue 
~~ def _setTypes ( self , encoderSpec ) : 
if self . encoderType is None : 
~~~ if self . dataType in [ 'int' , 'float' ] : 
~~~ self . encoderType = 'adaptiveScalar' 
~~ elif self . dataType == 'string' : 
~~~ self . encoderType = 'category' 
~~ elif self . dataType in [ 'date' , 'datetime' ] : 
~~~ self . encoderType = 'date' 
~~ ~~ if self . dataType is None : 
~~~ if self . encoderType in [ 'scalar' , 'adaptiveScalar' ] : 
~~~ self . dataType = 'float' 
~~ elif self . encoderType in [ 'category' , 'enumeration' ] : 
~~~ self . dataType = 'string' 
~~ elif self . encoderType in [ 'date' , 'datetime' ] : 
~~~ self . dataType = 'datetime' 
~~ ~~ ~~ def _initializeEncoders ( self , encoderSpec ) : 
if self . encoderType in [ 'adaptiveScalar' , 'scalar' ] : 
~~~ if 'minval' in encoderSpec : 
~~~ self . minval = encoderSpec . pop ( 'minval' ) 
~~ else : self . minval = None 
if 'maxval' in encoderSpec : 
~~~ self . maxval = encoderSpec . pop ( 'maxval' ) 
~~ else : self . maxval = None 
self . encoder = adaptive_scalar . AdaptiveScalarEncoder ( name = 'AdaptiveScalarEncoder' , w = self . w , n = self . n , minval = self . minval , maxval = self . maxval , periodic = False , forced = True ) 
~~ elif self . encoderType == 'category' : 
~~~ self . encoder = sdr_category . SDRCategoryEncoder ( name = 'categoryEncoder' , w = self . w , n = self . n ) 
~~~ self . encoder = date . DateEncoder ( name = 'dateEncoder' ) 
~~ ~~ def getScalars ( self , input ) : 
~~~ return numpy . array ( [ self . categoryToIndex . get ( input , 0 ) ] ) 
~~ ~~ def getBucketIndices ( self , input ) : 
~~~ return [ None ] 
~~~ return self . encoder . getBucketIndices ( self . categoryToIndex . get ( input , 0 ) ) 
~~ ~~ def decode ( self , encoded , parentFieldName = '' ) : 
( fieldsDict , fieldNames ) = self . encoder . decode ( encoded ) 
if len ( fieldsDict ) == 0 : 
~~~ return ( fieldsDict , fieldNames ) 
~~ assert ( len ( fieldsDict ) == 1 ) 
( inRanges , inDesc ) = fieldsDict . values ( ) [ 0 ] 
outRanges = [ ] 
desc = "" 
for ( minV , maxV ) in inRanges : 
~~~ minV = int ( round ( minV ) ) 
maxV = int ( round ( maxV ) ) 
outRanges . append ( ( minV , maxV ) ) 
while minV <= maxV : 
~~~ if len ( desc ) > 0 : 
~~ desc += self . indexToCategory [ minV ] 
minV += 1 
~~ ~~ if parentFieldName != '' : 
~~ return ( { fieldName : ( outRanges , desc ) } , [ fieldName ] ) 
~~ def closenessScores ( self , expValues , actValues , fractional = True , ) : 
expValue = expValues [ 0 ] 
actValue = actValues [ 0 ] 
if expValue == actValue : 
~~~ closeness = 1.0 
~~~ closeness = 0.0 
~~ if not fractional : 
~~~ closeness = 1.0 - closeness 
~~ return numpy . array ( [ closeness ] ) 
~~ def getBucketValues ( self ) : 
if self . _bucketValues is None : 
~~~ numBuckets = len ( self . encoder . getBucketValues ( ) ) 
self . _bucketValues = [ ] 
for bucketIndex in range ( numBuckets ) : 
~~~ self . _bucketValues . append ( self . getBucketInfo ( [ bucketIndex ] ) [ 0 ] . value ) 
~~ ~~ return self . _bucketValues 
bucketInfo = self . encoder . getBucketInfo ( buckets ) [ 0 ] 
categoryIndex = int ( round ( bucketInfo . value ) ) 
category = self . indexToCategory [ categoryIndex ] 
encoding = bucketInfo . encoding ) ] 
encoderResult = self . encoder . topDownCompute ( encoded ) [ 0 ] 
value = encoderResult . value 
categoryIndex = int ( round ( value ) ) 
return EncoderResult ( value = category , scalar = categoryIndex , 
encoding = encoderResult . encoding ) 
~~ def loadExperiment ( path ) : 
if not os . path . isdir ( path ) : 
~~~ path = os . path . dirname ( path ) 
~~ descriptionPyModule = loadExperimentDescriptionScriptFromDir ( path ) 
expIface = getExperimentDescriptionInterfaceFromModule ( descriptionPyModule ) 
return expIface . getModelDescription ( ) , expIface . getModelControl ( ) 
~~ def loadExperimentDescriptionScriptFromDir ( experimentDir ) : 
descriptionScriptPath = os . path . join ( experimentDir , "description.py" ) 
module = _loadDescriptionFile ( descriptionScriptPath ) 
return module 
~~ def getExperimentDescriptionInterfaceFromModule ( module ) : 
result = module . descriptionInterface 
~~ def _loadDescriptionFile ( descriptionPyPath ) : 
global g_descriptionImportCount 
if not os . path . isfile ( descriptionPyPath ) : 
~~ mod = imp . load_source ( "pf_description%d" % g_descriptionImportCount , 
descriptionPyPath ) 
g_descriptionImportCount += 1 
if not hasattr ( mod , "descriptionInterface" ) : 
~~ if not isinstance ( mod . descriptionInterface , exp_description_api . DescriptionIface ) : 
~~ return mod 
~~ def update ( self , modelID , modelParams , modelParamsHash , metricResult , 
completed , completionReason , matured , numRecords ) : 
assert ( modelParamsHash is not None ) 
if completed : 
~~~ matured = True 
~~ if metricResult is not None and matured and completionReason in [ ClientJobsDAO . CMPL_REASON_EOF , 
ClientJobsDAO . CMPL_REASON_STOPPED ] : 
~~~ if self . _hsObj . _maximize : 
~~~ errScore = - 1 * metricResult 
~~~ errScore = metricResult 
~~ if errScore < self . _bestResult : 
~~~ self . _bestResult = errScore 
self . _bestModelID = modelID 
self . _bestModelID ) ) 
~~ if completed and completionReason in [ ClientJobsDAO . CMPL_REASON_ORPHAN ] : 
hidden = True 
~~~ hidden = False 
~~ if completed : 
~~~ self . _completedModels . add ( modelID ) 
self . _numCompletedModels = len ( self . _completedModels ) 
if completionReason == ClientJobsDAO . CMPL_REASON_ERROR : 
~~~ self . _errModels . add ( modelID ) 
self . _numErrModels = len ( self . _errModels ) 
~~ ~~ wasHidden = False 
if modelID not in self . _modelIDToIdx : 
~~~ assert ( modelParams is not None ) 
entry = dict ( modelID = modelID , modelParams = modelParams , 
modelParamsHash = modelParamsHash , 
errScore = errScore , completed = completed , 
matured = matured , numRecords = numRecords , hidden = hidden ) 
self . _allResults . append ( entry ) 
entryIdx = len ( self . _allResults ) - 1 
self . _modelIDToIdx [ modelID ] = entryIdx 
self . _paramsHashToIndexes [ modelParamsHash ] = entryIdx 
swarmId = modelParams [ 'particleState' ] [ 'swarmId' ] 
if not hidden : 
~~~ if swarmId in self . _swarmIdToIndexes : 
~~~ self . _swarmIdToIndexes [ swarmId ] . append ( entryIdx ) 
~~~ self . _swarmIdToIndexes [ swarmId ] = [ entryIdx ] 
~~ genIdx = modelParams [ 'particleState' ] [ 'genIdx' ] 
numPsEntry = self . _swarmNumParticlesPerGeneration . get ( swarmId , [ 0 ] ) 
while genIdx >= len ( numPsEntry ) : 
~~~ numPsEntry . append ( 0 ) 
~~ numPsEntry [ genIdx ] += 1 
self . _swarmNumParticlesPerGeneration [ swarmId ] = numPsEntry 
~~~ entryIdx = self . _modelIDToIdx . get ( modelID , None ) 
assert ( entryIdx is not None ) 
entry = self . _allResults [ entryIdx ] 
wasHidden = entry [ 'hidden' ] 
if entry [ 'modelParamsHash' ] != modelParamsHash : 
~~~ self . _paramsHashToIndexes . pop ( entry [ 'modelParamsHash' ] ) 
entry [ 'modelParamsHash' ] = modelParamsHash 
~~ modelParams = entry [ 'modelParams' ] 
genIdx = modelParams [ 'particleState' ] [ 'genIdx' ] 
if hidden and not wasHidden : 
~~~ assert ( entryIdx in self . _swarmIdToIndexes [ swarmId ] ) 
self . _swarmIdToIndexes [ swarmId ] . remove ( entryIdx ) 
self . _swarmNumParticlesPerGeneration [ swarmId ] [ genIdx ] -= 1 
~~ entry [ 'errScore' ] = errScore 
entry [ 'completed' ] = completed 
entry [ 'matured' ] = matured 
entry [ 'numRecords' ] = numRecords 
entry [ 'hidden' ] = hidden 
~~ particleId = modelParams [ 'particleState' ] [ 'id' ] 
if matured and not hidden : 
~~~ ( oldResult , pos ) = self . _particleBest . get ( particleId , ( numpy . inf , None ) ) 
if errScore < oldResult : 
~~~ pos = Particle . getPositionFromState ( modelParams [ 'particleState' ] ) 
self . _particleBest [ particleId ] = ( errScore , pos ) 
~~ ~~ prevGenIdx = self . _particleLatestGenIdx . get ( particleId , - 1 ) 
if not hidden and genIdx > prevGenIdx : 
~~~ self . _particleLatestGenIdx [ particleId ] = genIdx 
~~ elif hidden and not wasHidden and genIdx == prevGenIdx : 
~~~ self . _particleLatestGenIdx [ particleId ] = genIdx - 1 
~~ if not hidden : 
~~~ swarmId = modelParams [ 'particleState' ] [ 'swarmId' ] 
if not swarmId in self . _swarmBestOverall : 
~~~ self . _swarmBestOverall [ swarmId ] = [ ] 
~~ bestScores = self . _swarmBestOverall [ swarmId ] 
while genIdx >= len ( bestScores ) : 
~~~ bestScores . append ( ( None , numpy . inf ) ) 
~~ if errScore < bestScores [ genIdx ] [ 1 ] : 
~~~ bestScores [ genIdx ] = ( modelID , errScore ) 
~~ ~~ if not hidden : 
~~~ key = ( swarmId , genIdx ) 
if not key in self . _maturedSwarmGens : 
~~~ self . _modifiedSwarmGens . add ( key ) 
~~ ~~ return errScore 
~~ def getModelIDFromParamsHash ( self , paramsHash ) : 
entryIdx = self . _paramsHashToIndexes . get ( paramsHash , None ) 
if entryIdx is not None : 
~~~ return self . _allResults [ entryIdx ] [ 'modelID' ] 
~~ ~~ def numModels ( self , swarmId = None , includeHidden = False ) : 
if includeHidden : 
~~~ if swarmId is None : 
~~~ return len ( self . _allResults ) 
~~~ return len ( self . _swarmIdToIndexes . get ( swarmId , [ ] ) ) 
~~~ entries = self . _allResults 
~~~ entries = [ self . _allResults [ entryIdx ] 
for entryIdx in self . _swarmIdToIndexes . get ( swarmId , [ ] ) ] 
~~ return len ( [ entry for entry in entries if not entry [ 'hidden' ] ] ) 
~~ ~~ def bestModelIdAndErrScore ( self , swarmId = None , genIdx = None ) : 
if swarmId is None : 
~~~ return ( self . _bestModelID , self . _bestResult ) 
~~~ if swarmId not in self . _swarmBestOverall : 
~~~ return ( None , numpy . inf ) 
~~ genScores = self . _swarmBestOverall [ swarmId ] 
bestScore = numpy . inf 
for ( i , ( modelId , errScore ) ) in enumerate ( genScores ) : 
~~~ if genIdx is not None and i > genIdx : 
~~ if errScore < bestScore : 
~~~ bestScore = errScore 
bestModelId = modelId 
~~ ~~ return ( bestModelId , bestScore ) 
~~ ~~ def getParticleInfo ( self , modelId ) : 
entry = self . _allResults [ self . _modelIDToIdx [ modelId ] ] 
return ( entry [ 'modelParams' ] [ 'particleState' ] , modelId , entry [ 'errScore' ] , 
entry [ 'completed' ] , entry [ 'matured' ] ) 
~~ def getParticleInfos ( self , swarmId = None , genIdx = None , completed = None , 
matured = None , lastDescendent = False ) : 
if swarmId is not None : 
~~~ entryIdxs = self . _swarmIdToIndexes . get ( swarmId , [ ] ) 
~~~ entryIdxs = range ( len ( self . _allResults ) ) 
~~ if len ( entryIdxs ) == 0 : 
~~~ return ( [ ] , [ ] , [ ] , [ ] , [ ] ) 
~~ particleStates = [ ] 
modelIds = [ ] 
errScores = [ ] 
completedFlags = [ ] 
maturedFlags = [ ] 
for idx in entryIdxs : 
~~~ entry = self . _allResults [ idx ] 
~~~ assert ( not entry [ 'hidden' ] ) 
isCompleted = entry [ 'completed' ] 
isMatured = entry [ 'matured' ] 
particleState = modelParams [ 'particleState' ] 
particleGenIdx = particleState [ 'genIdx' ] 
particleId = particleState [ 'id' ] 
if genIdx is not None and particleGenIdx != genIdx : 
~~ if completed is not None and ( completed != isCompleted ) : 
~~ if matured is not None and ( matured != isMatured ) : 
~~ if lastDescendent and ( self . _particleLatestGenIdx [ particleId ] != particleGenIdx ) : 
~~ particleStates . append ( particleState ) 
modelIds . append ( entry [ 'modelID' ] ) 
errScores . append ( entry [ 'errScore' ] ) 
completedFlags . append ( isCompleted ) 
maturedFlags . append ( isMatured ) 
~~ return ( particleStates , modelIds , errScores , completedFlags , maturedFlags ) 
~~ def getOrphanParticleInfos ( self , swarmId , genIdx ) : 
entryIdxs = range ( len ( self . _allResults ) ) 
if len ( entryIdxs ) == 0 : 
if not entry [ 'hidden' ] : 
if modelParams [ 'particleState' ] [ 'swarmId' ] != swarmId : 
~~ isCompleted = entry [ 'completed' ] 
~~ def getMaturedSwarmGenerations ( self ) : 
modifiedSwarmGens = sorted ( self . _modifiedSwarmGens ) 
for key in modifiedSwarmGens : 
~~~ ( swarmId , genIdx ) = key 
if key in self . _maturedSwarmGens : 
~~~ self . _modifiedSwarmGens . remove ( key ) 
~~ if ( genIdx >= 1 ) and not ( swarmId , genIdx - 1 ) in self . _maturedSwarmGens : 
~~ ( _ , _ , errScores , completedFlags , maturedFlags ) = self . getParticleInfos ( swarmId , genIdx ) 
maturedFlags = numpy . array ( maturedFlags ) 
numMatured = maturedFlags . sum ( ) 
if numMatured >= self . _hsObj . _minParticlesPerSwarm and numMatured == len ( maturedFlags ) : 
~~~ errScores = numpy . array ( errScores ) 
bestScore = errScores . min ( ) 
self . _maturedSwarmGens . add ( key ) 
self . _modifiedSwarmGens . remove ( key ) 
result . append ( ( swarmId , genIdx , bestScore ) ) 
~~ def firstNonFullGeneration ( self , swarmId , minNumParticles ) : 
if not swarmId in self . _swarmNumParticlesPerGeneration : 
~~ numPsPerGen = self . _swarmNumParticlesPerGeneration [ swarmId ] 
numPsPerGen = numpy . array ( numPsPerGen ) 
firstNonFull = numpy . where ( numPsPerGen < minNumParticles ) [ 0 ] 
if len ( firstNonFull ) == 0 : 
~~~ return len ( numPsPerGen ) 
~~~ return firstNonFull [ 0 ] 
~~ ~~ def getResultsPerChoice ( self , swarmId , maxGenIdx , varName ) : 
results = dict ( ) 
( allParticles , _ , resultErrs , _ , _ ) = self . getParticleInfos ( swarmId , 
genIdx = None , matured = True ) 
for particleState , resultErr in itertools . izip ( allParticles , resultErrs ) : 
~~~ if maxGenIdx is not None : 
~~~ if particleState [ 'genIdx' ] > maxGenIdx : 
~~ ~~ if resultErr == numpy . inf : 
~~ position = Particle . getPositionFromState ( particleState ) 
varPosition = position [ varName ] 
varPositionStr = str ( varPosition ) 
if varPositionStr in results : 
~~~ results [ varPositionStr ] [ 1 ] . append ( resultErr ) 
~~~ results [ varPositionStr ] = ( varPosition , [ resultErr ] ) 
~~ def _getStreamDef ( self , modelDescription ) : 
aggregationPeriod = { 
if 'aggregation' in modelDescription [ 'streamDef' ] : 
~~~ if key in modelDescription [ 'streamDef' ] [ 'aggregation' ] : 
~~~ aggregationPeriod [ key ] = modelDescription [ 'streamDef' ] [ 'aggregation' ] [ key ] 
~~ ~~ if 'fields' in modelDescription [ 'streamDef' ] [ 'aggregation' ] : 
~~~ for ( fieldName , func ) in modelDescription [ 'streamDef' ] [ 'aggregation' ] [ 'fields' ] : 
streamDef = copy . deepcopy ( modelDescription [ 'streamDef' ] ) 
streamDef [ 'aggregation' ] = copy . deepcopy ( aggregationInfo ) 
return streamDef 
if self . _tempDir is not None and os . path . isdir ( self . _tempDir ) : 
shutil . rmtree ( self . _tempDir ) 
self . _tempDir = None 
~~ def _readPermutationsFile ( self , filename , modelDescription ) : 
vars = { } 
permFile = execfile ( filename , globals ( ) , vars ) 
self . _reportKeys = vars . get ( 'report' , [ ] ) 
self . _filterFunc = vars . get ( 'permutationFilter' , None ) 
self . _dummyModelParamsFunc = vars . get ( 'dummyModelParams' , None ) 
self . _fastSwarmModelParams = vars . get ( 'fastSwarmModelParams' , None ) 
if self . _fastSwarmModelParams is not None : 
~~~ encoders = self . _fastSwarmModelParams [ 'structuredParams' ] [ 'modelParams' ] [ 'sensorParams' ] [ 'encoders' ] 
self . _fixedFields = [ ] 
for fieldName in encoders : 
~~~ if encoders [ fieldName ] is not None : 
~~~ self . _fixedFields . append ( fieldName ) 
~~ ~~ ~~ if 'fixedFields' in vars : 
~~~ self . _fixedFields = vars [ 'fixedFields' ] 
~~ self . _minParticlesPerSwarm = vars . get ( 'minParticlesPerSwarm' ) 
if self . _minParticlesPerSwarm == None : 
~~~ self . _minParticlesPerSwarm = Configuration . get ( 
'nupic.hypersearch.minParticlesPerSwarm' ) 
~~ self . _minParticlesPerSwarm = int ( self . _minParticlesPerSwarm ) 
self . _killUselessSwarms = vars . get ( 'killUselessSwarms' , True ) 
self . _inputPredictedField = vars . get ( "inputPredictedField" , "yes" ) 
self . _tryAll3FieldCombinations = vars . get ( 'tryAll3FieldCombinations' , False ) 
self . _tryAll3FieldCombinationsWTimestamps = vars . get ( 
'tryAll3FieldCombinationsWTimestamps' , False ) 
minFieldContribution = vars . get ( 'minFieldContribution' , None ) 
if minFieldContribution is not None : 
~~~ self . _minFieldContribution = minFieldContribution 
~~ maxBranching = vars . get ( 'maxFieldBranching' , None ) 
if maxBranching is not None : 
~~~ self . _maxBranching = maxBranching 
~~ if 'maximize' in vars : 
~~~ self . _optimizeKey = vars [ 'maximize' ] 
self . _maximize = True 
~~ elif 'minimize' in vars : 
~~~ self . _optimizeKey = vars [ 'minimize' ] 
self . _maximize = False 
~~ maxModels = vars . get ( 'maxModels' ) 
if maxModels is not None : 
~~~ if self . _maxModels is None : 
~~~ self . _maxModels = maxModels 
~~ ~~ inferenceType = modelDescription [ 'modelParams' ] [ 'inferenceType' ] 
if not InferenceType . validate ( inferenceType ) : 
~~ if inferenceType in [ InferenceType . TemporalMultiStep , 
InferenceType . NontemporalMultiStep ] : 
~~~ classifierOnlyEncoder = None 
for encoder in modelDescription [ "modelParams" ] [ "sensorParams" ] [ "encoders" ] . values ( ) : 
~~~ if encoder . get ( "classifierOnly" , False ) and encoder [ "fieldname" ] == vars . get ( 'predictedField' , None ) : 
~~~ classifierOnlyEncoder = encoder 
~~ ~~ if classifierOnlyEncoder is None or self . _inputPredictedField == "yes" : 
~~~ self . _searchType = HsSearchType . legacyTemporal 
~~~ self . _searchType = HsSearchType . temporal 
~~ ~~ elif inferenceType in [ InferenceType . TemporalNextStep , 
InferenceType . TemporalAnomaly ] : 
~~ elif inferenceType in ( InferenceType . TemporalClassification , 
InferenceType . NontemporalClassification ) : 
~~~ self . _searchType = HsSearchType . classification 
~~ self . _predictedField = vars . get ( 'predictedField' , None ) 
if self . _predictedField is None : 
~~ if 'permutations' not in vars : 
~~ if not isinstance ( vars [ 'permutations' ] , dict ) : 
~~ self . _encoderNames = [ ] 
self . _permutations = vars [ 'permutations' ] 
self . _flattenedPermutations = dict ( ) 
def _flattenPermutations ( value , keys ) : 
~~~ if ':' in keys [ - 1 ] : 
~~ flatKey = _flattenKeys ( keys ) 
if isinstance ( value , PermuteEncoder ) : 
~~~ self . _encoderNames . append ( flatKey ) 
if value . fieldName == self . _predictedField : 
~~~ self . _predictedFieldEncoder = flatKey 
~~ for encKey , encValue in value . kwArgs . iteritems ( ) : 
~~~ if isinstance ( encValue , PermuteVariable ) : 
~~~ self . _flattenedPermutations [ '%s:%s' % ( flatKey , encKey ) ] = encValue 
~~ ~~ ~~ elif isinstance ( value , PermuteVariable ) : 
~~~ self . _flattenedPermutations [ flatKey ] = value 
~~~ if isinstance ( value , PermuteVariable ) : 
~~~ self . _flattenedPermutations [ key ] = value 
~~ ~~ ~~ rApply ( self . _permutations , _flattenPermutations ) 
~~ def _checkForOrphanedModels ( self ) : 
~~~ orphanedModelId = self . _cjDAO . modelAdoptNextOrphan ( self . _jobID , 
self . _modelOrphanIntervalSecs ) 
if orphanedModelId is None : 
for attempt in range ( 100 ) : 
~~~ paramsHash = hashlib . md5 ( "OrphanParams.%d.%d" % ( orphanedModelId , 
attempt ) ) . digest ( ) 
particleHash = hashlib . md5 ( "OrphanParticle.%d.%d" % ( orphanedModelId , 
~~~ self . _cjDAO . modelSetFields ( orphanedModelId , 
dict ( engParamsHash = paramsHash , 
engParticleHash = particleHash ) ) 
~~~ success = False 
~~ if success : 
~~ ~~ if not success : 
~~ self . _cjDAO . modelSetCompleted ( modelID = orphanedModelId , 
completionReason = ClientJobsDAO . CMPL_REASON_ORPHAN , 
completionMsg = "Orphaned" ) 
self . _resultsDB . update ( modelID = orphanedModelId , 
modelParams = None , 
modelParamsHash = paramsHash , 
metricResult = None , 
completed = True , 
matured = True , 
numRecords = 0 ) 
~~ ~~ def _hsStatePeriodicUpdate ( self , exhaustedSwarmId = None ) : 
if self . _hsState is None : 
~~~ self . _hsState = HsState ( self ) 
~~ self . _hsState . readStateFromDB ( ) 
completedSwarms = set ( ) 
if exhaustedSwarmId is not None : 
"positions" % ( exhaustedSwarmId ) ) 
( particles , _ , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( 
swarmId = exhaustedSwarmId , matured = False ) 
if len ( particles ) > 0 : 
~~~ exhaustedSwarmStatus = 'completing' 
~~~ exhaustedSwarmStatus = 'completed' 
~~ ~~ if self . _killUselessSwarms : 
~~~ self . _hsState . killUselessSwarms ( ) 
~~ completingSwarms = self . _hsState . getCompletingSwarms ( ) 
for swarmId in completingSwarms : 
~~~ ( particles , _ , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( 
swarmId = swarmId , matured = False ) 
if len ( particles ) == 0 : 
~~~ completedSwarms . add ( swarmId ) 
~~ ~~ completedSwarmGens = self . _resultsDB . getMaturedSwarmGenerations ( ) 
priorCompletedSwarms = self . _hsState . getCompletedSwarms ( ) 
for ( swarmId , genIdx , errScore ) in completedSwarmGens : 
~~~ if swarmId in priorCompletedSwarms : 
~~ completedList = self . _swarmTerminator . recordDataPoint ( 
swarmId = swarmId , generation = genIdx , errScore = errScore ) 
if len ( completedList ) > 0 : 
~~ self . logger . info ( statusMsg ) 
self . _cjDAO . jobSetFields ( jobID = self . _jobID , 
fields = dict ( engStatus = statusMsg ) , 
if 'NTA_TEST_recordSwarmTerminations' in os . environ : 
~~~ resultsStr = self . _cjDAO . jobGetFields ( self . _jobID , [ 'results' ] ) [ 0 ] 
if resultsStr is None : 
~~~ results = { } 
~~~ results = json . loads ( resultsStr ) 
~~ if not 'terminatedSwarms' in results : 
~~~ results [ 'terminatedSwarms' ] = { } 
~~ for swarm in completedList : 
~~~ if swarm not in results [ 'terminatedSwarms' ] : 
~~~ results [ 'terminatedSwarms' ] [ swarm ] = ( genIdx , 
self . _swarmTerminator . swarmScores [ swarm ] ) 
~~ ~~ newResultsStr = json . dumps ( results ) 
if newResultsStr == resultsStr : 
~~ updated = self . _cjDAO . jobSetFieldIfEqual ( jobID = self . _jobID , 
curValue = resultsStr , 
newValue = json . dumps ( results ) ) 
if updated : 
~~ ~~ ~~ if len ( completedList ) > 0 : 
~~~ for name in completedList : 
"%s" % ( name , genIdx , errScore ) ) 
~~ completedSwarms = completedSwarms . union ( completedList ) 
~~ ~~ if len ( completedSwarms ) == 0 and ( exhaustedSwarmId is None ) : 
~~~ if exhaustedSwarmId is not None : 
~~~ self . _hsState . setSwarmState ( exhaustedSwarmId , exhaustedSwarmStatus ) 
~~ for swarmId in completedSwarms : 
~~~ self . _hsState . setSwarmState ( swarmId , 'completed' ) 
~~ if not self . _hsState . isDirty ( ) : 
~~ success = self . _hsState . writeStateToDB ( ) 
~~~ jobResultsStr = self . _cjDAO . jobGetFields ( self . _jobID , [ 'results' ] ) [ 0 ] 
if jobResultsStr is not None : 
bestModelId = jobResults . get ( 'bestModel' , None ) 
~~~ bestModelId = None 
~~ for swarmId in list ( completedSwarms ) : 
~~~ ( _ , modelIds , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( 
swarmId = swarmId , completed = False ) 
if bestModelId in modelIds : 
~~~ modelIds . remove ( bestModelId ) 
~~ if len ( modelIds ) == 0 : 
str ( modelIds ) ) ) 
for modelId in modelIds : 
~~~ self . _cjDAO . modelSetFields ( modelId , 
dict ( engStop = ClientJobsDAO . STOP_REASON_KILLED ) , 
~~ ~~ return 
~~ ~~ def _getCandidateParticleAndSwarm ( self , exhaustedSwarmId = None ) : 
jobCancel = self . _cjDAO . jobGetFields ( self . _jobID , [ 'cancel' ] ) [ 0 ] 
~~~ self . _jobCancelled = True 
( workerCmpReason , workerCmpMsg ) = self . _cjDAO . jobGetFields ( self . _jobID , 
[ 'workerCompletionReason' , 'workerCompletionMsg' ] ) 
self . _cjDAO . jobSetFields ( self . _jobID , 
useConnectionID = False , ignoreUnchanged = True ) 
( workerCmpReason , workerCmpMsg ) ) 
~~ return ( True , None , None ) 
~~ if self . _hsState is not None : 
~~~ priorActiveSwarms = self . _hsState . getActiveSwarms ( ) 
~~~ priorActiveSwarms = None 
~~ self . _hsStatePeriodicUpdate ( exhaustedSwarmId = exhaustedSwarmId ) 
activeSwarms = self . _hsState . getActiveSwarms ( ) 
if activeSwarms != priorActiveSwarms : 
priorActiveSwarms ) ) 
totalCmpModels = self . _resultsDB . getNumCompletedModels ( ) 
if totalCmpModels > 5 : 
~~~ numErrs = self . _resultsDB . getNumErrModels ( ) 
if ( float ( numErrs ) / totalCmpModels ) > self . _maxPctErrModels : 
~~~ errModelIds = self . _resultsDB . getErrModelIds ( ) 
resInfo = self . _cjDAO . modelsGetResultAndStatus ( [ errModelIds [ 0 ] ] ) [ 0 ] 
modelErrMsg = resInfo . completionMsg 
modelErrMsg ) 
self . logger . error ( cmpMsg ) 
workerCmpReason = self . _cjDAO . jobGetFields ( self . _jobID , 
~~~ self . _cjDAO . jobSetFields ( 
self . _jobID , 
fields = dict ( 
workerCompletionMsg = cmpMsg ) , 
~~ ~~ if self . _hsState . isSearchOver ( ) : 
self . logger . info ( cmpMsg ) 
dict ( workerCompletionMsg = cmpMsg ) , 
return ( True , None , None ) 
~~ sprintIdx = - 1 
~~~ sprintIdx += 1 
( active , eos ) = self . _hsState . isSprintActive ( sprintIdx ) 
if eos : 
~~~ if self . _hsState . anyGoodSprintsActive ( ) : 
return ( False , None , None ) 
~~ ~~ if not active : 
~~~ if not self . _speculativeParticles : 
~~~ if not self . _hsState . isSprintCompleted ( sprintIdx ) : 
~~ ~~ continue 
~~ swarmIds = self . _hsState . getActiveSwarms ( sprintIdx ) 
for swarmId in swarmIds : 
~~~ firstNonFullGenIdx = self . _resultsDB . firstNonFullGeneration ( 
swarmId = swarmId , 
minNumParticles = self . _minParticlesPerSwarm ) 
if firstNonFullGenIdx is None : 
~~ if firstNonFullGenIdx < self . _resultsDB . highestGeneration ( swarmId ) : 
firstNonFullGenIdx , swarmId , sprintIdx ) ) 
( allParticles , allModelIds , errScores , completed , matured ) = self . _resultsDB . getOrphanParticleInfos ( swarmId , firstNonFullGenIdx ) 
if len ( allModelIds ) > 0 : 
~~~ newParticleId = True 
( allParticles , allModelIds , errScores , completed , matured ) = self . _resultsDB . getParticleInfos ( swarmId = swarmId , 
genIdx = firstNonFullGenIdx ) 
~~ modelId = random . choice ( allModelIds ) 
( particleState , _ , _ , _ , _ ) = self . _resultsDB . getParticleInfo ( modelId ) 
particle = Particle ( hsObj = self , 
resultsDB = self . _resultsDB , 
flattenedPermuteVars = self . _flattenedPermutations , 
newFromClone = particleState , 
newParticleId = newParticleId ) 
return ( False , particle , swarmId ) 
~~ ~~ swarmSizes = numpy . array ( [ self . _resultsDB . numModels ( x ) for x in swarmIds ] ) 
swarmSizeAndIdList = zip ( swarmSizes , swarmIds ) 
swarmSizeAndIdList . sort ( ) 
for ( _ , swarmId ) in swarmSizeAndIdList : 
~~~ ( allParticles , allModelIds , errScores , completed , matured ) = ( 
self . _resultsDB . getParticleInfos ( swarmId ) ) 
if len ( allParticles ) < self . _minParticlesPerSwarm : 
~~~ particle = Particle ( hsObj = self , 
newFarFrom = allParticles ) 
bestPriorModel = None 
if sprintIdx >= 1 : 
~~~ ( bestPriorModel , errScore ) = self . _hsState . bestModelInSprint ( 0 ) 
~~ if bestPriorModel is not None : 
( baseState , modelId , errScore , completed , matured ) = self . _resultsDB . getParticleInfo ( bestPriorModel ) 
particle . copyEncoderStatesFrom ( baseState ) 
particle . copyVarStatesFrom ( baseState , [ 'modelParams|inferenceType' ] ) 
whichVars = [ ] 
for varName in baseState [ 'varStates' ] : 
~~~ if ':' in varName : 
~~~ whichVars . append ( varName ) 
~~ ~~ particle . newPosition ( whichVars ) 
~~ return ( False , particle , swarmId ) 
~~ ( readyParticles , readyModelIds , readyErrScores , _ , _ ) = ( 
self . _resultsDB . getParticleInfos ( swarmId , genIdx = None , 
matured = True , lastDescendent = True ) ) 
if len ( readyParticles ) > 0 : 
~~~ readyGenIdxs = [ x [ 'genIdx' ] for x in readyParticles ] 
sortedGenIdxs = sorted ( set ( readyGenIdxs ) ) 
genIdx = sortedGenIdxs [ 0 ] 
useParticle = None 
for particle in readyParticles : 
~~~ if particle [ 'genIdx' ] == genIdx : 
~~~ useParticle = particle 
~~ ~~ if not self . _speculativeParticles : 
swarmId , genIdx = genIdx , matured = False ) 
~~ ~~ particle = Particle ( hsObj = self , 
evolveFromState = useParticle ) 
~~ ~~ ~~ def _okToExit ( self ) : 
if not self . _jobCancelled : 
~~~ ( _ , modelIds , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( matured = False ) 
if len ( modelIds ) > 0 : 
time . sleep ( 5.0 * random . random ( ) ) 
~~ ~~ ( _ , modelIds , _ , _ , _ ) = self . _resultsDB . getParticleInfos ( completed = False ) 
self . _cjDAO . modelSetFields ( modelId , 
dict ( engStop = ClientJobsDAO . STOP_REASON_STOPPED ) , 
~~ self . _hsStatePeriodicUpdate ( ) 
pctFieldContributions , absFieldContributions = self . _hsState . getFieldContributions ( ) 
jobResultsStr = self . _cjDAO . jobGetFields ( self . _jobID , [ 'results' ] ) [ 0 ] 
~~ if pctFieldContributions != jobResults . get ( 'fieldContributions' , None ) : 
~~~ jobResults [ 'fieldContributions' ] = pctFieldContributions 
jobResults [ 'absoluteFieldContributions' ] = absFieldContributions 
isUpdated = self . _cjDAO . jobSetFieldIfEqual ( self . _jobID , 
pctFieldContributions ) 
~~ def createModels ( self , numModels = 1 ) : 
self . _checkForOrphanedModels ( ) 
modelResults = [ ] 
for _ in xrange ( numModels ) : 
~~~ candidateParticle = None 
if ( self . _maxModels is not None and 
( self . _resultsDB . numModels ( ) - self . _resultsDB . getNumErrModels ( ) ) >= 
self . _maxModels ) : 
~~~ return ( self . _okToExit ( ) , [ ] ) 
~~ if candidateParticle is None : 
~~~ ( exitNow , candidateParticle , candidateSwarm ) = ( 
self . _getCandidateParticleAndSwarm ( ) ) 
~~~ if exitNow : 
time . sleep ( self . _speculativeWaitSecondsMax * random . random ( ) ) 
return ( False , [ ] ) 
~~ ~~ useEncoders = candidateSwarm . split ( '.' ) 
numAttempts = 0 
~~~ if numAttempts >= 1 : 
candidateParticle . agitate ( ) 
~~ position = candidateParticle . getPosition ( ) 
structuredParams = dict ( ) 
def _buildStructuredParams ( value , keys ) : 
~~~ flatKey = _flattenKeys ( keys ) 
if flatKey in self . _encoderNames : 
~~~ if flatKey in useEncoders : 
~~~ return value . getDict ( flatKey , position ) 
~~ ~~ elif flatKey in position : 
~~~ return position [ flatKey ] 
~~ ~~ structuredParams = rCopy ( self . _permutations , 
_buildStructuredParams , 
discardNoneKeys = False ) 
modelParams = dict ( 
structuredParams = structuredParams , 
particleState = candidateParticle . getState ( ) 
m = hashlib . md5 ( ) 
m . update ( sortedJSONDumpS ( structuredParams ) ) 
m . update ( self . _baseDescriptionHash ) 
paramsHash = m . digest ( ) 
particleInst = "%s.%s" % ( modelParams [ 'particleState' ] [ 'id' ] , 
modelParams [ 'particleState' ] [ 'genIdx' ] ) 
particleHash = hashlib . md5 ( particleInst ) . digest ( ) 
numAttempts += 1 
if self . _filterFunc and not self . _filterFunc ( structuredParams ) : 
~~~ valid = False 
~~~ valid = True 
~~ if valid and self . _resultsDB . getModelIDFromParamsHash ( paramsHash ) is None : 
~~ if numAttempts >= self . _maxUniqueModelAttempts : 
~~~ ( exitNow , candidateParticle , candidateSwarm ) = self . _getCandidateParticleAndSwarm ( 
exhaustedSwarmId = candidateSwarm ) 
if candidateParticle is None : 
~~~ time . sleep ( self . _speculativeWaitSecondsMax * random . random ( ) ) 
~~ ~~ numAttempts = 0 
useEncoders = candidateSwarm . split ( '.' ) 
~~ ~~ if self . logger . getEffectiveLevel ( ) <= logging . DEBUG : 
% ( pprint . pformat ( modelParams , indent = 4 ) ) ) 
~~ modelResults . append ( ( modelParams , paramsHash , particleHash ) ) 
~~ return ( False , modelResults ) 
~~ def recordModelProgress ( self , modelID , modelParams , modelParamsHash , results , 
if results is None : 
~~~ metricResult = None 
~~~ metricResult = results [ 1 ] . values ( ) [ 0 ] 
~~ errScore = self . _resultsDB . update ( modelID = modelID , 
modelParams = modelParams , modelParamsHash = modelParamsHash , 
metricResult = metricResult , completed = completed , 
completionReason = completionReason , matured = matured , 
numRecords = numRecords ) 
modelID , completed , completionReason , numRecords , errScore ) 
( bestModelID , bestResult ) = self . _resultsDB . bestModelIdAndErrScore ( ) 
~~ def runModel ( self , modelID , jobID , modelParams , modelParamsHash , 
jobsDAO , modelCheckpointGUID ) : 
if not self . _createCheckpoints : 
~~~ modelCheckpointGUID = None 
~~ self . _resultsDB . update ( modelID = modelID , 
modelParams = modelParams , 
completed = False , 
completionReason = None , 
matured = False , 
structuredParams = modelParams [ 'structuredParams' ] 
if self . logger . getEffectiveLevel ( ) <= logging . DEBUG : 
~~ cpuTimeStart = time . clock ( ) 
logLevel = self . logger . getEffectiveLevel ( ) 
~~~ if self . _dummyModel is None or self . _dummyModel is False : 
~~~ ( cmpReason , cmpMsg ) = runModelGivenBaseAndParams ( 
baseDescription = self . _baseDescription , 
params = structuredParams , 
predictedField = self . _predictedField , 
reportKeys = self . _reportKeys , 
optimizeKey = self . _optimizeKey , 
predictionCacheMaxRecords = self . _predictionCacheMaxRecords ) 
~~~ dummyParams = dict ( self . _dummyModel ) 
dummyParams [ 'permutationParams' ] = structuredParams 
if self . _dummyModelParamsFunc is not None : 
~~~ permInfo = dict ( structuredParams ) 
permInfo [ 'generation' ] = modelParams [ 'particleState' ] [ 'genIdx' ] 
dummyParams . update ( self . _dummyModelParamsFunc ( permInfo ) ) 
~~ ( cmpReason , cmpMsg ) = runDummyModel ( 
params = dummyParams , 
~~ jobsDAO . modelSetCompleted ( modelID , 
completionReason = cmpReason , 
completionMsg = cmpMsg , 
cpuTime = time . clock ( ) - cpuTimeStart ) 
~~ except InvalidConnectionException , e : 
~~~ self . logger . warn ( "%s" , e ) 
~~ ~~ def _escape ( s ) : 
s = s . replace ( "\\\\" , "\\\\\\\\" ) 
s = s . replace ( "\\n" , "\\\\n" ) 
s = s . replace ( "\\t" , "\\\\t" ) 
s = s . replace ( "," , "\\t" ) 
~~ def _engineServicesRunning ( ) : 
process = subprocess . Popen ( [ "ps" , "aux" ] , stdout = subprocess . PIPE ) 
stdout = process . communicate ( ) [ 0 ] 
result = process . returncode 
if result != 0 : 
~~ running = False 
~~~ if "python" in line and "clientjobmanager.client_job_manager" in line : 
~~~ running = True 
~~ ~~ return running 
~~ def runWithConfig ( swarmConfig , options , 
outDir = None , outputLabel = "default" , 
permWorkDir = None , verbosity = 1 ) : 
global g_currentVerbosityLevel 
g_currentVerbosityLevel = verbosity 
if outDir is None : 
~~~ outDir = os . getcwd ( ) 
~~ if permWorkDir is None : 
~~~ permWorkDir = os . getcwd ( ) 
~~ _checkOverwrite ( options , outDir ) 
_generateExpFilesFromSwarmDescription ( swarmConfig , outDir ) 
options [ "expDescConfig" ] = swarmConfig 
options [ "outputLabel" ] = outputLabel 
options [ "outDir" ] = outDir 
options [ "permWorkDir" ] = permWorkDir 
runOptions = _injectDefaultOptions ( options ) 
_validateOptions ( runOptions ) 
return _runAction ( runOptions ) 
~~ def runWithJsonFile ( expJsonFilePath , options , outputLabel , permWorkDir ) : 
if "verbosityCount" in options : 
~~~ verbosity = options [ "verbosityCount" ] 
del options [ "verbosityCount" ] 
~~~ verbosity = 1 
~~ _setupInterruptHandling ( ) 
with open ( expJsonFilePath , "r" ) as jsonFile : 
~~~ expJsonConfig = json . loads ( jsonFile . read ( ) ) 
~~ outDir = os . path . dirname ( expJsonFilePath ) 
return runWithConfig ( expJsonConfig , options , outDir = outDir , 
outputLabel = outputLabel , permWorkDir = permWorkDir , 
verbosity = verbosity ) 
~~ def runWithPermutationsScript ( permutationsFilePath , options , 
outputLabel , permWorkDir ) : 
~~~ g_currentVerbosityLevel = options [ "verbosityCount" ] 
~~~ g_currentVerbosityLevel = 1 
options [ "permutationsScriptPath" ] = permutationsFilePath 
options [ "outDir" ] = permWorkDir 
~~ def _backupFile ( filePath ) : 
assert os . path . exists ( filePath ) 
stampNum = 0 
( prefix , suffix ) = os . path . splitext ( filePath ) 
~~~ backupPath = "%s.%d%s" % ( prefix , stampNum , suffix ) 
stampNum += 1 
if not os . path . exists ( backupPath ) : 
~~ ~~ shutil . copyfile ( filePath , backupPath ) 
return backupPath 
~~ def _iterModels ( modelIDs ) : 
class ModelInfoIterator ( object ) : 
__CACHE_LIMIT = 1000 
debug = False 
def __init__ ( self , modelIDs ) : 
self . __modelIDs = tuple ( modelIDs ) 
if self . debug : 
~~~ _emit ( Verbosity . DEBUG , 
~~ self . __nextIndex = 0 
self . __modelCache = collections . deque ( ) 
~~ def __iter__ ( self ) : 
~~ def next ( self ) : 
return self . __getNext ( ) 
~~ def __getNext ( self ) : 
len ( self . __modelCache ) ) ) 
~~ if not self . __modelCache : 
~~~ self . __fillCache ( ) 
~~~ raise StopIteration ( ) 
~~ return self . __modelCache . popleft ( ) 
~~ def __fillCache ( self ) : 
assert ( not self . __modelCache ) 
numModelIDs = len ( self . __modelIDs ) if self . __modelIDs else 0 
if self . __nextIndex >= numModelIDs : 
~~ idRange = self . __nextIndex + self . __CACHE_LIMIT 
if idRange > numModelIDs : 
~~~ idRange = numModelIDs 
~~ lookupIDs = self . __modelIDs [ self . __nextIndex : idRange ] 
self . __nextIndex += ( idRange - self . __nextIndex ) 
infoList = _clientJobsDB ( ) . modelsInfo ( lookupIDs ) 
for rawInfo in infoList : 
~~~ modelInfo = _NupicModelInfo ( rawInfo = rawInfo ) 
self . __modelCache . append ( modelInfo ) 
~~ ~~ ~~ return ModelInfoIterator ( modelIDs ) 
~~ def pickupSearch ( self ) : 
self . __searchJob = self . loadSavedHyperSearchJob ( 
permWorkDir = self . _options [ "permWorkDir" ] , 
outputLabel = self . _options [ "outputLabel" ] ) 
self . monitorSearchJob ( ) 
~~ def monitorSearchJob ( self ) : 
assert self . __searchJob is not None 
jobID = self . __searchJob . getJobID ( ) 
startTime = time . time ( ) 
lastUpdateTime = datetime . now ( ) 
expectedNumModels = self . __searchJob . getExpectedNumModels ( 
searchMethod = self . _options [ "searchMethod" ] ) 
lastNumFinished = 0 
finishedModelIDs = set ( ) 
finishedModelStats = _ModelStats ( ) 
lastWorkerState = None 
lastJobResults = None 
lastModelMilestones = None 
lastEngStatus = None 
hyperSearchFinished = False 
while not hyperSearchFinished : 
~~~ jobInfo = self . __searchJob . getJobStatus ( self . _workers ) 
hyperSearchFinished = jobInfo . isFinished ( ) 
modelIDs = self . __searchJob . queryModelIDs ( ) 
_emit ( Verbosity . DEBUG , 
len ( modelIDs ) , len ( finishedModelIDs ) ) ) 
if len ( modelIDs ) > 0 : 
~~~ checkModelIDs = [ ] 
for modelID in modelIDs : 
~~~ if modelID not in finishedModelIDs : 
~~~ checkModelIDs . append ( modelID ) 
~~ ~~ del modelIDs 
if checkModelIDs : 
errorCompletionMsg = None 
for ( i , modelInfo ) in enumerate ( _iterModels ( checkModelIDs ) ) : 
if modelInfo . isFinished ( ) : 
~~~ finishedModelIDs . add ( modelInfo . getModelID ( ) ) 
finishedModelStats . update ( modelInfo ) 
if ( modelInfo . getCompletionReason ( ) . isError ( ) and 
not errorCompletionMsg ) : 
~~~ errorCompletionMsg = modelInfo . getCompletionMsg ( ) 
~~ metrics = modelInfo . getReportMetrics ( ) 
self . __foundMetrcsKeySet . update ( metrics . keys ( ) ) 
~~ ~~ ~~ numFinished = len ( finishedModelIDs ) 
if numFinished != lastNumFinished : 
~~~ lastNumFinished = numFinished 
if expectedNumModels is None : 
~~~ expModelsStr = "" 
~~ stats = finishedModelStats 
jobID , 
numFinished , 
expModelsStr , 
#stats.numCompletedSuccess, 
( stats . numCompletedEOF + stats . numCompletedStopped ) , 
"EOF" if stats . numCompletedEOF else "eof" , 
stats . numCompletedEOF , 
"STOPPED" if stats . numCompletedStopped else "stopped" , 
stats . numCompletedStopped , 
"KILLED" if stats . numCompletedKilled else "killed" , 
stats . numCompletedKilled , 
"ERROR" if stats . numCompletedError else "error" , 
stats . numCompletedError , 
"ORPHANED" if stats . numCompletedError else "orphaned" , 
stats . numCompletedOrphaned , 
"UNKNOWN" if stats . numCompletedOther else "unknown" , 
stats . numCompletedOther ) ) 
if errorCompletionMsg : 
~~ ~~ workerState = jobInfo . getWorkerState ( ) 
if workerState != lastWorkerState : 
indent = 4 ) ) 
lastWorkerState = workerState 
~~ jobResults = jobInfo . getResults ( ) 
if jobResults != lastJobResults : 
lastJobResults = jobResults 
~~ modelMilestones = jobInfo . getModelMilestones ( ) 
if modelMilestones != lastModelMilestones : 
pprint . pformat ( modelMilestones , indent = 4 ) ) 
lastModelMilestones = modelMilestones 
~~ engStatus = jobInfo . getEngStatus ( ) 
if engStatus != lastEngStatus : 
lastEngStatus = engStatus 
~~ ~~ if not hyperSearchFinished : 
~~~ if self . _options [ "timeout" ] != None : 
~~~ if ( ( datetime . now ( ) - lastUpdateTime ) > 
timedelta ( minutes = self . _options [ "timeout" ] ) ) : 
self . __cjDAO . jobCancel ( jobID ) 
~~ ~~ time . sleep ( 1 ) 
~~ ~~ modelIDs = self . __searchJob . queryModelIDs ( ) 
jobInfo = self . __searchJob . getJobStatus ( self . _workers ) 
~~ def _launchWorkers ( self , cmdLine , numWorkers ) : 
self . _workers = [ ] 
for i in range ( numWorkers ) : 
~~~ stdout = tempfile . NamedTemporaryFile ( delete = False ) 
stderr = tempfile . NamedTemporaryFile ( delete = False ) 
p = subprocess . Popen ( cmdLine , bufsize = 1 , env = os . environ , shell = True , 
stdin = None , stdout = stdout , stderr = stderr ) 
p . _stderr_file = stderr 
p . _stdout_file = stdout 
self . _workers . append ( p ) 
~~ ~~ def __startSearch ( self ) : 
params = _ClientJobUtils . makeSearchJobParamsDict ( options = self . _options , 
forRunning = True ) 
if self . _options [ "action" ] == "dryRun" : 
~~~ args = [ sys . argv [ 0 ] , "--params=%s" % ( json . dumps ( params ) ) ] 
print 
print "==================================================================" 
jobID = hypersearch_worker . main ( args ) 
~~~ cmdLine = _setUpExports ( self . _options [ "exports" ] ) 
cmdLine += "$HYPERSEARCH" 
maxWorkers = self . _options [ "maxWorkers" ] 
jobID = self . __cjDAO . jobInsert ( 
client = "GRP" , 
cmdLine = cmdLine , 
params = json . dumps ( params ) , 
minimumWorkers = 1 , 
maximumWorkers = maxWorkers , 
jobType = self . __cjDAO . JOB_TYPE_HS ) 
self . _launchWorkers ( cmdLine , maxWorkers ) 
~~ searchJob = _HyperSearchJob ( jobID ) 
self . __saveHyperSearchJobID ( 
outputLabel = self . _options [ "outputLabel" ] , 
hyperSearchJob = searchJob ) 
~~ return searchJob 
~~ def generateReport ( cls , 
options , 
replaceReport , 
hyperSearchJob , 
metricsKeys ) : 
if hyperSearchJob is None : 
~~~ hyperSearchJob = cls . loadSavedHyperSearchJob ( 
permWorkDir = options [ "permWorkDir" ] , 
outputLabel = options [ "outputLabel" ] ) 
~~ modelIDs = hyperSearchJob . queryModelIDs ( ) 
bestModel = None 
metricstmp = set ( ) 
searchVar = set ( ) 
for modelInfo in _iterModels ( modelIDs ) : 
~~~ if modelInfo . isFinished ( ) : 
~~~ vars = modelInfo . getParamLabels ( ) . keys ( ) 
searchVar . update ( vars ) 
metrics = modelInfo . getReportMetrics ( ) 
metricstmp . update ( metrics . keys ( ) ) 
~~ ~~ if metricsKeys is None : 
~~~ metricsKeys = metricstmp 
~~ reportWriter = _ReportCSVWriter ( hyperSearchJob = hyperSearchJob , 
metricsKeys = metricsKeys , 
searchVar = searchVar , 
outputDirAbsPath = options [ "permWorkDir" ] , 
outputLabel = options [ "outputLabel" ] , 
replaceReport = replaceReport ) 
modelStats = _ModelStats ( ) 
print "----------------------------------------------------------------" 
searchParams = hyperSearchJob . getParams ( ) 
( optimizationMetricKey , maximizeMetric ) = ( 
_PermutationUtils . getOptimizationMetricInfo ( searchParams ) ) 
formatStr = None 
foundMetricsKeySet = set ( metricsKeys ) 
sortedMetricsKeys = [ ] 
jobInfo = _clientJobsDB ( ) . jobInfo ( hyperSearchJob . getJobID ( ) ) 
if jobInfo . cancel == 1 : 
~~~ raise Exception ( jobInfo . workerCompletionMsg ) 
~~~ results = json . loads ( jobInfo . results ) 
~~ bestModelNum = results [ "bestModel" ] 
bestModelIterIndex = None 
totalWallTime = 0 
totalRecords = 0 
scoreModelIDDescList = [ ] 
for ( i , modelInfo ) in enumerate ( _iterModels ( modelIDs ) ) : 
~~~ reportWriter . emit ( modelInfo ) 
totalRecords += modelInfo . getNumRecords ( ) 
startTime = modelInfo . getStartTime ( ) 
~~~ endTime = modelInfo . getEndTime ( ) 
st = datetime . strptime ( startTime , format ) 
et = datetime . strptime ( endTime , format ) 
totalWallTime += ( et - st ) . seconds 
~~ modelStats . update ( modelInfo ) 
expDesc = modelInfo . getModelDescription ( ) 
reportMetrics = modelInfo . getReportMetrics ( ) 
optimizationMetrics = modelInfo . getOptimizationMetrics ( ) 
if modelInfo . getModelID ( ) == bestModelNum : 
~~~ bestModel = modelInfo 
bestModelIterIndex = i 
bestMetric = optimizationMetrics . values ( ) [ 0 ] 
~~ if optimizationMetrics : 
~~~ assert len ( optimizationMetrics ) == 1 , ( 
len ( optimizationMetrics ) , optimizationMetrics , modelInfo ) ) 
~~ if modelInfo . getCompletionReason ( ) . isEOF ( ) : 
~~~ scoreModelIDDescList . append ( ( optimizationMetrics . values ( ) [ 0 ] , 
modelInfo . getModelID ( ) , 
modelInfo . getGeneratedDescriptionFile ( ) , 
modelInfo . getParamLabels ( ) ) ) 
if ( modelInfo . isFinished ( ) and 
not ( modelInfo . getCompletionReason ( ) . isStopped or 
modelInfo . getCompletionReason ( ) . isEOF ( ) ) ) : 
~~ if reportMetrics : 
~~~ foundMetricsKeySet . update ( reportMetrics . iterkeys ( ) ) 
if len ( sortedMetricsKeys ) != len ( foundMetricsKeySet ) : 
~~~ sortedMetricsKeys = sorted ( foundMetricsKeySet ) 
maxKeyLen = max ( [ len ( k ) for k in sortedMetricsKeys ] ) 
~~ for key in sortedMetricsKeys : 
~~~ if key in reportMetrics : 
~~~ if key == optimizationMetricKey : 
~~~ m = "%r" % reportMetrics [ key ] 
~~ print formatStr % ( key + ":" ) , m 
~~ ~~ print 
~~ ~~ print "--------------------------------------------------------------" 
len ( modelIDs ) , 
if ( modelStats . numCompletedKilled + modelStats . numCompletedEOF ) == 
len ( modelIDs ) 
len ( modelIDs ) - ( 
modelStats . numCompletedKilled + modelStats . numCompletedEOF + 
modelStats . numCompletedStopped ) ) ) ) 
if modelStats . numStatusOther > 0 : 
modelStats . numStatusOther ) 
if modelStats . numCompletedOther > 0 : 
modelStats . numCompletedOther ) 
assert modelStats . numStatusOther == 0 , "numStatusOther=%s" % ( 
assert modelStats . numCompletedOther == 0 , "numCompletedOther=%s" % ( 
global gCurrentSearch 
jobStatus = hyperSearchJob . getJobStatus ( gCurrentSearch . _workers ) 
jobResults = jobStatus . getResults ( ) 
if "fieldContributions" in jobResults : 
pprint . pprint ( jobResults [ "fieldContributions" ] , indent = 4 ) 
~~ if bestModel is not None : 
~~~ maxKeyLen = max ( [ len ( k ) for k in sortedMetricsKeys ] ) 
maxKeyLen = max ( maxKeyLen , len ( optimizationMetricKey ) ) 
bestMetricValue = bestModel . getOptimizationMetrics ( ) . values ( ) [ 0 ] 
optimizationMetricName = bestModel . getOptimizationMetrics ( ) . keys ( ) [ 0 ] 
optimizationMetricName , maximizeMetric ) 
bestModelIterIndex , bestModel , bestModel . getModelDescription ( ) ) 
print formatStr % ( optimizationMetricName + ":" ) , bestMetricValue 
hsJobParams = hyperSearchJob . getParams ( ) 
~~ if options [ "genTopNDescriptions" ] > 0 : 
options [ "genTopNDescriptions" ] ) 
scoreModelIDDescList . sort ( ) 
scoreModelIDDescList = scoreModelIDDescList [ 
0 : options [ "genTopNDescriptions" ] ] 
i = - 1 
for ( score , modelID , description , paramLabels ) in scoreModelIDDescList : 
~~~ i += 1 
outDir = os . path . join ( options [ "permWorkDir" ] , "model_%d" % ( i ) ) 
if not os . path . exists ( outDir ) : 
~~~ os . makedirs ( outDir ) 
~~ base_description_path = os . path . join ( options [ "outDir" ] , 
"description.py" ) 
base_description_relpath = os . path . relpath ( base_description_path , 
start = outDir ) 
description = description . replace ( 
"importBaseDescription(\ , 
"importBaseDescription(\ % base_description_relpath ) 
fd = open ( os . path . join ( outDir , "description.py" ) , "wb" ) 
fd . write ( description ) 
fd = open ( os . path . join ( outDir , "params.csv" ) , "wb" ) 
writer = csv . writer ( fd ) 
colNames = paramLabels . keys ( ) 
colNames . sort ( ) 
writer . writerow ( colNames ) 
row = [ paramLabels [ x ] for x in colNames ] 
writer . writerow ( row ) 
mod = imp . load_source ( "description" , os . path . join ( outDir , 
"description.py" ) ) 
model_description = mod . descriptionInterface . getModelDescription ( ) 
fd = open ( os . path . join ( outDir , "model_params.py" ) , "wb" ) 
pprint . pformat ( model_description ) ) ) 
~~ reportWriter . finalize ( ) 
return model_description 
~~ def loadSavedHyperSearchJob ( cls , permWorkDir , outputLabel ) : 
jobID = cls . __loadHyperSearchJobID ( permWorkDir = permWorkDir , 
outputLabel = outputLabel ) 
searchJob = _HyperSearchJob ( nupicJobID = jobID ) 
return searchJob 
~~ def __saveHyperSearchJobID ( cls , permWorkDir , outputLabel , hyperSearchJob ) : 
jobID = hyperSearchJob . getJobID ( ) 
filePath = cls . __getHyperSearchJobIDFilePath ( permWorkDir = permWorkDir , 
if os . path . exists ( filePath ) : 
~~~ _backupFile ( filePath ) 
~~ d = dict ( hyperSearchJobID = jobID ) 
with open ( filePath , "wb" ) as jobIdPickleFile : 
~~~ pickle . dump ( d , jobIdPickleFile ) 
~~ ~~ def __loadHyperSearchJobID ( cls , permWorkDir , outputLabel ) : 
jobID = None 
with open ( filePath , "r" ) as jobIdPickleFile : 
~~~ jobInfo = pickle . load ( jobIdPickleFile ) 
jobID = jobInfo [ "hyperSearchJobID" ] 
~~ return jobID 
~~ def __getHyperSearchJobIDFilePath ( cls , permWorkDir , outputLabel ) : 
basePath = permWorkDir 
filename = "%s_HyperSearchJobID.pkl" % ( outputLabel , ) 
filepath = os . path . join ( basePath , filename ) 
return filepath 
~~ def emit ( self , modelInfo ) : 
if self . __csvFileObj is None : 
~~~ self . __openAndInitCSVFile ( modelInfo ) 
~~ csv = self . __csvFileObj 
~~ if not modelInfo . isWaitingToStart ( ) : 
~~ if modelInfo . isFinished ( ) : 
endTime = modelInfo . getEndTime ( ) 
st = datetime . strptime ( startTime , dateFormat ) 
et = datetime . strptime ( endTime , dateFormat ) 
paramLabelsDict = modelInfo . getParamLabels ( ) 
for key in self . __sortedVariableNames : 
~~~ if key in paramLabelsDict : 
~~ ~~ metrics = modelInfo . getReportMetrics ( ) 
for key in self . __sortedMetricsKeys : 
~~~ value = metrics . get ( key , "NA" ) 
value = str ( value ) 
~~ print >> csv 
~~ def finalize ( self ) : 
if self . __csvFileObj is not None : 
~~~ self . __csvFileObj . close ( ) 
self . __csvFileObj = None 
if self . __backupCSVPath : 
~~ ~~ def __openAndInitCSVFile ( self , modelInfo ) : 
basePath = self . __outputDirAbsPath 
reportCSVName = "%s_Report.csv" % ( self . __outputLabel , ) 
reportCSVPath = self . __reportCSVPath = os . path . join ( basePath , reportCSVName ) 
backupCSVPath = None 
if os . path . exists ( reportCSVPath ) : 
~~~ backupCSVPath = self . __backupCSVPath = _backupFile ( reportCSVPath ) 
~~ if self . __replaceReport : 
~~~ mode = "w" 
~~~ mode = "a" 
~~ csv = self . __csvFileObj = open ( reportCSVPath , mode ) 
if not self . __replaceReport and backupCSVPath : 
~~~ print >> csv 
print >> csv 
~~ for key in self . __sortedMetricsKeys : 
~~ def getJobStatus ( self , workers ) : 
jobInfo = self . JobStatus ( self . __nupicJobID , workers ) 
return jobInfo 
~~ def queryModelIDs ( self ) : 
jobID = self . getJobID ( ) 
modelCounterPairs = _clientJobsDB ( ) . modelsGetUpdateCounters ( jobID ) 
modelIDs = tuple ( x [ 0 ] for x in modelCounterPairs ) 
return modelIDs 
~~ def makeSearchJobParamsDict ( cls , options , forRunning = False ) : 
if options [ "searchMethod" ] == "v2" : 
~~~ hsVersion = "v2" 
~~ maxModels = options [ "maxPermutations" ] 
if options [ "action" ] == "dryRun" and maxModels is None : 
~~~ maxModels = 1 
~~ useTerminators = options [ "useTerminators" ] 
if useTerminators is None : 
"hsVersion" : hsVersion , 
"maxModels" : maxModels , 
"useTerminators" : useTerminators , 
~~ if forRunning : 
~~~ params [ "persistentJobGUID" ] = str ( uuid . uuid1 ( ) ) 
~~ if options [ "permutationsScriptPath" ] : 
~~~ params [ "permutationsPyFilename" ] = options [ "permutationsScriptPath" ] 
~~ elif options [ "expDescConfig" ] : 
~~~ params [ "description" ] = options [ "expDescConfig" ] 
~~~ with open ( options [ "expDescJsonPath" ] , mode = "r" ) as fp : 
~~~ params [ "description" ] = json . load ( fp ) 
~~ ~~ return params 
~~ def getOptimizationMetricInfo ( cls , searchJobParams ) : 
if searchJobParams [ "hsVersion" ] == "v2" : 
~~~ search = HypersearchV2 ( searchParams = searchJobParams ) 
~~ info = search . getOptimizationMetricInfo ( ) 
~~ def getModelDescription ( self ) : 
params = self . __unwrapParams ( ) 
if "experimentName" in params : 
~~~ return params [ "experimentName" ] 
~~~ paramSettings = self . getParamLabels ( ) 
items = [ ] 
for key , value in paramSettings . items ( ) : 
~~~ items . append ( "%s_%s" % ( key , value ) ) 
~~ return "." . join ( items ) 
~~ ~~ def getParamLabels ( self ) : 
if "particleState" in params : 
~~~ retval = dict ( ) 
queue = [ ( pair , retval ) for pair in 
params [ "particleState" ] [ "varStates" ] . iteritems ( ) ] 
while len ( queue ) > 0 : 
~~~ pair , output = queue . pop ( ) 
k , v = pair 
if ( "position" in v and "bestPosition" in v and 
"velocity" in v ) : 
~~~ output [ k ] = v [ "position" ] 
~~~ if k not in output : 
~~~ output [ k ] = dict ( ) 
~~ queue . extend ( ( pair , output [ k ] ) for pair in v . iteritems ( ) ) 
~~ ~~ def __unwrapParams ( self ) : 
if self . __cachedParams is None : 
~~~ self . __cachedParams = json . loads ( self . __rawInfo . params ) 
~~ return self . __cachedParams 
~~ def getAllMetrics ( self ) : 
result = self . getReportMetrics ( ) 
result . update ( self . getOptimizationMetrics ( ) ) 
~~ def __unwrapResults ( self ) : 
if self . __cachedResults is None : 
~~~ if self . __rawInfo . results is not None : 
~~~ resultList = json . loads ( self . __rawInfo . results ) 
len ( resultList ) , resultList ) 
self . __cachedResults = self . ModelResults ( 
reportMetrics = resultList [ 0 ] , 
optimizationMetrics = resultList [ 1 ] ) 
~~~ self . __cachedResults = self . ModelResults ( 
reportMetrics = { } , 
optimizationMetrics = { } ) 
~~ ~~ return self . __cachedResults 
~~ def getData ( self , n ) : 
records = [ self . getNext ( ) for x in range ( n ) ] 
return records 
~~ def getTerminationCallbacks ( self , terminationFunc ) : 
activities = [ None ] * len ( ModelTerminator . _MILESTONES ) 
for index , ( iteration , _ ) in enumerate ( ModelTerminator . _MILESTONES ) : 
~~~ cb = functools . partial ( terminationFunc , index = index ) 
activities [ index ] = PeriodicActivityRequest ( repeating = False , 
period = iteration , 
cb = cb ) 
~~ ~~ def groupby2 ( * args ) : 
if len ( args ) % 2 == 1 : 
~~ advanceList = [ ] 
for i in xrange ( 0 , len ( args ) , 2 ) : 
~~~ listn = args [ i ] 
fn = args [ i + 1 ] 
if listn is not None : 
~~~ generatorList . append ( groupby ( listn , fn ) ) 
~~~ generatorList . append ( None ) 
advanceList . append ( False ) 
~~ ~~ n = len ( generatorList ) 
nextList = [ None ] * n 
~~~ for i in xrange ( n ) : 
~~~ if advanceList [ i ] : 
~~~ nextList [ i ] = generatorList [ i ] . next ( ) 
~~~ nextList [ i ] = None 
~~ ~~ ~~ if all ( entry is None for entry in nextList ) : 
~~ minKeyVal = min ( nextVal [ 0 ] for nextVal in nextList 
if nextVal is not None ) 
retGroups = [ minKeyVal ] 
for i in xrange ( n ) : 
~~~ if nextList [ i ] is not None and nextList [ i ] [ 0 ] == minKeyVal : 
~~~ retGroups . append ( nextList [ i ] [ 1 ] ) 
advanceList [ i ] = True 
~~~ advanceList [ i ] = False 
retGroups . append ( None ) 
~~ ~~ yield tuple ( retGroups ) 
~~ ~~ def _openStream ( dataUrl , 
bookmark , 
firstRecordIdx ) : 
filePath = dataUrl [ len ( FILE_PREF ) : ] 
if not os . path . isabs ( filePath ) : 
~~~ filePath = os . path . join ( os . getcwd ( ) , filePath ) 
~~ return FileRecordStream ( streamID = filePath , 
write = False , 
bookmark = bookmark , 
firstRecord = firstRecordIdx ) 
~~ def getNextRecord ( self ) : 
~~~ if self . _sourceLastRecordIdx is not None and self . _recordStore . getNextRecordIdx ( ) >= self . _sourceLastRecordIdx : 
bookmark = self . _recordStore . getBookmark ( ) 
~~~ preAggValues = self . _recordStore . getNextRecord ( ) 
~~~ if self . _eofOnTimeout : 
self . _recordStore . getNextRecordIdx ( ) - 1 , preAggValues ) 
( fieldValues , aggBookmark ) = self . _aggregator . next ( preAggValues , bookmark ) 
if fieldValues is not None : 
~~~ self . _aggBookmark = aggBookmark 
~~ if preAggValues is None and fieldValues is None : 
~~ if fieldValues is not None : 
~~ ~~ if self . _needFieldsFiltering : 
~~~ values = [ ] 
srcDict = dict ( zip ( self . _recordStoreFieldNames , fieldValues ) ) 
for name in self . _streamFieldNames : 
~~~ values . append ( srcDict [ name ] ) 
~~ fieldValues = values 
~~ if self . _writer is not None : 
~~~ self . _writer . appendRecord ( fieldValues ) 
~~ self . _recordCount += 1 
self . _recordCount - 1 , fieldValues , self . _aggBookmark ) 
return fieldValues 
~~ def getDataRowCount ( self ) : 
inputRowCountAfterAggregation = 0 
~~~ record = self . getNextRecord ( ) 
if record is None : 
~~~ return inputRowCountAfterAggregation 
~~ inputRowCountAfterAggregation += 1 
if inputRowCountAfterAggregation > 10000 : 
~~ ~~ ~~ def getStats ( self ) : 
recordStoreStats = self . _recordStore . getStats ( ) 
streamStats = dict ( ) 
for ( key , values ) in recordStoreStats . items ( ) : 
~~~ fieldStats = dict ( zip ( self . _recordStoreFieldNames , values ) ) 
streamValues = [ ] 
~~~ streamValues . append ( fieldStats [ name ] ) 
~~ streamStats [ key ] = streamValues 
~~ return streamStats 
~~ def get ( self , number ) : 
if not number in self . _patterns : 
~~ return self . _patterns [ number ] 
~~ def addNoise ( self , bits , amount ) : 
newBits = set ( ) 
for bit in bits : 
~~~ if self . _random . getReal64 ( ) < amount : 
~~~ newBits . add ( self . _random . getUInt32 ( self . _n ) ) 
~~~ newBits . add ( bit ) 
~~ ~~ return newBits 
~~ def numbersForBit ( self , bit ) : 
if bit >= self . _n : 
~~ numbers = set ( ) 
for index , pattern in self . _patterns . iteritems ( ) : 
~~~ if bit in pattern : 
~~~ numbers . add ( index ) 
~~ ~~ return numbers 
~~ def numberMapForBits ( self , bits ) : 
numberMap = dict ( ) 
~~~ numbers = self . numbersForBit ( bit ) 
for number in numbers : 
~~~ if not number in numberMap : 
~~~ numberMap [ number ] = set ( ) 
~~ numberMap [ number ] . add ( bit ) 
~~ ~~ return numberMap 
~~ def prettyPrintPattern ( self , bits , verbosity = 1 ) : 
numberMap = self . numberMapForBits ( bits ) 
text = "" 
numberList = [ ] 
numberItems = sorted ( numberMap . iteritems ( ) , 
key = lambda ( number , bits ) : len ( bits ) , 
reverse = True ) 
for number , bits in numberItems : 
~~~ if verbosity > 2 : 
~~~ strBits = [ str ( n ) for n in bits ] 
~~ elif verbosity > 1 : 
~~~ numberText = str ( number ) 
~~ numberList . append ( numberText ) 
return text 
~~ def _generate ( self ) : 
candidates = np . array ( range ( self . _n ) , np . uint32 ) 
for i in xrange ( self . _num ) : 
~~~ self . _random . shuffle ( candidates ) 
pattern = candidates [ 0 : self . _getW ( ) ] 
self . _patterns [ i ] = set ( pattern ) 
~~ ~~ def _getW ( self ) : 
w = self . _w 
if type ( w ) is list : 
~~~ return w [ self . _random . getUInt32 ( len ( w ) ) ] 
~~~ return w 
~~ ~~ def _generate ( self ) : 
n = self . _n 
for i in xrange ( n / w ) : 
~~~ pattern = set ( xrange ( i * w , ( i + 1 ) * w ) ) 
self . _patterns [ i ] = pattern 
~~ ~~ def compute ( self , recordNum , patternNZ , classification , learn , infer ) : 
if self . verbosity >= 1 : 
~~ if len ( self . _patternNZHistory ) > 0 : 
~~~ if recordNum < self . _patternNZHistory [ - 1 ] [ 0 ] : 
~~ ~~ if len ( self . _patternNZHistory ) == 0 or recordNum > self . _patternNZHistory [ - 1 ] [ 0 ] : 
~~~ self . _patternNZHistory . append ( ( recordNum , patternNZ ) ) 
~~ retval = { } 
if max ( patternNZ ) > self . _maxInputIdx : 
~~~ newMaxInputIdx = max ( patternNZ ) 
for nSteps in self . steps : 
~~~ self . _weightMatrix [ nSteps ] = numpy . concatenate ( ( 
self . _weightMatrix [ nSteps ] , 
numpy . zeros ( shape = ( newMaxInputIdx - self . _maxInputIdx , 
self . _maxBucketIdx + 1 ) ) ) , axis = 0 ) 
~~ self . _maxInputIdx = int ( newMaxInputIdx ) 
~~ if classification is not None : 
~~~ if type ( classification [ "bucketIdx" ] ) is not list : 
~~~ bucketIdxList = [ classification [ "bucketIdx" ] ] 
actValueList = [ classification [ "actValue" ] ] 
numCategory = 1 
~~~ bucketIdxList = classification [ "bucketIdx" ] 
actValueList = classification [ "actValue" ] 
numCategory = len ( classification [ "bucketIdx" ] ) 
~~~ if learn : 
~~ actValueList = None 
bucketIdxList = None 
~~ if infer : 
~~~ retval = self . infer ( patternNZ , actValueList ) 
~~ if learn and classification [ "bucketIdx" ] is not None : 
~~~ for categoryI in range ( numCategory ) : 
~~~ bucketIdx = bucketIdxList [ categoryI ] 
actValue = actValueList [ categoryI ] 
if bucketIdx > self . _maxBucketIdx : 
~~~ for nSteps in self . steps : 
numpy . zeros ( shape = ( self . _maxInputIdx + 1 , 
bucketIdx - self . _maxBucketIdx ) ) ) , axis = 1 ) 
~~ self . _maxBucketIdx = int ( bucketIdx ) 
~~ while self . _maxBucketIdx > len ( self . _actualValues ) - 1 : 
~~~ self . _actualValues . append ( None ) 
~~ if self . _actualValues [ bucketIdx ] is None : 
~~~ self . _actualValues [ bucketIdx ] = actValue 
~~~ if ( isinstance ( actValue , int ) or 
isinstance ( actValue , float ) or 
isinstance ( actValue , long ) ) : 
~~~ self . _actualValues [ bucketIdx ] = ( ( 1.0 - self . actValueAlpha ) 
* self . _actualValues [ bucketIdx ] 
+ self . actValueAlpha * actValue ) 
~~ ~~ ~~ for ( learnRecordNum , learnPatternNZ ) in self . _patternNZHistory : 
~~~ error = self . _calculateError ( recordNum , bucketIdxList ) 
nSteps = recordNum - learnRecordNum 
if nSteps in self . steps : 
~~~ for bit in learnPatternNZ : 
~~~ self . _weightMatrix [ nSteps ] [ bit , : ] += self . alpha * error [ nSteps ] 
~~ ~~ ~~ ~~ if infer and self . verbosity >= 1 : 
for ( nSteps , votes ) in retval . items ( ) : 
~~~ if nSteps == "actualValues" : 
bestBucketIdx = votes . argmax ( ) 
retval [ "actualValues" ] [ bestBucketIdx ] ) ) 
~~ return retval 
~~ def infer ( self , patternNZ , actValueList ) : 
if self . steps [ 0 ] == 0 or actValueList is None : 
~~~ defaultValue = 0 
~~~ defaultValue = actValueList [ 0 ] 
~~ actValues = [ x if x is not None else defaultValue 
for x in self . _actualValues ] 
retval = { "actualValues" : actValues } 
~~~ predictDist = self . inferSingleStep ( patternNZ , self . _weightMatrix [ nSteps ] ) 
retval [ nSteps ] = predictDist 
~~ def inferSingleStep ( self , patternNZ , weightMatrix ) : 
outputActivation = weightMatrix [ patternNZ ] . sum ( axis = 0 ) 
outputActivation = outputActivation - numpy . max ( outputActivation ) 
expOutputActivation = numpy . exp ( outputActivation ) 
predictDist = expOutputActivation / numpy . sum ( expOutputActivation ) 
return predictDist 
~~ def _calculateError ( self , recordNum , bucketIdxList ) : 
error = dict ( ) 
targetDist = numpy . zeros ( self . _maxBucketIdx + 1 ) 
numCategories = len ( bucketIdxList ) 
for bucketIdx in bucketIdxList : 
~~~ targetDist [ bucketIdx ] = 1.0 / numCategories 
~~ for ( learnRecordNum , learnPatternNZ ) in self . _patternNZHistory : 
~~~ nSteps = recordNum - learnRecordNum 
~~~ predictDist = self . inferSingleStep ( learnPatternNZ , 
self . _weightMatrix [ nSteps ] ) 
error [ nSteps ] = targetDist - predictDist 
~~ ~~ return error 
~~ def sort ( filename , key , outputFile , fields = None , watermark = 1024 * 1024 * 100 ) : 
if fields is not None : 
~~~ assert set ( key ) . issubset ( set ( [ f [ 0 ] for f in fields ] ) ) 
~~ with FileRecordStream ( filename ) as f : 
~~~ if fields : 
~~~ fieldNames = [ ff [ 0 ] for ff in fields ] 
indices = [ f . getFieldNames ( ) . index ( name ) for name in fieldNames ] 
assert len ( indices ) == len ( fields ) 
~~~ fileds = f . getFields ( ) 
fieldNames = f . getFieldNames ( ) 
indices = None 
~~ key = [ fieldNames . index ( name ) for name in key ] 
chunk = 0 
records = [ ] 
for i , r in enumerate ( f ) : 
~~~ if indices : 
~~~ temp = [ ] 
for i in indices : 
~~~ temp . append ( r [ i ] ) 
~~ r = temp 
~~ records . append ( r ) 
available_memory = psutil . avail_phymem ( ) 
if available_memory < watermark : 
~~~ _sortChunk ( records , key , chunk , fields ) 
chunk += 1 
~~ ~~ if len ( records ) > 0 : 
~~ _mergeFiles ( key , chunk , outputFile , fields ) 
~~ ~~ def _sortChunk ( records , key , chunkIndex , fields ) : 
assert len ( records ) > 0 
records . sort ( key = itemgetter ( * key ) ) 
if chunkIndex is not None : 
~~~ filename = 'chunk_%d.csv' % chunkIndex 
with FileRecordStream ( filename , write = True , fields = fields ) as o : 
~~~ for r in records : 
~~~ o . appendRecord ( r ) 
~~ ~~ assert os . path . getsize ( filename ) > 0 
~~ return records 
~~ def _mergeFiles ( key , chunkCount , outputFile , fields ) : 
title ( ) 
files = [ FileRecordStream ( 'chunk_%d.csv' % i ) for i in range ( chunkCount ) ] 
with FileRecordStream ( outputFile , write = True , fields = fields ) as o : 
~~~ files = [ FileRecordStream ( 'chunk_%d.csv' % i ) for i in range ( chunkCount ) ] 
records = [ f . getNextRecord ( ) for f in files ] 
while not all ( r is None for r in records ) : 
~~~ indices = [ i for i , r in enumerate ( records ) if r is not None ] 
records = [ records [ i ] for i in indices ] 
files = [ files [ i ] for i in indices ] 
r = min ( records , key = itemgetter ( * key ) ) 
o . appendRecord ( r ) 
index = records . index ( r ) 
records [ index ] = files [ index ] . getNextRecord ( ) 
~~ ~~ for i , f in enumerate ( files ) : 
~~~ f . close ( ) 
os . remove ( 'chunk_%d.csv' % i ) 
~~ ~~ def compute ( self , activeColumns , learn = True ) : 
bottomUpInput = numpy . zeros ( self . numberOfCols , dtype = dtype ) 
bottomUpInput [ list ( activeColumns ) ] = 1 
super ( TemporalMemoryShim , self ) . compute ( bottomUpInput , 
enableLearn = learn , 
enableInference = True ) 
predictedState = self . getPredictedState ( ) 
self . predictiveCells = set ( numpy . flatnonzero ( predictedState ) ) 
~~ def read ( cls , proto ) : 
tm = super ( TemporalMemoryShim , cls ) . read ( proto . baseTM ) 
tm . predictiveCells = set ( proto . predictedState ) 
tm . connections = Connections . read ( proto . conncetions ) 
super ( TemporalMemoryShim , self ) . write ( proto . baseTM ) 
proto . connections . write ( self . connections ) 
proto . predictiveCells = self . predictiveCells 
~~ def cPrint ( self , level , message , * args , ** kw ) : 
if level > self . consolePrinterVerbosity : 
~~ if len ( kw ) > 1 : 
~~ newline = kw . get ( "newline" , True ) 
if len ( kw ) == 1 and 'newline' not in kw : 
~~ if len ( args ) == 0 : 
~~~ if newline : 
~~~ print message 
~~~ print message , 
~~~ print message % args 
~~~ print message % args , 
~~ ~~ ~~ def profileTM ( tmClass , tmDim , nRuns ) : 
tm = tmClass ( numberOfCols = tmDim ) 
data = numpy . random . randint ( 0 , 2 , [ tmDim , nRuns ] ) . astype ( 'float32' ) 
for i in xrange ( nRuns ) : 
~~~ d = data [ : , i ] 
tm . compute ( d , True ) 
~~ ~~ def runPermutations ( args ) : 
helpString = ( 
"expGenerator/experimentDescriptionSchema.json." ) 
parser = optparse . OptionParser ( usage = helpString ) 
parser . add_option ( 
"--replaceReport" , dest = "replaceReport" , action = "store_true" , 
default = DEFAULT_OPTIONS [ "replaceReport" ] , 
"--action" , dest = "action" , default = DEFAULT_OPTIONS [ "action" ] , 
choices = [ "run" , "pickup" , "report" , "dryRun" ] , 
"--maxPermutations" , dest = "maxPermutations" , 
default = DEFAULT_OPTIONS [ "maxPermutations" ] , type = "int" , 
"--exports" , dest = "exports" , default = DEFAULT_OPTIONS [ "exports" ] , 
type = "string" , 
"--useTerminators" , dest = "useTerminators" , action = "store_true" , 
"--maxWorkers" , dest = "maxWorkers" , default = DEFAULT_OPTIONS [ "maxWorkers" ] , 
type = "int" , 
"-v" , dest = "verbosityCount" , action = "count" , default = 0 , 
"--timeout" , dest = "timeout" , default = DEFAULT_OPTIONS [ "timeout" ] , type = "int" , 
"--overwrite" , default = DEFAULT_OPTIONS [ "overwrite" ] , action = "store_true" , 
"--genTopNDescriptions" , dest = "genTopNDescriptions" , 
default = DEFAULT_OPTIONS [ "genTopNDescriptions" ] , type = "int" , 
( options , positionalArgs ) = parser . parse_args ( args ) 
if len ( positionalArgs ) != 1 : 
~~ fileArgPath = os . path . expanduser ( positionalArgs [ 0 ] ) 
fileArgPath = os . path . expandvars ( fileArgPath ) 
fileArgPath = os . path . abspath ( fileArgPath ) 
permWorkDir = os . path . dirname ( fileArgPath ) 
outputLabel = os . path . splitext ( os . path . basename ( fileArgPath ) ) [ 0 ] 
basename = os . path . basename ( fileArgPath ) 
fileExtension = os . path . splitext ( basename ) [ 1 ] 
optionsDict = vars ( options ) 
if fileExtension == ".json" : 
~~~ returnValue = permutations_runner . runWithJsonFile ( 
fileArgPath , optionsDict , outputLabel , permWorkDir ) 
~~~ returnValue = permutations_runner . runWithPermutationsScript ( 
~~ return returnValue 
~~ def _generateCategory ( filename = "simple.csv" , numSequences = 2 , elementsPerSeq = 1 , 
numRepeats = 10 , resets = False ) : 
fields = [ ( 'reset' , 'int' , 'R' ) , ( 'category' , 'int' , 'C' ) , 
( 'field1' , 'string' , '' ) ] 
for i in range ( numSequences ) : 
~~~ seq = [ x for x in range ( i * elementsPerSeq , ( i + 1 ) * elementsPerSeq ) ] 
sequences . append ( seq ) 
for i in range ( numRepeats ) : 
for x in seq : 
~~~ outFile . appendRecord ( [ reset , str ( seqIdx ) , str ( x ) ] ) 
altitude = None 
if len ( inputData ) == 4 : 
~~~ ( speed , longitude , latitude , altitude ) = inputData 
~~~ ( speed , longitude , latitude ) = inputData 
~~ coordinate = self . coordinateForPosition ( longitude , latitude , altitude ) 
radius = self . radiusForSpeed ( speed ) 
super ( GeospatialCoordinateEncoder , self ) . encodeIntoArray ( 
( coordinate , radius ) , output ) 
~~ def coordinateForPosition ( self , longitude , latitude , altitude = None ) : 
coords = PROJ ( longitude , latitude ) 
if altitude is not None : 
~~~ coords = transform ( PROJ , geocentric , coords [ 0 ] , coords [ 1 ] , altitude ) 
~~ coordinate = numpy . array ( coords ) 
coordinate = coordinate / self . scale 
return coordinate . astype ( int ) 
~~ def radiusForSpeed ( self , speed ) : 
overlap = 1.5 
coordinatesPerTimestep = speed * self . timestep / self . scale 
radius = int ( round ( float ( coordinatesPerTimestep ) / 2 * overlap ) ) 
minRadius = int ( math . ceil ( ( math . sqrt ( self . w ) - 1 ) / 2 ) ) 
return max ( radius , minRadius ) 
~~ def getSearch ( rootDir ) : 
dataPath = os . path . abspath ( os . path . join ( rootDir , 'datasets' , 'scalar_1.csv' ) ) 
streamDef = dict ( 
version = 1 , 
info = "testSpatialClassification" , 
streams = [ 
dict ( source = "file://%s" % ( dataPath ) , 
info = "scalar_1.csv" , 
columns = [ "*" ] , 
expDesc = { 
"environment" : 'nupic' , 
"inferenceArgs" : { 
"predictedField" : "classification" , 
"predictionSteps" : [ 0 ] , 
"inferenceType" : "MultiStep" , 
"streamDef" : streamDef , 
"includedFields" : [ 
{ "fieldName" : "field1" , 
"fieldType" : "float" , 
{ "fieldName" : "classification" , 
"fieldType" : "string" , 
{ "fieldName" : "randomData" , 
"iterationCount" : - 1 , 
return expDesc 
~~ def encodeIntoArray ( self , value , output ) : 
denseInput = numpy . zeros ( output . shape ) 
~~~ denseInput [ value ] = 1 
~~~ if isinstance ( value , numpy . ndarray ) : 
value . dtype ) ) 
~~ super ( SparsePassThroughEncoder , self ) . encodeIntoArray ( denseInput , output ) 
~~ def readFromFile ( cls , f , packed = True ) : 
schema = cls . getSchema ( ) 
if packed : 
~~~ proto = schema . read_packed ( f ) 
~~~ proto = schema . read ( f ) 
~~ return cls . read ( proto ) 
~~ def writeToFile ( self , f , packed = True ) : 
schema = self . getSchema ( ) 
proto = schema . new_message ( ) 
self . write ( proto ) 
~~~ proto . write_packed ( f ) 
~~~ proto . write ( f ) 
~~ ~~ def read ( cls , proto ) : 
instance = object . __new__ ( cls ) 
super ( TwoGramModel , instance ) . __init__ ( proto = proto . modelBase ) 
instance . _logger = opf_utils . initLogger ( instance ) 
instance . _reset = proto . reset 
instance . _hashToValueDict = { x . hash : x . value 
for x in proto . hashToValueDict } 
instance . _learningEnabled = proto . learningEnabled 
instance . _encoder = encoders . MultiEncoder . read ( proto . encoder ) 
instance . _fieldNames = instance . _encoder . getScalarNames ( ) 
instance . _prevValues = list ( proto . prevValues ) 
instance . _twoGramDicts = [ dict ( ) for _ in xrange ( len ( proto . twoGramDicts ) ) ] 
for idx , field in enumerate ( proto . twoGramDicts ) : 
~~~ for entry in field : 
~~~ prev = None if entry . value == - 1 else entry . value 
instance . _twoGramDicts [ idx ] [ prev ] = collections . defaultdict ( int ) 
for bucket in entry . buckets : 
~~~ instance . _twoGramDicts [ idx ] [ prev ] [ bucket . index ] = bucket . count 
~~ ~~ ~~ return instance 
super ( TwoGramModel , self ) . writeBaseToProto ( proto . modelBase ) 
proto . reset = self . _reset 
proto . learningEnabled = self . _learningEnabled 
proto . prevValues = self . _prevValues 
self . _encoder . write ( proto . encoder ) 
proto . hashToValueDict = [ { "hash" : h , "value" : v } 
for h , v in self . _hashToValueDict . items ( ) ] 
twoGramDicts = [ ] 
for items in self . _twoGramDicts : 
~~~ twoGramArr = [ ] 
for prev , values in items . iteritems ( ) : 
~~~ buckets = [ { "index" : index , "count" : count } 
for index , count in values . iteritems ( ) ] 
if prev is None : 
~~~ prev = - 1 
~~ twoGramArr . append ( { "value" : prev , "buckets" : buckets } ) 
~~ twoGramDicts . append ( twoGramArr ) 
~~ proto . twoGramDicts = twoGramDicts 
~~ def requireAnomalyModel ( func ) : 
def _decorator ( self , * args , ** kwargs ) : 
~~~ if not self . getInferenceType ( ) == InferenceType . TemporalAnomaly : 
~~ if self . _getAnomalyClassifier ( ) is None : 
~~ return func ( self , * args , ** kwargs ) 
~~ return _decorator 
~~ def anomalyRemoveLabels ( self , start , end , labelFilter ) : 
self . _getAnomalyClassifier ( ) . getSelf ( ) . removeLabels ( start , end , labelFilter ) 
~~ def anomalyAddLabel ( self , start , end , labelName ) : 
self . _getAnomalyClassifier ( ) . getSelf ( ) . addLabel ( start , end , labelName ) 
~~ def anomalyGetLabels ( self , start , end ) : 
return self . _getAnomalyClassifier ( ) . getSelf ( ) . getLabels ( start , end ) 
~~ def _getSensorInputRecord ( self , inputRecord ) : 
sensor = self . _getSensorRegion ( ) 
dataRow = copy . deepcopy ( sensor . getSelf ( ) . getOutputValues ( 'sourceOut' ) ) 
dataDict = copy . deepcopy ( inputRecord ) 
inputRecordEncodings = sensor . getSelf ( ) . getOutputValues ( 'sourceEncodings' ) 
inputRecordCategory = int ( sensor . getOutputData ( 'categoryOut' ) [ 0 ] ) 
resetOut = sensor . getOutputData ( 'resetOut' ) [ 0 ] 
return SensorInput ( dataRow = dataRow , 
dataDict = dataDict , 
dataEncodings = inputRecordEncodings , 
sequenceReset = resetOut , 
category = inputRecordCategory ) 
~~ def _getClassifierInputRecord ( self , inputRecord ) : 
absoluteValue = None 
bucketIdx = None 
if self . _predictedFieldName is not None and self . _classifierInputEncoder is not None : 
~~~ absoluteValue = inputRecord [ self . _predictedFieldName ] 
bucketIdx = self . _classifierInputEncoder . getBucketIndices ( absoluteValue ) [ 0 ] 
~~ return ClassifierInput ( dataRow = absoluteValue , 
bucketIndex = bucketIdx ) 
~~ def _anomalyCompute ( self ) : 
inferenceType = self . getInferenceType ( ) 
inferences = { } 
sp = self . _getSPRegion ( ) 
score = None 
if inferenceType == InferenceType . NontemporalAnomaly : 
~~ elif inferenceType == InferenceType . TemporalAnomaly : 
~~~ tm = self . _getTPRegion ( ) 
if sp is not None : 
~~~ activeColumns = sp . getOutputData ( "bottomUpOut" ) . nonzero ( ) [ 0 ] 
~~~ sensor = self . _getSensorRegion ( ) 
activeColumns = sensor . getOutputData ( 'dataOut' ) . nonzero ( ) [ 0 ] 
~~ if not self . _predictedFieldName in self . _input : 
% self . _predictedFieldName 
~~ score = tm . getOutputData ( "anomalyScore" ) [ 0 ] 
~~~ self . _getAnomalyClassifier ( ) . setParameter ( 
"activeColumnCount" , len ( activeColumns ) ) 
self . _getAnomalyClassifier ( ) . prepareInputs ( ) 
self . _getAnomalyClassifier ( ) . compute ( ) 
labels = self . _getAnomalyClassifier ( ) . getSelf ( ) . getLabelResults ( ) 
inferences [ InferenceElement . anomalyLabel ] = "%s" % labels 
~~ ~~ inferences [ InferenceElement . anomalyScore ] = score 
return inferences 
~~ def _handleSDRClassifierMultiStep ( self , patternNZ , 
inputTSRecordIdx , 
rawInput ) : 
inferenceArgs = self . getInferenceArgs ( ) 
predictedFieldName = inferenceArgs . get ( 'predictedField' , None ) 
if predictedFieldName is None : 
~~ self . _predictedFieldName = predictedFieldName 
classifier = self . _getClassifierRegion ( ) 
if not self . _hasCL or classifier is None : 
~~ sensor = self . _getSensorRegion ( ) 
minLikelihoodThreshold = self . _minLikelihoodThreshold 
maxPredictionsPerStep = self . _maxPredictionsPerStep 
needLearning = self . isLearningEnabled ( ) 
if self . _classifierInputEncoder is None : 
~~~ if predictedFieldName is None : 
~~ encoderList = sensor . getSelf ( ) . encoder . getEncoderList ( ) 
self . _numFields = len ( encoderList ) 
fieldNames = sensor . getSelf ( ) . encoder . getScalarNames ( ) 
if predictedFieldName in fieldNames : 
~~~ self . _predictedFieldIdx = fieldNames . index ( predictedFieldName ) 
~~~ self . _predictedFieldIdx = None 
~~ if sensor . getSelf ( ) . disabledEncoder is not None : 
~~~ encoderList = sensor . getSelf ( ) . disabledEncoder . getEncoderList ( ) 
~~~ encoderList = [ ] 
~~ if len ( encoderList ) >= 1 : 
~~~ fieldNames = sensor . getSelf ( ) . disabledEncoder . getScalarNames ( ) 
self . _classifierInputEncoder = encoderList [ fieldNames . index ( 
predictedFieldName ) ] 
~~~ encoderList = sensor . getSelf ( ) . encoder . getEncoderList ( ) 
self . _classifierInputEncoder = encoderList [ self . _predictedFieldIdx ] 
~~ ~~ if not predictedFieldName in rawInput : 
% predictedFieldName ) 
~~ absoluteValue = rawInput [ predictedFieldName ] 
if isinstance ( self . _classifierInputEncoder , DeltaEncoder ) : 
~~~ if not hasattr ( self , "_ms_prevVal" ) : 
~~~ self . _ms_prevVal = absoluteValue 
~~ prevValue = self . _ms_prevVal 
self . _ms_prevVal = absoluteValue 
actualValue = absoluteValue - prevValue 
~~~ actualValue = absoluteValue 
~~ if isinstance ( actualValue , float ) and math . isnan ( actualValue ) : 
~~~ actualValue = SENTINEL_VALUE_FOR_MISSING_DATA 
classifier . setParameter ( 'learningMode' , needLearning ) 
classificationIn = { 'bucketIdx' : bucketIdx , 
'actValue' : actualValue } 
if inputTSRecordIdx is not None : 
~~~ recordNum = inputTSRecordIdx 
~~~ recordNum = self . __numRunCalls 
~~ clResults = classifier . getSelf ( ) . customCompute ( recordNum = recordNum , 
classification = classificationIn ) 
predictionSteps = classifier . getParameter ( 'steps' ) 
predictionSteps = [ int ( x ) for x in predictionSteps . split ( ',' ) ] 
inferences [ InferenceElement . multiStepPredictions ] = dict ( ) 
inferences [ InferenceElement . multiStepBestPredictions ] = dict ( ) 
inferences [ InferenceElement . multiStepBucketLikelihoods ] = dict ( ) 
for steps in predictionSteps : 
~~~ likelihoodsVec = clResults [ steps ] 
bucketValues = clResults [ 'actualValues' ] 
likelihoodsDict = dict ( ) 
bestActValue = None 
bestProb = None 
for ( actValue , prob ) in zip ( bucketValues , likelihoodsVec ) : 
~~~ if actValue in likelihoodsDict : 
~~~ likelihoodsDict [ actValue ] += prob 
~~~ likelihoodsDict [ actValue ] = prob 
~~ if bestProb is None or likelihoodsDict [ actValue ] > bestProb : 
~~~ bestProb = likelihoodsDict [ actValue ] 
bestActValue = actValue 
~~ ~~ likelihoodsDict = HTMPredictionModel . _removeUnlikelyPredictions ( 
likelihoodsDict , minLikelihoodThreshold , maxPredictionsPerStep ) 
bucketLikelihood = { } 
for k in likelihoodsDict . keys ( ) : 
~~~ bucketLikelihood [ self . _classifierInputEncoder . getBucketIndices ( k ) [ 0 ] ] = ( 
likelihoodsDict [ k ] ) 
~~ if isinstance ( self . _classifierInputEncoder , DeltaEncoder ) : 
~~~ if not hasattr ( self , '_ms_predHistories' ) : 
~~~ self . _ms_predHistories = dict ( ) 
~~ predHistories = self . _ms_predHistories 
if not steps in predHistories : 
~~~ predHistories [ steps ] = deque ( ) 
~~ predHistory = predHistories [ steps ] 
sumDelta = sum ( predHistory ) 
offsetDict = dict ( ) 
for ( k , v ) in likelihoodsDict . iteritems ( ) : 
~~~ if k is not None : 
~~~ offsetDict [ absoluteValue + float ( k ) + sumDelta ] = v 
~~ ~~ bucketLikelihoodOffset = { } 
for k in offsetDict . keys ( ) : 
~~~ bucketLikelihoodOffset [ self . _classifierInputEncoder . getBucketIndices ( k ) [ 0 ] ] = ( 
offsetDict [ k ] ) 
~~ if bestActValue is not None : 
~~~ predHistory . append ( bestActValue ) 
~~ if len ( predHistory ) >= steps : 
~~~ predHistory . popleft ( ) 
~~ if len ( offsetDict ) > 0 : 
~~~ inferences [ InferenceElement . multiStepPredictions ] [ steps ] = offsetDict 
inferences [ InferenceElement . multiStepBucketLikelihoods ] [ steps ] = bucketLikelihoodOffset 
~~~ inferences [ InferenceElement . multiStepPredictions ] [ steps ] = likelihoodsDict 
inferences [ InferenceElement . multiStepBucketLikelihoods ] [ steps ] = bucketLikelihood 
~~ if bestActValue is None : 
~~~ inferences [ InferenceElement . multiStepBestPredictions ] [ steps ] = None 
~~~ inferences [ InferenceElement . multiStepBestPredictions ] [ steps ] = ( 
absoluteValue + sumDelta + bestActValue ) 
~~~ inferences [ InferenceElement . multiStepPredictions ] [ steps ] = ( 
likelihoodsDict ) 
inferences [ InferenceElement . multiStepBestPredictions ] [ steps ] = ( 
bestActValue ) 
inferences [ InferenceElement . multiStepBucketLikelihoods ] [ steps ] = ( 
bucketLikelihood ) 
~~ ~~ return inferences 
~~ def _removeUnlikelyPredictions ( cls , likelihoodsDict , minLikelihoodThreshold , 
maxPredictionsPerStep ) : 
maxVal = ( None , None ) 
for ( k , v ) in likelihoodsDict . items ( ) : 
~~~ if len ( likelihoodsDict ) <= 1 : 
~~ if maxVal [ 0 ] is None or v >= maxVal [ 1 ] : 
~~~ if maxVal [ 0 ] is not None and maxVal [ 1 ] < minLikelihoodThreshold : 
~~~ del likelihoodsDict [ maxVal [ 0 ] ] 
~~ maxVal = ( k , v ) 
~~ elif v < minLikelihoodThreshold : 
~~~ del likelihoodsDict [ k ] 
~~ ~~ likelihoodsDict = dict ( sorted ( likelihoodsDict . iteritems ( ) , 
key = itemgetter ( 1 ) , 
reverse = True ) [ : maxPredictionsPerStep ] ) 
return likelihoodsDict 
~~ def getRuntimeStats ( self ) : 
ret = { "numRunCalls" : self . __numRunCalls } 
#-------------------------------------------------- 
temporalStats = dict ( ) 
if self . _hasTP : 
~~~ for stat in self . _netInfo . statsCollectors : 
~~~ sdict = stat . getStats ( ) 
temporalStats . update ( sdict ) 
~~ ~~ ret [ InferenceType . getLabel ( InferenceType . TemporalNextStep ) ] = temporalStats 
return ret 
~~ def _getClassifierRegion ( self ) : 
if ( self . _netInfo . net is not None and 
"Classifier" in self . _netInfo . net . regions ) : 
~~~ return self . _netInfo . net . regions [ "Classifier" ] 
~~ ~~ def __createHTMNetwork ( self , sensorParams , spEnable , spParams , tmEnable , 
tmParams , clEnable , clParams , anomalyParams ) : 
n = Network ( ) 
n . addRegion ( "sensor" , "py.RecordSensor" , json . dumps ( dict ( verbosity = sensorParams [ 'verbosity' ] ) ) ) 
sensor = n . regions [ 'sensor' ] . getSelf ( ) 
enabledEncoders = copy . deepcopy ( sensorParams [ 'encoders' ] ) 
for name , params in enabledEncoders . items ( ) : 
~~~ if params is not None : 
~~~ classifierOnly = params . pop ( 'classifierOnly' , False ) 
if classifierOnly : 
~~~ enabledEncoders . pop ( name ) 
~~ ~~ ~~ disabledEncoders = copy . deepcopy ( sensorParams [ 'encoders' ] ) 
for name , params in disabledEncoders . items ( ) : 
~~~ if params is None : 
~~~ disabledEncoders . pop ( name ) 
if not classifierOnly : 
~~ ~~ ~~ encoder = MultiEncoder ( enabledEncoders ) 
sensor . encoder = encoder 
sensor . disabledEncoder = MultiEncoder ( disabledEncoders ) 
sensor . dataSource = DataBuffer ( ) 
prevRegion = "sensor" 
prevRegionWidth = encoder . getWidth ( ) 
if spEnable : 
~~~ spParams = spParams . copy ( ) 
spParams [ 'inputWidth' ] = prevRegionWidth 
n . addRegion ( "SP" , "py.SPRegion" , json . dumps ( spParams ) ) 
n . link ( "sensor" , "SP" , "UniformLink" , "" ) 
n . link ( "sensor" , "SP" , "UniformLink" , "" , srcOutput = "resetOut" , 
destInput = "resetIn" ) 
n . link ( "SP" , "sensor" , "UniformLink" , "" , srcOutput = "spatialTopDownOut" , 
destInput = "spatialTopDownIn" ) 
n . link ( "SP" , "sensor" , "UniformLink" , "" , srcOutput = "temporalTopDownOut" , 
destInput = "temporalTopDownIn" ) 
prevRegion = "SP" 
prevRegionWidth = spParams [ 'columnCount' ] 
~~ if tmEnable : 
~~~ tmParams = tmParams . copy ( ) 
if prevRegion == 'sensor' : 
~~~ tmParams [ 'inputWidth' ] = tmParams [ 'columnCount' ] = prevRegionWidth 
~~~ assert tmParams [ 'columnCount' ] == prevRegionWidth 
tmParams [ 'inputWidth' ] = tmParams [ 'columnCount' ] 
n . addRegion ( "TM" , "py.TMRegion" , json . dumps ( tmParams ) ) 
n . link ( prevRegion , "TM" , "UniformLink" , "" ) 
if prevRegion != "sensor" : 
~~~ n . link ( "TM" , prevRegion , "UniformLink" , "" , srcOutput = "topDownOut" , 
destInput = "topDownIn" ) 
~~ n . link ( "sensor" , "TM" , "UniformLink" , "" , srcOutput = "resetOut" , 
prevRegion = "TM" 
prevRegionWidth = tmParams [ 'inputWidth' ] 
~~ if clEnable and clParams is not None : 
~~~ clParams = clParams . copy ( ) 
clRegionName = clParams . pop ( 'regionName' ) 
clParams ) ) 
n . addRegion ( "Classifier" , "py.%s" % str ( clRegionName ) , json . dumps ( clParams ) ) 
if str ( clRegionName ) == "SDRClassifierRegion" : 
~~~ n . link ( "sensor" , "Classifier" , "UniformLink" , "" , srcOutput = "actValueOut" , 
destInput = "actValueIn" ) 
n . link ( "sensor" , "Classifier" , "UniformLink" , "" , srcOutput = "bucketIdxOut" , 
destInput = "bucketIdxIn" ) 
~~ n . link ( "sensor" , "Classifier" , "UniformLink" , "" , srcOutput = "categoryOut" , 
destInput = "categoryIn" ) 
n . link ( prevRegion , "Classifier" , "UniformLink" , "" ) 
~~ if self . getInferenceType ( ) == InferenceType . TemporalAnomaly : 
~~~ anomalyClParams = dict ( 
trainRecords = anomalyParams . get ( 'autoDetectWaitRecords' , None ) , 
cacheSize = anomalyParams . get ( 'anomalyCacheRecords' , None ) 
self . _addAnomalyClassifierRegion ( n , anomalyClParams , spEnable , tmEnable ) 
~~ n . initialize ( ) 
return NetworkInfo ( net = n , statsCollectors = [ ] ) 
super ( HTMPredictionModel , self ) . writeBaseToProto ( proto . modelBase ) 
proto . numRunCalls = self . __numRunCalls 
proto . minLikelihoodThreshold = self . _minLikelihoodThreshold 
proto . maxPredictionsPerStep = self . _maxPredictionsPerStep 
self . _netInfo . net . write ( proto . network ) 
proto . spLearningEnabled = self . __spLearningEnabled 
proto . tpLearningEnabled = self . __tpLearningEnabled 
if self . _predictedFieldIdx is None : 
~~~ proto . predictedFieldIdx . none = None 
~~~ proto . predictedFieldIdx . value = self . _predictedFieldIdx 
~~ if self . _predictedFieldName is None : 
~~~ proto . predictedFieldName . none = None 
~~~ proto . predictedFieldName . value = self . _predictedFieldName 
~~ if self . _numFields is None : 
~~~ proto . numFields . none = None 
~~~ proto . numFields . value = self . _numFields 
~~ proto . trainSPNetOnlyIfRequested = self . __trainSPNetOnlyIfRequested 
proto . finishedLearning = self . __finishedLearning 
obj = object . __new__ ( cls ) 
super ( HTMPredictionModel , obj ) . __init__ ( proto = proto . modelBase ) 
obj . _minLikelihoodThreshold = round ( proto . minLikelihoodThreshold , 
EPSILON_ROUND ) 
obj . _maxPredictionsPerStep = proto . maxPredictionsPerStep 
network = Network . read ( proto . network ) 
obj . _hasSP = ( "SP" in network . regions ) 
obj . _hasTP = ( "TM" in network . regions ) 
obj . _hasCL = ( "Classifier" in network . regions ) 
obj . _netInfo = NetworkInfo ( net = network , statsCollectors = [ ] ) 
obj . __spLearningEnabled = bool ( proto . spLearningEnabled ) 
obj . __tpLearningEnabled = bool ( proto . tpLearningEnabled ) 
obj . __numRunCalls = proto . numRunCalls 
obj . _classifierInputEncoder = None 
if proto . predictedFieldIdx . which ( ) == "none" : 
~~~ obj . _predictedFieldIdx = None 
~~~ obj . _predictedFieldIdx = proto . predictedFieldIdx . value 
~~ if proto . predictedFieldName . which ( ) == "none" : 
~~~ obj . _predictedFieldName = None 
~~~ obj . _predictedFieldName = proto . predictedFieldName . value 
~~ obj . _numFields = proto . numFields 
if proto . numFields . which ( ) == "none" : 
~~~ obj . _numFields = None 
~~~ obj . _numFields = proto . numFields . value 
~~ obj . __trainSPNetOnlyIfRequested = proto . trainSPNetOnlyIfRequested 
obj . __finishedLearning = proto . finishedLearning 
obj . _input = None 
sensor = network . regions [ 'sensor' ] . getSelf ( ) 
obj . __logger = initLogger ( obj ) 
obj . __restoringFromState = False 
obj . __restoringFromV1 = False 
return obj 
~~ def _serializeExtraData ( self , extraDataDir ) : 
makeDirectoryFromAbsolutePath ( extraDataDir ) 
outputDir = self . __getNetworkStateDirectory ( extraDataDir = extraDataDir ) 
self . _netInfo . net . save ( outputDir ) 
~~ def _deSerializeExtraData ( self , extraDataDir ) : 
assert self . __restoringFromState 
stateDir = self . __getNetworkStateDirectory ( extraDataDir = extraDataDir ) 
self . __logger . debug ( 
self . _netInfo . net = Network ( stateDir ) 
self . _netInfo . net . initialize ( ) 
if self . getInferenceType ( ) == InferenceType . TemporalAnomaly : 
~~~ classifierType = self . _getAnomalyClassifier ( ) . getSelf ( ) . __class__ . __name__ 
if classifierType is 'KNNClassifierRegion' : 
trainRecords = self . _classifier_helper . _autoDetectWaitRecords , 
cacheSize = self . _classifier_helper . _history_length , 
spEnable = ( self . _getSPRegion ( ) is not None ) 
tmEnable = True 
knnRegion = self . _getAnomalyClassifier ( ) . getSelf ( ) 
self . _addAnomalyClassifierRegion ( self . _netInfo . net , anomalyClParams , 
spEnable , tmEnable ) 
self . _getAnomalyClassifier ( ) . getSelf ( ) . _iteration = self . __numRunCalls 
self . _getAnomalyClassifier ( ) . getSelf ( ) . _recordsCache = ( 
self . _classifier_helper . saved_states ) 
self . _getAnomalyClassifier ( ) . getSelf ( ) . saved_categories = ( 
self . _classifier_helper . saved_categories ) 
self . _getAnomalyClassifier ( ) . getSelf ( ) . _knnclassifier = knnRegion 
self . _getTPRegion ( ) . setParameter ( 'anomalyMode' , True ) 
del self . _classifier_helper 
~~ ~~ self . __restoringFromState = False 
~~ def _addAnomalyClassifierRegion ( self , network , params , spEnable , tmEnable ) : 
allParams = copy . deepcopy ( params ) 
knnParams = dict ( k = 1 , 
distanceMethod = 'rawOverlap' , 
distanceNorm = 1 , 
doBinarization = 1 , 
replaceDuplicates = 0 , 
maxStoredPatterns = 1000 ) 
allParams . update ( knnParams ) 
if allParams [ 'trainRecords' ] is None : 
~~~ allParams [ 'trainRecords' ] = DEFAULT_ANOMALY_TRAINRECORDS 
~~ if allParams [ 'cacheSize' ] is None : 
~~~ allParams [ 'cacheSize' ] = DEFAULT_ANOMALY_CACHESIZE 
~~ if self . _netInfo is not None and self . _netInfo . net is not None and self . _getAnomalyClassifier ( ) is not None : 
~~~ self . _netInfo . net . removeRegion ( 'AnomalyClassifier' ) 
~~ network . addRegion ( "AnomalyClassifier" , 
"py.KNNAnomalyClassifierRegion" , 
json . dumps ( allParams ) ) 
~~~ network . link ( "SP" , "AnomalyClassifier" , "UniformLink" , "" , 
srcOutput = "bottomUpOut" , destInput = "spBottomUpOut" ) 
~~~ network . link ( "sensor" , "AnomalyClassifier" , "UniformLink" , "" , 
srcOutput = "dataOut" , destInput = "spBottomUpOut" ) 
~~~ network . link ( "TM" , "AnomalyClassifier" , "UniformLink" , "" , 
srcOutput = "topDownOut" , destInput = "tpTopDownOut" ) 
network . link ( "TM" , "AnomalyClassifier" , "UniformLink" , "" , 
srcOutput = "lrnActiveStateT" , destInput = "tpLrnActiveStateT" ) 
~~ ~~ def __getNetworkStateDirectory ( self , extraDataDir ) : 
if self . __restoringFromV1 : 
~~~ if self . getInferenceType ( ) == InferenceType . TemporalNextStep : 
~~~ leafName = 'temporal' + "-network.nta" 
~~~ leafName = 'nonTemporal' + "-network.nta" 
~~~ leafName = InferenceType . getLabel ( self . getInferenceType ( ) ) + "-network.nta" 
~~ path = os . path . join ( extraDataDir , leafName ) 
path = os . path . abspath ( path ) 
return path 
~~ def __manglePrivateMemberName ( self , privateMemberName , skipCheck = False ) : 
realName = "_" + ( self . __myClassName ) . lstrip ( "_" ) + privateMemberName 
if not skipCheck : 
~~~ getattr ( self , realName ) 
~~ return realName 
~~ def _setEncoderParams ( self ) : 
self . rangeInternal = float ( self . maxval - self . minval ) 
self . resolution = float ( self . rangeInternal ) / ( self . n - self . w ) 
self . radius = self . w * self . resolution 
self . range = self . rangeInternal + self . resolution 
self . nInternal = self . n - 2 * self . padding 
self . _bucketValues = None 
~~ def setFieldStats ( self , fieldName , fieldStats ) : 
if fieldStats [ fieldName ] [ 'min' ] == None or fieldStats [ fieldName ] [ 'max' ] == None : 
~~ self . minval = fieldStats [ fieldName ] [ 'min' ] 
self . maxval = fieldStats [ fieldName ] [ 'max' ] 
if self . minval == self . maxval : 
~~~ self . maxval += 1 
~~ self . _setEncoderParams ( ) 
~~ def _setMinAndMax ( self , input , learn ) : 
self . slidingWindow . next ( input ) 
if self . minval is None and self . maxval is None : 
~~~ self . minval = input 
self . _setEncoderParams ( ) 
~~ elif learn : 
~~~ sorted = self . slidingWindow . getSlidingWindow ( ) 
sorted . sort ( ) 
minOverWindow = sorted [ 0 ] 
maxOverWindow = sorted [ len ( sorted ) - 1 ] 
if minOverWindow < self . minval : 
~~~ if self . verbosity >= 2 : 
~~ self . minval = minOverWindow #-initialBump 
~~ if maxOverWindow > self . maxval : 
~~ self . maxval = maxOverWindow #+initialBump 
~~ ~~ ~~ def getBucketIndices ( self , input , learn = None ) : 
self . recordNum += 1 
if learn is None : 
~~~ learn = self . _learningEnabled 
~~ if type ( input ) is float and math . isnan ( input ) : 
~~~ input = SENTINEL_VALUE_FOR_MISSING_DATA 
~~ if input == SENTINEL_VALUE_FOR_MISSING_DATA : 
~~~ self . _setMinAndMax ( input , learn ) 
return super ( AdaptiveScalarEncoder , self ) . getBucketIndices ( input ) 
~~ ~~ def encodeIntoArray ( self , input , output , learn = None ) : 
~~~ output [ 0 : self . n ] = 0 
~~ elif not math . isnan ( input ) : 
~~ super ( AdaptiveScalarEncoder , self ) . encodeIntoArray ( input , output ) 
if self . minval is None or self . maxval is None : 
~~~ return [ EncoderResult ( value = 0 , scalar = 0 , 
encoding = numpy . zeros ( self . n ) ) ] 
~~ return super ( AdaptiveScalarEncoder , self ) . getBucketInfo ( buckets ) 
~~ return super ( AdaptiveScalarEncoder , self ) . topDownCompute ( encoded ) 
~~ def recordDataPoint ( self , swarmId , generation , errScore ) : 
terminatedSwarms = [ ] 
if swarmId in self . swarmScores : 
~~~ entry = self . swarmScores [ swarmId ] 
assert ( len ( entry ) == generation ) 
entry . append ( errScore ) 
entry = self . swarmBests [ swarmId ] 
entry . append ( min ( errScore , entry [ - 1 ] ) ) 
assert ( len ( self . swarmBests [ swarmId ] ) == len ( self . swarmScores [ swarmId ] ) ) 
~~~ assert ( generation == 0 ) 
self . swarmScores [ swarmId ] = [ errScore ] 
self . swarmBests [ swarmId ] = [ errScore ] 
~~ if generation + 1 < self . MATURITY_WINDOW : 
~~~ return terminatedSwarms 
~~ if self . MAX_GENERATIONS is not None and generation > self . MAX_GENERATIONS : 
~~~ self . _logger . info ( 
( swarmId , self . MAX_GENERATIONS ) ) 
terminatedSwarms . append ( swarmId ) 
~~ if self . _isTerminationEnabled : 
~~~ terminatedSwarms . extend ( self . _getTerminatedSwarms ( generation ) ) 
~~ cumulativeBestScores = self . swarmBests [ swarmId ] 
if cumulativeBestScores [ - 1 ] == cumulativeBestScores [ - self . MATURITY_WINDOW ] : 
'Stopping...' % ( swarmId , self . MATURITY_WINDOW ) ) 
~~ self . terminatedSwarms = self . terminatedSwarms . union ( terminatedSwarms ) 
return terminatedSwarms 
~~ def getState ( self ) : 
return dict ( _position = self . _position , 
position = self . getPosition ( ) , 
velocity = self . _velocity , 
bestPosition = self . _bestPosition , 
bestResult = self . _bestResult ) 
~~ def setState ( self , state ) : 
self . _position = state [ '_position' ] 
self . _velocity = state [ 'velocity' ] 
self . _bestPosition = state [ 'bestPosition' ] 
self . _bestResult = state [ 'bestResult' ] 
~~ def getPosition ( self ) : 
if self . stepSize is None : 
~~~ return self . _position 
~~ numSteps = ( self . _position - self . min ) / self . stepSize 
numSteps = int ( round ( numSteps ) ) 
position = self . min + ( numSteps * self . stepSize ) 
position = max ( self . min , position ) 
position = min ( self . max , position ) 
return position 
~~ def agitate ( self ) : 
self . _velocity *= 1.5 / self . _inertia 
maxV = ( self . max - self . min ) / 2 
if self . _velocity > maxV : 
~~~ self . _velocity = maxV 
~~ elif self . _velocity < - maxV : 
~~~ self . _velocity = - maxV 
~~ if self . _position == self . max and self . _velocity > 0 : 
~~~ self . _velocity *= - 1 
~~ if self . _position == self . min and self . _velocity < 0 : 
~~ ~~ def newPosition ( self , globalBestPosition , rng ) : 
lb = float ( Configuration . get ( "nupic.hypersearch.randomLowerBound" ) ) 
ub = float ( Configuration . get ( "nupic.hypersearch.randomUpperBound" ) ) 
self . _velocity = ( self . _velocity * self . _inertia + rng . uniform ( lb , ub ) * 
self . _cogRate * ( self . _bestPosition - self . getPosition ( ) ) ) 
if globalBestPosition is not None : 
~~~ self . _velocity += rng . uniform ( lb , ub ) * self . _socRate * ( 
globalBestPosition - self . getPosition ( ) ) 
~~ self . _position += self . _velocity 
self . _position = max ( self . min , self . _position ) 
self . _position = min ( self . max , self . _position ) 
return self . getPosition ( ) 
~~ def pushAwayFrom ( self , otherPositions , rng ) : 
if self . max == self . min : 
~~ numPositions = len ( otherPositions ) * 4 
if numPositions == 0 : 
~~ stepSize = float ( self . max - self . min ) / numPositions 
positions = numpy . arange ( self . min , self . max + stepSize , stepSize ) 
numPositions = len ( positions ) 
weights = numpy . zeros ( numPositions ) 
maxDistanceSq = - 1 * ( stepSize ** 2 ) 
for pos in otherPositions : 
~~~ distances = pos - positions 
varWeights = numpy . exp ( numpy . power ( distances , 2 ) / maxDistanceSq ) 
weights += varWeights 
~~ positionIdx = weights . argmin ( ) 
self . _position = positions [ positionIdx ] 
self . _bestPosition = self . getPosition ( ) 
self . _velocity *= rng . choice ( [ 1 , - 1 ] ) 
~~ def resetVelocity ( self , rng ) : 
maxVelocity = ( self . max - self . min ) / 5.0 
position = super ( PermuteInt , self ) . getPosition ( ) 
position = int ( round ( position ) ) 
return dict ( _position = self . getPosition ( ) , 
velocity = None , 
bestPosition = self . choices [ self . _bestPositionIdx ] , 
self . _positionIdx = self . choices . index ( state [ '_position' ] ) 
self . _bestPositionIdx = self . choices . index ( state [ 'bestPosition' ] ) 
~~ def setResultsPerChoice ( self , resultsPerChoice ) : 
self . _resultsPerChoice = [ [ ] ] * len ( self . choices ) 
for ( choiceValue , values ) in resultsPerChoice : 
~~~ choiceIndex = self . choices . index ( choiceValue ) 
self . _resultsPerChoice [ choiceIndex ] = list ( values ) 
numChoices = len ( self . choices ) 
meanScorePerChoice = [ ] 
overallSum = 0 
numResults = 0 
for i in range ( numChoices ) : 
~~~ if len ( self . _resultsPerChoice [ i ] ) > 0 : 
~~~ data = numpy . array ( self . _resultsPerChoice [ i ] ) 
meanScorePerChoice . append ( data . mean ( ) ) 
overallSum += data . sum ( ) 
numResults += data . size 
~~~ meanScorePerChoice . append ( None ) 
~~ ~~ if numResults == 0 : 
~~~ overallSum = 1.0 
numResults = 1 
~~ for i in range ( numChoices ) : 
~~~ if meanScorePerChoice [ i ] is None : 
~~~ meanScorePerChoice [ i ] = overallSum / numResults 
~~ ~~ meanScorePerChoice = numpy . array ( meanScorePerChoice ) 
meanScorePerChoice = ( 1.1 * meanScorePerChoice . max ( ) ) - meanScorePerChoice 
if self . _fixEarly : 
~~~ meanScorePerChoice **= ( numResults * self . _fixEarlyFactor / numChoices ) 
~~ total = meanScorePerChoice . sum ( ) 
if total == 0 : 
~~~ total = 1.0 
~~ meanScorePerChoice /= total 
distribution = meanScorePerChoice . cumsum ( ) 
r = rng . random ( ) * distribution [ - 1 ] 
choiceIdx = numpy . where ( r <= distribution ) [ 0 ] [ 0 ] 
self . _positionIdx = choiceIdx 
positions = [ self . choices . index ( x ) for x in otherPositions ] 
positionCounts = [ 0 ] * len ( self . choices ) 
for pos in positions : 
~~~ positionCounts [ pos ] += 1 
~~ self . _positionIdx = numpy . array ( positionCounts ) . argmin ( ) 
self . _bestPositionIdx = self . _positionIdx 
~~ def getDict ( self , encoderName , flattenedChosenValues ) : 
encoder = dict ( fieldname = self . fieldName , 
name = self . name ) 
for encoderArg , value in self . kwArgs . iteritems ( ) : 
~~~ value = flattenedChosenValues [ "%s:%s" % ( encoderName , encoderArg ) ] 
~~ encoder [ encoderArg ] = value 
~~ if '.' in self . encoderClass : 
~~~ ( encoder [ 'type' ] , argName ) = self . encoderClass . split ( '.' ) 
argValue = ( encoder [ 'w' ] , encoder [ 'radius' ] ) 
encoder [ argName ] = argValue 
encoder . pop ( 'w' ) 
encoder . pop ( 'radius' ) 
~~~ encoder [ 'type' ] = self . encoderClass 
~~ return encoder 
~~ def _translateMetricsToJSON ( self , metrics , label ) : 
metricsDict = metrics 
def _mapNumpyValues ( obj ) : 
import numpy 
if isinstance ( obj , numpy . float32 ) : 
~~~ return float ( obj ) 
~~ elif isinstance ( obj , numpy . bool_ ) : 
~~~ return bool ( obj ) 
~~ elif isinstance ( obj , numpy . ndarray ) : 
~~~ return obj . tolist ( ) 
~~ ~~ jsonString = json . dumps ( metricsDict , indent = 4 , default = _mapNumpyValues ) 
return jsonString 
~~ def __openDatafile ( self , modelResult ) : 
resetFieldMeta = FieldMetaInfo ( 
name = "reset" , 
type = FieldMetaType . integer , 
special = FieldMetaSpecial . reset ) 
self . __outputFieldsMeta . append ( resetFieldMeta ) 
rawInput = modelResult . rawInput 
rawFields = rawInput . keys ( ) 
rawFields . sort ( ) 
for field in rawFields : 
~~~ if field . startswith ( '_' ) or field == 'reset' : 
~~ value = rawInput [ field ] 
meta = FieldMetaInfo ( name = field , type = FieldMetaType . string , 
special = FieldMetaSpecial . none ) 
self . __outputFieldsMeta . append ( meta ) 
self . _rawInputNames . append ( field ) 
~~ for inferenceElement , value in modelResult . inferences . iteritems ( ) : 
~~~ inferenceLabel = InferenceElement . getLabel ( inferenceElement ) 
if type ( value ) in ( list , tuple ) : 
~~~ self . __outputFieldsMeta . extend ( self . __getListMetaInfo ( inferenceElement ) ) 
~~ elif isinstance ( value , dict ) : 
~~~ self . __outputFieldsMeta . extend ( self . __getDictMetaInfo ( inferenceElement , 
value ) ) 
~~~ if InferenceElement . getInputElement ( inferenceElement ) : 
~~~ self . __outputFieldsMeta . append ( FieldMetaInfo ( name = inferenceLabel + ".actual" , 
type = FieldMetaType . string , special = '' ) ) 
~~ self . __outputFieldsMeta . append ( FieldMetaInfo ( name = inferenceLabel , 
~~ ~~ if self . __metricNames : 
~~~ for metricName in self . __metricNames : 
~~~ metricField = FieldMetaInfo ( 
name = metricName , 
type = FieldMetaType . float , 
self . __outputFieldsMeta . append ( metricField ) 
~~ ~~ inferenceDir = _FileUtils . createExperimentInferenceDir ( self . __experimentDir ) 
filename = ( self . __label + "." + 
opf_utils . InferenceType . getLabel ( self . __inferenceType ) + 
".predictionLog.csv" ) 
self . __datasetPath = os . path . join ( inferenceDir , filename ) 
self . __dataset = FileRecordStream ( streamID = self . __datasetPath , write = True , 
fields = self . __outputFieldsMeta ) 
if self . __checkpointCache is not None : 
~~~ self . __checkpointCache . seek ( 0 ) 
reader = csv . reader ( self . __checkpointCache , dialect = 'excel' ) 
~~~ header = reader . next ( ) 
tuple ( self . __dataset . getFieldNames ( ) ) , tuple ( header ) ) 
~~ numRowsCopied = 0 
~~~ row = reader . next ( ) 
~~ self . __dataset . appendRecord ( row ) 
numRowsCopied += 1 
~~ self . __dataset . flush ( ) 
numRowsCopied , self . __datasetPath ) 
self . __checkpointCache . close ( ) 
self . __checkpointCache = None 
~~ def setLoggedMetrics ( self , metricNames ) : 
if metricNames is None : 
~~~ self . __metricNames = set ( [ ] ) 
~~~ self . __metricNames = set ( metricNames ) 
~~ ~~ def __getListMetaInfo ( self , inferenceElement ) : 
fieldMetaInfo = [ ] 
inferenceLabel = InferenceElement . getLabel ( inferenceElement ) 
for inputFieldMeta in self . __inputFieldsMeta : 
~~~ outputFieldMeta = FieldMetaInfo ( 
name = inputFieldMeta . name + ".actual" , 
type = inputFieldMeta . type , 
special = inputFieldMeta . special 
~~ predictionField = FieldMetaInfo ( 
name = inputFieldMeta . name + "." + inferenceLabel , 
fieldMetaInfo . append ( outputFieldMeta ) 
fieldMetaInfo . append ( predictionField ) 
~~ return fieldMetaInfo 
~~ def __getDictMetaInfo ( self , inferenceElement , inferenceDict ) : 
if InferenceElement . getInputElement ( inferenceElement ) : 
~~~ fieldMetaInfo . append ( FieldMetaInfo ( name = inferenceLabel + ".actual" , 
type = FieldMetaType . string , 
special = '' ) ) 
~~ keys = sorted ( inferenceDict . keys ( ) ) 
for key in keys : 
~~~ fieldMetaInfo . append ( FieldMetaInfo ( name = inferenceLabel + "." + str ( key ) , 
~~ def append ( self , modelResult ) : 
inferences = modelResult . inferences 
hasInferences = False 
if inferences is not None : 
~~~ for value in inferences . itervalues ( ) : 
~~~ hasInferences = hasInferences or ( value is not None ) 
~~ ~~ if not hasInferences : 
~~ if self . __dataset is None : 
~~~ self . __openDatafile ( modelResult ) 
~~ inputData = modelResult . sensorInput 
sequenceReset = int ( bool ( inputData . sequenceReset ) ) 
outputRow = [ sequenceReset ] 
for field in self . _rawInputNames : 
~~~ outputRow . append ( str ( rawInput [ field ] ) ) 
~~ for inferenceElement , outputVal in inferences . iteritems ( ) : 
~~~ inputElement = InferenceElement . getInputElement ( inferenceElement ) 
if inputElement : 
~~~ inputVal = getattr ( inputData , inputElement ) 
~~~ inputVal = None 
~~ if type ( outputVal ) in ( list , tuple ) : 
~~~ assert type ( inputVal ) in ( list , tuple , None ) 
for iv , ov in zip ( inputVal , outputVal ) : 
~~~ outputRow . append ( str ( iv ) ) 
outputRow . append ( str ( ov ) ) 
~~ ~~ elif isinstance ( outputVal , dict ) : 
~~~ if inputVal is not None : 
~~~ if modelResult . predictedFieldName is not None : 
~~~ outputRow . append ( str ( inputVal [ modelResult . predictedFieldName ] ) ) 
~~~ outputRow . append ( str ( inputVal ) ) 
~~ ~~ for key in sorted ( outputVal . keys ( ) ) : 
~~~ outputRow . append ( str ( outputVal [ key ] ) ) 
~~ outputRow . append ( str ( outputVal ) ) 
~~ ~~ metrics = modelResult . metrics 
for metricName in self . __metricNames : 
~~~ outputRow . append ( metrics . get ( metricName , 0.0 ) ) 
~~ self . __dataset . appendRecord ( outputRow ) 
self . __dataset . flush ( ) 
~~ def checkpoint ( self , checkpointSink , maxRows ) : 
checkpointSink . truncate ( ) 
if self . __dataset is None : 
~~~ if self . __checkpointCache is not None : 
shutil . copyfileobj ( self . __checkpointCache , checkpointSink ) 
checkpointSink . flush ( ) 
~~ ~~ self . __dataset . flush ( ) 
totalDataRows = self . __dataset . getDataRowCount ( ) 
if totalDataRows == 0 : 
~~ reader = FileRecordStream ( self . __datasetPath , missingValues = [ ] ) 
writer = csv . writer ( checkpointSink ) 
writer . writerow ( reader . getFieldNames ( ) ) 
numToWrite = min ( maxRows , totalDataRows ) 
numRowsToSkip = totalDataRows - numToWrite 
for i in xrange ( numRowsToSkip ) : 
~~~ reader . next ( ) 
~~ numWritten = 0 
~~~ row = reader . getNextRecord ( ) 
if row is None : 
~~ row = [ str ( element ) for element in row ] 
numWritten += 1 
~~ def update ( self , modelResult ) : 
self . __writer . append ( self . __inferenceShifter . shift ( modelResult ) ) 
~~ def createExperimentInferenceDir ( cls , experimentDir ) : 
path = cls . getExperimentInferenceDirPath ( experimentDir ) 
cls . makeDirectory ( path ) 
~~ def _generateModel0 ( numCategories ) : 
initProb [ 0 ] = 0.5 
initProb [ 4 ] = 0.5 
for catIdx in range ( numCategories ) : 
~~~ key = str ( [ catIdx ] ) 
probs = numpy . ones ( numCategories ) / numCategories 
if catIdx == 0 or catIdx == 4 : 
~~~ probs . fill ( 0 ) 
~~ firstOrder [ key ] = probs 
~~ secondOrder = dict ( ) 
for firstIdx in range ( numCategories ) : 
~~~ for secondIdx in range ( numCategories ) : 
~~~ key = str ( [ firstIdx , secondIdx ] ) 
if key == str ( [ 0 , 1 ] ) : 
~~ elif key == str ( [ 4 , 1 ] ) : 
~~ secondOrder [ key ] = probs 
~~ ~~ return ( initProb , firstOrder , secondOrder , 3 ) 
~~ def _generateModel1 ( numCategories ) : 
initProb [ 1 ] = 0.5 
if catIdx == 0 or catIdx == 1 : 
~~~ indices = numpy . array ( [ 10 , 11 , 12 , 13 , 14 ] ) 
probs . fill ( 0 ) 
probs /= probs . sum ( ) 
if key == str ( [ 0 , 10 ] ) : 
probs [ 15 ] = 1 
~~ elif key == str ( [ 0 , 11 ] ) : 
probs [ 16 ] = 1 
~~ elif key == str ( [ 0 , 12 ] ) : 
probs [ 17 ] = 1 
~~ elif key == str ( [ 0 , 13 ] ) : 
probs [ 18 ] = 1 
~~ elif key == str ( [ 0 , 14 ] ) : 
probs [ 19 ] = 1 
~~ elif key == str ( [ 1 , 10 ] ) : 
probs [ 20 ] = 1 
~~ elif key == str ( [ 1 , 11 ] ) : 
probs [ 21 ] = 1 
~~ elif key == str ( [ 1 , 12 ] ) : 
probs [ 22 ] = 1 
~~ elif key == str ( [ 1 , 13 ] ) : 
probs [ 23 ] = 1 
~~ elif key == str ( [ 1 , 14 ] ) : 
probs [ 24 ] = 1 
~~ def _generateModel2 ( numCategories , alpha = 0.25 ) : 
initProb = numpy . ones ( numCategories ) / numCategories 
def generatePeakedProbabilities ( lastIdx , 
numCategories = numCategories , 
alpha = alpha ) : 
~~~ probs = numpy . random . dirichlet ( alpha = [ alpha ] * numCategories ) 
probs [ lastIdx ] = 0.0 
return probs 
~~ firstOrder = dict ( ) 
probs = generatePeakedProbabilities ( catIdx ) 
firstOrder [ key ] = probs 
probs = generatePeakedProbabilities ( secondIdx ) 
secondOrder [ key ] = probs 
~~ ~~ return ( initProb , firstOrder , secondOrder , None ) 
~~ def _generateFile ( filename , numRecords , categoryList , initProb , 
fields = [ ( 'reset' , 'int' , 'R' ) , ( 'name' , 'string' , '' ) ] 
outFile = FileRecordStream ( filename , write = True , fields = fields ) 
~~ secondOrderCumProb = dict ( ) 
for i in xrange ( numRecords ) : 
if len ( elementsInSeq ) == 0 : 
~~ catIdx = min ( maxCatIdx , catIdx ) 
outFile . appendRecord ( [ reset , categoryList [ catIdx ] ] ) 
~~ def _allow_new_attributes ( f ) : 
def decorated ( self , * args , ** kw ) : 
if not hasattr ( self , '_canAddAttributes' ) : 
~~~ self . __dict__ [ '_canAddAttributes' ] = 1 
~~~ self . _canAddAttributes += 1 
~~ assert self . _canAddAttributes >= 1 
count = self . _canAddAttributes 
f ( self , * args , ** kw ) 
if hasattr ( self , '_canAddAttributes' ) : 
~~~ self . _canAddAttributes -= 1 
~~~ self . _canAddAttributes = count - 1 
~~ assert self . _canAddAttributes >= 0 
if self . _canAddAttributes == 0 : 
~~~ del self . _canAddAttributes 
~~ ~~ decorated . __doc__ = f . __doc__ 
decorated . __name__ = f . __name__ 
return decorated 
~~ def _simple_init ( self , * args , ** kw ) : 
type ( self ) . __base__ . __init__ ( self , * args , ** kw ) 
~~ def generatePlot ( outputs , origData ) : 
PLOT_PRECISION = 100 
distribMatrix = np . zeros ( ( PLOT_PRECISION + 1 , PLOT_PRECISION + 1 ) ) 
outputSize = len ( outputs ) 
for i in range ( 0 , outputSize ) : 
~~~ for j in range ( i + 1 , outputSize ) : 
~~~ in1 = outputs [ i ] 
in2 = outputs [ j ] 
dist = ( abs ( in1 - in2 ) > 0.1 ) 
intDist = int ( dist . sum ( ) / 2 + 0.1 ) 
orig1 = origData [ i ] 
orig2 = origData [ j ] 
origDist = ( abs ( orig1 - orig2 ) > 0.1 ) 
intOrigDist = int ( origDist . sum ( ) / 2 + 0.1 ) 
if intDist < 2 and intOrigDist > 10 : 
~~ x = int ( PLOT_PRECISION * intDist / 40.0 ) 
y = int ( PLOT_PRECISION * intOrigDist / 42.0 ) 
if distribMatrix [ x , y ] < 0.1 : 
~~~ distribMatrix [ x , y ] = 3 
~~~ if distribMatrix [ x , y ] < 10 : 
~~~ distribMatrix [ x , y ] += 1 
~~ ~~ ~~ ~~ distribMatrix [ 4 , 50 ] = 3 
distribMatrix [ 4 , 52 ] = 4 
distribMatrix [ 4 , 54 ] = 5 
distribMatrix [ 4 , 56 ] = 6 
distribMatrix [ 4 , 58 ] = 7 
distribMatrix [ 4 , 60 ] = 8 
distribMatrix [ 4 , 62 ] = 9 
distribMatrix [ 4 , 64 ] = 10 
return distribMatrix 
~~ def generateRandomInput ( numRecords , elemSize = 400 , numSet = 42 ) : 
inputs = [ ] 
~~~ input = np . zeros ( elemSize , dtype = realDType ) 
for _ in range ( 0 , numSet ) : 
~~~ ind = np . random . random_integers ( 0 , elemSize - 1 , 1 ) [ 0 ] 
input [ ind ] = 1 
~~ while abs ( input . sum ( ) - numSet ) > 0.1 : 
~~ inputs . append ( input ) 
~~ return inputs 
~~ def appendInputWithSimilarValues ( inputs ) : 
numInputs = len ( inputs ) 
for i in xrange ( numInputs ) : 
~~~ input = inputs [ i ] 
for j in xrange ( len ( input ) - 1 ) : 
~~~ if input [ j ] == 1 and input [ j + 1 ] == 0 : 
~~~ newInput = copy . deepcopy ( input ) 
newInput [ j ] = 0 
newInput [ j + 1 ] = 1 
inputs . append ( newInput ) 
~~ ~~ ~~ ~~ def appendInputWithNSimilarValues ( inputs , numNear = 10 ) : 
skipOne = False 
numChanged = 0 
newInput = copy . deepcopy ( input ) 
~~~ if skipOne : 
~~~ skipOne = False 
~~ if input [ j ] == 1 and input [ j + 1 ] == 0 : 
~~~ newInput [ j ] = 0 
newInput = copy . deepcopy ( newInput ) 
numChanged += 1 
skipOne = True 
if numChanged == numNear : 
~~ ~~ ~~ ~~ ~~ def modifyBits ( inputVal , maxChanges ) : 
changes = np . random . random_integers ( 0 , maxChanges , 1 ) [ 0 ] 
if changes == 0 : 
~~~ return inputVal 
~~ inputWidth = len ( inputVal ) 
whatToChange = np . random . random_integers ( 0 , 41 , changes ) 
runningIndex = - 1 
numModsDone = 0 
for i in xrange ( inputWidth ) : 
~~~ if numModsDone >= changes : 
~~ if inputVal [ i ] == 1 : 
~~~ runningIndex += 1 
if runningIndex in whatToChange : 
~~~ if i != 0 and inputVal [ i - 1 ] == 0 : 
~~~ inputVal [ i - 1 ] = 1 
inputVal [ i ] = 0 
~~ ~~ ~~ ~~ return inputVal 
~~ def getRandomWithMods ( inputSpace , maxChanges ) : 
size = len ( inputSpace ) 
ind = np . random . random_integers ( 0 , size - 1 , 1 ) [ 0 ] 
value = copy . deepcopy ( inputSpace [ ind ] ) 
if maxChanges == 0 : 
~~ return modifyBits ( value , maxChanges ) 
encoder . addMultipleEncoders ( { 
"consumption" : { "fieldname" : u"consumption" , 
"type" : "ScalarEncoder" , 
"name" : u"consumption" , 
"minval" : 0.0 , 
"maxval" : 100.0 , 
"clipInput" : True , 
"w" : 21 , 
"n" : 500 } , 
"timestamp_timeOfDay" : { "fieldname" : u"timestamp" , 
"type" : "DateEncoder" , 
"name" : u"timestamp_timeOfDay" , 
"timeOfDay" : ( 21 , 9.5 ) } 
~~ def createRecordSensor ( network , name , dataSource ) : 
regionType = "py.RecordSensor" 
regionParams = json . dumps ( { "verbosity" : _VERBOSITY } ) 
network . addRegion ( name , regionType , regionParams ) 
sensorRegion = network . regions [ name ] . getSelf ( ) 
sensorRegion . encoder = createEncoder ( ) 
network . regions [ name ] . setParameter ( "predictedField" , "consumption" ) 
return sensorRegion 
sensor = createRecordSensor ( network , name = _RECORD_SENSOR , 
dataSource = dataSource ) 
createSpatialPooler ( network , name = _L1_SPATIAL_POOLER , 
inputWidth = sensor . encoder . getWidth ( ) ) 
linkType = "UniformLink" 
linkParams = "" 
network . link ( _RECORD_SENSOR , _L1_SPATIAL_POOLER , linkType , linkParams ) 
l1temporalMemory = createTemporalMemory ( network , _L1_TEMPORAL_MEMORY ) 
network . link ( _L1_SPATIAL_POOLER , _L1_TEMPORAL_MEMORY , linkType , linkParams ) 
'alpha' : 0.005 , 
'steps' : '1' , 
'implementation' : 'py' , 
'verbosity' : 0 } 
l1Classifier = network . addRegion ( _L1_CLASSIFIER , "py.SDRClassifierRegion" , 
json . dumps ( classifierParams ) ) 
l1Classifier . setParameter ( 'inferenceMode' , True ) 
l1Classifier . setParameter ( 'learningMode' , True ) 
network . link ( _L1_TEMPORAL_MEMORY , _L1_CLASSIFIER , linkType , linkParams , 
network . link ( _RECORD_SENSOR , _L1_CLASSIFIER , linkType , linkParams , 
l2inputWidth = l1temporalMemory . getSelf ( ) . getOutputElementCount ( "bottomUpOut" ) 
createSpatialPooler ( network , name = _L2_SPATIAL_POOLER , inputWidth = l2inputWidth ) 
network . link ( _L1_TEMPORAL_MEMORY , _L2_SPATIAL_POOLER , linkType , linkParams ) 
createTemporalMemory ( network , _L2_TEMPORAL_MEMORY ) 
network . link ( _L2_SPATIAL_POOLER , _L2_TEMPORAL_MEMORY , linkType , linkParams ) 
l2Classifier = network . addRegion ( _L2_CLASSIFIER , "py.SDRClassifierRegion" , 
l2Classifier . setParameter ( 'inferenceMode' , True ) 
l2Classifier . setParameter ( 'learningMode' , True ) 
network . link ( _L2_TEMPORAL_MEMORY , _L2_CLASSIFIER , linkType , linkParams , 
network . link ( _RECORD_SENSOR , _L2_CLASSIFIER , linkType , linkParams , 
~~ def runNetwork ( network , numRecords , writer ) : 
sensorRegion = network . regions [ _RECORD_SENSOR ] 
l1SpRegion = network . regions [ _L1_SPATIAL_POOLER ] 
l1TpRegion = network . regions [ _L1_TEMPORAL_MEMORY ] 
l1Classifier = network . regions [ _L1_CLASSIFIER ] 
l2SpRegion = network . regions [ _L2_SPATIAL_POOLER ] 
l2TpRegion = network . regions [ _L2_TEMPORAL_MEMORY ] 
l2Classifier = network . regions [ _L2_CLASSIFIER ] 
l1PreviousPredictedColumns = [ ] 
l2PreviousPredictedColumns = [ ] 
l1PreviousPrediction = None 
l2PreviousPrediction = None 
l1ErrorSum = 0.0 
l2ErrorSum = 0.0 
for record in xrange ( numRecords ) : 
actual = float ( sensorRegion . getOutputData ( "actValueOut" ) [ 0 ] ) 
l1Predictions = l1Classifier . getOutputData ( "actualValues" ) 
l1Probabilities = l1Classifier . getOutputData ( "probabilities" ) 
l1Prediction = l1Predictions [ l1Probabilities . argmax ( ) ] 
if l1PreviousPrediction is not None : 
~~~ l1ErrorSum += math . fabs ( l1PreviousPrediction - actual ) 
~~ l1PreviousPrediction = l1Prediction 
l2Predictions = l2Classifier . getOutputData ( "actualValues" ) 
l2Probabilities = l2Classifier . getOutputData ( "probabilities" ) 
l2Prediction = l2Predictions [ l2Probabilities . argmax ( ) ] 
if l2PreviousPrediction is not None : 
~~~ l2ErrorSum += math . fabs ( l2PreviousPrediction - actual ) 
~~ l2PreviousPrediction = l2Prediction 
l1AnomalyScore = l1TpRegion . getOutputData ( "anomalyScore" ) [ 0 ] 
l2AnomalyScore = l2TpRegion . getOutputData ( "anomalyScore" ) [ 0 ] 
writer . writerow ( ( record , actual , l1PreviousPrediction , l1AnomalyScore , l2PreviousPrediction , l2AnomalyScore ) ) 
l1PredictedColumns = l1TpRegion . getOutputData ( "topDownOut" ) . nonzero ( ) [ 0 ] 
l1PreviousPredictedColumns = copy . deepcopy ( l1PredictedColumns ) 
l2PredictedColumns = l2TpRegion . getOutputData ( "topDownOut" ) . nonzero ( ) [ 0 ] 
l2PreviousPredictedColumns = copy . deepcopy ( l2PredictedColumns ) 
~~ if numRecords > 1 : 
~~ ~~ def clean ( s ) : 
lines = [ l . rstrip ( ) for l in s . split ( '\\n' ) ] 
return '\\n' . join ( lines ) 
~~ def update ( self , results ) : 
self . _addResults ( results ) 
if not self . __metricSpecs or self . __currentInference is None : 
~~ metricResults = { } 
for metric , spec , label in zip ( self . __metrics , 
self . __metricSpecs , 
self . __metricLabels ) : 
~~~ inferenceElement = spec . inferenceElement 
field = spec . field 
groundTruth = self . _getGroundTruth ( inferenceElement ) 
inference = self . _getInference ( inferenceElement ) 
rawRecord = self . _getRawGroundTruth ( ) 
result = self . __currentResult 
if field : 
~~~ if type ( inference ) in ( list , tuple ) : 
~~~ if field in self . __fieldNameIndexMap : 
~~~ fieldIndex = self . __fieldNameIndexMap [ field ] 
inference = inference [ fieldIndex ] 
~~~ inference = None 
~~ ~~ if groundTruth is not None : 
~~~ if type ( groundTruth ) in ( list , tuple ) : 
groundTruth = groundTruth [ fieldIndex ] 
~~~ groundTruth = None 
~~~ groundTruth = groundTruth [ field ] 
~~ ~~ ~~ metric . addInstance ( groundTruth = groundTruth , 
prediction = inference , 
record = rawRecord , 
result = result ) 
metricResults [ label ] = metric . getMetric ( ) [ 'value' ] 
~~ return metricResults 
~~ def getMetrics ( self ) : 
result = { } 
for metricObj , label in zip ( self . __metrics , self . __metricLabels ) : 
~~~ value = metricObj . getMetric ( ) 
result [ label ] = value [ 'value' ] 
~~ def getMetricDetails ( self , metricLabel ) : 
~~~ metricIndex = self . __metricLabels . index ( metricLabel ) 
~~ return self . __metrics [ metricIndex ] . getMetric ( ) 
~~ def _addResults ( self , results ) : 
if self . __isTemporal : 
~~~ shiftedInferences = self . __inferenceShifter . shift ( results ) . inferences 
self . __currentResult = copy . deepcopy ( results ) 
self . __currentResult . inferences = shiftedInferences 
self . __currentInference = shiftedInferences 
~~~ self . __currentResult = copy . deepcopy ( results ) 
self . __currentInference = copy . deepcopy ( results . inferences ) 
~~ self . __currentGroundTruth = copy . deepcopy ( results ) 
~~ def _getGroundTruth ( self , inferenceElement ) : 
sensorInputElement = InferenceElement . getInputElement ( inferenceElement ) 
if sensorInputElement is None : 
~~ return getattr ( self . __currentGroundTruth . sensorInput , sensorInputElement ) 
~~ def __constructMetricsModules ( self , metricSpecs ) : 
if not metricSpecs : 
~~ self . __metricSpecs = metricSpecs 
for spec in metricSpecs : 
~~~ if not InferenceElement . validate ( spec . inferenceElement ) : 
~~ self . __metrics . append ( metrics . getModule ( spec ) ) 
self . __metricLabels . append ( spec . getLabel ( ) ) 
~~ ~~ def _generateSimple ( filename = "simple.csv" , numSequences = 1 , elementsPerSeq = 3 , 
numRepeats = 10 ) : 
fields = [ ( 'timestamp' , 'datetime' , 'T' ) , 
timestamp = datetime . datetime ( year = 2012 , month = 1 , day = 1 , hour = 0 , minute = 0 , 
second = 0 ) 
timeDelta = datetime . timedelta ( hours = 1 ) 
~~~ seq = sequences [ seqIdx ] 
~~~ outFile . appendRecord ( [ timestamp , str ( x ) , x ] ) 
timestamp += timeDelta 
~~ ~~ for seqIdx in seqIdxs : 
for i , x in enumerate ( seq ) : 
~~~ if i != 1 : 
~~ timestamp += timeDelta 
~~ def shift ( self , modelResult ) : 
inferencesToWrite = { } 
if self . _inferenceBuffer is None : 
~~~ maxDelay = InferenceElement . getMaxDelay ( modelResult . inferences ) 
self . _inferenceBuffer = collections . deque ( maxlen = maxDelay + 1 ) 
~~ self . _inferenceBuffer . appendleft ( copy . deepcopy ( modelResult . inferences ) ) 
for inferenceElement , inference in modelResult . inferences . iteritems ( ) : 
~~~ if isinstance ( inference , dict ) : 
~~~ inferencesToWrite [ inferenceElement ] = { } 
for key , _ in inference . iteritems ( ) : 
~~~ delay = InferenceElement . getTemporalDelay ( inferenceElement , key ) 
if len ( self . _inferenceBuffer ) > delay : 
~~~ prevInference = self . _inferenceBuffer [ delay ] [ inferenceElement ] [ key ] 
inferencesToWrite [ inferenceElement ] [ key ] = prevInference 
~~~ inferencesToWrite [ inferenceElement ] [ key ] = None 
~~~ delay = InferenceElement . getTemporalDelay ( inferenceElement ) 
~~~ inferencesToWrite [ inferenceElement ] = ( 
self . _inferenceBuffer [ delay ] [ inferenceElement ] ) 
~~~ inferencesToWrite [ inferenceElement ] = [ None ] * len ( inference ) 
~~~ inferencesToWrite [ inferenceElement ] = None 
~~ ~~ ~~ ~~ shiftedResult = ModelResult ( rawInput = modelResult . rawInput , 
sensorInput = modelResult . sensorInput , 
inferences = inferencesToWrite , 
metrics = modelResult . metrics , 
predictedFieldIdx = modelResult . predictedFieldIdx , 
predictedFieldName = modelResult . predictedFieldName ) 
return shiftedResult 
~~ def generateStats ( filename , maxSamples = None , ) : 
statsCollectorMapping = { 'float' : FloatStatsCollector , 
'int' : IntStatsCollector , 
'string' : StringStatsCollector , 
'datetime' : DateTimeStatsCollector , 
'bool' : BoolStatsCollector , 
filename = resource_filename ( "nupic.datafiles" , filename ) 
print "*" * 40 
dataFile = FileRecordStream ( filename ) 
statsCollectors = [ ] 
for fieldName , fieldType , fieldSpecial in dataFile . getFields ( ) : 
~~~ statsCollector = statsCollectorMapping [ fieldType ] ( fieldName , fieldType , fieldSpecial ) 
statsCollectors . append ( statsCollector ) 
~~ if maxSamples is None : 
~~~ maxSamples = 500000 
~~ for i in xrange ( maxSamples ) : 
~~~ record = dataFile . getNextRecord ( ) 
~~ for i , value in enumerate ( record ) : 
~~~ statsCollectors [ i ] . addValue ( value ) 
~~ ~~ stats = { } 
for statsCollector in statsCollectors : 
~~~ statsCollector . getStats ( stats ) 
~~ if dataFile . getResetFieldIdx ( ) is not None : 
~~~ resetFieldName , _ , _ = dataFile . getFields ( ) [ dataFile . reset ] 
stats . pop ( resetFieldName ) 
~~ if VERBOSITY > 0 : 
~~~ pprint . pprint ( stats ) 
~~ return stats 
~~ def getStats ( self , stats ) : 
BaseStatsCollector . getStats ( self , stats ) 
sortedNumberList = sorted ( self . valueList ) 
listLength = len ( sortedNumberList ) 
min = sortedNumberList [ 0 ] 
max = sortedNumberList [ - 1 ] 
mean = numpy . mean ( self . valueList ) 
median = sortedNumberList [ int ( 0.5 * listLength ) ] 
percentile1st = sortedNumberList [ int ( 0.01 * listLength ) ] 
percentile99th = sortedNumberList [ int ( 0.99 * listLength ) ] 
differenceList = [ ( cur - prev ) for prev , cur in itertools . izip ( list ( self . valueSet ) [ : - 1 ] , 
list ( self . valueSet ) [ 1 : ] ) ] 
if min > max : 
~~~ print self . fieldname , min , max , '-----' 
~~ meanResolution = numpy . mean ( differenceList ) 
stats [ self . fieldname ] [ 'min' ] = min 
stats [ self . fieldname ] [ 'max' ] = max 
stats [ self . fieldname ] [ 'mean' ] = mean 
stats [ self . fieldname ] [ 'median' ] = median 
stats [ self . fieldname ] [ 'percentile1st' ] = percentile1st 
stats [ self . fieldname ] [ 'percentile99th' ] = percentile99th 
stats [ self . fieldname ] [ 'meanResolution' ] = meanResolution 
passData = True 
if passData : 
~~~ stats [ self . fieldname ] [ 'data' ] = self . valueList 
~~ if VERBOSITY > 2 : 
~~~ print '--' 
print "Statistics:" 
print "min:" , min 
print "max:" , max 
print "mean:" , mean 
print "median:" , median 
print '--' 
print "Resolution:" 
~~ if VERBOSITY > 3 : 
print "Histogram:" 
counts , bins = numpy . histogram ( self . valueList , new = True ) 
print "Counts:" , counts . tolist ( ) 
print "Bins:" , bins . tolist ( ) 
~~ ~~ def main ( ) : 
initLogging ( verbose = True ) 
initExperimentPrng ( ) 
@ staticmethod 
def _mockCreate ( * args , ** kwargs ) : 
~~~ kwargs . pop ( 'implementation' , None ) 
return SDRClassifierDiff ( * args , ** kwargs ) 
~~ SDRClassifierFactory . create = _mockCreate 
runExperiment ( sys . argv [ 1 : ] ) 
~~ def _abbreviate ( text , threshold ) : 
if text is not None and len ( text ) > threshold : 
~~~ text = text [ : threshold ] + "..." 
~~ return text 
~~ def __getDBNameForVersion ( cls , dbVersion ) : 
prefix = cls . __getDBNamePrefixForVersion ( dbVersion ) 
suffix = Configuration . get ( 'nupic.cluster.database.nameSuffix' ) 
suffix = suffix . replace ( "-" , "_" ) 
suffix = suffix . replace ( "." , "_" ) 
dbName = '%s_%s' % ( prefix , suffix ) 
return dbName 
~~ def get ( ) : 
if ClientJobsDAO . _instance is None : 
~~~ cjDAO = ClientJobsDAO ( ) 
cjDAO . connect ( ) 
ClientJobsDAO . _instance = cjDAO 
~~ return ClientJobsDAO . _instance 
~~ def _columnNameDBToPublic ( self , dbName ) : 
words = dbName . split ( '_' ) 
if dbName . startswith ( '_' ) : 
~~~ words = words [ 1 : ] 
~~ pubWords = [ words [ 0 ] ] 
for word in words [ 1 : ] : 
~~~ pubWords . append ( word [ 0 ] . upper ( ) + word [ 1 : ] ) 
~~ return '' . join ( pubWords ) 
~~ def connect ( self , deleteOldVersions = False , recreate = False ) : 
with ConnectionFactory . get ( ) as conn : 
~~~ self . _initTables ( cursor = conn . cursor , deleteOldVersions = deleteOldVersions , 
recreate = recreate ) 
self . _connectionID = conn . cursor . fetchall ( ) [ 0 ] [ 0 ] 
self . _logger . info ( "clientJobsConnectionID=%r" , self . _connectionID ) 
~~ def _initTables ( self , cursor , deleteOldVersions , recreate ) : 
if deleteOldVersions : 
traceback . format_stack ( ) ) 
for i in range ( self . _DB_VERSION ) : 
( self . __getDBNameForVersion ( i ) , ) ) 
~~ ~~ if recreate : 
self . dbName , traceback . format_stack ( ) ) 
output = cursor . fetchall ( ) 
tableNames = [ x [ 0 ] for x in output ] 
if 'jobs' not in tableNames : 
fields = [ 
\ % self . CMPL_REASON_SUCCESS , 
\ % self . CLEAN_NOT_DONE , 
options = [ 
'AUTO_INCREMENT=1000' , 
cursor . execute ( query ) 
~~ if 'models' not in tableNames : 
fields = cursor . fetchall ( ) 
self . _jobs . dbFieldNames = [ str ( field [ 0 ] ) for field in fields ] 
self . _models . dbFieldNames = [ str ( field [ 0 ] ) for field in fields ] 
self . _jobs . publicFieldNames = [ self . _columnNameDBToPublic ( x ) 
for x in self . _jobs . dbFieldNames ] 
self . _models . publicFieldNames = [ self . _columnNameDBToPublic ( x ) 
for x in self . _models . dbFieldNames ] 
self . _jobs . pubToDBNameDict = dict ( 
zip ( self . _jobs . publicFieldNames , self . _jobs . dbFieldNames ) ) 
self . _jobs . dbToPubNameDict = dict ( 
zip ( self . _jobs . dbFieldNames , self . _jobs . publicFieldNames ) ) 
self . _models . pubToDBNameDict = dict ( 
zip ( self . _models . publicFieldNames , self . _models . dbFieldNames ) ) 
self . _models . dbToPubNameDict = dict ( 
zip ( self . _models . dbFieldNames , self . _models . publicFieldNames ) ) 
self . _models . modelInfoNamedTuple = collections . namedtuple ( 
'_modelInfoNamedTuple' , self . _models . publicFieldNames ) 
self . _jobs . jobInfoNamedTuple = collections . namedtuple ( 
'_jobInfoNamedTuple' , self . _jobs . publicFieldNames ) 
~~ def _getMatchingRowsNoRetries ( self , tableInfo , conn , fieldsToMatch , 
selectFieldNames , maxRows = None ) : 
assert fieldsToMatch , repr ( fieldsToMatch ) 
assert all ( k in tableInfo . dbFieldNames 
for k in fieldsToMatch . iterkeys ( ) ) , repr ( fieldsToMatch ) 
assert selectFieldNames , repr ( selectFieldNames ) 
assert all ( f in tableInfo . dbFieldNames for f in selectFieldNames ) , repr ( 
selectFieldNames ) 
matchPairs = fieldsToMatch . items ( ) 
matchExpressionGen = ( 
p [ 0 ] + 
else '=%s' ) 
for p in matchPairs ) 
matchFieldValues = [ p [ 1 ] for p in matchPairs 
if ( not isinstance ( p [ 1 ] , ( bool ) ) and p [ 1 ] is not None ) ] 
',' . join ( selectFieldNames ) , tableInfo . tableName , 
sqlParams = matchFieldValues 
if maxRows is not None : 
sqlParams . append ( maxRows ) 
~~ conn . cursor . execute ( query , sqlParams ) 
rows = conn . cursor . fetchall ( ) 
if rows : 
len ( rows ) , maxRows ) 
len ( rows [ 0 ] ) , len ( selectFieldNames ) ) 
~~~ rows = tuple ( ) 
~~ return rows 
~~ def _getMatchingRowsWithRetries ( self , tableInfo , fieldsToMatch , 
~~~ return self . _getMatchingRowsNoRetries ( tableInfo , conn , fieldsToMatch , 
selectFieldNames , maxRows ) 
~~ ~~ def _getOneMatchingRowNoRetries ( self , tableInfo , conn , fieldsToMatch , 
selectFieldNames ) : 
rows = self . _getMatchingRowsNoRetries ( tableInfo , conn , fieldsToMatch , 
selectFieldNames , maxRows = 1 ) 
~~~ assert len ( rows ) == 1 , repr ( len ( rows ) ) 
result = rows [ 0 ] 
~~ def _getOneMatchingRowWithRetries ( self , tableInfo , fieldsToMatch , 
~~~ return self . _getOneMatchingRowNoRetries ( tableInfo , conn , fieldsToMatch , 
~~ ~~ def _insertOrGetUniqueJobNoRetries ( 
self , conn , client , cmdLine , jobHash , clientInfo , clientKey , params , 
minimumWorkers , maximumWorkers , jobType , priority , alreadyRunning ) : 
if alreadyRunning : 
~~~ initStatus = self . STATUS_TESTMODE 
~~~ initStatus = self . STATUS_NOTSTARTED 
sqlParams = ( initStatus , client , clientInfo , clientKey , cmdLine , params , 
jobHash , minimumWorkers , maximumWorkers , priority , jobType ) 
numRowsInserted = conn . cursor . execute ( query , sqlParams ) 
jobID = 0 
if numRowsInserted == 1 : 
jobID = conn . cursor . fetchall ( ) [ 0 ] [ 0 ] 
if jobID == 0 : 
~~~ self . _logger . warn ( 
'cmdLine=%r' , 
jobType , client , _abbreviate ( clientInfo , 32 ) , clientKey , jobHash , 
cmdLine ) 
~~~ assert numRowsInserted == 0 , repr ( numRowsInserted ) 
~~ if jobID == 0 : 
~~~ row = self . _getOneMatchingRowNoRetries ( 
self . _jobs , conn , dict ( client = client , job_hash = jobHash ) , [ 'job_id' ] ) 
assert row is not None 
jobID = row [ 0 ] 
~~ if alreadyRunning : 
conn . cursor . execute ( query , ( self . _connectionID , jobID ) ) 
~~ def _resumeJobNoRetries ( self , conn , jobID , alreadyRunning ) : 
~~ assignments = [ 
'status=%s' , 
'completion_reason=DEFAULT' , 
'completion_msg=DEFAULT' , 
'worker_completion_reason=DEFAULT' , 
'worker_completion_msg=DEFAULT' , 
'end_time=DEFAULT' , 
'cancel=DEFAULT' , 
'_eng_last_update_time=UTC_TIMESTAMP()' , 
'_eng_allocate_new_workers=DEFAULT' , 
'_eng_untended_dead_workers=DEFAULT' , 
'num_failed_workers=DEFAULT' , 
'last_failed_worker_error_msg=DEFAULT' , 
'_eng_cleaning_status=DEFAULT' , 
assignmentValues = [ initStatus ] 
~~~ assignments += [ '_eng_cjm_conn_id=%s' , 'start_time=UTC_TIMESTAMP()' , 
'_eng_last_update_time=UTC_TIMESTAMP()' ] 
assignmentValues . append ( self . _connectionID ) 
~~~ assignments += [ '_eng_cjm_conn_id=DEFAULT' , 'start_time=DEFAULT' ] 
sqlParams = assignmentValues + [ jobID , self . STATUS_COMPLETED ] 
numRowsAffected = conn . cursor . execute ( query , sqlParams ) 
assert numRowsAffected <= 1 , repr ( numRowsAffected ) 
if numRowsAffected == 0 : 
~~ def jobResume ( self , jobID , alreadyRunning = False ) : 
row = self . jobGetFields ( jobID , [ 'status' ] ) 
( jobStatus , ) = row 
if jobStatus != self . STATUS_COMPLETED : 
~~ @ g_retrySQL 
def resumeWithRetries ( ) : 
~~~ with ConnectionFactory . get ( ) as conn : 
~~~ self . _resumeJobNoRetries ( conn , jobID , alreadyRunning ) 
~~ ~~ resumeWithRetries ( ) 
~~ def jobInsert ( self , client , cmdLine , clientInfo = '' , clientKey = '' , params = '' , 
alreadyRunning = False , minimumWorkers = 0 , maximumWorkers = 0 , 
jobType = '' , priority = DEFAULT_JOB_PRIORITY ) : 
jobHash = self . _normalizeHash ( uuid . uuid1 ( ) . bytes ) 
@ g_retrySQL 
def insertWithRetries ( ) : 
~~~ return self . _insertOrGetUniqueJobNoRetries ( 
conn , client = client , cmdLine = cmdLine , jobHash = jobHash , 
clientInfo = clientInfo , clientKey = clientKey , params = params , 
minimumWorkers = minimumWorkers , maximumWorkers = maximumWorkers , 
jobType = jobType , priority = priority , alreadyRunning = alreadyRunning ) 
~~~ jobID = insertWithRetries ( ) 
jobType , client , _abbreviate ( clientInfo , 48 ) , clientKey , jobHash , 
jobID , jobType , client , _abbreviate ( clientInfo , 48 ) , clientKey , 
jobHash , cmdLine ) 
~~ def jobInsertUnique ( self , client , cmdLine , jobHash , clientInfo = '' , 
clientKey = '' , params = '' , minimumWorkers = 0 , 
maximumWorkers = 0 , jobType = '' , 
priority = DEFAULT_JOB_PRIORITY ) : 
def insertUniqueWithRetries ( ) : 
~~~ jobHashValue = self . _normalizeHash ( jobHash ) 
self . _jobs , conn , dict ( client = client , job_hash = jobHashValue ) , 
[ 'job_id' , 'status' ] ) 
if row is not None : 
~~~ ( jobID , status ) = row 
if status == self . STATUS_COMPLETED : 
sqlParams = ( clientInfo , clientKey , cmdLine , params , 
minimumWorkers , maximumWorkers , priority , 
jobType , jobID , self . STATUS_COMPLETED ) 
numRowsUpdated = conn . cursor . execute ( query , sqlParams ) 
assert numRowsUpdated <= 1 , repr ( numRowsUpdated ) 
if numRowsUpdated == 0 : 
~~ self . _resumeJobNoRetries ( conn , jobID , alreadyRunning = False ) 
~~~ jobID = self . _insertOrGetUniqueJobNoRetries ( 
conn , client = client , cmdLine = cmdLine , jobHash = jobHashValue , 
jobType = jobType , priority = priority , alreadyRunning = False ) 
~~~ jobID = insertUniqueWithRetries ( ) 
~~ def _startJobWithRetries ( self , jobID ) : 
sqlParams = [ self . STATUS_RUNNING , self . _connectionID , 
jobID , self . STATUS_NOTSTARTED ] 
if numRowsUpdated != 1 : 
'failure' , numRowsUpdated ) 
~~ def jobStartNext ( self ) : 
row = self . _getOneMatchingRowWithRetries ( 
self . _jobs , dict ( status = self . STATUS_NOTSTARTED ) , [ 'job_id' ] ) 
~~ ( jobID , ) = row 
self . _startJobWithRetries ( jobID ) 
return jobID 
~~ def jobReactivateRunningJobs ( self ) : 
conn . cursor . execute ( query , [ self . _connectionID , self . STATUS_RUNNING ] ) 
~~ def jobGetDemand ( self , ) : 
rows = self . _getMatchingRowsWithRetries ( 
self . _jobs , dict ( status = self . STATUS_RUNNING ) , 
[ self . _jobs . pubToDBNameDict [ f ] 
for f in self . _jobs . jobDemandNamedTuple . _fields ] ) 
return [ self . _jobs . jobDemandNamedTuple . _make ( r ) for r in rows ] 
~~ def jobCancelAllRunningJobs ( self ) : 
conn . cursor . execute ( query , [ self . STATUS_COMPLETED ] ) 
~~ def jobCountCancellingJobs ( self , ) : 
~~ return rows [ 0 ] [ 0 ] 
~~ def jobGetCancellingJobs ( self , ) : 
~~ return tuple ( r [ 0 ] for r in rows ) 
~~ def partitionAtIntervals ( data , intervals ) : 
assert sum ( intervals ) <= len ( data ) 
for interval in intervals : 
~~~ end = start + interval 
yield data [ start : end ] 
start = end 
~~ raise StopIteration 
~~ def _combineResults ( result , * namedTuples ) : 
results = ClientJobsDAO . partitionAtIntervals ( 
result , [ len ( nt . _fields ) for nt in namedTuples ] ) 
return [ nt . _make ( result ) for nt , result in zip ( namedTuples , results ) ] 
~~ def jobInfoWithModels ( self , jobID ) : 
combinedResults = None 
conn . cursor . execute ( query , ( jobID , ) ) 
if conn . cursor . rowcount > 0 : 
~~~ combinedResults = [ 
ClientJobsDAO . _combineResults ( 
result , self . _jobs . jobInfoNamedTuple , 
self . _models . modelInfoNamedTuple 
) for result in conn . cursor . fetchall ( ) ] 
~~ ~~ if combinedResults is not None : 
~~~ return combinedResults 
~~ def jobInfo ( self , jobID ) : 
self . _jobs , dict ( job_id = jobID ) , 
[ self . _jobs . pubToDBNameDict [ n ] 
for n in self . _jobs . jobInfoNamedTuple . _fields ] ) 
~~ return self . _jobs . jobInfoNamedTuple . _make ( row ) 
~~ def jobSetStatus ( self , jobID , status , useConnectionID = True , ) : 
sqlParams = [ status , jobID ] 
if useConnectionID : 
sqlParams . append ( self . _connectionID ) 
~~ result = conn . cursor . execute ( query , sqlParams ) 
if result != 1 : 
jobID , status ) ) 
~~ ~~ ~~ def jobSetCompleted ( self , jobID , completionReason , completionMsg , 
useConnectionID = True ) : 
sqlParams = [ self . STATUS_COMPLETED , completionReason , completionMsg , 
jobID ] 
~~ ~~ ~~ def jobCancel ( self , jobID ) : 
self . jobSetFields ( jobID , { "cancel" : True } , useConnectionID = False ) 
~~ def jobGetModelIDs ( self , jobID ) : 
rows = self . _getMatchingRowsWithRetries ( self . _models , dict ( job_id = jobID ) , 
[ 'model_id' ] ) 
return [ r [ 0 ] for r in rows ] 
~~ def getActiveJobCountForClientInfo ( self , clientInfo ) : 
conn . cursor . execute ( query , [ clientInfo , self . STATUS_COMPLETED ] ) 
activeJobCount = conn . cursor . fetchone ( ) [ 0 ] 
~~ return activeJobCount 
~~ def getActiveJobCountForClientKey ( self , clientKey ) : 
conn . cursor . execute ( query , [ clientKey , self . STATUS_COMPLETED ] ) 
~~ def getActiveJobsForClientInfo ( self , clientInfo , fields = [ ] ) : 
dbFields = [ self . _jobs . pubToDBNameDict [ x ] for x in fields ] 
dbFieldsStr = ',' . join ( [ 'job_id' ] + dbFields ) 
~~ def getFieldsForActiveJobsOfType ( self , jobType , fields = [ ] ) : 
self . modelsTableName ) 
conn . cursor . execute ( query , [ self . STATUS_COMPLETED , jobType ] ) 
return conn . cursor . fetchall ( ) 
~~ ~~ def jobGetFields ( self , jobID , fields ) : 
return self . jobsGetFields ( [ jobID ] , fields , requireAll = True ) [ 0 ] [ 1 ] 
~~ def jobsGetFields ( self , jobIDs , fields , requireAll = True ) : 
assert isinstance ( jobIDs , self . _SEQUENCE_TYPES ) 
assert len ( jobIDs ) >= 1 
self . _jobs , dict ( job_id = jobIDs ) , 
[ 'job_id' ] + [ self . _jobs . pubToDBNameDict [ x ] for x in fields ] ) 
if requireAll and len ( rows ) < len ( jobIDs ) : 
( set ( jobIDs ) - set ( r [ 0 ] for r in rows ) ) , ) ) 
~~ return [ ( r [ 0 ] , list ( r [ 1 : ] ) ) for r in rows ] 
~~ def jobSetFields ( self , jobID , fields , useConnectionID = True , 
ignoreUnchanged = False ) : 
assignmentExpressions = ',' . join ( 
[ "%s=%%s" % ( self . _jobs . pubToDBNameDict [ f ] , ) for f in fields . iterkeys ( ) ] ) 
assignmentValues = fields . values ( ) 
sqlParams = assignmentValues + [ jobID ] 
~~ with ConnectionFactory . get ( ) as conn : 
~~~ result = conn . cursor . execute ( query , sqlParams ) 
~~ if result != 1 and not ignoreUnchanged : 
assignmentExpressions , jobID , self . _connectionID , result , query ) ) 
~~ ~~ def jobSetFieldIfEqual ( self , jobID , fieldName , newValue , curValue ) : 
dbFieldName = self . _jobs . pubToDBNameDict [ fieldName ] 
conditionValue = [ ] 
if isinstance ( curValue , bool ) : 
dbFieldName , { True : 'TRUE' , False : 'FALSE' } [ curValue ] ) 
~~ elif curValue is None : 
~~~ conditionExpression = '%s=%%s' % ( dbFieldName , ) 
conditionValue . append ( curValue ) 
sqlParams = [ newValue , jobID ] + conditionValue 
~~ return ( result == 1 ) 
~~ def jobIncrementIntField ( self , jobID , fieldName , increment = 1 , 
useConnectionID = False ) : 
sqlParams = [ increment , jobID ] 
~~ if result != 1 : 
dbFieldName , jobID , self . _connectionID , result , query ) ) 
~~ ~~ def jobUpdateResults ( self , jobID , results ) : 
conn . cursor . execute ( query , [ results , jobID ] ) 
~~ ~~ def modelsClearAll ( self ) : 
conn . cursor . execute ( query ) 
~~ ~~ def modelInsertAndStart ( self , jobID , params , paramsHash , particleHash = None ) : 
if particleHash is None : 
~~~ particleHash = paramsHash 
~~ paramsHash = self . _normalizeHash ( paramsHash ) 
particleHash = self . _normalizeHash ( particleHash ) 
def findExactMatchNoRetries ( conn ) : 
~~~ return self . _getOneMatchingRowNoRetries ( 
self . _models , conn , 
{ 'job_id' : jobID , '_eng_params_hash' : paramsHash , 
'_eng_particle_hash' : particleHash } , 
[ 'model_id' , '_eng_worker_conn_id' ] ) 
def findExactMatchWithRetries ( ) : 
~~~ return findExactMatchNoRetries ( conn ) 
~~ ~~ row = findExactMatchWithRetries ( ) 
~~~ return ( row [ 0 ] , False ) 
def insertModelWithRetries ( ) : 
sqlParams = ( jobID , params , self . STATUS_RUNNING , paramsHash , 
particleHash , self . _connectionID ) 
~~~ numRowsAffected = conn . cursor . execute ( query , sqlParams ) 
jobID , paramsHash . encode ( 'hex' ) , 
particleHash . encode ( 'hex' ) , e ) 
~~~ if numRowsAffected == 1 : 
modelID = conn . cursor . fetchall ( ) [ 0 ] [ 0 ] 
if modelID != 0 : 
~~~ return ( modelID , True ) 
jobID , paramsHash , particleHash ) 
~~~ self . _logger . error ( 
'particleHash=%r' , 
numRowsAffected , jobID , paramsHash , particleHash ) 
~~ ~~ row = findExactMatchNoRetries ( conn ) 
~~~ ( modelID , connectionID ) = row 
return ( modelID , connectionID == self . _connectionID ) 
sqlParams = [ jobID , paramsHash , particleHash ] 
numRowsFound = conn . cursor . execute ( query , sqlParams ) 
assert numRowsFound == 1 , ( 
'numRowsFound=%r' ) % ( jobID , paramsHash , particleHash , numRowsFound ) 
( modelID , ) = conn . cursor . fetchall ( ) [ 0 ] 
return ( modelID , False ) 
~~ ~~ return insertModelWithRetries ( ) 
~~ def modelsInfo ( self , modelIDs ) : 
assert isinstance ( modelIDs , self . _SEQUENCE_TYPES ) , ( 
self . _models , dict ( model_id = modelIDs ) , 
[ self . _models . pubToDBNameDict [ f ] 
for f in self . _models . modelInfoNamedTuple . _fields ] ) 
results = [ self . _models . modelInfoNamedTuple . _make ( r ) for r in rows ] 
set ( modelIDs ) - set ( r . modelId for r in results ) ) 
return results 
~~ def modelsGetFields ( self , modelIDs , fields ) : 
isSequence = isinstance ( modelIDs , self . _SEQUENCE_TYPES ) 
if isSequence : 
~~~ modelIDs = [ modelIDs ] 
~~ rows = self . _getMatchingRowsWithRetries ( 
[ 'model_id' ] + [ self . _models . pubToDBNameDict [ f ] for f in fields ] ) 
if len ( rows ) < len ( modelIDs ) : 
( set ( modelIDs ) - set ( r [ 0 ] for r in rows ) ) , ) ) 
~~ if not isSequence : 
~~~ return list ( rows [ 0 ] [ 1 : ] ) 
~~ def modelsGetFieldsForJob ( self , jobID , fields , ignoreKilled = False ) : 
dbFields = [ self . _models . pubToDBNameDict [ x ] for x in fields ] 
dbFieldsStr = ',' . join ( dbFields ) 
sqlParams = [ jobID ] 
if ignoreKilled : 
sqlParams . append ( self . CMPL_REASON_KILLED ) 
~~~ conn . cursor . execute ( query , sqlParams ) 
~~ if rows is None : 
query , traceback . format_exc ( ) ) 
~~ def modelsGetFieldsForCheckpointed ( self , jobID , fields ) : 
~~~ dbFields = [ self . _models . pubToDBNameDict [ f ] for f in fields ] 
fields = dbFieldStr , models = self . modelsTableName ) 
conn . cursor . execute ( query , [ jobID ] ) 
~~ def modelSetFields ( self , modelID , fields , ignoreUnchanged = False ) : 
'%s=%%s' % ( self . _models . pubToDBNameDict [ f ] , ) for f in fields . iterkeys ( ) ) 
sqlParams = assignmentValues + [ modelID ] 
~~~ numAffectedRows = conn . cursor . execute ( query , sqlParams ) 
numAffectedRows , query , sqlParams ) 
~~ if numAffectedRows != 1 and not ignoreUnchanged : 
fields , modelID , self . _connectionID , numAffectedRows , query , 
sqlParams , ) ) 
~~ ~~ def modelsGetParams ( self , modelIDs ) : 
self . _models , { 'model_id' : modelIDs } , 
for f in self . _models . getParamsNamedTuple . _fields ] ) 
assert len ( rows ) == len ( modelIDs ) , "Didn\ % ( 
( set ( modelIDs ) - set ( r [ 0 ] for r in rows ) ) , ) 
return [ self . _models . getParamsNamedTuple . _make ( r ) for r in rows ] 
~~ def modelsGetResultAndStatus ( self , modelIDs ) : 
for f in self . _models . getResultAndStatusNamedTuple . _fields ] ) 
return [ self . _models . getResultAndStatusNamedTuple . _make ( r ) for r in rows ] 
~~ def modelsGetUpdateCounters ( self , jobID ) : 
self . _models , { 'job_id' : jobID } , 
for f in self . _models . getUpdateCountersNamedTuple . _fields ] ) 
return [ self . _models . getUpdateCountersNamedTuple . _make ( r ) for r in rows ] 
~~ def modelUpdateResults ( self , modelID , results = None , metricValue = None , 
numRecords = None ) : 
assignmentExpressions = [ '_eng_last_update_time=UTC_TIMESTAMP()' , 
'update_counter=update_counter+1' ] 
assignmentValues = [ ] 
if results is not None : 
~~~ assignmentExpressions . append ( 'results=%s' ) 
assignmentValues . append ( results ) 
~~ if numRecords is not None : 
~~~ assignmentExpressions . append ( 'num_records=%s' ) 
assignmentValues . append ( numRecords ) 
~~ if metricValue is not None and ( metricValue == metricValue ) : 
~~~ assignmentExpressions . append ( 'optimized_metric=%s' ) 
assignmentValues . append ( float ( metricValue ) ) 
sqlParams = assignmentValues + [ modelID , self . _connectionID ] 
~~ if numRowsAffected != 1 : 
~~~ raise InvalidConnectionException ( 
"numRowsAffected=%r" ) % ( modelID , self . _connectionID , numRowsAffected , ) ) 
~~ ~~ def modelSetCompleted ( self , modelID , completionReason , completionMsg , 
cpuTime = 0 , useConnectionID = True ) : 
if completionMsg is None : 
~~~ completionMsg = '' 
cpuTime , modelID ] 
"numRowsAffected=%r" ) % ( modelID , self . _connectionID , numRowsAffected ) ) 
~~ ~~ def modelAdoptNextOrphan ( self , jobId , maxUpdateInterval ) : 
def findCandidateModelWithRetries ( ) : 
~~~ modelID = None 
sqlParams = [ self . STATUS_RUNNING , jobId , maxUpdateInterval ] 
numRows = conn . cursor . execute ( query , sqlParams ) 
if numRows == 1 : 
~~~ ( modelID , ) = rows [ 0 ] 
~~ return modelID 
def adoptModelWithRetries ( modelID ) : 
~~~ adopted = False 
sqlParams = [ self . _connectionID , modelID , self . STATUS_RUNNING , 
maxUpdateInterval ] 
numRowsAffected , ) 
if numRowsAffected == 1 : 
~~~ adopted = True 
~~~ ( status , connectionID ) = self . _getOneMatchingRowNoRetries ( 
self . _models , conn , { 'model_id' : modelID } , 
[ 'status' , '_eng_worker_conn_id' ] ) 
adopted = ( status == self . STATUS_RUNNING and 
connectionID == self . _connectionID ) 
~~ ~~ return adopted 
~~ adoptedModelID = None 
~~~ modelID = findCandidateModelWithRetries ( ) 
if modelID is None : 
~~ if adoptModelWithRetries ( modelID ) : 
~~~ adoptedModelID = modelID 
~~ ~~ return adoptedModelID 
~~ def profileSP ( spClass , spDim , nRuns ) : 
inDim = [ 10000 , 1 , 1 ] 
colDim = [ spDim , 1 , 1 ] 
sp = spClass ( 
inputDimensions = inDim , 
columnDimensions = colDim , 
potentialRadius = 3 , 
potentialPct = 0.5 , 
globalInhibition = False , 
localAreaDensity = - 1.0 , 
numActiveColumnsPerInhArea = 3 , 
stimulusThreshold = 1 , 
synPermInactiveDec = 0.01 , 
synPermActiveInc = 0.1 , 
synPermConnected = 0.10 , 
minPctOverlapDutyCycle = 0.1 , 
dutyCyclePeriod = 10 , 
boostStrength = 10.0 , 
seed = 42 , 
spVerbosity = 0 ) 
dataDim = inDim 
dataDim . append ( nRuns ) 
data = numpy . random . randint ( 0 , 2 , dataDim ) . astype ( 'float32' ) 
~~~ d = data [ : , : , : , i ] 
activeArray = numpy . zeros ( colDim ) 
sp . compute ( d , True , activeArray ) 
~~ ~~ def getSpec ( cls ) : 
description = KNNClassifierRegion . __doc__ , 
partitionIn = dict ( 
auxDataIn = dict ( 
requireSplitterMap = False ) 
bestPrototypeIndices = dict ( 
categoryProbabilitiesOut = dict ( 
acceptanceProbability = dict ( 
defaultValue = 1.0 , 
#accessMode='Create'), 
confusion = dict ( 
dataType = 'Handle' , 
count = 2 , 
defaultValue = None , 
accessMode = 'Read' ) , 
categoryCount = dict ( 
patternCount = dict ( 
patternMatrix = dict ( 
k = dict ( 
defaultValue = 2 , 
distanceNorm = dict ( 
defaultValue = 2.0 , 
distanceMethod = dict ( 
defaultValue = 'norm' , 
outputProbabilitiesByDist = dict ( 
distThreshold = dict ( 
defaultValue = 0.0 , 
inputThresh = dict ( 
defaultValue = 0.5 , 
doBinarization = dict ( 
useSparseMemory = dict ( 
minSparsity = dict ( 
sparseThreshold = dict ( 
relativeThreshold = dict ( 
winnerCount = dict ( 
doSphering = dict ( 
SVDSampleCount = dict ( 
SVDDimCount = dict ( 
dataType = 'Int32' , 
defaultValue = - 1 , 
fractionOfMax = dict ( 
useAuxiliary = dict ( 
justUseAuxiliary = dict ( 
keepAllDistances = dict ( 
replaceDuplicates = dict ( 
'learning.' , 
cellsPerCol = dict ( 
maxStoredPatterns = dict ( 
self . _firstComputeCall = True 
self . _accuracy = None 
self . _protoScores = None 
self . _categoryDistances = None 
self . _knn = knn_classifier . KNNClassifier ( ** self . knnParams ) 
for x in ( '_partitions' , '_useAuxiliary' , '_doSphering' , 
'_scanInfo' , '_protoScores' ) : 
~~~ if not hasattr ( self , x ) : 
~~~ setattr ( self , x , None ) 
~~ ~~ ~~ def getParameter ( self , name , index = - 1 ) : 
if name == "patternCount" : 
~~~ return self . _knn . _numPatterns 
~~ elif name == "patternMatrix" : 
~~~ return self . _getPatternMatrix ( ) 
~~ elif name == "k" : 
~~~ return self . _knn . k 
~~ elif name == "distanceNorm" : 
~~~ return self . _knn . distanceNorm 
~~ elif name == "distanceMethod" : 
~~~ return self . _knn . distanceMethod 
~~ elif name == "distThreshold" : 
~~~ return self . _knn . distThreshold 
~~ elif name == "inputThresh" : 
~~~ return self . _knn . binarizationThreshold 
~~ elif name == "doBinarization" : 
~~~ return self . _knn . doBinarization 
~~ elif name == "useSparseMemory" : 
~~~ return self . _knn . useSparseMemory 
~~ elif name == "sparseThreshold" : 
~~~ return self . _knn . sparseThreshold 
~~ elif name == "winnerCount" : 
~~~ return self . _knn . numWinners 
~~ elif name == "relativeThreshold" : 
~~~ return self . _knn . relativeThreshold 
~~ elif name == "SVDSampleCount" : 
~~~ v = self . _knn . numSVDSamples 
return v if v is not None else 0 
~~ elif name == "SVDDimCount" : 
~~~ v = self . _knn . numSVDDims 
~~ elif name == "fractionOfMax" : 
~~~ v = self . _knn . fractionOfMax 
~~ elif name == "useAuxiliary" : 
~~~ return self . _useAuxiliary 
~~ elif name == "justUseAuxiliary" : 
~~~ return self . _justUseAuxiliary 
~~ elif name == "doSphering" : 
~~~ return self . _doSphering 
~~ elif name == "cellsPerCol" : 
~~~ return self . _knn . cellsPerCol 
~~ elif name == "maxStoredPatterns" : 
~~~ return self . maxStoredPatterns 
~~ elif name == 'categoryRecencyList' : 
~~~ return self . _knn . _categoryRecencyList 
self . _epoch = 0 
~~~ self . _epoch = 0 
if int ( value ) and not self . inferenceMode : 
~~~ self . _finishLearning ( ) 
~~ self . inferenceMode = bool ( int ( value ) ) 
~~~ self . _knn . distanceNorm = value 
~~~ self . _knn . distanceMethod = value 
~~ elif name == "keepAllDistances" : 
~~~ self . keepAllDistances = bool ( value ) 
if not self . keepAllDistances : 
~~~ if self . _protoScores is not None and self . _protoScores . shape [ 0 ] > 1 : 
~~~ self . _protoScores = self . _protoScores [ - 1 , : ] 
~~ if self . _protoScores is not None : 
~~~ self . _protoScoreCount = 1 
~~~ self . _protoScoreCount = 0 
~~ ~~ ~~ elif name == "verbosity" : 
~~~ self . verbosity = value 
self . _knn . verbosity = value 
~~ ~~ def enableTap ( self , tapPath ) : 
self . _tapFileIn = open ( tapPath + '.in' , 'w' ) 
self . _tapFileOut = open ( tapPath + '.out' , 'w' ) 
~~ def disableTap ( self ) : 
if self . _tapFileIn is not None : 
~~~ self . _tapFileIn . close ( ) 
self . _tapFileIn = None 
~~ if self . _tapFileOut is not None : 
~~~ self . _tapFileOut . close ( ) 
self . _tapFileOut = None 
~~ ~~ def handleLogInput ( self , inputs ) : 
~~~ for input in inputs : 
~~~ for k in range ( len ( input ) ) : 
~~~ print >> self . _tapFileIn , input [ k ] , 
~~ print >> self . _tapFileIn 
~~ ~~ ~~ def handleLogOutput ( self , output ) : 
if self . _tapFileOut is not None : 
~~~ for k in range ( len ( output ) ) : 
~~~ print >> self . _tapFileOut , output [ k ] , 
~~ print >> self . _tapFileOut 
~~ ~~ def _storeSample ( self , inputVector , trueCatIndex , partition = 0 ) : 
if self . _samples is None : 
~~~ self . _samples = numpy . zeros ( ( 0 , len ( inputVector ) ) , dtype = RealNumpyDType ) 
assert self . _labels is None 
self . _labels = [ ] 
~~ self . _samples = numpy . concatenate ( ( self . _samples , numpy . atleast_2d ( inputVector ) ) , axis = 0 ) 
self . _labels += [ trueCatIndex ] 
if self . _partitions is None : 
~~~ self . _partitions = [ ] 
~~ if partition is None : 
~~~ partition = 0 
~~ self . _partitions += [ partition ] 
if self . _useAuxiliary is None : 
~~~ self . _useAuxiliary = False 
~~ if self . _firstComputeCall : 
~~~ self . _firstComputeCall = False 
if self . _useAuxiliary : 
~~~ if self . _justUseAuxiliary == True : 
~~ ~~ ~~ inputVector = inputs [ 'bottomUpIn' ] 
if self . _useAuxiliary == True : 
~~~ auxVector = inputs [ 'auxDataIn' ] 
if auxVector . dtype != numpy . float32 : 
~~ if self . _justUseAuxiliary == True : 
~~~ inputVector = inputs [ 'auxDataIn' ] 
~~~ inputVector = numpy . concatenate ( [ inputVector , inputs [ 'auxDataIn' ] ] ) 
#self.handleLogInput(childInputs) 
~~ ~~ self . handleLogInput ( [ inputVector ] ) 
categories = inputs [ 'categoryIn' ] 
if "partitionIn" in inputs : 
partInput = inputs [ 'partitionIn' ] 
partition = int ( partInput [ 0 ] ) 
~~~ partition = None 
~~ if self . inferenceMode : 
~~~ categoriesOut = outputs [ 'categoriesOut' ] 
probabilitiesOut = outputs [ 'categoryProbabilitiesOut' ] 
if self . _doSphering : 
~~~ inputVector = ( inputVector + self . _normOffset ) * self . _normScale 
~~ nPrototypes = 0 
if "bestPrototypeIndices" in outputs : 
~~~ bestPrototypeIndicesOut = outputs [ "bestPrototypeIndices" ] 
nPrototypes = len ( bestPrototypeIndicesOut ) 
~~ winner , inference , protoScores , categoryDistances = self . _knn . infer ( inputVector , partitionId = partition ) 
~~~ self . _protoScores = protoScores 
~~~ if self . _protoScores is None : 
~~~ self . _protoScores = numpy . zeros ( ( 1 , protoScores . shape [ 0 ] ) , 
protoScores . dtype ) 
self . _protoScoreCount = 1 
~~~ if self . _protoScoreCount == self . _protoScores . shape [ 0 ] : 
~~~ newProtoScores = numpy . zeros ( ( self . _protoScores . shape [ 0 ] * 2 , 
self . _protoScores . shape [ 1 ] ) , 
self . _protoScores . dtype ) 
newProtoScores [ : self . _protoScores . shape [ 0 ] , : ] = self . _protoScores 
self . _protoScores = newProtoScores 
~~ self . _protoScores [ self . _protoScoreCount , : ] = protoScores 
self . _protoScoreCount += 1 
~~ ~~ self . _categoryDistances = categoryDistances 
if self . outputProbabilitiesByDist : 
~~~ scores = 1.0 - self . _categoryDistances 
~~~ scores = inference 
~~ total = scores . sum ( ) 
~~~ numScores = len ( scores ) 
probabilities = numpy . ones ( numScores ) / numScores 
~~~ probabilities = scores / total 
~~ nout = min ( len ( categoriesOut ) , len ( inference ) ) 
categoriesOut . fill ( 0 ) 
categoriesOut [ 0 : nout ] = inference [ 0 : nout ] 
probabilitiesOut . fill ( 0 ) 
probabilitiesOut [ 0 : nout ] = probabilities [ 0 : nout ] 
~~ if self . _scanInfo is not None : 
~~~ self . _scanResults = [ tuple ( inference [ : nout ] ) ] 
~~ for category in categories : 
~~~ if category >= 0 : 
~~~ dims = max ( int ( category ) + 1 , len ( inference ) ) 
oldDims = len ( self . confusion ) 
if oldDims < dims : 
~~~ confusion = numpy . zeros ( ( dims , dims ) ) 
confusion [ 0 : oldDims , 0 : oldDims ] = self . confusion 
self . confusion = confusion 
~~ self . confusion [ inference . argmax ( ) , int ( category ) ] += 1 
~~ ~~ if nPrototypes > 1 : 
~~~ bestPrototypeIndicesOut . fill ( 0 ) 
if categoryDistances is not None : 
~~~ indices = categoryDistances . argsort ( ) 
nout = min ( len ( indices ) , nPrototypes ) 
bestPrototypeIndicesOut [ 0 : nout ] = indices [ 0 : nout ] 
~~ ~~ elif nPrototypes == 1 : 
~~~ if ( categoryDistances is not None ) and len ( categoryDistances ) : 
~~~ bestPrototypeIndicesOut [ 0 ] = categoryDistances . argmin ( ) 
~~~ bestPrototypeIndicesOut [ 0 ] = 0 
~~ ~~ self . handleLogOutput ( inference ) 
~~ if self . learningMode : 
~~~ if ( self . acceptanceProbability < 1.0 ) and ( self . _rgen . getReal64 ( ) > self . acceptanceProbability ) : 
~~~ for category in categories : 
~~~ if self . _doSphering : 
~~~ self . _storeSample ( inputVector , category , partition ) 
~~~ self . _knn . learn ( inputVector , category , partition ) 
~~ ~~ ~~ ~~ ~~ self . _epoch += 1 
~~ def _finishLearning ( self ) : 
~~~ self . _finishSphering ( ) 
~~ self . _knn . finishLearning ( ) 
~~ def _finishSphering ( self ) : 
self . _normOffset = self . _samples . mean ( axis = 0 ) * - 1.0 
self . _samples += self . _normOffset 
variance = self . _samples . var ( axis = 0 ) 
variance [ numpy . where ( variance == 0.0 ) ] = 1.0 
self . _normScale = 1.0 / numpy . sqrt ( variance ) 
self . _samples *= self . _normScale 
for sampleIndex in range ( len ( self . _labels ) ) : 
~~~ self . _knn . learn ( self . _samples [ sampleIndex ] , 
self . _labels [ sampleIndex ] , 
self . _partitions [ sampleIndex ] ) 
~~ ~~ def getOutputElementCount ( self , name ) : 
if name == 'categoriesOut' : 
~~ elif name == 'categoryProbabilitiesOut' : 
~~ elif name == 'bestPrototypeIndices' : 
~~~ return self . _bestPrototypeIndexCount if self . _bestPrototypeIndexCount else 0 
~~ ~~ def generateStats ( filename , statsInfo , maxSamples = None , filters = [ ] , cache = True ) : 
if not isinstance ( statsInfo , dict ) : 
~~ filename = resource_filename ( "nupic.datafiles" , filename ) 
if cache : 
~~~ statsFilename = getStatsFilename ( filename , statsInfo , filters ) 
if os . path . exists ( statsFilename ) : 
~~~ r = pickle . load ( open ( statsFilename , "rb" ) ) 
r = dict ( ) 
~~ requestedKeys = set ( [ s for s in statsInfo ] ) 
availableKeys = set ( r . keys ( ) ) 
unavailableKeys = requestedKeys . difference ( availableKeys ) 
if len ( unavailableKeys ) == 0 : 
~~~ return r 
os . remove ( filename ) 
sensor = RecordSensor ( ) 
sensor . dataSource = FileRecordStream ( filename ) 
sensor . preEncodingFilters = filters 
stats = [ ] 
for field in statsInfo : 
~~~ if statsInfo [ field ] == "number" : 
~~~ statsInfo [ field ] = NumberStatsCollector ( ) 
~~ elif statsInfo [ field ] == "category" : 
~~~ statsInfo [ field ] = CategoryStatsCollector ( ) 
~~ ~~ if maxSamples is None : 
~~~ record = sensor . getNextRecord ( ) 
~~ for ( name , collector ) in statsInfo . items ( ) : 
~~~ collector . add ( record [ name ] ) 
~~ ~~ del sensor 
for ( field , collector ) in statsInfo . items ( ) : 
~~~ stats = collector . getStats ( ) 
if field not in r : 
~~~ r [ field ] = stats 
~~~ r [ field ] . update ( stats ) 
~~ ~~ if cache : 
~~~ f = open ( statsFilename , "wb" ) 
pickle . dump ( r , f ) 
r [ "_filename" ] = statsFilename 
~~ return r 
~~ def getScalarMetricWithTimeOfDayAnomalyParams ( metricData , 
minVal = None , 
maxVal = None , 
minResolution = None , 
tmImplementation = "cpp" ) : 
if minResolution is None : 
~~~ minResolution = 0.001 
~~ if minVal is None or maxVal is None : 
~~~ compMinVal , compMaxVal = _rangeGen ( metricData ) 
if minVal is None : 
~~~ minVal = compMinVal 
~~ if maxVal is None : 
~~~ maxVal = compMaxVal 
~~ ~~ if minVal == maxVal : 
~~~ maxVal = minVal + 1 
~~ if ( tmImplementation is "cpp" ) : 
~~~ paramFileRelativePath = os . path . join ( 
"anomaly_params_random_encoder" , 
"best_single_metric_anomaly_params_cpp.json" ) 
~~ elif ( tmImplementation is "tm_cpp" ) : 
"best_single_metric_anomaly_params_tm_cpp.json" ) 
~~ with resource_stream ( __name__ , paramFileRelativePath ) as infile : 
~~~ paramSet = json . load ( infile ) 
~~ _fixupRandomEncoderParams ( paramSet , minVal , maxVal , minResolution ) 
return paramSet 
~~ def _rangeGen ( data , std = 1 ) : 
dataStd = np . std ( data ) 
if dataStd == 0 : 
~~~ dataStd = 1 
~~ minval = np . min ( data ) - std * dataStd 
maxval = np . max ( data ) + std * dataStd 
return minval , maxval 
~~ def _fixupRandomEncoderParams ( params , minVal , maxVal , minResolution ) : 
encodersDict = ( 
params [ "modelConfig" ] [ "modelParams" ] [ "sensorParams" ] [ "encoders" ] 
for encoder in encodersDict . itervalues ( ) : 
~~~ if encoder is not None : 
~~~ if encoder [ "type" ] == "RandomDistributedScalarEncoder" : 
~~~ resolution = max ( minResolution , 
( maxVal - minVal ) / encoder . pop ( "numBuckets" ) 
encodersDict [ "c1" ] [ "resolution" ] = resolution 
tm = super ( TemporalMemoryMonitorMixin , cls ) . read ( proto ) 
tm . mmName = None 
tm . _mmTraces = None 
tm . _mmData = None 
tm . mmClearHistory ( ) 
tm . _mmResetActive = True 
return tm 
tm = super ( TMShimMixin , cls ) . read ( proto ) 
tm . infActiveState = { "t" : None } 
~~ def topDownCompute ( self , topDownIn = None ) : 
output = numpy . zeros ( self . numberOfColumns ( ) ) 
columns = [ self . columnForCell ( idx ) for idx in self . getPredictiveCells ( ) ] 
output [ columns ] = 1 
tm = super ( MonitoredTMShim , cls ) . read ( proto ) 
~~ def compute ( self , bottomUpInput , enableLearn , computeInfOutput = None ) : 
super ( MonitoredTMShim , self ) . compute ( set ( bottomUpInput . nonzero ( ) [ 0 ] ) , 
learn = enableLearn ) 
numberOfCells = self . numberOfCells ( ) 
activeState = numpy . zeros ( numberOfCells ) 
activeState [ self . getActiveCells ( ) ] = 1 
self . infActiveState [ "t" ] = activeState 
output = numpy . zeros ( numberOfCells ) 
output [ self . getPredictiveCells ( ) + self . getActiveCells ( ) ] = 1 
~~ def pickByDistribution ( distribution , r = None ) : 
if r is None : 
~~~ r = random 
~~ x = r . uniform ( 0 , sum ( distribution ) ) 
for i , d in enumerate ( distribution ) : 
~~~ if x <= d : 
~~ x -= d 
~~ ~~ def Indicator ( pos , size , dtype ) : 
x = numpy . zeros ( size , dtype = dtype ) 
x [ pos ] = 1 
~~ def MultiArgMax ( x ) : 
m = x . max ( ) 
return ( i for i , v in enumerate ( x ) if v == m ) 
~~ def Any ( sequence ) : 
return bool ( reduce ( lambda x , y : x or y , sequence , False ) ) 
~~ def All ( sequence ) : 
return bool ( reduce ( lambda x , y : x and y , sequence , True ) ) 
~~ def MultiIndicator ( pos , size , dtype ) : 
if hasattr ( pos , '__iter__' ) : 
~~~ for i in pos : x [ i ] = 1 
~~ else : x [ pos ] = 1 
~~ def Distribution ( pos , size , counts , dtype ) : 
~~~ total = 0 
for i in pos : 
~~~ total += counts [ i ] 
~~ total = float ( total ) 
~~~ x [ i ] = counts [ i ] / total 
~~ ~~ else : x [ pos ] = 1 
~~ def grow ( self , rows , cols ) : 
if not self . hist_ : 
~~~ self . hist_ = SparseMatrix ( rows , cols ) 
self . rowSums_ = numpy . zeros ( rows , dtype = dtype ) 
self . colSums_ = numpy . zeros ( cols , dtype = dtype ) 
self . hack_ = None 
~~~ oldRows = self . hist_ . nRows ( ) 
oldCols = self . hist_ . nCols ( ) 
nextRows = max ( oldRows , rows ) 
nextCols = max ( oldCols , cols ) 
if ( oldRows < nextRows ) or ( oldCols < nextCols ) : 
~~~ self . hist_ . resize ( nextRows , nextCols ) 
if oldRows < nextRows : 
~~~ oldSums = self . rowSums_ 
self . rowSums_ = numpy . zeros ( nextRows , dtype = dtype ) 
self . rowSums_ [ 0 : len ( oldSums ) ] = oldSums 
~~ if oldCols < nextCols : 
~~~ oldSums = self . colSums_ 
self . colSums_ = numpy . zeros ( nextCols , dtype = dtype ) 
self . colSums_ [ 0 : len ( oldSums ) ] = oldSums 
~~ ~~ ~~ ~~ def updateRow ( self , row , distribution ) : 
self . grow ( row + 1 , len ( distribution ) ) 
self . hist_ . axby ( row , 1 , 1 , distribution ) 
self . rowSums_ [ row ] += distribution . sum ( ) 
self . colSums_ += distribution 
~~ def inferRowCompat ( self , distribution ) : 
if self . hack_ is None : 
~~~ self . clean_outcpd ( ) 
~~ return self . hack_ . vecMaxProd ( distribution ) 
~~ def clean_outcpd ( self ) : 
m = self . hist_ . toDense ( ) 
~~~ cmax = m [ : , j ] . max ( ) 
if cmax : 
~~~ m [ : , j ] = numpy . array ( m [ : , j ] == cmax , dtype = dtype ) 
~~ ~~ self . hack_ = SparseMatrix ( 0 , self . hist_ . nCols ( ) ) 
for i in xrange ( m . shape [ 0 ] ) : 
~~~ self . hack_ . addRow ( m [ i , : ] ) 
~~ ~~ def importAndRunFunction ( 
path , 
moduleName , 
funcName , 
** keywords 
import sys 
originalPath = sys . path 
~~~ augmentedPath = [ path ] + sys . path 
sys . path = augmentedPath 
func = getattr ( __import__ ( moduleName , fromlist = [ funcName ] ) , funcName ) 
sys . path = originalPath 
~~~ sys . path = originalPath 
~~ return func ( ** keywords ) 
~~ def transferCoincidences ( network , fromElementName , toElementName ) : 
coincidenceHandle = getLockedHandle ( 
runtimeElement = network . getElement ( fromElementName ) , 
expression = "self._cd._W" 
network . getElement ( toElementName ) . setParameter ( "coincidencesAbove" , 
coincidenceHandle ) 
~~ def compute ( slidingWindow , total , newVal , windowSize ) : 
if len ( slidingWindow ) == windowSize : 
~~~ total -= slidingWindow . pop ( 0 ) 
~~ slidingWindow . append ( newVal ) 
total += newVal 
return float ( total ) / len ( slidingWindow ) , slidingWindow , total 
~~ def next ( self , newValue ) : 
newAverage , self . slidingWindow , self . total = self . compute ( 
self . slidingWindow , self . total , newValue , self . windowSize ) 
return newAverage 
~~ def getModule ( metricSpec ) : 
metricName = metricSpec . metric 
if metricName == 'rmse' : 
~~~ return MetricRMSE ( metricSpec ) 
~~ if metricName == 'nrmse' : 
~~~ return MetricNRMSE ( metricSpec ) 
~~ elif metricName == 'aae' : 
~~~ return MetricAAE ( metricSpec ) 
~~ elif metricName == 'acc' : 
~~~ return MetricAccuracy ( metricSpec ) 
~~ elif metricName == 'avg_err' : 
~~~ return MetricAveError ( metricSpec ) 
~~ elif metricName == 'trivial' : 
~~~ return MetricTrivial ( metricSpec ) 
~~ elif metricName == 'two_gram' : 
~~~ return MetricTwoGram ( metricSpec ) 
~~ elif metricName == 'moving_mean' : 
~~~ return MetricMovingMean ( metricSpec ) 
~~ elif metricName == 'moving_mode' : 
~~~ return MetricMovingMode ( metricSpec ) 
~~ elif metricName == 'neg_auc' : 
~~~ return MetricNegAUC ( metricSpec ) 
~~ elif metricName == 'custom_error_metric' : 
~~~ return CustomErrorMetric ( metricSpec ) 
~~ elif metricName == 'multiStep' : 
~~~ return MetricMultiStep ( metricSpec ) 
~~ elif metricName == 'multiStepProbability' : 
~~~ return MetricMultiStepProbability ( metricSpec ) 
~~ elif metricName == 'ms_aae' : 
~~~ return MetricMultiStepAAE ( metricSpec ) 
~~ elif metricName == 'ms_avg_err' : 
~~~ return MetricMultiStepAveError ( metricSpec ) 
~~ elif metricName == 'passThruPrediction' : 
~~~ return MetricPassThruPrediction ( metricSpec ) 
~~ elif metricName == 'altMAPE' : 
~~~ return MetricAltMAPE ( metricSpec ) 
~~ elif metricName == 'MAPE' : 
~~~ return MetricMAPE ( metricSpec ) 
~~ elif metricName == 'multi' : 
~~~ return MetricMulti ( metricSpec ) 
~~ elif metricName == 'negativeLogLikelihood' : 
~~~ return MetricNegativeLogLikelihood ( metricSpec ) 
~~ ~~ def getLabel ( self , inferenceType = None ) : 
if inferenceType is not None : 
~~~ result . append ( InferenceType . getLabel ( inferenceType ) ) 
~~ result . append ( self . inferenceElement ) 
result . append ( self . metric ) 
params = self . params 
~~~ sortedParams = params . keys ( ) 
sortedParams . sort ( ) 
for param in sortedParams : 
~~~ if param in ( 'customFuncSource' , 'customFuncDef' , 'customExpr' ) : 
~~ value = params [ param ] 
if isinstance ( value , str ) : 
~~~ result . extend ( [ "%s=\ % ( param , value ) ] ) 
~~~ result . extend ( [ "%s=%s" % ( param , value ) ] ) 
~~ ~~ ~~ if self . field : 
~~~ result . append ( "field=%s" % ( self . field ) ) 
~~ return self . _LABEL_SEPARATOR . join ( result ) 
~~ def getInferenceTypeFromLabel ( cls , label ) : 
infType , _ , _ = label . partition ( cls . _LABEL_SEPARATOR ) 
if not InferenceType . validate ( infType ) : 
~~ return infType 
~~ def _getShiftedGroundTruth ( self , groundTruth ) : 
self . _groundTruthHistory . append ( groundTruth ) 
assert ( len ( self . _predictionSteps ) == 1 ) 
if len ( self . _groundTruthHistory ) > self . _predictionSteps [ 0 ] : 
~~~ return self . _groundTruthHistory . popleft ( ) 
~~~ if hasattr ( groundTruth , '__iter__' ) : 
~~~ return [ None ] * len ( groundTruth ) 
~~ ~~ ~~ def addInstance ( self , groundTruth , prediction , record = None , result = None ) : 
self . value = self . avg ( prediction ) 
~~ def mostLikely ( self , pred ) : 
if len ( pred ) == 1 : 
~~~ return pred . keys ( ) [ 0 ] 
~~ mostLikelyOutcome = None 
maxProbability = 0 
for prediction , probability in pred . items ( ) : 
~~~ if probability > maxProbability : 
~~~ mostLikelyOutcome = prediction 
maxProbability = probability 
~~ ~~ return mostLikelyOutcome 
~~ def expValue ( self , pred ) : 
~~ return sum ( [ x * p for x , p in pred . items ( ) ] ) 
~~ def encode ( self , inputData ) : 
output = numpy . zeros ( ( self . getWidth ( ) , ) , dtype = defaultDtype ) 
self . encodeIntoArray ( inputData , output ) 
if self . encoders is not None : 
~~~ for ( name , encoder , offset ) in self . encoders : 
~~~ subNames = encoder . getScalarNames ( parentFieldName = name ) 
if parentFieldName != '' : 
~~~ subNames = [ '%s.%s' % ( parentFieldName , name ) for name in subNames ] 
~~ names . extend ( subNames ) 
~~~ if parentFieldName != '' : 
~~~ names . append ( parentFieldName ) 
~~~ names . append ( self . name ) 
~~ def getDecoderOutputFieldTypes ( self ) : 
if hasattr ( self , '_flattenedFieldTypeList' ) and self . _flattenedFieldTypeList is not None : 
~~~ return self . _flattenedFieldTypeList 
~~ fieldTypes = [ ] 
for ( name , encoder , offset ) in self . encoders : 
~~~ subTypes = encoder . getDecoderOutputFieldTypes ( ) 
fieldTypes . extend ( subTypes ) 
~~ self . _flattenedFieldTypeList = fieldTypes 
return fieldTypes 
~~ def _getInputValue ( self , obj , fieldName ) : 
~~~ if not fieldName in obj : 
key for key in obj . keys ( ) if not key . startswith ( "_" ) 
raise ValueError ( 
fieldName , knownFields , fieldName 
~~ return obj [ fieldName ] 
~~~ return getattr ( obj , fieldName ) 
~~ ~~ def getEncoderList ( self ) : 
if hasattr ( self , '_flattenedEncoderList' ) and self . _flattenedEncoderList is not None : 
~~~ return self . _flattenedEncoderList 
~~ encoders = [ ] 
~~~ subEncoders = encoder . getEncoderList ( ) 
encoders . extend ( subEncoders ) 
~~~ encoders . append ( self ) 
~~ self . _flattenedEncoderList = encoders 
return encoders 
~~ def getScalars ( self , inputData ) : 
retVals = numpy . array ( [ ] ) 
~~~ values = encoder . getScalars ( self . _getInputValue ( inputData , name ) ) 
retVals = numpy . hstack ( ( retVals , values ) ) 
~~~ retVals = numpy . hstack ( ( retVals , inputData ) ) 
~~ return retVals 
~~ def getEncodedValues ( self , inputData ) : 
retVals = [ ] 
~~~ for name , encoders , offset in self . encoders : 
~~~ values = encoders . getEncodedValues ( self . _getInputValue ( inputData , name ) ) 
if _isSequence ( values ) : 
~~~ retVals . extend ( values ) 
~~~ retVals . append ( values ) 
~~~ if _isSequence ( inputData ) : 
~~~ retVals . extend ( inputData ) 
~~~ retVals . append ( inputData ) 
~~ ~~ return tuple ( retVals ) 
~~ def getBucketIndices ( self , inputData ) : 
~~~ values = encoder . getBucketIndices ( self . _getInputValue ( inputData , name ) ) 
retVals . extend ( values ) 
~~ def scalarsToStr ( self , scalarValues , scalarNames = None ) : 
if scalarNames is None : 
~~~ scalarNames = self . getScalarNames ( ) 
~~ desc = '' 
for ( name , value ) in zip ( scalarNames , scalarValues ) : 
~~~ desc += "%s:%.2f" % ( name , value ) 
~~ ~~ return desc 
~~ def getFieldDescription ( self , fieldName ) : 
description = self . getDescription ( ) + [ ( "end" , self . getWidth ( ) ) ] 
for i in xrange ( len ( description ) ) : 
~~~ ( name , offset ) = description [ i ] 
if ( name == fieldName ) : 
~~ ~~ if i >= len ( description ) - 1 : 
~~ return ( offset , description [ i + 1 ] [ 1 ] - offset ) 
~~ def encodedBitDescription ( self , bitOffset , formatted = False ) : 
( prevFieldName , prevFieldOffset ) = ( None , None ) 
description = self . getDescription ( ) 
if formatted : 
~~~ offset = offset + i 
if bitOffset == offset - 1 : 
~~~ prevFieldName = "separator" 
prevFieldOffset = bitOffset 
~~ ~~ if bitOffset < offset : 
~~ ( prevFieldName , prevFieldOffset ) = ( name , offset ) 
~~ width = self . getDisplayWidth ( ) if formatted else self . getWidth ( ) 
if prevFieldOffset is None or bitOffset > self . getWidth ( ) : 
~~ return ( prevFieldName , bitOffset - prevFieldOffset ) 
~~ def pprintHeader ( self , prefix = "" ) : 
print prefix , 
for i in xrange ( len ( description ) - 1 ) : 
~~~ name = description [ i ] [ 0 ] 
width = description [ i + 1 ] [ 1 ] - description [ i ] [ 1 ] 
if len ( name ) > width : 
~~~ pname = name [ 0 : width ] 
~~~ pname = name 
~~ print formatStr % pname , 
print prefix , "-" * ( self . getWidth ( ) + ( len ( description ) - 1 ) * 3 - 1 ) 
~~ def pprint ( self , output , prefix = "" ) : 
~~~ offset = description [ i ] [ 1 ] 
nextoffset = description [ i + 1 ] [ 1 ] 
fieldsDict = dict ( ) 
fieldsOrder = [ ] 
if parentFieldName == '' : 
~~~ parentName = self . name 
~~~ parentName = "%s.%s" % ( parentFieldName , self . name ) 
~~ if self . encoders is not None : 
~~~ for i in xrange ( len ( self . encoders ) ) : 
if i < len ( self . encoders ) - 1 : 
~~~ nextOffset = self . encoders [ i + 1 ] [ 2 ] 
~~~ nextOffset = self . width 
~~ fieldOutput = encoded [ offset : nextOffset ] 
( subFieldsDict , subFieldsOrder ) = encoder . decode ( fieldOutput , 
parentFieldName = parentName ) 
fieldsDict . update ( subFieldsDict ) 
fieldsOrder . extend ( subFieldsOrder ) 
~~ ~~ return ( fieldsDict , fieldsOrder ) 
~~ def decodedToStr ( self , decodeResults ) : 
( fieldsDict , fieldsOrder ) = decodeResults 
desc = '' 
for fieldName in fieldsOrder : 
~~~ ( ranges , rangesStr ) = fieldsDict [ fieldName ] 
if len ( desc ) > 0 : 
~~~ desc += "%s:" % ( fieldName ) 
~~ desc += "[%s]" % ( rangesStr ) 
~~ return desc 
if self . encoders is None : 
~~ retVals = [ ] 
bucketOffset = 0 
if encoder . encoders is not None : 
~~~ nextBucketOffset = bucketOffset + len ( encoder . encoders ) 
~~~ nextBucketOffset = bucketOffset + 1 
~~ bucketIndices = buckets [ bucketOffset : nextBucketOffset ] 
values = encoder . getBucketInfo ( bucketIndices ) 
bucketOffset = nextBucketOffset 
values = encoder . topDownCompute ( fieldOutput ) 
~~ ~~ return retVals 
~~ def closenessScores ( self , expValues , actValues , fractional = True ) : 
~~~ err = abs ( expValues [ 0 ] - actValues [ 0 ] ) 
if fractional : 
~~~ denom = max ( expValues [ 0 ] , actValues [ 0 ] ) 
if denom == 0 : 
~~~ denom = 1.0 
~~ closeness = 1.0 - float ( err ) / denom 
if closeness < 0 : 
~~~ closeness = 0 
~~~ closeness = err 
~~ scalarIdx = 0 
~~~ values = encoder . closenessScores ( expValues [ scalarIdx : ] , actValues [ scalarIdx : ] , 
fractional = fractional ) 
scalarIdx += len ( values ) 
~~ def POST ( self , name ) : 
global g_models 
data = json . loads ( web . data ( ) ) 
modelParams = data [ "modelParams" ] 
predictedFieldName = data [ "predictedFieldName" ] 
if name in g_models . keys ( ) : 
~~ model = ModelFactory . create ( modelParams ) 
model . enableInference ( { 'predictedField' : predictedFieldName } ) 
g_models [ name ] = model 
return json . dumps ( { "success" : name } ) 
data [ "timestamp" ] = datetime . datetime . strptime ( 
if name not in g_models . keys ( ) : 
~~ modelResult = g_models [ name ] . run ( data ) 
predictionNumber = modelResult . predictionNumber 
anomalyScore = modelResult . inferences [ "anomalyScore" ] 
return json . dumps ( { "predictionNumber" : predictionNumber , 
"anomalyScore" : anomalyScore } ) 
~~ def analyzeOverlaps ( activeCoincsFile , encodingsFile , dataset ) : 
lines = activeCoincsFile . readlines ( ) 
inputs = encodingsFile . readlines ( ) 
patterns = set ( [ ] ) 
encodings = set ( [ ] ) 
reUsedCoincs = [ ] 
size = int ( firstLine . pop ( 0 ) ) 
spOutput = np . zeros ( ( len ( lines ) , 40 ) ) 
inputBits = np . zeros ( ( len ( lines ) , w ) ) 
print 'w:' , w 
for x in xrange ( len ( lines ) ) : 
temp = set ( spBUout ) 
spOutput [ x ] = spBUout 
tempInput = set ( input ) 
inputBits [ x ] = input 
for m in xrange ( size ) : 
~~~ if m in tempInput : 
~~~ inputSpace . append ( m ) 
if len ( reUsed ) == 0 : 
~~~ reUsedCoincs . append ( ( count , temp , repeatedBits , inputSpace , tempInput ) ) 
encodings = encodings . union ( tempInput ) 
~~ overlap = { } 
overlapVal = 0 
seen = [ ] 
seen = ( printOverlaps ( coincs , coincs , seen ) ) 
seen = printOverlaps ( reUsedCoincs , coincs , seen ) 
Summ = [ ] 
for z in coincs : 
~~~ c = 0 
for y in reUsedCoincs : 
~~~ c += len ( z [ 1 ] . intersection ( y [ 1 ] ) ) 
~~ Summ . append ( c ) 
for m in xrange ( 3 ) : 
~~~ displayLimit = min ( 51 , len ( spOutput [ m * 200 : ] ) ) 
if displayLimit > 0 : 
~~~ drawFile ( dataset , np . zeros ( [ len ( inputBits [ : ( m + 1 ) * displayLimit ] ) , len ( inputBits [ : ( m + 1 ) * displayLimit ] ) ] ) , inputBits [ : ( m + 1 ) * displayLimit ] , spOutput [ : ( m + 1 ) * displayLimit ] , w , m + 1 ) 
~~ ~~ pyl . show ( ) 
~~ def drawFile ( dataset , matrix , patterns , cells , w , fnum ) : 
assert len ( patterns ) == len ( cells ) 
for p in xrange ( len ( patterns ) - 1 ) : 
~~~ matrix [ p + 1 : , p ] = [ len ( set ( patterns [ p ] ) . intersection ( set ( q ) ) ) * 100 / w for q in patterns [ p + 1 : ] ] 
matrix [ p , p + 1 : ] = [ len ( set ( cells [ p ] ) . intersection ( set ( r ) ) ) * 5 / 2 for r in cells [ p + 1 : ] ] 
score += sum ( abs ( np . array ( matrix [ p + 1 : , p ] ) - np . array ( matrix [ p , p + 1 : ] ) ) ) 
count += len ( matrix [ p + 1 : , p ] ) 
~~ print 'Score' , score / count 
fig = pyl . figure ( figsize = ( 10 , 10 ) , num = fnum ) 
pyl . matshow ( matrix , fignum = fnum ) 
pyl . colorbar ( ) 
~~ def printOverlaps ( comparedTo , coincs , seen ) : 
inputOverlap = 0 
cellOverlap = 0 
for y in comparedTo : 
~~~ closestInputs = [ ] 
closestCells = [ ] 
if len ( seen ) > 0 : 
~~~ inputOverlap = max ( [ len ( seen [ m ] [ 1 ] . intersection ( y [ 4 ] ) ) for m in xrange ( len ( seen ) ) ] ) 
cellOverlap = max ( [ len ( seen [ m ] [ 0 ] . intersection ( y [ 1 ] ) ) for m in xrange ( len ( seen ) ) ] ) 
for m in xrange ( len ( seen ) ) : 
~~~ if len ( seen [ m ] [ 1 ] . intersection ( y [ 4 ] ) ) == inputOverlap : 
~~~ closestInputs . append ( seen [ m ] [ 2 ] ) 
~~ if len ( seen [ m ] [ 0 ] . intersection ( y [ 1 ] ) ) == cellOverlap : 
~~~ closestCells . append ( seen [ m ] [ 2 ] ) 
~~ ~~ ~~ seen . append ( ( y [ 1 ] , y [ 4 ] , y [ 0 ] ) ) 
~~ return seen 
~~ def createInput ( self ) : 
self . inputArray [ 0 : ] = 0 
for i in range ( self . inputSize ) : 
~~~ self . inputArray [ i ] = random . randrange ( 2 ) 
self . sp . compute ( self . inputArray , True , self . activeArray ) 
print self . activeArray . nonzero ( ) 
~~ def addNoise ( self , noiseLevel ) : 
for _ in range ( int ( noiseLevel * self . inputSize ) ) : 
~~~ randomPosition = int ( random . random ( ) * self . inputSize ) 
if self . inputArray [ randomPosition ] == 1 : 
~~~ self . inputArray [ randomPosition ] = 0 
~~~ self . inputArray [ randomPosition ] = 1 
~~ ~~ ~~ def _labeledInput ( activeInputs , cellsPerCol = 32 ) : 
if cellsPerCol == 0 : 
~~~ cellsPerCol = 1 
~~ cols = activeInputs . size / cellsPerCol 
activeInputs = activeInputs . reshape ( cols , cellsPerCol ) 
( cols , cellIdxs ) = activeInputs . nonzero ( ) 
if len ( cols ) == 0 : 
~~~ return "NONE" 
prevCol = - 1 
for ( col , cellIdx ) in zip ( cols , cellIdxs ) : 
~~~ if col != prevCol : 
~~~ if prevCol != - 1 : 
prevCol = col 
~~ items . append ( "%d," % cellIdx ) 
~~ items . append ( "]" ) 
~~ def clear ( self ) : 
self . _Memory = None 
self . _numPatterns = 0 
self . _M = None 
self . _categoryList = [ ] 
self . _partitionIdList = [ ] 
self . _partitionIdMap = { } 
self . _finishedLearning = False 
self . _iterationIdx = - 1 
if self . maxStoredPatterns > 0 : 
self . fixedCapacity = True 
self . _categoryRecencyList = [ ] 
~~~ self . fixedCapacity = False 
~~ self . _protoSizes = None 
self . _s = None 
self . _vt = None 
self . _nc = None 
self . _mean = None 
self . _specificIndexTraining = False 
self . _nextTrainingIndices = None 
~~ def prototypeSetCategory ( self , idToCategorize , newCategory ) : 
if idToCategorize not in self . _categoryRecencyList : 
~~ recordIndex = self . _categoryRecencyList . index ( idToCategorize ) 
self . _categoryList [ recordIndex ] = newCategory 
~~ def removeIds ( self , idsToRemove ) : 
rowsToRemove = [ k for k , rowID in enumerate ( self . _categoryRecencyList ) if rowID in idsToRemove ] 
self . _removeRows ( rowsToRemove ) 
~~ def removeCategory ( self , categoryToRemove ) : 
removedRows = 0 
if self . _Memory is None : 
~~~ return removedRows 
~~ catToRemove = float ( categoryToRemove ) 
rowsToRemove = [ k for k , catID in enumerate ( self . _categoryList ) if catID == catToRemove ] 
assert catToRemove not in self . _categoryList 
~~ def _removeRows ( self , rowsToRemove ) : 
removalArray = numpy . array ( rowsToRemove ) 
self . _categoryList = numpy . delete ( numpy . array ( self . _categoryList ) , 
removalArray ) . tolist ( ) 
if self . fixedCapacity : 
~~~ self . _categoryRecencyList = numpy . delete ( 
numpy . array ( self . _categoryRecencyList ) , removalArray ) . tolist ( ) 
~~~ self . _partitionIdList . pop ( row ) 
~~ self . _rebuildPartitionIdMap ( self . _partitionIdList ) 
if self . useSparseMemory : 
~~~ for rowIndex in rowsToRemove [ : : - 1 ] : 
~~~ self . _Memory . deleteRow ( rowIndex ) 
~~~ self . _M = numpy . delete ( self . _M , removalArray , 0 ) 
~~ numRemoved = len ( rowsToRemove ) 
numRowsExpected = self . _numPatterns - numRemoved 
~~~ if self . _Memory is not None : 
~~~ assert self . _Memory . nRows ( ) == numRowsExpected 
~~~ assert self . _M . shape [ 0 ] == numRowsExpected 
~~ assert len ( self . _categoryList ) == numRowsExpected 
self . _numPatterns -= numRemoved 
return numRemoved 
~~ def learn ( self , inputPattern , inputCategory , partitionId = None , isSparse = 0 , 
rowID = None ) : 
cellsPerCol = self . cellsPerCol ) 
~~ if isSparse > 0 : 
~~~ assert all ( inputPattern [ i ] <= inputPattern [ i + 1 ] 
"representation\ ) 
~~ if rowID is None : 
~~~ rowID = self . _iterationIdx 
~~ if not self . useSparseMemory : 
if isSparse > 0 : 
~~~ denseInput = numpy . zeros ( isSparse ) 
denseInput [ inputPattern ] = 1.0 
inputPattern = denseInput 
~~ if self . _specificIndexTraining and not self . _nextTrainingIndices : 
~~~ return self . _numPatterns 
~~ if self . _Memory is None : 
~~~ inputWidth = len ( inputPattern ) 
self . _Memory = numpy . zeros ( ( 100 , inputWidth ) ) 
self . _M = self . _Memory [ : self . _numPatterns ] 
~~ addRow = True 
if self . _vt is not None : 
~~~ inputPattern = numpy . dot ( self . _vt , inputPattern - self . _mean ) 
~~ if self . distThreshold > 0 : 
~~~ dist = self . _calcDistance ( inputPattern ) 
minDist = dist . min ( ) 
addRow = ( minDist >= self . distThreshold ) 
~~ if addRow : 
if self . _numPatterns == self . _Memory . shape [ 0 ] : 
~~~ self . _doubleMemoryNumRows ( ) 
~~ if not self . _specificIndexTraining : 
~~~ self . _Memory [ self . _numPatterns ] = inputPattern 
self . _numPatterns += 1 
self . _categoryList . append ( int ( inputCategory ) ) 
~~~ vectorIndex = self . _nextTrainingIndices . pop ( 0 ) 
while vectorIndex >= self . _Memory . shape [ 0 ] : 
~~ self . _Memory [ vectorIndex ] = inputPattern 
self . _numPatterns = max ( self . _numPatterns , vectorIndex + 1 ) 
if vectorIndex >= len ( self . _categoryList ) : 
~~~ self . _categoryList += [ - 1 ] * ( vectorIndex - 
len ( self . _categoryList ) + 1 ) 
~~ self . _categoryList [ vectorIndex ] = int ( inputCategory ) 
~~ self . _M = self . _Memory [ 0 : self . _numPatterns ] 
self . _addPartitionId ( self . _numPatterns - 1 , partitionId ) 
~~~ if isSparse > 0 and ( self . _vt is not None or self . distThreshold > 0 or self . numSVDDims is not None or self . numSVDSamples > 0 or self . numWinners > 0 ) : 
isSparse = 0 
~~~ inputWidth = isSparse 
~~~ self . _Memory = NearestNeighbor ( 0 , inputWidth ) 
~~ if self . _vt is not None : 
~~ if isSparse == 0 : 
~~~ thresholdedInput = self . _sparsifyVector ( inputPattern , True ) 
if self . cellsPerCol >= 1 : 
~~~ burstingCols = thresholdedInput . reshape ( - 1 , 
self . cellsPerCol ) . min ( axis = 1 ) . nonzero ( ) [ 0 ] 
for col in burstingCols : 
~~~ thresholdedInput [ ( col * self . cellsPerCol ) + 1 : 
( col * self . cellsPerCol ) + self . cellsPerCol ] = 0 
~~ ~~ if self . _Memory . nRows ( ) > 0 : 
~~~ dist = None 
if self . replaceDuplicates : 
~~~ dist = self . _calcDistance ( thresholdedInput , distanceNorm = 1 ) 
if dist . min ( ) == 0 : 
~~~ rowIdx = dist . argmin ( ) 
self . _categoryList [ rowIdx ] = int ( inputCategory ) 
~~~ self . _categoryRecencyList [ rowIdx ] = rowID 
~~ addRow = False 
~~ ~~ if self . distThreshold > 0 : 
~~~ if dist is None or self . distanceNorm != 1 : 
~~~ dist = self . _calcDistance ( thresholdedInput ) 
~~ minDist = dist . min ( ) 
if not addRow : 
~~~ if self . fixedCapacity : 
self . _categoryRecencyList [ rowIdx ] = rowID 
~~ ~~ ~~ ~~ if addRow and self . minSparsity > 0.0 : 
~~~ if isSparse == 0 : 
~~~ sparsity = ( float ( len ( thresholdedInput . nonzero ( ) [ 0 ] ) ) / 
len ( thresholdedInput ) ) 
~~~ sparsity = float ( len ( inputPattern ) ) / isSparse 
~~ if sparsity < self . minSparsity : 
~~~ addRow = False 
~~ ~~ if addRow : 
if isSparse == 0 : 
~~~ self . _Memory . addRow ( thresholdedInput ) 
~~~ self . _Memory . addRowNZ ( inputPattern , [ 1 ] * len ( inputPattern ) ) 
~~ self . _numPatterns += 1 
~~~ self . _categoryRecencyList . append ( rowID ) 
if self . _numPatterns > self . maxStoredPatterns and self . maxStoredPatterns > 0 : 
~~~ leastRecentlyUsedPattern = numpy . argmin ( self . _categoryRecencyList ) 
self . _Memory . deleteRow ( leastRecentlyUsedPattern ) 
self . _categoryList . pop ( leastRecentlyUsedPattern ) 
self . _categoryRecencyList . pop ( leastRecentlyUsedPattern ) 
self . _numPatterns -= 1 
~~ ~~ ~~ ~~ if self . numSVDDims is not None and self . numSVDSamples > 0 and self . _numPatterns == self . numSVDSamples : 
~~~ self . computeSVD ( ) 
~~ return self . _numPatterns 
~~ def getOverlaps ( self , inputPattern ) : 
overlaps = self . _Memory . rightVecSumAtNZ ( inputPattern ) 
return ( overlaps , self . _categoryList ) 
~~ def getDistances ( self , inputPattern ) : 
dist = self . _getDistances ( inputPattern ) 
return ( dist , self . _categoryList ) 
~~ def infer ( self , inputPattern , computeScores = True , overCategories = True , 
partitionId = None ) : 
sparsity = 0.0 
if self . minSparsity > 0.0 : 
~~~ sparsity = ( float ( len ( inputPattern . nonzero ( ) [ 0 ] ) ) / 
len ( inputPattern ) ) 
~~ if len ( self . _categoryList ) == 0 or sparsity < self . minSparsity : 
~~~ winner = None 
inferenceResult = numpy . zeros ( 1 ) 
dist = numpy . ones ( 1 ) 
categoryDist = numpy . ones ( 1 ) 
~~~ maxCategoryIdx = max ( self . _categoryList ) 
inferenceResult = numpy . zeros ( maxCategoryIdx + 1 ) 
dist = self . _getDistances ( inputPattern , partitionId = partitionId ) 
validVectorCount = len ( self . _categoryList ) - self . _categoryList . count ( - 1 ) 
if self . exact : 
~~~ exactMatches = numpy . where ( dist < 0.00001 ) [ 0 ] 
if len ( exactMatches ) > 0 : 
~~~ for i in exactMatches [ : min ( self . k , validVectorCount ) ] : 
~~~ inferenceResult [ self . _categoryList [ i ] ] += 1.0 
~~~ sorted = dist . argsort ( ) 
for j in sorted [ : min ( self . k , validVectorCount ) ] : 
~~~ inferenceResult [ self . _categoryList [ j ] ] += 1.0 
~~ ~~ if inferenceResult . any ( ) : 
~~~ winner = inferenceResult . argmax ( ) 
inferenceResult /= inferenceResult . sum ( ) 
~~ categoryDist = min_score_per_category ( maxCategoryIdx , 
self . _categoryList , dist ) 
categoryDist . clip ( 0 , 1.0 , categoryDist ) 
~~ if self . verbosity >= 1 : 
~~ result = ( winner , inferenceResult , dist , categoryDist ) 
~~ def getClosest ( self , inputPattern , topKCategories = 3 ) : 
inferenceResult = numpy . zeros ( max ( self . _categoryList ) + 1 ) 
sorted = dist . argsort ( ) 
~~ winner = inferenceResult . argmax ( ) 
topNCats = [ ] 
for i in range ( topKCategories ) : 
~~~ topNCats . append ( ( self . _categoryList [ sorted [ i ] ] , dist [ sorted [ i ] ] ) ) 
~~ return winner , dist , topNCats 
~~ def closestTrainingPattern ( self , inputPattern , cat ) : 
for patIdx in sorted : 
~~~ patternCat = self . _categoryList [ patIdx ] 
if patternCat == cat : 
~~~ if self . useSparseMemory : 
~~~ closestPattern = self . _Memory . getRow ( int ( patIdx ) ) 
~~~ closestPattern = self . _M [ patIdx ] 
~~ return closestPattern 
~~ def getPattern ( self , idx , sparseBinaryForm = False , cat = None ) : 
if cat is not None : 
~~~ assert idx is None 
idx = self . _categoryList . index ( cat ) 
~~~ pattern = self . _Memory [ idx ] 
if sparseBinaryForm : 
~~~ pattern = pattern . nonzero ( ) [ 0 ] 
~~~ ( nz , values ) = self . _Memory . rowNonZeros ( idx ) 
if not sparseBinaryForm : 
~~~ pattern = numpy . zeros ( self . _Memory . nCols ( ) ) 
numpy . put ( pattern , nz , 1 ) 
~~~ pattern = nz 
~~ ~~ return pattern 
~~ def getPartitionId ( self , i ) : 
if ( i < 0 ) or ( i >= self . _numPatterns ) : 
~~ partitionId = self . _partitionIdList [ i ] 
if partitionId == numpy . inf : 
~~~ return partitionId 
~~ ~~ def _addPartitionId ( self , index , partitionId = None ) : 
if partitionId is None : 
~~~ self . _partitionIdList . append ( numpy . inf ) 
~~~ self . _partitionIdList . append ( partitionId ) 
indices = self . _partitionIdMap . get ( partitionId , [ ] ) 
indices . append ( index ) 
self . _partitionIdMap [ partitionId ] = indices 
~~ ~~ def _rebuildPartitionIdMap ( self , partitionIdList ) : 
for row , partitionId in enumerate ( partitionIdList ) : 
~~~ indices = self . _partitionIdMap . get ( partitionId , [ ] ) 
indices . append ( row ) 
~~ ~~ def _calcDistance ( self , inputPattern , distanceNorm = None ) : 
if distanceNorm is None : 
~~~ distanceNorm = self . distanceNorm 
~~ if self . useSparseMemory : 
~~~ if self . _protoSizes is None : 
~~~ self . _protoSizes = self . _Memory . rowSums ( ) 
~~ overlapsWithProtos = self . _Memory . rightVecSumAtNZ ( inputPattern ) 
inputPatternSum = inputPattern . sum ( ) 
if self . distanceMethod == "rawOverlap" : 
~~~ dist = inputPattern . sum ( ) - overlapsWithProtos 
~~ elif self . distanceMethod == "pctOverlapOfInput" : 
~~~ dist = inputPatternSum - overlapsWithProtos 
if inputPatternSum > 0 : 
~~~ dist /= inputPatternSum 
~~ ~~ elif self . distanceMethod == "pctOverlapOfProto" : 
~~~ overlapsWithProtos /= self . _protoSizes 
dist = 1.0 - overlapsWithProtos 
~~ elif self . distanceMethod == "pctOverlapOfLarger" : 
~~~ maxVal = numpy . maximum ( self . _protoSizes , inputPatternSum ) 
if maxVal . all ( ) > 0 : 
~~~ overlapsWithProtos /= maxVal 
~~ dist = 1.0 - overlapsWithProtos 
~~ elif self . distanceMethod == "norm" : 
~~~ dist = self . _Memory . vecLpDist ( self . distanceNorm , inputPattern ) 
distMax = dist . max ( ) 
if distMax > 0 : 
~~~ dist /= distMax 
self . distanceMethod ) 
~~~ if self . distanceMethod == "norm" : 
~~~ dist = numpy . power ( numpy . abs ( self . _M - inputPattern ) , self . distanceNorm ) 
dist = dist . sum ( 1 ) 
dist = numpy . power ( dist , 1.0 / self . distanceNorm ) 
dist /= dist . max ( ) 
~~ ~~ return dist 
~~ def _getDistances ( self , inputPattern , partitionId = None ) : 
if not self . _finishedLearning : 
~~~ self . finishLearning ( ) 
self . _finishedLearning = True 
~~ if self . _vt is not None and len ( self . _vt ) > 0 : 
~~ sparseInput = self . _sparsifyVector ( inputPattern ) 
dist = self . _calcDistance ( sparseInput ) 
if self . _specificIndexTraining : 
~~~ dist [ numpy . array ( self . _categoryList ) == - 1 ] = numpy . inf 
~~ if partitionId is not None : 
~~~ dist [ self . _partitionIdMap . get ( partitionId , [ ] ) ] = numpy . inf 
~~ return dist 
~~ def computeSVD ( self , numSVDSamples = 0 , finalize = True ) : 
if numSVDSamples == 0 : 
~~~ numSVDSamples = self . _numPatterns 
~~~ self . _a = self . _Memory [ : self . _numPatterns ] 
~~~ self . _a = self . _Memory . toDense ( ) [ : self . _numPatterns ] 
~~ self . _mean = numpy . mean ( self . _a , axis = 0 ) 
self . _a -= self . _mean 
u , self . _s , self . _vt = numpy . linalg . svd ( self . _a [ : numSVDSamples ] ) 
if finalize : 
~~~ self . _finalizeSVD ( ) 
~~ return self . _s 
~~ def getAdaptiveSVDDims ( self , singularValues , fractionOfMax = 0.001 ) : 
v = singularValues / singularValues [ 0 ] 
idx = numpy . where ( v < fractionOfMax ) [ 0 ] 
if len ( idx ) : 
return idx [ 0 ] 
return len ( v ) - 1 
~~ ~~ def _finalizeSVD ( self , numSVDDims = None ) : 
if numSVDDims is not None : 
~~~ self . numSVDDims = numSVDDims 
~~ if self . numSVDDims == "adaptive" : 
~~~ if self . fractionOfMax is not None : 
~~~ self . numSVDDims = self . getAdaptiveSVDDims ( self . _s , self . fractionOfMax ) 
~~~ self . numSVDDims = self . getAdaptiveSVDDims ( self . _s ) 
~~ ~~ if self . _vt . shape [ 0 ] < self . numSVDDims : 
~~~ print "******************************************************************" 
print "******************************************************************" 
self . numSVDDims = self . _vt . shape [ 0 ] 
~~ self . _vt = self . _vt [ : self . numSVDDims ] 
if len ( self . _vt ) == 0 : 
~~ self . _Memory = numpy . zeros ( ( self . _numPatterns , self . numSVDDims ) ) 
self . _M = self . _Memory 
self . useSparseMemory = False 
for i in range ( self . _numPatterns ) : 
~~~ self . _Memory [ i ] = numpy . dot ( self . _vt , self . _a [ i ] ) 
~~ self . _a = None 
~~ def remapCategories ( self , mapping ) : 
categoryArray = numpy . array ( self . _categoryList ) 
newCategoryArray = numpy . zeros ( categoryArray . shape [ 0 ] ) 
newCategoryArray . fill ( - 1 ) 
for i in xrange ( len ( mapping ) ) : 
~~~ newCategoryArray [ categoryArray == i ] = mapping [ i ] 
~~ self . _categoryList = list ( newCategoryArray ) 
~~ def setCategoryOfVectors ( self , vectorIndices , categoryIndices ) : 
if not hasattr ( vectorIndices , "__iter__" ) : 
~~~ vectorIndices = [ vectorIndices ] 
categoryIndices = [ categoryIndices ] 
~~ elif not hasattr ( categoryIndices , "__iter__" ) : 
~~~ categoryIndices = [ categoryIndices ] * len ( vectorIndices ) 
~~ for i in xrange ( len ( vectorIndices ) ) : 
~~~ vectorIndex = vectorIndices [ i ] 
categoryIndex = categoryIndices [ i ] 
if vectorIndex < len ( self . _categoryList ) : 
~~~ self . _categoryList [ vectorIndex ] = categoryIndex 
~~ ~~ ~~ def getNextRecord ( self ) : 
allFiltersHaveEnoughData = False 
while not allFiltersHaveEnoughData : 
~~~ data = self . dataSource . getNextRecordDict ( ) 
if not data : 
~~ if "_reset" not in data : 
~~~ data [ "_reset" ] = 0 
~~ if "_sequenceId" not in data : 
~~~ data [ "_sequenceId" ] = 0 
~~ if "_category" not in data : 
~~~ data [ "_category" ] = [ None ] 
~~ data , allFiltersHaveEnoughData = self . applyFilters ( data ) 
~~ self . lastRecord = data 
~~ def applyFilters ( self , data ) : 
if self . verbosity > 0 : 
~~ allFiltersHaveEnoughData = True 
if len ( self . preEncodingFilters ) > 0 : 
~~~ originalReset = data [ '_reset' ] 
actualReset = originalReset 
for f in self . preEncodingFilters : 
~~~ filterHasEnoughData = f . process ( data ) 
allFiltersHaveEnoughData = ( allFiltersHaveEnoughData 
and filterHasEnoughData ) 
actualReset = actualReset or data [ '_reset' ] 
data [ '_reset' ] = originalReset 
~~ data [ '_reset' ] = actualReset 
~~ return data , allFiltersHaveEnoughData 
~~ def populateCategoriesOut ( self , categories , output ) : 
if categories [ 0 ] is None : 
~~~ output [ : ] = - 1 
~~~ for i , cat in enumerate ( categories [ : len ( output ) ] ) : 
~~~ output [ i ] = cat 
~~ output [ len ( categories ) : ] = - 1 
if not self . topDownMode : 
~~~ data = self . getNextRecord ( ) 
reset = data [ "_reset" ] 
sequenceId = data [ "_sequenceId" ] 
categories = data [ "_category" ] 
self . encoder . encodeIntoArray ( data , outputs [ "dataOut" ] ) 
if self . predictedField is not None and self . predictedField != "vector" : 
~~~ allEncoders = list ( self . encoder . encoders ) 
if self . disabledEncoder is not None : 
~~~ allEncoders . extend ( self . disabledEncoder . encoders ) 
~~ encoders = [ e for e in allEncoders 
if e [ 0 ] == self . predictedField ] 
if len ( encoders ) == 0 : 
~~~ encoder = encoders [ 0 ] [ 1 ] 
~~ actualValue = data [ self . predictedField ] 
outputs [ "bucketIdxOut" ] [ : ] = encoder . getBucketIndices ( actualValue ) 
if isinstance ( actualValue , str ) : 
~~~ outputs [ "actValueOut" ] [ : ] = encoder . getBucketIndices ( actualValue ) 
~~~ outputs [ "actValueOut" ] [ : ] = actualValue 
~~ ~~ outputs [ "sourceOut" ] [ : ] = self . encoder . getScalars ( data ) 
self . _outputValues [ "sourceOut" ] = self . encoder . getEncodedValues ( data ) 
encoders = self . encoder . getEncoderList ( ) 
prevOffset = 0 
sourceEncodings = [ ] 
bitData = outputs [ "dataOut" ] 
for encoder in encoders : 
~~~ nextOffset = prevOffset + encoder . getWidth ( ) 
sourceEncodings . append ( bitData [ prevOffset : nextOffset ] ) 
prevOffset = nextOffset 
~~ self . _outputValues [ 'sourceEncodings' ] = sourceEncodings 
for filter in self . postEncodingFilters : 
~~~ filter . process ( encoder = self . encoder , data = outputs [ 'dataOut' ] ) 
~~ outputs [ 'resetOut' ] [ 0 ] = reset 
outputs [ 'sequenceIdOut' ] [ 0 ] = sequenceId 
self . populateCategoriesOut ( categories , outputs [ 'categoryOut' ] ) 
~~~ if self . _iterNum == 0 : 
~~~ self . encoder . pprintHeader ( prefix = "sensor:" ) 
~~ if reset : 
~~ if self . verbosity >= 2 : 
~~~ print 
~~ ~~ if self . verbosity >= 1 : 
~~~ self . encoder . pprint ( outputs [ "dataOut" ] , prefix = "%7d:" % ( self . _iterNum ) ) 
scalarValues = self . encoder . getScalars ( data ) 
nz = outputs [ "dataOut" ] . nonzero ( ) [ 0 ] 
~~ if self . verbosity >= 3 : 
~~~ decoded = self . encoder . decode ( outputs [ "dataOut" ] ) 
print "decoded:" , self . encoder . decodedToStr ( decoded ) 
~~ self . _iterNum += 1 
~~~ spatialTopDownIn = inputs [ 'spatialTopDownIn' ] 
spatialTopDownOut = self . encoder . topDownCompute ( spatialTopDownIn ) 
values = [ elem . value for elem in spatialTopDownOut ] 
scalars = [ elem . scalar for elem in spatialTopDownOut ] 
encodings = [ elem . encoding for elem in spatialTopDownOut ] 
self . _outputValues [ 'spatialTopDownOut' ] = values 
outputs [ 'spatialTopDownOut' ] [ : ] = numpy . array ( scalars ) 
self . _outputValues [ 'spatialTopDownEncodings' ] = encodings 
temporalTopDownIn = inputs [ 'temporalTopDownIn' ] 
temporalTopDownOut = self . encoder . topDownCompute ( temporalTopDownIn ) 
values = [ elem . value for elem in temporalTopDownOut ] 
scalars = [ elem . scalar for elem in temporalTopDownOut ] 
encodings = [ elem . encoding for elem in temporalTopDownOut ] 
self . _outputValues [ 'temporalTopDownOut' ] = values 
outputs [ 'temporalTopDownOut' ] [ : ] = numpy . array ( scalars ) 
self . _outputValues [ 'temporalTopDownEncodings' ] = encodings 
assert len ( spatialTopDownOut ) == len ( temporalTopDownOut ) , ( 
"size" ) 
~~ ~~ def _convertNonNumericData ( self , spatialOutput , temporalOutput , output ) : 
types = self . encoder . getDecoderOutputFieldTypes ( ) 
for i , ( encoder , type ) in enumerate ( zip ( encoders , types ) ) : 
~~~ spatialData = spatialOutput [ i ] 
temporalData = temporalOutput [ i ] 
if type != FieldMetaType . integer and type != FieldMetaType . float : 
~~~ spatialData = encoder . getScalars ( spatialData ) [ 0 ] 
temporalData = encoder . getScalars ( temporalData ) [ 0 ] 
~~ assert isinstance ( spatialData , ( float , int ) ) 
assert isinstance ( temporalData , ( float , int ) ) 
output [ 'spatialTopDownOut' ] [ i ] = spatialData 
output [ 'temporalTopDownOut' ] [ i ] = temporalData 
if name == "resetOut" : 
"resetOut" ) 
return 1 
~~ elif name == "sequenceIdOut" : 
"sequenceIdOut" ) 
~~ elif name == "dataOut" : 
~~~ if self . encoder is None : 
~~ return self . encoder . getWidth ( ) 
~~ elif name == "sourceOut" : 
~~ return len ( self . encoder . getDescription ( ) ) 
~~ elif name == "bucketIdxOut" : 
~~~ return 1 
~~ elif name == "actValueOut" : 
~~ elif name == "categoryOut" : 
~~~ return self . numCategories 
~~ elif name == 'spatialTopDownOut' or name == 'temporalTopDownOut' : 
if parameterName == 'topDownMode' : 
~~~ self . topDownMode = parameterValue 
~~ elif parameterName == 'predictedField' : 
~~~ self . predictedField = parameterValue 
self . encoder . write ( proto . encoder ) 
~~~ self . disabledEncoder . write ( proto . disabledEncoder ) 
~~ proto . topDownMode = int ( self . topDownMode ) 
proto . numCategories = self . numCategories 
instance . encoder = MultiEncoder . read ( proto . encoder ) 
if proto . disabledEncoder is not None : 
~~~ instance . disabledEncoder = MultiEncoder . read ( proto . disabledEncoder ) 
~~ instance . topDownMode = bool ( proto . topDownMode ) 
instance . numCategories = proto . numCategories 
~~ def computeAccuracy ( model , size , top ) : 
accuracy = [ ] 
filename = os . path . join ( os . path . dirname ( __file__ ) , "msnbc990928.zip" ) 
with zipfile . ZipFile ( filename ) as archive : 
~~~ with archive . open ( "msnbc990928.seq" ) as datafile : 
~~~ for _ in xrange ( 7 ) : 
~~~ next ( datafile ) 
~~ for _ in xrange ( LEARNING_RECORDS ) : 
~~ for _ in xrange ( size ) : 
~~~ pages = readUserSession ( datafile ) 
model . resetSequenceStates ( ) 
for i in xrange ( len ( pages ) - 1 ) : 
~~~ result = model . run ( { "page" : pages [ i ] } ) 
inferences = result . inferences [ "multiStepPredictions" ] [ 1 ] 
predicted = sorted ( inferences . items ( ) , key = itemgetter ( 1 ) , reverse = True ) [ : top ] 
accuracy . append ( 1 if pages [ i + 1 ] in zip ( * predicted ) [ 0 ] else 0 ) 
~~ ~~ ~~ ~~ return np . mean ( accuracy ) 
~~ def readUserSession ( datafile ) : 
for line in datafile : 
~~~ pages = line . split ( ) 
total = len ( pages ) 
if total < 2 : 
~~ if total > 500 : 
~~ return [ PAGE_CATEGORIES [ int ( i ) - 1 ] for i in pages ] 
~~ def rewind ( self ) : 
super ( FileRecordStream , self ) . rewind ( ) 
self . close ( ) 
self . _file = open ( self . _filename , self . _mode ) 
self . _reader = csv . reader ( self . _file , dialect = "excel" ) 
self . _reader . next ( ) 
self . _recordCount = 0 
~~ def getNextRecord ( self , useCache = True ) : 
assert self . _file is not None 
assert self . _mode == self . _FILE_READ_MODE 
~~~ line = self . _reader . next ( ) 
~~~ if self . rewindAtEOF : 
~~~ if self . _recordCount == 0 : 
"\ % self . _filename ) 
~~ self . rewind ( ) 
line = self . _reader . next ( ) 
~~ ~~ self . _recordCount += 1 
record = [ ] 
for i , f in enumerate ( line ) : 
#sys.stdout.flush() 
~~~ if f in self . _missingValues : 
~~~ record . append ( SENTINEL_VALUE_FOR_MISSING_DATA ) 
~~~ record . append ( self . _adapters [ i ] ( f ) ) 
~~ ~~ return record 
~~ def appendRecord ( self , record ) : 
assert self . _mode == self . _FILE_WRITE_MODE 
if self . _recordCount == 0 : 
~~~ names , types , specials = zip ( * self . getFields ( ) ) 
for line in names , types , specials : 
~~~ self . _writer . writerow ( line ) 
~~ ~~ self . _updateSequenceInfo ( record ) 
line = [ self . _adapters [ i ] ( f ) for i , f in enumerate ( record ) ] 
self . _writer . writerow ( line ) 
self . _recordCount += 1 
~~ def appendRecords ( self , records , progressCB = None ) : 
~~~ self . appendRecord ( record ) 
if progressCB is not None : 
~~~ progressCB ( ) 
~~ ~~ ~~ def getBookmark ( self ) : 
if self . _write and self . _recordCount == 0 : 
~~ rowDict = dict ( filepath = os . path . realpath ( self . _filename ) , 
currentRow = self . _recordCount ) 
return json . dumps ( rowDict ) 
~~ def seekFromEnd ( self , numRecords ) : 
self . _file . seek ( self . _getTotalLineCount ( ) - numRecords ) 
return self . getBookmark ( ) 
~~ def getStats ( self ) : 
if self . _stats == None : 
~~~ assert self . _mode == self . _FILE_READ_MODE 
inFile = open ( self . _filename , self . _FILE_READ_MODE ) 
reader = csv . reader ( inFile , dialect = "excel" ) 
names = [ n . strip ( ) for n in reader . next ( ) ] 
types = [ t . strip ( ) for t in reader . next ( ) ] 
reader . next ( ) 
self . _stats = dict ( ) 
self . _stats [ 'min' ] = [ ] 
self . _stats [ 'max' ] = [ ] 
for i in xrange ( len ( names ) ) : 
~~~ self . _stats [ 'min' ] . append ( None ) 
self . _stats [ 'max' ] . append ( None ) 
~~~ line = reader . next ( ) 
~~~ if ( len ( types ) > i and 
types [ i ] in [ FieldMetaType . integer , FieldMetaType . float ] and 
f not in self . _missingValues ) : 
~~~ value = self . _adapters [ i ] ( f ) 
if self . _stats [ 'max' ] [ i ] == None or self . _stats [ 'max' ] [ i ] < value : 
~~~ self . _stats [ 'max' ] [ i ] = value 
~~ if self . _stats [ 'min' ] [ i ] == None or self . _stats [ 'min' ] [ i ] > value : 
~~~ self . _stats [ 'min' ] [ i ] = value 
~~ ~~ ~~ ~~ except StopIteration : 
~~ ~~ ~~ return self . _stats 
~~ def _updateSequenceInfo ( self , r ) : 
newSequence = False 
sequenceId = ( r [ self . _sequenceIdIdx ] 
if self . _sequenceIdIdx is not None else None ) 
if sequenceId != self . _currSequence : 
~~~ if sequenceId in self . _sequences : 
~~ self . _sequences . add ( self . _currSequence ) 
self . _currSequence = sequenceId 
if self . _resetIdx : 
~~~ assert r [ self . _resetIdx ] == 1 
~~ newSequence = True 
~~~ reset = False 
~~~ reset = r [ self . _resetIdx ] 
if reset == 1 : 
~~~ newSequence = True 
~~ ~~ ~~ if not newSequence : 
~~~ if self . _timeStampIdx and self . _currTime is not None : 
~~~ t = r [ self . _timeStampIdx ] 
if t < self . _currTime : 
~~ ~~ ~~ if self . _timeStampIdx : 
~~~ self . _currTime = r [ self . _timeStampIdx ] 
~~ ~~ def _getStartRow ( self , bookmark ) : 
bookMarkDict = json . loads ( bookmark ) 
realpath = os . path . realpath ( self . _filename ) 
bookMarkFile = bookMarkDict . get ( 'filepath' , None ) 
if bookMarkFile != realpath : 
realpath , bookMarkDict ) 
return 0 
~~~ return bookMarkDict [ 'currentRow' ] 
~~ ~~ def _getTotalLineCount ( self ) : 
if self . _mode == self . _FILE_WRITE_MODE : 
~~~ self . _file . flush ( ) 
~~ return sum ( 1 for line in open ( self . _filename , self . _FILE_READ_MODE ) ) 
numLines = self . _getTotalLineCount ( ) 
if numLines == 0 : 
~~~ assert self . _mode == self . _FILE_WRITE_MODE and self . _recordCount == 0 
numDataRows = 0 
~~~ numDataRows = numLines - self . _NUM_HEADER_ROWS 
~~ assert numDataRows >= 0 
return numDataRows 
~~ def runIoThroughNupic ( inputData , model , gymName , plot ) : 
inputFile = open ( inputData , "rb" ) 
csvReader = csv . reader ( inputFile ) 
csvReader . next ( ) 
shifter = InferenceShifter ( ) 
if plot : 
~~~ output = nupic_anomaly_output . NuPICPlotOutput ( gymName ) 
~~~ output = nupic_anomaly_output . NuPICFileOutput ( gymName ) 
~~ counter = 0 
for row in csvReader : 
if ( counter % 100 == 0 ) : 
~~ timestamp = datetime . datetime . strptime ( row [ 0 ] , DATE_FORMAT ) 
consumption = float ( row [ 1 ] ) 
result = model . run ( { 
"timestamp" : timestamp , 
"kw_energy_consumption" : consumption 
~~~ result = shifter . shift ( result ) 
~~ prediction = result . inferences [ "multiStepBestPredictions" ] [ 1 ] 
anomalyScore = result . inferences [ "anomalyScore" ] 
output . write ( timestamp , consumption , prediction , anomalyScore ) 
~~ inputFile . close ( ) 
output . close ( ) 
if self . _prevAbsolute == None or self . _prevDelta == None : 
~~ ret = self . _adaptiveScalarEnc . topDownCompute ( encoded ) 
if self . _prevAbsolute != None : 
~~~ ret = [ EncoderResult ( value = ret [ 0 ] . value + self . _prevAbsolute , 
scalar = ret [ 0 ] . scalar + self . _prevAbsolute , 
encoding = ret [ 0 ] . encoding ) ] 
~~ def isTemporal ( inferenceElement ) : 
if InferenceElement . __temporalInferenceElements is None : 
~~~ InferenceElement . __temporalInferenceElements = set ( [ InferenceElement . prediction ] ) 
~~ return inferenceElement in InferenceElement . __temporalInferenceElements 
~~ def getTemporalDelay ( inferenceElement , key = None ) : 
if inferenceElement in ( InferenceElement . prediction , 
InferenceElement . encodings ) : 
~~ if inferenceElement in ( InferenceElement . anomalyScore , 
InferenceElement . anomalyLabel , 
InferenceElement . classification , 
InferenceElement . classConfidences ) : 
~~ if inferenceElement in ( InferenceElement . multiStepPredictions , 
InferenceElement . multiStepBestPredictions ) : 
~~~ return int ( key ) 
~~ return 0 
~~ def getMaxDelay ( inferences ) : 
maxDelay = 0 
for inferenceElement , inference in inferences . iteritems ( ) : 
~~~ for key in inference . iterkeys ( ) : 
~~~ maxDelay = max ( InferenceElement . getTemporalDelay ( inferenceElement , 
key ) , 
maxDelay ) 
~~~ maxDelay = max ( InferenceElement . getTemporalDelay ( inferenceElement ) , 
~~ ~~ return maxDelay 
~~ def isTemporal ( inferenceType ) : 
if InferenceType . __temporalInferenceTypes is None : 
~~~ InferenceType . __temporalInferenceTypes = set ( [ InferenceType . TemporalNextStep , 
InferenceType . TemporalClassification , 
InferenceType . NontemporalMultiStep ] ) 
~~ return inferenceType in InferenceType . __temporalInferenceTypes 
~~ def Enum ( * args , ** kwargs ) : 
def getLabel ( cls , val ) : 
return cls . __labels [ val ] 
~~ def validate ( cls , val ) : 
return val in cls . __values 
~~ def getValues ( cls ) : 
return list ( cls . __values ) 
~~ def getLabels ( cls ) : 
return list ( cls . __labels . values ( ) ) 
~~ def getValue ( cls , label ) : 
return cls . __labels [ label ] 
~~ for arg in list ( args ) + kwargs . keys ( ) : 
~~~ if type ( arg ) is not str : 
~~ if not __isidentifier ( arg ) : 
~~ ~~ kwargs . update ( zip ( args , args ) ) 
newType = type ( "Enum" , ( object , ) , kwargs ) 
newType . __labels = dict ( ( v , k ) for k , v in kwargs . iteritems ( ) ) 
newType . __values = set ( newType . __labels . keys ( ) ) 
newType . getLabel = functools . partial ( getLabel , newType ) 
newType . validate = functools . partial ( validate , newType ) 
newType . getValues = functools . partial ( getValues , newType ) 
newType . getLabels = functools . partial ( getLabels , newType ) 
newType . getValue = functools . partial ( getValue , newType ) 
return newType 
~~ def makeDirectoryFromAbsolutePath ( absDirPath ) : 
assert os . path . isabs ( absDirPath ) 
~~~ os . makedirs ( absDirPath ) 
~~ except OSError , e : 
~~~ if e . errno != os . errno . EEXIST : 
~~ ~~ return absDirPath 
~~ def _readConfigFile ( cls , filename , path = None ) : 
outputProperties = dict ( ) 
~~~ filePath = cls . findConfigFile ( filename ) 
~~~ filePath = os . path . join ( path , filename ) 
~~~ if filePath is not None : 
with open ( filePath , 'r' ) as inp : 
~~~ contents = inp . read ( ) 
~~~ contents = resource_string ( "nupic.support" , filename ) 
~~ except Exception as resourceException : 
~~~ if filename in [ USER_CONFIG , CUSTOM_CONFIG ] : 
~~~ contents = '<configuration/>' 
~~~ raise resourceException 
~~ ~~ ~~ elements = ElementTree . XML ( contents ) 
if elements . tag != 'configuration' : 
~~ propertyElements = elements . findall ( './property' ) 
for propertyItem in propertyElements : 
~~~ propInfo = dict ( ) 
propertyAttributes = list ( propertyItem ) 
for propertyAttribute in propertyAttributes : 
~~~ propInfo [ propertyAttribute . tag ] = propertyAttribute . text 
~~ name = propInfo . get ( 'name' , None ) 
if 'value' in propInfo and propInfo [ 'value' ] is None : 
~~~ value = '' 
~~~ value = propInfo . get ( 'value' , None ) 
~~~ if 'novalue' in propInfo : 
~~ ~~ ~~ restOfValue = value 
value = '' 
~~~ pos = restOfValue . find ( '${env.' ) 
if pos == - 1 : 
~~~ value += restOfValue 
~~ value += restOfValue [ 0 : pos ] 
varTailPos = restOfValue . find ( '}' , pos ) 
if varTailPos == - 1 : 
~~ varname = restOfValue [ pos + 6 : varTailPos ] 
if varname not in os . environ : 
varname ) ) 
~~ envVarValue = os . environ [ varname ] 
value += envVarValue 
restOfValue = restOfValue [ varTailPos + 1 : ] 
~~ propInfo [ 'value' ] = value 
outputProperties [ name ] = propInfo 
~~ return outputProperties 
filePath ) 
~~ ~~ def setCustomProperties ( cls , properties ) : 
properties , traceback . format_stack ( ) ) 
_CustomConfigurationFileWrapper . edit ( properties ) 
for propertyName , value in properties . iteritems ( ) : 
~~~ cls . set ( propertyName , value ) 
~~ ~~ def clear ( cls ) : 
super ( Configuration , cls ) . clear ( ) 
_CustomConfigurationFileWrapper . clear ( persistent = False ) 
~~ def resetCustomConfig ( cls ) : 
"caller=%r" , traceback . format_stack ( ) ) 
_CustomConfigurationFileWrapper . clear ( persistent = True ) 
~~ def clear ( cls , persistent = False ) : 
if persistent : 
~~~ os . unlink ( cls . getPath ( ) ) 
~~~ if e . errno != errno . ENOENT : 
cls . getPath ( ) ) 
~~ ~~ ~~ cls . _path = None 
~~ def getCustomDict ( cls ) : 
if not os . path . exists ( cls . getPath ( ) ) : 
~~~ return dict ( ) 
~~ properties = Configuration . _readConfigFile ( os . path . basename ( 
cls . getPath ( ) ) , os . path . dirname ( cls . getPath ( ) ) ) 
values = dict ( ) 
for propName in properties : 
~~~ if 'value' in properties [ propName ] : 
~~~ values [ propName ] = properties [ propName ] [ 'value' ] 
~~ ~~ return values 
~~ def edit ( cls , properties ) : 
copyOfProperties = copy ( properties ) 
configFilePath = cls . getPath ( ) 
~~~ with open ( configFilePath , 'r' ) as fp : 
~~~ contents = fp . read ( ) 
~~ ~~ except IOError , e : 
e . errno , configFilePath , properties ) 
~~ contents = '<configuration/>' 
~~~ elements = ElementTree . XML ( contents ) 
ElementTree . tostring ( elements ) 
_getLogger ( ) . exception ( msg ) 
raise RuntimeError ( msg ) , None , sys . exc_info ( ) [ 2 ] 
~~ if elements . tag != 'configuration' : 
_getLogger ( ) . error ( e ) 
raise RuntimeError ( e ) 
~~ for propertyItem in elements . findall ( './property' ) : 
~~~ propInfo = dict ( ( attr . tag , attr . text ) for attr in propertyItem ) 
name = propInfo [ 'name' ] 
if name in copyOfProperties : 
~~~ foundValues = propertyItem . findall ( './value' ) 
if len ( foundValues ) > 0 : 
~~~ foundValues [ 0 ] . text = str ( copyOfProperties . pop ( name ) ) 
if not copyOfProperties : 
~~ ~~ ~~ for propertyName , value in copyOfProperties . iteritems ( ) : 
~~~ newProp = ElementTree . Element ( 'property' ) 
nameTag = ElementTree . Element ( 'name' ) 
nameTag . text = propertyName 
newProp . append ( nameTag ) 
valueTag = ElementTree . Element ( 'value' ) 
valueTag . text = str ( value ) 
newProp . append ( valueTag ) 
elements . append ( newProp ) 
~~~ makeDirectoryFromAbsolutePath ( os . path . dirname ( configFilePath ) ) 
with open ( configFilePath , 'w' ) as fp : 
~~~ fp . write ( ElementTree . tostring ( elements ) ) 
configFilePath ) 
~~ ~~ def _setPath ( cls ) : 
cls . _path = os . path . join ( os . environ [ 'NTA_DYNAMIC_CONF_DIR' ] , 
cls . customFileName ) 
varStates = dict ( ) 
for varName , var in self . permuteVars . iteritems ( ) : 
~~~ varStates [ varName ] = var . getState ( ) 
~~ return dict ( id = self . particleId , 
genIdx = self . genIdx , 
swarmId = self . swarmId , 
varStates = varStates ) 
~~ def initStateFrom ( self , particleId , particleState , newBest ) : 
if newBest : 
~~~ ( bestResult , bestPosition ) = self . _resultsDB . getParticleBest ( particleId ) 
~~~ bestResult = bestPosition = None 
~~ varStates = particleState [ 'varStates' ] 
for varName in varStates . keys ( ) : 
~~~ varState = copy . deepcopy ( varStates [ varName ] ) 
~~~ varState [ 'bestResult' ] = bestResult 
~~ if bestPosition is not None : 
~~~ varState [ 'bestPosition' ] = bestPosition [ varName ] 
~~ self . permuteVars [ varName ] . setState ( varState ) 
~~ ~~ def copyVarStatesFrom ( self , particleState , varNames ) : 
allowedToMove = True 
for varName in particleState [ 'varStates' ] : 
~~~ if varName in varNames : 
~~~ if varName not in self . permuteVars : 
~~ state = copy . deepcopy ( particleState [ 'varStates' ] [ varName ] ) 
state [ '_position' ] = state [ 'position' ] 
state [ 'bestPosition' ] = state [ 'position' ] 
if not allowedToMove : 
~~~ state [ 'velocity' ] = 0 
~~ self . permuteVars [ varName ] . setState ( state ) 
if allowedToMove : 
~~~ self . permuteVars [ varName ] . resetVelocity ( self . _rng ) 
~~ ~~ ~~ ~~ def getPosition ( self ) : 
result = dict ( ) 
for ( varName , value ) in self . permuteVars . iteritems ( ) : 
~~~ result [ varName ] = value . getPosition ( ) 
~~ def getPositionFromState ( pState ) : 
for ( varName , value ) in pState [ 'varStates' ] . iteritems ( ) : 
~~~ result [ varName ] = value [ 'position' ] 
for ( varName , var ) in self . permuteVars . iteritems ( ) : 
~~~ var . agitate ( ) 
~~ self . newPosition ( ) 
~~ def newPosition ( self , whichVars = None ) : 
globalBestPosition = None 
if self . _hsObj . _speculativeParticles : 
~~~ genIdx = self . genIdx 
~~~ genIdx = self . genIdx - 1 
~~ if genIdx >= 0 : 
~~~ ( bestModelId , _ ) = self . _resultsDB . bestModelIdAndErrScore ( self . swarmId , 
genIdx ) 
if bestModelId is not None : 
~~~ ( particleState , _ , _ , _ , _ ) = self . _resultsDB . getParticleInfo ( 
globalBestPosition = Particle . getPositionFromState ( particleState ) 
~~ ~~ for ( varName , var ) in self . permuteVars . iteritems ( ) : 
~~~ if whichVars is not None and varName not in whichVars : 
~~ if globalBestPosition is None : 
~~~ var . newPosition ( None , self . _rng ) 
~~~ var . newPosition ( globalBestPosition [ varName ] , self . _rng ) 
~~ ~~ position = self . getPosition ( ) 
~~~ msg = StringIO . StringIO ( ) 
~~ self . logger . debug ( msg . getvalue ( ) ) 
msg . close ( ) 
~~ return position 
~~ def propose ( self , current , r ) : 
stay = ( r . uniform ( 0 , 1 ) < self . kernel ) 
if stay : 
~~~ logKernel = numpy . log ( self . kernel ) 
return current , logKernel , logKernel 
~~~ curIndex = self . keyMap [ current ] 
ri = r . randint ( 0 , self . nKeys - 1 ) 
logKernel = numpy . log ( 1.0 - self . kernel ) 
lp = logKernel + self . logp 
if ri < curIndex : return self . keys [ ri ] , lp , lp 
else : return self . keys [ ri + 1 ] , lp , lp 
~~ ~~ def propose ( self , current , r ) : 
curLambda = current + self . offset 
x , logProb = PoissonDistribution ( curLambda ) . sample ( r ) 
logBackward = PoissonDistribution ( x + self . offset ) . logDensity ( current ) 
return x , logProb , logBackward 
~~ def __getLogger ( cls ) : 
if cls . __logger is None : 
~~~ cls . __logger = opf_utils . initLogger ( cls ) 
~~ return cls . __logger 
~~ def create ( modelConfig , logLevel = logging . ERROR ) : 
logger = ModelFactory . __getLogger ( ) 
logger . setLevel ( logLevel ) 
modelClass = None 
if modelConfig [ 'model' ] == "HTMPrediction" : 
~~~ modelClass = HTMPredictionModel 
~~ elif modelConfig [ 'model' ] == "TwoGram" : 
~~~ modelClass = TwoGramModel 
~~ elif modelConfig [ 'model' ] == "PreviousValue" : 
~~~ modelClass = PreviousValueModel 
~~ return modelClass ( ** modelConfig [ 'modelParams' ] ) 
~~ def loadFromCheckpoint ( savedModelDir , newSerialization = False ) : 
if newSerialization : 
~~~ return HTMPredictionModel . readFromCheckpoint ( savedModelDir ) 
~~~ return Model . load ( savedModelDir ) 
self . activateCells ( sorted ( activeColumns ) , learn ) 
self . activateDendrites ( learn ) 
~~ def activateCells ( self , activeColumns , learn = True ) : 
prevActiveCells = self . activeCells 
prevWinnerCells = self . winnerCells 
self . activeCells = [ ] 
self . winnerCells = [ ] 
segToCol = lambda segment : int ( segment . cell / self . cellsPerColumn ) 
identity = lambda x : x 
for columnData in groupby2 ( activeColumns , identity , 
self . activeSegments , segToCol , 
self . matchingSegments , segToCol ) : 
~~~ ( column , 
activeColumns , 
columnActiveSegments , 
columnMatchingSegments ) = columnData 
if activeColumns is not None : 
~~~ if columnActiveSegments is not None : 
~~~ cellsToAdd = self . activatePredictedColumn ( column , 
columnMatchingSegments , 
prevActiveCells , 
prevWinnerCells , 
learn ) 
self . activeCells += cellsToAdd 
self . winnerCells += cellsToAdd 
~~~ ( cellsToAdd , 
winnerCell ) = self . burstColumn ( column , 
self . winnerCells . append ( winnerCell ) 
~~~ self . punishPredictedColumn ( column , 
prevWinnerCells ) 
~~ ~~ ~~ ~~ def activateDendrites ( self , learn = True ) : 
( numActiveConnected , 
numActivePotential ) = self . connections . computeActivity ( 
self . activeCells , 
self . connectedPermanence ) 
activeSegments = ( 
self . connections . segmentForFlatIdx ( i ) 
for i in xrange ( len ( numActiveConnected ) ) 
if numActiveConnected [ i ] >= self . activationThreshold 
matchingSegments = ( 
for i in xrange ( len ( numActivePotential ) ) 
if numActivePotential [ i ] >= self . minThreshold 
self . activeSegments = sorted ( activeSegments , 
key = self . connections . segmentPositionSortKey ) 
self . matchingSegments = sorted ( matchingSegments , 
self . numActiveConnectedSynapsesForSegment = numActiveConnected 
self . numActivePotentialSynapsesForSegment = numActivePotential 
if learn : 
~~~ for segment in self . activeSegments : 
~~~ self . lastUsedIterationForSegment [ segment . flatIdx ] = self . iteration 
~~ self . iteration += 1 
~~ ~~ def reset ( self ) : 
self . activeSegments = [ ] 
self . matchingSegments = [ ] 
~~ def activatePredictedColumn ( self , column , columnActiveSegments , 
columnMatchingSegments , prevActiveCells , 
prevWinnerCells , learn ) : 
return self . _activatePredictedColumn ( 
self . connections , self . _random , 
columnActiveSegments , prevActiveCells , prevWinnerCells , 
self . numActivePotentialSynapsesForSegment , 
self . maxNewSynapseCount , self . initialPermanence , 
self . permanenceIncrement , self . permanenceDecrement , 
self . maxSynapsesPerSegment , learn ) 
~~ def punishPredictedColumn ( self , column , columnActiveSegments , 
prevWinnerCells ) : 
self . _punishPredictedColumn ( 
self . connections , columnMatchingSegments , prevActiveCells , 
self . predictedSegmentDecrement ) 
return self . _createSegment ( 
self . connections , self . lastUsedIterationForSegment , cell , self . iteration , 
self . maxSegmentsPerCell ) 
~~ def _activatePredictedColumn ( cls , connections , random , columnActiveSegments , 
prevActiveCells , prevWinnerCells , 
numActivePotentialSynapsesForSegment , 
maxNewSynapseCount , initialPermanence , 
permanenceIncrement , permanenceDecrement , 
maxSynapsesPerSegment , learn ) : 
cellsToAdd = [ ] 
previousCell = None 
for segment in columnActiveSegments : 
~~~ if segment . cell != previousCell : 
~~~ cellsToAdd . append ( segment . cell ) 
previousCell = segment . cell 
~~ if learn : 
~~~ cls . _adaptSegment ( connections , segment , prevActiveCells , 
permanenceIncrement , permanenceDecrement ) 
active = numActivePotentialSynapsesForSegment [ segment . flatIdx ] 
nGrowDesired = maxNewSynapseCount - active 
if nGrowDesired > 0 : 
~~~ cls . _growSynapses ( connections , random , segment , nGrowDesired , 
prevWinnerCells , initialPermanence , 
maxSynapsesPerSegment ) 
~~ ~~ ~~ return cellsToAdd 
~~ def _burstColumn ( cls , connections , random , lastUsedIterationForSegment , 
column , columnMatchingSegments , prevActiveCells , 
prevWinnerCells , cellsForColumn , 
numActivePotentialSynapsesForSegment , iteration , 
maxNewSynapseCount , initialPermanence , permanenceIncrement , 
permanenceDecrement , maxSegmentsPerCell , 
if columnMatchingSegments is not None : 
~~~ numActive = lambda s : numActivePotentialSynapsesForSegment [ s . flatIdx ] 
bestMatchingSegment = max ( columnMatchingSegments , key = numActive ) 
winnerCell = bestMatchingSegment . cell 
~~~ cls . _adaptSegment ( connections , bestMatchingSegment , prevActiveCells , 
nGrowDesired = maxNewSynapseCount - numActive ( bestMatchingSegment ) 
~~~ cls . _growSynapses ( connections , random , bestMatchingSegment , 
nGrowDesired , prevWinnerCells , initialPermanence , 
~~~ winnerCell = cls . _leastUsedCell ( random , cellsForColumn , connections ) 
~~~ nGrowExact = min ( maxNewSynapseCount , len ( prevWinnerCells ) ) 
if nGrowExact > 0 : 
~~~ segment = cls . _createSegment ( connections , 
lastUsedIterationForSegment , winnerCell , 
iteration , maxSegmentsPerCell ) 
cls . _growSynapses ( connections , random , segment , nGrowExact , 
~~ ~~ ~~ return cellsForColumn , winnerCell 
~~ def _punishPredictedColumn ( cls , connections , columnMatchingSegments , 
prevActiveCells , predictedSegmentDecrement ) : 
if predictedSegmentDecrement > 0.0 and columnMatchingSegments is not None : 
~~~ for segment in columnMatchingSegments : 
- predictedSegmentDecrement , 0.0 ) 
~~ ~~ ~~ def _createSegment ( cls , connections , lastUsedIterationForSegment , cell , 
iteration , maxSegmentsPerCell ) : 
while connections . numSegments ( cell ) >= maxSegmentsPerCell : 
~~~ leastRecentlyUsedSegment = min ( 
connections . segmentsForCell ( cell ) , 
key = lambda segment : lastUsedIterationForSegment [ segment . flatIdx ] ) 
connections . destroySegment ( leastRecentlyUsedSegment ) 
~~ segment = connections . createSegment ( cell ) 
if segment . flatIdx == len ( lastUsedIterationForSegment ) : 
~~~ lastUsedIterationForSegment . append ( iteration ) 
~~ elif segment . flatIdx < len ( lastUsedIterationForSegment ) : 
~~~ lastUsedIterationForSegment [ segment . flatIdx ] = iteration 
~~ return segment 
~~ def _destroyMinPermanenceSynapses ( cls , connections , random , segment , 
nDestroy , excludeCells ) : 
destroyCandidates = sorted ( 
( synapse for synapse in connections . synapsesForSegment ( segment ) 
if synapse . presynapticCell not in excludeCells ) , 
key = lambda s : s . _ordinal 
for _ in xrange ( nDestroy ) : 
~~~ if len ( destroyCandidates ) == 0 : 
~~ minSynapse = None 
minPermanence = float ( "inf" ) 
for synapse in destroyCandidates : 
~~~ if synapse . permanence < minPermanence - EPSILON : 
~~~ minSynapse = synapse 
minPermanence = synapse . permanence 
~~ ~~ connections . destroySynapse ( minSynapse ) 
destroyCandidates . remove ( minSynapse ) 
~~ ~~ def _leastUsedCell ( cls , random , cells , connections ) : 
leastUsedCells = [ ] 
minNumSegments = float ( "inf" ) 
for cell in cells : 
~~~ numSegments = connections . numSegments ( cell ) 
if numSegments < minNumSegments : 
~~~ minNumSegments = numSegments 
~~ if numSegments == minNumSegments : 
~~~ leastUsedCells . append ( cell ) 
~~ ~~ i = random . getUInt32 ( len ( leastUsedCells ) ) 
return leastUsedCells [ i ] 
~~ def _growSynapses ( cls , connections , random , segment , nDesiredNewSynapes , 
prevWinnerCells , initialPermanence , maxSynapsesPerSegment ) : 
candidates = list ( prevWinnerCells ) 
for synapse in connections . synapsesForSegment ( segment ) : 
~~~ i = binSearch ( candidates , synapse . presynapticCell ) 
if i != - 1 : 
~~~ del candidates [ i ] 
~~ ~~ nActual = min ( nDesiredNewSynapes , len ( candidates ) ) 
overrun = connections . numSynapses ( segment ) + nActual - maxSynapsesPerSegment 
if overrun > 0 : 
~~~ cls . _destroyMinPermanenceSynapses ( connections , random , segment , overrun , 
~~ nActual = min ( nActual , 
maxSynapsesPerSegment - connections . numSynapses ( segment ) ) 
for _ in range ( nActual ) : 
~~~ i = random . getUInt32 ( len ( candidates ) ) 
connections . createSynapse ( segment , candidates [ i ] , initialPermanence ) 
del candidates [ i ] 
~~ ~~ def _adaptSegment ( cls , connections , segment , prevActiveCells , 
permanenceIncrement , permanenceDecrement ) : 
synapsesToDestroy = [ ] 
~~~ permanence = synapse . permanence 
if binSearch ( prevActiveCells , synapse . presynapticCell ) != - 1 : 
~~~ permanence += permanenceIncrement 
~~~ permanence -= permanenceDecrement 
~~ permanence = max ( 0.0 , min ( 1.0 , permanence ) ) 
if permanence < EPSILON : 
~~~ synapsesToDestroy . append ( synapse ) 
~~~ connections . updateSynapsePermanence ( synapse , permanence ) 
~~ ~~ for synapse in synapsesToDestroy : 
~~~ connections . destroySynapse ( synapse ) 
~~ if connections . numSynapses ( segment ) == 0 : 
~~~ connections . destroySegment ( segment ) 
~~ ~~ def columnForCell ( self , cell ) : 
self . _validateCell ( cell ) 
return int ( cell / self . cellsPerColumn ) 
~~ def cellsForColumn ( self , column ) : 
self . _validateColumn ( column ) 
start = self . cellsPerColumn * column 
end = start + self . cellsPerColumn 
return range ( start , end ) 
~~ def mapCellsToColumns ( self , cells ) : 
cellsForColumns = defaultdict ( set ) 
~~~ column = self . columnForCell ( cell ) 
cellsForColumns [ column ] . add ( cell ) 
~~ return cellsForColumns 
~~ def getPredictiveCells ( self ) : 
predictiveCells = [ ] 
for segment in self . activeSegments : 
~~~ predictiveCells . append ( segment . cell ) 
~~ ~~ return predictiveCells 
proto . columnDimensions = list ( self . columnDimensions ) 
proto . activationThreshold = self . activationThreshold 
proto . initialPermanence = round ( self . initialPermanence , EPSILON_ROUND ) 
proto . connectedPermanence = round ( self . connectedPermanence , EPSILON_ROUND ) 
proto . minThreshold = self . minThreshold 
proto . maxNewSynapseCount = self . maxNewSynapseCount 
proto . permanenceIncrement = round ( self . permanenceIncrement , EPSILON_ROUND ) 
proto . permanenceDecrement = round ( self . permanenceDecrement , EPSILON_ROUND ) 
proto . predictedSegmentDecrement = self . predictedSegmentDecrement 
proto . maxSegmentsPerCell = self . maxSegmentsPerCell 
proto . maxSynapsesPerSegment = self . maxSynapsesPerSegment 
self . connections . write ( proto . connections ) 
self . _random . write ( proto . random ) 
proto . activeCells = list ( self . activeCells ) 
proto . winnerCells = list ( self . winnerCells ) 
protoActiveSegments = proto . init ( "activeSegments" , len ( self . activeSegments ) ) 
for i , segment in enumerate ( self . activeSegments ) : 
~~~ protoActiveSegments [ i ] . cell = segment . cell 
idx = self . connections . segmentsForCell ( segment . cell ) . index ( segment ) 
protoActiveSegments [ i ] . idxOnCell = idx 
~~ protoMatchingSegments = proto . init ( "matchingSegments" , 
len ( self . matchingSegments ) ) 
for i , segment in enumerate ( self . matchingSegments ) : 
~~~ protoMatchingSegments [ i ] . cell = segment . cell 
protoMatchingSegments [ i ] . idxOnCell = idx 
~~ protoNumActivePotential = proto . init ( 
"numActivePotentialSynapsesForSegment" , 
len ( self . numActivePotentialSynapsesForSegment ) ) 
for i , numActivePotentialSynapses in enumerate ( 
self . numActivePotentialSynapsesForSegment ) : 
~~~ segment = self . connections . segmentForFlatIdx ( i ) 
if segment is not None : 
~~~ protoNumActivePotential [ i ] . cell = segment . cell 
protoNumActivePotential [ i ] . idxOnCell = idx 
protoNumActivePotential [ i ] . number = numActivePotentialSynapses 
~~ ~~ proto . iteration = self . iteration 
protoLastUsedIteration = proto . init ( 
"lastUsedIterationForSegment" , 
for i , lastUsed in enumerate ( self . lastUsedIterationForSegment ) : 
~~~ protoLastUsedIteration [ i ] . cell = segment . cell 
protoLastUsedIteration [ i ] . idxOnCell = idx 
protoLastUsedIteration [ i ] . number = lastUsed 
~~ ~~ ~~ def read ( cls , proto ) : 
tm = object . __new__ ( cls ) 
tm . columnDimensions = tuple ( proto . columnDimensions ) 
tm . cellsPerColumn = int ( proto . cellsPerColumn ) 
tm . activationThreshold = int ( proto . activationThreshold ) 
tm . initialPermanence = round ( proto . initialPermanence , EPSILON_ROUND ) 
tm . connectedPermanence = round ( proto . connectedPermanence , EPSILON_ROUND ) 
tm . minThreshold = int ( proto . minThreshold ) 
tm . maxNewSynapseCount = int ( proto . maxNewSynapseCount ) 
tm . permanenceIncrement = round ( proto . permanenceIncrement , EPSILON_ROUND ) 
tm . permanenceDecrement = round ( proto . permanenceDecrement , EPSILON_ROUND ) 
tm . predictedSegmentDecrement = round ( proto . predictedSegmentDecrement , 
tm . maxSegmentsPerCell = int ( proto . maxSegmentsPerCell ) 
tm . maxSynapsesPerSegment = int ( proto . maxSynapsesPerSegment ) 
tm . connections = Connections . read ( proto . connections ) 
tm . _random = Random ( ) 
tm . _random . read ( proto . random ) 
tm . activeCells = [ int ( x ) for x in proto . activeCells ] 
tm . winnerCells = [ int ( x ) for x in proto . winnerCells ] 
flatListLength = tm . connections . segmentFlatListLength ( ) 
tm . numActiveConnectedSynapsesForSegment = [ 0 ] * flatListLength 
tm . numActivePotentialSynapsesForSegment = [ 0 ] * flatListLength 
tm . lastUsedIterationForSegment = [ 0 ] * flatListLength 
tm . activeSegments = [ ] 
tm . matchingSegments = [ ] 
for protoSegment in proto . activeSegments : 
~~~ tm . activeSegments . append ( 
tm . connections . getSegment ( protoSegment . cell , 
protoSegment . idxOnCell ) ) 
~~ for protoSegment in proto . matchingSegments : 
~~~ tm . matchingSegments . append ( 
~~ for protoSegment in proto . numActivePotentialSynapsesForSegment : 
~~~ segment = tm . connections . getSegment ( protoSegment . cell , 
protoSegment . idxOnCell ) 
tm . numActivePotentialSynapsesForSegment [ segment . flatIdx ] = ( 
int ( protoSegment . number ) ) 
~~ tm . iteration = long ( proto . iteration ) 
for protoSegment in proto . lastUsedIterationForSegment : 
tm . lastUsedIterationForSegment [ segment . flatIdx ] = ( 
long ( protoSegment . number ) ) 
~~ return tm 
~~ def generateFromNumbers ( self , numbers ) : 
sequence = [ ] 
~~~ if number == None : 
~~~ sequence . append ( number ) 
~~~ pattern = self . patternMachine . get ( number ) 
sequence . append ( pattern ) 
~~ ~~ return sequence 
~~ def addSpatialNoise ( self , sequence , amount ) : 
newSequence = [ ] 
for pattern in sequence : 
~~~ if pattern is not None : 
~~~ pattern = self . patternMachine . addNoise ( pattern , amount ) 
~~ newSequence . append ( pattern ) 
~~ return newSequence 
~~ def prettyPrintSequence ( self , sequence , verbosity = 1 ) : 
for i in xrange ( len ( sequence ) ) : 
~~~ pattern = sequence [ i ] 
if pattern == None : 
~~~ text += "<reset>" 
if i < len ( sequence ) - 1 : 
~~~ text += "\\n" 
~~~ text += self . patternMachine . prettyPrintPattern ( pattern , 
~~ ~~ return text 
~~ def ROCCurve ( y_true , y_score ) : 
y_true = np . ravel ( y_true ) 
classes = np . unique ( y_true ) 
if classes . shape [ 0 ] != 2 : 
~~ y_score = np . ravel ( y_score ) 
thresholds = np . unique ( y_score ) 
neg_value , pos_value = classes [ 0 ] , classes [ 1 ] 
current_pos_count = current_neg_count = sum_pos = sum_neg = idx = 0 
signal = np . c_ [ y_score , y_true ] 
sorted_signal = signal [ signal [ : , 0 ] . argsort ( ) , : ] [ : : - 1 ] 
last_score = sorted_signal [ 0 ] [ 0 ] 
for score , value in sorted_signal : 
~~~ if score == last_score : 
~~~ if value == pos_value : 
~~~ current_pos_count += 1 
~~~ current_neg_count += 1 
~~~ tpr [ idx ] = ( sum_pos + current_pos_count ) / n_pos 
fpr [ idx ] = ( sum_neg + current_neg_count ) / n_neg 
sum_pos += current_pos_count 
sum_neg += current_neg_count 
current_pos_count = 1 if value == pos_value else 0 
current_neg_count = 1 if value == neg_value else 0 
idx += 1 
last_score = score 
~~~ tpr [ - 1 ] = ( sum_pos + current_pos_count ) / n_pos 
fpr [ - 1 ] = ( sum_neg + current_neg_count ) / n_neg 
~~ if fpr . shape [ 0 ] == 2 : 
~~~ fpr = np . array ( [ 0.0 , fpr [ 0 ] , fpr [ 1 ] ] ) 
tpr = np . array ( [ 0.0 , tpr [ 0 ] , tpr [ 1 ] ] ) 
~~ elif fpr . shape [ 0 ] == 1 : 
~~~ fpr = np . array ( [ 0.0 , fpr [ 0 ] , 1.0 ] ) 
tpr = np . array ( [ 0.0 , tpr [ 0 ] , 1.0 ] ) 
~~ return fpr , tpr , thresholds 
~~ def AreaUnderCurve ( x , y ) : 
if x . shape [ 0 ] != y . shape [ 0 ] : 
% ( x . shape , y . shape ) ) 
~~ if x . shape [ 0 ] < 2 : 
~~ order = np . argsort ( x ) 
x = x [ order ] 
y = y [ order ] 
h = np . diff ( x ) 
area = np . sum ( h * ( y [ 1 : ] + y [ : - 1 ] ) ) / 2.0 
return area 
~~ def mmPrettyPrintTraces ( traces , breakOnResets = None ) : 
table = PrettyTable ( [ "#" ] + [ trace . prettyPrintTitle ( ) for trace in traces ] ) 
for i in xrange ( len ( traces [ 0 ] . data ) ) : 
~~~ if breakOnResets and breakOnResets . data [ i ] : 
~~~ table . add_row ( [ "<reset>" ] * ( len ( traces ) + 1 ) ) 
~~ table . add_row ( [ i ] + 
[ trace . prettyPrintDatum ( trace . data [ i ] ) for trace in traces ] ) 
~~ return table . get_string ( ) . encode ( "utf-8" ) 
~~ def mmPrettyPrintMetrics ( metrics , sigFigs = 5 ) : 
"min" , "max" , "sum" , ] ) 
for metric in metrics : 
~~~ table . add_row ( [ metric . prettyPrintTitle ( ) ] + metric . getStats ( ) ) 
~~ def mmGetCellTracePlot ( self , cellTrace , cellCount , activityType , title = "" , 
showReset = False , resetShading = 0.25 ) : 
plot = Plot ( self , title ) 
resetTrace = self . mmGetTraceResets ( ) . data 
data = numpy . zeros ( ( cellCount , 1 ) ) 
for i in xrange ( len ( cellTrace ) ) : 
~~~ if showReset and resetTrace [ i ] : 
~~~ activity = numpy . ones ( ( cellCount , 1 ) ) * resetShading 
~~~ activity = numpy . zeros ( ( cellCount , 1 ) ) 
~~ activeIndices = cellTrace [ i ] 
activity [ list ( activeIndices ) ] = 1 
data = numpy . concatenate ( ( data , activity ) , 1 ) 
~~ plot . add2DArray ( data , xlabel = "Time" , ylabel = activityType , name = title ) 
return plot 
~~ def estimateAnomalyLikelihoods ( anomalyScores , 
averagingWindow = 10 , 
skipRecords = 0 , 
verbosity = 0 ) : 
if verbosity > 1 : 
~~ if len ( anomalyScores ) == 0 : 
~~ aggRecordList , historicalValues , total = _anomalyScoreMovingAverage ( 
anomalyScores , 
windowSize = averagingWindow , 
s = [ r [ 2 ] for r in aggRecordList ] 
dataValues = numpy . array ( s ) 
if len ( aggRecordList ) <= skipRecords : 
~~~ distributionParams = nullDistribution ( verbosity = verbosity ) 
~~~ distributionParams = estimateNormal ( dataValues [ skipRecords : ] ) 
s = [ r [ 1 ] for r in aggRecordList ] 
if all ( [ isinstance ( r [ 1 ] , numbers . Number ) for r in aggRecordList ] ) : 
~~~ metricValues = numpy . array ( s ) 
metricDistribution = estimateNormal ( metricValues [ skipRecords : ] , 
performLowerBoundCheck = False ) 
if metricDistribution [ "variance" ] < 1.5e-5 : 
~~ ~~ ~~ likelihoods = numpy . array ( dataValues , dtype = float ) 
for i , s in enumerate ( dataValues ) : 
~~~ likelihoods [ i ] = tailProbability ( s , distributionParams ) 
~~ filteredLikelihoods = numpy . array ( 
_filterLikelihoods ( likelihoods ) ) 
"distribution" : distributionParams , 
"movingAverage" : { 
"historicalValues" : historicalValues , 
"total" : total , 
"windowSize" : averagingWindow , 
"historicalLikelihoods" : 
list ( likelihoods [ - min ( averagingWindow , len ( likelihoods ) ) : ] ) , 
print ( params ) 
filteredLikelihoods [ 0 : min ( 20 , len ( filteredLikelihoods ) ) ] ) ) 
~~ return ( filteredLikelihoods , aggRecordList , params ) 
~~ def updateAnomalyLikelihoods ( anomalyScores , 
params , 
if verbosity > 3 : 
print ( "Params:" , params ) 
~~ if not isValidEstimatorParams ( params ) : 
~~~ raise ValueError ( "\ ) 
~~ if "historicalLikelihoods" not in params : 
~~~ params [ "historicalLikelihoods" ] = [ 1.0 ] 
~~ historicalValues = params [ "movingAverage" ] [ "historicalValues" ] 
total = params [ "movingAverage" ] [ "total" ] 
windowSize = params [ "movingAverage" ] [ "windowSize" ] 
aggRecordList = numpy . zeros ( len ( anomalyScores ) , dtype = float ) 
likelihoods = numpy . zeros ( len ( anomalyScores ) , dtype = float ) 
for i , v in enumerate ( anomalyScores ) : 
~~~ newAverage , historicalValues , total = ( 
MovingAverage . compute ( historicalValues , total , v [ 2 ] , windowSize ) 
aggRecordList [ i ] = newAverage 
likelihoods [ i ] = tailProbability ( newAverage , params [ "distribution" ] ) 
~~ likelihoods2 = params [ "historicalLikelihoods" ] + list ( likelihoods ) 
filteredLikelihoods = _filterLikelihoods ( likelihoods2 ) 
likelihoods [ : ] = filteredLikelihoods [ - len ( likelihoods ) : ] 
historicalLikelihoods = likelihoods2 [ - min ( windowSize , len ( likelihoods2 ) ) : ] 
newParams = { 
"distribution" : params [ "distribution" ] , 
"windowSize" : windowSize , 
"historicalLikelihoods" : historicalLikelihoods , 
assert len ( newParams [ "historicalLikelihoods" ] ) <= windowSize 
~~ return ( likelihoods , aggRecordList , newParams ) 
~~ def _filterLikelihoods ( likelihoods , 
redThreshold = 0.99999 , yellowThreshold = 0.999 ) : 
redThreshold = 1.0 - redThreshold 
yellowThreshold = 1.0 - yellowThreshold 
filteredLikelihoods = [ likelihoods [ 0 ] ] 
for i , v in enumerate ( likelihoods [ 1 : ] ) : 
~~~ if v <= redThreshold : 
~~~ if likelihoods [ i ] > redThreshold : 
~~~ filteredLikelihoods . append ( v ) 
~~~ filteredLikelihoods . append ( yellowThreshold ) 
~~ ~~ return filteredLikelihoods 
~~ def _anomalyScoreMovingAverage ( anomalyScores , 
windowSize = 10 , 
verbosity = 0 , 
historicalValues = [ ] 
total = 0.0 
for record in anomalyScores : 
~~~ if not isinstance ( record , ( list , tuple ) ) or len ( record ) != 3 : 
~~~ if verbosity >= 1 : 
~~ avg , historicalValues , total = ( 
MovingAverage . compute ( historicalValues , total , record [ 2 ] , windowSize ) 
averagedRecordList . append ( [ record [ 0 ] , record [ 1 ] , avg ] ) 
if verbosity > 2 : 
print ( "Result:" , [ record [ 0 ] , record [ 1 ] , avg ] ) 
~~ ~~ return averagedRecordList , historicalValues , total 
~~ def estimateNormal ( sampleData , performLowerBoundCheck = True ) : 
"name" : "normal" , 
"mean" : numpy . mean ( sampleData ) , 
"variance" : numpy . var ( sampleData ) , 
if performLowerBoundCheck : 
~~~ if params [ "mean" ] < 0.03 : 
~~~ params [ "mean" ] = 0.03 
~~ if params [ "variance" ] < 0.0003 : 
~~~ params [ "variance" ] = 0.0003 
~~ ~~ if params [ "variance" ] > 0 : 
~~~ params [ "stdev" ] = math . sqrt ( params [ "variance" ] ) 
~~~ params [ "stdev" ] = 0 
~~ return params 
~~ def tailProbability ( x , distributionParams ) : 
if "mean" not in distributionParams or "stdev" not in distributionParams : 
~~ if x < distributionParams [ "mean" ] : 
~~~ xp = 2 * distributionParams [ "mean" ] - x 
return tailProbability ( xp , distributionParams ) 
~~ z = ( x - distributionParams [ "mean" ] ) / distributionParams [ "stdev" ] 
return 0.5 * math . erfc ( z / 1.4142 ) 
~~ def isValidEstimatorParams ( p ) : 
if not isinstance ( p , dict ) : 
~~ if "distribution" not in p : 
~~ if "movingAverage" not in p : 
~~ dist = p [ "distribution" ] 
if not ( "mean" in dist and "name" in dist 
and "variance" in dist and "stdev" in dist ) : 
~~ def _calcSkipRecords ( numIngested , windowSize , learningPeriod ) : 
numShiftedOut = max ( 0 , numIngested - windowSize ) 
return min ( numIngested , max ( 0 , learningPeriod - numShiftedOut ) ) 
anomalyLikelihood = object . __new__ ( cls ) 
anomalyLikelihood . _iteration = proto . iteration 
anomalyLikelihood . _historicalScores = collections . deque ( 
maxlen = proto . historicWindowSize ) 
for i , score in enumerate ( proto . historicalScores ) : 
~~~ anomalyLikelihood . _historicalScores . append ( ( i , score . value , 
score . anomalyScore ) ) 
~~~ anomalyLikelihood . _distribution = dict ( ) 
anomalyLikelihood . _distribution [ 'distribution' ] = dict ( ) 
anomalyLikelihood . _distribution [ 'distribution' ] [ "name" ] = proto . distribution . name 
anomalyLikelihood . _distribution [ 'distribution' ] [ "mean" ] = proto . distribution . mean 
anomalyLikelihood . _distribution [ 'distribution' ] [ "variance" ] = proto . distribution . variance 
anomalyLikelihood . _distribution [ 'distribution' ] [ "stdev" ] = proto . distribution . stdev 
anomalyLikelihood . _distribution [ "movingAverage" ] = { } 
anomalyLikelihood . _distribution [ "movingAverage" ] [ "windowSize" ] = proto . distribution . movingAverage . windowSize 
anomalyLikelihood . _distribution [ "movingAverage" ] [ "historicalValues" ] = [ ] 
for value in proto . distribution . movingAverage . historicalValues : 
~~~ anomalyLikelihood . _distribution [ "movingAverage" ] [ "historicalValues" ] . append ( value ) 
~~ anomalyLikelihood . _distribution [ "movingAverage" ] [ "total" ] = proto . distribution . movingAverage . total 
anomalyLikelihood . _distribution [ "historicalLikelihoods" ] = [ ] 
for likelihood in proto . distribution . historicalLikelihoods : 
~~~ anomalyLikelihood . _distribution [ "historicalLikelihoods" ] . append ( likelihood ) 
~~~ anomalyLikelihood . _distribution = None 
~~ anomalyLikelihood . _probationaryPeriod = proto . probationaryPeriod 
anomalyLikelihood . _learningPeriod = proto . learningPeriod 
anomalyLikelihood . _reestimationPeriod = proto . reestimationPeriod 
return anomalyLikelihood 
proto . iteration = self . _iteration 
pHistScores = proto . init ( 'historicalScores' , len ( self . _historicalScores ) ) 
for i , score in enumerate ( list ( self . _historicalScores ) ) : 
~~~ _ , value , anomalyScore = score 
record = pHistScores [ i ] 
record . value = float ( value ) 
record . anomalyScore = float ( anomalyScore ) 
~~ if self . _distribution : 
~~~ proto . distribution . name = self . _distribution [ "distribution" ] [ "name" ] 
proto . distribution . mean = float ( self . _distribution [ "distribution" ] [ "mean" ] ) 
proto . distribution . variance = float ( self . _distribution [ "distribution" ] [ "variance" ] ) 
proto . distribution . stdev = float ( self . _distribution [ "distribution" ] [ "stdev" ] ) 
proto . distribution . movingAverage . windowSize = float ( self . _distribution [ "movingAverage" ] [ "windowSize" ] ) 
historicalValues = self . _distribution [ "movingAverage" ] [ "historicalValues" ] 
pHistValues = proto . distribution . movingAverage . init ( 
"historicalValues" , len ( historicalValues ) ) 
for i , value in enumerate ( historicalValues ) : 
~~~ pHistValues [ i ] = float ( value ) 
~~ proto . distribution . movingAverage . total = float ( self . _distribution [ "movingAverage" ] [ "total" ] ) 
historicalLikelihoods = self . _distribution [ "historicalLikelihoods" ] 
pHistLikelihoods = proto . distribution . init ( "historicalLikelihoods" , 
len ( historicalLikelihoods ) ) 
for i , likelihood in enumerate ( historicalLikelihoods ) : 
~~~ pHistLikelihoods [ i ] = float ( likelihood ) 
~~ ~~ proto . probationaryPeriod = self . _probationaryPeriod 
proto . learningPeriod = self . _learningPeriod 
proto . reestimationPeriod = self . _reestimationPeriod 
proto . historicWindowSize = self . _historicalScores . maxlen 
~~ def anomalyProbability ( self , value , anomalyScore , timestamp = None ) : 
if timestamp is None : 
~~~ timestamp = self . _iteration 
~~ dataPoint = ( timestamp , value , anomalyScore ) 
if self . _iteration < self . _probationaryPeriod : 
~~~ likelihood = 0.5 
~~~ if ( ( self . _distribution is None ) or 
( self . _iteration % self . _reestimationPeriod == 0 ) ) : 
~~~ numSkipRecords = self . _calcSkipRecords ( 
numIngested = self . _iteration , 
windowSize = self . _historicalScores . maxlen , 
learningPeriod = self . _learningPeriod ) 
_ , _ , self . _distribution = estimateAnomalyLikelihoods ( 
self . _historicalScores , 
skipRecords = numSkipRecords ) 
~~ likelihoods , _ , self . _distribution = updateAnomalyLikelihoods ( 
[ dataPoint ] , 
self . _distribution ) 
likelihood = 1.0 - likelihoods [ 0 ] 
~~ self . _historicalScores . append ( dataPoint ) 
return likelihood 
~~ def _getImpl ( self , model ) : 
impl = _IterationPhaseLearnOnly ( model = model , 
nIters = self . __nIters ) 
return impl 
impl = _IterationPhaseInferOnly ( model = model , 
nIters = self . __nIters , 
inferenceArgs = self . __inferenceArgs ) 
~~ def replaceIterationCycle ( self , phaseSpecs ) : 
self . __phaseManager = _PhaseManager ( 
model = self . __model , 
phaseSpecs = phaseSpecs ) 
~~ def handleInputRecord ( self , inputRecord ) : 
results = self . __phaseManager . handleInputRecord ( inputRecord ) 
metrics = self . __metricsMgr . update ( results ) 
for cb in self . __userCallbacks [ 'postIter' ] : 
~~~ cb ( self . __model ) 
~~ results . metrics = metrics 
~~ def __advancePhase ( self ) : 
self . __currentPhase = self . __phaseCycler . next ( ) 
self . __currentPhase . enterPhase ( ) 
results = self . __model . run ( inputRecord ) 
shouldContinue = self . __currentPhase . advance ( ) 
if not shouldContinue : 
~~~ self . __advancePhase ( ) 
~~ def enterPhase ( self ) : 
self . __iter = iter ( xrange ( self . __nIters ) ) 
self . __iter . next ( ) 
~~ def advance ( self ) : 
hasMore = True 
~~~ self . __iter . next ( ) 
~~~ self . __iter = None 
hasMore = False 
~~ return hasMore 
super ( _IterationPhaseLearnOnly , self ) . enterPhase ( ) 
self . __model . enableLearning ( ) 
self . __model . disableInference ( ) 
super ( _IterationPhaseInferCommon , self ) . enterPhase ( ) 
self . _model . enableInference ( inferenceArgs = self . _inferenceArgs ) 
super ( PreviousValueModel , self ) . writeBaseToProto ( proto . modelBase ) 
proto . fieldNames = self . _fieldNames 
proto . fieldTypes = self . _fieldTypes 
if self . _predictedField : 
~~~ proto . predictedField = self . _predictedField 
~~ proto . predictionSteps = self . _predictionSteps 
super ( PreviousValueModel , instance ) . __init__ ( proto = proto . modelBase ) 
if len ( proto . predictedField ) : 
~~~ instance . _predictedField = proto . predictedField 
~~~ instance . _predictedField = None 
~~ instance . _fieldNames = list ( proto . fieldNames ) 
instance . _fieldTypes = list ( proto . fieldTypes ) 
instance . _predictionSteps = list ( proto . predictionSteps ) 
~~ def lscsum ( lx , epsilon = None ) : 
lx = numpy . asarray ( lx ) 
base = lx . max ( ) 
if numpy . isinf ( base ) : 
~~~ return base 
~~ if ( epsilon is not None ) and ( base < epsilon ) : 
~~~ return epsilon 
~~ x = numpy . exp ( lx - base ) 
ssum = x . sum ( ) 
result = numpy . log ( ssum ) + base 
~~ def lscsum0 ( lx ) : 
x = numpy . exp ( lx - bases ) 
ssum = x . sum ( 0 ) 
result = numpy . log ( ssum ) + bases 
~~~ conventional = numpy . log ( numpy . exp ( lx ) . sum ( 0 ) ) 
if not similar ( result , conventional ) : 
~~~ if numpy . isinf ( conventional ) . any ( ) and not numpy . isinf ( result ) . any ( ) : 
~~~ import sys 
print >> sys . stderr , result 
print >> sys . stderr , conventional 
~~ ~~ ~~ except FloatingPointError , e : 
~~ def normalize ( lx ) : 
x = numpy . exp ( lx - base ) 
result = x / x . sum ( ) 
conventional = ( numpy . exp ( lx ) / numpy . exp ( lx ) . sum ( ) ) 
assert similar ( result , conventional ) 
~~ def nsum0 ( lx ) : 
result = ssum / ssum . sum ( ) 
conventional = ( numpy . exp ( lx ) . sum ( 0 ) / numpy . exp ( lx ) . sum ( ) ) 
~~ def logSumExp ( A , B , out = None ) : 
if out is None : 
~~~ out = numpy . zeros ( A . shape ) 
~~ indicator1 = A >= B 
indicator2 = numpy . logical_not ( indicator1 ) 
out [ indicator1 ] = A [ indicator1 ] + numpy . log1p ( numpy . exp ( B [ indicator1 ] - A [ indicator1 ] ) ) 
out [ indicator2 ] = B [ indicator2 ] + numpy . log1p ( numpy . exp ( A [ indicator2 ] - B [ indicator2 ] ) ) 
~~ def logDiffExp ( A , B , out = None ) : 
out [ indicator1 ] = A [ indicator1 ] + numpy . log ( 1 - numpy . exp ( B [ indicator1 ] - A [ indicator1 ] ) ) 
~~ def _getFieldIndexBySpecial ( fields , special ) : 
for i , field in enumerate ( fields ) : 
~~~ if field . special == special : 
~~ def encode ( self , inputRow ) : 
result = dict ( zip ( self . _fieldNames , inputRow ) ) 
if self . _categoryFieldIndex is not None : 
~~~ if isinstance ( inputRow [ self . _categoryFieldIndex ] , int ) : 
~~~ result [ '_category' ] = [ inputRow [ self . _categoryFieldIndex ] ] 
~~~ result [ '_category' ] = ( inputRow [ self . _categoryFieldIndex ] 
if inputRow [ self . _categoryFieldIndex ] 
else [ None ] ) 
~~~ result [ '_category' ] = [ None ] 
~~ if self . _resetFieldIndex is not None : 
~~~ result [ '_reset' ] = int ( bool ( inputRow [ self . _resetFieldIndex ] ) ) 
~~~ result [ '_reset' ] = 0 
~~ if self . _learningFieldIndex is not None : 
~~~ result [ '_learning' ] = int ( bool ( inputRow [ self . _learningFieldIndex ] ) ) 
~~ result [ '_timestampRecordIdx' ] = None 
if self . _timestampFieldIndex is not None : 
~~~ result [ '_timestamp' ] = inputRow [ self . _timestampFieldIndex ] 
result [ '_timestampRecordIdx' ] = self . _computeTimestampRecordIdx ( 
inputRow [ self . _timestampFieldIndex ] ) 
~~~ result [ '_timestamp' ] = None 
~~ hasReset = self . _resetFieldIndex is not None 
hasSequenceId = self . _sequenceFieldIndex is not None 
~~~ if result [ '_reset' ] : 
~~~ self . _sequenceId += 1 
~~ sequenceId = self . _sequenceId 
~~~ sequenceId = inputRow [ self . _sequenceFieldIndex ] 
result [ '_reset' ] = int ( sequenceId != self . _sequenceId ) 
self . _sequenceId = sequenceId 
~~~ sequenceId = 0 
~~ if sequenceId is not None : 
~~~ result [ '_sequenceId' ] = hash ( sequenceId ) 
~~~ result [ '_sequenceId' ] = None 
~~ def _computeTimestampRecordIdx ( self , recordTS ) : 
if self . _aggregationPeriod is None : 
~~ if self . _aggregationPeriod [ 'months' ] > 0 : 
~~~ assert self . _aggregationPeriod [ 'seconds' ] == 0 
result = int ( 
( recordTS . year * 12 + ( recordTS . month - 1 ) ) / 
self . _aggregationPeriod [ 'months' ] ) 
~~ elif self . _aggregationPeriod [ 'seconds' ] > 0 : 
~~~ delta = recordTS - datetime . datetime ( year = 1 , month = 1 , day = 1 ) 
deltaSecs = delta . days * 24 * 60 * 60 + delta . seconds + delta . microseconds / 1000000.0 
result = int ( deltaSecs / self . _aggregationPeriod [ 'seconds' ] ) 
~~ def getNextRecordDict ( self ) : 
values = self . getNextRecord ( ) 
if values is None : 
~~ if not values : 
~~ if self . _modelRecordEncoder is None : 
~~~ self . _modelRecordEncoder = ModelRecordEncoder ( 
fields = self . getFields ( ) , 
aggregationPeriod = self . getAggregationMonthsAndSeconds ( ) ) 
~~ return self . _modelRecordEncoder . encode ( values ) 
~~ def getFieldMin ( self , fieldName ) : 
stats = self . getStats ( ) 
if stats == None : 
~~ minValues = stats . get ( 'min' , None ) 
if minValues == None : 
~~ index = self . getFieldNames ( ) . index ( fieldName ) 
return minValues [ index ] 
~~ def getFieldMax ( self , fieldName ) : 
~~ maxValues = stats . get ( 'max' , None ) 
if maxValues == None : 
return maxValues [ index ] 
~~ def debug ( self , msg , * args , ** kwargs ) : 
self . _baseLogger . debug ( self , self . getExtendedMsg ( msg ) , * args , ** kwargs ) 
~~ def info ( self , msg , * args , ** kwargs ) : 
self . _baseLogger . info ( self , self . getExtendedMsg ( msg ) , * args , ** kwargs ) 
~~ def warning ( self , msg , * args , ** kwargs ) : 
self . _baseLogger . warning ( self , self . getExtendedMsg ( msg ) , * args , ** kwargs ) 
~~ def error ( self , msg , * args , ** kwargs ) : 
self . _baseLogger . error ( self , self . getExtendedMsg ( msg ) , * args , ** kwargs ) 
~~ def critical ( self , msg , * args , ** kwargs ) : 
self . _baseLogger . critical ( self , self . getExtendedMsg ( msg ) , * args , ** kwargs ) 
~~ def log ( self , level , msg , * args , ** kwargs ) : 
self . _baseLogger . log ( self , level , self . getExtendedMsg ( msg ) , * args , 
** kwargs ) 
~~ def initFilter ( input , filterInfo = None ) : 
if filterInfo is None : 
~~ filterList = [ ] 
for i , fieldName in enumerate ( input . getFieldNames ( ) ) : 
~~~ fieldFilter = filterInfo . get ( fieldName , None ) 
if fieldFilter == None : 
~~ var = dict ( ) 
var [ 'acceptValues' ] = None 
min = fieldFilter . get ( 'min' , None ) 
max = fieldFilter . get ( 'max' , None ) 
var [ 'min' ] = min 
var [ 'max' ] = max 
if fieldFilter [ 'type' ] == 'category' : 
~~~ var [ 'acceptValues' ] = fieldFilter [ 'acceptValues' ] 
fp = lambda x : ( x [ 'value' ] != SENTINEL_VALUE_FOR_MISSING_DATA and x [ 'value' ] in x [ 'acceptValues' ] ) 
~~ elif fieldFilter [ 'type' ] == 'number' : 
~~~ if min != None and max != None : 
~~~ fp = lambda x : ( x [ 'value' ] != SENTINEL_VALUE_FOR_MISSING_DATA and x [ 'value' ] >= x [ 'min' ] and x [ 'value' ] <= x [ 'max' ] ) 
~~ elif min != None : 
~~~ fp = lambda x : ( x [ 'value' ] != SENTINEL_VALUE_FOR_MISSING_DATA and x [ 'value' ] >= x [ 'min' ] ) 
~~~ fp = lambda x : ( x [ 'value' ] != SENTINEL_VALUE_FOR_MISSING_DATA and x [ 'value' ] <= x [ 'max' ] ) 
~~ ~~ filterList . append ( ( i , fp , var ) ) 
~~ return ( _filterRecord , filterList ) 
~~ def _filterRecord ( filterList , record ) : 
for ( fieldIdx , fp , params ) in filterList : 
~~~ x = dict ( ) 
x [ 'value' ] = record [ fieldIdx ] 
x [ 'acceptValues' ] = params [ 'acceptValues' ] 
x [ 'min' ] = params [ 'min' ] 
x [ 'max' ] = params [ 'max' ] 
if not fp ( x ) : 
~~ def _aggr_sum ( inList ) : 
aggrMean = _aggr_mean ( inList ) 
if aggrMean == None : 
~~ aggrSum = 0 
for elem in inList : 
~~~ if elem != SENTINEL_VALUE_FOR_MISSING_DATA : 
~~~ aggrSum += elem 
~~~ aggrSum += aggrMean 
~~ ~~ return aggrSum 
~~ def _aggr_mean ( inList ) : 
aggrSum = 0 
nonNone = 0 
nonNone += 1 
~~ ~~ if nonNone != 0 : 
~~~ return aggrSum / nonNone 
~~ ~~ def _aggr_mode ( inList ) : 
valueCounts = dict ( ) 
~~~ if elem == SENTINEL_VALUE_FOR_MISSING_DATA : 
~~ nonNone += 1 
if elem in valueCounts : 
~~~ valueCounts [ elem ] += 1 
~~~ valueCounts [ elem ] = 1 
~~ ~~ if nonNone == 0 : 
~~ sortedCounts = valueCounts . items ( ) 
sortedCounts . sort ( cmp = lambda x , y : x [ 1 ] - y [ 1 ] , reverse = True ) 
return sortedCounts [ 0 ] [ 0 ] 
~~ def _aggr_weighted_mean ( inList , params ) : 
assert ( len ( inList ) == len ( params ) ) 
weightsSum = sum ( params ) 
if weightsSum == 0 : 
~~ weightedMean = 0 
for i , elem in enumerate ( inList ) : 
~~~ weightedMean += elem * params [ i ] 
~~ return weightedMean / weightsSum 
~~ def generateDataset ( aggregationInfo , inputFilename , outputFilename = None ) : 
inputFullPath = resource_filename ( "nupic.datafiles" , inputFilename ) 
inputObj = FileRecordStream ( inputFullPath ) 
aggregator = Aggregator ( aggregationInfo = aggregationInfo , 
inputFields = inputObj . getFields ( ) ) 
if aggregator . isNullAggregation ( ) : 
~~~ return inputFullPath 
~~ if outputFilename is None : 
~~~ outputFilename = 'agg_%s' % os . path . splitext ( os . path . basename ( inputFullPath ) ) [ 0 ] 
for k in timePeriods . split ( ) : 
~~~ if aggregationInfo . get ( k , 0 ) > 0 : 
~~~ outputFilename += '_%s_%d' % ( k , aggregationInfo [ k ] ) 
~~ ~~ outputFilename += '.csv' 
outputFilename = os . path . join ( os . path . dirname ( inputFullPath ) , outputFilename ) 
~~ lockFilePath = outputFilename + '.please_wait' 
if os . path . isfile ( outputFilename ) or os . path . isfile ( lockFilePath ) : 
~~~ while os . path . isfile ( lockFilePath ) : 
time . sleep ( 1 ) 
~~ return outputFilename 
~~ lockFD = open ( lockFilePath , 'w' ) 
outputObj = FileRecordStream ( streamID = outputFilename , write = True , 
fields = inputObj . getFields ( ) ) 
~~~ inRecord = inputObj . getNextRecord ( ) 
( aggRecord , aggBookmark ) = aggregator . next ( inRecord , None ) 
if aggRecord is None and inRecord is None : 
~~ if aggRecord is not None : 
~~~ outputObj . appendRecord ( aggRecord ) 
~~ ~~ return outputFilename 
~~ def getFilename ( aggregationInfo , inputFile ) : 
inputFile = resource_filename ( "nupic.datafiles" , inputFile ) 
a = defaultdict ( lambda : 0 , aggregationInfo ) 
outputDir = os . path . dirname ( inputFile ) 
outputFile = 'agg_%s' % os . path . splitext ( os . path . basename ( inputFile ) ) [ 0 ] 
noAggregation = True 
~~~ if a [ k ] > 0 : 
~~~ noAggregation = False 
outputFile += '_%s_%d' % ( k , a [ k ] ) 
~~ ~~ if noAggregation : 
~~~ return inputFile 
~~ outputFile += '.csv' 
outputFile = os . path . join ( outputDir , outputFile ) 
return outputFile 
~~ def _getEndTime ( self , t ) : 
assert isinstance ( t , datetime . datetime ) 
if self . _aggTimeDelta : 
~~~ return t + self . _aggTimeDelta 
~~~ year = t . year + self . _aggYears + ( t . month - 1 + self . _aggMonths ) / 12 
month = ( t . month - 1 + self . _aggMonths ) % 12 + 1 
return t . replace ( year = year , month = month ) 
~~ ~~ def _getFuncPtrAndParams ( self , funcName ) : 
params = None 
if isinstance ( funcName , basestring ) : 
~~~ if funcName == 'sum' : 
~~~ fp = _aggr_sum 
~~ elif funcName == 'first' : 
~~~ fp = _aggr_first 
~~ elif funcName == 'last' : 
~~~ fp = _aggr_last 
~~ elif funcName == 'mean' : 
~~~ fp = _aggr_mean 
~~ elif funcName == 'max' : 
~~~ fp = max 
~~ elif funcName == 'min' : 
~~~ fp = min 
~~ elif funcName == 'mode' : 
~~~ fp = _aggr_mode 
~~ elif funcName . startswith ( 'wmean:' ) : 
~~~ fp = _aggr_weighted_mean 
paramsName = funcName [ 6 : ] 
params = [ f [ 0 ] for f in self . _inputFields ] . index ( paramsName ) 
~~~ fp = funcName 
~~ return ( fp , params ) 
~~ def _createAggregateRecord ( self ) : 
for i , ( fieldIdx , aggFP , paramIdx ) in enumerate ( self . _fields ) : 
~~ values = self . _slice [ i ] 
refIndex = None 
if paramIdx is not None : 
~~~ record . append ( aggFP ( values , self . _slice [ paramIdx ] ) ) 
~~~ record . append ( aggFP ( values ) ) 
~~ def next ( self , record , curInputBookmark ) : 
outRecord = None 
retInputBookmark = None 
~~~ self . _inIdx += 1 
if self . _filter != None and not self . _filter [ 0 ] ( self . _filter [ 1 ] , record ) : 
~~ if self . _nullAggregation : 
~~~ return ( record , curInputBookmark ) 
~~ t = record [ self . _timeFieldIdx ] 
if self . _firstSequenceStartTime == None : 
~~~ self . _firstSequenceStartTime = t 
~~ if self . _startTime is None : 
~~~ self . _startTime = t 
~~ if self . _endTime is None : 
~~~ self . _endTime = self . _getEndTime ( t ) 
assert self . _endTime > t 
~~ if self . _resetFieldIdx is not None : 
~~~ resetSignal = record [ self . _resetFieldIdx ] 
~~~ resetSignal = None 
~~ if self . _sequenceIdFieldIdx is not None : 
~~~ currSequenceId = record [ self . _sequenceIdFieldIdx ] 
~~~ currSequenceId = None 
~~ newSequence = ( resetSignal == 1 and self . _inIdx > 0 ) or self . _sequenceId != currSequenceId or self . _inIdx == 0 
if newSequence : 
~~~ self . _sequenceId = currSequenceId 
~~ sliceEnded = ( t >= self . _endTime or t < self . _startTime ) 
if ( newSequence or sliceEnded ) and len ( self . _slice ) > 0 : 
~~~ for j , f in enumerate ( self . _fields ) : 
~~~ index = f [ 0 ] 
if index == self . _timeFieldIdx : 
~~~ self . _slice [ j ] [ 0 ] = self . _startTime 
~~ ~~ outRecord = self . _createAggregateRecord ( ) 
retInputBookmark = self . _aggrInputBookmark 
self . _slice = defaultdict ( list ) 
~~ for j , f in enumerate ( self . _fields ) : 
self . _slice [ j ] . append ( record [ index ] ) 
self . _aggrInputBookmark = curInputBookmark 
~~ if newSequence : 
self . _endTime = self . _getEndTime ( t ) 
~~ if sliceEnded : 
~~~ if t < self . _startTime : 
~~~ self . _endTime = self . _firstSequenceStartTime 
~~ while t >= self . _endTime : 
~~~ self . _startTime = self . _endTime 
self . _endTime = self . _getEndTime ( self . _endTime ) 
~~ ~~ if outRecord is not None : 
~~~ return ( outRecord , retInputBookmark ) 
~~ ~~ elif self . _slice : 
~~ return ( outRecord , retInputBookmark ) 
~~ def processClubAttendance ( f , clubs ) : 
~~~ line = f . next ( ) 
while line == ',,,,,,,,,,,,,,,,,,,\\n' : 
~~ name = line . split ( ',' ) [ 0 ] 
if name not in clubs : 
~~~ clubs [ name ] = Club ( name ) 
~~ c = clubs [ name ] 
c . processAttendance ( f ) 
~~ ~~ def processClubConsumption ( f , clubs ) : 
assert line . endswith ( \ ) 
valid_times = range ( 24 ) 
club = None 
clubName = None 
lastDate = None 
~~~ assert t in valid_times 
consumption = 0 
for x in range ( 4 ) : 
~~~ line = f . next ( ) [ : - 1 ] 
fields = line . split ( ',' ) 
assert len ( fields ) == 4 
~~~ assert field [ 0 ] == \ and field [ - 1 ] == \ 
fields [ i ] = field [ 1 : - 1 ] 
~~ name = fields [ 1 ] 
for pn in partialNames : 
~~~ if pn in name : 
~~~ name = pn 
~~ ~~ if name != clubName : 
~~~ clubName = name 
club = clubs [ name ] 
~~ tokens = fields [ 2 ] . split ( ) 
if len ( tokens ) == 1 : 
~~~ assert consumption == 0 and t == 0 
~~ date = tokens [ 0 ] 
consumption += float ( fields [ 3 ] ) 
~~ club . updateRecord ( date , t , consumption ) 
t += 1 
t %= 24 
~~ ~~ except StopIteration : 
~~ ~~ def run ( self , inputRecord ) : 
predictionNumber = self . _numPredictions 
self . _numPredictions += 1 
result = opf_utils . ModelResult ( predictionNumber = predictionNumber , 
rawInput = inputRecord ) 
~~ def _getModelCheckpointFilePath ( checkpointDir ) : 
path = os . path . join ( checkpointDir , "model.data" ) 
~~ def writeToCheckpoint ( self , checkpointDir ) : 
proto = self . getSchema ( ) . new_message ( ) 
checkpointPath = self . _getModelCheckpointFilePath ( checkpointDir ) 
if os . path . exists ( checkpointDir ) : 
~~~ if not os . path . isdir ( checkpointDir ) : 
~~ if not os . path . isfile ( checkpointPath ) : 
~~ shutil . rmtree ( checkpointDir ) 
~~ self . __makeDirectoryFromAbsolutePath ( checkpointDir ) 
with open ( checkpointPath , 'wb' ) as f : 
~~ ~~ def readFromCheckpoint ( cls , checkpointDir ) : 
checkpointPath = cls . _getModelCheckpointFilePath ( checkpointDir ) 
with open ( checkpointPath , 'r' ) as f : 
~~~ proto = cls . getSchema ( ) . read ( f , 
traversal_limit_in_words = _TRAVERSAL_LIMIT_IN_WORDS ) 
~~ model = cls . read ( proto ) 
return model 
~~ def writeBaseToProto ( self , proto ) : 
inferenceType = inferenceType [ : 1 ] . lower ( ) + inferenceType [ 1 : ] 
proto . inferenceType = inferenceType 
proto . numPredictions = self . _numPredictions 
proto . learningEnabled = self . __learningEnabled 
proto . inferenceEnabled = self . __inferenceEnabled 
proto . inferenceArgs = json . dumps ( self . __inferenceArgs ) 
~~ def save ( self , saveModelDir ) : 
logger = self . _getLogger ( ) 
self , saveModelDir ) 
modelPickleFilePath = self . _getModelPickleFilePath ( saveModelDir ) 
if os . path . exists ( saveModelDir ) : 
~~~ if not os . path . isdir ( saveModelDir ) : 
~~ if not os . path . isfile ( modelPickleFilePath ) : 
~~ shutil . rmtree ( saveModelDir ) 
~~ self . __makeDirectoryFromAbsolutePath ( saveModelDir ) 
with open ( modelPickleFilePath , 'wb' ) as modelPickleFile : 
pickle . dump ( self , modelPickleFile , protocol = pickle . HIGHEST_PROTOCOL ) 
~~ self . _serializeExtraData ( extraDataDir = self . _getModelExtraDataDir ( saveModelDir ) ) 
~~ def load ( cls , savedModelDir ) : 
logger = opf_utils . initLogger ( cls ) 
modelPickleFilePath = Model . _getModelPickleFilePath ( savedModelDir ) 
with open ( modelPickleFilePath , 'rb' ) as modelPickleFile : 
model = pickle . load ( modelPickleFile ) 
~~ model . _deSerializeExtraData ( 
extraDataDir = Model . _getModelExtraDataDir ( savedModelDir ) ) 
~~ def _getModelPickleFilePath ( saveModelDir ) : 
path = os . path . join ( saveModelDir , "model.pkl" ) 
~~ def _getModelExtraDataDir ( saveModelDir ) : 
path = os . path . join ( saveModelDir , "modelextradata" ) 
~~ def runExperiment ( args , model = None ) : 
opt = _parseCommandLineOptions ( args ) 
model = _runExperimentImpl ( opt , model ) 
~~ def _parseCommandLineOptions ( args ) : 
usageStr = ( 
parser = optparse . OptionParser ( usage = usageStr ) 
parser . add_option ( "-c" , 
dest = "createCheckpointName" , 
action = "store" , type = "string" , default = "" , 
metavar = "<CHECKPOINT>" ) 
parser . add_option ( "--listCheckpoints" , 
dest = "listAvailableCheckpoints" , 
action = "store_true" , default = False ) 
parser . add_option ( "--listTasks" , 
dest = "listTasks" , 
parser . add_option ( "--load" , 
dest = "runCheckpointName" , 
parser . add_option ( "--newSerialization" , 
dest = "newSerialization" , 
#parser.add_option("--reuseDatasets", 
parser . add_option ( "--tasks" , 
"description.py]" , 
dest = "taskLabels" , default = [ ] , 
action = "callback" , callback = reapVarArgsCallback , 
metavar = "TASK_LABELS" ) 
parser . add_option ( "--testMode" , 
dest = "testMode" , action = "store_true" , 
default = False ) 
parser . add_option ( "--noCheckpoint" , 
help = "Don\ , 
dest = "checkpointModel" , action = "store_false" , 
default = True ) 
options , experiments = parser . parse_args ( args ) 
mutuallyExclusiveOptionCount = sum ( [ bool ( options . createCheckpointName ) , 
options . listAvailableCheckpoints , 
options . listTasks , 
bool ( options . runCheckpointName ) ] ) 
if mutuallyExclusiveOptionCount > 1 : 
~~~ _reportCommandLineUsageErrorAndExit ( 
parser , 
~~ mutuallyExclusiveOptionCount = sum ( [ bool ( not options . checkpointModel ) , 
bool ( options . createCheckpointName ) ] ) 
~~ if len ( experiments ) != 1 : 
len ( experiments ) , experiments ) ) 
~~ parser . destroy ( ) 
experimentDir = os . path . abspath ( experiments [ 0 ] ) 
privateOptions = dict ( ) 
privateOptions [ 'createCheckpointName' ] = options . createCheckpointName 
privateOptions [ 'listAvailableCheckpoints' ] = options . listAvailableCheckpoints 
privateOptions [ 'listTasks' ] = options . listTasks 
privateOptions [ 'runCheckpointName' ] = options . runCheckpointName 
privateOptions [ 'newSerialization' ] = options . newSerialization 
privateOptions [ 'testMode' ] = options . testMode 
privateOptions [ 'taskLabels' ] = options . taskLabels 
privateOptions [ 'checkpointModel' ] = options . checkpointModel 
result = ParseCommandLineOptionsResult ( experimentDir = experimentDir , 
privateOptions = privateOptions ) 
~~ def reapVarArgsCallback ( option , optStr , value , parser ) : 
newValues = [ ] 
gotDot = False 
for arg in parser . rargs : 
~~~ if arg . startswith ( "--" ) and len ( arg ) > 2 : 
~~ if arg . startswith ( "-" ) and len ( arg ) > 1 : 
~~ if arg == "." : 
~~~ gotDot = True 
~~ newValues . append ( arg ) 
~~ if not newValues : 
~~~ raise optparse . OptionValueError ( 
~~ del parser . rargs [ : len ( newValues ) + int ( gotDot ) ] 
value = getattr ( parser . values , option . dest , [ ] ) 
~~~ value = [ ] 
~~ value . extend ( newValues ) 
setattr ( parser . values , option . dest , value ) 
~~ def _reportCommandLineUsageErrorAndExit ( parser , message ) : 
print parser . get_usage ( ) 
print message 
~~ def _runExperimentImpl ( options , model = None ) : 
json_helpers . validate ( options . privateOptions , 
schemaDict = g_parsedPrivateCommandLineOptionsSchema ) 
experimentDir = options . experimentDir 
experimentDir ) 
if options . privateOptions [ 'listAvailableCheckpoints' ] : 
~~~ _printAvailableCheckpoints ( experimentDir ) 
~~ experimentTasks = expIface . getModelControl ( ) . get ( 'tasks' , [ ] ) 
if ( len ( experimentTasks ) == 0 and 
expIface . getModelControl ( ) [ 'environment' ] == OpfEnvironment . Nupic ) : 
~~~ expIface . convertNupicEnvToOPF ( ) 
experimentTasks = expIface . getModelControl ( ) . get ( 'tasks' , [ ] ) 
~~ expIface . normalizeStreamSources ( ) 
newSerialization = options . privateOptions [ 'newSerialization' ] 
if options . privateOptions [ 'listTasks' ] : 
for label in [ t [ 'taskLabel' ] for t in experimentTasks ] : 
~~~ print "\\t" , label 
~~ if options . privateOptions [ 'runCheckpointName' ] : 
~~~ assert model is None 
checkpointName = options . privateOptions [ 'runCheckpointName' ] 
model = ModelFactory . loadFromCheckpoint ( 
savedModelDir = _getModelCheckpointDir ( experimentDir , checkpointName ) , 
newSerialization = newSerialization ) 
~~ elif model is not None : 
~~~ modelDescription = expIface . getModelDescription ( ) 
model = ModelFactory . create ( modelDescription ) 
~~ if options . privateOptions [ 'createCheckpointName' ] : 
~~~ checkpointName = options . privateOptions [ 'createCheckpointName' ] 
_saveModel ( model = model , 
checkpointLabel = checkpointName , 
~~ taskIndexList = range ( len ( experimentTasks ) ) 
customTaskExecutionLabelsList = options . privateOptions [ 'taskLabels' ] 
if customTaskExecutionLabelsList : 
~~~ taskLabelsList = [ t [ 'taskLabel' ] for t in experimentTasks ] 
taskLabelsSet = set ( taskLabelsList ) 
customTaskExecutionLabelsSet = set ( customTaskExecutionLabelsList ) 
customTaskExecutionLabelsList ) 
taskIndexList = [ taskLabelsList . index ( label ) for label in 
customTaskExecutionLabelsList ] 
i in taskIndexList ] 
~~ for taskIndex in taskIndexList : 
~~~ task = experimentTasks [ taskIndex ] 
taskRunner = _TaskRunner ( model = model , 
task = task , 
cmdOptions = options ) 
taskRunner . run ( ) 
del taskRunner 
if options . privateOptions [ 'checkpointModel' ] : 
~~~ _saveModel ( model = model , 
checkpointLabel = task [ 'taskLabel' ] , 
~~ def _saveModel ( model , experimentDir , checkpointLabel , newSerialization = False ) : 
checkpointDir = _getModelCheckpointDir ( experimentDir , checkpointLabel ) 
~~~ model . writeToCheckpoint ( checkpointDir ) 
~~~ model . save ( saveModelDir = checkpointDir ) 
~~ ~~ def _getModelCheckpointDir ( experimentDir , checkpointLabel ) : 
checkpointDir = os . path . join ( getCheckpointParentDir ( experimentDir ) , 
checkpointLabel + g_defaultCheckpointExtension ) 
checkpointDir = os . path . abspath ( checkpointDir ) 
return checkpointDir 
~~ def getCheckpointParentDir ( experimentDir ) : 
baseDir = os . path . join ( experimentDir , "savedmodels" ) 
baseDir = os . path . abspath ( baseDir ) 
return baseDir 
~~ def _checkpointLabelFromCheckpointDir ( checkpointDir ) : 
assert checkpointDir . endswith ( g_defaultCheckpointExtension ) 
lastSegment = os . path . split ( checkpointDir ) [ 1 ] 
checkpointLabel = lastSegment [ 0 : - len ( g_defaultCheckpointExtension ) ] 
return checkpointLabel 
~~ def _isCheckpointDir ( checkpointDir ) : 
if lastSegment [ 0 ] == '.' : 
~~ if not checkpointDir . endswith ( g_defaultCheckpointExtension ) : 
~~ if not os . path . isdir ( checkpointDir ) : 
~~ def _printAvailableCheckpoints ( experimentDir ) : 
checkpointParentDir = getCheckpointParentDir ( experimentDir ) 
if not os . path . exists ( checkpointParentDir ) : 
~~ checkpointDirs = [ x for x in os . listdir ( checkpointParentDir ) 
if _isCheckpointDir ( os . path . join ( checkpointParentDir , x ) ) ] 
if not checkpointDirs : 
checkpointList = [ _checkpointLabelFromCheckpointDir ( x ) 
for x in checkpointDirs ] 
for checkpoint in sorted ( checkpointList ) : 
~~~ print "\\t" , checkpoint 
if self . __cmdOptions . privateOptions [ 'testMode' ] : 
~~~ numIters = 10 
~~~ numIters = self . __task [ 'iterationCount' ] 
~~ if numIters >= 0 : 
~~~ iterTracker = iter ( xrange ( numIters ) ) 
~~ periodic = PeriodicActivityMgr ( 
requestedActivities = self . _createPeriodicActivities ( ) ) 
self . __model . resetSequenceStates ( ) 
self . __taskDriver . setup ( ) 
~~~ next ( iterTracker ) 
~~~ inputRecord = self . __datasetReader . next ( ) 
~~ result = self . __taskDriver . handleInputRecord ( inputRecord = inputRecord ) 
if InferenceElement . encodings in result . inferences : 
~~ self . __predictionLogger . writeRecord ( result ) 
~~ self . _getAndEmitExperimentMetrics ( final = True ) 
self . __taskDriver . finalize ( ) 
~~ def _createPeriodicActivities ( self ) : 
periodicActivities = [ ] 
class MetricsReportCb ( object ) : 
~~~ def __init__ ( self , taskRunner ) : 
~~~ self . __taskRunner = taskRunner 
~~ def __call__ ( self ) : 
~~~ self . __taskRunner . _getAndEmitExperimentMetrics ( ) 
~~ ~~ reportMetrics = PeriodicActivityRequest ( 
repeating = True , 
period = 1000 , 
cb = MetricsReportCb ( self ) ) 
periodicActivities . append ( reportMetrics ) 
class IterationProgressCb ( object ) : 
~~~ PROGRESS_UPDATE_PERIOD_TICKS = 1000 
def __init__ ( self , taskLabel , requestedIterationCount , logger ) : 
~~~ self . __taskLabel = taskLabel 
self . __requestedIterationCount = requestedIterationCount 
self . __logger = logger 
self . __numIterationsSoFar = 0 
~~~ self . __numIterationsSoFar += self . PROGRESS_UPDATE_PERIOD_TICKS 
self . __taskLabel , 
self . __numIterationsSoFar , 
self . __requestedIterationCount ) ) 
~~ ~~ iterationProgressCb = IterationProgressCb ( 
taskLabel = self . __task [ 'taskLabel' ] , 
requestedIterationCount = self . __task [ 'iterationCount' ] , 
logger = self . __logger ) 
iterationProgressReporter = PeriodicActivityRequest ( 
period = IterationProgressCb . PROGRESS_UPDATE_PERIOD_TICKS , 
cb = iterationProgressCb ) 
periodicActivities . append ( iterationProgressReporter ) 
return periodicActivities 
~~ def _generateFile ( filename , data ) : 
numRecords , numFields = data . shape 
fields = [ ( 'field%d' % ( i + 1 ) , 'float' , '' ) for i in range ( numFields ) ] 
outFile = File ( filename , fields ) 
~~~ outFile . write ( data [ i ] . tolist ( ) ) 
~~ outFile . close ( ) 
~~ def corruptVector ( v1 , noiseLevel , numActiveCols ) : 
size = len ( v1 ) 
v2 = np . zeros ( size , dtype = "uint32" ) 
bitsToSwap = int ( noiseLevel * numActiveCols ) 
for i in range ( size ) : 
~~~ v2 [ i ] = v1 [ i ] 
~~ for _ in range ( bitsToSwap ) : 
~~~ i = random . randrange ( size ) 
if v2 [ i ] == 1 : 
~~~ v2 [ i ] = 0 
~~~ v2 [ i ] = 1 
~~ ~~ return v2 
~~ def showPredictions ( ) : 
for k in range ( 6 ) : 
~~~ tm . reset ( ) 
tm . compute ( set ( seqT [ k ] [ : ] . nonzero ( ) [ 0 ] . tolist ( ) ) , learn = False ) 
activeColumnsIndices = [ tm . columnForCell ( i ) for i in tm . getActiveCells ( ) ] 
predictedColumnIndices = [ tm . columnForCell ( i ) for i in tm . getPredictiveCells ( ) ] 
currentColumns = [ 1 if i in activeColumnsIndices else 0 for i in range ( tm . numberOfColumns ( ) ) ] 
predictedColumns = [ 1 if i in predictedColumnIndices else 0 for i in range ( tm . numberOfColumns ( ) ) ] 
print "" 
~~ ~~ def trainTM ( sequence , timeSteps , noiseLevel ) : 
currentColumns = np . zeros ( tm . numberOfColumns ( ) , dtype = "uint32" ) 
predictedColumns = np . zeros ( tm . numberOfColumns ( ) , dtype = "uint32" ) 
ts = 0 
for t in range ( timeSteps ) : 
for k in range ( 4 ) : 
~~~ v = corruptVector ( sequence [ k ] [ : ] , noiseLevel , sparseCols ) 
tm . compute ( set ( v [ : ] . nonzero ( ) [ 0 ] . tolist ( ) ) , learn = True ) 
acc = accuracy ( currentColumns , predictedColumns ) 
x . append ( ts ) 
y . append ( acc ) 
ts += 1 
~~ ~~ ~~ def encodeIntoArray ( self , inputVal , outputVal ) : 
if len ( inputVal ) != len ( outputVal ) : 
len ( inputVal ) , len ( outputVal ) ) ) 
~~ if self . w is not None and sum ( inputVal ) != self . w : 
sum ( inputVal ) , self . w ) ) 
~~ outputVal [ : ] = inputVal [ : ] 
~~~ print "input:" , inputVal , "output:" , outputVal 
print "decoded:" , self . decodedToStr ( self . decode ( outputVal ) ) 
~~ ~~ def decode ( self , encoded , parentFieldName = "" ) : 
if parentFieldName != "" : 
~~ return ( { fieldName : ( [ [ 0 , 0 ] ] , "input" ) } , [ fieldName ] ) 
return [ EncoderResult ( value = 0 , scalar = 0 , encoding = numpy . zeros ( self . n ) ) ] 
return EncoderResult ( value = 0 , scalar = 0 , 
encoding = numpy . zeros ( self . n ) ) 
~~ def closenessScores ( self , expValues , actValues , ** kwargs ) : 
ratio = 1.0 
esum = int ( expValues . sum ( ) ) 
asum = int ( actValues . sum ( ) ) 
if asum > esum : 
~~~ diff = asum - esum 
if diff < esum : 
~~~ ratio = 1 - diff / float ( esum ) 
~~~ ratio = 1 / float ( diff ) 
~~ ~~ olap = expValues & actValues 
osum = int ( olap . sum ( ) ) 
if esum == 0 : 
~~~ r = 0.0 
~~~ r = osum / float ( esum ) 
~~ r = r * ratio 
return numpy . array ( [ r ] ) 
~~ def getCallerInfo ( depth = 2 ) : 
f = sys . _getframe ( depth ) 
method_name = f . f_code . co_name 
filename = f . f_code . co_filename 
arg_class = None 
args = inspect . getargvalues ( f ) 
if len ( args [ 0 ] ) > 0 : 
arg_class = args [ 3 ] [ arg_name ] . __class__ . __name__ 
~~ return ( method_name , filename , arg_class ) 
~~ def title ( s = None , additional = '' , stream = sys . stdout ) : 
~~~ callable_name , file_name , class_name = getCallerInfo ( 2 ) 
s = callable_name 
if class_name is not None : 
~~~ s = class_name + '.' + callable_name 
~~ ~~ lines = ( s + additional ) . split ( '\\n' ) 
length = max ( len ( line ) for line in lines ) 
print >> stream , '-' * length 
print >> stream , s + additional 
~~ def getArgumentDescriptions ( f ) : 
argspec = inspect . getargspec ( f ) 
docstring = f . __doc__ 
descriptions = { } 
if docstring : 
~~~ lines = docstring . split ( '\\n' ) 
while i < len ( lines ) : 
~~~ stripped = lines [ i ] . lstrip ( ) 
if not stripped : 
~~ indentLevel = lines [ i ] . index ( stripped [ 0 ] ) 
firstWord = stripped . split ( ) [ 0 ] 
if firstWord . endswith ( ':' ) : 
~~~ firstWord = firstWord [ : - 1 ] 
~~ if firstWord in argspec . args : 
~~~ argName = firstWord 
restOfLine = stripped [ len ( firstWord ) + 1 : ] . strip ( ) 
argLines = [ restOfLine ] 
i += 1 
~~ if lines [ i ] . index ( stripped [ 0 ] ) <= indentLevel : 
~~ argLines . append ( lines [ i ] . strip ( ) ) 
~~ ~~ ~~ args = [ ] 
if argspec . defaults : 
~~~ defaultCount = len ( argspec . defaults ) 
~~~ defaultCount = 0 
~~ nonDefaultArgCount = len ( argspec . args ) - defaultCount 
for i , argName in enumerate ( argspec . args ) : 
~~~ if i >= nonDefaultArgCount : 
~~~ defaultValue = argspec . defaults [ i - nonDefaultArgCount ] 
args . append ( ( argName , descriptions . get ( argName , "" ) , defaultValue ) ) 
~~~ args . append ( ( argName , descriptions . get ( argName , "" ) ) ) 
~~ def initLogging ( verbose = False , console = 'stdout' , consoleLevel = 'DEBUG' ) : 
global gLoggingInitialized 
if gLoggingInitialized : 
~~ consoleStreamMappings = { 
'stdout' : 'stdoutConsoleHandler' , 
'stderr' : 'stderrConsoleHandler' , 
consoleLogLevels = [ 'DEBUG' , 'INFO' , 'WARNING' , 'WARN' , 'ERROR' , 'CRITICAL' , 
'FATAL' ] 
assert console is None or console in consoleStreamMappings . keys ( ) , ( 
assert consoleLevel in consoleLogLevels , ( 
configFilename = 'nupic-logging.conf' 
configFilePath = resource_filename ( "nupic.support" , configFilename ) 
configLogDir = os . environ . get ( 'NTA_LOG_DIR' , None ) 
~~~ print >> sys . stderr , ( 
~~ replacements = dict ( ) 
def makeKey ( name ) : 
return "$$%s$$" % ( name ) 
~~ platform = sys . platform . lower ( ) 
if platform . startswith ( 'java' ) : 
~~~ import java . lang 
platform = java . lang . System . getProperty ( "os.name" ) . lower ( ) 
~~~ platform = 'darwin' 
~~ ~~ if platform . startswith ( 'darwin' ) : 
~~~ replacements [ makeKey ( 'SYSLOG_HANDLER_ADDRESS' ) ] = \ 
~~ elif platform . startswith ( 'linux' ) : 
~~ elif platform . startswith ( 'win' ) : 
sys . platform , ) ) 
~~ replacements [ makeKey ( 'PERSISTENT_LOG_HANDLER' ) ] = 'fileHandler' 
if platform . startswith ( 'win' ) : 
~~~ replacements [ makeKey ( 'FILE_HANDLER_LOG_FILENAME' ) ] = \ 
~~ handlers = list ( ) 
if configLogDir is not None : 
~~~ logFilePath = _genLoggingFilePath ( ) 
makeDirectoryFromAbsolutePath ( os . path . dirname ( logFilePath ) ) 
replacements [ makeKey ( 'FILE_HANDLER_LOG_FILENAME' ) ] = repr ( logFilePath ) 
handlers . append ( replacements [ makeKey ( 'PERSISTENT_LOG_HANDLER' ) ] ) 
~~ if console is not None : 
~~~ handlers . append ( consoleStreamMappings [ console ] ) 
replacements [ makeKey ( 'CONSOLE_LOG_LEVEL' ) ] = consoleLevel 
customConfig = StringIO ( ) 
loggingFileContents = resource_string ( __name__ , configFilename ) 
for lineNum , line in enumerate ( loggingFileContents . splitlines ( ) ) : 
~~~ if "$$" in line : 
~~~ for ( key , value ) in replacements . items ( ) : 
~~~ line = line . replace ( key , value ) 
~~ ~~ if "$$" in line and "$$<key>$$" not in line : 
"dict." ) % ( line , lineNum , configFilePath ) ) 
~~ customConfig . write ( "%s\\n" % line ) 
~~ customConfig . seek ( 0 ) 
if python_version ( ) [ : 3 ] >= '2.6' : 
~~~ logging . config . fileConfig ( customConfig , disable_existing_loggers = False ) 
~~~ logging . config . fileConfig ( customConfig ) 
~~ gLoggingInitialized = True 
~~ def _genLoggingFilePath ( ) : 
appName = os . path . splitext ( os . path . basename ( sys . argv [ 0 ] ) ) [ 0 ] or 'UnknownApp' 
appLogDir = os . path . abspath ( os . path . join ( 
os . environ [ 'NTA_LOG_DIR' ] , 
'numenta-logs-%s' % ( os . environ [ 'USER' ] , ) , 
appName ) ) 
appLogFileName = '%s-%s-%s.log' % ( 
appName , long ( time . mktime ( time . gmtime ( ) ) ) , os . getpid ( ) ) 
return os . path . join ( appLogDir , appLogFileName ) 
~~ def aggregationToMonthsSeconds ( interval ) : 
seconds = interval . get ( 'microseconds' , 0 ) * 0.000001 
seconds += interval . get ( 'milliseconds' , 0 ) * 0.001 
seconds += interval . get ( 'seconds' , 0 ) 
seconds += interval . get ( 'minutes' , 0 ) * 60 
seconds += interval . get ( 'hours' , 0 ) * 60 * 60 
seconds += interval . get ( 'days' , 0 ) * 24 * 60 * 60 
seconds += interval . get ( 'weeks' , 0 ) * 7 * 24 * 60 * 60 
months = interval . get ( 'months' , 0 ) 
months += 12 * interval . get ( 'years' , 0 ) 
return { 'months' : months , 'seconds' : seconds } 
~~ def aggregationDivide ( dividend , divisor ) : 
dividendMonthSec = aggregationToMonthsSeconds ( dividend ) 
divisorMonthSec = aggregationToMonthsSeconds ( divisor ) 
if ( dividendMonthSec [ 'months' ] != 0 and divisorMonthSec [ 'seconds' ] != 0 ) or ( dividendMonthSec [ 'seconds' ] != 0 and divisorMonthSec [ 'months' ] != 0 ) : 
"months/years" ) 
~~ if dividendMonthSec [ 'months' ] > 0 : 
~~~ return float ( dividendMonthSec [ 'months' ] ) / divisor [ 'months' ] 
~~~ return float ( dividendMonthSec [ 'seconds' ] ) / divisorMonthSec [ 'seconds' ] 
~~ ~~ def validateOpfJsonValue ( value , opfJsonSchemaFilename ) : 
jsonSchemaPath = os . path . join ( os . path . dirname ( __file__ ) , 
"jsonschema" , 
opfJsonSchemaFilename ) 
jsonhelpers . validate ( value , schemaPath = jsonSchemaPath ) 
~~ def initLogger ( obj ) : 
if inspect . isclass ( obj ) : 
~~~ myClass = obj 
~~~ myClass = obj . __class__ 
~~ logger = logging . getLogger ( "." . join ( 
[ 'com.numenta' , myClass . __module__ , myClass . __name__ ] ) ) 
return logger 
~~ def matchPatterns ( patterns , keys ) : 
if patterns : 
~~~ for pattern in patterns : 
~~~ prog = re . compile ( pattern ) 
~~~ if prog . match ( key ) : 
~~~ results . append ( key ) 
~~ def _getScaledValue ( self , inpt ) : 
if inpt == SENTINEL_VALUE_FOR_MISSING_DATA : 
~~~ val = inpt 
if val < self . minval : 
~~~ val = self . minval 
~~ elif val > self . maxval : 
~~~ val = self . maxval 
~~ scaledVal = math . log10 ( val ) 
return scaledVal 
~~ ~~ def getBucketIndices ( self , inpt ) : 
scaledVal = self . _getScaledValue ( inpt ) 
if scaledVal is None : 
~~~ return self . encoder . getBucketIndices ( scaledVal ) 
~~ ~~ def encodeIntoArray ( self , inpt , output ) : 
~~~ self . encoder . encodeIntoArray ( scaledVal , output ) 
~~~ print "input:" , inpt , "scaledVal:" , scaledVal , "output:" , output 
print "decoded:" , self . decodedToStr ( self . decode ( output ) ) 
~~ ~~ ~~ def decode ( self , encoded , parentFieldName = '' ) : 
~~~ outRanges . append ( ( math . pow ( 10 , minV ) , 
math . pow ( 10 , maxV ) ) ) 
~~ desc = "" 
numRanges = len ( outRanges ) 
for i in xrange ( numRanges ) : 
~~~ if outRanges [ i ] [ 0 ] != outRanges [ i ] [ 1 ] : 
~~~ desc += "%.2f-%.2f" % ( outRanges [ i ] [ 0 ] , outRanges [ i ] [ 1 ] ) 
~~~ desc += "%.2f" % ( outRanges [ i ] [ 0 ] ) 
~~ if i < numRanges - 1 : 
~~~ scaledValues = self . encoder . getBucketValues ( ) 
for scaledValue in scaledValues : 
~~~ value = math . pow ( 10 , scaledValue ) 
self . _bucketValues . append ( value ) 
scaledResult = self . encoder . getBucketInfo ( buckets ) [ 0 ] 
scaledValue = scaledResult . value 
value = math . pow ( 10 , scaledValue ) 
return [ EncoderResult ( value = value , scalar = value , 
encoding = scaledResult . encoding ) ] 
scaledResult = self . encoder . topDownCompute ( encoded ) [ 0 ] 
return EncoderResult ( value = value , scalar = value , 
encoding = scaledResult . encoding ) 
if expValues [ 0 ] > 0 : 
~~~ expValue = math . log10 ( expValues [ 0 ] ) 
~~~ expValue = self . minScaledValue 
~~ if actValues [ 0 ] > 0 : 
~~~ actValue = math . log10 ( actValues [ 0 ] ) 
~~~ actValue = self . minScaledValue 
~~ if fractional : 
~~~ err = abs ( expValue - actValue ) 
pctErr = err / ( self . maxScaledValue - self . minScaledValue ) 
pctErr = min ( 1.0 , pctErr ) 
closeness = 1.0 - pctErr 
closeness = err 
~~ def export ( self ) : 
graph = nx . MultiDiGraph ( ) 
regions = self . network . getRegions ( ) 
for idx in xrange ( regions . getCount ( ) ) : 
~~~ regionPair = regions . getByIndex ( idx ) 
regionName = regionPair [ 0 ] 
graph . add_node ( regionName , label = regionName ) 
~~ for linkName , link in self . network . getLinks ( ) : 
~~~ graph . add_edge ( link . getSrcRegionName ( ) , 
link . getDestRegionName ( ) , 
src = link . getSrcOutputName ( ) , 
dest = link . getDestInputName ( ) ) 
~~ return graph 
~~ def bitsToString ( arr ) : 
s = array ( 'c' , '.' * len ( arr ) ) 
for i in xrange ( len ( arr ) ) : 
~~~ if arr [ i ] == 1 : 
~~~ s [ i ] = '*' 
~~ ~~ return s 
~~ def percentOverlap ( x1 , x2 , size ) : 
nonZeroX1 = np . count_nonzero ( x1 ) 
nonZeroX2 = np . count_nonzero ( x2 ) 
minX1X2 = min ( nonZeroX1 , nonZeroX2 ) 
percentOverlap = 0 
if minX1X2 > 0 : 
~~~ percentOverlap = float ( np . dot ( x1 , x2 ) ) / float ( minX1X2 ) 
~~ return percentOverlap 
~~ def resetVector ( x1 , x2 ) : 
size = len ( x1 ) 
~~~ x2 [ i ] = x1 [ i ] 
~~ ~~ def runCPU ( ) : 
model = ModelFactory . create ( model_params . MODEL_PARAMS ) 
model . enableInference ( { 'predictedField' : 'cpu' } ) 
actHistory = deque ( [ 0.0 ] * WINDOW , maxlen = 60 ) 
predHistory = deque ( [ 0.0 ] * WINDOW , maxlen = 60 ) 
actline , = plt . plot ( range ( WINDOW ) , actHistory ) 
predline , = plt . plot ( range ( WINDOW ) , predHistory ) 
actline . axes . set_ylim ( 0 , 100 ) 
predline . axes . set_ylim ( 0 , 100 ) 
~~~ s = time . time ( ) 
cpu = psutil . cpu_percent ( ) 
modelInput = { 'cpu' : cpu } 
result = shifter . shift ( model . run ( modelInput ) ) 
inference = result . inferences [ 'multiStepBestPredictions' ] [ 5 ] 
if inference is not None : 
~~~ actHistory . append ( result . rawInput [ 'cpu' ] ) 
predHistory . append ( inference ) 
plt . legend ( ( 'actual' , 'predicted' ) ) 
~~~ plt . pause ( SECONDS_PER_STEP ) 
~~ ~~ ~~ def _extractCallingMethodArgs ( ) : 
import inspect 
import copy 
callingFrame = inspect . stack ( ) [ 1 ] [ 0 ] 
argNames , _ , _ , frameLocalVarDict = inspect . getargvalues ( callingFrame ) 
argNames . remove ( "self" ) 
args = copy . copy ( frameLocalVarDict ) 
for varName in frameLocalVarDict : 
~~~ if varName not in argNames : 
~~~ args . pop ( varName ) 
super ( BacktrackingTMCPP , self ) . write ( proto . baseTM ) 
self . cells4 . write ( proto . cells4 ) 
proto . makeCells4Ephemeral = self . makeCells4Ephemeral 
proto . seed = self . seed 
proto . checkSynapseConsistency = self . checkSynapseConsistency 
proto . initArgs = json . dumps ( self . _initArgsDict ) 
obj = BacktrackingTM . read ( proto . baseTM ) 
obj . __class__ = cls 
newCells4 = Cells4 . read ( proto . cells4 ) 
print newCells4 
obj . cells4 = newCells4 
obj . makeCells4Ephemeral = proto . makeCells4Ephemeral 
obj . seed = proto . seed 
obj . checkSynapseConsistency = proto . checkSynapseConsistency 
obj . _initArgsDict = json . loads ( proto . initArgs ) 
obj . _initArgsDict [ "outputType" ] = str ( obj . _initArgsDict [ "outputType" ] ) 
obj . allocateStatesInCPP = False 
obj . retrieveLearningStates = False 
obj . _setStatePointers ( ) 
~~ def _getEphemeralMembers ( self ) : 
e = BacktrackingTM . _getEphemeralMembers ( self ) 
if self . makeCells4Ephemeral : 
~~~ e . extend ( [ 'cells4' ] ) 
~~ return e 
BacktrackingTM . _initEphemerals ( self ) 
#--------------------------------------------------------------------------------- 
self . allocateStatesInCPP = False 
self . retrieveLearningStates = False 
~~~ self . _initCells4 ( ) 
~~ ~~ def compute ( self , bottomUpInput , enableLearn , enableInference = None ) : 
assert ( bottomUpInput . dtype == numpy . dtype ( 'float32' ) ) or ( bottomUpInput . dtype == numpy . dtype ( 'uint32' ) ) or ( bottomUpInput . dtype == numpy . dtype ( 'int32' ) ) 
self . iterationIdx = self . iterationIdx + 1 
if enableInference is None : 
~~~ if enableLearn : 
~~~ enableInference = False 
~~~ enableInference = True 
~~ ~~ self . _setStatePointers ( ) 
y = self . cells4 . compute ( bottomUpInput , enableInference , enableLearn ) 
self . currentOutput = y . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) 
self . avgLearnedSeqLength = self . cells4 . getAvgLearnedSeqLength ( ) 
self . _copyAllocatedStates ( ) 
if self . collectStats : 
~~~ activeColumns = bottomUpInput . nonzero ( ) [ 0 ] 
if enableInference : 
~~~ predictedState = self . infPredictedState [ 't-1' ] 
~~~ predictedState = self . lrnPredictedState [ 't-1' ] 
~~ self . _updateStatsInferEnd ( self . _internalStats , 
predictedState , 
self . colConfidence [ 't-1' ] ) 
~~ output = self . _computeOutput ( ) 
self . printComputeEnd ( output , learn = enableLearn ) 
self . resetCalled = False 
~~ def _copyAllocatedStates ( self ) : 
if self . verbosity > 1 or self . retrieveLearningStates : 
~~~ ( activeT , activeT1 , predT , predT1 ) = self . cells4 . getLearnStates ( ) 
self . lrnActiveState [ 't-1' ] = activeT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) 
self . lrnActiveState [ 't' ] = activeT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) 
self . lrnPredictedState [ 't-1' ] = predT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) 
self . lrnPredictedState [ 't' ] = predT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) 
~~ if self . allocateStatesInCPP : 
~~~ assert False 
( activeT , activeT1 , predT , predT1 , colConfidenceT , colConfidenceT1 , confidenceT , 
confidenceT1 ) = self . cells4 . getStates ( ) 
self . cellConfidence [ 't' ] = confidenceT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) 
self . cellConfidence [ 't-1' ] = confidenceT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) 
self . colConfidence [ 't' ] = colConfidenceT . reshape ( self . numberOfCols ) 
self . colConfidence [ 't-1' ] = colConfidenceT1 . reshape ( self . numberOfCols ) 
self . infActiveState [ 't-1' ] = activeT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) 
self . infActiveState [ 't' ] = activeT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) 
self . infPredictedState [ 't-1' ] = predT1 . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) 
self . infPredictedState [ 't' ] = predT . reshape ( ( self . numberOfCols , self . cellsPerColumn ) ) 
~~ ~~ def _setStatePointers ( self ) : 
if not self . allocateStatesInCPP : 
~~~ self . cells4 . setStatePointers ( 
self . infActiveState [ "t" ] , self . infActiveState [ "t-1" ] , 
self . infPredictedState [ "t" ] , self . infPredictedState [ "t-1" ] , 
self . colConfidence [ "t" ] , self . colConfidence [ "t-1" ] , 
self . cellConfidence [ "t" ] , self . cellConfidence [ "t-1" ] ) 
if self . verbosity >= 3 : 
~~ self . _setStatePointers ( ) 
self . cells4 . reset ( ) 
BacktrackingTM . reset ( self ) 
~~ def trimSegments ( self , minPermanence = None , minNumSyns = None ) : 
if minPermanence is None : 
~~~ minPermanence = 0.0 
~~ if minNumSyns is None : 
~~~ minNumSyns = 0 
~~ if self . verbosity >= 5 : 
self . printCells ( predictedOnly = False ) 
~~ return self . cells4 . trimSegments ( minPermanence = minPermanence , minNumSyns = minNumSyns ) 
~~ def printSegmentUpdates ( self ) : 
assert False 
for key , updateList in self . segmentUpdates . iteritems ( ) : 
~~~ c , i = key [ 0 ] , key [ 1 ] 
print c , i , updateList 
~~ ~~ def _slowIsSegmentActive ( self , seg , timeStep ) : 
numSyn = seg . size ( ) 
numActiveSyns = 0 
for synIdx in xrange ( numSyn ) : 
~~~ if seg . getPermanence ( synIdx ) < self . connectedPerm : 
~~ sc , si = self . getColCellIdx ( seg . getSrcCellIdx ( synIdx ) ) 
if self . infActiveState [ timeStep ] [ sc , si ] : 
~~~ numActiveSyns += 1 
if numActiveSyns >= self . activationThreshold : 
~~ ~~ ~~ return numActiveSyns >= self . activationThreshold 
~~ def printCell ( self , c , i , onlyActiveSegments = False ) : 
nSegs = self . cells4 . nSegmentsOnCell ( c , i ) 
if nSegs > 0 : 
~~~ segList = self . cells4 . getNonEmptySegList ( c , i ) 
gidx = c * self . cellsPerColumn + i 
print "Column" , c , "Cell" , i , "(%d)" % ( gidx ) , ":" , nSegs , "segment(s)" 
for k , segIdx in enumerate ( segList ) : 
~~~ seg = self . cells4 . getSegment ( c , i , segIdx ) 
isActive = self . _slowIsSegmentActive ( seg , 't' ) 
if onlyActiveSegments and not isActive : 
print seg . size ( ) , 
print seg . isSequenceSegment ( ) , "%9.7f" % ( seg . dutyCycle ( 
self . cells4 . getNLrnIterations ( ) , False , True ) ) , 
print "(%4d/%-4d)" % ( seg . getPositiveActivations ( ) , 
seg . getTotalActivations ( ) ) , 
print "%4d" % ( self . cells4 . getNLrnIterations ( ) 
- seg . getLastActiveIteration ( ) ) , 
for s in xrange ( numSyn ) : 
~~~ sc , si = self . getColCellIdx ( seg . getSrcCellIdx ( s ) ) 
print "[%d,%d]%4.2f" % ( sc , si , seg . getPermanence ( s ) ) , 
~~ ~~ ~~ def getColCellIdx ( self , idx ) : 
c = idx // self . cellsPerColumn 
i = idx - c * self . cellsPerColumn 
return c , i 
~~ def getSegmentOnCell ( self , c , i , segIdx ) : 
segList = self . cells4 . getNonEmptySegList ( c , i ) 
seg = self . cells4 . getSegment ( c , i , segList [ segIdx ] ) 
assert numSyn != 0 
result . append ( [ int ( segIdx ) , bool ( seg . isSequenceSegment ( ) ) , 
seg . getPositiveActivations ( ) , 
seg . getTotalActivations ( ) , seg . getLastActiveIteration ( ) , 
seg . getLastPosDutyCycle ( ) , 
seg . getLastPosDutyCycleIteration ( ) ] ) 
result . append ( [ int ( sc ) , int ( si ) , seg . getPermanence ( s ) ] ) 
~~ def getSegmentInfo ( self , collectActiveData = False ) : 
assert collectActiveData == False 
nSegments , nSynapses = self . getNumSegments ( ) , self . cells4 . nSynapses ( ) 
distSegSizes , distNSegsPerCell = { } , { } 
nActiveSegs , nActiveSynapses = 0 , 0 
numAgeBuckets = 20 
distAges = [ ] 
ageBucketSize = int ( ( self . iterationIdx + 20 ) / 20 ) 
for i in range ( numAgeBuckets ) : 
~~~ distAges . append ( [ '%d-%d' % ( i * ageBucketSize , ( i + 1 ) * ageBucketSize - 1 ) , 0 ] ) 
~~ for c in xrange ( self . numberOfCols ) : 
~~~ for i in xrange ( self . cellsPerColumn ) : 
~~~ nSegmentsThisCell = self . getNumSegmentsInCell ( c , i ) 
if nSegmentsThisCell > 0 : 
~~~ if distNSegsPerCell . has_key ( nSegmentsThisCell ) : 
~~~ distNSegsPerCell [ nSegmentsThisCell ] += 1 
~~~ distNSegsPerCell [ nSegmentsThisCell ] = 1 
~~ segList = self . cells4 . getNonEmptySegList ( c , i ) 
for segIdx in xrange ( nSegmentsThisCell ) : 
~~~ seg = self . getSegmentOnCell ( c , i , segIdx ) 
nSynapsesThisSeg = len ( seg ) - 1 
if nSynapsesThisSeg > 0 : 
~~~ if distSegSizes . has_key ( nSynapsesThisSeg ) : 
~~~ distSegSizes [ nSynapsesThisSeg ] += 1 
~~~ distSegSizes [ nSynapsesThisSeg ] = 1 
~~ for syn in seg [ 1 : ] : 
~~~ p = int ( syn [ 2 ] * 10 ) 
if distPermValues . has_key ( p ) : 
~~~ distPermValues [ p ] += 1 
~~~ distPermValues [ p ] = 1 
~~ ~~ ~~ segObj = self . cells4 . getSegment ( c , i , segList [ segIdx ] ) 
age = self . iterationIdx - segObj . getLastActiveIteration ( ) 
ageBucket = int ( age / ageBucketSize ) 
distAges [ ageBucket ] [ 1 ] += 1 
~~ ~~ ~~ ~~ return ( nSegments , nSynapses , nActiveSegs , nActiveSynapses , distSegSizes , distNSegsPerCell , distPermValues , distAges ) 
~~ def main ( argv ) : 
parser = OptionParser ( helpString ) 
parser . add_option ( "--jobID" , action = "store" , type = "int" , default = None , 
parser . add_option ( "--modelID" , action = "store" , type = "str" , default = None , 
parser . add_option ( "--workerID" , action = "store" , type = "str" , default = None , 
parser . add_option ( "--params" , action = "store" , default = None , 
parser . add_option ( "--clearModels" , action = "store_true" , default = False , 
parser . add_option ( "--resetJobStatus" , action = "store_true" , default = False , 
parser . add_option ( "--logLevel" , action = "store" , type = "int" , default = None , 
( options , args ) = parser . parse_args ( argv [ 1 : ] ) 
if len ( args ) != 0 : 
~~ if ( options . jobID and options . params ) : 
~~ if ( options . jobID is None and options . params is None ) : 
~~ initLogging ( verbose = True ) 
hst = HypersearchWorker ( options , argv [ 1 : ] ) 
if options . params is None : 
~~~ jobID = hst . run ( ) 
~~~ jobID = options . jobID 
completionReason = ClientJobsDAO . CMPL_REASON_ERROR 
hst . logger . error ( completionMsg ) 
jobsDAO = ClientJobsDAO . get ( ) 
workerCmpReason = jobsDAO . jobGetFields ( options . jobID , 
~~~ jobsDAO . jobSetFields ( options . jobID , fields = dict ( 
workerCompletionMsg = completionMsg ) , 
~~~ jobID = None 
completionReason = ClientJobsDAO . CMPL_REASON_SUCCESS 
completionMsg = "Success" 
~~~ jobID = hst . _options . jobID 
~~~ if jobID is not None : 
~~~ cjDAO = ClientJobsDAO . get ( ) 
cjDAO . jobSetCompleted ( jobID = jobID , 
completionReason = completionReason , 
completionMsg = completionMsg ) 
~~ ~~ ~~ return jobID 
~~ def _processUpdatedModels ( self , cjDAO ) : 
curModelIDCtrList = cjDAO . modelsGetUpdateCounters ( self . _options . jobID ) 
if len ( curModelIDCtrList ) == 0 : 
curModelIDCtrList = sorted ( curModelIDCtrList ) 
numItems = len ( curModelIDCtrList ) 
changedEntries = filter ( lambda x : x [ 1 ] [ 1 ] != x [ 2 ] [ 1 ] , 
itertools . izip ( xrange ( numItems ) , curModelIDCtrList , 
self . _modelIDCtrList ) ) 
if len ( changedEntries ) > 0 : 
for entry in changedEntries : 
~~~ ( idx , ( modelID , curCtr ) , ( _ , oldCtr ) ) = entry 
self . _modelIDCtrDict [ modelID ] = curCtr 
assert ( self . _modelIDCtrList [ idx ] [ 0 ] == modelID ) 
assert ( curCtr != oldCtr ) 
self . _modelIDCtrList [ idx ] [ 1 ] = curCtr 
~~ changedModelIDs = [ x [ 1 ] [ 0 ] for x in changedEntries ] 
modelResults = cjDAO . modelsGetResultAndStatus ( changedModelIDs ) 
for mResult in modelResults : 
~~~ results = mResult . results 
~~~ results = json . loads ( results ) 
~~ self . _hs . recordModelProgress ( modelID = mResult . modelId , 
modelParamsHash = mResult . engParamsHash , 
results = results , 
completed = ( mResult . status == cjDAO . STATUS_COMPLETED ) , 
completionReason = mResult . completionReason , 
matured = mResult . engMatured , 
numRecords = mResult . numRecords ) 
~~ ~~ curModelIDSet = set ( [ x [ 0 ] for x in curModelIDCtrList ] ) 
newModelIDs = curModelIDSet . difference ( self . _modelIDSet ) 
if len ( newModelIDs ) > 0 : 
~~~ self . _modelIDSet . update ( newModelIDs ) 
curModelIDCtrDict = dict ( curModelIDCtrList ) 
modelInfos = cjDAO . modelsGetResultAndStatus ( newModelIDs ) 
modelInfos . sort ( ) 
modelParamsAndHashs = cjDAO . modelsGetParams ( newModelIDs ) 
modelParamsAndHashs . sort ( ) 
for ( mResult , mParamsAndHash ) in itertools . izip ( modelInfos , 
modelParamsAndHashs ) : 
~~~ modelID = mResult . modelId 
assert ( modelID == mParamsAndHash . modelId ) 
self . _modelIDCtrDict [ modelID ] = curModelIDCtrDict [ modelID ] 
self . _modelIDCtrList . append ( [ modelID , curModelIDCtrDict [ modelID ] ] ) 
results = mResult . results 
~~~ results = json . loads ( mResult . results ) 
~~ self . _hs . recordModelProgress ( modelID = modelID , 
modelParams = json . loads ( mParamsAndHash . params ) , 
modelParamsHash = mParamsAndHash . engParamsHash , 
completionReason = ( mResult . completionReason ) , 
~~ self . _modelIDCtrList . sort ( ) 
options = self . _options 
self . _workerID = cjDAO . getConnectionID ( ) 
if options . clearModels : 
~~~ cjDAO . modelsClearAll ( ) 
~~ if options . params is not None : 
params = options . params , alreadyRunning = True , 
minimumWorkers = 1 , maximumWorkers = 1 , 
jobType = cjDAO . JOB_TYPE_HS ) 
~~ if options . workerID is not None : 
~~~ wID = options . workerID 
~~~ wID = self . _workerID 
~~ buildID = Configuration . get ( 'nupic.software.buildNumber' , 'N/A' ) 
ExtendedLogger . setLogPrefix ( logPrefix ) 
if options . resetJobStatus : 
~~~ cjDAO . jobSetFields ( options . jobID , 
fields = { 'workerCompletionReason' : ClientJobsDAO . CMPL_REASON_SUCCESS , 
'cancel' : False , 
~~ jobInfo = cjDAO . jobInfo ( options . jobID ) 
jobParams = json . loads ( jobInfo . params ) 
"jobParamsSchema.json" ) 
validate ( jobParams , schemaPath = jsonSchemaPath ) 
hsVersion = jobParams . get ( 'hsVersion' , None ) 
if hsVersion == 'v2' : 
~~~ self . _hs = HypersearchV2 ( searchParams = jobParams , workerID = self . _workerID , 
cjDAO = cjDAO , jobID = options . jobID , logLevel = options . logLevel ) 
~~~ exit = False 
numModelsTotal = 0 
while not exit : 
modelIDToRun = None 
while modelIDToRun is None : 
~~~ if options . modelID is None : 
~~~ self . _processUpdatedModels ( cjDAO ) 
( exit , newModels ) = self . _hs . createModels ( numModels = batchSize ) 
if exit : 
~~ if len ( newModels ) == 0 : 
~~ for ( modelParams , modelParamsHash , particleHash ) in newModels : 
~~~ jsonModelParams = json . dumps ( modelParams ) 
( modelID , ours ) = cjDAO . modelInsertAndStart ( options . jobID , 
jsonModelParams , modelParamsHash , particleHash ) 
if not ours : 
~~~ mParamsAndHash = cjDAO . modelsGetParams ( [ modelID ] ) [ 0 ] 
mResult = cjDAO . modelsGetResultAndStatus ( [ modelID ] ) [ 0 ] 
~~ modelParams = json . loads ( mParamsAndHash . params ) 
particleHash = cjDAO . modelsGetFields ( modelID , 
[ 'engParticleHash' ] ) [ 0 ] 
particleInst = "%s.%s" % ( 
modelParams [ 'particleState' ] [ 'id' ] , 
mParamsAndHash . engParamsHash . encode ( 'hex' ) , 
particleHash . encode ( 'hex' ) , particleInst ) 
self . _hs . recordModelProgress ( modelID = modelID , 
~~~ modelIDToRun = modelID 
~~~ modelIDToRun = int ( options . modelID ) 
mParamsAndHash = cjDAO . modelsGetParams ( [ modelIDToRun ] ) [ 0 ] 
modelParams = json . loads ( mParamsAndHash . params ) 
modelParamsHash = mParamsAndHash . engParamsHash 
cjDAO . modelSetFields ( modelIDToRun , 
dict ( engWorkerConnId = self . _workerID ) ) 
if False : 
~~~ for attempt in range ( 1000 ) : 
~~~ paramsHash = hashlib . md5 ( "OrphanParams.%d.%d" % ( modelIDToRun , 
particleHash = hashlib . md5 ( "OrphanParticle.%d.%d" % ( modelIDToRun , 
~~~ cjDAO . modelSetFields ( modelIDToRun , 
~~ ( modelIDToRun , ours ) = cjDAO . modelInsertAndStart ( options . jobID , 
mParamsAndHash . params , modelParamsHash ) 
~~ ~~ ~~ if exit : 
modelIDToRun , modelParamsHash . encode ( 'hex' ) , modelParams ) 
persistentJobGUID = jobParams [ 'persistentJobGUID' ] 
modelCheckpointGUID = jobInfo . client + "_" + persistentJobGUID + ( 
'_' + str ( modelIDToRun ) ) 
self . _hs . runModel ( modelID = modelIDToRun , jobID = options . jobID , 
jobsDAO = cjDAO , modelCheckpointGUID = modelCheckpointGUID ) 
numModelsTotal += 1 
modelIDToRun , numModelsTotal ) 
print >> sys . stderr , "reporter:counter:HypersearchWorker,numModels,1" 
if options . modelID is not None : 
~~~ exit = True 
~~ ~~ ~~ finally : 
~~~ self . _hs . close ( ) 
return options . jobID 
~~ def getBucketIndices ( self , x ) : 
if ( ( isinstance ( x , float ) and math . isnan ( x ) ) or 
x == SENTINEL_VALUE_FOR_MISSING_DATA ) : 
~~ if self . _offset is None : 
~~~ self . _offset = x 
~~ bucketIdx = ( 
( self . _maxBuckets / 2 ) + int ( round ( ( x - self . _offset ) / self . resolution ) ) 
if bucketIdx < 0 : 
~~~ bucketIdx = 0 
~~ elif bucketIdx >= self . _maxBuckets : 
~~~ bucketIdx = self . _maxBuckets - 1 
~~ return [ bucketIdx ] 
~~ def mapBucketIndexToNonZeroBits ( self , index ) : 
if index < 0 : 
~~ if index >= self . _maxBuckets : 
~~~ index = self . _maxBuckets - 1 
~~ if not self . bucketMap . has_key ( index ) : 
~~ self . _createBucket ( index ) 
~~ return self . bucketMap [ index ] 
~~ def encodeIntoArray ( self , x , output ) : 
if x is not None and not isinstance ( x , numbers . Number ) : 
~~ bucketIdx = self . getBucketIndices ( x ) [ 0 ] 
output [ 0 : self . n ] = 0 
if bucketIdx is not None : 
~~~ output [ self . mapBucketIndexToNonZeroBits ( bucketIdx ) ] = 1 
~~ ~~ def _createBucket ( self , index ) : 
if index < self . minIndex : 
~~~ if index == self . minIndex - 1 : 
~~~ self . bucketMap [ index ] = self . _newRepresentation ( self . minIndex , 
index ) 
self . minIndex = index 
~~~ self . _createBucket ( index + 1 ) 
self . _createBucket ( index ) 
~~~ if index == self . maxIndex + 1 : 
~~~ self . bucketMap [ index ] = self . _newRepresentation ( self . maxIndex , 
self . maxIndex = index 
~~~ self . _createBucket ( index - 1 ) 
~~ ~~ ~~ def _newRepresentation ( self , index , newIndex ) : 
newRepresentation = self . bucketMap [ index ] . copy ( ) 
ri = newIndex % self . w 
newBit = self . random . getUInt32 ( self . n ) 
newRepresentation [ ri ] = newBit 
while newBit in self . bucketMap [ index ] or not self . _newRepresentationOK ( newRepresentation , newIndex ) : 
~~~ self . numTries += 1 
~~ return newRepresentation 
~~ def _newRepresentationOK ( self , newRep , newIndex ) : 
if newRep . size != self . w : 
~~ if ( newIndex < self . minIndex - 1 ) or ( newIndex > self . maxIndex + 1 ) : 
~~ newRepBinary = numpy . array ( [ False ] * self . n ) 
newRepBinary [ newRep ] = True 
midIdx = self . _maxBuckets / 2 
runningOverlap = self . _countOverlap ( self . bucketMap [ self . minIndex ] , newRep ) 
if not self . _overlapOK ( self . minIndex , newIndex , overlap = runningOverlap ) : 
~~ for i in range ( self . minIndex + 1 , midIdx + 1 ) : 
~~~ newBit = ( i - 1 ) % self . w 
if newRepBinary [ self . bucketMap [ i - 1 ] [ newBit ] ] : 
~~~ runningOverlap -= 1 
~~ if newRepBinary [ self . bucketMap [ i ] [ newBit ] ] : 
~~~ runningOverlap += 1 
~~ if not self . _overlapOK ( i , newIndex , overlap = runningOverlap ) : 
~~ ~~ for i in range ( midIdx + 1 , self . maxIndex + 1 ) : 
~~~ newBit = i % self . w 
~~ def _countOverlapIndices ( self , i , j ) : 
if self . bucketMap . has_key ( i ) and self . bucketMap . has_key ( j ) : 
~~~ iRep = self . bucketMap [ i ] 
jRep = self . bucketMap [ j ] 
return self . _countOverlap ( iRep , jRep ) 
~~ ~~ def _countOverlap ( rep1 , rep2 ) : 
overlap = 0 
for e in rep1 : 
~~~ if e in rep2 : 
~~~ overlap += 1 
~~ ~~ return overlap 
~~ def _overlapOK ( self , i , j , overlap = None ) : 
if overlap is None : 
~~~ overlap = self . _countOverlapIndices ( i , j ) 
~~ if abs ( i - j ) < self . w : 
~~~ if overlap == ( self . w - abs ( i - j ) ) : 
~~~ if overlap <= self . _maxOverlap : 
~~ ~~ ~~ def _initializeBucketMap ( self , maxBuckets , offset ) : 
self . _maxBuckets = maxBuckets 
self . minIndex = self . _maxBuckets / 2 
self . maxIndex = self . _maxBuckets / 2 
self . _offset = offset 
self . bucketMap = { } 
def _permutation ( n ) : 
~~~ r = numpy . arange ( n , dtype = numpy . uint32 ) 
self . random . shuffle ( r ) 
~~ self . bucketMap [ self . minIndex ] = _permutation ( self . n ) [ 0 : self . w ] 
self . numTries = 0 
~~ def retrySQL ( timeoutSec = 60 * 5 , logger = None ) : 
if logger is None : 
~~~ logger = logging . getLogger ( __name__ ) 
~~ def retryFilter ( e , args , kwargs ) : 
~~~ if isinstance ( e , ( pymysql . InternalError , pymysql . OperationalError ) ) : 
~~~ if e . args and e . args [ 0 ] in _ALL_RETRIABLE_ERROR_CODES : 
~~ ~~ elif isinstance ( e , pymysql . Error ) : 
~~~ if ( e . args and 
inspect . isclass ( e . args [ 0 ] ) and issubclass ( e . args [ 0 ] , socket_error ) ) : 
~~ retryExceptions = tuple ( [ 
pymysql . InternalError , 
pymysql . OperationalError , 
pymysql . Error , 
return make_retry_decorator ( 
timeoutSec = timeoutSec , initialRetryDelaySec = 0.1 , maxRetryDelaySec = 10 , 
retryExceptions = retryExceptions , retryFilter = retryFilter , 
logger = logger ) 
~~ def create ( * args , ** kwargs ) : 
impl = kwargs . pop ( 'implementation' , None ) 
if impl is None : 
~~~ impl = Configuration . get ( 'nupic.opf.sdrClassifier.implementation' ) 
~~ if impl == 'py' : 
~~~ return SDRClassifier ( * args , ** kwargs ) 
~~ elif impl == 'cpp' : 
~~~ return FastSDRClassifier ( * args , ** kwargs ) 
~~ elif impl == 'diff' : 
~~~ return SDRClassifierDiff ( * args , ** kwargs ) 
\ % impl ) 
~~ ~~ def read ( proto ) : 
impl = proto . implementation 
if impl == 'py' : 
~~~ return SDRClassifier . read ( proto . sdrClassifier ) 
~~~ return FastSDRClassifier . read ( proto . sdrClassifier ) 
~~~ return SDRClassifierDiff . read ( proto . sdrClassifier ) 
~~ ~~ def cross_list ( * sequences ) : 
result = [ [ ] ] 
for seq in sequences : 
~~~ result = [ sublist + [ item ] for sublist in result for item in seq ] 
~~ def cross ( * sequences ) : 
wheels = map ( iter , sequences ) 
digits = [ it . next ( ) for it in wheels ] 
~~~ yield tuple ( digits ) 
for i in range ( len ( digits ) - 1 , - 1 , - 1 ) : 
~~~ digits [ i ] = wheels [ i ] . next ( ) 
~~~ wheels [ i ] = iter ( sequences [ i ] ) 
digits [ i ] = wheels [ i ] . next ( ) 
~~ ~~ ~~ def dcross ( ** keywords ) : 
keys = keywords . keys ( ) 
sequences = [ keywords [ key ] for key in keys ] 
~~~ yield dict ( zip ( keys , digits ) ) 
~~ ~~ ~~ def mmGetMetricFromTrace ( self , trace ) : 
return Metric . createFromTrace ( trace . makeCountsTrace ( ) , 
excludeResets = self . mmGetTraceResets ( ) ) 
~~ def mmGetMetricSequencesPredictedActiveCellsPerColumn ( self ) : 
self . _mmComputeTransitionTraces ( ) 
numCellsPerColumn = [ ] 
for predictedActiveCells in ( 
self . _mmData [ "predictedActiveCellsForSequence" ] . values ( ) ) : 
~~~ cellsForColumn = self . mapCellsToColumns ( predictedActiveCells ) 
numCellsPerColumn += [ len ( x ) for x in cellsForColumn . values ( ) ] 
~~ return Metric ( self , 
numCellsPerColumn ) 
~~ def mmGetMetricSequencesPredictedActiveCellsShared ( self ) : 
numSequencesForCell = defaultdict ( lambda : 0 ) 
~~~ for cell in predictedActiveCells : 
~~~ numSequencesForCell [ cell ] += 1 
~~ ~~ return Metric ( self , 
numSequencesForCell . values ( ) ) 
~~ def mmPrettyPrintConnections ( self ) : 
text += "------------------------------------\\n" 
columns = range ( self . numberOfColumns ( ) ) 
for column in columns : 
~~~ cells = self . cellsForColumn ( column ) 
~~~ segmentDict = dict ( ) 
for seg in self . connections . segmentsForCell ( cell ) : 
~~~ synapseList = [ ] 
for synapse in self . connections . synapsesForSegment ( seg ) : 
~~~ synapseData = self . connections . dataForSynapse ( synapse ) 
synapseList . append ( 
( synapseData . presynapticCell , synapseData . permanence ) ) 
~~ synapseList . sort ( ) 
synapseStringList = [ "{0:3}={1:.2f}" . format ( sourceCell , permanence ) for 
sourceCell , permanence in synapseList ] 
column , cell , 
len ( segmentDict . values ( ) ) , 
~~ ~~ text += "------------------------------------\\n" 
~~ def mmPrettyPrintSequenceCellRepresentations ( self , sortby = "Column" ) : 
for sequenceLabel , predictedActiveCells in ( 
self . _mmData [ "predictedActiveCellsForSequence" ] . iteritems ( ) ) : 
for column , cells in cellsForColumn . iteritems ( ) : 
~~~ table . add_row ( [ sequenceLabel , column , list ( cells ) ] ) 
~~ ~~ return table . get_string ( sortby = sortby ) . encode ( "utf-8" ) 
~~ def createTemporalAnomaly ( recordParams , spatialParams = _SP_PARAMS , 
temporalParams = _TM_PARAMS , 
verbosity = _VERBOSITY ) : 
inputFilePath = recordParams [ "inputFilePath" ] 
scalarEncoderArgs = recordParams [ "scalarEncoderArgs" ] 
dateEncoderArgs = recordParams [ "dateEncoderArgs" ] 
scalarEncoder = ScalarEncoder ( ** scalarEncoderArgs ) 
dateEncoder = DateEncoder ( ** dateEncoderArgs ) 
encoder . addEncoder ( scalarEncoderArgs [ "name" ] , scalarEncoder ) 
encoder . addEncoder ( dateEncoderArgs [ "name" ] , dateEncoder ) 
json . dumps ( { "verbosity" : verbosity } ) ) 
sensor . dataSource = FileRecordStream ( streamID = inputFilePath ) 
spatialParams [ "inputWidth" ] = sensor . encoder . getWidth ( ) 
network . addRegion ( "spatialPoolerRegion" , "py.SPRegion" , 
json . dumps ( spatialParams ) ) 
json . dumps ( temporalParams ) ) 
writer . writerow ( ( i , consumption , anomalyScore ) ) 
~~ ~~ def __appendActivities ( self , periodicActivities ) : 
for req in periodicActivities : 
~~~ act = self . Activity ( repeating = req . repeating , 
period = req . period , 
cb = req . cb , 
iteratorHolder = [ iter ( xrange ( req . period - 1 ) ) ] ) 
self . __activities . append ( act ) 
~~ def add ( reader , writer , column , start , stop , value ) : 
for i , row in enumerate ( reader ) : 
~~~ if i >= start and i <= stop : 
~~~ row [ column ] = type ( value ) ( row [ column ] ) + value 
~~ writer . appendRecord ( row ) 
~~ ~~ def scale ( reader , writer , column , start , stop , multiple ) : 
~~~ row [ column ] = type ( multiple ) ( row [ column ] ) * multiple 
~~ ~~ def copy ( reader , writer , start , stop , insertLocation = None , tsCol = None ) : 
assert stop >= start 
startRows = [ ] 
copyRows = [ ] 
ts = None 
inc = None 
if tsCol is None : 
~~~ tsCol = reader . getTimestampFieldIdx ( ) 
~~ for i , row in enumerate ( reader ) : 
~~~ if ts is None : 
~~~ ts = row [ tsCol ] 
~~ elif inc is None : 
~~~ inc = row [ tsCol ] - ts 
~~ if i >= start and i <= stop : 
~~~ copyRows . append ( row ) 
~~ startRows . append ( row ) 
~~ if insertLocation is None : 
~~~ insertLocation = stop + 1 
~~ startRows [ insertLocation : insertLocation ] = copyRows 
for row in startRows : 
~~~ row [ tsCol ] = ts 
writer . appendRecord ( row ) 
ts += inc 
~~ ~~ def sample ( reader , writer , n , start = None , stop = None , tsCol = None , 
writeSampleOnly = True ) : 
rows = list ( reader ) 
if tsCol is not None : 
~~~ ts = rows [ 0 ] [ tsCol ] 
inc = rows [ 1 ] [ tsCol ] - ts 
~~ if start is None : 
~~ if stop is None : 
~~~ stop = len ( rows ) - 1 
~~ initialN = stop - start + 1 
numDeletes = initialN - n 
for i in xrange ( numDeletes ) : 
~~~ delIndex = random . randint ( start , stop - i ) 
del rows [ delIndex ] 
~~ if writeSampleOnly : 
~~~ rows = rows [ start : start + n ] 
~~ if tsCol is not None : 
~~ for row in rows : 
~~~ if tsCol is not None : 
~~ ~~ def _initEncoder ( self , w , minval , maxval , n , radius , resolution ) : 
if n != 0 : 
~~~ if ( radius != 0 or resolution != 0 ) : 
~~ assert n > w 
self . n = n 
if ( minval is not None and maxval is not None ) : 
~~~ if not self . periodic : 
~~~ self . resolution = float ( self . rangeInternal ) / ( self . n - self . w ) 
~~~ self . resolution = float ( self . rangeInternal ) / ( self . n ) 
~~ self . radius = self . w * self . resolution 
if self . periodic : 
~~~ self . range = self . rangeInternal 
~~~ self . range = self . rangeInternal + self . resolution 
~~~ if radius != 0 : 
~~~ if ( resolution != 0 ) : 
~~ self . radius = radius 
self . resolution = float ( self . radius ) / w 
~~ elif resolution != 0 : 
~~~ self . resolution = float ( resolution ) 
self . radius = self . resolution * self . w 
~~ if ( minval is not None and maxval is not None ) : 
~~~ if self . periodic : 
~~ nfloat = self . w * ( self . range / self . radius ) + 2 * self . padding 
self . n = int ( math . ceil ( nfloat ) ) 
~~ ~~ ~~ def _getFirstOnBit ( self , input ) : 
~~~ if input < self . minval : 
~~~ if self . clipInput and not self . periodic : 
~~~ if self . verbosity > 0 : 
self . minval ) 
~~ input = self . minval 
( str ( input ) , str ( self . minval ) , str ( self . maxval ) ) ) 
~~ ~~ if self . periodic : 
~~~ if input >= self . maxval : 
~~~ if input > self . maxval : 
~~~ if self . clipInput : 
self . maxval ) 
~~ input = self . maxval 
~~ ~~ ~~ if self . periodic : 
~~~ centerbin = int ( ( input - self . minval ) * self . nInternal / self . range ) + self . padding 
~~~ centerbin = int ( ( ( input - self . minval ) + self . resolution / 2 ) / self . resolution ) + self . padding 
~~ minbin = centerbin - self . halfwidth 
return [ minbin ] 
if type ( input ) is float and math . isnan ( input ) : 
~~ minbin = self . _getFirstOnBit ( input ) [ 0 ] 
~~~ bucketIdx = minbin + self . halfwidth 
~~~ bucketIdx += self . n 
~~~ bucketIdx = minbin 
~~ def encodeIntoArray ( self , input , output , learn = True ) : 
if input is not None and not isinstance ( input , numbers . Number ) : 
~~ bucketIdx = self . _getFirstOnBit ( input ) [ 0 ] 
if bucketIdx is None : 
~~~ output [ : self . n ] = 0 
minbin = bucketIdx 
maxbin = minbin + 2 * self . halfwidth 
~~~ if maxbin >= self . n : 
~~~ bottombins = maxbin - self . n + 1 
output [ : bottombins ] = 1 
maxbin = self . n - 1 
~~ if minbin < 0 : 
~~~ topbins = - minbin 
output [ self . n - topbins : self . n ] = 1 
minbin = 0 
~~ ~~ assert minbin >= 0 
assert maxbin < self . n 
output [ minbin : maxbin + 1 ] = 1 
print "input:" , input 
print "range:" , self . minval , "-" , self . maxval 
print "n:" , self . n , "w:" , self . w , "resolution:" , self . resolution , "radius" , self . radius , "periodic:" , self . periodic 
print "output:" , 
self . pprint ( output ) 
tmpOutput = numpy . array ( encoded [ : self . n ] > 0 ) . astype ( encoded . dtype ) 
if not tmpOutput . any ( ) : 
~~~ return ( dict ( ) , [ ] ) 
~~ maxZerosInARow = self . halfwidth 
for i in xrange ( maxZerosInARow ) : 
~~~ searchStr = numpy . ones ( i + 3 , dtype = encoded . dtype ) 
searchStr [ 1 : - 1 ] = 0 
subLen = len ( searchStr ) 
~~~ for j in xrange ( self . n ) : 
~~~ outputIndices = numpy . arange ( j , j + subLen ) 
outputIndices %= self . n 
if numpy . array_equal ( searchStr , tmpOutput [ outputIndices ] ) : 
~~~ tmpOutput [ outputIndices ] = 1 
~~~ for j in xrange ( self . n - subLen + 1 ) : 
~~~ if numpy . array_equal ( searchStr , tmpOutput [ j : j + subLen ] ) : 
~~~ tmpOutput [ j : j + subLen ] = 1 
~~ ~~ ~~ ~~ if self . verbosity >= 2 : 
~~ nz = tmpOutput . nonzero ( ) [ 0 ] 
run = [ nz [ 0 ] , 1 ] 
i = 1 
while ( i < len ( nz ) ) : 
~~~ if nz [ i ] == run [ 0 ] + run [ 1 ] : 
~~~ run [ 1 ] += 1 
~~~ runs . append ( run ) 
run = [ nz [ i ] , 1 ] 
~~ i += 1 
~~ runs . append ( run ) 
if self . periodic and len ( runs ) > 1 : 
~~~ if runs [ 0 ] [ 0 ] == 0 and runs [ - 1 ] [ 0 ] + runs [ - 1 ] [ 1 ] == self . n : 
~~~ runs [ - 1 ] [ 1 ] += runs [ 0 ] [ 1 ] 
runs = runs [ 1 : ] 
~~ ~~ ranges = [ ] 
for run in runs : 
~~~ ( start , runLen ) = run 
if runLen <= self . w : 
~~~ left = right = start + runLen / 2 
~~~ left = start + self . halfwidth 
right = start + runLen - 1 - self . halfwidth 
~~ if not self . periodic : 
~~~ inMin = ( left - self . padding ) * self . resolution + self . minval 
inMax = ( right - self . padding ) * self . resolution + self . minval 
~~~ inMin = ( left - self . padding ) * self . range / self . nInternal + self . minval 
inMax = ( right - self . padding ) * self . range / self . nInternal + self . minval 
~~ if self . periodic : 
~~~ if inMin >= self . maxval : 
~~~ inMin -= self . range 
inMax -= self . range 
~~ ~~ if inMin < self . minval : 
~~~ inMin = self . minval 
~~ if inMax < self . minval : 
~~~ inMax = self . minval 
~~ if self . periodic and inMax >= self . maxval : 
~~~ ranges . append ( [ inMin , self . maxval ] ) 
ranges . append ( [ self . minval , inMax - self . range ] ) 
~~~ if inMax > self . maxval : 
~~~ inMax = self . maxval 
~~ if inMin > self . maxval : 
~~~ inMin = self . maxval 
~~ ranges . append ( [ inMin , inMax ] ) 
~~ ~~ desc = self . _generateRangeDescription ( ranges ) 
~~ return ( { fieldName : ( ranges , desc ) } , [ fieldName ] ) 
~~ def _generateRangeDescription ( self , ranges ) : 
numRanges = len ( ranges ) 
~~~ if ranges [ i ] [ 0 ] != ranges [ i ] [ 1 ] : 
~~~ desc += "%.2f-%.2f" % ( ranges [ i ] [ 0 ] , ranges [ i ] [ 1 ] ) 
~~~ desc += "%.2f" % ( ranges [ i ] [ 0 ] ) 
~~~ self . _topDownValues = numpy . arange ( self . minval + self . resolution / 2.0 , 
self . maxval , 
self . resolution ) 
~~~ self . _topDownValues = numpy . arange ( self . minval , 
self . maxval + self . resolution / 2.0 , 
~~ numCategories = len ( self . _topDownValues ) 
self . _topDownMappingM = SM32 ( numCategories , self . n ) 
for i in xrange ( numCategories ) : 
~~~ value = self . _topDownValues [ i ] 
value = max ( value , self . minval ) 
value = min ( value , self . maxval ) 
self . encodeIntoArray ( value , outputSpace , learn = False ) 
~~~ topDownMappingM = self . _getTopDownMapping ( ) 
numBuckets = topDownMappingM . nRows ( ) 
for bucketIdx in range ( numBuckets ) : 
~~~ self . _bucketValues . append ( self . getBucketInfo ( [ bucketIdx ] ) [ 0 ] . value ) 
topDownMappingM = self . _getTopDownMapping ( ) 
category = buckets [ 0 ] 
encoding = self . _topDownMappingM . getRow ( category ) 
~~~ inputVal = ( self . minval + ( self . resolution / 2.0 ) + 
( category * self . resolution ) ) 
~~~ inputVal = self . minval + ( category * self . resolution ) 
~~ return [ EncoderResult ( value = inputVal , scalar = inputVal , encoding = encoding ) ] 
category = topDownMappingM . rightVecProd ( encoded ) . argmax ( ) 
return self . getBucketInfo ( [ category ] ) 
~~~ expValue = expValue % self . maxval 
actValue = actValue % self . maxval 
~~ err = abs ( expValue - actValue ) 
~~~ err = min ( err , self . maxval - err ) 
~~~ pctErr = float ( err ) / ( self . maxval - self . minval ) 
self . segmentUpdates = { } 
self . resetStats ( ) 
self . _prevInfPatterns = [ ] 
self . _prevLrnPatterns = [ ] 
stateShape = ( self . numberOfCols , self . cellsPerColumn ) 
self . lrnActiveState = { } 
self . lrnActiveState [ "t" ] = numpy . zeros ( stateShape , dtype = "int8" ) 
self . lrnActiveState [ "t-1" ] = numpy . zeros ( stateShape , dtype = "int8" ) 
self . lrnPredictedState = { } 
self . lrnPredictedState [ "t" ] = numpy . zeros ( stateShape , dtype = "int8" ) 
self . lrnPredictedState [ "t-1" ] = numpy . zeros ( stateShape , dtype = "int8" ) 
self . infActiveState = { } 
self . infActiveState [ "t" ] = numpy . zeros ( stateShape , dtype = "int8" ) 
self . infActiveState [ "t-1" ] = numpy . zeros ( stateShape , dtype = "int8" ) 
self . infActiveState [ "backup" ] = numpy . zeros ( stateShape , dtype = "int8" ) 
self . infActiveState [ "candidate" ] = numpy . zeros ( stateShape , dtype = "int8" ) 
self . infPredictedState = { } 
self . infPredictedState [ "t" ] = numpy . zeros ( stateShape , dtype = "int8" ) 
self . infPredictedState [ "t-1" ] = numpy . zeros ( stateShape , dtype = "int8" ) 
self . infPredictedState [ "backup" ] = numpy . zeros ( stateShape , dtype = "int8" ) 
self . infPredictedState [ "candidate" ] = numpy . zeros ( stateShape , dtype = "int8" ) 
self . cellConfidence = { } 
self . cellConfidence [ "t" ] = numpy . zeros ( stateShape , dtype = "float32" ) 
self . cellConfidence [ "t-1" ] = numpy . zeros ( stateShape , dtype = "float32" ) 
self . cellConfidence [ "candidate" ] = numpy . zeros ( stateShape , dtype = "float32" ) 
self . colConfidence = { } 
self . colConfidence [ "t" ] = numpy . zeros ( self . numberOfCols , dtype = "float32" ) 
self . colConfidence [ "t-1" ] = numpy . zeros ( self . numberOfCols , dtype = "float32" ) 
self . colConfidence [ "candidate" ] = numpy . zeros ( self . numberOfCols , 
dtype = "float32" ) 
proto . version = TM_VERSION 
proto . numberOfCols = self . numberOfCols 
proto . initialPerm = float ( self . initialPerm ) 
proto . connectedPerm = float ( self . connectedPerm ) 
proto . newSynapseCount = self . newSynapseCount 
proto . permanenceInc = float ( self . permanenceInc ) 
proto . permanenceDec = float ( self . permanenceDec ) 
proto . permanenceMax = float ( self . permanenceMax ) 
proto . globalDecay = float ( self . globalDecay ) 
proto . doPooling = self . doPooling 
proto . segUpdateValidDuration = self . segUpdateValidDuration 
proto . burnIn = self . burnIn 
proto . collectStats = self . collectStats 
proto . pamLength = self . pamLength 
proto . maxAge = self . maxAge 
proto . maxInfBacktrack = self . maxInfBacktrack 
proto . maxLrnBacktrack = self . maxLrnBacktrack 
proto . maxSeqLength = self . maxSeqLength 
proto . outputType = self . outputType 
proto . activeColumns = self . activeColumns 
cellListProto = proto . init ( "cells" , len ( self . cells ) ) 
for i , columnSegments in enumerate ( self . cells ) : 
~~~ columnSegmentsProto = cellListProto . init ( i , len ( columnSegments ) ) 
for j , cellSegments in enumerate ( columnSegments ) : 
~~~ cellSegmentsProto = columnSegmentsProto . init ( j , len ( cellSegments ) ) 
for k , segment in enumerate ( cellSegments ) : 
~~~ segment . write ( cellSegmentsProto [ k ] ) 
~~ ~~ ~~ proto . lrnIterationIdx = self . lrnIterationIdx 
proto . iterationIdx = self . iterationIdx 
proto . segID = self . segID 
if self . currentOutput is None : 
~~~ proto . currentOutput . none = None 
~~~ proto . currentOutput . list = self . currentOutput . tolist ( ) 
~~ proto . pamCounter = self . pamCounter 
proto . collectSequenceStats = self . collectSequenceStats 
proto . resetCalled = self . resetCalled 
proto . avgInputDensity = self . avgInputDensity or - 1.0 
proto . learnedSeqLength = self . learnedSeqLength 
proto . avgLearnedSeqLength = self . avgLearnedSeqLength 
proto . prevLrnPatterns = self . _prevLrnPatterns 
proto . prevInfPatterns = self . _prevInfPatterns 
segmentUpdatesListProto = proto . init ( "segmentUpdates" , 
len ( self . segmentUpdates ) ) 
for i , ( key , updates ) in enumerate ( self . segmentUpdates . iteritems ( ) ) : 
~~~ cellSegmentUpdatesProto = segmentUpdatesListProto [ i ] 
cellSegmentUpdatesProto . columnIdx = key [ 0 ] 
cellSegmentUpdatesProto . cellIdx = key [ 1 ] 
segmentUpdatesProto = cellSegmentUpdatesProto . init ( "segmentUpdates" , 
len ( updates ) ) 
for j , ( lrnIterationIdx , segmentUpdate ) in enumerate ( updates ) : 
~~~ segmentUpdateWrapperProto = segmentUpdatesProto [ j ] 
segmentUpdateWrapperProto . lrnIterationIdx = lrnIterationIdx 
segmentUpdate . write ( segmentUpdateWrapperProto . segmentUpdate ) 
~~ ~~ proto . cellConfidenceT = self . cellConfidence [ "t" ] . tolist ( ) 
proto . cellConfidenceT1 = self . cellConfidence [ "t-1" ] . tolist ( ) 
proto . cellConfidenceCandidate = self . cellConfidence [ "candidate" ] . tolist ( ) 
proto . colConfidenceT = self . colConfidence [ "t" ] . tolist ( ) 
proto . colConfidenceT1 = self . colConfidence [ "t-1" ] . tolist ( ) 
proto . colConfidenceCandidate = self . colConfidence [ "candidate" ] . tolist ( ) 
proto . lrnActiveStateT = self . lrnActiveState [ "t" ] . tolist ( ) 
proto . lrnActiveStateT1 = self . lrnActiveState [ "t-1" ] . tolist ( ) 
proto . infActiveStateT = self . infActiveState [ "t" ] . tolist ( ) 
proto . infActiveStateT1 = self . infActiveState [ "t-1" ] . tolist ( ) 
proto . infActiveStateBackup = self . infActiveState [ "backup" ] . tolist ( ) 
proto . infActiveStateCandidate = self . infActiveState [ "candidate" ] . tolist ( ) 
proto . lrnPredictedStateT = self . lrnPredictedState [ "t" ] . tolist ( ) 
proto . lrnPredictedStateT1 = self . lrnPredictedState [ "t-1" ] . tolist ( ) 
proto . infPredictedStateT = self . infPredictedState [ "t" ] . tolist ( ) 
proto . infPredictedStateT1 = self . infPredictedState [ "t-1" ] . tolist ( ) 
proto . infPredictedStateBackup = self . infPredictedState [ "backup" ] . tolist ( ) 
proto . infPredictedStateCandidate = self . infPredictedState [ "candidate" ] . tolist ( ) 
proto . consolePrinterVerbosity = self . consolePrinterVerbosity 
assert proto . version == TM_VERSION 
obj . _random = Random ( ) 
obj . _random . read ( proto . random ) 
obj . numberOfCols = int ( proto . numberOfCols ) 
obj . cellsPerColumn = int ( proto . cellsPerColumn ) 
obj . _numberOfCells = obj . numberOfCols * obj . cellsPerColumn 
obj . initialPerm = numpy . float32 ( proto . initialPerm ) 
obj . connectedPerm = numpy . float32 ( proto . connectedPerm ) 
obj . minThreshold = int ( proto . minThreshold ) 
obj . newSynapseCount = int ( proto . newSynapseCount ) 
obj . permanenceInc = numpy . float32 ( proto . permanenceInc ) 
obj . permanenceDec = numpy . float32 ( proto . permanenceDec ) 
obj . permanenceMax = numpy . float32 ( proto . permanenceMax ) 
obj . globalDecay = numpy . float32 ( proto . globalDecay ) 
obj . activationThreshold = int ( proto . activationThreshold ) 
obj . doPooling = proto . doPooling 
obj . segUpdateValidDuration = int ( proto . segUpdateValidDuration ) 
obj . burnIn = int ( proto . burnIn ) 
obj . collectStats = proto . collectStats 
obj . verbosity = int ( proto . verbosity ) 
obj . pamLength = int ( proto . pamLength ) 
obj . maxAge = int ( proto . maxAge ) 
obj . maxInfBacktrack = int ( proto . maxInfBacktrack ) 
obj . maxLrnBacktrack = int ( proto . maxLrnBacktrack ) 
obj . maxSeqLength = int ( proto . maxSeqLength ) 
obj . maxSegmentsPerCell = proto . maxSegmentsPerCell 
obj . maxSynapsesPerSegment = proto . maxSynapsesPerSegment 
obj . outputType = proto . outputType 
obj . activeColumns = [ int ( col ) for col in proto . activeColumns ] 
obj . cells = [ [ ] for _ in xrange ( len ( proto . cells ) ) ] 
for columnSegments , columnSegmentsProto in zip ( obj . cells , proto . cells ) : 
~~~ columnSegments . extend ( [ [ ] for _ in xrange ( len ( columnSegmentsProto ) ) ] ) 
for cellSegments , cellSegmentsProto in zip ( columnSegments , 
columnSegmentsProto ) : 
~~~ for segmentProto in cellSegmentsProto : 
~~~ segment = Segment . read ( segmentProto , obj ) 
cellSegments . append ( segment ) 
~~ ~~ ~~ obj . lrnIterationIdx = int ( proto . lrnIterationIdx ) 
obj . iterationIdx = int ( proto . iterationIdx ) 
obj . segID = int ( proto . segID ) 
obj . pamCounter = int ( proto . pamCounter ) 
obj . collectSequenceStats = proto . collectSequenceStats 
obj . resetCalled = proto . resetCalled 
avgInputDensity = proto . avgInputDensity 
if avgInputDensity < 0.0 : 
~~~ obj . avgInputDensity = None 
~~~ obj . avgInputDensity = avgInputDensity 
~~ obj . learnedSeqLength = int ( proto . learnedSeqLength ) 
obj . avgLearnedSeqLength = proto . avgLearnedSeqLength 
obj . _initEphemerals ( ) 
if proto . currentOutput . which ( ) == "none" : 
~~~ obj . currentOutput = None 
~~~ obj . currentOutput = numpy . array ( proto . currentOutput . list , 
dtype = 'float32' ) 
~~ for pattern in proto . prevLrnPatterns : 
~~~ obj . prevLrnPatterns . append ( [ v for v in pattern ] ) 
~~ for pattern in proto . prevInfPatterns : 
~~~ obj . prevInfPatterns . append ( [ v for v in pattern ] ) 
~~ for cellWrapperProto in proto . segmentUpdates : 
~~~ key = ( cellWrapperProto . columnIdx , cellWrapperProto . cellIdx ) 
value = [ ] 
for updateWrapperProto in cellWrapperProto . segmentUpdates : 
~~~ segmentUpdate = SegmentUpdate . read ( updateWrapperProto . segmentUpdate , obj ) 
value . append ( ( int ( updateWrapperProto . lrnIterationIdx ) , segmentUpdate ) ) 
~~ obj . segmentUpdates [ key ] = value 
~~ numpy . copyto ( obj . cellConfidence [ "t" ] , proto . cellConfidenceT ) 
numpy . copyto ( obj . cellConfidence [ "t-1" ] , proto . cellConfidenceT1 ) 
numpy . copyto ( obj . cellConfidence [ "candidate" ] , 
proto . cellConfidenceCandidate ) 
numpy . copyto ( obj . colConfidence [ "t" ] , proto . colConfidenceT ) 
numpy . copyto ( obj . colConfidence [ "t-1" ] , proto . colConfidenceT1 ) 
numpy . copyto ( obj . colConfidence [ "candidate" ] , proto . colConfidenceCandidate ) 
numpy . copyto ( obj . lrnActiveState [ "t" ] , proto . lrnActiveStateT ) 
numpy . copyto ( obj . lrnActiveState [ "t-1" ] , proto . lrnActiveStateT1 ) 
numpy . copyto ( obj . infActiveState [ "t" ] , proto . infActiveStateT ) 
numpy . copyto ( obj . infActiveState [ "t-1" ] , proto . infActiveStateT1 ) 
numpy . copyto ( obj . infActiveState [ "backup" ] , proto . infActiveStateBackup ) 
numpy . copyto ( obj . infActiveState [ "candidate" ] , 
proto . infActiveStateCandidate ) 
numpy . copyto ( obj . lrnPredictedState [ "t" ] , proto . lrnPredictedStateT ) 
numpy . copyto ( obj . lrnPredictedState [ "t-1" ] , proto . lrnPredictedStateT1 ) 
numpy . copyto ( obj . infPredictedState [ "t" ] , proto . infPredictedStateT ) 
numpy . copyto ( obj . infPredictedState [ "t-1" ] , proto . infPredictedStateT1 ) 
numpy . copyto ( obj . infPredictedState [ "backup" ] , 
proto . infPredictedStateBackup ) 
numpy . copyto ( obj . infPredictedState [ "candidate" ] , 
proto . infPredictedStateCandidate ) 
obj . consolePrinterVerbosity = int ( proto . consolePrinterVerbosity ) 
~~ def reset ( self , ) : 
~~ self . lrnActiveState [ 't-1' ] . fill ( 0 ) 
self . lrnActiveState [ 't' ] . fill ( 0 ) 
self . lrnPredictedState [ 't-1' ] . fill ( 0 ) 
self . lrnPredictedState [ 't' ] . fill ( 0 ) 
self . infActiveState [ 't-1' ] . fill ( 0 ) 
self . infActiveState [ 't' ] . fill ( 0 ) 
self . infPredictedState [ 't-1' ] . fill ( 0 ) 
self . infPredictedState [ 't' ] . fill ( 0 ) 
self . cellConfidence [ 't-1' ] . fill ( 0 ) 
self . cellConfidence [ 't' ] . fill ( 0 ) 
self . _internalStats [ 'nInfersSinceReset' ] = 0 
self . _internalStats [ 'curPredictionScore' ] = 0 
self . _internalStats [ 'curPredictionScore2' ] = 0 
self . _internalStats [ 'curFalseNegativeScore' ] = 0 
self . _internalStats [ 'curFalsePositiveScore' ] = 0 
self . _internalStats [ 'curMissing' ] = 0 
self . _internalStats [ 'curExtra' ] = 0 
self . _internalStats [ 'prevSequenceSignature' ] = None 
if self . collectSequenceStats : 
~~~ if self . _internalStats [ 'confHistogram' ] . sum ( ) > 0 : 
~~~ sig = self . _internalStats [ 'confHistogram' ] . copy ( ) 
sig . reshape ( self . numberOfCols * self . cellsPerColumn ) 
self . _internalStats [ 'prevSequenceSignature' ] = sig 
~~ self . _internalStats [ 'confHistogram' ] . fill ( 0 ) 
~~ self . resetCalled = True 
~~ def _updateStatsInferEnd ( self , stats , bottomUpNZ , predictedState , 
colConfidence ) : 
if not self . collectStats : 
~~ stats [ 'nInfersSinceReset' ] += 1 
( numExtra2 , numMissing2 , confidences2 ) = self . _checkPrediction ( 
patternNZs = [ bottomUpNZ ] , output = predictedState , 
colConfidence = colConfidence ) 
predictionScore , positivePredictionScore , negativePredictionScore = ( 
confidences2 [ 0 ] ) 
stats [ 'curPredictionScore2' ] = float ( predictionScore ) 
stats [ 'curFalseNegativeScore' ] = 1.0 - float ( positivePredictionScore ) 
stats [ 'curFalsePositiveScore' ] = float ( negativePredictionScore ) 
stats [ 'curMissing' ] = numMissing2 
stats [ 'curExtra' ] = numExtra2 
if stats [ 'nInfersSinceReset' ] <= self . burnIn : 
~~ stats [ 'nPredictions' ] += 1 
numExpected = max ( 1.0 , float ( len ( bottomUpNZ ) ) ) 
stats [ 'totalMissing' ] += numMissing2 
stats [ 'totalExtra' ] += numExtra2 
stats [ 'pctExtraTotal' ] += 100.0 * numExtra2 / numExpected 
stats [ 'pctMissingTotal' ] += 100.0 * numMissing2 / numExpected 
stats [ 'predictionScoreTotal2' ] += float ( predictionScore ) 
stats [ 'falseNegativeScoreTotal' ] += 1.0 - float ( positivePredictionScore ) 
stats [ 'falsePositiveScoreTotal' ] += float ( negativePredictionScore ) 
~~~ cc = self . cellConfidence [ 't-1' ] * self . infActiveState [ 't' ] 
sconf = cc . sum ( axis = 1 ) 
for c in range ( self . numberOfCols ) : 
~~~ if sconf [ c ] > 0 : 
~~~ cc [ c , : ] /= sconf [ c ] 
~~ ~~ self . _internalStats [ 'confHistogram' ] += cc 
~~ ~~ def printState ( self , aState ) : 
def formatRow ( var , i ) : 
~~~ s = '' 
~~~ if c > 0 and c % 10 == 0 : 
~~ s += str ( var [ c , i ] ) 
~~ for i in xrange ( self . cellsPerColumn ) : 
~~~ print formatRow ( aState , i ) 
~~ ~~ def printConfidence ( self , aState , maxCols = 20 ) : 
def formatFPRow ( var , i ) : 
for c in range ( min ( maxCols , self . numberOfCols ) ) : 
~~~ print formatFPRow ( aState , i ) 
~~ ~~ def printColConfidence ( self , aState , maxCols = 20 ) : 
def formatFPRow ( var ) : 
~~ print formatFPRow ( aState ) 
~~ def printStates ( self , printPrevious = True , printLearnState = True ) : 
for i in xrange ( self . cellsPerColumn ) : 
~~~ if printPrevious : 
~~~ print formatRow ( self . infActiveState [ 't-1' ] , i ) , 
~~ print formatRow ( self . infActiveState [ 't' ] , i ) 
~~~ print formatRow ( self . infPredictedState [ 't-1' ] , i ) , 
~~ print formatRow ( self . infPredictedState [ 't' ] , i ) 
~~ if printLearnState : 
~~~ print formatRow ( self . lrnActiveState [ 't-1' ] , i ) , 
~~ print formatRow ( self . lrnActiveState [ 't' ] , i ) 
~~~ print formatRow ( self . lrnPredictedState [ 't-1' ] , i ) , 
~~ print formatRow ( self . lrnPredictedState [ 't' ] , i ) 
~~ ~~ ~~ def printOutput ( self , y ) : 
print "Output" 
~~~ for c in xrange ( self . numberOfCols ) : 
~~~ print int ( y [ c , i ] ) , 
~~ ~~ def printInput ( self , x ) : 
print "Input" 
for c in xrange ( self . numberOfCols ) : 
~~~ print int ( x [ c ] ) , 
~~ def printParameters ( self ) : 
print "numberOfCols=" , self . numberOfCols 
print "cellsPerColumn=" , self . cellsPerColumn 
print "minThreshold=" , self . minThreshold 
print "newSynapseCount=" , self . newSynapseCount 
print "activationThreshold=" , self . activationThreshold 
print "initialPerm=" , self . initialPerm 
print "connectedPerm=" , self . connectedPerm 
print "permanenceInc=" , self . permanenceInc 
print "permanenceDec=" , self . permanenceDec 
print "permanenceMax=" , self . permanenceMax 
print "globalDecay=" , self . globalDecay 
print "doPooling=" , self . doPooling 
print "segUpdateValidDuration=" , self . segUpdateValidDuration 
print "pamLength=" , self . pamLength 
~~ def printActiveIndices ( self , state , andValues = False ) : 
if len ( state . shape ) == 2 : 
~~~ ( cols , cellIdxs ) = state . nonzero ( ) 
~~~ cols = state . nonzero ( ) [ 0 ] 
cellIdxs = numpy . zeros ( len ( cols ) ) 
~~ if len ( cols ) == 0 : 
~~~ print "NONE" 
~~ prevCol = - 1 
~~ if andValues : 
~~~ if len ( state . shape ) == 2 : 
~~~ value = state [ col , cellIdx ] 
~~~ value = state [ col ] 
~~~ print "%d," % ( cellIdx ) , 
~~ ~~ print "]" 
~~ def printComputeEnd ( self , output , learn = False ) : 
print "learn:" , learn 
self . infActiveState [ 't' ] . min ( axis = 1 ) . sum ( ) ) , 
self . _internalStats [ 'curPredictionScore2' ] ) , 
self . _internalStats [ 'curFalsePositiveScore' ] ) , 
1 - self . _internalStats [ 'curFalseNegativeScore' ] ) 
self . infActiveState [ 't' ] . sum ( ) ) 
self . printActiveIndices ( self . infActiveState [ 't' ] ) 
if self . verbosity >= 6 : 
~~~ self . printState ( self . infActiveState [ 't' ] ) 
self . infPredictedState [ 't' ] . sum ( ) ) 
self . printActiveIndices ( self . infPredictedState [ 't' ] ) 
~~~ self . printState ( self . infPredictedState [ 't' ] ) 
self . lrnActiveState [ 't' ] . sum ( ) ) 
self . printActiveIndices ( self . lrnActiveState [ 't' ] ) 
~~~ self . printState ( self . lrnActiveState [ 't' ] ) 
self . lrnPredictedState [ 't' ] . sum ( ) ) 
self . printActiveIndices ( self . lrnPredictedState [ 't' ] ) 
~~~ self . printState ( self . lrnPredictedState [ 't' ] ) 
self . printActiveIndices ( self . cellConfidence [ 't' ] , andValues = True ) 
~~~ self . printConfidence ( self . cellConfidence [ 't' ] ) 
self . printActiveIndices ( self . colConfidence [ 't' ] , andValues = True ) 
cc = self . cellConfidence [ 't-1' ] * self . infActiveState [ 't' ] 
self . printActiveIndices ( cc , andValues = True ) 
if self . verbosity == 4 : 
self . printCells ( predictedOnly = True ) 
~~ elif self . verbosity >= 5 : 
~~ elif self . verbosity >= 1 : 
self . printActiveIndices ( output . reshape ( self . numberOfCols , 
self . cellsPerColumn ) ) 
~~ ~~ def printCell ( self , c , i , onlyActiveSegments = False ) : 
if len ( self . cells [ c ] [ i ] ) > 0 : 
~~~ print "Column" , c , "Cell" , i , ":" , 
print len ( self . cells [ c ] [ i ] ) , "segment(s)" 
for j , s in enumerate ( self . cells [ c ] [ i ] ) : 
~~~ isActive = self . _isSegmentActive ( s , self . infActiveState [ 't' ] ) 
if not onlyActiveSegments or isActive : 
s . debugPrint ( ) 
~~ ~~ ~~ ~~ def printCells ( self , predictedOnly = False ) : 
if predictedOnly : 
~~~ if not predictedOnly or self . infPredictedState [ 't' ] [ c , i ] : 
~~~ self . printCell ( c , i , predictedOnly ) 
~~ ~~ ~~ ~~ def getSegmentOnCell ( self , c , i , segIdx ) : 
seg = self . cells [ c ] [ i ] [ segIdx ] 
retlist = [ [ seg . segID , seg . isSequenceSeg , seg . positiveActivations , 
seg . totalActivations , seg . lastActiveIteration , 
seg . _lastPosDutyCycle , seg . _lastPosDutyCycleIteration ] ] 
retlist += seg . syns 
return retlist 
~~ def _addToSegmentUpdates ( self , c , i , segUpdate ) : 
if segUpdate is None or len ( segUpdate . activeSynapses ) == 0 : 
if self . segmentUpdates . has_key ( key ) : 
~~~ self . segmentUpdates [ key ] += [ ( self . lrnIterationIdx , segUpdate ) ] 
~~~ self . segmentUpdates [ key ] = [ ( self . lrnIterationIdx , segUpdate ) ] 
~~ ~~ def _computeOutput ( self ) : 
if self . outputType == 'activeState1CellPerCol' : 
~~~ mostActiveCellPerCol = self . cellConfidence [ 't' ] . argmax ( axis = 1 ) 
self . currentOutput = numpy . zeros ( self . infActiveState [ 't' ] . shape , 
numCols = self . currentOutput . shape [ 0 ] 
self . currentOutput [ ( xrange ( numCols ) , mostActiveCellPerCol ) ] = 1 
activeCols = self . infActiveState [ 't' ] . max ( axis = 1 ) 
inactiveCols = numpy . where ( activeCols == 0 ) [ 0 ] 
self . currentOutput [ inactiveCols , : ] = 0 
~~ elif self . outputType == 'activeState' : 
~~~ self . currentOutput = self . infActiveState [ 't' ] 
~~ elif self . outputType == 'normal' : 
~~~ self . currentOutput = numpy . logical_or ( self . infPredictedState [ 't' ] , 
self . infActiveState [ 't' ] ) 
~~ return self . currentOutput . reshape ( - 1 ) . astype ( 'float32' ) 
~~ def predict ( self , nSteps ) : 
pristineTPDynamicState = self . _getTPDynamicState ( ) 
assert ( nSteps > 0 ) 
multiStepColumnPredictions = numpy . zeros ( ( nSteps , self . numberOfCols ) , 
step = 0 
~~~ multiStepColumnPredictions [ step , : ] = self . topDownCompute ( ) 
if step == nSteps - 1 : 
~~ step += 1 
self . infActiveState [ 't-1' ] [ : , : ] = self . infActiveState [ 't' ] [ : , : ] 
self . infPredictedState [ 't-1' ] [ : , : ] = self . infPredictedState [ 't' ] [ : , : ] 
self . cellConfidence [ 't-1' ] [ : , : ] = self . cellConfidence [ 't' ] [ : , : ] 
self . infActiveState [ 't' ] [ : , : ] = self . infPredictedState [ 't-1' ] [ : , : ] 
self . cellConfidence [ 't' ] . fill ( 0.0 ) 
self . _inferPhase2 ( ) 
~~ self . _setTPDynamicState ( pristineTPDynamicState ) 
return multiStepColumnPredictions 
~~ def _getTPDynamicState ( self , ) : 
tpDynamicState = dict ( ) 
for variableName in self . _getTPDynamicStateVariableNames ( ) : 
~~~ tpDynamicState [ variableName ] = copy . deepcopy ( self . __dict__ [ variableName ] ) 
~~ return tpDynamicState 
~~ def _setTPDynamicState ( self , tpDynamicState ) : 
~~~ self . __dict__ [ variableName ] = tpDynamicState . pop ( variableName ) 
~~ ~~ def _updateAvgLearnedSeqLength ( self , prevSeqLength ) : 
if self . lrnIterationIdx < 100 : 
~~~ alpha = 0.5 
~~~ alpha = 0.1 
~~ self . avgLearnedSeqLength = ( ( 1.0 - alpha ) * self . avgLearnedSeqLength + 
( alpha * prevSeqLength ) ) 
~~ def _inferBacktrack ( self , activeColumns ) : 
numPrevPatterns = len ( self . _prevInfPatterns ) 
if numPrevPatterns <= 0 : 
~~ currentTimeStepsOffset = numPrevPatterns - 1 
self . infActiveState [ 'backup' ] [ : , : ] = self . infActiveState [ 't' ] [ : , : ] 
self . infPredictedState [ 'backup' ] [ : , : ] = self . infPredictedState [ 't-1' ] [ : , : ] 
badPatterns = [ ] 
inSequence = False 
candConfidence = None 
candStartOffset = None 
for startOffset in range ( 0 , numPrevPatterns ) : 
~~~ if startOffset == currentTimeStepsOffset and candConfidence is not None : 
numPrevPatterns - 1 - startOffset ) , 
self . _prevInfPatterns [ startOffset ] ) 
~~ inSequence = False 
for offset in range ( startOffset , numPrevPatterns ) : 
~~~ if offset == currentTimeStepsOffset : 
~~~ totalConfidence = self . colConfidence [ 't' ] [ activeColumns ] . sum ( ) 
~~ self . infPredictedState [ 't-1' ] [ : , : ] = self . infPredictedState [ 't' ] [ : , : ] 
inSequence = self . _inferPhase1 ( self . _prevInfPatterns [ offset ] , 
useStartCells = ( offset == startOffset ) ) 
if not inSequence : 
self . _prevInfPatterns [ offset ] ) 
~~ inSequence = self . _inferPhase2 ( ) 
~~ ~~ if not inSequence : 
~~~ badPatterns . append ( startOffset ) 
~~ candConfidence = totalConfidence 
candStartOffset = startOffset 
if self . verbosity >= 3 and startOffset != currentTimeStepsOffset : 
totalConfidence ) 
~~ self . infActiveState [ 'candidate' ] [ : , : ] = self . infActiveState [ 't' ] [ : , : ] 
self . infPredictedState [ 'candidate' ] [ : , : ] = ( 
self . infPredictedState [ 't' ] [ : , : ] ) 
self . cellConfidence [ 'candidate' ] [ : , : ] = self . cellConfidence [ 't' ] [ : , : ] 
self . colConfidence [ 'candidate' ] [ : ] = self . colConfidence [ 't' ] [ : ] 
~~ if candStartOffset is None : 
~~~ if self . verbosity >= 3 : 
~~ self . infActiveState [ 't' ] [ : , : ] = self . infActiveState [ 'backup' ] [ : , : ] 
self . _prevInfPatterns [ candStartOffset ] ) 
~~ if candStartOffset != currentTimeStepsOffset : 
~~~ self . infActiveState [ 't' ] [ : , : ] = self . infActiveState [ 'candidate' ] [ : , : ] 
self . infPredictedState [ 't' ] [ : , : ] = ( 
self . infPredictedState [ 'candidate' ] [ : , : ] ) 
self . cellConfidence [ 't' ] [ : , : ] = self . cellConfidence [ 'candidate' ] [ : , : ] 
self . colConfidence [ 't' ] [ : ] = self . colConfidence [ 'candidate' ] [ : ] 
~~ ~~ for i in range ( numPrevPatterns ) : 
~~~ if ( i in badPatterns or 
( candStartOffset is not None and i <= candStartOffset ) ) : 
self . _prevInfPatterns [ 0 ] ) 
~~ self . _prevInfPatterns . pop ( 0 ) 
~~ ~~ self . infPredictedState [ 't-1' ] [ : , : ] = self . infPredictedState [ 'backup' ] [ : , : ] 
~~ def _inferPhase1 ( self , activeColumns , useStartCells ) : 
numPredictedColumns = 0 
if useStartCells : 
~~~ for c in activeColumns : 
~~~ self . infActiveState [ 't' ] [ c , 0 ] = 1 
~~~ predictingCells = numpy . where ( self . infPredictedState [ 't-1' ] [ c ] == 1 ) [ 0 ] 
numPredictingCells = len ( predictingCells ) 
if numPredictingCells > 0 : 
~~~ self . infActiveState [ 't' ] [ c , predictingCells ] = 1 
numPredictedColumns += 1 
~~ ~~ ~~ if useStartCells or numPredictedColumns >= 0.50 * len ( activeColumns ) : 
~~ ~~ def _inferPhase2 ( self ) : 
self . colConfidence [ 't' ] . fill ( 0 ) 
~~~ for s in self . cells [ c ] [ i ] : 
~~~ numActiveSyns = self . _getSegmentActivityLevel ( 
s , self . infActiveState [ 't' ] , connectedSynapsesOnly = False ) 
if numActiveSyns < self . activationThreshold : 
~~ if self . verbosity >= 6 : 
~~ dc = s . dutyCycle ( ) 
self . cellConfidence [ 't' ] [ c , i ] += dc 
self . colConfidence [ 't' ] [ c ] += dc 
if self . _isSegmentActive ( s , self . infActiveState [ 't' ] ) : 
~~~ self . infPredictedState [ 't' ] [ c , i ] = 1 
~~ ~~ ~~ ~~ sumConfidences = self . colConfidence [ 't' ] . sum ( ) 
if sumConfidences > 0 : 
~~~ self . colConfidence [ 't' ] /= sumConfidences 
self . cellConfidence [ 't' ] /= sumConfidences 
~~ numPredictedCols = self . infPredictedState [ 't' ] . max ( axis = 1 ) . sum ( ) 
if numPredictedCols >= 0.5 * self . avgInputDensity : 
~~ ~~ def _updateInferenceState ( self , activeColumns ) : 
self . colConfidence [ 't-1' ] [ : ] = self . colConfidence [ 't' ] [ : ] 
if self . maxInfBacktrack > 0 : 
~~~ if len ( self . _prevInfPatterns ) > self . maxInfBacktrack : 
~~~ self . _prevInfPatterns . pop ( 0 ) 
~~ self . _prevInfPatterns . append ( activeColumns ) 
~~ inSequence = self . _inferPhase1 ( activeColumns , self . resetCalled ) 
~~ self . _inferBacktrack ( activeColumns ) 
~~ ~~ def _learnBacktrackFrom ( self , startOffset , readOnly = True ) : 
numPrevPatterns = len ( self . _prevLrnPatterns ) 
currentTimeStepsOffset = numPrevPatterns - 1 
if not readOnly : 
~~~ self . segmentUpdates = { } 
~~~ if readOnly : 
self . _prevLrnPatterns [ startOffset ] ) 
~~ ~~ inSequence = True 
~~~ self . lrnPredictedState [ 't-1' ] [ : , : ] = self . lrnPredictedState [ 't' ] [ : , : ] 
self . lrnActiveState [ 't-1' ] [ : , : ] = self . lrnActiveState [ 't' ] [ : , : ] 
inputColumns = self . _prevLrnPatterns [ offset ] 
~~~ self . _processSegmentUpdates ( inputColumns ) 
~~ if offset == startOffset : 
~~~ self . lrnActiveState [ 't' ] . fill ( 0 ) 
for c in inputColumns : 
~~~ self . lrnActiveState [ 't' ] [ c , 0 ] = 1 
~~ inSequence = True 
~~~ inSequence = self . _learnPhase1 ( inputColumns , readOnly = readOnly ) 
~~ if not inSequence or offset == currentTimeStepsOffset : 
~~ self . _learnPhase2 ( readOnly = readOnly ) 
~~ return inSequence 
~~ def _learnBacktrack ( self ) : 
numPrevPatterns = len ( self . _prevLrnPatterns ) - 1 
~~ badPatterns = [ ] 
~~~ inSequence = self . _learnBacktrackFrom ( startOffset , readOnly = True ) 
if inSequence : 
~~ badPatterns . append ( startOffset ) 
~~ if not inSequence : 
~~ self . _prevLrnPatterns = [ ] 
~~ self . _learnBacktrackFrom ( startOffset , readOnly = False ) 
for i in range ( numPrevPatterns ) : 
~~~ if i in badPatterns or i <= startOffset : 
self . _prevLrnPatterns [ 0 ] ) 
~~ self . _prevLrnPatterns . pop ( 0 ) 
~~ ~~ return numPrevPatterns - startOffset 
~~ def _learnPhase1 ( self , activeColumns , readOnly = False ) : 
numUnpredictedColumns = 0 
for c in activeColumns : 
~~~ predictingCells = numpy . where ( self . lrnPredictedState [ 't-1' ] [ c ] == 1 ) [ 0 ] 
numPredictedCells = len ( predictingCells ) 
assert numPredictedCells <= 1 
if numPredictedCells == 1 : 
~~~ i = predictingCells [ 0 ] 
self . lrnActiveState [ 't' ] [ c , i ] = 1 
~~ numUnpredictedColumns += 1 
if readOnly : 
~~ i , s , numActive = self . _getBestMatchingCell ( 
c , self . lrnActiveState [ 't-1' ] , self . minThreshold ) 
if s is not None and s . isSequenceSegment ( ) : 
~~~ if self . verbosity >= 4 : 
~~ self . lrnActiveState [ 't' ] [ c , i ] = 1 
segUpdate = self . _getSegmentActiveSynapses ( 
c , i , s , self . lrnActiveState [ 't-1' ] , newSynapses = True ) 
s . totalActivations += 1 
trimSegment = self . _adaptSegment ( segUpdate ) 
if trimSegment : 
~~~ self . _trimSegmentsInCell ( c , i , [ s ] , minPermanence = 0.00001 , 
minNumSyns = 0 ) 
~~~ i = self . _getCellForNewSegment ( c ) 
if ( self . verbosity >= 4 ) : 
c , i , None , self . lrnActiveState [ 't-1' ] , newSynapses = True ) 
~~ ~~ numBottomUpColumns = len ( activeColumns ) 
if numUnpredictedColumns < numBottomUpColumns / 2 : 
~~ ~~ def _learnPhase2 ( self , readOnly = False ) : 
~~~ i , s , numActive = self . _getBestMatchingCell ( 
c , self . lrnActiveState [ 't' ] , minThreshold = self . activationThreshold ) 
if i is None : 
~~ self . lrnPredictedState [ 't' ] [ c , i ] = 1 
~~ segUpdate = self . _getSegmentActiveSynapses ( 
c , i , s , activeState = self . lrnActiveState [ 't' ] , 
newSynapses = ( numActive < self . newSynapseCount ) ) 
self . _addToSegmentUpdates ( c , i , segUpdate ) 
if self . doPooling : 
~~~ predSegment = self . _getBestMatchingSegment ( c , i , 
self . lrnActiveState [ 't-1' ] ) 
segUpdate = self . _getSegmentActiveSynapses ( c , i , predSegment , 
self . lrnActiveState [ 't-1' ] , newSynapses = True ) 
~~ ~~ ~~ def _updateLearningState ( self , activeColumns ) : 
self . lrnPredictedState [ 't-1' ] [ : , : ] = self . lrnPredictedState [ 't' ] [ : , : ] 
if self . maxLrnBacktrack > 0 : 
~~~ if len ( self . _prevLrnPatterns ) > self . maxLrnBacktrack : 
~~~ self . _prevLrnPatterns . pop ( 0 ) 
~~ self . _prevLrnPatterns . append ( activeColumns ) 
if self . verbosity >= 4 : 
print self . _prevLrnPatterns 
~~ ~~ self . _processSegmentUpdates ( activeColumns ) 
if self . pamCounter > 0 : 
~~~ self . pamCounter -= 1 
~~ self . learnedSeqLength += 1 
if not self . resetCalled : 
~~~ inSequence = self . _learnPhase1 ( activeColumns ) 
~~~ self . pamCounter = self . pamLength 
~~ ~~ if self . verbosity >= 3 : 
~~ if ( self . resetCalled or self . pamCounter == 0 or 
( self . maxSeqLength != 0 and 
self . learnedSeqLength >= self . maxSeqLength ) ) : 
~~~ if self . resetCalled : 
~~ elif self . pamCounter == 0 : 
~~ ~~ if self . pamCounter == 0 : 
~~~ seqLength = self . learnedSeqLength - self . pamLength 
~~~ seqLength = self . learnedSeqLength 
~~ self . _updateAvgLearnedSeqLength ( seqLength ) 
backSteps = 0 
~~~ backSteps = self . _learnBacktrack ( ) 
~~ if self . resetCalled or backSteps is None or backSteps == 0 : 
~~~ backSteps = 0 
~~ self . pamCounter = self . pamLength 
self . learnedSeqLength = backSteps 
~~ self . _learnPhase2 ( ) 
~~ def compute ( self , bottomUpInput , enableLearn , enableInference = None ) : 
~~ ~~ assert ( enableLearn or enableInference ) 
activeColumns = bottomUpInput . nonzero ( ) [ 0 ] 
if enableLearn : 
~~~ self . lrnIterationIdx += 1 
~~ self . iterationIdx += 1 
~~ if enableLearn : 
~~~ if self . lrnIterationIdx in Segment . dutyCycleTiers : 
~~~ for c , i in itertools . product ( xrange ( self . numberOfCols ) , 
xrange ( self . cellsPerColumn ) ) : 
~~~ for segment in self . cells [ c ] [ i ] : 
~~~ segment . dutyCycle ( ) 
~~ ~~ ~~ ~~ if self . avgInputDensity is None : 
~~~ self . avgInputDensity = len ( activeColumns ) 
~~~ self . avgInputDensity = ( 0.99 * self . avgInputDensity + 
0.01 * len ( activeColumns ) ) 
~~ if enableInference : 
~~~ self . _updateInferenceState ( activeColumns ) 
~~~ self . _updateLearningState ( activeColumns ) 
if self . globalDecay > 0.0 and ( ( self . lrnIterationIdx % self . maxAge ) == 0 ) : 
for segment in self . cells [ c ] [ i ] : 
~~~ age = self . lrnIterationIdx - segment . lastActiveIteration 
if age <= self . maxAge : 
for synapse in segment . syns : 
if synapse [ 2 ] <= 0 : 
~~ ~~ if len ( synsToDel ) == segment . getNumSynapses ( ) : 
~~ elif len ( synsToDel ) > 0 : 
~~~ segment . syns . remove ( syn ) 
~~~ self . _cleanUpdatesList ( c , i , seg ) 
self . cells [ c ] [ i ] . remove ( seg ) 
~~ ~~ ~~ ~~ if self . collectStats : 
~~~ if enableInference : 
~~ def learn ( self , bottomUpInput , enableInference = None ) : 
return self . compute ( bottomUpInput , enableLearn = True , 
enableInference = enableInference ) 
~~ def _trimSegmentsInCell ( self , colIdx , cellIdx , segList , minPermanence , 
minNumSyns ) : 
~~~ minPermanence = self . connectedPerm 
~~~ minNumSyns = self . activationThreshold 
~~ nSegsRemoved , nSynsRemoved = 0 , 0 
for segment in segList : 
~~~ synsToDel = [ syn for syn in segment . syns if syn [ 2 ] < minPermanence ] 
if len ( synsToDel ) == len ( segment . syns ) : 
~~~ if len ( synsToDel ) > 0 : 
nSynsRemoved += 1 
~~ ~~ if len ( segment . syns ) < minNumSyns : 
~~~ segsToDel . append ( segment ) 
~~ ~~ ~~ nSegsRemoved += len ( segsToDel ) 
~~~ self . _cleanUpdatesList ( colIdx , cellIdx , seg ) 
self . cells [ colIdx ] [ cellIdx ] . remove ( seg ) 
nSynsRemoved += len ( seg . syns ) 
~~ return nSegsRemoved , nSynsRemoved 
~~ totalSegsRemoved , totalSynsRemoved = 0 , 0 
for c , i in itertools . product ( xrange ( self . numberOfCols ) , 
~~~ ( segsRemoved , synsRemoved ) = self . _trimSegmentsInCell ( 
colIdx = c , cellIdx = i , segList = self . cells [ c ] [ i ] , 
minPermanence = minPermanence , minNumSyns = minNumSyns ) 
totalSegsRemoved += segsRemoved 
totalSynsRemoved += synsRemoved 
~~ return totalSegsRemoved , totalSynsRemoved 
~~ def _cleanUpdatesList ( self , col , cellIdx , seg ) : 
if c == col and i == cellIdx : 
~~~ for update in updateList : 
~~~ if update [ 1 ] . segment == seg : 
~~~ self . _removeSegmentUpdate ( update ) 
~~ ~~ ~~ ~~ ~~ def finishLearning ( self ) : 
self . trimSegments ( minPermanence = 0.0001 ) 
~~ ~~ if self . cellsPerColumn > 1 : 
~~~ assert self . getNumSegmentsInCell ( c , 0 ) == 0 
~~ ~~ ~~ def _getBestMatchingCell ( self , c , activeState , minThreshold ) : 
bestActivityInCol = minThreshold 
bestSegIdxInCol = - 1 
bestCellInCol = - 1 
~~~ maxSegActivity = 0 
maxSegIdx = 0 
~~~ activity = self . _getSegmentActivityLevel ( s , activeState ) 
if activity > maxSegActivity : 
~~~ maxSegActivity = activity 
maxSegIdx = j 
~~ ~~ if maxSegActivity >= bestActivityInCol : 
~~~ bestActivityInCol = maxSegActivity 
bestSegIdxInCol = maxSegIdx 
bestCellInCol = i 
~~ ~~ if bestCellInCol == - 1 : 
~~~ return ( None , None , None ) 
~~~ return ( bestCellInCol , self . cells [ c ] [ bestCellInCol ] [ bestSegIdxInCol ] , 
bestActivityInCol ) 
~~ ~~ def _getBestMatchingSegment ( self , c , i , activeState ) : 
maxActivity , which = self . minThreshold , - 1 
~~~ activity = self . _getSegmentActivityLevel ( s , activeState , 
connectedSynapsesOnly = False ) 
if activity >= maxActivity : 
~~~ maxActivity , which = activity , j 
~~ ~~ if which == - 1 : 
~~~ return self . cells [ c ] [ i ] [ which ] 
~~ ~~ def _getCellForNewSegment ( self , colIdx ) : 
if self . maxSegmentsPerCell < 0 : 
~~~ if self . cellsPerColumn > 1 : 
~~~ i = self . _random . getUInt32 ( self . cellsPerColumn - 1 ) + 1 
~~~ i = 0 
~~ return i 
~~ candidateCellIdxs = [ ] 
if self . cellsPerColumn == 1 : 
~~~ minIdx = 0 
maxIdx = 0 
maxIdx = self . cellsPerColumn - 1 
~~ for i in xrange ( minIdx , maxIdx + 1 ) : 
~~~ numSegs = len ( self . cells [ colIdx ] [ i ] ) 
if numSegs < self . maxSegmentsPerCell : 
~~~ candidateCellIdxs . append ( i ) 
~~ ~~ if len ( candidateCellIdxs ) > 0 : 
~~~ candidateCellIdx = ( 
candidateCellIdxs [ self . _random . getUInt32 ( len ( candidateCellIdxs ) ) ] ) 
if self . verbosity >= 5 : 
colIdx , candidateCellIdx , len ( self . cells [ colIdx ] [ candidateCellIdx ] ) ) 
~~ return candidateCellIdx 
~~ candidateSegment = None 
candidateSegmentDC = 1.0 
for i in xrange ( minIdx , maxIdx + 1 ) : 
~~~ for s in self . cells [ colIdx ] [ i ] : 
~~~ dc = s . dutyCycle ( ) 
if dc < candidateSegmentDC : 
~~~ candidateCellIdx = i 
candidateSegmentDC = dc 
candidateSegment = s 
~~ ~~ ~~ if self . verbosity >= 5 : 
"segment" % ( candidateSegment . segID , colIdx , candidateCellIdx ) ) 
candidateSegment . debugPrint ( ) 
~~ self . _cleanUpdatesList ( colIdx , candidateCellIdx , candidateSegment ) 
self . cells [ colIdx ] [ candidateCellIdx ] . remove ( candidateSegment ) 
return candidateCellIdx 
~~ def _getSegmentActiveSynapses ( self , c , i , s , activeState , newSynapses = False ) : 
activeSynapses = [ ] 
~~~ activeSynapses = [ idx for idx , syn in enumerate ( s . syns ) if activeState [ syn [ 0 ] , syn [ 1 ] ] ] 
~~~ nSynapsesToAdd = self . newSynapseCount - len ( activeSynapses ) 
activeSynapses += self . _chooseCellsToLearnFrom ( c , i , s , nSynapsesToAdd , 
activeState ) 
~~ update = BacktrackingTM . _SegmentUpdate ( c , i , s , activeSynapses ) 
return update 
~~ def _chooseCellsToLearnFrom ( self , c , i , s , n , activeState ) : 
~~ tmpCandidates = numpy . where ( activeState == 1 ) 
if len ( tmpCandidates [ 0 ] ) == 0 : 
~~~ cands = [ syn for syn in zip ( tmpCandidates [ 0 ] , tmpCandidates [ 1 ] ) ] 
~~~ synapsesAlreadyInSegment = set ( ( syn [ 0 ] , syn [ 1 ] ) for syn in s . syns ) 
cands = [ syn for syn in zip ( tmpCandidates [ 0 ] , tmpCandidates [ 1 ] ) 
if ( syn [ 0 ] , syn [ 1 ] ) not in synapsesAlreadyInSegment ] 
~~ if len ( cands ) <= n : 
~~~ return cands 
~~~ idx = self . _random . getUInt32 ( len ( cands ) ) 
~~ indices = numpy . array ( [ j for j in range ( len ( cands ) ) ] , dtype = 'uint32' ) 
tmp = numpy . zeros ( min ( n , len ( indices ) ) , dtype = 'uint32' ) 
self . _random . sample ( indices , tmp ) 
return sorted ( [ cands [ j ] for j in tmp ] ) 
~~ def _processSegmentUpdates ( self , activeColumns ) : 
removeKeys = [ ] 
trimSegments = [ ] 
if c in activeColumns : 
~~~ action = 'update' 
~~~ if self . doPooling and self . lrnPredictedState [ 't' ] [ c , i ] == 1 : 
~~~ action = 'keep' 
~~~ action = 'remove' 
~~ ~~ updateListKeep = [ ] 
if action != 'remove' : 
~~~ for ( createDate , segUpdate ) in updateList : 
print segUpdate 
~~ if self . lrnIterationIdx - createDate > self . segUpdateValidDuration : 
~~ if action == 'update' : 
~~~ trimSegment = self . _adaptSegment ( segUpdate ) 
~~~ trimSegments . append ( ( segUpdate . columnIdx , segUpdate . cellIdx , 
segUpdate . segment ) ) 
~~~ updateListKeep . append ( ( createDate , segUpdate ) ) 
~~ ~~ ~~ self . segmentUpdates [ key ] = updateListKeep 
if len ( updateListKeep ) == 0 : 
~~~ removeKeys . append ( key ) 
~~ ~~ for key in removeKeys : 
~~~ self . segmentUpdates . pop ( key ) 
~~ for ( c , i , segment ) in trimSegments : 
~~~ self . _trimSegmentsInCell ( c , i , [ segment ] , minPermanence = 0.00001 , 
~~ ~~ def _adaptSegment ( self , segUpdate ) : 
trimSegment = False 
c , i , segment = segUpdate . columnIdx , segUpdate . cellIdx , segUpdate . segment 
activeSynapses = segUpdate . activeSynapses 
synToUpdate = set ( [ syn for syn in activeSynapses if type ( syn ) == int ] ) 
segment . debugPrint ( ) 
~~ segment . lastActiveIteration = self . lrnIterationIdx 
segment . dutyCycle ( active = True ) 
lastSynIndex = len ( segment . syns ) - 1 
inactiveSynIndices = [ s for s in xrange ( 0 , lastSynIndex + 1 ) if s not in synToUpdate ] 
trimSegment = segment . updateSynapses ( inactiveSynIndices , 
- self . permanenceDec ) 
activeSynIndices = [ syn for syn in synToUpdate if syn <= lastSynIndex ] 
segment . updateSynapses ( activeSynIndices , self . permanenceInc ) 
synsToAdd = [ syn for syn in activeSynapses if type ( syn ) != int ] 
if self . maxSynapsesPerSegment > 0 and len ( synsToAdd ) + len ( segment . syns ) > self . maxSynapsesPerSegment : 
~~~ numToFree = ( len ( segment . syns ) + len ( synsToAdd ) - 
self . maxSynapsesPerSegment ) 
segment . freeNSynapses ( numToFree , inactiveSynIndices , self . verbosity ) 
~~ for newSyn in synsToAdd : 
~~~ segment . addSynapse ( newSyn [ 0 ] , newSyn [ 1 ] , self . initialPerm ) 
~~ if self . verbosity >= 4 : 
~~~ newSegment = Segment ( tm = self , isSequenceSeg = segUpdate . sequenceSegment ) 
for synapse in activeSynapses : 
~~~ newSegment . addSynapse ( synapse [ 0 ] , synapse [ 1 ] , self . initialPerm ) 
newSegment . debugPrint ( ) 
~~ self . cells [ c ] [ i ] . append ( newSegment ) 
~~ return trimSegment 
~~ def dutyCycle ( self , active = False , readOnly = False ) : 
if self . tm . lrnIterationIdx <= self . dutyCycleTiers [ 1 ] : 
~~~ dutyCycle = float ( self . positiveActivations ) / self . tm . lrnIterationIdx 
~~~ self . _lastPosDutyCycleIteration = self . tm . lrnIterationIdx 
self . _lastPosDutyCycle = dutyCycle 
~~ return dutyCycle 
~~ age = self . tm . lrnIterationIdx - self . _lastPosDutyCycleIteration 
if age == 0 and not active : 
~~~ return self . _lastPosDutyCycle 
~~ for tierIdx in range ( len ( self . dutyCycleTiers ) - 1 , 0 , - 1 ) : 
~~~ if self . tm . lrnIterationIdx > self . dutyCycleTiers [ tierIdx ] : 
~~~ alpha = self . dutyCycleAlphas [ tierIdx ] 
~~ ~~ dutyCycle = pow ( 1.0 - alpha , age ) * self . _lastPosDutyCycle 
if active : 
~~~ dutyCycle += alpha 
~~ if not readOnly : 
~~ def addSynapse ( self , srcCellCol , srcCellIdx , perm ) : 
self . syns . append ( [ int ( srcCellCol ) , int ( srcCellIdx ) , numpy . float32 ( perm ) ] ) 
~~ def logExceptions ( logger = None ) : 
logger = ( logger if logger is not None else logging . getLogger ( __name__ ) ) 
def exceptionLoggingDecorator ( func ) : 
~~~ @ functools . wraps ( func ) 
def exceptionLoggingWrap ( * args , ** kwargs ) : 
~~~ logger . exception ( 
sys . exc_info ( ) [ 1 ] , func , '' . join ( traceback . format_stack ( ) ) , ) 
~~ ~~ return exceptionLoggingWrap 
~~ return exceptionLoggingDecorator 
~~ def logEntryExit ( getLoggerCallback = logging . getLogger , 
entryExitLogLevel = logging . DEBUG , logArgs = False , 
logTraceback = False ) : 
def entryExitLoggingDecorator ( func ) : 
def entryExitLoggingWrap ( * args , ** kwargs ) : 
~~~ if entryExitLogLevel is None : 
~~~ enabled = False 
~~~ logger = getLoggerCallback ( ) 
enabled = logger . isEnabledFor ( entryExitLogLevel ) 
~~ if not enabled : 
~~ funcName = str ( func ) 
if logArgs : 
[ repr ( a ) for a in args ] + 
[ '%s=%r' % ( k , v , ) for k , v in kwargs . iteritems ( ) ] ) 
~~~ argsRepr = '' 
~~ logger . log ( 
~~~ logger . log ( 
~~ ~~ return entryExitLoggingWrap 
~~ return entryExitLoggingDecorator 
~~ def retry ( timeoutSec , initialRetryDelaySec , maxRetryDelaySec , 
retryExceptions = ( Exception , ) , 
retryFilter = lambda e , args , kwargs : True , 
logger = None , clientLabel = "" ) : 
assert initialRetryDelaySec > 0 , str ( initialRetryDelaySec ) 
assert timeoutSec >= 0 , str ( timeoutSec ) 
assert isinstance ( retryExceptions , tuple ) , ( 
~~ def retryDecorator ( func ) : 
def retryWrap ( * args , ** kwargs ) : 
~~~ numAttempts = 0 
delaySec = initialRetryDelaySec 
~~~ numAttempts += 1 
~~~ result = func ( * args , ** kwargs ) 
~~ except retryExceptions , e : 
~~~ if not retryFilter ( e , args , kwargs ) : 
~~~ if logger . isEnabledFor ( logging . DEBUG ) : 
'' . join ( traceback . format_stack ( ) ) , exc_info = True ) 
~~ now = time . time ( ) 
if now < startTime : 
~~~ startTime = now 
~~ if ( now - startTime ) >= timeoutSec : 
'' . join ( traceback . format_stack ( ) ) ) 
~~ if numAttempts == 1 : 
~~~ logger . warning ( 
timeoutSec , '' . join ( traceback . format_stack ( ) ) , exc_info = True ) 
clientLabel , func , numAttempts , delaySec , timeoutSec , 
~~ time . sleep ( delaySec ) 
delaySec = min ( delaySec * 2 , maxRetryDelaySec ) 
~~~ if numAttempts > 1 : 
clientLabel , func , numAttempts ) 
~~ ~~ ~~ return retryWrap 
~~ return retryDecorator 
~~ def getSimplePatterns ( numOnes , numPatterns , patternOverlap = 0 ) : 
assert ( patternOverlap < numOnes ) 
numNewBitsInEachPattern = numOnes - patternOverlap 
numCols = numNewBitsInEachPattern * numPatterns + patternOverlap 
p = [ ] 
for i in xrange ( numPatterns ) : 
~~~ x = numpy . zeros ( numCols , dtype = 'float32' ) 
startBit = i * numNewBitsInEachPattern 
nextStartBit = startBit + numOnes 
x [ startBit : nextStartBit ] = 1 
p . append ( x ) 
~~ def buildOverlappedSequences ( numSequences = 2 , 
seqLen = 5 , 
sharedElements = [ 3 , 4 ] , 
numOnBitsPerPattern = 3 , 
patternOverlap = 0 , 
seqOverlap = 0 , 
** kwargs 
numSharedElements = len ( sharedElements ) 
numUniqueElements = seqLen - numSharedElements 
numPatterns = numSharedElements + numUniqueElements * numSequences 
patterns = getSimplePatterns ( numOnBitsPerPattern , numPatterns , patternOverlap ) 
numCols = len ( patterns [ 0 ] ) 
trainingSequences = [ ] 
uniquePatternIndices = range ( numSharedElements , numPatterns ) 
for _ in xrange ( numSequences ) : 
~~~ sequence = [ ] 
sharedPatternIndices = range ( numSharedElements ) 
for j in xrange ( seqLen ) : 
~~~ if j in sharedElements : 
~~~ patIdx = sharedPatternIndices . pop ( 0 ) 
~~~ patIdx = uniquePatternIndices . pop ( 0 ) 
~~ sequence . append ( patterns [ patIdx ] ) 
~~ trainingSequences . append ( sequence ) 
~~ if VERBOSITY >= 3 : 
printAllTrainingSequences ( trainingSequences ) 
~~ return ( numCols , trainingSequences ) 
~~ def buildSequencePool ( numSequences = 10 , 
seqLen = [ 2 , 3 , 4 ] , 
numPatterns = 5 , 
length = random . choice ( seqLen ) 
for _ in xrange ( length ) : 
~~~ patIdx = random . choice ( xrange ( numPatterns ) ) 
sequence . append ( patterns [ patIdx ] ) 
~~ def createTMs ( includeCPP = True , 
includePy = True , 
numCols = 100 , 
cellsPerCol = 4 , 
activationThreshold = 3 , 
minThreshold = 3 , 
newSynapseCount = 3 , 
initialPerm = 0.6 , 
permanenceInc = 0.1 , 
permanenceDec = 0.0 , 
globalDecay = 0.0 , 
pamLength = 0 , 
checkSynapseConsistency = True , 
maxInfBacktrack = 0 , 
maxLrnBacktrack = 0 , 
connectedPerm = 0.5 
tms = dict ( ) 
if includeCPP : 
~~~ if VERBOSITY >= 2 : 
~~ cpp_tm = BacktrackingTMCPP ( numberOfCols = numCols , cellsPerColumn = cellsPerCol , 
initialPerm = initialPerm , connectedPerm = connectedPerm , 
minThreshold = minThreshold , newSynapseCount = newSynapseCount , 
permanenceInc = permanenceInc , permanenceDec = permanenceDec , 
activationThreshold = activationThreshold , 
globalDecay = globalDecay , burnIn = 1 , 
seed = SEED , verbosity = VERBOSITY , 
checkSynapseConsistency = checkSynapseConsistency , 
collectStats = True , 
pamLength = pamLength , 
maxInfBacktrack = maxInfBacktrack , 
maxLrnBacktrack = maxLrnBacktrack , 
cpp_tm . retrieveLearningStates = True 
tms [ 'CPP' ] = cpp_tm 
~~ if includePy : 
~~ py_tm = BacktrackingTM ( numberOfCols = numCols , 
cellsPerColumn = cellsPerCol , 
initialPerm = initialPerm , 
connectedPerm = connectedPerm , 
minThreshold = minThreshold , 
newSynapseCount = newSynapseCount , 
permanenceInc = permanenceInc , 
permanenceDec = permanenceDec , 
~~ return tms 
~~ def assertNoTMDiffs ( tms ) : 
if len ( tms ) == 1 : 
~~ if len ( tms ) > 2 : 
~~ same = fdrutils . tmDiff2 ( tms . values ( ) , verbosity = VERBOSITY ) 
assert ( same ) 
~~ def evalSequences ( tms , 
trainingSequences , 
testSequences = None , 
nTrainRepetitions = 1 , 
doResets = True , 
** kwargs ) : 
if testSequences == None : 
~~~ testSequences = trainingSequences 
~~ firstTM = tms . values ( ) [ 0 ] 
assertNoTMDiffs ( tms ) 
for trainingNum in xrange ( nTrainRepetitions ) : 
~~~ print "\\n##############################################################" 
for ( name , tm ) in tms . iteritems ( ) : 
print "---------------------" 
tm . printParameters ( ) 
~~ ~~ numSequences = len ( testSequences ) 
for sequenceNum , trainingSequence in enumerate ( trainingSequences ) : 
~~~ numTimeSteps = len ( trainingSequence ) 
if VERBOSITY >= 2 : 
~~ if doResets : 
~~~ for tm in tms . itervalues ( ) : 
~~ ~~ for t , x in enumerate ( trainingSequence ) : 
if VERBOSITY >= 3 : 
~~~ print "------------------------------------------------------------" 
firstTM . printInput ( x ) 
~~ x = numpy . array ( x ) . astype ( 'float32' ) 
for tm in tms . itervalues ( ) : 
~~~ tm . learn ( x , enableInference = True ) 
~~~ for ( name , tm ) in tms . iteritems ( ) : 
print "-------------------------------------" , 
tm . printStates ( printPrevious = ( VERBOSITY >= 5 ) ) 
~~ ~~ assertNoTMDiffs ( tms ) 
~~~ stats = tm . getStats ( ) 
numBurstingCols = tm . infActiveState [ 't' ] . min ( axis = 1 ) . sum ( ) 
~~ ~~ ~~ if VERBOSITY >= 4 : 
tm . printCells ( ) 
~~ ~~ ~~ if VERBOSITY >= 2 : 
~~ prevResult = None 
if VERBOSITY >= 1 : 
~~ if prevResult is None : 
~~~ prevResult = ( stats [ 'totalMissing' ] , stats [ 'totalExtra' ] ) 
~~~ assert ( stats [ 'totalMissing' ] == prevResult [ 0 ] ) 
assert ( stats [ 'totalExtra' ] == prevResult [ 1 ] ) 
~~ tm . resetStats ( ) 
~~ ~~ if VERBOSITY >= 3 : 
~~~ nSegsRemoved , nSynsRemoved = tm . trimSegments ( ) 
if prevResult is None : 
~~~ prevResult = ( nSegsRemoved , nSynsRemoved ) 
~~~ assert ( nSegsRemoved == prevResult [ 0 ] ) 
assert ( nSynsRemoved == prevResult [ 1 ] ) 
if VERBOSITY >= 4 : 
~~~ print "%s:" % ( name ) 
~~ ~~ if VERBOSITY >= 2 : 
~~ for tm in tms . itervalues ( ) : 
~~~ tm . resetStats ( ) 
~~ numSequences = len ( testSequences ) 
for sequenceNum , testSequence in enumerate ( testSequences ) : 
~~~ numTimeSteps = len ( testSequence ) 
~~ ~~ for t , x in enumerate ( testSequence ) : 
~~~ tm . infer ( x ) 
~~ assertNoTMDiffs ( tms ) 
tm . printStates ( printPrevious = ( VERBOSITY >= 5 ) , 
printLearnState = False ) 
print "---------------------------------" 
pprint . pprint ( tm . getStats ( ) ) 
~~ tmStats = dict ( ) 
~~~ tmStats [ name ] = stats = tm . getStats ( ) 
~~ ~~ for ( name , tm ) in tms . iteritems ( ) : 
~~~ if VERBOSITY >= 3 : 
pprint . pprint ( tmStats [ name ] ) 
~~ ~~ return tmStats 
~~ def tf_loss ( self , states , internals , reward , update , reference = None ) : 
prediction = self . predict ( states = states , internals = internals , update = update ) 
return tf . nn . l2_loss ( t = ( prediction - reward ) ) 
~~ def get_variables ( self , include_nontrainable = False ) : 
if include_nontrainable : 
~~~ return [ self . all_variables [ key ] for key in sorted ( self . all_variables ) ] 
~~~ return [ self . variables [ key ] for key in sorted ( self . variables ) ] 
~~ ~~ def from_spec ( spec , kwargs = None ) : 
baseline = util . get_object ( 
obj = spec , 
predefined_objects = tensorforce . core . baselines . baselines , 
kwargs = kwargs 
assert isinstance ( baseline , Baseline ) 
return baseline 
~~ def reset ( self ) : 
self . protocol . send ( { "cmd" : "reset" } , self . socket ) 
response = self . protocol . recv ( self . socket ) 
return self . extract_observation ( response ) 
~~ def execute ( self , action ) : 
action_mappings , axis_mappings = [ ] , [ ] 
if self . discretize_actions : 
~~~ combination = self . discretized_actions [ action ] 
for key , value in combination : 
~~~ if isinstance ( value , bool ) : 
~~~ action_mappings . append ( ( key , value ) ) 
~~~ axis_mappings . append ( ( key , value ) ) 
~~ ~~ ~~ elif action : 
~~~ action_mappings , axis_mappings = self . translate_abstract_actions_to_keys ( action ) 
~~ except KeyError as e : 
format ( e ) ) 
~~ ~~ message = dict ( 
cmd = "step" , 
delta_time = self . delta_time , 
num_ticks = self . num_ticks , 
actions = action_mappings , 
axes = axis_mappings 
self . protocol . send ( message , self . socket ) 
r = response . pop ( b"_reward" , 0.0 ) 
is_terminal = response . pop ( b"_is_terminal" , False ) 
obs = self . extract_observation ( response ) 
self . last_observation = obs 
return obs , is_terminal , r 
~~ def translate_abstract_actions_to_keys ( self , abstract ) : 
if len ( abstract ) >= 2 and not isinstance ( abstract [ 1 ] , ( list , tuple ) ) : 
~~~ abstract = list ( ( abstract , ) ) 
~~ actions , axes = [ ] , [ ] 
for a in abstract : 
~~~ first_key = self . action_space_desc [ a [ 0 ] ] [ "keys" ] [ 0 ] 
if isinstance ( first_key , ( bytes , str ) ) : 
~~~ actions . append ( ( first_key , a [ 1 ] ) ) 
~~ elif isinstance ( first_key , tuple ) : 
~~~ axes . append ( ( first_key [ 0 ] , a [ 1 ] * first_key [ 1 ] ) ) 
~~ ~~ return actions , axes 
~~ def discretize_action_space_desc ( self ) : 
unique_list = [ ] 
for nice , record in self . action_space_desc . items ( ) : 
~~~ list_for_record = [ ] 
if record [ "type" ] == "axis" : 
~~~ head_key = record [ "keys" ] [ 0 ] [ 0 ] 
head_value = record [ "keys" ] [ 0 ] [ 1 ] 
list_for_record . append ( ( head_key , 0.0 ) ) 
set_ = set ( ) 
for key_and_scale in self . action_space_desc [ nice ] [ "keys" ] : 
~~~ if key_and_scale [ 1 ] not in set_ : 
~~~ list_for_record . append ( ( head_key , key_and_scale [ 1 ] / head_value ) ) 
set_ . add ( key_and_scale [ 1 ] ) 
~~~ list_for_record = [ ( record [ "keys" ] [ 0 ] , False ) , ( record [ "keys" ] [ 0 ] , True ) ] 
~~ unique_list . append ( list_for_record ) 
~~ def so ( in_ ) : 
~~~ st = "" 
for i in in_ : 
~~~ st += str ( i [ 1 ] ) 
~~ return st 
~~ combinations = list ( itertools . product ( * unique_list ) ) 
combinations = list ( map ( lambda x : sorted ( list ( x ) , key = lambda y : y [ 0 ] ) , combinations ) ) 
combinations = sorted ( combinations , key = so ) 
self . discretized_actions = combinations 
return self . level . observations ( ) [ self . state_attribute ] 
adjusted_action = list ( ) 
for action_spec in self . level . action_spec ( ) : 
~~~ if action_spec [ 'min' ] == - 1 and action_spec [ 'max' ] == 1 : 
~~~ adjusted_action . append ( action [ action_spec [ 'name' ] ] - 1 ) 
~~ ~~ action = np . array ( adjusted_action , dtype = np . intc ) 
reward = self . level . step ( action = action , num_steps = self . repeat_action ) 
state = self . level . observations ( ) [ 'RGB_INTERLACED' ] 
terminal = not self . level . is_running ( ) 
return state , terminal , reward 
~~ def tf_solve ( self , fn_x , x_init , b ) : 
return super ( ConjugateGradient , self ) . tf_solve ( fn_x , x_init , b ) 
~~ def tf_initialize ( self , x_init , b ) : 
if x_init is None : 
~~~ x_init = [ tf . zeros ( shape = util . shape ( t ) ) for t in b ] 
~~ initial_args = super ( ConjugateGradient , self ) . tf_initialize ( x_init ) 
conjugate = residual = [ t - fx for t , fx in zip ( b , self . fn_x ( x_init ) ) ] 
squared_residual = tf . add_n ( inputs = [ tf . reduce_sum ( input_tensor = ( res * res ) ) for res in residual ] ) 
return initial_args + ( conjugate , residual , squared_residual ) 
~~ def tf_step ( self , x , iteration , conjugate , residual , squared_residual ) : 
x , next_iteration , conjugate , residual , squared_residual = super ( ConjugateGradient , self ) . tf_step ( 
x , iteration , conjugate , residual , squared_residual 
A_conjugate = self . fn_x ( conjugate ) 
if self . damping > 0.0 : 
~~~ A_conjugate = [ A_conj + self . damping * conj for A_conj , conj in zip ( A_conjugate , conjugate ) ] 
~~ conjugate_A_conjugate = tf . add_n ( 
inputs = [ tf . reduce_sum ( input_tensor = ( conj * A_conj ) ) for conj , A_conj in zip ( conjugate , A_conjugate ) ] 
alpha = squared_residual / tf . maximum ( x = conjugate_A_conjugate , y = util . epsilon ) 
next_x = [ t + alpha * conj for t , conj in zip ( x , conjugate ) ] 
next_residual = [ res - alpha * A_conj for res , A_conj in zip ( residual , A_conjugate ) ] 
next_squared_residual = tf . add_n ( inputs = [ tf . reduce_sum ( input_tensor = ( res * res ) ) for res in next_residual ] ) 
beta = next_squared_residual / tf . maximum ( x = squared_residual , y = util . epsilon ) 
next_conjugate = [ res + beta * conj for res , conj in zip ( next_residual , conjugate ) ] 
return next_x , next_iteration , next_conjugate , next_residual , next_squared_residual 
~~ def tf_next_step ( self , x , iteration , conjugate , residual , squared_residual ) : 
next_step = super ( ConjugateGradient , self ) . tf_next_step ( x , iteration , conjugate , residual , squared_residual ) 
return tf . logical_and ( x = next_step , y = ( squared_residual >= util . epsilon ) ) 
~~ def tf_step ( self , time , variables , ** kwargs ) : 
deltas = self . optimizer . step ( time = time , variables = variables , ** kwargs ) 
with tf . control_dependencies ( control_inputs = deltas ) : 
~~~ clipped_deltas = list ( ) 
exceeding_deltas = list ( ) 
for delta in deltas : 
~~~ clipped_delta = tf . clip_by_value ( 
t = delta , 
clip_value_min = - self . clipping_value , 
clip_value_max = self . clipping_value 
clipped_deltas . append ( clipped_delta ) 
exceeding_deltas . append ( clipped_delta - delta ) 
~~ ~~ applied = self . apply_step ( variables = variables , deltas = exceeding_deltas ) 
with tf . control_dependencies ( control_inputs = ( applied , ) ) : 
~~~ return [ delta + 0.0 for delta in clipped_deltas ] 
layer = util . get_object ( 
predefined_objects = tensorforce . core . networks . layers , 
assert isinstance ( layer , Layer ) 
~~ def tf_q_delta ( self , q_value , next_q_value , terminal , reward ) : 
for _ in range ( util . rank ( q_value ) - 1 ) : 
~~~ terminal = tf . expand_dims ( input = terminal , axis = 1 ) 
reward = tf . expand_dims ( input = reward , axis = 1 ) 
~~ multiples = ( 1 , ) + util . shape ( q_value ) [ 1 : ] 
terminal = tf . tile ( input = terminal , multiples = multiples ) 
reward = tf . tile ( input = reward , multiples = multiples ) 
zeros = tf . zeros_like ( tensor = next_q_value ) 
next_q_value = tf . where ( condition = terminal , x = zeros , y = ( self . discount * next_q_value ) ) 
return reward + next_q_value - q_value 
~~ def target_optimizer_arguments ( self ) : 
variables = self . target_network . get_variables ( ) + [ 
variable for name in sorted ( self . target_distributions ) 
for variable in self . target_distributions [ name ] . get_variables ( ) 
source_variables = self . network . get_variables ( ) + [ 
variable for name in sorted ( self . distributions ) 
for variable in self . distributions [ name ] . get_variables ( ) 
arguments = dict ( 
time = self . global_timestep , 
variables = variables , 
source_variables = source_variables 
if self . global_model is not None : 
~~~ arguments [ 'global_variables' ] = self . global_model . target_network . get_variables ( ) + [ 
variable for name in sorted ( self . global_model . target_distributions ) 
for variable in self . global_model . target_distributions [ name ] . get_variables ( ) 
~~ return arguments 
~~ def from_spec ( spec , kwargs ) : 
env = tensorforce . util . get_object ( 
predefined_objects = tensorforce . environments . environments , 
assert isinstance ( env , Environment ) 
global _is_sphinx 
_is_sphinx = True 
app . add_config_value ( 'no_underscore_emphasis' , False , 'env' ) 
app . add_source_parser ( '.md' , M2RParser ) 
app . add_directive ( 'mdinclude' , MdInclude ) 
~~ def output_image_link ( self , m ) : 
return self . renderer . image_link ( 
m . group ( 'url' ) , m . group ( 'target' ) , m . group ( 'alt' ) ) 
~~ def output_eol_literal_marker ( self , m ) : 
marker = ':' if m . group ( 1 ) is None else '' 
return self . renderer . eol_literal_marker ( marker ) 
~~ def header ( self , text , level , raw = None ) : 
return '\\n{0}\\n{1}\\n' . format ( text , self . hmarks [ level ] * len ( text ) ) 
~~ def list ( self , body , ordered = True ) : 
lines = body . splitlines ( ) 
for i , line in enumerate ( lines ) : 
~~~ if line and not line . startswith ( self . list_marker ) : 
~~ ~~ return '\\n{}\\n' . format ( 
'\\n' . join ( lines ) ) . replace ( self . list_marker , mark ) 
~~ def table ( self , header , body ) : 
if header and not header . isspace ( ) : 
self . _indent_block ( header ) + '\\n' ) 
~~~ table = table + '\\n' 
~~ table = table + self . _indent_block ( body ) + '\\n\\n' 
return table 
~~ def table_row ( self , content ) : 
contents = content . splitlines ( ) 
if not contents : 
if len ( contents ) > 1 : 
~~~ for c in contents [ 1 : ] : 
~~ ~~ return '\\n' . join ( clist ) + '\\n' 
~~ def codespan ( self , text ) : 
if '``' not in text : 
~~~ return self . _raw_html ( 
'</code>' . format ( text . replace ( '`' , '&#96;' ) ) ) 
~~ ~~ def link ( self , link , title , text ) : 
if title : 
~~~ raise NotImplementedError ( 'sorry' ) 
~~ def image ( self , src , title , text ) : 
return '\\n' . join ( [ 
'' , 
if not self . state . document . settings . file_insertion_enabled : 
~~~ raise self . warning ( \ % self . name ) 
~~ source = self . state_machine . input_lines . source ( 
self . lineno - self . state_machine . input_offset - 1 ) 
source_dir = os . path . dirname ( os . path . abspath ( source ) ) 
path = rst . directives . path ( self . arguments [ 0 ] ) 
path = os . path . normpath ( os . path . join ( source_dir , path ) ) 
path = utils . relative_path ( None , path ) 
path = nodes . reprunicode ( path ) 
encoding = self . options . get ( 
'encoding' , self . state . document . settings . input_encoding ) 
e_handler = self . state . document . settings . input_encoding_error_handler 
tab_width = self . options . get ( 
'tab-width' , self . state . document . settings . tab_width ) 
~~~ self . state . document . settings . record_dependencies . add ( path ) 
include_file = io . FileInput ( source_path = path , 
encoding = encoding , 
error_handler = e_handler ) 
~~ except UnicodeEncodeError as error : 
~~~ raise self . severe ( \ 
( self . name , SafeString ( path ) ) ) 
~~ except IOError as error : 
~~~ raise self . severe ( \ % 
( self . name , ErrorString ( error ) ) ) 
~~~ rawtext = include_file . read ( ) 
~~ except UnicodeError as error : 
~~ config = self . state . document . settings . env . config 
converter = M2R ( no_underscore_emphasis = config . no_underscore_emphasis ) 
include_lines = statemachine . string2lines ( converter ( rawtext ) , 
tab_width , 
convert_whitespace = True ) 
self . state_machine . insert_input ( include_lines , path ) 
return [ ] 
~~ def WorkerAgentGenerator ( agent_class ) : 
if isinstance ( agent_class , str ) : 
~~~ agent_class = AgentsDictionary . get ( agent_class ) 
if not agent_class and agent_class . find ( '.' ) != - 1 : 
~~~ module_name , function_name = agent_class . rsplit ( '.' , 1 ) 
module = importlib . import_module ( module_name ) 
agent_class = getattr ( module , function_name ) 
~~ ~~ class WorkerAgent ( agent_class ) : 
def __init__ ( self , model = None , ** kwargs ) : 
~~~ self . model = model 
if not issubclass ( agent_class , LearningAgent ) : 
~~~ kwargs . pop ( "network" ) 
~~ super ( WorkerAgent , self ) . __init__ ( ** kwargs ) 
~~ def initialize_model ( self ) : 
~~~ return self . model 
~~ ~~ return WorkerAgent 
~~ def clone_worker_agent ( agent , factor , environment , network , agent_config ) : 
ret = [ agent ] 
for i in xrange ( factor - 1 ) : 
~~~ worker = WorkerAgentGenerator ( type ( agent ) ) ( 
states = environment . states , 
actions = environment . actions , 
network = network , 
model = agent . model , 
** agent_config 
ret . append ( worker ) 
~~ def run ( 
self , 
num_episodes = - 1 , 
max_episode_timesteps = - 1 , 
episode_finished = None , 
summary_report = None , 
summary_interval = 0 , 
num_timesteps = None , 
deterministic = False , 
episodes = None , 
max_timesteps = None , 
testing = False , 
sleep = None 
if episodes is not None : 
~~~ num_episodes = episodes 
category = DeprecationWarning ) 
~~ assert isinstance ( num_episodes , int ) 
if max_timesteps is not None : 
~~~ max_episode_timesteps = max_timesteps 
~~ assert isinstance ( max_episode_timesteps , int ) 
if summary_report is not None : 
~~ self . reset ( ) 
self . global_episode = 0 
self . global_timestep = 0 
self . should_stop = False 
threads = [ threading . Thread ( target = self . _run_single , args = ( t , self . agent [ t ] , self . environment [ t ] , ) , 
kwargs = { "deterministic" : deterministic , 
"max_episode_timesteps" : max_episode_timesteps , 
"episode_finished" : episode_finished , 
"testing" : testing , 
"sleep" : sleep } ) 
for t in range ( len ( self . agent ) ) ] 
self . start_time = time . time ( ) 
[ t . start ( ) for t in threads ] 
~~~ next_summary = 0 
next_save = 0 if self . save_frequency_unit != "s" else time . time ( ) 
while any ( [ t . is_alive ( ) for t in threads ] ) and self . global_episode < num_episodes or num_episodes == - 1 : 
~~~ self . time = time . time ( ) 
if summary_report is not None and self . global_episode > next_summary : 
~~~ summary_report ( self ) 
next_summary += summary_interval 
~~ if self . save_path and self . save_frequency is not None : 
~~~ do_save = True 
current = None 
if self . save_frequency_unit == "e" and self . global_episode > next_save : 
~~~ current = self . global_episode 
~~ elif self . save_frequency_unit == "s" and self . time > next_save : 
~~~ current = self . time 
~~ elif self . save_frequency_unit == "t" and self . global_timestep > next_save : 
~~~ current = self . global_timestep 
~~~ do_save = False 
~~ if do_save : 
~~~ self . agent [ 0 ] . save_model ( self . save_path ) 
while next_save < current : 
~~~ next_save += self . save_frequency 
~~ ~~ ~~ time . sleep ( 1 ) 
~~ self . should_stop = True 
[ t . join ( ) for t in threads ] 
~~ def _run_single ( self , thread_id , agent , environment , deterministic = False , 
max_episode_timesteps = - 1 , episode_finished = None , testing = False , sleep = None ) : 
old_episode_finished = False 
if episode_finished is not None and len ( getargspec ( episode_finished ) . args ) == 1 : 
~~~ old_episode_finished = True 
~~ episode = 0 
while not self . should_stop : 
~~~ state = environment . reset ( ) 
agent . reset ( ) 
self . global_timestep , self . global_episode = agent . timestep , agent . episode 
episode_reward = 0 
time_step = 0 
time_start = time . time ( ) 
~~~ action , internals , states = agent . act ( states = state , deterministic = deterministic , buffered = False ) 
reward = 0 
for repeat in xrange ( self . repeat_actions ) : 
~~~ state , terminal , step_reward = environment . execute ( action = action ) 
reward += step_reward 
if terminal : 
~~ ~~ if not testing : 
~~~ agent . atomic_observe ( 
states = state , 
actions = action , 
internals = internals , 
reward = reward , 
terminal = terminal 
~~ if sleep is not None : 
~~~ time . sleep ( sleep ) 
~~ time_step += 1 
episode_reward += reward 
if terminal or time_step == max_episode_timesteps : 
~~ if self . should_stop : 
~~ ~~ self . global_timestep += time_step 
self . episode_list_lock . acquire ( ) 
self . episode_rewards . append ( episode_reward ) 
self . episode_timesteps . append ( time_step ) 
self . episode_times . append ( time . time ( ) - time_start ) 
self . episode_list_lock . release ( ) 
if episode_finished is not None : 
~~~ if old_episode_finished : 
~~~ summary_data = { 
"thread_id" : thread_id , 
"episode" : episode , 
"timestep" : time_step , 
"episode_reward" : episode_reward 
if not episode_finished ( summary_data ) : 
~~ ~~ elif not episode_finished ( self , thread_id ) : 
~~ ~~ episode += 1 
~~ ~~ def _int_to_pos ( self , flat_position ) : 
return flat_position % self . env . action_space . screen_shape [ 0 ] , flat_position % self . env . action_space . screen_shape [ 1 ] 
~~ def _wait_state ( self , state , reward , terminal ) : 
while state == [ None ] or not state : 
~~~ state , terminal , reward = self . _execute ( dict ( key = 0 ) ) 
~~ return state , terminal , reward 
~~ def apply_step ( self , variables , deltas ) : 
if len ( variables ) != len ( deltas ) : 
~~ return tf . group ( 
* ( tf . assign_add ( ref = variable , value = delta ) for variable , delta in zip ( variables , deltas ) ) 
~~ def minimize ( self , time , variables , ** kwargs ) : 
deltas = self . step ( time = time , variables = variables , ** kwargs ) 
~~~ return tf . no_op ( ) 
optimizer = util . get_object ( 
predefined_objects = tensorforce . core . optimizers . optimizers , 
assert isinstance ( optimizer , Optimizer ) 
return optimizer 
~~ def np_dtype ( dtype ) : 
if dtype == 'float' or dtype == float or dtype == np . float32 or dtype == tf . float32 : 
~~~ return np . float32 
~~ elif dtype == np . float64 or dtype == tf . float64 : 
~~~ return np . float64 
~~ elif dtype == np . float16 or dtype == tf . float16 : 
~~~ return np . float16 
~~ elif dtype == 'int' or dtype == int or dtype == np . int32 or dtype == tf . int32 : 
~~~ return np . int32 
~~ elif dtype == np . int64 or dtype == tf . int64 : 
~~~ return np . int64 
~~ elif dtype == np . int16 or dtype == tf . int16 : 
~~~ return np . int16 
~~ elif dtype == 'bool' or dtype == bool or dtype == np . bool_ or dtype == tf . bool : 
~~~ return np . bool_ 
~~ ~~ def get_tensor_dependencies ( tensor ) : 
dependencies = set ( ) 
dependencies . update ( tensor . op . inputs ) 
for sub_op in tensor . op . inputs : 
~~~ dependencies . update ( get_tensor_dependencies ( sub_op ) ) 
~~ return dependencies 
~~ def get_object ( obj , predefined_objects = None , default_object = None , kwargs = None ) : 
kwargs = dict ( ) if kwargs is None else kwargs 
if isinstance ( obj , str ) and os . path . isfile ( obj ) : 
~~~ with open ( obj , 'r' ) as fp : 
~~~ obj = json . load ( fp = fp ) 
~~ ~~ if isinstance ( obj , dict ) : 
~~~ kwargs . update ( obj ) 
obj = kwargs . pop ( 'type' , None ) 
~~ if predefined_objects is not None and obj in predefined_objects : 
~~~ obj = predefined_objects [ obj ] 
~~ elif isinstance ( obj , str ) : 
~~~ if obj . find ( '.' ) != - 1 : 
~~~ module_name , function_name = obj . rsplit ( '.' , 1 ) 
obj = getattr ( module , function_name ) 
obj , 
list ( predefined_objects or ( ) ) 
~~ ~~ elif callable ( obj ) : 
~~ elif default_object is not None : 
~~~ args = ( obj , ) 
obj = default_object 
~~ return obj ( * args , ** kwargs ) 
~~ def prepare_kwargs ( raw , string_parameter = 'name' ) : 
kwargs = dict ( ) 
if isinstance ( raw , dict ) : 
~~~ kwargs . update ( raw ) 
~~ elif isinstance ( raw , str ) : 
~~~ kwargs [ string_parameter ] = raw 
~~ return kwargs 
~~ def register_saver_ops ( self ) : 
variables = self . get_savable_variables ( ) 
if variables is None or len ( variables ) == 0 : 
~~~ self . _saver = None 
~~ base_scope = self . _get_base_variable_scope ( ) 
variables_map = { strip_name_scope ( v . name , base_scope ) : v for v in variables } 
self . _saver = tf . train . Saver ( 
var_list = variables_map , 
reshape = False , 
sharded = False , 
max_to_keep = 5 , 
keep_checkpoint_every_n_hours = 10000.0 , 
name = None , 
restore_sequentially = False , 
saver_def = None , 
builder = None , 
defer_build = False , 
allow_empty = True , 
write_version = tf . train . SaverDef . V2 , 
pad_step_number = False , 
save_relative_paths = True 
~~ def save ( self , sess , save_path , timestep = None ) : 
if self . _saver is None : 
~~ return self . _saver . save ( 
sess = sess , 
save_path = save_path , 
global_step = timestep , 
write_meta_graph = False , 
~~ def restore ( self , sess , save_path ) : 
~~ self . _saver . restore ( sess = sess , save_path = save_path ) 
fetches = [ ] 
for processor in self . preprocessors : 
~~~ fetches . extend ( processor . reset ( ) or [ ] ) 
~~ return fetches 
~~ def process ( self , tensor ) : 
~~~ tensor = processor . process ( tensor = tensor ) 
~~ return tensor 
~~ def processed_shape ( self , shape ) : 
~~~ shape = processor . processed_shape ( shape = shape ) 
~~ return shape 
~~ def from_spec ( spec , kwargs = None ) : 
if isinstance ( spec , dict ) : 
~~~ spec = [ spec ] 
~~ stack = PreprocessorStack ( ) 
for preprocessor_spec in spec : 
~~~ preprocessor_kwargs = copy . deepcopy ( kwargs ) 
preprocessor = util . get_object ( 
obj = preprocessor_spec , 
predefined_objects = tensorforce . core . preprocessors . preprocessors , 
kwargs = preprocessor_kwargs 
assert isinstance ( preprocessor , Preprocessor ) 
stack . preprocessors . append ( preprocessor ) 
~~ return stack 
~~ def tf_solve ( self , fn_x , x_init , * args ) : 
self . fn_x = fn_x 
args = self . initialize ( x_init , * args ) 
if self . unroll_loop : 
~~~ for _ in range ( self . max_iterations ) : 
~~~ next_step = self . next_step ( * args ) 
step = ( lambda : self . step ( * args ) ) 
do_nothing = ( lambda : args ) 
args = tf . cond ( pred = next_step , true_fn = step , false_fn = do_nothing ) 
~~~ args = tf . while_loop ( cond = self . next_step , body = self . step , loop_vars = args ) 
~~ return args [ 0 ] 
next_state , rew , done , _ = self . env . step ( action ) 
return next_state , rew , done 
~~ def as_local_model ( self ) : 
super ( MemoryModel , self ) . as_local_model ( ) 
self . optimizer_spec = dict ( 
type = 'global_optimizer' , 
optimizer = self . optimizer_spec 
~~ def setup_components_and_tf_funcs ( self , custom_getter = None ) : 
custom_getter = super ( MemoryModel , self ) . setup_components_and_tf_funcs ( custom_getter ) 
self . memory = Memory . from_spec ( 
spec = self . memory_spec , 
kwargs = dict ( 
states = self . states_spec , 
internals = self . internals_spec , 
actions = self . actions_spec , 
summary_labels = self . summary_labels 
self . optimizer = Optimizer . from_spec ( 
spec = self . optimizer_spec , 
kwargs = dict ( summary_labels = self . summary_labels ) 
self . fn_discounted_cumulative_reward = tf . make_template ( 
name_ = 'discounted-cumulative-reward' , 
func_ = self . tf_discounted_cumulative_reward , 
custom_getter_ = custom_getter 
self . fn_reference = tf . make_template ( 
name_ = 'reference' , 
func_ = self . tf_reference , 
self . fn_loss_per_instance = tf . make_template ( 
name_ = 'loss-per-instance' , 
func_ = self . tf_loss_per_instance , 
self . fn_regularization_losses = tf . make_template ( 
name_ = 'regularization-losses' , 
func_ = self . tf_regularization_losses , 
self . fn_loss = tf . make_template ( 
name_ = 'loss' , 
func_ = self . tf_loss , 
self . fn_optimization = tf . make_template ( 
name_ = 'optimization' , 
func_ = self . tf_optimization , 
self . fn_import_experience = tf . make_template ( 
name_ = 'import-experience' , 
func_ = self . tf_import_experience , 
return custom_getter 
~~ def tf_discounted_cumulative_reward ( self , terminal , reward , discount = None , final_reward = 0.0 , horizon = 0 ) : 
if discount is None : 
~~~ discount = self . discount 
~~ def cumulate ( cumulative , reward_terminal_horizon_subtract ) : 
~~~ rew , is_terminal , is_over_horizon , sub = reward_terminal_horizon_subtract 
condition = is_terminal , 
x = rew , 
y = tf . where ( 
condition = is_over_horizon , 
x = ( rew + cumulative * discount - sub ) , 
y = ( rew + cumulative * discount ) 
~~ def len_ ( cumulative , term ) : 
~~~ return tf . where ( 
condition = term , 
x = tf . ones ( shape = ( ) , dtype = tf . int32 ) , 
y = cumulative + 1 
~~ reward = tf . reverse ( tensor = reward , axis = ( 0 , ) ) 
terminal = tf . reverse ( tensor = terminal , axis = ( 0 , ) ) 
lengths = tf . scan ( fn = len_ , elems = terminal , initializer = 0 ) 
off_horizon = tf . greater ( lengths , tf . fill ( dims = tf . shape ( lengths ) , value = horizon ) ) 
if horizon > 0 : 
~~~ horizon_subtractions = tf . map_fn ( lambda x : ( discount ** horizon ) * x , reward , dtype = tf . float32 ) 
horizon_subtractions = tf . concat ( [ np . zeros ( shape = ( horizon , ) ) , horizon_subtractions ] , axis = 0 ) 
horizon_subtractions = tf . slice ( horizon_subtractions , begin = ( 0 , ) , size = tf . shape ( reward ) ) 
~~~ horizon_subtractions = tf . zeros ( shape = tf . shape ( reward ) ) 
~~ reward = tf . scan ( 
fn = cumulate , 
elems = ( reward , terminal , off_horizon , horizon_subtractions ) , 
initializer = final_reward if horizon != 1 else 0.0 
return tf . reverse ( tensor = reward , axis = ( 0 , ) ) 
~~ def tf_reference ( self , states , internals , actions , terminal , reward , next_states , next_internals , update ) : 
~~ def tf_loss_per_instance ( self , states , internals , actions , terminal , reward , 
next_states , next_internals , update , reference = None ) : 
raise NotImplementedError 
~~ def tf_loss ( self , states , internals , actions , terminal , reward , next_states , next_internals , update , reference = None ) : 
loss_per_instance = self . fn_loss_per_instance ( 
states = states , 
actions = actions , 
terminal = terminal , 
next_states = next_states , 
next_internals = next_internals , 
update = update , 
reference = reference 
updated = self . memory . update_batch ( loss_per_instance = loss_per_instance ) 
with tf . control_dependencies ( control_inputs = ( updated , ) ) : 
~~~ loss = tf . reduce_mean ( input_tensor = loss_per_instance , axis = 0 ) 
if 'losses' in self . summary_labels : 
~~~ tf . contrib . summary . scalar ( name = 'loss-without-regularization' , tensor = loss ) 
~~ losses = self . fn_regularization_losses ( states = states , internals = internals , update = update ) 
if len ( losses ) > 0 : 
~~~ loss += tf . add_n ( inputs = [ losses [ name ] for name in sorted ( losses ) ] ) 
if 'regularization' in self . summary_labels : 
~~~ for name in sorted ( losses ) : 
~~~ tf . contrib . summary . scalar ( name = ( 'regularization/' + name ) , tensor = losses [ name ] ) 
~~ ~~ ~~ if 'losses' in self . summary_labels or 'total-loss' in self . summary_labels : 
~~~ tf . contrib . summary . scalar ( name = 'total-loss' , tensor = loss ) 
~~ ~~ def optimizer_arguments ( self , states , internals , actions , terminal , reward , next_states , next_internals ) : 
variables = self . get_variables ( ) , 
update = tf . constant ( value = True ) 
fn_reference = self . fn_reference , 
fn_loss = self . fn_loss 
~~~ arguments [ 'global_variables' ] = self . global_model . get_variables ( ) 
~~ def tf_optimization ( self , states , internals , actions , terminal , reward , next_states = None , next_internals = None ) : 
arguments = self . optimizer_arguments ( 
next_internals = next_internals 
return self . optimizer . minimize ( ** arguments ) 
~~ def tf_observe_timestep ( self , states , internals , actions , terminal , reward ) : 
stored = self . memory . store ( 
reward = reward 
with tf . control_dependencies ( control_inputs = ( stored , ) ) : 
~~~ unit = self . update_mode [ 'unit' ] 
batch_size = self . update_mode [ 'batch_size' ] 
frequency = self . update_mode . get ( 'frequency' , batch_size ) 
first_update = self . update_mode . get ( 'first_update' , 0 ) 
if unit == 'timesteps' : 
~~~ optimize = tf . logical_and ( 
x = tf . equal ( x = ( self . timestep % frequency ) , y = 0 ) , 
y = tf . logical_and ( 
x = tf . greater_equal ( x = self . timestep , y = batch_size ) , 
y = tf . greater_equal ( x = self . timestep , y = first_update ) 
~~ elif unit == 'episodes' : 
x = tf . equal ( x = ( self . episode % frequency ) , y = 0 ) , 
x = tf . greater ( x = tf . count_nonzero ( input_tensor = terminal ) , y = 0 ) , 
x = tf . greater_equal ( x = self . episode , y = batch_size ) , 
y = tf . greater_equal ( x = self . episode , y = first_update ) 
~~ elif unit == 'sequences' : 
~~~ sequence_length = self . update_mode . get ( 'length' , 8 ) 
optimize = tf . logical_and ( 
x = tf . greater_equal ( x = self . timestep , y = ( batch_size + sequence_length - 1 ) ) , 
~~ def true_fn ( ) : 
~~~ if unit == 'timesteps' : 
~~~ batch = self . memory . retrieve_timesteps ( n = batch_size ) 
~~~ batch = self . memory . retrieve_episodes ( n = batch_size ) 
~~~ batch = self . memory . retrieve_sequences ( n = batch_size , sequence_length = sequence_length ) 
~~ batch = util . map_tensors ( 
fn = ( lambda tensor : tf . stop_gradient ( input = tensor ) ) , 
tensors = batch 
optimize = self . fn_optimization ( ** batch ) 
with tf . control_dependencies ( control_inputs = ( optimize , ) ) : 
~~~ return tf . logical_and ( x = True , y = True ) 
~~ ~~ return tf . cond ( pred = optimize , true_fn = true_fn , false_fn = tf . no_op ) 
~~ ~~ def tf_import_experience ( self , states , internals , actions , terminal , reward ) : 
return self . memory . store ( 
~~ def import_experience ( self , states , internals , actions , terminal , reward ) : 
fetches = self . import_experience_output 
feed_dict = self . get_feed_dict ( 
self . monitored_session . run ( fetches = fetches , feed_dict = feed_dict ) 
~~ def tf_step ( 
time , 
variables , 
arguments , 
fn_loss , 
fn_reference , 
arguments [ 'reference' ] = fn_reference ( ** arguments ) 
loss_before = - fn_loss ( ** arguments ) 
with tf . control_dependencies ( control_inputs = ( loss_before , ) ) : 
~~~ deltas = self . optimizer . step ( 
time = time , 
arguments = arguments , 
fn_loss = fn_loss , 
return_estimated_improvement = True , 
if isinstance ( deltas , tuple ) : 
~~~ if len ( deltas ) != 2 : 
~~ deltas , estimated_improvement = deltas 
estimated_improvement = - estimated_improvement 
~~~ estimated_improvement = None 
~~ ~~ with tf . control_dependencies ( control_inputs = deltas ) : 
~~~ loss_step = - fn_loss ( ** arguments ) 
~~ with tf . control_dependencies ( control_inputs = ( loss_step , ) ) : 
~~~ def evaluate_step ( deltas ) : 
~~~ with tf . control_dependencies ( control_inputs = deltas ) : 
~~~ applied = self . apply_step ( variables = variables , deltas = deltas ) 
~~ with tf . control_dependencies ( control_inputs = ( applied , ) ) : 
~~~ return - fn_loss ( ** arguments ) 
~~ ~~ return self . solver . solve ( 
fn_x = evaluate_step , 
x_init = deltas , 
base_value = loss_before , 
target_value = loss_step , 
estimated_improvement = estimated_improvement 
distribution = util . get_object ( 
predefined_objects = tensorforce . core . distributions . distributions , 
assert isinstance ( distribution , Distribution ) 
return distribution 
self . episode , self . timestep , self . next_internals = self . model . reset ( ) 
self . current_internals = self . next_internals 
~~ def act ( self , states , deterministic = False , independent = False , fetch_tensors = None , buffered = True , index = 0 ) : 
if self . unique_state : 
~~~ self . current_states = dict ( state = np . asarray ( states ) ) 
~~~ self . current_states = { name : np . asarray ( states [ name ] ) for name in sorted ( states ) } 
~~ if fetch_tensors is not None : 
~~~ self . current_actions , self . next_internals , self . timestep , self . fetched_tensors = self . model . act ( 
states = self . current_states , 
internals = self . current_internals , 
deterministic = deterministic , 
independent = independent , 
fetch_tensors = fetch_tensors , 
index = index 
if self . unique_action : 
~~~ return self . current_actions [ 'action' ] , self . fetched_tensors 
~~~ return self . current_actions , self . fetched_tensors 
~~ ~~ self . current_actions , self . next_internals , self . timestep = self . model . act ( 
if buffered : 
~~~ if self . unique_action : 
~~~ return self . current_actions [ 'action' ] 
~~~ return self . current_actions 
~~~ return self . current_actions [ 'action' ] , self . current_states , self . current_internals 
~~~ return self . current_actions , self . current_states , self . current_internals 
~~ ~~ ~~ def observe ( self , terminal , reward , index = 0 ) : 
self . current_terminal = terminal 
self . current_reward = reward 
if self . batched_observe : 
~~~ self . observe_terminal [ index ] . append ( self . current_terminal ) 
self . observe_reward [ index ] . append ( self . current_reward ) 
if self . current_terminal or len ( self . observe_terminal [ index ] ) >= self . batching_capacity : 
~~~ self . episode = self . model . observe ( 
terminal = self . observe_terminal [ index ] , 
reward = self . observe_reward [ index ] , 
self . observe_terminal [ index ] = list ( ) 
self . observe_reward [ index ] = list ( ) 
terminal = self . current_terminal , 
reward = self . current_reward 
~~ ~~ def atomic_observe ( self , states , actions , internals , reward , terminal ) : 
~~~ states = dict ( state = states ) 
~~ if self . unique_action : 
~~~ actions = dict ( action = actions ) 
~~ self . episode = self . model . atomic_observe ( 
~~ def save_model ( self , directory = None , append_timestep = True ) : 
return self . model . save ( directory = directory , append_timestep = append_timestep ) 
~~ def restore_model ( self , directory = None , file = None ) : 
self . model . restore ( directory = directory , file = file ) 
agent = util . get_object ( 
predefined_objects = tensorforce . agents . agents , 
assert isinstance ( agent , Agent ) 
return agent 
~~ def get_named_tensor ( self , name ) : 
if name in self . named_tensors : 
~~~ return True , self . named_tensors [ name ] 
~~~ return False , None 
network = util . get_object ( 
default_object = LayeredNetwork , 
assert isinstance ( network , Network ) 
~~ def put ( self , item , priority = None ) : 
if not self . _isfull ( ) : 
~~~ self . _memory . append ( None ) 
~~ position = self . _next_position_then_increment ( ) 
old_priority = 0 if self . _memory [ position ] is None else ( self . _memory [ position ] . priority or 0 ) 
row = _SumRow ( item , priority ) 
self . _memory [ position ] = row 
self . _update_internal_nodes ( 
position , ( row . priority or 0 ) - old_priority ) 
~~ def move ( self , external_index , new_priority ) : 
index = external_index + ( self . _capacity - 1 ) 
return self . _move ( index , new_priority ) 
~~ def _move ( self , index , new_priority ) : 
item , old_priority = self . _memory [ index ] 
old_priority = old_priority or 0 
self . _memory [ index ] = _SumRow ( item , new_priority ) 
self . _update_internal_nodes ( index , new_priority - old_priority ) 
~~ def _update_internal_nodes ( self , index , delta ) : 
while index > 0 : 
~~~ index = ( index - 1 ) // 2 
self . _memory [ index ] += delta 
~~ ~~ def _next_position_then_increment ( self ) : 
start = self . _capacity - 1 
position = start + self . _position 
self . _position = ( self . _position + 1 ) % self . _capacity 
~~ def _sample_with_priority ( self , p ) : 
parent = 0 
~~~ left = 2 * parent + 1 
if left >= len ( self . _memory ) : 
~~~ return parent 
~~ left_p = self . _memory [ left ] if left < self . _capacity - 1 else ( self . _memory [ left ] . priority or 0 ) 
if p <= left_p : 
~~~ parent = left 
~~~ if left + 1 >= len ( self . _memory ) : 
~~ p -= left_p 
parent = left + 1 
~~ ~~ ~~ def sample_minibatch ( self , batch_size ) : 
pool_size = len ( self ) 
if pool_size == 0 : 
~~ delta_p = self . _memory [ 0 ] / batch_size 
chosen_idx = [ ] 
if abs ( self . _memory [ 0 ] ) < util . epsilon : 
~~~ chosen_idx = np . random . randint ( self . _capacity - 1 , self . _capacity - 1 + len ( self ) , size = batch_size ) . tolist ( ) 
~~~ for i in xrange ( batch_size ) : 
~~~ lower = max ( i * delta_p , 0 ) 
upper = min ( ( i + 1 ) * delta_p , self . _memory [ 0 ] ) 
p = random . uniform ( lower , upper ) 
chosen_idx . append ( self . _sample_with_priority ( p ) ) 
~~ ~~ return [ ( i , self . _memory [ i ] ) for i in chosen_idx ] 
~~ def get_batch ( self , batch_size , next_states = False ) : 
if batch_size > len ( self . observations ) : 
~~~ raise TensorForceError ( 
~~ states = { name : np . zeros ( ( batch_size , ) + tuple ( state [ 'shape' ] ) , dtype = util . np_dtype ( 
state [ 'type' ] ) ) for name , state in self . states_spec . items ( ) } 
internals = [ np . zeros ( ( batch_size , ) + shape , dtype ) 
for shape , dtype in self . internals_spec ] 
actions = { name : np . zeros ( ( batch_size , ) + tuple ( action [ 'shape' ] ) , dtype = util . np_dtype ( action [ 'type' ] ) ) for name , action in self . actions_spec . items ( ) } 
terminal = np . zeros ( ( batch_size , ) , dtype = util . np_dtype ( 'bool' ) ) 
reward = np . zeros ( ( batch_size , ) , dtype = util . np_dtype ( 'float' ) ) 
if next_states : 
~~~ next_states = { name : np . zeros ( ( batch_size , ) + tuple ( state [ 'shape' ] ) , dtype = util . np_dtype ( 
next_internals = [ np . zeros ( ( batch_size , ) + shape , dtype ) 
~~ unseen_indices = list ( xrange ( 
self . none_priority_index + self . observations . _capacity - 1 , 
len ( self . observations ) + self . observations . _capacity - 1 ) 
self . batch_indices = unseen_indices [ : batch_size ] 
remaining = batch_size - len ( self . batch_indices ) 
if remaining : 
~~~ samples = self . observations . sample_minibatch ( remaining ) 
sample_indices = [ i for i , o in samples ] 
self . batch_indices += sample_indices 
~~ np . random . shuffle ( self . batch_indices ) 
for n , index in enumerate ( self . batch_indices ) : 
~~~ observation , _ = self . observations . _memory [ index ] 
for name , state in states . items ( ) : 
~~~ state [ n ] = observation [ 0 ] [ name ] 
~~ for k , internal in enumerate ( internals ) : 
~~~ internal [ n ] = observation [ 1 ] [ k ] 
~~ for name , action in actions . items ( ) : 
~~~ action [ n ] = observation [ 2 ] [ name ] 
~~ terminal [ n ] = observation [ 3 ] 
reward [ n ] = observation [ 4 ] 
~~~ for name , next_state in next_states . items ( ) : 
~~~ next_state [ n ] = observation [ 5 ] [ name ] 
~~ for k , next_internal in enumerate ( next_internals ) : 
~~~ next_internal [ n ] = observation [ 6 ] [ k ] 
~~ ~~ ~~ if next_states : 
~~~ return dict ( 
~~ ~~ def update_batch ( self , loss_per_instance ) : 
if self . batch_indices is None : 
~~ for index , loss in zip ( self . batch_indices , loss_per_instance ) : 
~~~ new_priority = ( np . abs ( loss ) + self . prioritization_constant ) ** self . prioritization_weight 
self . observations . _move ( index , new_priority ) 
self . none_priority_index += 1 
~~ ~~ def import_experience ( self , experiences ) : 
if isinstance ( experiences , dict ) : 
~~~ if self . unique_state : 
~~~ experiences [ 'states' ] = dict ( state = experiences [ 'states' ] ) 
~~~ experiences [ 'actions' ] = dict ( action = experiences [ 'actions' ] ) 
~~ self . model . import_experience ( ** experiences ) 
~~~ states = dict ( state = list ( ) ) 
~~~ states = { name : list ( ) for name in experiences [ 0 ] [ 'states' ] } 
~~ internals = [ list ( ) for _ in experiences [ 0 ] [ 'internals' ] ] 
~~~ actions = dict ( action = list ( ) ) 
~~~ actions = { name : list ( ) for name in experiences [ 0 ] [ 'actions' ] } 
~~ terminal = list ( ) 
reward = list ( ) 
for experience in experiences : 
~~~ states [ 'state' ] . append ( experience [ 'states' ] ) 
~~~ for name in sorted ( states ) : 
~~~ states [ name ] . append ( experience [ 'states' ] [ name ] ) 
~~ ~~ for n , internal in enumerate ( internals ) : 
~~~ internal . append ( experience [ 'internals' ] [ n ] ) 
~~~ actions [ 'action' ] . append ( experience [ 'actions' ] ) 
~~~ for name in sorted ( actions ) : 
~~~ actions [ name ] . append ( experience [ 'actions' ] [ name ] ) 
~~ ~~ terminal . append ( experience [ 'terminal' ] ) 
reward . append ( experience [ 'reward' ] ) 
~~ self . model . import_experience ( 
~~ ~~ def connect ( self , timeout = 600 ) : 
if self . socket : 
~~ self . socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) 
if timeout < 5 or timeout is None : 
~~~ timeout = 5 
~~ err = 0 
while time . time ( ) - start_time < timeout : 
~~~ self . socket . settimeout ( 5 ) 
err = self . socket . connect_ex ( ( self . host , self . port ) ) 
if err == 0 : 
~~ time . sleep ( 1 ) 
~~ if err != 0 : 
format ( self . host , self . port , err , errno . errorcode [ err ] , os . strerror ( err ) ) ) 
~~ ~~ def disconnect ( self ) : 
if not self . socket : 
~~ self . socket . close ( ) 
self . socket = None 
~~ def send ( self , message , socket_ ) : 
if not socket_ : 
~~ elif not isinstance ( message , dict ) : 
~~ message = msgpack . packb ( message ) 
len_ = len ( message ) 
socket_ . send ( bytes ( "{:08d}" . format ( len_ ) , encoding = "ascii" ) + message ) 
~~ def recv ( self , socket_ , encoding = None ) : 
unpacker = msgpack . Unpacker ( encoding = encoding ) 
if response == b"" : 
~~ orig_len = int ( response ) 
received_len = 0 
~~~ data = socket_ . recv ( min ( orig_len - received_len , self . max_msg_len ) ) 
format ( orig_len - received_len ) ) 
~~ data_len = len ( data ) 
received_len += data_len 
unpacker . feed ( data ) 
if received_len == orig_len : 
~~ ~~ for message in unpacker : 
~~~ sts = message . get ( "status" , message . get ( b"status" ) ) 
if sts : 
~~~ if sts == "ok" or sts == b"ok" : 
~~~ return message 
format ( orig_len ) ) 
~~ def is_action_available ( self , action ) : 
temp_state = np . rot90 ( self . _state , action ) 
return self . _is_action_available_left ( temp_state ) 
~~ def _is_action_available_left ( self , state ) : 
for row in range ( 4 ) : 
~~~ has_empty = False 
for col in range ( 4 ) : 
~~~ has_empty |= state [ row , col ] == 0 
if state [ row , col ] != 0 and has_empty : 
~~ if ( state [ row , col ] != 0 and col > 0 and 
state [ row , col ] == state [ row , col - 1 ] ) : 
~~ def do_action ( self , action ) : 
reward = self . _do_action_left ( temp_state ) 
self . _state = np . rot90 ( temp_state , - action ) 
self . _score += reward 
self . add_random_tile ( ) 
return reward 
~~ def _do_action_left ( self , state ) : 
~~~ merge_candidate = - 1 
merged = np . zeros ( ( 4 , ) , dtype = np . bool ) 
~~~ if state [ row , col ] == 0 : 
~~ if ( merge_candidate != - 1 and 
not merged [ merge_candidate ] and 
state [ row , merge_candidate ] == state [ row , col ] ) : 
~~~ state [ row , col ] = 0 
merged [ merge_candidate ] = True 
state [ row , merge_candidate ] += 1 
reward += 2 ** state [ row , merge_candidate ] 
~~~ merge_candidate += 1 
if col != merge_candidate : 
~~~ state [ row , merge_candidate ] = state [ row , col ] 
state [ row , col ] = 0 
~~ ~~ ~~ ~~ return reward 
~~ def add_random_tile ( self ) : 
x_pos , y_pos = np . where ( self . _state == 0 ) 
assert len ( x_pos ) != 0 
empty_index = np . random . choice ( len ( x_pos ) ) 
value = np . random . choice ( [ 1 , 2 ] , p = [ 0.9 , 0.1 ] ) 
self . _state [ x_pos [ empty_index ] , y_pos [ empty_index ] ] = value 
~~ def print_state ( self ) : 
def tile_string ( value ) : 
if value > 0 : 
~~ separator_line = '-' * 25 
print ( separator_line ) 
~~~ print ( "|" + "|" . join ( [ tile_string ( v ) for v in self . _state [ row , : ] ] ) + "|" ) 
~~ ~~ def setup ( self ) : 
graph_default_context = self . setup_graph ( ) 
if self . execution_type == "distributed" and self . server is None and self . is_local_model : 
~~~ self . start_server ( ) 
~~ with tf . device ( device_name_or_function = self . device ) : 
~~~ with tf . variable_scope ( name_or_scope = self . scope , reuse = False ) : 
~~~ self . variables = dict ( ) 
self . all_variables = dict ( ) 
self . registered_variables = set ( ) 
self . setup_placeholders ( ) 
self . setup_components_and_tf_funcs ( ) 
self . fn_initialize ( ) 
if self . summarizer_spec is not None : 
~~~ with tf . name_scope ( name = 'summarizer' ) : 
~~~ self . summarizer = tf . contrib . summary . create_file_writer ( 
logdir = self . summarizer_spec [ 'directory' ] , 
max_queue = None , 
flush_millis = ( self . summarizer_spec . get ( 'flush' , 10 ) * 1000 ) , 
filename_suffix = None , 
name = None 
default_summarizer = self . summarizer . as_default ( ) 
assert 'steps' not in self . summarizer_spec 
record_summaries = tf . contrib . summary . always_record_summaries ( ) 
~~ default_summarizer . __enter__ ( ) 
record_summaries . __enter__ ( ) 
~~ states = util . map_tensors ( fn = tf . identity , tensors = self . states_input ) 
internals = util . map_tensors ( fn = tf . identity , tensors = self . internals_input ) 
actions = util . map_tensors ( fn = tf . identity , tensors = self . actions_input ) 
terminal = tf . identity ( input = self . terminal_input ) 
reward = tf . identity ( input = self . reward_input ) 
deterministic = tf . identity ( input = self . deterministic_input ) 
independent = tf . identity ( input = self . independent_input ) 
episode_index = tf . identity ( input = self . episode_index_input ) 
states , actions , reward = self . fn_preprocess ( states = states , actions = actions , reward = reward ) 
self . create_operations ( 
index = episode_index 
if 'inputs' in self . summary_labels or 'states' in self . summary_labels : 
~~~ tf . contrib . summary . histogram ( name = ( 'states-' + name ) , tensor = states [ name ] ) 
~~ ~~ if 'inputs' in self . summary_labels or 'actions' in self . summary_labels : 
~~~ tf . contrib . summary . histogram ( name = ( 'actions-' + name ) , tensor = actions [ name ] ) 
~~ ~~ if 'inputs' in self . summary_labels or 'reward' in self . summary_labels : 
~~~ tf . contrib . summary . histogram ( name = 'reward' , tensor = reward ) 
~~ if 'graph' in self . summary_labels : 
~~~ graph_def = self . graph . as_graph_def ( ) 
graph_str = tf . constant ( 
value = graph_def . SerializeToString ( ) , 
dtype = tf . string , 
shape = ( ) 
self . graph_summary = tf . contrib . summary . graph ( 
param = graph_str , 
step = self . global_timestep 
if 'meta_param_recorder_class' in self . summarizer_spec : 
~~~ self . graph_summary = tf . group ( 
self . graph_summary , 
* self . summarizer_spec [ 'meta_param_recorder_class' ] . build_metagraph_list ( ) 
~~ ~~ ~~ if self . summarizer_spec is not None : 
~~~ record_summaries . __exit__ ( None , None , None ) 
default_summarizer . __exit__ ( None , None , None ) 
with tf . name_scope ( name = 'summarizer' ) : 
~~~ self . flush_summarizer = tf . contrib . summary . flush ( ) 
self . summarizer_init_op = tf . contrib . summary . summary_writer_initializer_op ( ) 
assert len ( self . summarizer_init_op ) == 1 
self . summarizer_init_op = self . summarizer_init_op [ 0 ] 
~~ ~~ ~~ ~~ if self . execution_type == "distributed" and not self . is_local_model : 
~~ self . setup_saver ( ) 
self . setup_scaffold ( ) 
hooks = self . setup_hooks ( ) 
self . setup_session ( self . server , hooks , graph_default_context ) 
~~ def setup_graph ( self ) : 
graph_default_context = None 
if self . execution_type == "single" : 
~~~ self . graph = tf . Graph ( ) 
graph_default_context = self . graph . as_default ( ) 
graph_default_context . __enter__ ( ) 
self . global_model = None 
~~ elif self . execution_type == "distributed" : 
~~~ if self . distributed_spec [ "job" ] == "ps" : 
~~ elif self . distributed_spec [ "job" ] == "worker" : 
~~~ if self . is_local_model : 
~~~ graph = tf . Graph ( ) 
graph_default_context = graph . as_default ( ) 
self . global_model = deepcopy ( self ) 
self . global_model . is_local_model = False 
self . global_model . setup ( ) 
self . graph = graph 
self . as_local_model ( ) 
self . scope += '-worker' + str ( self . distributed_spec [ "task_index" ] ) 
self . device = tf . train . replica_device_setter ( 
worker_device = self . device , 
cluster = self . distributed_spec [ "cluster_spec" ] 
~~ return graph_default_context 
~~ def start_server ( self ) : 
self . server = tf . train . Server ( 
server_or_cluster_def = self . distributed_spec [ "cluster_spec" ] , 
job_name = self . distributed_spec [ "job" ] , 
task_index = self . distributed_spec [ "task_index" ] , 
protocol = self . distributed_spec . get ( "protocol" ) , 
config = self . distributed_spec . get ( "session_config" ) , 
start = True 
if self . distributed_spec [ "job" ] == "ps" : 
~~~ self . server . join ( ) 
quit ( ) 
~~ ~~ def setup_placeholders ( self ) : 
for name in sorted ( self . states_spec ) : 
~~~ self . states_input [ name ] = tf . placeholder ( 
dtype = util . tf_dtype ( self . states_spec [ name ] [ 'type' ] ) , 
shape = ( None , ) + tuple ( self . states_spec [ name ] [ 'shape' ] ) , 
name = ( 'state-' + name ) 
~~ if self . states_preprocessing_spec is None : 
~~~ for name in sorted ( self . states_spec ) : 
~~~ self . states_spec [ name ] [ 'unprocessed_shape' ] = self . states_spec [ name ] [ 'shape' ] 
~~ ~~ elif not isinstance ( self . states_preprocessing_spec , list ) and all ( name in self . states_spec for name in self . states_preprocessing_spec ) : 
~~~ if name in self . states_preprocessing_spec : 
~~~ preprocessing = PreprocessorStack . from_spec ( 
spec = self . states_preprocessing_spec [ name ] , 
kwargs = dict ( shape = self . states_spec [ name ] [ 'shape' ] ) 
self . states_spec [ name ] [ 'unprocessed_shape' ] = self . states_spec [ name ] [ 'shape' ] 
self . states_spec [ name ] [ 'shape' ] = preprocessing . processed_shape ( shape = self . states_spec [ name ] [ 'unprocessed_shape' ] ) 
self . states_preprocessing [ name ] = preprocessing 
~~ ~~ ~~ elif "type" in self . states_preprocessing_spec : 
~~~ preprocessing = PreprocessorStack . from_spec ( spec = self . states_preprocessing_spec , 
kwargs = dict ( shape = self . states_spec [ name ] [ 'shape' ] ) ) 
spec = self . states_preprocessing_spec , 
~~ ~~ for name in sorted ( self . actions_spec ) : 
~~~ self . actions_input [ name ] = tf . placeholder ( 
dtype = util . tf_dtype ( self . actions_spec [ name ] [ 'type' ] ) , 
shape = ( None , ) + tuple ( self . actions_spec [ name ] [ 'shape' ] ) , 
name = ( 'action-' + name ) 
~~ if self . actions_exploration_spec is None : 
~~ elif all ( name in self . actions_spec for name in self . actions_exploration_spec ) : 
~~~ for name in sorted ( self . actions_spec ) : 
~~~ if name in self . actions_exploration : 
~~~ self . actions_exploration [ name ] = Exploration . from_spec ( spec = self . actions_exploration_spec [ name ] ) 
~~~ self . actions_exploration [ name ] = Exploration . from_spec ( spec = self . actions_exploration_spec ) 
~~ ~~ self . terminal_input = tf . placeholder ( dtype = util . tf_dtype ( 'bool' ) , shape = ( None , ) , name = 'terminal' ) 
self . reward_input = tf . placeholder ( dtype = util . tf_dtype ( 'float' ) , shape = ( None , ) , name = 'reward' ) 
if self . reward_preprocessing_spec is not None : 
~~~ self . reward_preprocessing = PreprocessorStack . from_spec ( 
spec = self . reward_preprocessing_spec , 
kwargs = dict ( shape = ( ) ) 
if self . reward_preprocessing . processed_shape ( shape = ( ) ) != ( ) : 
~~ ~~ self . deterministic_input = tf . placeholder ( dtype = util . tf_dtype ( 'bool' ) , shape = ( ) , name = 'deterministic' ) 
self . independent_input = tf . placeholder ( dtype = util . tf_dtype ( 'bool' ) , shape = ( ) , name = 'independent' ) 
if custom_getter is None : 
~~~ def custom_getter ( getter , name , registered = False , ** kwargs ) : 
if registered : 
~~~ self . registered_variables . add ( name ) 
~~ elif name in self . registered_variables : 
~~~ registered = True 
~~ variable = getter ( name = name , ** kwargs ) 
~~ elif name in self . all_variables : 
~~~ assert variable is self . all_variables [ name ] 
if kwargs . get ( 'trainable' , True ) : 
~~~ assert variable is self . variables [ name ] 
if 'variables' in self . summary_labels : 
~~~ tf . contrib . summary . histogram ( name = name , tensor = variable ) 
~~~ self . all_variables [ name ] = variable 
~~~ self . variables [ name ] = variable 
~~ ~~ ~~ return variable 
~~ ~~ self . fn_initialize = tf . make_template ( 
name_ = 'initialize' , 
func_ = self . tf_initialize , 
self . fn_preprocess = tf . make_template ( 
name_ = 'preprocess' , 
func_ = self . tf_preprocess , 
self . fn_actions_and_internals = tf . make_template ( 
name_ = 'actions-and-internals' , 
func_ = self . tf_actions_and_internals , 
self . fn_observe_timestep = tf . make_template ( 
name_ = 'observe-timestep' , 
func_ = self . tf_observe_timestep , 
self . fn_action_exploration = tf . make_template ( 
name_ = 'action-exploration' , 
func_ = self . tf_action_exploration , 
~~ def setup_saver ( self ) : 
~~~ global_variables = self . get_variables ( include_submodules = True , include_nontrainable = True ) 
~~~ global_variables = self . global_model . get_variables ( include_submodules = True , include_nontrainable = True ) 
~~ for c in self . get_savable_components ( ) : 
~~~ c . register_saver_ops ( ) 
~~ self . saver = tf . train . Saver ( 
~~ def setup_scaffold ( self ) : 
init_op = tf . variables_initializer ( var_list = global_variables ) 
if self . summarizer_init_op is not None : 
~~~ init_op = tf . group ( init_op , self . summarizer_init_op ) 
~~ if self . graph_summary is None : 
~~~ ready_op = tf . report_uninitialized_variables ( var_list = global_variables ) 
ready_for_local_init_op = None 
local_init_op = None 
~~~ ready_op = None 
ready_for_local_init_op = tf . report_uninitialized_variables ( var_list = global_variables ) 
local_init_op = self . graph_summary 
local_variables = self . get_variables ( include_submodules = True , include_nontrainable = True ) 
~~ ready_op = tf . report_uninitialized_variables ( var_list = ( global_variables + local_variables ) ) 
if self . graph_summary is None : 
~~~ local_init_op = tf . group ( 
tf . variables_initializer ( var_list = local_variables ) , 
* ( tf . assign ( ref = local_var , value = global_var ) for local_var , global_var in zip ( 
self . get_variables ( include_submodules = True ) , 
self . global_model . get_variables ( include_submodules = True ) 
~~ ~~ def init_fn ( scaffold , session ) : 
~~~ if self . saver_spec is not None and self . saver_spec . get ( 'load' , True ) : 
~~~ directory = self . saver_spec [ 'directory' ] 
file = self . saver_spec . get ( 'file' ) 
if file is None : 
~~~ file = tf . train . latest_checkpoint ( 
checkpoint_dir = directory , 
~~ elif not os . path . isfile ( file ) : 
~~~ file = os . path . join ( directory , file ) 
~~ if file is not None : 
~~~ scaffold . saver . restore ( sess = session , save_path = file ) 
session . run ( fetches = self . list_buffer_index_reset_op ) 
~~ except tf . errors . NotFoundError : 
~~ ~~ ~~ ~~ self . scaffold = tf . train . Scaffold ( 
init_op = init_op , 
init_feed_dict = None , 
init_fn = init_fn , 
ready_op = ready_op , 
ready_for_local_init_op = ready_for_local_init_op , 
local_init_op = local_init_op , 
summary_op = None , 
saver = self . saver , 
copy_from_scaffold = None 
~~ def setup_hooks ( self ) : 
hooks = list ( ) 
if self . saver_spec is not None and ( self . execution_type == 'single' or self . distributed_spec [ 'task_index' ] == 0 ) : 
~~~ self . saver_directory = self . saver_spec [ 'directory' ] 
hooks . append ( tf . train . CheckpointSaverHook ( 
checkpoint_dir = self . saver_directory , 
save_secs = self . saver_spec . get ( 'seconds' , None if 'steps' in self . saver_spec else 600 ) , 
checkpoint_basename = self . saver_spec . get ( 'basename' , 'model.ckpt' ) , 
scaffold = self . scaffold , 
listeners = None 
~~~ self . saver_directory = None 
~~ return hooks 
~~ def setup_session ( self , server , hooks , graph_default_context ) : 
if self . execution_type == "distributed" : 
~~~ session_creator = tf . train . ChiefSessionCreator ( 
master = server . target , 
config = self . session_config , 
checkpoint_dir = None , 
checkpoint_filename_with_path = None 
self . monitored_session = tf . train . MonitoredSession ( 
session_creator = session_creator , 
hooks = hooks , 
if self . tf_session_dump_dir != "" : 
~~~ self . monitored_session = DumpingDebugWrapperSession ( self . monitored_session , self . tf_session_dump_dir ) 
~~~ self . monitored_session = tf . train . SingularMonitoredSession ( 
checkpoint_dir = None 
~~ if graph_default_context : 
~~~ graph_default_context . __exit__ ( None , None , None ) 
~~ self . graph . finalize ( ) 
self . monitored_session . __enter__ ( ) 
self . session = self . monitored_session . _tf_sess ( ) 
if self . flush_summarizer is not None : 
~~~ self . monitored_session . run ( fetches = self . flush_summarizer ) 
~~ if self . saver_directory is not None : 
~~~ self . save ( append_timestep = True ) 
~~ self . monitored_session . __exit__ ( None , None , None ) 
~~ def tf_initialize ( self ) : 
with tf . device ( device_name_or_function = ( self . global_model . device if self . global_model else self . device ) ) : 
~~~ collection = self . graph . get_collection ( name = 'global-timestep' ) 
if len ( collection ) == 0 : 
~~~ self . global_timestep = tf . get_variable ( 
name = 'global-timestep' , 
shape = ( ) , 
dtype = tf . int64 , 
trainable = False , 
initializer = tf . constant_initializer ( value = 0 , dtype = tf . int64 ) , 
collections = [ 'global-timestep' , tf . GraphKeys . GLOBAL_STEP ] 
~~~ assert len ( collection ) == 1 
self . global_timestep = collection [ 0 ] 
~~ collection = self . graph . get_collection ( name = 'global-episode' ) 
~~~ self . global_episode = tf . get_variable ( 
name = 'global-episode' , 
collections = [ 'global-episode' ] 
self . global_episode = collection [ 0 ] 
~~ ~~ self . timestep = tf . get_variable ( 
name = 'timestep' , 
trainable = False 
self . episode = tf . get_variable ( 
name = 'episode' , 
self . episode_index_input = tf . placeholder ( 
name = 'episode_index' , 
dtype = tf . int32 , 
~~~ self . list_states_buffer [ name ] = tf . get_variable ( 
name = ( 'state-{}' . format ( name ) ) , 
shape = ( ( self . num_parallel , self . batching_capacity , ) + tuple ( self . states_spec [ name ] [ 'shape' ] ) ) , 
~~ for name in sorted ( self . internals_spec ) : 
~~~ self . list_internals_buffer [ name ] = tf . get_variable ( 
name = ( 'internal-{}' . format ( name ) ) , 
shape = ( ( self . num_parallel , self . batching_capacity , ) + tuple ( self . internals_spec [ name ] [ 'shape' ] ) ) , 
dtype = util . tf_dtype ( self . internals_spec [ name ] [ 'type' ] ) , 
~~ for name in sorted ( self . actions_spec ) : 
~~~ self . list_actions_buffer [ name ] = tf . get_variable ( 
name = ( 'action-{}' . format ( name ) ) , 
shape = ( ( self . num_parallel , self . batching_capacity , ) + tuple ( self . actions_spec [ name ] [ 'shape' ] ) ) , 
~~ self . list_buffer_index = tf . get_variable ( 
name = 'buffer-index' , 
shape = ( self . num_parallel , ) , 
dtype = util . tf_dtype ( 'int' ) , 
~~ def tf_preprocess ( self , states , actions , reward ) : 
for name in sorted ( self . states_preprocessing ) : 
~~~ states [ name ] = self . states_preprocessing [ name ] . process ( tensor = states [ name ] ) 
~~ if self . reward_preprocessing is not None : 
~~~ reward = self . reward_preprocessing . process ( tensor = reward ) 
~~ return states , actions , reward 
~~ def tf_action_exploration ( self , action , exploration , action_spec ) : 
action_shape = tf . shape ( input = action ) 
exploration_value = exploration . tf_explore ( 
episode = self . global_episode , 
timestep = self . global_timestep , 
shape = action_spec [ 'shape' ] 
exploration_value = tf . expand_dims ( input = exploration_value , axis = 0 ) 
if action_spec [ 'type' ] == 'bool' : 
~~~ action = tf . where ( 
condition = ( tf . random_uniform ( shape = action_shape ) < exploration_value ) , 
x = ( tf . random_uniform ( shape = action_shape ) < 0.5 ) , 
y = action 
~~ elif action_spec [ 'type' ] == 'int' : 
x = tf . random_uniform ( shape = action_shape , maxval = action_spec [ 'num_actions' ] , dtype = util . tf_dtype ( 'int' ) ) , 
~~ elif action_spec [ 'type' ] == 'float' : 
~~~ noise = tf . random_normal ( shape = action_shape , dtype = util . tf_dtype ( 'float' ) ) 
action += noise * exploration_value 
if 'min_value' in action_spec : 
~~~ action = tf . clip_by_value ( 
t = action , 
clip_value_min = action_spec [ 'min_value' ] , 
clip_value_max = action_spec [ 'max_value' ] 
~~ ~~ return action 
~~ def create_act_operations ( self , states , internals , deterministic , independent , index ) : 
operations = list ( ) 
if self . variable_noise is not None and self . variable_noise > 0.0 : 
~~~ self . fn_actions_and_internals ( 
deterministic = deterministic 
noise_deltas = list ( ) 
for variable in self . get_variables ( ) : 
~~~ noise_delta = tf . random_normal ( shape = util . shape ( variable ) , mean = 0.0 , stddev = self . variable_noise ) 
noise_deltas . append ( noise_delta ) 
operations . append ( variable . assign_add ( delta = noise_delta ) ) 
~~ ~~ with tf . control_dependencies ( control_inputs = operations ) : 
~~~ self . actions_output , self . internals_output = self . fn_actions_and_internals ( 
~~ with tf . control_dependencies ( control_inputs = [ self . actions_output [ name ] for name in sorted ( self . actions_output ) ] ) : 
~~~ operations = list ( ) 
~~~ for variable , noise_delta in zip ( self . get_variables ( ) , noise_deltas ) : 
~~~ operations . append ( variable . assign_sub ( delta = noise_delta ) ) 
~~ ~~ ~~ with tf . control_dependencies ( control_inputs = operations ) : 
~~~ for name in sorted ( self . actions_exploration ) : 
~~~ self . actions_output [ name ] = tf . cond ( 
pred = self . deterministic_input , 
true_fn = ( lambda : self . actions_output [ name ] ) , 
false_fn = ( lambda : self . fn_action_exploration ( 
action = self . actions_output [ name ] , 
exploration = self . actions_exploration [ name ] , 
action_spec = self . actions_spec [ name ] 
~~ ~~ def independent_act ( ) : 
return self . global_timestep 
~~ def normal_act ( ) : 
batch_size = tf . shape ( input = states [ next ( iter ( sorted ( states ) ) ) ] ) [ 0 ] 
for name in sorted ( states ) : 
~~~ operations . append ( tf . assign ( 
ref = self . list_states_buffer [ name ] [ index , self . list_buffer_index [ index ] : self . list_buffer_index [ index ] + batch_size ] , 
value = states [ name ] 
~~ for name in sorted ( internals ) : 
ref = self . list_internals_buffer [ name ] [ index , self . list_buffer_index [ index ] : self . list_buffer_index [ index ] + batch_size ] , 
value = internals [ name ] 
~~ for name in sorted ( self . actions_output ) : 
ref = self . list_actions_buffer [ name ] [ index , self . list_buffer_index [ index ] : self . list_buffer_index [ index ] + batch_size ] , 
value = self . actions_output [ name ] 
~~ with tf . control_dependencies ( control_inputs = operations ) : 
operations . append ( tf . assign ( 
ref = self . list_buffer_index [ index : index + 1 ] , 
value = tf . add ( self . list_buffer_index [ index : index + 1 ] , tf . constant ( [ 1 ] ) ) 
operations . append ( tf . assign_add ( 
ref = self . timestep , 
value = tf . to_int64 ( x = batch_size ) 
ref = self . global_timestep , 
~~~ return self . global_timestep + 0 
~~ ~~ self . timestep_output = tf . cond ( 
pred = independent , 
true_fn = independent_act , 
false_fn = normal_act 
~~ def create_observe_operations ( self , terminal , reward , index ) : 
num_episodes = tf . count_nonzero ( input_tensor = terminal , dtype = util . tf_dtype ( 'int' ) ) 
increment_episode = tf . assign_add ( ref = self . episode , value = tf . to_int64 ( x = num_episodes ) ) 
increment_global_episode = tf . assign_add ( ref = self . global_episode , value = tf . to_int64 ( x = num_episodes ) ) 
with tf . control_dependencies ( control_inputs = ( increment_episode , increment_global_episode ) ) : 
~~~ fn = ( lambda x : tf . stop_gradient ( input = x [ : self . list_buffer_index [ index ] ] ) ) 
states = util . map_tensors ( fn = fn , tensors = self . list_states_buffer , index = index ) 
internals = util . map_tensors ( fn = fn , tensors = self . list_internals_buffer , index = index ) 
actions = util . map_tensors ( fn = fn , tensors = self . list_actions_buffer , index = index ) 
terminal = tf . stop_gradient ( input = terminal ) 
reward = tf . stop_gradient ( input = reward ) 
observation = self . fn_observe_timestep ( 
~~ with tf . control_dependencies ( control_inputs = ( observation , ) ) : 
~~~ reset_index = tf . assign ( ref = self . list_buffer_index [ index ] , value = 0 ) 
~~ with tf . control_dependencies ( control_inputs = ( reset_index , ) ) : 
~~~ self . episode_output = self . global_episode + 0 
~~ self . list_buffer_index_reset_op = tf . group ( 
* ( tf . assign ( ref = self . list_buffer_index [ n ] , value = 0 ) for n in range ( self . num_parallel ) ) 
~~ def create_atomic_observe_operations ( self , states , actions , internals , terminal , reward , index ) : 
~~~ states = util . map_tensors ( fn = tf . stop_gradient , tensors = states ) 
internals = util . map_tensors ( fn = tf . stop_gradient , tensors = internals ) 
actions = util . map_tensors ( fn = tf . stop_gradient , tensors = actions ) 
~~~ self . unbuffered_episode_output = self . global_episode + 0 
~~ ~~ def create_operations ( self , states , internals , actions , terminal , reward , deterministic , independent , index ) : 
self . create_act_operations ( 
self . create_observe_operations ( 
self . create_atomic_observe_operations ( 
~~ def get_variables ( self , include_submodules = False , include_nontrainable = False ) : 
~~~ model_variables = [ self . all_variables [ key ] for key in sorted ( self . all_variables ) ] 
states_preprocessing_variables = [ 
variable for name in sorted ( self . states_preprocessing ) 
for variable in self . states_preprocessing [ name ] . get_variables ( ) 
model_variables += states_preprocessing_variables 
actions_exploration_variables = [ 
variable for name in sorted ( self . actions_exploration ) 
for variable in self . actions_exploration [ name ] . get_variables ( ) 
model_variables += actions_exploration_variables 
if self . reward_preprocessing is not None : 
~~~ reward_preprocessing_variables = self . reward_preprocessing . get_variables ( ) 
model_variables += reward_preprocessing_variables 
~~~ model_variables = [ self . variables [ key ] for key in sorted ( self . variables ) ] 
~~ return model_variables 
fetches = [ self . global_episode , self . global_timestep ] 
~~~ fetch = self . states_preprocessing [ name ] . reset ( ) 
if fetch is not None : 
~~~ fetches . extend ( fetch ) 
~~ ~~ if self . flush_summarizer is not None : 
~~~ fetches . append ( self . flush_summarizer ) 
~~ fetch_list = self . monitored_session . run ( fetches = fetches ) 
episode , timestep = fetch_list [ : 2 ] 
return episode , timestep , self . internals_init 
~~ def get_feed_dict ( 
states = None , 
internals = None , 
actions = None , 
terminal = None , 
reward = None , 
deterministic = None , 
independent = None , 
index = None 
feed_dict = dict ( ) 
batched = None 
if states is not None : 
~~~ if batched is None : 
~~~ name = next ( iter ( states ) ) 
state = np . asarray ( states [ name ] ) 
batched = ( state . ndim != len ( self . states_spec [ name ] [ 'unprocessed_shape' ] ) ) 
~~ if batched : 
~~~ feed_dict . update ( { self . states_input [ name ] : states [ name ] for name in sorted ( self . states_input ) } ) 
~~~ feed_dict . update ( { self . states_input [ name ] : ( states [ name ] , ) for name in sorted ( self . states_input ) } ) 
~~ ~~ if internals is not None : 
~~~ name = next ( iter ( internals ) ) 
internal = np . asarray ( internals [ name ] ) 
batched = ( internal . ndim != len ( self . internals_spec [ name ] [ 'shape' ] ) ) 
~~~ feed_dict . update ( { self . internals_input [ name ] : internals [ name ] for name in sorted ( self . internals_input ) } ) 
~~~ feed_dict . update ( { self . internals_input [ name ] : ( internals [ name ] , ) for name in sorted ( self . internals_input ) } ) 
~~ ~~ if actions is not None : 
~~~ name = next ( iter ( actions ) ) 
action = np . asarray ( actions [ name ] ) 
batched = ( action . ndim != len ( self . actions_spec [ name ] [ 'shape' ] ) ) 
~~~ feed_dict . update ( { self . actions_input [ name ] : actions [ name ] for name in sorted ( self . actions_input ) } ) 
~~~ feed_dict . update ( { self . actions_input [ name ] : ( actions [ name ] , ) for name in sorted ( self . actions_input ) } ) 
~~ ~~ if terminal is not None : 
~~~ terminal = np . asarray ( terminal ) 
batched = ( terminal . ndim == 1 ) 
~~~ feed_dict [ self . terminal_input ] = terminal 
~~~ feed_dict [ self . terminal_input ] = ( terminal , ) 
~~ ~~ if reward is not None : 
~~~ reward = np . asarray ( reward ) 
batched = ( reward . ndim == 1 ) 
~~~ feed_dict [ self . reward_input ] = reward 
~~~ feed_dict [ self . reward_input ] = ( reward , ) 
~~ ~~ if deterministic is not None : 
~~~ feed_dict [ self . deterministic_input ] = deterministic 
~~ if independent is not None : 
~~~ feed_dict [ self . independent_input ] = independent 
~~ feed_dict [ self . episode_index_input ] = index 
return feed_dict 
~~ def act ( self , states , internals , deterministic = False , independent = False , fetch_tensors = None , index = 0 ) : 
name = next ( iter ( states ) ) 
if batched : 
~~~ assert state . shape [ 0 ] <= self . batching_capacity 
~~ fetches = [ self . actions_output , self . internals_output , self . timestep_output ] 
if self . network is not None and fetch_tensors is not None : 
~~~ for name in fetch_tensors : 
~~~ valid , tensor = self . network . get_named_tensor ( name ) 
if valid : 
~~~ fetches . append ( tensor ) 
~~~ keys = self . network . get_list_of_named_tensor ( ) 
raise TensorForceError ( \ . format ( name , keys ) ) 
~~ ~~ ~~ feed_dict = self . get_feed_dict ( 
fetch_list = self . monitored_session . run ( fetches = fetches , feed_dict = feed_dict ) 
actions , internals , timestep = fetch_list [ 0 : 3 ] 
if not batched : 
~~~ actions = { name : actions [ name ] [ 0 ] for name in sorted ( actions ) } 
internals = { name : internals [ name ] [ 0 ] for name in sorted ( internals ) } 
~~ if self . network is not None and fetch_tensors is not None : 
~~~ fetch_dict = dict ( ) 
for index_ , tensor in enumerate ( fetch_list [ 3 : ] ) : 
~~~ name = fetch_tensors [ index_ ] 
fetch_dict [ name ] = tensor 
~~ return actions , internals , timestep , fetch_dict 
~~~ return actions , internals , timestep 
~~ ~~ def observe ( self , terminal , reward , index = 0 ) : 
fetches = self . episode_output 
feed_dict = self . get_feed_dict ( terminal = terminal , reward = reward , index = index ) 
episode = self . monitored_session . run ( fetches = fetches , feed_dict = feed_dict ) 
return episode 
~~ def save ( self , directory = None , append_timestep = True ) : 
~~ return self . saver . save ( 
sess = self . session , 
save_path = ( self . saver_directory if directory is None else directory ) , 
global_step = ( self . global_timestep if append_timestep else None ) , 
meta_graph_suffix = 'meta' , 
write_meta_graph = True , 
write_state = True 
~~ def restore ( self , directory = None , file = None ) : 
checkpoint_dir = ( self . saver_directory if directory is None else directory ) , 
~~ elif directory is None : 
~~~ file = os . path . join ( self . saver_directory , file ) 
~~ self . saver . restore ( sess = self . session , save_path = file ) 
self . session . run ( fetches = self . list_buffer_index_reset_op ) 
~~ def get_savable_components ( self ) : 
components = self . get_components ( ) 
components = [ components [ name ] for name in sorted ( components ) ] 
return set ( filter ( lambda x : isinstance ( x , util . SavableComponent ) , components ) ) 
~~ def save_component ( self , component_name , save_path ) : 
component = self . get_component ( component_name = component_name ) 
self . _validate_savable ( component = component , component_name = component_name ) 
return component . save ( sess = self . session , save_path = save_path ) 
~~ def restore_component ( self , component_name , save_path ) : 
component . restore ( sess = self . session , save_path = save_path ) 
~~ def get_component ( self , component_name ) : 
mapping = self . get_components ( ) 
return mapping [ component_name ] if component_name in mapping else None 
~~ def import_demonstrations ( self , demonstrations ) : 
if isinstance ( demonstrations , dict ) : 
~~~ demonstrations [ 'states' ] = dict ( state = demonstrations [ 'states' ] ) 
~~~ demonstrations [ 'actions' ] = dict ( action = demonstrations [ 'actions' ] ) 
~~ self . model . import_demo_experience ( ** demonstrations ) 
~~~ states = { name : list ( ) for name in demonstrations [ 0 ] [ 'states' ] } 
~~ internals = { name : list ( ) for name in demonstrations [ 0 ] [ 'internals' ] } 
~~~ actions = { name : list ( ) for name in demonstrations [ 0 ] [ 'actions' ] } 
for demonstration in demonstrations : 
~~~ states [ 'state' ] . append ( demonstration [ 'states' ] ) 
~~~ for name , state in states . items ( ) : 
~~~ state . append ( demonstration [ 'states' ] [ name ] ) 
~~ ~~ for name , internal in internals . items ( ) : 
~~~ internal . append ( demonstration [ 'internals' ] [ name ] ) 
~~~ actions [ 'action' ] . append ( demonstration [ 'actions' ] ) 
~~~ for name , action in actions . items ( ) : 
~~~ action . append ( demonstration [ 'actions' ] [ name ] ) 
~~ ~~ terminal . append ( demonstration [ 'terminal' ] ) 
reward . append ( demonstration [ 'reward' ] ) 
~~ self . model . import_demo_experience ( 
if seed is None : 
~~~ self . env . seed = round ( time . time ( ) ) 
~~~ self . env . seed = seed 
~~ return self . env . seed 
if self . env . game_over ( ) : 
~~~ return self . env . getScreenRGB ( ) , True , 0 
~~ action_space = self . env . getActionSet ( ) 
reward = self . env . act ( action_space [ action ] ) 
new_state = self . env . getScreenRGB ( ) 
done = self . env . game_over ( ) 
return new_state , done , reward 
~~ def states ( self ) : 
screen = self . env . getScreenRGB ( ) 
return dict ( shape = screen . shape , type = 'int' ) 
~~ def sanity_check_states ( states_spec ) : 
states = copy . deepcopy ( states_spec ) 
is_unique = ( 'shape' in states ) 
if is_unique : 
~~ for name , state in states . items ( ) : 
~~~ if isinstance ( state [ 'shape' ] , int ) : 
~~~ state [ 'shape' ] = ( state [ 'shape' ] , ) 
~~ if 'type' not in state : 
~~~ state [ 'type' ] = 'float' 
~~ ~~ return states , is_unique 
~~ def sanity_check_actions ( actions_spec ) : 
actions = copy . deepcopy ( actions_spec ) 
is_unique = ( 'type' in actions ) 
~~~ if 'type' not in action : 
~~~ action [ 'type' ] = 'int' 
~~ if action [ 'type' ] == 'int' : 
~~~ if 'num_actions' not in action : 
~~ ~~ elif action [ 'type' ] == 'float' : 
~~~ if ( 'min_value' in action ) != ( 'max_value' in action ) : 
~~ ~~ if 'shape' not in action : 
~~~ action [ 'shape' ] = ( ) 
~~ if isinstance ( action [ 'shape' ] , int ) : 
~~~ action [ 'shape' ] = ( action [ 'shape' ] , ) 
~~ ~~ return actions , is_unique 
~~ def sanity_check_execution_spec ( execution_spec ) : 
def_ = dict ( type = "single" , 
distributed_spec = None , 
session_config = None ) 
if execution_spec is None : 
~~~ return def_ 
type_ = execution_spec . get ( "type" ) 
if type_ == "distributed" : 
~~~ def_ = dict ( job = "ps" , task_index = 0 , cluster_spec = { 
"ps" : [ "localhost:22222" ] , 
"worker" : [ "localhost:22223" ] 
def_ . update ( execution_spec . get ( "distributed_spec" , { } ) ) 
execution_spec [ "distributed_spec" ] = def_ 
execution_spec [ "session_config" ] = execution_spec . get ( "session_config" ) 
return execution_spec 
~~ elif type_ == "multi-threaded" : 
~~~ return execution_spec 
~~ elif type_ == "single" : 
~~ if execution_spec . get ( 'num_parallel' ) != None : 
~~ def make_game ( ) : 
return ascii_art . ascii_art_to_game ( 
sprites = dict ( 
[ ( 'P' , PlayerSprite ) ] + 
[ ( c , UpwardLaserBoltSprite ) for c in UPWARD_BOLT_CHARS ] + 
[ ( c , DownwardLaserBoltSprite ) for c in DOWNWARD_BOLT_CHARS ] ) , 
drapes = dict ( X = MarauderDrape , 
B = BunkerDrape ) , 
update_schedule = [ 'P' , 'B' , 'X' ] + list ( _ALL_BOLT_CHARS ) ) 
~~ def _fly ( self , board , layers , things , the_plot ) : 
if ( self . character in the_plot [ 'bunker_hitters' ] or 
self . character in the_plot [ 'marauder_hitters' ] ) : 
~~~ return self . _teleport ( ( - 1 , - 1 ) ) 
~~ self . _north ( board , the_plot ) 
~~ def _fire ( self , layers , things , the_plot ) : 
if the_plot . get ( 'last_player_shot' ) == the_plot . frame : return 
the_plot [ 'last_player_shot' ] = the_plot . frame 
row , col = things [ 'P' ] . position 
self . _teleport ( ( row - 1 , col ) ) 
if self . character in the_plot [ 'bunker_hitters' ] : 
~~ if self . position == things [ 'P' ] . position : the_plot . terminate_episode ( ) 
self . _south ( board , the_plot ) 
~~ def _fire ( self , layers , the_plot ) : 
if the_plot . get ( 'last_marauder_shot' ) == the_plot . frame : return 
the_plot [ 'last_marauder_shot' ] = the_plot . frame 
col = np . random . choice ( np . nonzero ( layers [ 'X' ] . sum ( axis = 0 ) ) [ 0 ] ) 
row = np . nonzero ( layers [ 'X' ] [ : , col ] ) [ 0 ] [ - 1 ] + 1 
self . _teleport ( ( row , col ) ) 
~~ def reset ( self , history = None ) : 
if not history : 
~~~ history = dict ( ) 
~~ self . episode_rewards = history . get ( "episode_rewards" , list ( ) ) 
self . episode_timesteps = history . get ( "episode_timesteps" , list ( ) ) 
self . episode_times = history . get ( "episode_times" , list ( ) ) 
~~ def run ( self , num_episodes , num_timesteps , max_episode_timesteps , deterministic , episode_finished , summary_report , 
summary_interval ) : 
~~ def tf_retrieve_indices ( self , buffer_elements , priority_indices ) : 
states = dict ( ) 
buffer_start = self . buffer_index - buffer_elements 
buffer_end = self . buffer_index 
for name in sorted ( self . states_memory ) : 
~~~ buffer_state_memory = self . states_buffer [ name ] 
buffer_states = buffer_state_memory [ buffer_start : buffer_end ] 
memory_states = tf . gather ( params = self . states_memory [ name ] , indices = priority_indices ) 
states [ name ] = tf . concat ( values = ( buffer_states , memory_states ) , axis = 0 ) 
~~ internals = dict ( ) 
for name in sorted ( self . internals_memory ) : 
~~~ internal_buffer_memory = self . internals_buffer [ name ] 
buffer_internals = internal_buffer_memory [ buffer_start : buffer_end ] 
memory_internals = tf . gather ( params = self . internals_memory [ name ] , indices = priority_indices ) 
internals [ name ] = tf . concat ( values = ( buffer_internals , memory_internals ) , axis = 0 ) 
~~ actions = dict ( ) 
for name in sorted ( self . actions_memory ) : 
~~~ action_buffer_memory = self . actions_buffer [ name ] 
buffer_action = action_buffer_memory [ buffer_start : buffer_end ] 
memory_action = tf . gather ( params = self . actions_memory [ name ] , indices = priority_indices ) 
actions [ name ] = tf . concat ( values = ( buffer_action , memory_action ) , axis = 0 ) 
~~ buffer_terminal = self . terminal_buffer [ buffer_start : buffer_end ] 
priority_terminal = tf . gather ( params = self . terminal_memory , indices = priority_indices ) 
terminal = tf . concat ( values = ( buffer_terminal , priority_terminal ) , axis = 0 ) 
buffer_reward = self . reward_buffer [ buffer_start : buffer_end ] 
priority_reward = tf . gather ( params = self . reward_memory , indices = priority_indices ) 
reward = tf . concat ( values = ( buffer_reward , priority_reward ) , axis = 0 ) 
if self . include_next_states : 
~~~ assert util . rank ( priority_indices ) == 1 
next_priority_indices = ( priority_indices + 1 ) % self . capacity 
next_buffer_start = ( buffer_start + 1 ) % self . buffer_size 
next_buffer_end = ( buffer_end + 1 ) % self . buffer_size 
next_states = dict ( ) 
buffer_next_states = buffer_state_memory [ next_buffer_start : next_buffer_end ] 
memory_next_states = tf . gather ( params = self . states_memory [ name ] , indices = next_priority_indices ) 
next_states [ name ] = tf . concat ( values = ( buffer_next_states , memory_next_states ) , axis = 0 ) 
~~ next_internals = dict ( ) 
~~~ buffer_internal_memory = self . internals_buffer [ name ] 
buffer_next_internals = buffer_internal_memory [ next_buffer_start : next_buffer_end ] 
memory_next_internals = tf . gather ( params = self . internals_memory [ name ] , indices = next_priority_indices ) 
next_internals [ name ] = tf . concat ( values = ( buffer_next_internals , memory_next_internals ) , axis = 0 ) 
~~ return dict ( 
~~ ~~ def tf_update_batch ( self , loss_per_instance ) : 
mask = tf . not_equal ( 
x = self . batch_indices , 
y = tf . zeros ( shape = tf . shape ( input = self . batch_indices ) , dtype = tf . int32 ) 
priority_indices = tf . reshape ( tensor = tf . where ( condition = mask ) , shape = [ - 1 ] ) 
sampled_buffer_batch = self . tf_retrieve_indices ( 
buffer_elements = self . last_batch_buffer_elems , 
priority_indices = priority_indices 
states = sampled_buffer_batch [ 'states' ] 
internals = sampled_buffer_batch [ 'internals' ] 
actions = sampled_buffer_batch [ 'actions' ] 
terminal = sampled_buffer_batch [ 'terminal' ] 
reward = sampled_buffer_batch [ 'reward' ] 
priorities = loss_per_instance ** self . prioritization_weight 
assignments = list ( ) 
memory_end_index = self . memory_index + self . last_batch_buffer_elems 
memory_insert_indices = tf . range ( 
start = self . memory_index , 
limit = memory_end_index 
) % self . capacity 
~~~ assignments . append ( tf . scatter_update ( 
ref = self . states_memory [ name ] , 
indices = memory_insert_indices , 
updates = states [ name ] [ 0 : self . last_batch_buffer_elems ] ) 
ref = self . internals_buffer [ name ] , 
updates = internals [ name ] [ 0 : self . last_batch_buffer_elems ] 
~~ assignments . append ( tf . scatter_update ( 
ref = self . priorities , 
updates = priorities [ 0 : self . last_batch_buffer_elems ] 
assignments . append ( tf . scatter_update ( 
ref = self . terminal_memory , 
updates = terminal [ 0 : self . last_batch_buffer_elems ] ) 
ref = self . reward_memory , 
updates = reward [ 0 : self . last_batch_buffer_elems ] ) 
for name in sorted ( actions ) : 
ref = self . actions_memory [ name ] , 
updates = actions [ name ] [ 0 : self . last_batch_buffer_elems ] 
~~ main_memory_priorities = priorities [ self . last_batch_buffer_elems : ] 
main_memory_priorities = main_memory_priorities [ 0 : tf . shape ( priority_indices ) [ 0 ] ] 
indices = priority_indices , 
updates = main_memory_priorities 
with tf . control_dependencies ( control_inputs = assignments ) : 
~~~ assignments = list ( ) 
sorted_priorities , sorted_indices = tf . nn . top_k ( 
input = self . priorities , 
k = self . capacity , 
sorted = True 
assignments . append ( tf . assign ( ref = self . priorities , value = sorted_priorities ) ) 
indices = sorted_indices , 
updates = self . terminal_memory 
updates = self . states_memory [ name ] 
~~ for name in sorted ( self . actions_memory ) : 
updates = self . actions_memory [ name ] 
~~ for name in sorted ( self . internals_memory ) : 
ref = self . internals_memory [ name ] , 
updates = self . internals_memory [ name ] 
updates = self . reward_memory 
~~ with tf . control_dependencies ( control_inputs = assignments ) : 
assignments . append ( tf . assign_sub ( ref = self . buffer_index , value = self . last_batch_buffer_elems ) ) 
total_inserted_elements = self . memory_size + self . last_batch_buffer_elems 
assignments . append ( tf . assign ( 
ref = self . memory_size , 
value = tf . minimum ( x = total_inserted_elements , y = self . capacity ) ) 
assignments . append ( tf . assign ( ref = self . memory_index , value = memory_end_index ) ) 
ref = self . batch_indices , 
value = tf . zeros ( shape = tf . shape ( self . batch_indices ) , dtype = tf . int32 ) 
~~ ~~ def setup_components_and_tf_funcs ( self , custom_getter = None ) : 
self . network = Network . from_spec ( 
spec = self . network_spec , 
assert len ( self . internals_spec ) == 0 
self . internals_spec = self . network . internals_spec ( ) 
for name in sorted ( self . internals_spec ) : 
~~~ internal = self . internals_spec [ name ] 
self . internals_input [ name ] = tf . placeholder ( 
dtype = util . tf_dtype ( internal [ 'type' ] ) , 
shape = ( None , ) + tuple ( internal [ 'shape' ] ) , 
name = ( 'internal-' + name ) 
if internal [ 'initialization' ] == 'zeros' : 
~~~ self . internals_init [ name ] = np . zeros ( shape = internal [ 'shape' ] ) 
~~ ~~ custom_getter = super ( DistributionModel , self ) . setup_components_and_tf_funcs ( custom_getter ) 
self . distributions = self . create_distributions ( ) 
self . fn_kl_divergence = tf . make_template ( 
name_ = 'kl-divergence' , 
func_ = self . tf_kl_divergence , 
~~ def create_distributions ( self ) : 
distributions = dict ( ) 
for name in sorted ( self . actions_spec ) : 
~~~ action = self . actions_spec [ name ] 
if self . distributions_spec is not None and name in self . distributions_spec : 
~~~ kwargs = dict ( action ) 
kwargs [ 'scope' ] = name 
kwargs [ 'summary_labels' ] = self . summary_labels 
distributions [ name ] = Distribution . from_spec ( 
spec = self . distributions_spec [ name ] , 
~~ elif action [ 'type' ] == 'bool' : 
~~~ distributions [ name ] = Bernoulli ( 
shape = action [ 'shape' ] , 
scope = name , 
~~ elif action [ 'type' ] == 'int' : 
~~~ distributions [ name ] = Categorical ( 
num_actions = action [ 'num_actions' ] , 
~~ elif action [ 'type' ] == 'float' : 
~~~ if 'min_value' in action : 
~~~ distributions [ name ] = Beta ( 
min_value = action [ 'min_value' ] , 
max_value = action [ 'max_value' ] , 
~~~ distributions [ name ] = Gaussian ( 
~~ ~~ ~~ return distributions 
~~ def from_spec ( spec ) : 
exploration = util . get_object ( 
predefined_objects = tensorforce . core . explorations . explorations 
assert isinstance ( exploration , Exploration ) 
return exploration 
memory = util . get_object ( 
predefined_objects = tensorforce . core . memories . memories , 
assert isinstance ( memory , Memory ) 
return memory 
arguments_iter = iter ( arguments . values ( ) ) 
some_argument = next ( arguments_iter ) 
~~~ while not isinstance ( some_argument , tf . Tensor ) or util . rank ( some_argument ) == 0 : 
~~~ if isinstance ( some_argument , dict ) : 
~~~ if some_argument : 
~~~ arguments_iter = iter ( some_argument . values ( ) ) 
~~ some_argument = next ( arguments_iter ) 
~~ elif isinstance ( some_argument , list ) : 
~~~ arguments_iter = iter ( some_argument ) 
~~ elif some_argument is None or util . rank ( some_argument ) == 0 : 
~~~ some_argument = next ( arguments_iter ) 
~~ ~~ ~~ except StopIteration : 
~~ batch_size = tf . shape ( input = some_argument ) [ 0 ] 
num_samples = tf . cast ( 
x = ( self . fraction * tf . cast ( x = batch_size , dtype = util . tf_dtype ( 'float' ) ) ) , 
dtype = util . tf_dtype ( 'int' ) 
num_samples = tf . maximum ( x = num_samples , y = 1 ) 
indices = tf . random_uniform ( shape = ( num_samples , ) , maxval = batch_size , dtype = tf . int32 ) 
subsampled_arguments = util . map_tensors ( 
fn = ( lambda arg : arg if util . rank ( arg ) == 0 else tf . gather ( params = arg , indices = indices ) ) , 
tensors = arguments 
return self . optimizer . step ( 
arguments = subsampled_arguments , 
~~ def run ( self , num_timesteps = None , num_episodes = None , max_episode_timesteps = None , deterministic = False , 
episode_finished = None , summary_report = None , summary_interval = None , timesteps = None , episodes = None , testing = False , sleep = None 
if timesteps is not None : 
~~~ num_timesteps = timesteps 
~~ if episodes is not None : 
~~ old_episode_finished = False 
~~ self . start_time = time . time ( ) 
self . agent . reset ( ) 
if num_episodes is not None : 
~~~ num_episodes += self . agent . episode 
~~ if num_timesteps is not None : 
~~~ num_timesteps += self . agent . timestep 
~~ with tqdm ( total = num_episodes ) as pbar : 
~~~ episode_start_time = time . time ( ) 
state = self . environment . reset ( ) 
self . current_timestep = 0 
~~~ action = self . agent . act ( states = state , deterministic = deterministic ) 
for _ in xrange ( self . repeat_actions ) : 
~~~ state , terminal , step_reward = self . environment . execute ( action = action ) 
~~ ~~ if max_episode_timesteps is not None and self . current_timestep >= max_episode_timesteps : 
~~~ terminal = True 
~~ if not testing : 
~~~ self . agent . observe ( terminal = terminal , reward = reward ) 
~~ self . global_timestep += 1 
self . current_timestep += 1 
~~ ~~ time_passed = time . time ( ) - episode_start_time 
self . episode_timesteps . append ( self . current_timestep ) 
self . episode_times . append ( time_passed ) 
self . global_episode += 1 
pbar . update ( 1 ) 
~~~ if not episode_finished ( self ) : 
~~ ~~ elif not episode_finished ( self , self . id ) : 
~~ ~~ if ( num_episodes is not None and self . global_episode >= num_episodes ) or ( num_timesteps is not None and self . global_timestep >= num_timesteps ) or self . agent . should_stop ( ) : 
~~ ~~ pbar . update ( num_episodes - self . global_episode ) 
~~ ~~ def tf_retrieve_indices ( self , indices ) : 
~~~ states [ name ] = tf . gather ( params = self . states_memory [ name ] , indices = indices ) 
~~~ internals [ name ] = tf . gather ( params = self . internals_memory [ name ] , indices = indices ) 
~~~ actions [ name ] = tf . gather ( params = self . actions_memory [ name ] , indices = indices ) 
~~ terminal = tf . gather ( params = self . terminal_memory , indices = indices ) 
reward = tf . gather ( params = self . reward_memory , indices = indices ) 
~~~ assert util . rank ( indices ) == 1 
next_indices = ( indices + 1 ) % self . capacity 
~~~ next_states [ name ] = tf . gather ( params = self . states_memory [ name ] , indices = next_indices ) 
~~~ next_internals [ name ] = tf . gather ( params = self . internals_memory [ name ] , indices = next_indices ) 
~~ ~~ def tf_solve ( self , fn_x , x_init , base_value , target_value , estimated_improvement = None ) : 
return super ( LineSearch , self ) . tf_solve ( fn_x , x_init , base_value , target_value , estimated_improvement ) 
~~ def tf_initialize ( self , x_init , base_value , target_value , estimated_improvement ) : 
self . base_value = base_value 
~~~ estimated_improvement = tf . abs ( x = base_value ) 
~~ first_step = super ( LineSearch , self ) . tf_initialize ( x_init ) 
improvement = tf . divide ( 
x = ( target_value - self . base_value ) , 
y = tf . maximum ( x = estimated_improvement , y = util . epsilon ) 
last_improvement = improvement - 1.0 
if self . mode == 'linear' : 
~~~ deltas = [ - t * self . parameter for t in x_init ] 
self . estimated_incr = - estimated_improvement * self . parameter 
~~ elif self . mode == 'exponential' : 
~~ return first_step + ( deltas , improvement , last_improvement , estimated_improvement ) 
~~ def tf_step ( self , x , iteration , deltas , improvement , last_improvement , estimated_improvement ) : 
x , next_iteration , deltas , improvement , last_improvement , estimated_improvement = super ( LineSearch , self ) . tf_step ( 
x , iteration , deltas , improvement , last_improvement , estimated_improvement 
next_x = [ t + delta for t , delta in zip ( x , deltas ) ] 
~~~ next_deltas = deltas 
next_estimated_improvement = estimated_improvement + self . estimated_incr 
~~~ next_deltas = [ delta * self . parameter for delta in deltas ] 
next_estimated_improvement = estimated_improvement * self . parameter 
~~ target_value = self . fn_x ( next_deltas ) 
next_improvement = tf . divide ( 
y = tf . maximum ( x = next_estimated_improvement , y = util . epsilon ) 
return next_x , next_iteration , next_deltas , next_improvement , improvement , next_estimated_improvement 
~~ def tf_next_step ( self , x , iteration , deltas , improvement , last_improvement , estimated_improvement ) : 
next_step = super ( LineSearch , self ) . tf_next_step ( 
def undo_deltas ( ) : 
~~~ value = self . fn_x ( [ - delta for delta in deltas ] ) 
with tf . control_dependencies ( control_inputs = ( value , ) ) : 
~~ ~~ improved = tf . cond ( 
pred = ( improvement > last_improvement ) , 
true_fn = ( lambda : True ) , 
false_fn = undo_deltas 
next_step = tf . logical_and ( x = next_step , y = improved ) 
next_step = tf . logical_and ( x = next_step , y = ( improvement < self . accept_ratio ) ) 
return tf . logical_and ( x = next_step , y = ( estimated_improvement > util . epsilon ) ) 
~~ def escape ( text , quote = False , smart_amp = True ) : 
if smart_amp : 
~~~ text = _escape_pattern . sub ( '&amp;' , text ) 
~~~ text = text . replace ( '&' , '&amp;' ) 
~~ text = text . replace ( '<' , '&lt;' ) 
text = text . replace ( '>' , '&gt;' ) 
if quote : 
~~~ text = text . replace ( \ , '&quot;' ) 
text = text . replace ( "\ , '&#39;' ) 
~~ def escape_link ( url ) : 
for scheme in _scheme_blacklist : 
~~~ if lower_url . startswith ( scheme ) : 
~~ ~~ return escape ( url , quote = True , smart_amp = False ) 
~~ def markdown ( text , escape = True , ** kwargs ) : 
return Markdown ( escape = escape , ** kwargs ) ( text ) 
~~ def parse_lheading ( self , m ) : 
self . tokens . append ( { 
'type' : 'heading' , 
'level' : 1 if m . group ( 2 ) == '=' else 2 , 
'text' : m . group ( 1 ) , 
~~ def hard_wrap ( self ) : 
self . text = re . compile ( 
~~ def block_code ( self , code , lang = None ) : 
code = code . rstrip ( '\\n' ) 
if not lang : 
~~~ code = escape ( code , smart_amp = False ) 
return '<pre><code>%s\\n</code></pre>\\n' % code 
~~ code = escape ( code , quote = True , smart_amp = False ) 
return \ % ( lang , code ) 
~~ def block_html ( self , html ) : 
if self . options . get ( 'skip_style' ) and html . lower ( ) . startswith ( '<style' ) : 
~~ if self . options . get ( 'escape' ) : 
~~~ return escape ( html ) 
~~ return html 
tag = 'ul' 
if ordered : 
~~~ tag = 'ol' 
~~ return '<%s>\\n%s</%s>\\n' % ( tag , body , tag ) 
~~ def table_cell ( self , content , ** flags ) : 
if flags [ 'header' ] : 
~~~ tag = 'th' 
~~~ tag = 'td' 
~~ align = flags [ 'align' ] 
if not align : 
~~~ return '<%s>%s</%s>\\n' % ( tag , content , tag ) 
~~ return \ % ( 
tag , align , content , tag 
text = escape ( text . rstrip ( ) , smart_amp = False ) 
return '<code>%s</code>' % text 
~~ def autolink ( self , link , is_email = False ) : 
text = link = escape ( link ) 
if is_email : 
~~~ link = 'mailto:%s' % link 
~~ return \ % ( link , text ) 
~~ def link ( self , link , title , text ) : 
link = escape_link ( link ) 
if not title : 
~~~ return \ % ( link , text ) 
~~ title = escape ( title , quote = True ) 
return \ % ( link , title , text ) 
src = escape_link ( src ) 
text = escape ( text , quote = True ) 
~~~ title = escape ( title , quote = True ) 
html = \ % ( src , text , title ) 
~~~ html = \ % ( src , text ) 
~~ if self . options . get ( 'use_xhtml' ) : 
~~ return '%s>' % html 
~~ def footnote_ref ( self , key , index ) : 
html = ( 
) % ( escape ( key ) , escape ( key ) , index ) 
~~ def footnote_item ( self , key , text ) : 
back = ( 
) % escape ( key ) 
text = text . rstrip ( ) 
if text . endswith ( '</p>' ) : 
~~~ text = re . sub ( r'<\\/p>$' , r'%s</p>' % back , text ) 
~~~ text = '%s<p>%s</p>' % ( text , back ) 
~~ html = \ % ( escape ( key ) , text ) 
~~ def build_metagraph_list ( self ) : 
self . ignore_unknown_dtypes = True 
for key in sorted ( self . meta_params ) : 
~~~ value = self . convert_data_to_string ( self . meta_params [ key ] ) 
if len ( value ) == 0 : 
~~ if isinstance ( value , str ) : 
~~~ ops . append ( tf . contrib . summary . generic ( name = key , tensor = tf . convert_to_tensor ( str ( value ) ) ) ) 
~~~ ops . append ( tf . contrib . summary . generic ( name = key , tensor = tf . as_string ( tf . convert_to_tensor ( value ) ) ) ) 
~~ ~~ return ops 
~~ def process_docstring ( app , what , name , obj , options , lines ) : 
markdown = "\\n" . join ( lines ) 
rest = m2r ( markdown ) 
rest . replace ( "\\r\\n" , "\\n" ) 
del lines [ : ] 
lines . extend ( rest . split ( "\\n" ) ) 
fn_kl_divergence , 
return_estimated_improvement = False , 
kldiv = fn_kl_divergence ( ** arguments ) 
kldiv_gradients = tf . gradients ( ys = kldiv , xs = variables ) 
def fisher_matrix_product ( deltas ) : 
~~~ deltas = [ tf . stop_gradient ( input = delta ) for delta in deltas ] 
delta_kldiv_gradients = tf . add_n ( inputs = [ 
tf . reduce_sum ( input_tensor = ( delta * grad ) ) for delta , grad in zip ( deltas , kldiv_gradients ) 
return tf . gradients ( ys = delta_kldiv_gradients , xs = variables ) 
~~ loss = fn_loss ( ** arguments ) 
loss_gradients = tf . gradients ( ys = loss , xs = variables ) 
deltas = self . solver . solve ( fn_x = fisher_matrix_product , x_init = None , b = [ - grad for grad in loss_gradients ] ) 
delta_fisher_matrix_product = fisher_matrix_product ( deltas = deltas ) 
constant = 0.5 * tf . add_n ( inputs = [ 
tf . reduce_sum ( input_tensor = ( delta_F * delta ) ) 
for delta_F , delta in zip ( delta_fisher_matrix_product , deltas ) 
def natural_gradient_step ( ) : 
~~~ lagrange_multiplier = tf . sqrt ( x = ( constant / self . learning_rate ) ) 
estimated_deltas = [ delta / lagrange_multiplier for delta in deltas ] 
estimated_improvement = tf . add_n ( inputs = [ 
tf . reduce_sum ( input_tensor = ( grad * delta ) ) 
for grad , delta in zip ( loss_gradients , estimated_deltas ) 
applied = self . apply_step ( variables = variables , deltas = estimated_deltas ) 
~~~ if return_estimated_improvement : 
~~~ return [ estimated_delta + 0.0 for estimated_delta in estimated_deltas ] , estimated_improvement 
~~~ return [ estimated_delta + 0.0 for estimated_delta in estimated_deltas ] 
~~ ~~ ~~ def zero_step ( ) : 
~~~ return [ tf . zeros_like ( tensor = delta ) for delta in deltas ] , 0.0 
~~~ return [ tf . zeros_like ( tensor = delta ) for delta in deltas ] 
~~ ~~ return tf . cond ( pred = ( constant > 0.0 ) , true_fn = natural_gradient_step , false_fn = zero_step ) 
~~ def tf_step ( self , time , variables , arguments , fn_reference = None , ** kwargs ) : 
deltas = self . optimizer . step ( time = time , variables = variables , arguments = arguments , ** kwargs ) 
~~~ for _ in xrange ( self . num_steps - 1 ) : 
~~~ step_deltas = self . optimizer . step ( time = time , variables = variables , arguments = arguments , ** kwargs ) 
deltas = [ delta1 + delta2 for delta1 , delta2 in zip ( deltas , step_deltas ) ] 
~~ ~~ return deltas 
~~~ def body ( iteration , deltas ) : 
return iteration + 1 , deltas 
~~ ~~ def cond ( iteration , deltas ) : 
~~~ return iteration < self . num_steps - 1 
~~ _ , deltas = tf . while_loop ( cond = cond , body = body , loop_vars = ( 0 , deltas ) ) 
return deltas 
~~ ~~ def tf_baseline_loss ( self , states , internals , reward , update , reference = None ) : 
if self . baseline_mode == 'states' : 
~~~ loss = self . baseline . loss ( 
~~ elif self . baseline_mode == 'network' : 
states = self . network . apply ( x = states , internals = internals , update = update ) , 
~~ regularization_loss = self . baseline . regularization_loss ( ) 
if regularization_loss is not None : 
~~~ loss += regularization_loss 
~~ def baseline_optimizer_arguments ( self , states , internals , reward ) : 
variables = self . baseline . get_variables ( ) , 
update = tf . constant ( value = True ) , 
fn_reference = self . baseline . reference , 
fn_loss = self . fn_baseline_loss , 
~~~ arguments [ 'global_variables' ] = self . global_model . baseline . get_variables ( ) 
~~ def tf_step ( self , time , variables , source_variables , ** kwargs ) : 
assert all ( util . shape ( source ) == util . shape ( target ) for source , target in zip ( source_variables , variables ) ) 
last_sync = tf . get_variable ( 
name = 'last-sync' , 
initializer = tf . constant_initializer ( value = ( - self . sync_frequency ) , dtype = tf . int64 ) , 
def sync ( ) : 
~~~ deltas = list ( ) 
for source_variable , target_variable in zip ( source_variables , variables ) : 
~~~ delta = self . update_weight * ( source_variable - target_variable ) 
deltas . append ( delta ) 
~~ applied = self . apply_step ( variables = variables , deltas = deltas ) 
last_sync_updated = last_sync . assign ( value = time ) 
with tf . control_dependencies ( control_inputs = ( applied , last_sync_updated ) ) : 
~~~ return [ delta + 0.0 for delta in deltas ] 
~~ ~~ def no_sync ( ) : 
for variable in variables : 
~~~ delta = tf . zeros ( shape = util . shape ( variable ) ) 
~~ return deltas 
~~ do_sync = ( time - last_sync >= self . sync_frequency ) 
return tf . cond ( pred = do_sync , true_fn = sync , false_fn = no_sync ) 
unperturbed_loss = fn_loss ( ** arguments ) 
perturbations = [ tf . random_normal ( shape = util . shape ( variable ) ) * self . learning_rate for variable in variables ] 
applied = self . apply_step ( variables = variables , deltas = perturbations ) 
~~~ perturbed_loss = fn_loss ( ** arguments ) 
direction = tf . sign ( x = ( unperturbed_loss - perturbed_loss ) ) 
deltas_sum = [ direction * perturbation for perturbation in perturbations ] 
~~ if self . unroll_loop : 
~~~ previous_perturbations = perturbations 
for sample in xrange ( self . num_samples ) : 
~~~ with tf . control_dependencies ( control_inputs = deltas_sum ) : 
~~~ perturbations = [ tf . random_normal ( shape = util . shape ( variable ) ) * self . learning_rate for variable in variables ] 
perturbation_deltas = [ 
pert - prev_pert for pert , prev_pert in zip ( perturbations , previous_perturbations ) 
applied = self . apply_step ( variables = variables , deltas = perturbation_deltas ) 
previous_perturbations = perturbations 
deltas_sum = [ delta + direction * perturbation for delta , perturbation in zip ( deltas_sum , perturbations ) ] 
~~~ def body ( iteration , deltas_sum , previous_perturbations ) : 
~~ return iteration + 1 , deltas_sum , perturbations 
~~ def cond ( iteration , deltas_sum , previous_perturbation ) : 
~~~ return iteration < self . num_samples - 1 
~~ _ , deltas_sum , perturbations = tf . while_loop ( cond = cond , body = body , loop_vars = ( 0 , deltas_sum , perturbations ) ) 
~~ with tf . control_dependencies ( control_inputs = deltas_sum ) : 
~~~ deltas = [ delta / self . num_samples for delta in deltas_sum ] 
perturbation_deltas = [ delta - pert for delta , pert in zip ( deltas , perturbations ) ] 
~~ ~~ def tf_step ( self , time , variables , ** kwargs ) : 
global_variables = kwargs [ "global_variables" ] 
assert all ( 
util . shape ( global_variable ) == util . shape ( local_variable ) 
for global_variable , local_variable in zip ( global_variables , variables ) 
local_deltas = self . optimizer . step ( time = time , variables = variables , ** kwargs ) 
with tf . control_dependencies ( control_inputs = local_deltas ) : 
~~~ applied = self . optimizer . apply_step ( variables = global_variables , deltas = local_deltas ) 
~~~ update_deltas = list ( ) 
for global_variable , local_variable in zip ( global_variables , variables ) : 
~~~ delta = global_variable - local_variable 
update_deltas . append ( delta ) 
~~ applied = self . apply_step ( variables = variables , deltas = update_deltas ) 
~~~ return [ local_delta + update_delta for local_delta , update_delta in zip ( local_deltas , update_deltas ) ] 
~~ ~~ def computeStatsEigen ( self ) : 
with tf . device ( '/cpu:0' ) : 
~~~ def removeNone ( tensor_list ) : 
~~~ local_list = [ ] 
for item in tensor_list : 
~~~ if item is not None : 
~~~ local_list . append ( item ) 
~~ ~~ return local_list 
~~ def copyStats ( var_list ) : 
redundant_stats = { } 
copied_list = [ ] 
for item in var_list : 
~~~ if item not in redundant_stats : 
~~~ if self . _use_float64 : 
~~~ redundant_stats [ item ] = tf . cast ( 
tf . identity ( item ) , tf . float64 ) 
~~~ redundant_stats [ item ] = tf . identity ( item ) 
~~ ~~ copied_list . append ( redundant_stats [ item ] ) 
~~~ copied_list . append ( None ) 
~~ ~~ return copied_list 
~~ stats_eigen = self . stats_eigen 
computedEigen = { } 
eigen_reverse_lookup = { } 
updateOps = [ ] 
with tf . control_dependencies ( [ ] ) : 
~~~ for stats_var in stats_eigen : 
~~~ if stats_var not in computedEigen : 
~~~ eigens = tf . self_adjoint_eig ( stats_var ) 
e = eigens [ 0 ] 
Q = eigens [ 1 ] 
if self . _use_float64 : 
~~~ e = tf . cast ( e , tf . float32 ) 
Q = tf . cast ( Q , tf . float32 ) 
~~ updateOps . append ( e ) 
updateOps . append ( Q ) 
computedEigen [ stats_var ] = { 'e' : e , 'Q' : Q } 
eigen_reverse_lookup [ e ] = stats_eigen [ stats_var ] [ 'e' ] 
eigen_reverse_lookup [ Q ] = stats_eigen [ stats_var ] [ 'Q' ] 
~~ ~~ ~~ self . eigen_reverse_lookup = eigen_reverse_lookup 
self . eigen_update_list = updateOps 
~~ return updateOps 
fn_loss = kwargs [ "fn_loss" ] 
if variables is None : 
~~~ variables = tf . trainable_variables 
~~ return tf . gradients ( fn_loss , variables ) 
~~ def apply_step ( self , variables , deltas , loss_sampled ) : 
update_stats_op = self . compute_and_apply_stats ( 
loss_sampled , var_list = var_list ) 
grads = [ ( a , b ) for a , b in zip ( deltas , varlist ) ] 
kfacOptim , _ = self . apply_gradients_kfac ( grads ) 
return kfacOptim 
loss = kwargs [ "fn_loss" ] 
sampled_loss = kwargs [ "sampled_loss" ] 
min_op , _ = self . minimize_ ( loss , sampled_loss , var_list = variables ) 
return min_op 
custom_getter = super ( QDemoModel , self ) . setup_components_and_tf_funcs ( custom_getter ) 
self . demo_memory = Replay ( 
include_next_states = True , 
capacity = self . demo_memory_capacity , 
scope = 'demo-replay' , 
self . fn_import_demo_experience = tf . make_template ( 
name_ = 'import-demo-experience' , 
func_ = self . tf_import_demo_experience , 
self . fn_demo_loss = tf . make_template ( 
name_ = 'demo-loss' , 
func_ = self . tf_demo_loss , 
self . fn_combined_loss = tf . make_template ( 
name_ = 'combined-loss' , 
func_ = self . tf_combined_loss , 
self . fn_demo_optimization = tf . make_template ( 
name_ = 'demo-optimization' , 
func_ = self . tf_demo_optimization , 
~~ def tf_import_demo_experience ( self , states , internals , actions , terminal , reward ) : 
return self . demo_memory . store ( 
~~ def tf_demo_loss ( self , states , actions , terminal , reward , internals , update , reference = None ) : 
embedding = self . network . apply ( x = states , internals = internals , update = update ) 
deltas = list ( ) 
~~~ action = actions [ name ] 
distr_params = self . distributions [ name ] . parameterize ( x = embedding ) 
state_action_value = self . distributions [ name ] . state_action_value ( distr_params = distr_params , action = action ) 
if self . actions_spec [ name ] [ 'type' ] == 'bool' : 
~~~ num_actions = 2 
action = tf . cast ( x = action , dtype = util . tf_dtype ( 'int' ) ) 
~~~ num_actions = self . actions_spec [ name ] [ 'num_actions' ] 
~~ one_hot = tf . one_hot ( indices = action , depth = num_actions ) 
ones = tf . ones_like ( tensor = one_hot , dtype = tf . float32 ) 
inverted_one_hot = ones - one_hot 
state_action_values = self . distributions [ name ] . state_action_value ( distr_params = distr_params ) 
state_action_values = state_action_values + inverted_one_hot * self . expert_margin 
supervised_selector = tf . reduce_max ( input_tensor = state_action_values , axis = - 1 ) 
delta = supervised_selector - state_action_value 
action_size = util . prod ( self . actions_spec [ name ] [ 'shape' ] ) 
delta = tf . reshape ( tensor = delta , shape = ( - 1 , action_size ) ) 
~~ loss_per_instance = tf . reduce_mean ( input_tensor = tf . concat ( values = deltas , axis = 1 ) , axis = 1 ) 
loss_per_instance = tf . square ( x = loss_per_instance ) 
return tf . reduce_mean ( input_tensor = loss_per_instance , axis = 0 ) 
~~ def tf_combined_loss ( self , states , internals , actions , terminal , reward , next_states , next_internals , update , reference = None ) : 
q_model_loss = self . fn_loss ( 
demo_loss = self . fn_demo_loss ( 
return q_model_loss + self . supervised_weight * demo_loss 
model_variables = super ( QDemoModel , self ) . get_variables ( 
include_submodules = include_submodules , 
include_nontrainable = include_nontrainable 
~~~ demo_memory_variables = self . demo_memory . get_variables ( ) 
model_variables += demo_memory_variables 
~~ def import_demo_experience ( self , states , internals , actions , terminal , reward ) : 
fetches = self . import_demo_experience_output 
~~ def demo_update ( self ) : 
fetches = self . demo_optimization_output 
self . monitored_session . run ( fetches = fetches ) 
~~ def from_config ( config , kwargs = None ) : 
return util . get_object ( 
obj = config , 
predefined = tensorforce . core . optimizers . solvers . solvers , 
arguments = kwargs [ "arguments" ] 
loss = fn_loss ( ** arguments ) 
with tf . control_dependencies ( control_inputs = ( loss , ) ) : 
~~~ previous_variables = [ variable + 0.0 for variable in variables ] 
~~ with tf . control_dependencies ( control_inputs = previous_variables ) : 
~~~ return [ 
variable - previous_variable 
for variable , previous_variable in zip ( variables , previous_variables ) 
~~ ~~ def _saliency_map ( self , a , image , target , labels , mask , fast = False ) : 
alphas = a . gradient ( image , target ) * mask 
if fast : 
~~~ betas = - np . ones_like ( alphas ) 
~~~ betas = np . sum ( [ 
a . gradient ( image , label ) * mask - alphas 
for label in labels ] , 0 ) 
~~ salmap = np . abs ( alphas ) * np . abs ( betas ) * np . sign ( alphas * betas ) 
idx = np . argmin ( salmap ) 
idx = np . unravel_index ( idx , mask . shape ) 
pix_sign = np . sign ( alphas ) [ idx ] 
return idx , pix_sign 
~~ def from_keras ( cls , model , bounds , input_shape = None , 
channel_axis = 3 , preprocessing = ( 0 , 1 ) ) : 
if input_shape is None : 
~~~ input_shape = model . input_shape [ 1 : ] 
~~ ~~ with tf . keras . backend . get_session ( ) . as_default ( ) : 
~~~ inputs = tf . placeholder ( tf . float32 , ( None , ) + input_shape ) 
logits = model ( inputs ) 
return cls ( inputs , logits , bounds = bounds , 
channel_axis = channel_axis , preprocessing = preprocessing ) 
~~ ~~ def normalized_distance ( self , image ) : 
return self . __distance ( 
self . __original_image_for_distance , 
bounds = self . bounds ( ) ) 
~~ def __is_adversarial ( self , image , predictions , in_bounds ) : 
is_adversarial = self . __criterion . is_adversarial ( 
predictions , self . __original_class ) 
assert isinstance ( is_adversarial , bool ) or isinstance ( is_adversarial , np . bool_ ) 
if is_adversarial : 
~~~ is_best , distance = self . __new_adversarial ( 
image , predictions , in_bounds ) 
~~~ is_best = False 
distance = None 
~~ return is_adversarial , is_best , distance 
~~ def channel_axis ( self , batch ) : 
axis = self . __model . channel_axis ( ) 
if not batch : 
~~~ axis = axis - 1 
~~ return axis 
~~ def has_gradient ( self ) : 
~~~ self . __model . gradient 
self . __model . predictions_and_gradient 
~~ ~~ def predictions ( self , image , strict = True , return_details = False ) : 
in_bounds = self . in_bounds ( image ) 
assert not strict or in_bounds 
self . _total_prediction_calls += 1 
predictions = self . __model . predictions ( image ) 
is_adversarial , is_best , distance = self . __is_adversarial ( 
assert predictions . ndim == 1 
~~~ return predictions , is_adversarial , is_best , distance 
~~~ return predictions , is_adversarial 
~~ ~~ def batch_predictions ( 
self , images , greedy = False , strict = True , return_details = False ) : 
if strict : 
~~~ in_bounds = self . in_bounds ( images ) 
assert in_bounds 
~~ self . _total_prediction_calls += len ( images ) 
predictions = self . __model . batch_predictions ( images ) 
assert predictions . ndim == 2 
assert predictions . shape [ 0 ] == images . shape [ 0 ] 
~~~ assert greedy 
~~ adversarials = [ ] 
for i in range ( len ( predictions ) ) : 
~~~ if strict : 
~~~ in_bounds_i = True 
~~~ in_bounds_i = self . in_bounds ( images [ i ] ) 
~~ is_adversarial , is_best , distance = self . __is_adversarial ( 
images [ i ] , predictions [ i ] , in_bounds_i ) 
if is_adversarial and greedy : 
~~~ if return_details : 
~~~ return predictions , is_adversarial , i , is_best , distance 
~~~ return predictions , is_adversarial , i 
~~ ~~ adversarials . append ( is_adversarial ) 
~~~ return predictions , False , None , False , None 
~~~ return predictions , False , None 
~~ ~~ is_adversarial = np . array ( adversarials ) 
assert is_adversarial . ndim == 1 
assert is_adversarial . shape [ 0 ] == images . shape [ 0 ] 
return predictions , is_adversarial 
~~ def gradient ( self , image = None , label = None , strict = True ) : 
assert self . has_gradient ( ) 
if image is None : 
~~~ image = self . __original_image 
~~~ label = self . __original_class 
~~ assert not strict or self . in_bounds ( image ) 
self . _total_gradient_calls += 1 
gradient = self . __model . gradient ( image , label ) 
assert gradient . shape == image . shape 
return gradient 
~~ def predictions_and_gradient ( 
self , image = None , label = None , strict = True , return_details = False ) : 
~~ in_bounds = self . in_bounds ( image ) 
~~~ return predictions , gradient , is_adversarial , is_best , distance 
~~~ return predictions , gradient , is_adversarial 
~~ ~~ def backward ( self , gradient , image = None , strict = True ) : 
assert gradient . ndim == 1 
gradient = self . __model . backward ( gradient , image ) 
~~ def loss_function ( cls , const , a , x , logits , reconstructed_original , 
confidence , min_ , max_ ) : 
targeted = a . target_class ( ) is not None 
if targeted : 
~~~ c_minimize = cls . best_other_class ( logits , a . target_class ( ) ) 
c_maximize = a . target_class ( ) 
~~~ c_minimize = a . original_class 
c_maximize = cls . best_other_class ( logits , a . original_class ) 
~~ is_adv_loss = logits [ c_minimize ] - logits [ c_maximize ] 
is_adv_loss += confidence 
is_adv_loss = max ( 0 , is_adv_loss ) 
s = max_ - min_ 
squared_l2_distance = np . sum ( ( x - reconstructed_original ) ** 2 ) / s ** 2 
total_loss = squared_l2_distance + const * is_adv_loss 
logits_diff_grad = np . zeros_like ( logits ) 
logits_diff_grad [ c_minimize ] = 1 
logits_diff_grad [ c_maximize ] = - 1 
is_adv_loss_grad = a . backward ( logits_diff_grad , x ) 
assert is_adv_loss >= 0 
if is_adv_loss == 0 : 
~~~ is_adv_loss_grad = 0 
~~ squared_l2_distance_grad = ( 2 / s ** 2 ) * ( x - reconstructed_original ) 
total_loss_grad = squared_l2_distance_grad + const * is_adv_loss_grad 
return total_loss , total_loss_grad 
~~ def best_other_class ( logits , exclude ) : 
other_logits = logits - onehot_like ( logits , exclude , value = np . inf ) 
return np . argmax ( other_logits ) 
~~ def name ( self ) : 
names = ( criterion . name ( ) for criterion in self . _criteria ) 
return '__' . join ( sorted ( names ) ) 
~~ def _difference_map ( image , color_axis ) : 
if color_axis == 2 : 
~~~ image = _transpose_image ( image ) 
~~ dfdx = np . zeros_like ( image ) 
dfdx [ : , : , 0 ] = image [ : , : , 1 ] - image [ : , : , 0 ] 
dfdx [ : , : , - 1 ] = image [ : , : , - 1 ] - image [ : , : , - 2 ] 
dfdx [ : , : , 1 : - 1 ] = 0.5 * ( image [ : , : , 2 : ] - image [ : , : , : - 2 ] ) 
dfdy = np . zeros_like ( image ) 
dfdy [ : , 0 , : ] = image [ : , 1 , : ] - image [ : , 0 , : ] 
dfdy [ : , - 1 , : ] = image [ : , - 1 , : ] - image [ : , - 2 , : ] 
dfdy [ : , 1 : - 1 , : ] = 0.5 * ( image [ : , 2 : , : ] - image [ : , : - 2 , : ] ) 
return dfdx , dfdy 
~~ def _compose ( image , vec_field , color_axis ) : 
hrange = np . arange ( h ) 
wrange = np . arange ( w ) 
MGx , MGy = np . meshgrid ( wrange , hrange ) 
defMGx = ( MGx + vec_field [ : , : , 0 ] ) . clip ( 0 , w - 1 ) 
defMGy = ( MGy + vec_field [ : , : , 1 ] ) . clip ( 0 , h - 1 ) 
new_image = np . empty_like ( image ) 
for channel in range ( c ) : 
~~~ interpolation = RectBivariateSpline ( hrange , wrange , image [ channel ] , 
kx = 1 , ky = 1 ) 
new_image [ channel ] = interpolation ( defMGy , defMGx , grid = False ) 
~~ if color_axis == 2 : 
~~~ return _re_transpose_image ( new_image ) 
~~~ return new_image 
~~ ~~ def _create_vec_field ( fval , gradf , d1x , d2x , color_axis , smooth = 0 ) : 
~~~ gradf = _transpose_image ( gradf ) 
alpha1 = np . sum ( gradf * d1x , axis = 0 ) 
alpha2 = np . sum ( gradf * d2x , axis = 0 ) 
norm_squared_alpha = ( alpha1 ** 2 ) . sum ( ) + ( alpha2 ** 2 ) . sum ( ) 
if smooth > 0 : 
~~~ alpha1 = gaussian_filter ( alpha1 , smooth ) 
alpha2 = gaussian_filter ( alpha2 , smooth ) 
alpha1 = gaussian_filter ( alpha1 , smooth ) 
~~ vec_field = np . empty ( ( h , w , 2 ) ) 
vec_field [ : , : , 0 ] = - fval * alpha1 / norm_squared_alpha 
vec_field [ : , : , 1 ] = - fval * alpha2 / norm_squared_alpha 
return vec_field 
~~ def softmax ( logits ) : 
assert logits . ndim == 1 
logits = logits - np . max ( logits ) 
e = np . exp ( logits ) 
return e / np . sum ( e ) 
~~ def crossentropy ( label , logits ) : 
s = np . sum ( e ) 
ce = np . log ( s ) - logits [ label ] 
return ce 
~~ def batch_crossentropy ( label , logits ) : 
assert logits . ndim == 2 
logits = logits - np . max ( logits , axis = 1 , keepdims = True ) 
s = np . sum ( e , axis = 1 ) 
ces = np . log ( s ) - logits [ : , label ] 
return ces 
~~ def binarize ( x , values , threshold = None , included_in = 'upper' ) : 
lower , upper = values 
if threshold is None : 
~~~ threshold = ( lower + upper ) / 2. 
~~ x = x . copy ( ) 
if included_in == 'lower' : 
~~~ x [ x <= threshold ] = lower 
x [ x > threshold ] = upper 
~~ elif included_in == 'upper' : 
~~~ x [ x < threshold ] = lower 
x [ x >= threshold ] = upper 
~~~ raise ValueError ( \ ) 
~~ def imagenet_example ( shape = ( 224 , 224 ) , data_format = 'channels_last' ) : 
assert len ( shape ) == 2 
assert data_format in [ 'channels_first' , 'channels_last' ] 
from PIL import Image 
path = os . path . join ( os . path . dirname ( __file__ ) , 'example.png' ) 
image = Image . open ( path ) 
image = image . resize ( shape ) 
image = np . asarray ( image , dtype = np . float32 ) 
image = image [ : , : , : 3 ] 
assert image . shape == shape + ( 3 , ) 
if data_format == 'channels_first' : 
~~~ image = np . transpose ( image , ( 2 , 0 , 1 ) ) 
~~ return image , 282 
~~ def samples ( dataset = 'imagenet' , index = 0 , batchsize = 1 , shape = ( 224 , 224 ) , 
data_format = 'channels_last' ) : 
images , labels = [ ] , [ ] 
basepath = os . path . dirname ( __file__ ) 
samplepath = os . path . join ( basepath , 'data' ) 
files = os . listdir ( samplepath ) 
for idx in range ( index , index + batchsize ) : 
~~~ i = idx % 20 
file = [ n for n in files if '{}_{:02d}_' . format ( dataset , i ) in n ] [ 0 ] 
label = int ( file . split ( '.' ) [ 0 ] . split ( '_' ) [ - 1 ] ) 
path = os . path . join ( samplepath , file ) 
if dataset == 'imagenet' : 
~~~ image = image . resize ( shape ) 
~~ image = np . asarray ( image , dtype = np . float32 ) 
if dataset != 'mnist' and data_format == 'channels_first' : 
~~ images . append ( image ) 
labels . append ( label ) 
~~ labels = np . array ( labels ) 
images = np . stack ( images ) 
return images , labels 
~~ def onehot_like ( a , index , value = 1 ) : 
x = np . zeros_like ( a ) 
x [ index ] = value 
~~ def _get_output ( self , a , image ) : 
sd = np . square ( self . _input_images - image ) 
mses = np . mean ( sd , axis = tuple ( range ( 1 , sd . ndim ) ) ) 
index = np . argmin ( mses ) 
if mses [ index ] > 0 : 
~~ return self . _output_images [ index ] 
~~ def _process_gradient ( self , backward , dmdp ) : 
~~ dmdx = backward ( dmdp ) 
assert dmdx . dtype == dmdp . dtype 
return dmdx 
~~ def predictions ( self , image ) : 
return np . squeeze ( self . batch_predictions ( image [ np . newaxis ] ) , axis = 0 ) 
~~ def gradient ( self , image , label ) : 
_ , gradient = self . predictions_and_gradient ( image , label ) 
~~ def clone ( git_uri ) : 
hash_digest = sha256_hash ( git_uri ) 
local_path = home_directory_path ( FOLDER , hash_digest ) 
exists_locally = path_exists ( local_path ) 
if not exists_locally : 
~~~ _clone_repo ( git_uri , local_path ) 
~~ return local_path 
~~ def run ( command , parser , cl_args , unknown_args ) : 
~~ def poll ( self , timeout = 0.0 ) : 
if self . sock_map is None : 
~~ readable_lst = [ ] 
writable_lst = [ ] 
error_lst = [ ] 
if self . sock_map is not None : 
~~~ for fd , obj in self . sock_map . items ( ) : 
~~~ is_r = obj . readable ( ) 
is_w = obj . writable ( ) 
if is_r : 
~~~ readable_lst . append ( fd ) 
~~ if is_w and not obj . accepting : 
~~~ writable_lst . append ( fd ) 
~~ if is_r or is_w : 
~~~ error_lst . append ( fd ) 
~~ ~~ ~~ readable_lst . append ( self . pipe_r ) 
~~~ readable_lst , writable_lst , error_lst = select . select ( readable_lst , writable_lst , error_lst , timeout ) 
~~ except select . error as err : 
if err . args [ 0 ] != errno . EINTR : 
if self . pipe_r in readable_lst : 
os . read ( self . pipe_r , 1024 ) 
readable_lst . remove ( self . pipe_r ) 
~~ if self . sock_map is not None : 
~~~ for fd in readable_lst : 
~~~ obj = self . sock_map . get ( fd ) 
if obj is None : 
~~ asyncore . read ( obj ) 
~~ for fd in writable_lst : 
~~ asyncore . write ( obj ) 
~~ for fd in error_lst : 
~~ asyncore . _exception ( obj ) 
~~ ~~ ~~ def configure ( level , logfile = None ) : 
logging . basicConfig ( format = log_format , datefmt = date_format ) 
Log . setLevel ( level ) 
if logfile is not None : 
~~~ fh = logging . FileHandler ( logfile ) 
fh . setFormatter ( logging . Formatter ( log_format ) ) 
Log . addHandler ( fh ) 
~~ ~~ def write_success_response ( self , result ) : 
response = self . make_success_response ( result ) 
spent = now - self . basehandler_starttime 
response [ constants . RESPONSE_KEY_EXECUTION_TIME ] = spent 
self . write_json_response ( response ) 
~~ def write_error_response ( self , message ) : 
self . set_status ( 404 ) 
response = self . make_error_response ( str ( message ) ) 
~~ def write_json_response ( self , response ) : 
self . write ( tornado . escape . json_encode ( response ) ) 
self . set_header ( "Content-Type" , "application/json" ) 
~~ def make_response ( self , status ) : 
response = { 
constants . RESPONSE_KEY_STATUS : status , 
constants . RESPONSE_KEY_VERSION : constants . API_VERSION , 
constants . RESPONSE_KEY_EXECUTION_TIME : 0 , 
constants . RESPONSE_KEY_MESSAGE : "" , 
return response 
~~ def make_success_response ( self , result ) : 
response = self . make_response ( constants . RESPONSE_STATUS_SUCCESS ) 
response [ constants . RESPONSE_KEY_RESULT ] = result 
~~ def make_error_response ( self , message ) : 
response = self . make_response ( constants . RESPONSE_STATUS_FAILURE ) 
response [ constants . RESPONSE_KEY_MESSAGE ] = message 
~~ def get_argument_cluster ( self ) : 
~~~ return self . get_argument ( constants . PARAM_CLUSTER ) 
~~ except tornado . web . MissingArgumentError as e : 
~~~ raise Exception ( e . log_message ) 
~~ ~~ def get_argument_role ( self ) : 
~~~ return self . get_argument ( constants . PARAM_ROLE , default = None ) 
~~ ~~ def get_argument_environ ( self ) : 
~~~ return self . get_argument ( constants . PARAM_ENVIRON ) 
~~ ~~ def get_argument_topology ( self ) : 
~~~ topology = self . get_argument ( constants . PARAM_TOPOLOGY ) 
return topology 
~~ ~~ def get_argument_component ( self ) : 
~~~ component = self . get_argument ( constants . PARAM_COMPONENT ) 
~~ ~~ def get_argument_instance ( self ) : 
~~~ instance = self . get_argument ( constants . PARAM_INSTANCE ) 
~~ ~~ def get_argument_starttime ( self ) : 
~~~ starttime = self . get_argument ( constants . PARAM_STARTTIME ) 
return starttime 
~~ ~~ def get_argument_endtime ( self ) : 
~~~ endtime = self . get_argument ( constants . PARAM_ENDTIME ) 
return endtime 
~~ ~~ def get_argument_query ( self ) : 
~~~ query = self . get_argument ( constants . PARAM_QUERY ) 
return query 
~~ ~~ def get_argument_offset ( self ) : 
~~~ offset = self . get_argument ( constants . PARAM_OFFSET ) 
return offset 
~~ ~~ def get_argument_length ( self ) : 
~~~ length = self . get_argument ( constants . PARAM_LENGTH ) 
return length 
~~ ~~ def get_required_arguments_metricnames ( self ) : 
~~~ metricnames = self . get_arguments ( constants . PARAM_METRICNAME ) 
if not metricnames : 
~~~ raise tornado . web . MissingArgumentError ( constants . PARAM_METRICNAME ) 
~~ return metricnames 
~~ ~~ def validateInterval ( self , startTime , endTime ) : 
start = int ( startTime ) 
end = int ( endTime ) 
if start > end : 
~~ ~~ def start_connect ( self ) : 
self . create_socket ( socket . AF_INET , socket . SOCK_STREAM ) 
self . _connecting = True 
self . connect ( self . endpoint ) 
~~ def register_on_message ( self , msg_builder ) : 
message = msg_builder ( ) 
self . registered_message_map [ message . DESCRIPTOR . full_name ] = msg_builder 
~~ def send_request ( self , request , context , response_type , timeout_sec ) : 
reqid = REQID . generate ( ) 
self . response_message_map [ reqid ] = response_type 
self . context_map [ reqid ] = context 
if timeout_sec > 0 : 
~~~ def timeout_task ( ) : 
~~~ self . handle_timeout ( reqid ) 
~~ self . looper . register_timer_task_in_sec ( timeout_task , timeout_sec ) 
~~ outgoing_pkt = OutgoingPacket . create_packet ( reqid , request ) 
self . _send_packet ( outgoing_pkt ) 
~~ def send_message ( self , message ) : 
outgoing_pkt = OutgoingPacket . create_packet ( REQID . generate_zero ( ) , message ) 
~~ def handle_timeout ( self , reqid ) : 
if reqid in self . context_map : 
~~~ context = self . context_map . pop ( reqid ) 
self . response_message_map . pop ( reqid ) 
self . on_response ( StatusCode . TIMEOUT_ERROR , context , None ) 
~~ ~~ def create_tar ( tar_filename , files , config_dir , config_files ) : 
with contextlib . closing ( tarfile . open ( tar_filename , 'w:gz' , dereference = True ) ) as tar : 
~~~ for filename in files : 
~~~ if os . path . isfile ( filename ) : 
~~~ tar . add ( filename , arcname = os . path . basename ( filename ) ) 
~~ ~~ if os . path . isdir ( config_dir ) : 
~~~ tar . add ( config_dir , arcname = get_heron_sandbox_conf_dir ( ) ) 
~~ for filename in config_files : 
~~~ arcfile = os . path . join ( get_heron_sandbox_conf_dir ( ) , os . path . basename ( filename ) ) 
tar . add ( filename , arcname = arcfile ) 
~~ ~~ ~~ ~~ def get_subparser ( parser , command ) : 
subparsers_actions = [ action for action in parser . _actions 
if isinstance ( action , argparse . _SubParsersAction ) ] 
for subparsers_action in subparsers_actions : 
~~~ for choice , subparser in subparsers_action . choices . items ( ) : 
~~~ if choice == command : 
~~~ return subparser 
~~ ~~ ~~ return None 
~~ def get_heron_dir ( ) : 
go_above_dirs = 9 
path = "/" . join ( os . path . realpath ( __file__ ) . split ( '/' ) [ : - go_above_dirs ] ) 
return normalized_class_path ( path ) 
~~ def get_heron_libs ( local_jars ) : 
heron_lib_dir = get_heron_lib_dir ( ) 
heron_libs = [ os . path . join ( heron_lib_dir , f ) for f in local_jars ] 
return heron_libs 
~~ def parse_cluster_role_env ( cluster_role_env , config_path ) : 
parts = cluster_role_env . split ( '/' ) [ : 3 ] 
if not os . path . isdir ( config_path ) : 
~~ if len ( parts ) < 3 : 
~~~ cli_conf_file = os . path . join ( config_path , CLIENT_YAML ) 
if not os . path . isfile ( cli_conf_file ) : 
~~~ if len ( parts ) == 1 : 
~~~ parts . append ( getpass . getuser ( ) ) 
~~ if len ( parts ) == 2 : 
~~~ parts . append ( ENVIRON ) 
~~~ cli_confs = { } 
with open ( cli_conf_file , 'r' ) as conf_file : 
~~~ tmp_confs = yaml . load ( conf_file ) 
if tmp_confs is not None : 
~~~ cli_confs = tmp_confs 
~~ ~~ if len ( parts ) == 1 : 
~~~ if ( ROLE_REQUIRED in cli_confs ) and ( cli_confs [ ROLE_REQUIRED ] is True ) : 
% ( cluster_role_env , ROLE_REQUIRED , cli_conf_file ) ) 
~~ ~~ if len ( parts ) == 2 : 
~~~ if ( ENV_REQUIRED in cli_confs ) and ( cli_confs [ ENV_REQUIRED ] is True ) : 
% ( cluster_role_env , ENV_REQUIRED , cli_conf_file ) ) 
~~ ~~ ~~ ~~ if len ( parts [ 0 ] ) == 0 or len ( parts [ 1 ] ) == 0 or len ( parts [ 2 ] ) == 0 : 
~~ return ( parts [ 0 ] , parts [ 1 ] , parts [ 2 ] ) 
~~ def get_cluster_role_env ( cluster_role_env ) : 
if len ( parts ) == 3 : 
~~~ return ( parts [ 0 ] , parts [ 1 ] , parts [ 2 ] ) 
~~~ return ( parts [ 0 ] , parts [ 1 ] , "" ) 
~~ if len ( parts ) == 1 : 
~~~ return ( parts [ 0 ] , "" , "" ) 
~~ return ( "" , "" , "" ) 
~~ def direct_mode_cluster_role_env ( cluster_role_env , config_path ) : 
cli_conf_file = os . path . join ( config_path , CLIENT_YAML ) 
~~ client_confs = { } 
~~~ client_confs = yaml . load ( conf_file ) 
if not client_confs : 
~~ role_present = True if len ( cluster_role_env [ 1 ] ) > 0 else False 
if ROLE_REQUIRED in client_confs and client_confs [ ROLE_REQUIRED ] and not role_present : 
~~ environ_present = True if len ( cluster_role_env [ 2 ] ) > 0 else False 
if ENV_REQUIRED in client_confs and client_confs [ ENV_REQUIRED ] and not environ_present : 
~~ def server_mode_cluster_role_env ( cluster_role_env , config_map ) : 
cmap = config_map [ cluster_role_env [ 0 ] ] 
role_present = True if len ( cluster_role_env [ 1 ] ) > 0 else False 
if ROLE_KEY in cmap and cmap [ ROLE_KEY ] and not role_present : 
if ENVIRON_KEY in cmap and cmap [ ENVIRON_KEY ] and not environ_present : 
~~ def defaults_cluster_role_env ( cluster_role_env ) : 
if len ( cluster_role_env [ 1 ] ) == 0 and len ( cluster_role_env [ 2 ] ) == 0 : 
~~~ return ( cluster_role_env [ 0 ] , getpass . getuser ( ) , ENVIRON ) 
~~ return ( cluster_role_env [ 0 ] , cluster_role_env [ 1 ] , cluster_role_env [ 2 ] ) 
~~ def parse_override_config_and_write_file ( namespace ) : 
overrides = parse_override_config ( namespace ) 
~~~ tmp_dir = tempfile . mkdtemp ( ) 
override_config_file = os . path . join ( tmp_dir , OVERRIDE_YAML ) 
with open ( override_config_file , 'w' ) as f : 
~~~ f . write ( yaml . dump ( overrides ) ) 
~~ return override_config_file 
~~ ~~ def parse_override_config ( namespace ) : 
overrides = dict ( ) 
for config in namespace : 
~~~ kv = config . split ( "=" ) 
if len ( kv ) != 2 : 
~~ if kv [ 1 ] in [ 'true' , 'True' , 'TRUE' ] : 
~~~ overrides [ kv [ 0 ] ] = True 
~~ elif kv [ 1 ] in [ 'false' , 'False' , 'FALSE' ] : 
~~~ overrides [ kv [ 0 ] ] = False 
~~~ overrides [ kv [ 0 ] ] = kv [ 1 ] 
~~ ~~ return overrides 
~~ def get_java_path ( ) : 
java_home = os . environ . get ( "JAVA_HOME" ) 
return os . path . join ( java_home , BIN_DIR , "java" ) 
~~ def check_java_home_set ( ) : 
if "JAVA_HOME" not in os . environ : 
~~ java_path = get_java_path ( ) 
if os . path . isfile ( java_path ) and os . access ( java_path , os . X_OK ) : 
~~ def check_release_file_exists ( ) : 
release_file = get_heron_release_file ( ) 
if not os . path . isfile ( release_file ) : 
~~ def print_build_info ( zipped_pex = False ) : 
if zipped_pex : 
~~~ release_file = get_zipped_heron_release_file ( ) 
~~~ release_file = get_heron_release_file ( ) 
~~ with open ( release_file ) as release_info : 
~~~ release_map = yaml . load ( release_info ) 
release_items = sorted ( release_map . items ( ) , key = lambda tup : tup [ 0 ] ) 
for key , value in release_items : 
~~ ~~ ~~ def get_version_number ( zipped_pex = False ) : 
~~~ for line in release_info : 
if trunks [ 0 ] == 'heron.build.version' : 
~~~ return trunks [ - 1 ] . replace ( "\ , "" ) 
~~ ~~ return 'unknown' 
~~ ~~ def insert_bool ( param , command_args ) : 
for lelem in command_args : 
~~~ if lelem == '--' and not found : 
~~ if lelem == param : 
~~~ found = True 
~~ index = index + 1 
~~ if found : 
~~~ command_args . insert ( index + 1 , 'True' ) 
~~ return command_args 
~~ def run ( command , parser , args , unknown_args ) : 
command_help = args [ 'help-command' ] 
if command_help == 'help' : 
~~~ parser . print_help ( ) 
~~ subparser = config . get_subparser ( parser , command_help ) 
if subparser : 
~~~ print ( subparser . format_help ( ) ) 
~~ ~~ def get ( self ) : 
~~~ cluster = self . get_argument_cluster ( ) 
environ = self . get_argument_environ ( ) 
role = self . get_argument_role ( ) 
topology_name = self . get_argument_topology ( ) 
component = self . get_argument_component ( ) 
topology = self . tracker . getTopologyByClusterRoleEnvironAndName ( 
cluster , role , environ , topology_name ) 
instances = self . get_arguments ( constants . PARAM_INSTANCE ) 
exceptions_summary = yield tornado . gen . Task ( self . getComponentExceptionSummary , 
topology . tmaster , component , instances ) 
self . write_success_response ( exceptions_summary ) 
~~~ Log . debug ( traceback . format_exc ( ) ) 
self . write_error_response ( e ) 
~~ ~~ def getComponentExceptionSummary ( self , tmaster , component_name , instances = [ ] , callback = None ) : 
if not tmaster or not tmaster . host or not tmaster . stats_port : 
~~ exception_request = tmaster_pb2 . ExceptionLogRequest ( ) 
exception_request . component_name = component_name 
if len ( instances ) > 0 : 
~~~ exception_request . instances . extend ( instances ) 
~~ request_str = exception_request . SerializeToString ( ) 
port = str ( tmaster . stats_port ) 
host = tmaster . host 
url = "http://{0}:{1}/exceptionsummary" . format ( host , port ) 
request = tornado . httpclient . HTTPRequest ( url , 
body = request_str , 
method = 'POST' , 
request_timeout = 5 ) 
~~~ client = tornado . httpclient . AsyncHTTPClient ( ) 
result = yield client . fetch ( request ) 
~~ except tornado . httpclient . HTTPError as e : 
~~~ raise Exception ( str ( e ) ) 
~~ responseCode = result . code 
if responseCode >= 400 : 
Log . error ( message ) 
raise tornado . gen . Return ( { 
"message" : message 
~~ exception_response = tmaster_pb2 . ExceptionLogResponse ( ) 
exception_response . ParseFromString ( result . body ) 
if exception_response . status . status == common_pb2 . NOTOK : 
~~~ if exception_response . status . HasField ( "message" ) : 
~~~ raise tornado . gen . Return ( { 
"message" : exception_response . status . message 
~~ ~~ ret = [ ] 
for exception_log in exception_response . exceptions : 
~~~ ret . append ( { 'class_name' : exception_log . stacktrace , 
'lasttime' : exception_log . lasttime , 
'firsttime' : exception_log . firsttime , 
'count' : str ( exception_log . count ) } ) 
~~ raise tornado . gen . Return ( ret ) 
~~ def get ( self , cluster , environ , topology ) : 
options = dict ( 
cluster = cluster , 
environ = environ , 
topology = topology , 
active = "topologies" , 
function = common . className , 
baseUrl = self . baseUrl ) 
self . render ( "config.html" , ** options ) 
~~ def get ( self ) : 
clusters = yield access . get_clusters ( ) 
clusters = [ str ( cluster ) for cluster in clusters ] , 
baseUrl = self . baseUrl 
self . render ( "topologies.html" , ** options ) 
execution_state = yield access . get_execution_state ( cluster , environ , topology ) 
scheduler_location = yield access . get_scheduler_location ( cluster , environ , topology ) 
job_page_link = scheduler_location [ "job_page_link" ] 
launched_at = datetime . utcfromtimestamp ( execution_state [ 'submission_time' ] ) 
execution_state = execution_state , 
launched = launched_time , 
status = "running" if random . randint ( 0 , 1 ) else "errors" , 
job_page_link = job_page_link , 
self . render ( "topology.html" , ** options ) 
~~ def get ( self , cluster , environ , topology , container ) : 
path = self . get_argument ( "path" ) 
container = container , 
path = path , 
self . render ( "file.html" , ** options ) 
offset = self . get_argument ( "offset" ) 
length = self . get_argument ( "length" ) 
data = yield access . get_container_file_data ( cluster , environ , topology , container , path , 
offset , length ) 
self . write ( data ) 
self . finish ( ) 
path = self . get_argument ( "path" , default = "." ) 
data = yield access . get_filestats ( cluster , environ , topology , container , path ) 
filestats = data , 
self . render ( "browse.html" , ** options ) 
self . connection_closed = False 
filename = path . split ( "/" ) [ - 1 ] 
file_download_url = access . get_container_file_download_url ( cluster , environ , 
topology , container , path ) 
def streaming_callback ( chunk ) : 
~~~ self . write ( chunk ) 
self . flush ( ) 
~~ http_client = tornado . httpclient . AsyncHTTPClient ( ) 
yield http_client . fetch ( file_download_url , streaming_callback = streaming_callback ) 
~~ def get ( self , path ) : 
path = tornado . escape . url_unescape ( path ) 
if not path : 
~~~ path = "." 
~~ if not utils . check_path ( path ) : 
self . set_status ( 403 ) 
~~ listing = utils . get_listing ( path ) 
file_stats = { } 
for fn in listing : 
~~~ is_dir = False 
formatted_stat = utils . format_prefix ( fn , utils . get_stat ( path , fn ) ) 
if stat . S_ISDIR ( utils . get_stat ( path , fn ) . st_mode ) : 
~~~ is_dir = True 
~~ file_stats [ fn ] = { 
"formatted_stat" : formatted_stat , 
"is_dir" : is_dir , 
"path" : tornado . escape . url_escape ( os . path . join ( path , fn ) ) , 
if fn == ".." : 
~~~ path_fragments = path . split ( "/" ) 
if not path_fragments : 
~~~ file_stats [ fn ] [ "path" ] = "." 
~~~ file_stats [ fn ] [ "path" ] = tornado . escape . url_escape ( "/" . join ( path_fragments [ : - 1 ] ) ) 
~~ ~~ ~~ except : 
~~ ~~ self . write ( json . dumps ( file_stats ) ) 
~~ def register_watch ( self , callback ) : 
RETRY_COUNT = 5 
for _ in range ( RETRY_COUNT ) : 
~~~ uid = uuid . uuid4 ( ) 
if uid not in self . watches : 
~~~ callback ( self ) 
Log . debug ( traceback . format_exc ( ) ) 
~~ self . watches [ uid ] = callback 
return uid 
~~ def unregister_watch ( self , uid ) : 
self . watches . pop ( uid , None ) 
~~ def trigger_watches ( self ) : 
to_remove = [ ] 
for uid , callback in self . watches . items ( ) : 
to_remove . append ( uid ) 
~~ ~~ for uid in to_remove : 
~~~ self . unregister_watch ( uid ) 
~~ ~~ def set_physical_plan ( self , physical_plan ) : 
if not physical_plan : 
~~~ self . physical_plan = None 
self . id = None 
~~~ self . physical_plan = physical_plan 
self . id = physical_plan . topology . id 
~~ self . trigger_watches ( ) 
~~ def set_packing_plan ( self , packing_plan ) : 
if not packing_plan : 
~~~ self . packing_plan = None 
~~~ self . packing_plan = packing_plan 
self . id = packing_plan . id 
~~ def set_execution_state ( self , execution_state ) : 
if not execution_state : 
~~~ self . execution_state = None 
self . cluster = None 
self . environ = None 
~~~ self . execution_state = execution_state 
cluster , environ = self . get_execution_state_dc_environ ( execution_state ) 
self . cluster = cluster 
self . environ = environ 
self . zone = cluster 
~~ def num_instances ( self ) : 
num = 0 
components = self . spouts ( ) + self . bolts ( ) 
for component in components : 
~~~ config = component . comp . config 
for kvs in config . kvs : 
~~~ if kvs . key == api_constants . TOPOLOGY_COMPONENT_PARALLELISM : 
~~~ num += int ( kvs . value ) 
~~ ~~ ~~ return num 
~~ def get_machines ( self ) : 
if self . physical_plan : 
~~~ stmgrs = list ( self . physical_plan . stmgrs ) 
return map ( lambda s : s . host_name , stmgrs ) 
~~ def get_status ( self ) : 
status = None 
if self . physical_plan and self . physical_plan . topology : 
~~~ status = self . physical_plan . topology . state 
~~ if status == 1 : 
~~~ return "Running" 
~~ elif status == 2 : 
~~~ return "Paused" 
~~ elif status == 3 : 
~~~ return "Killed" 
~~~ return "Unknown" 
~~ ~~ def convert_pb_kvs ( kvs , include_non_primitives = True ) : 
config = { } 
for kv in kvs : 
~~~ if kv . value : 
~~~ config [ kv . key ] = kv . value 
~~ elif kv . serialized_value : 
~~~ if topology_pb2 . JAVA_SERIALIZED_VALUE == kv . type : 
~~~ jv = _convert_java_value ( kv , include_non_primitives = include_non_primitives ) 
if jv is not None : 
~~~ config [ kv . key ] = jv 
~~~ config [ kv . key ] = _raw_value ( kv ) 
~~ ~~ ~~ return config 
~~ def synch_topologies ( self ) : 
self . state_managers = statemanagerfactory . get_all_state_managers ( self . config . statemgr_config ) 
~~~ for state_manager in self . state_managers : 
~~~ state_manager . start ( ) 
~~ ~~ except Exception as ex : 
traceback . print_exc ( ) 
~~ def on_topologies_watch ( state_manager , topologies ) : 
existingTopologies = self . getTopologiesForStateLocation ( state_manager . name ) 
existingTopNames = map ( lambda t : t . name , existingTopologies ) 
for name in existingTopNames : 
~~~ if name not in topologies : 
name , state_manager . rootpath ) 
self . removeTopology ( name , state_manager . name ) 
~~ ~~ for name in topologies : 
~~~ if name not in existingTopNames : 
~~~ self . addNewTopology ( state_manager , name ) 
~~ ~~ ~~ for state_manager in self . state_managers : 
~~~ onTopologiesWatch = partial ( on_topologies_watch , state_manager ) 
state_manager . get_topologies ( onTopologiesWatch ) 
~~ ~~ def getTopologyByClusterRoleEnvironAndName ( self , cluster , role , environ , topologyName ) : 
topologies = list ( filter ( lambda t : t . name == topologyName 
and t . cluster == cluster 
and ( not role or t . execution_state . role == role ) 
and t . environ == environ , self . topologies ) ) 
if not topologies or len ( topologies ) > 1 : 
~~~ if role is not None : 
cluster , role , environ , topologyName ) ) 
cluster , environ , topologyName ) ) 
~~ ~~ return topologies [ 0 ] 
~~ def getTopologiesForStateLocation ( self , name ) : 
return filter ( lambda t : t . state_manager_name == name , self . topologies ) 
~~ def addNewTopology ( self , state_manager , topologyName ) : 
topology = Topology ( topologyName , state_manager . name ) 
topologyName , state_manager . name ) 
self . topologies . append ( topology ) 
topology . register_watch ( self . setTopologyInfo ) 
def on_topology_pplan ( data ) : 
topology . set_physical_plan ( data ) 
~~ ~~ def on_topology_packing_plan ( data ) : 
topology . set_packing_plan ( data ) 
~~ ~~ def on_topology_execution_state ( data ) : 
topology . set_execution_state ( data ) 
~~ ~~ def on_topology_tmaster ( data ) : 
topology . set_tmaster ( data ) 
~~ ~~ def on_topology_scheduler_location ( data ) : 
topology . set_scheduler_location ( data ) 
~~ ~~ state_manager . get_pplan ( topologyName , on_topology_pplan ) 
state_manager . get_packing_plan ( topologyName , on_topology_packing_plan ) 
state_manager . get_execution_state ( topologyName , on_topology_execution_state ) 
state_manager . get_tmaster ( topologyName , on_topology_tmaster ) 
state_manager . get_scheduler_location ( topologyName , on_topology_scheduler_location ) 
~~ def removeTopology ( self , topology_name , state_manager_name ) : 
topologies = [ ] 
for top in self . topologies : 
~~~ if ( top . name == topology_name and 
top . state_manager_name == state_manager_name ) : 
~~~ if ( topology_name , state_manager_name ) in self . topologyInfos : 
~~~ self . topologyInfos . pop ( ( topology_name , state_manager_name ) ) 
~~~ topologies . append ( top ) 
~~ ~~ self . topologies = topologies 
~~ def extract_execution_state ( self , topology ) : 
execution_state = topology . execution_state 
executionState = { 
"cluster" : execution_state . cluster , 
"environ" : execution_state . environ , 
"role" : execution_state . role , 
"jobname" : topology . name , 
"submission_time" : execution_state . submission_time , 
"submission_user" : execution_state . submission_user , 
"release_username" : execution_state . release_state . release_username , 
"release_tag" : execution_state . release_state . release_tag , 
"release_version" : execution_state . release_state . release_version , 
"has_physical_plan" : None , 
"has_tmaster_location" : None , 
"has_scheduler_location" : None , 
"extra_links" : [ ] , 
for extra_link in self . config . extra_links : 
~~~ link = extra_link . copy ( ) 
link [ "url" ] = self . config . get_formatted_url ( executionState , 
link [ EXTRA_LINK_FORMATTER_KEY ] ) 
executionState [ "extra_links" ] . append ( link ) 
~~ return executionState 
~~ def extract_scheduler_location ( self , topology ) : 
schedulerLocation = { 
"name" : None , 
"http_endpoint" : None , 
"job_page_link" : None , 
if topology . scheduler_location : 
~~~ schedulerLocation [ "name" ] = topology . scheduler_location . topology_name 
schedulerLocation [ "http_endpoint" ] = topology . scheduler_location . http_endpoint 
schedulerLocation [ "job_page_link" ] = topology . scheduler_location . job_page_link [ 0 ] if len ( topology . scheduler_location . job_page_link ) > 0 else "" 
~~ return schedulerLocation 
~~ def extract_tmaster ( self , topology ) : 
tmasterLocation = { 
"id" : None , 
"host" : None , 
"controller_port" : None , 
"master_port" : None , 
"stats_port" : None , 
if topology . tmaster : 
~~~ tmasterLocation [ "name" ] = topology . tmaster . topology_name 
tmasterLocation [ "id" ] = topology . tmaster . topology_id 
tmasterLocation [ "host" ] = topology . tmaster . host 
tmasterLocation [ "controller_port" ] = topology . tmaster . controller_port 
tmasterLocation [ "master_port" ] = topology . tmaster . master_port 
tmasterLocation [ "stats_port" ] = topology . tmaster . stats_port 
~~ return tmasterLocation 
~~ def extract_logical_plan ( self , topology ) : 
logicalPlan = { 
"spouts" : { } , 
"bolts" : { } , 
for spout in topology . spouts ( ) : 
~~~ spoutName = spout . comp . name 
spoutType = "default" 
spoutSource = "NA" 
spoutVersion = "NA" 
spoutConfigs = spout . comp . config . kvs 
for kvs in spoutConfigs : 
~~~ if kvs . key == "spout.type" : 
~~~ spoutType = javaobj . loads ( kvs . serialized_value ) 
~~ elif kvs . key == "spout.source" : 
~~~ spoutSource = javaobj . loads ( kvs . serialized_value ) 
~~ elif kvs . key == "spout.version" : 
~~~ spoutVersion = javaobj . loads ( kvs . serialized_value ) 
~~ ~~ spoutPlan = { 
"config" : convert_pb_kvs ( spoutConfigs , include_non_primitives = False ) , 
"type" : spoutType , 
"source" : spoutSource , 
"version" : spoutVersion , 
"outputs" : [ ] 
for outputStream in list ( spout . outputs ) : 
~~~ spoutPlan [ "outputs" ] . append ( { 
"stream_name" : outputStream . stream . id 
~~ logicalPlan [ "spouts" ] [ spoutName ] = spoutPlan 
~~ for bolt in topology . bolts ( ) : 
~~~ boltName = bolt . comp . name 
boltPlan = { 
"config" : convert_pb_kvs ( bolt . comp . config . kvs , include_non_primitives = False ) , 
"outputs" : [ ] , 
"inputs" : [ ] 
for outputStream in list ( bolt . outputs ) : 
~~~ boltPlan [ "outputs" ] . append ( { 
~~ for inputStream in list ( bolt . inputs ) : 
~~~ boltPlan [ "inputs" ] . append ( { 
"stream_name" : inputStream . stream . id , 
"component_name" : inputStream . stream . component_name , 
"grouping" : topology_pb2 . Grouping . Name ( inputStream . gtype ) 
~~ logicalPlan [ "bolts" ] [ boltName ] = boltPlan 
~~ return logicalPlan 
~~ def extract_physical_plan ( self , topology ) : 
physicalPlan = { 
"instances" : { } , 
"instance_groups" : { } , 
"stmgrs" : { } , 
"config" : { } , 
"components" : { } 
if not topology . physical_plan : 
~~~ return physicalPlan 
~~ spouts = topology . spouts ( ) 
bolts = topology . bolts ( ) 
stmgrs = None 
instances = None 
stmgrs = list ( topology . physical_plan . stmgrs ) 
instances = list ( topology . physical_plan . instances ) 
if topology . physical_plan . topology . topology_config : 
~~~ physicalPlan [ "config" ] = convert_pb_kvs ( topology . physical_plan . topology . topology_config . kvs ) 
~~ for spout in spouts : 
~~~ spout_name = spout . comp . name 
physicalPlan [ "spouts" ] [ spout_name ] = [ ] 
if spout_name not in physicalPlan [ "components" ] : 
~~~ physicalPlan [ "components" ] [ spout_name ] = { 
"config" : convert_pb_kvs ( spout . comp . config . kvs ) 
~~ ~~ for bolt in bolts : 
~~~ bolt_name = bolt . comp . name 
physicalPlan [ "bolts" ] [ bolt_name ] = [ ] 
if bolt_name not in physicalPlan [ "components" ] : 
~~~ physicalPlan [ "components" ] [ bolt_name ] = { 
"config" : convert_pb_kvs ( bolt . comp . config . kvs ) 
~~ ~~ for stmgr in stmgrs : 
~~~ host = stmgr . host_name 
cwd = stmgr . cwd 
shell_port = stmgr . shell_port if stmgr . HasField ( "shell_port" ) else None 
physicalPlan [ "stmgrs" ] [ stmgr . id ] = { 
"id" : stmgr . id , 
"host" : host , 
"port" : stmgr . data_port , 
"shell_port" : shell_port , 
"cwd" : cwd , 
"pid" : stmgr . pid , 
"joburl" : utils . make_shell_job_url ( host , shell_port , cwd ) , 
"logfiles" : utils . make_shell_logfiles_url ( host , shell_port , cwd ) , 
"instance_ids" : [ ] 
~~ instance_groups = collections . OrderedDict ( ) 
for instance in instances : 
~~~ instance_id = instance . instance_id 
stmgrId = instance . stmgr_id 
name = instance . info . component_name 
stmgrInfo = physicalPlan [ "stmgrs" ] [ stmgrId ] 
host = stmgrInfo [ "host" ] 
cwd = stmgrInfo [ "cwd" ] 
shell_port = stmgrInfo [ "shell_port" ] 
group_name = instance_id . rsplit ( "_" , 2 ) [ 0 ] 
igroup = instance_groups . get ( group_name , list ( ) ) 
igroup . append ( instance_id ) 
instance_groups [ group_name ] = igroup 
physicalPlan [ "instances" ] [ instance_id ] = { 
"id" : instance_id , 
"name" : name , 
"stmgrId" : stmgrId , 
"logfile" : utils . make_shell_logfiles_url ( host , shell_port , cwd , instance . instance_id ) , 
physicalPlan [ "stmgrs" ] [ stmgrId ] [ "instance_ids" ] . append ( instance_id ) 
if name in physicalPlan [ "spouts" ] : 
~~~ physicalPlan [ "spouts" ] [ name ] . append ( instance_id ) 
~~~ physicalPlan [ "bolts" ] [ name ] . append ( instance_id ) 
~~ ~~ physicalPlan [ "instance_groups" ] = instance_groups 
return physicalPlan 
~~ def extract_packing_plan ( self , topology ) : 
packingPlan = { 
"id" : "" , 
"container_plans" : [ ] 
if not topology . packing_plan : 
~~~ return packingPlan 
~~ container_plans = topology . packing_plan . container_plans 
containers = [ ] 
for container_plan in container_plans : 
~~~ instances = [ ] 
for instance_plan in container_plan . instance_plans : 
~~~ instance_resources = { "cpu" : instance_plan . resource . cpu , 
"ram" : instance_plan . resource . ram , 
"disk" : instance_plan . resource . disk } 
instance = { "component_name" : instance_plan . component_name , 
"task_id" : instance_plan . task_id , 
"component_index" : instance_plan . component_index , 
"instance_resources" : instance_resources } 
instances . append ( instance ) 
~~ required_resource = { "cpu" : container_plan . requiredResource . cpu , 
"ram" : container_plan . requiredResource . ram , 
"disk" : container_plan . requiredResource . disk } 
scheduled_resource = { } 
if container_plan . scheduledResource : 
~~~ scheduled_resource = { "cpu" : container_plan . scheduledResource . cpu , 
"ram" : container_plan . scheduledResource . ram , 
"disk" : container_plan . scheduledResource . disk } 
~~ container = { "id" : container_plan . id , 
"instances" : instances , 
"required_resources" : required_resource , 
"scheduled_resources" : scheduled_resource } 
containers . append ( container ) 
~~ packingPlan [ "id" ] = topology . packing_plan . id 
packingPlan [ "container_plans" ] = containers 
return json . dumps ( packingPlan ) 
~~ def setTopologyInfo ( self , topology ) : 
if not topology . execution_state : 
has_physical_plan = True 
~~~ has_physical_plan = False 
has_packing_plan = True 
~~~ has_packing_plan = False 
~~ has_tmaster_location = True 
if not topology . tmaster : 
~~~ has_tmaster_location = False 
~~ has_scheduler_location = True 
if not topology . scheduler_location : 
~~~ has_scheduler_location = False 
~~ topologyInfo = { 
"name" : topology . name , 
"id" : topology . id , 
"logical_plan" : None , 
"physical_plan" : None , 
"packing_plan" : None , 
"execution_state" : None , 
"tmaster_location" : None , 
"scheduler_location" : None , 
executionState = self . extract_execution_state ( topology ) 
executionState [ "has_physical_plan" ] = has_physical_plan 
executionState [ "has_packing_plan" ] = has_packing_plan 
executionState [ "has_tmaster_location" ] = has_tmaster_location 
executionState [ "has_scheduler_location" ] = has_scheduler_location 
executionState [ "status" ] = topology . get_status ( ) 
topologyInfo [ "metadata" ] = self . extract_metadata ( topology ) 
topologyInfo [ "runtime_state" ] = self . extract_runtime_state ( topology ) 
topologyInfo [ "execution_state" ] = executionState 
topologyInfo [ "logical_plan" ] = self . extract_logical_plan ( topology ) 
topologyInfo [ "physical_plan" ] = self . extract_physical_plan ( topology ) 
topologyInfo [ "packing_plan" ] = self . extract_packing_plan ( topology ) 
topologyInfo [ "tmaster_location" ] = self . extract_tmaster ( topology ) 
topologyInfo [ "scheduler_location" ] = self . extract_scheduler_location ( topology ) 
self . topologyInfos [ ( topology . name , topology . state_manager_name ) ] = topologyInfo 
~~ def getTopologyInfo ( self , topologyName , cluster , role , environ ) : 
for ( topology_name , _ ) , topologyInfo in self . topologyInfos . items ( ) : 
~~~ executionState = topologyInfo [ "execution_state" ] 
if ( topologyName == topology_name and 
cluster == executionState [ "cluster" ] and 
environ == executionState [ "environ" ] ) : 
~~~ if not role or executionState . get ( "role" ) == role : 
~~~ return topologyInfo 
~~ ~~ ~~ if role is not None : 
topologyName , cluster , role , environ ) 
~~ def load_configs ( self ) : 
self . statemgr_config . set_state_locations ( self . configs [ STATEMGRS_KEY ] ) 
if EXTRA_LINKS_KEY in self . configs : 
~~~ for extra_link in self . configs [ EXTRA_LINKS_KEY ] : 
~~~ self . extra_links . append ( self . validate_extra_link ( extra_link ) ) 
~~ ~~ ~~ def validate_extra_link ( self , extra_link ) : 
if EXTRA_LINK_NAME_KEY not in extra_link or EXTRA_LINK_FORMATTER_KEY not in extra_link : 
~~ self . validated_formatter ( extra_link [ EXTRA_LINK_FORMATTER_KEY ] ) 
return extra_link 
~~ def validated_formatter ( self , url_format ) : 
valid_parameters = { 
"${CLUSTER}" : "cluster" , 
"${ENVIRON}" : "environ" , 
"${TOPOLOGY}" : "topology" , 
"${ROLE}" : "role" , 
"${USER}" : "user" , 
dummy_formatted_url = url_format 
for key , value in valid_parameters . items ( ) : 
~~~ dummy_formatted_url = dummy_formatted_url . replace ( key , value ) 
~~ if '$' in dummy_formatted_url : 
~~ return url_format 
~~ def emit ( self , tup , tup_id = None , stream = Stream . DEFAULT_STREAM_ID , 
direct_task = None , need_task_ids = False ) : 
self . pplan_helper . check_output_schema ( stream , tup ) 
custom_target_task_ids = self . pplan_helper . choose_tasks_for_custom_grouping ( stream , tup ) 
self . pplan_helper . context . invoke_hook_emit ( tup , stream , None ) 
data_tuple = tuple_pb2 . HeronDataTuple ( ) 
data_tuple . key = 0 
if direct_task is not None : 
~~~ if not isinstance ( direct_task , int ) : 
% str ( type ( direct_task ) ) ) 
~~ data_tuple . dest_task_ids . append ( direct_task ) 
~~ elif custom_target_task_ids is not None : 
~~~ for task_id in custom_target_task_ids : 
~~~ data_tuple . dest_task_ids . append ( task_id ) 
~~ ~~ if tup_id is not None : 
~~~ tuple_info = TupleHelper . make_root_tuple_info ( stream , tup_id ) 
if self . acking_enabled : 
~~~ root = data_tuple . roots . add ( ) 
root . taskid = self . pplan_helper . my_task_id 
root . key = tuple_info . key 
self . in_flight_tuples [ tuple_info . key ] = tuple_info 
~~~ self . immediate_acks . append ( tuple_info ) 
~~ ~~ tuple_size_in_bytes = 0 
for obj in tup : 
~~~ serialized = self . serializer . serialize ( obj ) 
data_tuple . values . append ( serialized ) 
tuple_size_in_bytes += len ( serialized ) 
~~ serialize_latency_ns = ( time . time ( ) - start_time ) * system_constants . SEC_TO_NS 
self . spout_metrics . serialize_data_tuple ( stream , serialize_latency_ns ) 
super ( SpoutInstance , self ) . admit_data_tuple ( stream_id = stream , data_tuple = data_tuple , 
tuple_size_in_bytes = tuple_size_in_bytes ) 
self . total_tuples_emitted += 1 
self . spout_metrics . update_emit_count ( stream ) 
if need_task_ids : 
~~~ sent_task_ids = custom_target_task_ids or [ ] 
~~~ sent_task_ids . append ( direct_task ) 
~~ return sent_task_ids 
~~ ~~ def _is_continue_to_work ( self ) : 
if not self . _is_topology_running ( ) : 
~~ max_spout_pending = self . pplan_helper . context . get_cluster_config ( ) . get ( api_constants . TOPOLOGY_MAX_SPOUT_PENDING ) 
if not self . acking_enabled and self . output_helper . is_out_queue_available ( ) : 
~~ elif self . acking_enabled and self . output_helper . is_out_queue_available ( ) and len ( self . in_flight_tuples ) < max_spout_pending : 
~~ elif self . acking_enabled and not self . in_stream . is_empty ( ) : 
~~ ~~ def create_parser ( subparsers ) : 
components_parser = subparsers . add_parser ( 
'components' , 
add_help = False ) 
args . add_cluster_role_env ( components_parser ) 
args . add_topology_name ( components_parser ) 
args . add_spouts ( components_parser ) 
args . add_bolts ( components_parser ) 
args . add_verbose ( components_parser ) 
args . add_tracker_url ( components_parser ) 
args . add_config ( components_parser ) 
components_parser . set_defaults ( subcommand = 'components' ) 
return subparsers 
~~ def to_table ( components , topo_info ) : 
inputs , outputs = defaultdict ( list ) , defaultdict ( list ) 
for ctype , component in components . items ( ) : 
~~~ if ctype == 'bolts' : 
~~~ for component_name , component_info in component . items ( ) : 
~~~ for input_stream in component_info [ 'inputs' ] : 
~~~ input_name = input_stream [ 'component_name' ] 
inputs [ component_name ] . append ( input_name ) 
outputs [ input_name ] . append ( component_name ) 
~~ ~~ ~~ ~~ info = [ ] 
spouts_instance = topo_info [ 'physical_plan' ] [ 'spouts' ] 
bolts_instance = topo_info [ 'physical_plan' ] [ 'bolts' ] 
~~~ if ctype == "stages" : 
~~ for component_name , component_info in component . items ( ) : 
~~~ row = [ ctype [ : - 1 ] , component_name ] 
if ctype == 'spouts' : 
~~~ row . append ( len ( spouts_instance [ component_name ] ) ) 
~~~ row . append ( len ( bolts_instance [ component_name ] ) ) 
~~ row . append ( ',' . join ( inputs . get ( component_name , [ '-' ] ) ) ) 
row . append ( ',' . join ( outputs . get ( component_name , [ '-' ] ) ) ) 
info . append ( row ) 
~~ ~~ header = [ 'type' , 'name' , 'parallelism' , 'input' , 'output' ] 
return info , header 
~~ def filter_bolts ( table , header ) : 
bolts_info = [ ] 
for row in table : 
~~~ if row [ 0 ] == 'bolt' : 
~~~ bolts_info . append ( row ) 
~~ ~~ return bolts_info , header 
~~ def filter_spouts ( table , header ) : 
spouts_info = [ ] 
~~~ if row [ 0 ] == 'spout' : 
~~~ spouts_info . append ( row ) 
~~ ~~ return spouts_info , header 
~~ def run ( cl_args , compo_type ) : 
cluster , role , env = cl_args [ 'cluster' ] , cl_args [ 'role' ] , cl_args [ 'environ' ] 
topology = cl_args [ 'topology-name' ] 
spouts_only , bolts_only = cl_args [ 'spout' ] , cl_args [ 'bolt' ] 
~~~ components = tracker_access . get_logical_plan ( cluster , env , topology , role ) 
topo_info = tracker_access . get_topology_info ( cluster , env , topology , role ) 
table , header = to_table ( components , topo_info ) 
if spouts_only == bolts_only : 
~~~ print ( tabulate ( table , headers = header ) ) 
~~ elif spouts_only : 
~~~ table , header = filter_spouts ( table , header ) 
print ( tabulate ( table , headers = header ) ) 
~~~ table , header = filter_bolts ( table , header ) 
~~ ~~ def start ( self ) : 
if self . is_host_port_reachable ( ) : 
~~~ self . client = self . _kazoo_client ( _makehostportlist ( self . hostportlist ) ) 
~~~ localhostports = self . establish_ssh_tunnel ( ) 
self . client = self . _kazoo_client ( _makehostportlist ( localhostports ) ) 
~~ self . client . start ( ) 
def on_connection_change ( state ) : 
~~ self . client . add_listener ( on_connection_change ) 
~~ def get_topologies ( self , callback = None ) : 
isWatching = False 
ret = { 
"result" : None 
if callback : 
~~~ isWatching = True 
~~~ def callback ( data ) : 
ret [ "result" ] = data 
~~~ self . client . ensure_path ( self . get_topologies_path ( ) ) 
self . _get_topologies_with_watch ( callback , isWatching ) 
~~ except NoNodeError : 
~~~ self . client . stop ( ) 
path = self . get_topologies_path ( ) 
StateException . EX_TYPE_NO_NODE_ERROR ) , sys . exc_info ( ) [ 2 ] ) 
~~ return ret [ "result" ] 
~~ def _get_topologies_with_watch ( self , callback , isWatching ) : 
if isWatching : 
~~ @ self . client . ChildrenWatch ( path ) 
def watch_topologies ( topologies ) : 
callback ( topologies ) 
return isWatching 
~~ ~~ def get_topology ( self , topologyName , callback = None ) : 
~~ ~~ self . _get_topology_with_watch ( topologyName , callback , isWatching ) 
return ret [ "result" ] 
~~ def _get_topology_with_watch ( self , topologyName , callback , isWatching ) : 
path = self . get_topology_path ( topologyName ) 
~~ @ self . client . DataWatch ( path ) 
def watch_topology ( data , stats ) : 
if data : 
~~~ topology = Topology ( ) 
topology . ParseFromString ( data ) 
callback ( topology ) 
~~~ callback ( None ) 
~~ return isWatching 
~~ ~~ def create_topology ( self , topologyName , topology ) : 
if not topology or not topology . IsInitialized ( ) : 
StateException . EX_TYPE_PROTOBUF_ERROR ) , sys . exc_info ( ) [ 2 ] ) 
~~ path = self . get_topology_path ( topologyName ) 
topologyName , path ) ) 
topologyString = topology . SerializeToString ( ) 
~~~ self . client . create ( path , value = topologyString , makepath = True ) 
~~ except NodeExistsError : 
StateException . EX_TYPE_NODE_EXISTS_ERROR ) , sys . exc_info ( ) [ 2 ] ) 
~~ except ZookeeperError : 
StateException . EX_TYPE_ZOOKEEPER_ERROR ) , sys . exc_info ( ) [ 2 ] ) 
~~ ~~ def delete_topology ( self , topologyName ) : 
~~~ self . client . delete ( path ) 
~~ except NotEmptyError : 
StateException . EX_TYPE_NOT_EMPTY_ERROR ) , sys . exc_info ( ) [ 2 ] ) 
~~ ~~ def get_packing_plan ( self , topologyName , callback = None ) : 
~~ ~~ self . _get_packing_plan_with_watch ( topologyName , callback , isWatching ) 
~~ def _get_packing_plan_with_watch ( self , topologyName , callback , isWatching ) : 
path = self . get_packing_plan_path ( topologyName ) 
def watch_packing_plan ( data , stats ) : 
~~~ packing_plan = PackingPlan ( ) 
packing_plan . ParseFromString ( data ) 
callback ( packing_plan ) 
~~ ~~ def get_pplan ( self , topologyName , callback = None ) : 
~~ ~~ self . _get_pplan_with_watch ( topologyName , callback , isWatching ) 
~~ def _get_pplan_with_watch ( self , topologyName , callback , isWatching ) : 
path = self . get_pplan_path ( topologyName ) 
def watch_pplan ( data , stats ) : 
~~~ pplan = PhysicalPlan ( ) 
pplan . ParseFromString ( data ) 
callback ( pplan ) 
~~ ~~ def create_pplan ( self , topologyName , pplan ) : 
if not pplan or not pplan . IsInitialized ( ) : 
~~ path = self . get_pplan_path ( topologyName ) 
pplanString = pplan . SerializeToString ( ) 
~~~ self . client . create ( path , value = pplanString , makepath = True ) 
~~ ~~ def get_execution_state ( self , topologyName , callback = None ) : 
~~ ~~ self . _get_execution_state_with_watch ( topologyName , callback , isWatching ) 
~~ def _get_execution_state_with_watch ( self , topologyName , callback , isWatching ) : 
path = self . get_execution_state_path ( topologyName ) 
def watch_execution_state ( data , stats ) : 
~~~ executionState = ExecutionState ( ) 
executionState . ParseFromString ( data ) 
callback ( executionState ) 
~~ ~~ def create_execution_state ( self , topologyName , executionState ) : 
if not executionState or not executionState . IsInitialized ( ) : 
~~ path = self . get_execution_state_path ( topologyName ) 
executionStateString = executionState . SerializeToString ( ) 
~~~ self . client . create ( path , value = executionStateString , makepath = True ) 
~~ ~~ def get_tmaster ( self , topologyName , callback = None ) : 
~~ ~~ self . _get_tmaster_with_watch ( topologyName , callback , isWatching ) 
~~ def _get_tmaster_with_watch ( self , topologyName , callback , isWatching ) : 
path = self . get_tmaster_path ( topologyName ) 
def watch_tmaster ( data , stats ) : 
~~~ tmaster = TMasterLocation ( ) 
tmaster . ParseFromString ( data ) 
callback ( tmaster ) 
~~ ~~ def get_scheduler_location ( self , topologyName , callback = None ) : 
~~ ~~ self . _get_scheduler_location_with_watch ( topologyName , callback , isWatching ) 
~~ def _get_scheduler_location_with_watch ( self , topologyName , callback , isWatching ) : 
path = self . get_scheduler_location_path ( topologyName ) 
def watch_scheduler_location ( data , stats ) : 
~~~ scheduler_location = SchedulerLocation ( ) 
scheduler_location . ParseFromString ( data ) 
callback ( scheduler_location ) 
~~ ~~ def load ( file_object ) : 
marshaller = JavaObjectUnmarshaller ( file_object ) 
marshaller . add_transformer ( DefaultObjectTransformer ( ) ) 
return marshaller . readObject ( ) 
~~ def loads ( string ) : 
f = StringIO . StringIO ( string ) 
marshaller = JavaObjectUnmarshaller ( f ) 
~~ def copy ( self , new_object ) : 
new_object . classdesc = self . classdesc 
for name in self . classdesc . fields_names : 
~~~ new_object . __setattr__ ( name , getattr ( self , name ) ) 
~~ ~~ def readObject ( self ) : 
~~~ _ , res = self . _read_and_exec_opcode ( ident = 0 ) 
position_bak = self . object_stream . tell ( ) 
the_rest = self . object_stream . read ( ) 
if len ( the_rest ) : 
log_debug ( self . _create_hexdump ( the_rest ) ) 
~~ self . object_stream . seek ( position_bak ) 
return res 
~~~ self . _oops_dump_state ( ) 
~~ ~~ def do_classdesc ( self , parent = None , ident = 0 ) : 
~~~ """do_classdesc""" 
clazz = JavaClass ( ) 
log_debug ( "[classdesc]" , ident ) 
ba = self . _readString ( ) 
clazz . name = ba 
( serialVersionUID , newHandle , classDescFlags ) = self . _readStruct ( ">LLB" ) 
clazz . serialVersionUID = serialVersionUID 
clazz . flags = classDescFlags 
self . _add_reference ( clazz ) 
( length , ) = self . _readStruct ( ">H" ) 
clazz . fields_names = [ ] 
clazz . fields_types = [ ] 
for _ in range ( length ) : 
~~~ ( typecode , ) = self . _readStruct ( ">B" ) 
field_name = self . _readString ( ) 
field_type = None 
field_type = self . _convert_char_to_type ( typecode ) 
if field_type == self . TYPE_ARRAY : 
~~~ _ , field_type = self . _read_and_exec_opcode ( 
ident = ident + 1 , expect = [ self . TC_STRING , self . TC_REFERENCE ] ) 
assert isinstance ( field_type , str ) 
~~ elif field_type == self . TYPE_OBJECT : 
assert field_name is not None 
assert field_type is not None 
clazz . fields_names . append ( field_name ) 
clazz . fields_types . append ( field_type ) 
~~ if parent : 
~~~ parent . __fields = clazz . fields_names 
parent . __types = clazz . fields_types 
~~ ( opid , ) = self . _readStruct ( ">B" ) 
if opid != self . TC_ENDBLOCKDATA : 
~~ _ , superclassdesc = self . _read_and_exec_opcode ( 
ident = ident + 1 , expect = [ self . TC_CLASSDESC , self . TC_NULL , self . TC_REFERENCE ] ) 
log_debug ( str ( superclassdesc ) , ident ) 
clazz . superclass = superclassdesc 
return clazz 
~~ def getMetricsTimeline ( tmaster , 
component_name , 
metric_names , 
instances , 
start_time , 
end_time , 
callback = None ) : 
~~ host = tmaster . host 
port = tmaster . stats_port 
metricRequest = tmaster_pb2 . MetricRequest ( ) 
metricRequest . component_name = component_name 
~~~ for instance in instances : 
~~~ metricRequest . instance_id . append ( instance ) 
~~ ~~ for metricName in metric_names : 
~~~ metricRequest . metric . append ( metricName ) 
~~ metricRequest . explicit_interval . start = start_time 
metricRequest . explicit_interval . end = end_time 
metricRequest . minutely = True 
metricRequestString = metricRequest . SerializeToString ( ) 
url = "http://{0}:{1}/stats" . format ( host , port ) 
body = metricRequestString , 
raise Exception ( message ) 
~~ metricResponse = tmaster_pb2 . MetricResponse ( ) 
metricResponse . ParseFromString ( result . body ) 
if metricResponse . status . status == common_pb2 . NOTOK : 
~~~ if metricResponse . status . HasField ( "message" ) : 
~~ ~~ ret = { } 
ret [ "starttime" ] = start_time 
ret [ "endtime" ] = end_time 
ret [ "component" ] = component_name 
ret [ "timeline" ] = { } 
for metric in metricResponse . metric : 
~~~ instance = metric . instance_id 
for im in metric . metric : 
~~~ metricname = im . name 
if metricname not in ret [ "timeline" ] : 
~~~ ret [ "timeline" ] [ metricname ] = { } 
~~ if instance not in ret [ "timeline" ] [ metricname ] : 
~~~ ret [ "timeline" ] [ metricname ] [ instance ] = { } 
~~ for interval_value in im . interval_values : 
~~~ ret [ "timeline" ] [ metricname ] [ instance ] [ interval_value . interval . start ] = interval_value . value 
~~ ~~ ~~ raise tornado . gen . Return ( ret ) 
~~ def create_parser ( subparsers ) : 
parser = subparsers . add_parser ( 
'version' , 
add_help = True ) 
add_version_titles ( parser ) 
'cluster' , 
nargs = '?' , 
cli_args . add_service_url ( parser ) 
parser . set_defaults ( subcommand = 'version' ) 
cluster = cl_args [ 'cluster' ] 
if cluster : 
~~~ config_file = config . heron_rc_file ( ) 
client_confs = dict ( ) 
client_confs = cdefs . read_server_mode_cluster_definition ( cluster , cl_args , config_file ) 
if not client_confs [ cluster ] : 
return SimpleResult ( Status . HeronError ) 
~~ if not 'service_url' in client_confs [ cluster ] : 
~~ service_endpoint = cl_args [ 'service_url' ] 
service_apiurl = service_endpoint + rest . ROUTE_SIGNATURES [ command ] [ 1 ] 
service_method = rest . ROUTE_SIGNATURES [ command ] [ 0 ] 
~~~ r = service_method ( service_apiurl ) 
if r . status_code != requests . codes . ok : 
~~ sorted_items = sorted ( r . json ( ) . items ( ) , key = lambda tup : tup [ 0 ] ) 
for key , value in sorted_items : 
~~ ~~ except ( requests . exceptions . ConnectionError , requests . exceptions . HTTPError ) as err : 
~~~ Log . error ( err ) 
~~~ config . print_build_info ( ) 
~~ return SimpleResult ( Status . Ok ) 
~~ def validate_state_locations ( self ) : 
names = map ( lambda loc : loc [ "name" ] , self . locations ) 
~~ def add_arguments ( parser ) : 
'--tracker_url' , 
metavar = \ + consts . DEFAULT_TRACKER_URL + \ , 
default = consts . DEFAULT_TRACKER_URL ) 
'--address' , 
metavar = \ + consts . DEFAULT_ADDRESS + \ , 
default = consts . DEFAULT_ADDRESS ) 
'--port' , 
type = int , 
default = consts . DEFAULT_PORT ) 
'--base_url' , 
+ str ( consts . DEFAULT_BASE_URL ) + ')' , 
default = consts . DEFAULT_BASE_URL ) 
cluster = self . get_argument ( "cluster" ) 
environ = self . get_argument ( "environ" ) 
topology = self . get_argument ( "topology" ) 
component = self . get_argument ( "component" , default = None ) 
metricnames = self . get_arguments ( "metricname" ) 
instances = self . get_arguments ( "instance" ) 
interval = self . get_argument ( "interval" , default = - 1 ) 
time_range = ( 0 , interval ) 
compnames = [ component ] if component else ( yield access . get_comps ( cluster , environ , topology ) ) 
futures = { } 
for comp in compnames : 
~~~ future = access . get_comp_metrics ( 
cluster , environ , topology , comp , instances , 
metricnames , time_range ) 
futures [ comp ] = future 
~~ results = yield futures 
self . write ( results [ component ] if component else results ) 
metric = self . get_argument ( "metric" ) 
instances = self . get_argument ( "instance" ) 
start = self . get_argument ( "starttime" ) 
end = self . get_argument ( "endtime" ) 
maxquery = self . get_argument ( "max" , default = False ) 
timerange = ( start , end ) 
compnames = [ component ] 
if metric == "backpressure" : 
~~~ for comp in compnames : 
~~~ future = query_handler . fetch_backpressure ( cluster , metric , topology , component , 
instances , timerange , maxquery , environ ) 
~~~ fetch = query_handler . fetch_max if maxquery else query_handler . fetch 
~~~ future = fetch ( cluster , metric , topology , component , 
instances , timerange , environ ) 
~~ ~~ results = yield futures 
~~ def get ( self , instance_id ) : 
self . content_type = 'application/json' 
self . write ( json . dumps ( utils . chain ( [ 
[ 'ps' , 'auxwwww' ] , 
[ 'grep' , instance_id ] , 
[ 'grep' , 'java' ] , 
~~ offset = self . get_argument ( "offset" , default = - 1 ) 
length = self . get_argument ( "length" , default = - 1 ) 
if not os . path . isfile ( path ) : 
~~ data = utils . read_chunk ( path , offset = offset , length = length , escape_data = True ) 
self . write ( json . dumps ( data ) ) 
~~ def initialize ( self , config , context ) : 
self . emit_count = 0 
self . ack_count = 0 
self . fail_count = 0 
if not PulsarSpout . serviceUrl in config or not PulsarSpout . topicName in config : 
~~ self . pulsar_cluster = str ( config [ PulsarSpout . serviceUrl ] ) 
self . topic = str ( config [ PulsarSpout . topicName ] ) 
mode = config [ api_constants . TOPOLOGY_RELIABILITY_MODE ] 
if mode == api_constants . TopologyReliabilityMode . ATLEAST_ONCE : 
~~~ self . acking_timeout = 1000 * int ( config [ api_constants . TOPOLOGY_MESSAGE_TIMEOUT_SECS ] ) 
~~~ self . acking_timeout = 30000 
~~ if PulsarSpout . receiveTimeoutMs in config : 
~~~ self . receive_timeout_ms = config [ PulsarSpout . receiveTimeoutMs ] 
~~~ self . receive_timeout_ms = 10 
~~ if PulsarSpout . deserializer in config : 
~~~ self . deserializer = config [ PulsarSpout . deserializer ] 
if not callable ( self . deserializer ) : 
~~~ self . deserializer = self . default_deserializer 
~~ self . logConfFileName = GenerateLogConfig ( context ) 
self . client = pulsar . Client ( self . pulsar_cluster , log_conf_file_path = self . logConfFileName ) 
~~~ self . consumer = self . client . subscribe ( self . topic , context . get_topology_name ( ) , 
consumer_type = pulsar . ConsumerType . Failover , 
unacked_messages_timeout_ms = self . acking_timeout ) 
clusters = self . get_arguments ( constants . PARAM_CLUSTER ) 
environs = self . get_arguments ( constants . PARAM_ENVIRON ) 
topologies = self . tracker . topologies 
for topology in topologies : 
~~~ cluster = topology . cluster 
environ = topology . environ 
if not cluster or not environ : 
~~ if clusters and cluster not in clusters : 
~~ if environs and environ not in environs : 
~~ if cluster not in ret : 
~~~ ret [ cluster ] = { } 
~~ if environ not in ret [ cluster ] : 
~~~ ret [ cluster ] [ environ ] = { } 
~~~ topology_info = self . tracker . getTopologyInfo ( topology . name , cluster , role , environ ) 
if topology_info and "execution_state" in topology_info : 
~~~ ret [ cluster ] [ environ ] [ topology . name ] = topology_info [ "execution_state" ] 
~~ ~~ self . write_success_response ( ret ) 
~~ def getInstanceJstack ( self , topology_info , instance_id ) : 
pid_response = yield getInstancePid ( topology_info , instance_id ) 
~~~ http_client = tornado . httpclient . AsyncHTTPClient ( ) 
pid_json = json . loads ( pid_response ) 
pid = pid_json [ 'stdout' ] . strip ( ) 
if pid == '' : 
~~ endpoint = utils . make_shell_endpoint ( topology_info , instance_id ) 
url = "%s/jstack/%s" % ( endpoint , pid ) 
response = yield http_client . fetch ( url ) 
raise tornado . gen . Return ( response . body ) 
'update' , 
args . add_titles ( parser ) 
args . add_cluster_role_env ( parser ) 
args . add_topology ( parser ) 
args . add_config ( parser ) 
args . add_dry_run ( parser ) 
args . add_service_url ( parser ) 
args . add_verbose ( parser ) 
def parallelism_type ( value ) : 
~~~ pattern = re . compile ( r"^[\\w\\.-]+:[\\d]+$" ) 
if not pattern . match ( value ) : 
~~~ raise argparse . ArgumentTypeError ( 
~~ parser . add_argument ( 
'--component-parallelism' , 
action = 'append' , 
type = parallelism_type , 
def runtime_config_type ( value ) : 
~~~ pattern = re . compile ( r"^([\\w\\.-]+:){1,2}[\\w\\.-]+$" ) 
% value ) 
'--runtime-config' , 
type = runtime_config_type , 
def container_number_type ( value ) : 
~~~ pattern = re . compile ( r"^\\d+$" ) 
'--container-number' , 
type = container_number_type , 
parser . set_defaults ( subcommand = 'update' ) 
~~ def build_extra_args_dict ( cl_args ) : 
component_parallelism = cl_args [ 'component_parallelism' ] 
runtime_configs = cl_args [ 'runtime_config' ] 
container_number = cl_args [ 'container_number' ] 
if ( component_parallelism and runtime_configs ) or ( container_number and runtime_configs ) : 
"can\ ) 
~~ dict_extra_args = { } 
nothing_set = True 
if component_parallelism : 
~~~ dict_extra_args . update ( { 'component_parallelism' : component_parallelism } ) 
nothing_set = False 
~~ if container_number : 
~~~ dict_extra_args . update ( { 'container_number' : container_number } ) 
~~ if runtime_configs : 
~~~ dict_extra_args . update ( { 'runtime_config' : runtime_configs } ) 
~~ if nothing_set : 
~~ if cl_args [ 'dry_run' ] : 
~~~ dict_extra_args . update ( { 'dry_run' : True } ) 
if 'dry_run_format' in cl_args : 
~~~ dict_extra_args . update ( { 'dry_run_format' : cl_args [ "dry_run_format" ] } ) 
~~ ~~ return dict_extra_args 
~~ def convert_args_dict_to_list ( dict_extra_args ) : 
list_extra_args = [ ] 
if 'component_parallelism' in dict_extra_args : 
~~~ list_extra_args += [ "--component_parallelism" , 
',' . join ( dict_extra_args [ 'component_parallelism' ] ) ] 
~~ if 'runtime_config' in dict_extra_args : 
~~~ list_extra_args += [ "--runtime_config" , 
',' . join ( dict_extra_args [ 'runtime_config' ] ) ] 
~~ if 'container_number' in dict_extra_args : 
~~~ list_extra_args += [ "--container_number" , 
',' . join ( dict_extra_args [ 'container_number' ] ) ] 
~~ if 'dry_run' in dict_extra_args and dict_extra_args [ 'dry_run' ] : 
~~~ list_extra_args += [ '--dry_run' ] 
~~ if 'dry_run_format' in dict_extra_args : 
~~~ list_extra_args += [ '--dry_run_format' , dict_extra_args [ 'dry_run_format' ] ] 
~~ return list_extra_args 
extra_lib_jars = jars . packing_jars ( ) 
dict_extra_args = { } 
~~~ dict_extra_args = build_extra_args_dict ( cl_args ) 
~~~ return SimpleResult ( Status . InvocationError , err . message ) 
~~ if cl_args [ 'deploy_mode' ] == config . SERVER_MODE : 
~~~ return cli_helper . run_server ( command , cl_args , action , dict_extra_args ) 
~~~ list_extra_args = convert_args_dict_to_list ( dict_extra_args ) 
return cli_helper . run_direct ( command , cl_args , action , list_extra_args , extra_lib_jars ) 
~~ ~~ def getInstancePid ( topology_info , instance_id ) : 
endpoint = utils . make_shell_endpoint ( topology_info , instance_id ) 
url = "%s/pid/%s" % ( endpoint , instance_id ) 
instance = self . get_argument_instance ( ) 
topology_info = self . tracker . getTopologyInfo ( topology_name , cluster , role , environ ) 
result = yield getInstancePid ( topology_info , instance ) 
self . write_success_response ( result ) 
~~ ~~ def is_grouping_sane ( cls , gtype ) : 
if gtype == cls . SHUFFLE or gtype == cls . ALL or gtype == cls . LOWEST or gtype == cls . NONE : 
~~ elif isinstance ( gtype , cls . FIELDS ) : 
~~~ return gtype . gtype == topology_pb2 . Grouping . Value ( "FIELDS" ) and gtype . fields is not None 
~~ elif isinstance ( gtype , cls . CUSTOM ) : 
~~~ return gtype . gtype == topology_pb2 . Grouping . Value ( "CUSTOM" ) and gtype . python_serialized is not None 
~~ ~~ def fields ( cls , * fields ) : 
if len ( fields ) == 1 and isinstance ( fields [ 0 ] , list ) : 
~~~ fields = fields [ 0 ] 
~~~ fields = list ( fields ) 
~~ for i in fields : 
~~~ if not isinstance ( i , str ) : 
~~ ~~ if not fields : 
~~ return cls . FIELDS ( gtype = topology_pb2 . Grouping . Value ( "FIELDS" ) , 
fields = fields ) 
~~ def custom ( cls , customgrouper ) : 
if customgrouper is None : 
~~ if not isinstance ( customgrouper , ICustomGrouping ) and not isinstance ( customgrouper , str ) : 
~~ serialized = default_serializer . serialize ( customgrouper ) 
return cls . custom_serialized ( serialized , is_java = False ) 
~~ def custom_serialized ( cls , serialized , is_java = True ) : 
if not isinstance ( serialized , bytes ) : 
~~ if not is_java : 
~~~ return cls . CUSTOM ( gtype = topology_pb2 . Grouping . Value ( "CUSTOM" ) , 
python_serialized = serialized ) 
~~ ~~ def register_metrics ( self , metrics_collector , interval ) : 
for field , metrics in self . metrics . items ( ) : 
~~~ metrics_collector . register_metric ( field , metrics , interval ) 
~~ ~~ def update_count ( self , name , incr_by = 1 , key = None ) : 
if name not in self . metrics : 
~~ if key is None and isinstance ( self . metrics [ name ] , CountMetric ) : 
~~~ self . metrics [ name ] . incr ( incr_by ) 
~~ elif key is not None and isinstance ( self . metrics [ name ] , MultiCountMetric ) : 
~~~ self . metrics [ name ] . incr ( key , incr_by ) 
~~ ~~ def update_reduced_metric ( self , name , value , key = None ) : 
~~ if key is None and isinstance ( self . metrics [ name ] , ReducedMetric ) : 
~~~ self . metrics [ name ] . update ( value ) 
~~ elif key is not None and isinstance ( self . metrics [ name ] , MultiReducedMetric ) : 
~~~ self . metrics [ name ] . update ( key , value ) 
~~ ~~ def update_received_packet ( self , received_pkt_size_bytes ) : 
self . update_count ( self . RECEIVED_PKT_COUNT ) 
self . update_count ( self . RECEIVED_PKT_SIZE , incr_by = received_pkt_size_bytes ) 
~~ def update_sent_packet ( self , sent_pkt_size_bytes ) : 
self . update_count ( self . SENT_PKT_COUNT ) 
self . update_count ( self . SENT_PKT_SIZE , incr_by = sent_pkt_size_bytes ) 
~~ def register_metrics ( self , context ) : 
sys_config = system_config . get_sys_config ( ) 
interval = float ( sys_config [ constants . HERON_METRICS_EXPORT_INTERVAL_SEC ] ) 
collector = context . get_metrics_collector ( ) 
super ( ComponentMetrics , self ) . register_metrics ( collector , interval ) 
~~ def serialize_data_tuple ( self , stream_id , latency_in_ns ) : 
self . update_count ( self . TUPLE_SERIALIZATION_TIME_NS , incr_by = latency_in_ns , key = stream_id ) 
~~ def _init_multi_count_metrics ( self , pplan_helper ) : 
to_init = [ self . metrics [ i ] for i in self . to_multi_init 
if i in self . metrics and isinstance ( self . metrics [ i ] , MultiCountMetric ) ] 
for out_stream in pplan_helper . get_my_spout ( ) . outputs : 
~~~ stream_id = out_stream . stream . id 
for metric in to_init : 
~~~ metric . add_key ( stream_id ) 
~~ ~~ ~~ def next_tuple ( self , latency_in_ns ) : 
self . update_reduced_metric ( self . NEXT_TUPLE_LATENCY , latency_in_ns ) 
self . update_count ( self . NEXT_TUPLE_COUNT ) 
~~ def acked_tuple ( self , stream_id , complete_latency_ns ) : 
self . update_count ( self . ACK_COUNT , key = stream_id ) 
self . update_reduced_metric ( self . COMPLETE_LATENCY , complete_latency_ns , key = stream_id ) 
~~ def failed_tuple ( self , stream_id , fail_latency_ns ) : 
self . update_count ( self . FAIL_COUNT , key = stream_id ) 
self . update_reduced_metric ( self . FAIL_LATENCY , fail_latency_ns , key = stream_id ) 
to_in_init = [ self . metrics [ i ] for i in self . inputs_init 
for in_stream in pplan_helper . get_my_bolt ( ) . inputs : 
~~~ stream_id = in_stream . stream . id 
global_stream_id = in_stream . stream . component_name + "/" + stream_id 
for metric in to_in_init : 
metric . add_key ( global_stream_id ) 
~~ ~~ to_out_init = [ self . metrics [ i ] for i in self . outputs_init 
for out_stream in pplan_helper . get_my_bolt ( ) . outputs : 
for metric in to_out_init : 
~~ ~~ ~~ def execute_tuple ( self , stream_id , source_component , latency_in_ns ) : 
self . update_count ( self . EXEC_COUNT , key = stream_id ) 
self . update_reduced_metric ( self . EXEC_LATENCY , latency_in_ns , stream_id ) 
self . update_count ( self . EXEC_TIME_NS , incr_by = latency_in_ns , key = stream_id ) 
global_stream_id = source_component + "/" + stream_id 
self . update_count ( self . EXEC_COUNT , key = global_stream_id ) 
self . update_reduced_metric ( self . EXEC_LATENCY , latency_in_ns , global_stream_id ) 
self . update_count ( self . EXEC_TIME_NS , incr_by = latency_in_ns , key = global_stream_id ) 
~~ def deserialize_data_tuple ( self , stream_id , source_component , latency_in_ns ) : 
self . update_count ( self . TUPLE_DESERIALIZATION_TIME_NS , incr_by = latency_in_ns , key = stream_id ) 
self . update_count ( self . TUPLE_DESERIALIZATION_TIME_NS , incr_by = latency_in_ns , 
key = global_stream_id ) 
~~ def acked_tuple ( self , stream_id , source_component , latency_in_ns ) : 
self . update_reduced_metric ( self . PROCESS_LATENCY , latency_in_ns , stream_id ) 
global_stream_id = source_component + '/' + stream_id 
self . update_count ( self . ACK_COUNT , key = global_stream_id ) 
self . update_reduced_metric ( self . PROCESS_LATENCY , latency_in_ns , global_stream_id ) 
~~ def failed_tuple ( self , stream_id , source_component , latency_in_ns ) : 
self . update_reduced_metric ( self . FAIL_LATENCY , latency_in_ns , stream_id ) 
self . update_count ( self . FAIL_COUNT , key = global_stream_id ) 
self . update_reduced_metric ( self . FAIL_LATENCY , latency_in_ns , global_stream_id ) 
~~ def register_metric ( self , name , metric , time_bucket_in_sec ) : 
if name in self . metrics_map : 
self . metrics_map [ name ] = metric 
if time_bucket_in_sec in self . time_bucket_in_sec_to_metrics_name : 
~~~ self . time_bucket_in_sec_to_metrics_name [ time_bucket_in_sec ] . append ( name ) 
~~~ self . time_bucket_in_sec_to_metrics_name [ time_bucket_in_sec ] = [ name ] 
self . _register_timer_task ( time_bucket_in_sec ) 
~~ ~~ def poll ( self ) : 
~~~ ret = self . _buffer . get ( block = False ) 
if self . _producer_callback is not None : 
~~~ self . _producer_callback ( ) 
~~ except Queue . Empty : 
raise Queue . Empty 
~~ ~~ def offer ( self , item ) : 
~~~ self . _buffer . put ( item , block = False ) 
if self . _consumer_callback is not None : 
~~~ self . _consumer_callback ( ) 
~~ except Queue . Full : 
raise Queue . Full 
~~ ~~ def parse ( version ) : 
match = _REGEX . match ( version ) 
if match is None : 
~~ verinfo = match . groupdict ( ) 
verinfo [ 'major' ] = int ( verinfo [ 'major' ] ) 
verinfo [ 'minor' ] = int ( verinfo [ 'minor' ] ) 
verinfo [ 'patch' ] = int ( verinfo [ 'patch' ] ) 
return verinfo 
~~ def create_parser ( subparsers , action , help_arg ) : 
action , 
help = help_arg , 
parser . set_defaults ( subcommand = action ) 
~~ def run_server ( command , cl_args , action , extra_args = dict ( ) ) : 
topology_name = cl_args [ 'topology-name' ] 
service_endpoint = cl_args [ 'service_url' ] 
apiroute = rest . ROUTE_SIGNATURES [ command ] [ 1 ] % ( 
cl_args [ 'cluster' ] , 
cl_args [ 'role' ] , 
cl_args [ 'environ' ] , 
topology_name 
service_apiurl = service_endpoint + apiroute 
data = flatten_args ( extra_args ) 
~~~ r = service_method ( service_apiurl , data = data ) 
s = Status . Ok if r . status_code == requests . codes . ok else Status . HeronError 
return SimpleResult ( Status . HeronError , err_msg , succ_msg ) 
~~ return SimpleResult ( s , err_msg , succ_msg ) 
~~ def run_direct ( command , cl_args , action , extra_args = [ ] , extra_lib_jars = [ ] ) : 
new_args = [ 
"--cluster" , cl_args [ 'cluster' ] , 
"--role" , cl_args [ 'role' ] , 
"--environment" , cl_args [ 'environ' ] , 
"--submit_user" , cl_args [ 'submit_user' ] , 
"--heron_home" , config . get_heron_dir ( ) , 
"--config_path" , cl_args [ 'config_path' ] , 
"--override_config_file" , cl_args [ 'override_config_file' ] , 
"--release_file" , config . get_heron_release_file ( ) , 
"--topology_name" , topology_name , 
"--command" , command , 
new_args += extra_args 
lib_jars = config . get_heron_libs ( jars . scheduler_jars ( ) + jars . statemgr_jars ( ) ) 
lib_jars += extra_lib_jars 
if Log . getEffectiveLevel ( ) == logging . DEBUG : 
~~~ new_args . append ( "--verbose" ) 
~~ result = execute . heron_class ( 
'org.apache.heron.scheduler.RuntimeManagerMain' , 
lib_jars , 
extra_jars = [ ] , 
args = new_args 
result . add_context ( err_msg , succ_msg ) 
~~ def get_all_zk_state_managers ( conf ) : 
state_managers = [ ] 
state_locations = conf . get_state_locations_of_type ( "zookeeper" ) 
for location in state_locations : 
~~~ name = location [ 'name' ] 
hostport = location [ 'hostport' ] 
hostportlist = [ ] 
for hostportpair in hostport . split ( ',' ) : 
~~~ host = None 
port = None 
if ':' in hostport : 
~~~ hostandport = hostportpair . split ( ':' ) 
if len ( hostandport ) == 2 : 
~~~ host = hostandport [ 0 ] 
port = int ( hostandport [ 1 ] ) 
~~ ~~ if not host or not port : 
~~ hostportlist . append ( ( host , port ) ) 
~~ tunnelhost = location [ 'tunnelhost' ] 
rootpath = location [ 'rootpath' ] 
state_manager = ZkStateManager ( name , hostportlist , rootpath , tunnelhost ) 
state_managers . append ( state_manager ) 
~~ return state_managers 
~~ def get_all_file_state_managers ( conf ) : 
state_locations = conf . get_state_locations_of_type ( "file" ) 
rootpath = os . path . expanduser ( location [ 'rootpath' ] ) 
state_manager = FileStateManager ( name , rootpath ) 
~~ def incr ( self , key , to_add = 1 ) : 
if key not in self . value : 
~~~ self . value [ key ] = CountMetric ( ) 
~~ self . value [ key ] . incr ( to_add ) 
~~ def update ( self , key , value ) : 
~~~ self . value [ key ] = ReducedMetric ( self . reducer ) 
~~ self . value [ key ] . update ( value ) 
~~ def add_key ( self , key ) : 
~~ ~~ def add_data_tuple ( self , stream_id , new_data_tuple , tuple_size_in_bytes ) : 
if ( self . current_data_tuple_set is None ) or ( self . current_data_tuple_set . stream . id != stream_id ) or ( len ( self . current_data_tuple_set . tuples ) >= self . data_tuple_set_capacity ) or ( self . current_data_tuple_size_in_bytes >= self . max_data_tuple_size_in_bytes ) : 
~~~ self . _init_new_data_tuple ( stream_id ) 
~~ added_tuple = self . current_data_tuple_set . tuples . add ( ) 
added_tuple . CopyFrom ( new_data_tuple ) 
self . current_data_tuple_size_in_bytes += tuple_size_in_bytes 
self . total_data_emitted_in_bytes += tuple_size_in_bytes 
~~ def add_control_tuple ( self , new_control_tuple , tuple_size_in_bytes , is_ack ) : 
if self . current_control_tuple_set is None : 
~~~ self . _init_new_control_tuple ( ) 
~~ elif is_ack and ( len ( self . current_control_tuple_set . fails ) > 0 or 
len ( self . current_control_tuple_set . acks ) >= self . control_tuple_set_capacity ) : 
~~ elif not is_ack and ( len ( self . current_control_tuple_set . acks ) > 0 or 
len ( self . current_control_tuple_set . fails ) >= self . control_tuple_set_capacity ) : 
~~ if is_ack : 
~~~ added_tuple = self . current_control_tuple_set . acks . add ( ) 
~~~ added_tuple = self . current_control_tuple_set . fails . add ( ) 
~~ added_tuple . CopyFrom ( new_control_tuple ) 
~~ def add_ckpt_state ( self , ckpt_id , ckpt_state ) : 
self . _flush_remaining ( ) 
msg = ckptmgr_pb2 . StoreInstanceStateCheckpoint ( ) 
istate = ckptmgr_pb2 . InstanceStateCheckpoint ( ) 
istate . checkpoint_id = ckpt_id 
istate . state = ckpt_state 
msg . state . CopyFrom ( istate ) 
self . _push_tuple_to_stream ( msg ) 
'config' , 
ex_subparsers = parser . add_subparsers ( 
title = "Commands" , 
description = None ) 
list_parser = ex_subparsers . add_parser ( 
'list' , 
usage = "%(prog)s" , 
list_parser . set_defaults ( configcommand = 'list' ) 
set_parser = ex_subparsers . add_parser ( 
'set' , 
set_parser . add_argument ( 
'property' , 
'value' , 
set_parser . set_defaults ( configcommand = 'set' ) 
unset_parser = ex_subparsers . add_parser ( 
'unset' , 
unset_parser . add_argument ( 
unset_parser . set_defaults ( configcommand = 'unset' ) 
parser . set_defaults ( subcommand = 'config' ) 
configcommand = cl_args . get ( 'configcommand' , None ) 
if configcommand == 'set' : 
~~~ return _set ( cl_args ) 
~~ elif configcommand == 'unset' : 
~~~ return _unset ( cl_args ) 
~~~ return _list ( cl_args ) 
~~ ~~ def valid_path ( path ) : 
if path . endswith ( '*' ) : 
if os . path . isdir ( path [ : - 1 ] ) : 
if os . path . isdir ( path ) : 
if os . path . isfile ( path ) : 
~~ def valid_java_classpath ( classpath ) : 
paths = classpath . split ( ':' ) 
for path_entry in paths : 
~~~ if not valid_path ( path_entry . strip ( ) ) : 
~~ def add_edge ( self , U , V ) : 
if not U in self . edges : 
~~~ self . edges [ U ] = set ( ) 
~~ if not V in self . edges : 
~~~ self . edges [ V ] = set ( ) 
~~ if not V in self . edges [ U ] : 
~~~ self . edges [ U ] . add ( V ) 
~~ ~~ def bfs_depth ( self , U ) : 
visited = set ( ) 
max_depth = 0 
while bfs_queue : 
~~~ [ V , depth ] = bfs_queue . pop ( ) 
if max_depth < depth : 
~~~ max_depth = depth 
~~ visited . add ( V ) 
adj_set = self . edges [ V ] 
for W in adj_set : 
~~~ if W not in visited : 
~~~ bfs_queue . append ( [ W , depth + 1 ] ) 
~~ ~~ ~~ return max_depth 
~~ def diameter ( self ) : 
diameter = 0 
for U in self . edges : 
~~~ depth = self . bfs_depth ( U ) 
if depth > diameter : 
~~~ diameter = depth 
~~ ~~ return diameter 
~~ def _get_deps_list ( abs_path_to_pex ) : 
pex = zipfile . ZipFile ( abs_path_to_pex , mode = 'r' ) 
deps = list ( set ( [ re . match ( egg_regex , i ) . group ( 1 ) for i in pex . namelist ( ) 
if re . match ( egg_regex , i ) is not None ] ) ) 
return deps 
~~ def load_pex ( path_to_pex , include_deps = True ) : 
abs_path_to_pex = os . path . abspath ( path_to_pex ) 
if abs_path_to_pex not in sys . path : 
~~~ sys . path . insert ( 0 , os . path . dirname ( abs_path_to_pex ) ) 
~~ if include_deps : 
~~~ for dep in _get_deps_list ( abs_path_to_pex ) : 
~~~ to_join = os . path . join ( os . path . dirname ( abs_path_to_pex ) , dep ) 
if to_join not in sys . path : 
sys . path . insert ( 0 , to_join ) 
~~ def resolve_heron_suffix_issue ( abs_pex_path , class_path ) : 
importer = zipimport . zipimporter ( abs_pex_path ) 
importer . load_module ( "heron" ) 
to_load_lst = class_path . split ( '.' ) [ 1 : - 1 ] 
loaded = [ 'heron' ] 
loaded_mod = None 
for to_load in to_load_lst : 
~~~ sub_importer = zipimport . zipimporter ( os . path . join ( abs_pex_path , '/' . join ( loaded ) ) ) 
loaded_mod = sub_importer . load_module ( to_load ) 
loaded . append ( to_load ) 
~~ return loaded_mod 
~~ def import_and_get_class ( path_to_pex , python_class_name ) : 
split = python_class_name . split ( '.' ) 
from_path = '.' . join ( split [ : - 1 ] ) 
import_name = python_class_name . split ( '.' ) [ - 1 ] 
if python_class_name . startswith ( "heron." ) : 
~~~ mod = resolve_heron_suffix_issue ( abs_path_to_pex , python_class_name ) 
return getattr ( mod , import_name ) 
~~ ~~ mod = __import__ ( from_path , fromlist = [ import_name ] , level = - 1 ) 
~~ def new_source ( self , source ) : 
source_streamlet = None 
if callable ( source ) : 
~~~ source_streamlet = SupplierStreamlet ( source ) 
~~ elif isinstance ( source , Generator ) : 
~~~ source_streamlet = GeneratorStreamlet ( source ) 
~~~ raise RuntimeError ( "Builder\ ) 
~~ self . _sources . append ( source_streamlet ) 
return source_streamlet 
~~ def build ( self , bldr ) : 
stage_names = sets . Set ( ) 
for source in self . _sources : 
~~~ source . _build ( bldr , stage_names ) 
~~ for source in self . _sources : 
~~~ if not source . _all_built ( ) : 
~~ ~~ ~~ def get ( self , path ) : 
t = Template ( utils . get_asset ( "file.html" ) ) 
~~~ self . set_status ( 404 ) 
~~ args = dict ( 
filename = path , 
jquery = utils . get_asset ( "jquery.js" ) , 
pailer = utils . get_asset ( "jquery.pailer.js" ) , 
css = utils . get_asset ( "bootstrap.css" ) , 
self . write ( t . generate ( ** args ) ) 
~~ def load_state_manager_locations ( cluster , state_manager_config_file = 'heron-conf/statemgr.yaml' , 
overrides = { } ) : 
with open ( state_manager_config_file , 'r' ) as stream : 
~~~ config = yaml . load ( stream ) 
~~ home_dir = os . path . expanduser ( "~" ) 
wildcards = { 
"~" : home_dir , 
"${HOME}" : home_dir , 
"${CLUSTER}" : cluster , 
if os . getenv ( 'JAVA_HOME' ) : 
~~~ wildcards [ "${JAVA_HOME}" ] = os . getenv ( 'JAVA_HOME' ) 
~~ config = __replace ( config , wildcards , state_manager_config_file ) 
if overrides : 
~~~ config . update ( overrides ) 
~~ state_manager_location = { 
'type' : 'file' , 
'name' : 'local' , 
'tunnelhost' : '127.0.0.1' , 
'rootpath' : '~/.herondata/repository/state/local' , 
key_mappings = { 
'heron.statemgr.connection.string' : 'hostport' , 
'heron.statemgr.tunnel.host' : 'tunnelhost' , 
'heron.statemgr.root.path' : 'rootpath' , 
for config_key in key_mappings : 
~~~ if config_key in config : 
~~~ state_manager_location [ key_mappings [ config_key ] ] = config [ config_key ] 
~~ ~~ state_manager_class = config [ 'heron.class.state.manager' ] 
if state_manager_class == 'org.apache.heron.statemgr.zookeeper.curator.CuratorStateManager' : 
~~~ state_manager_location [ 'type' ] = 'zookeeper' 
state_manager_location [ 'name' ] = 'zk' 
~~ return [ state_manager_location ] 
~~ def __replace ( config , wildcards , config_file ) : 
for config_key in config : 
~~~ config_value = config [ config_key ] 
original_value = config_value 
if isinstance ( config_value , str ) : 
~~~ for token in wildcards : 
~~~ if wildcards [ token ] : 
~~~ config_value = config_value . replace ( token , wildcards [ token ] ) 
~~ ~~ found = re . findall ( r'\\${[A-Z_]+}' , config_value ) 
if found : 
~~ config [ config_key ] = config_value 
~~ ~~ return config 
~~ def get_command_handlers ( ) : 
return { 
'activate' : activate , 
'config' : hconfig , 
'deactivate' : deactivate , 
'help' : cli_help , 
'kill' : kill , 
'restart' : restart , 
'submit' : submit , 
'update' : update , 
'version' : version 
~~ def create_parser ( command_handlers ) : 
prog = 'heron' , 
epilog = HELP_EPILOG , 
formatter_class = config . SubcommandHelpFormatter , 
subparsers = parser . add_subparsers ( 
command_list = sorted ( command_handlers . items ( ) ) 
for command in command_list : 
~~~ command [ 1 ] . create_parser ( subparsers ) 
~~ return parser 
~~ def run ( handlers , command , parser , command_args , unknown_args ) : 
if command in handlers : 
~~~ return handlers [ command ] . run ( command , parser , command_args , unknown_args ) 
return result . SimpleResult ( result . Status . InvocationError , err_context ) 
~~ ~~ def cleanup ( files ) : 
for cur_file in files : 
~~~ if os . path . isdir ( cur_file ) : 
~~~ shutil . rmtree ( cur_file ) 
~~~ shutil . rmtree ( os . path . dirname ( cur_file ) ) 
~~ ~~ ~~ def server_deployment_mode ( command , parser , cluster , cl_args ) : 
client_confs = cdefs . read_server_mode_cluster_definition ( cluster , cl_args ) 
~~ if not cl_args . get ( 'service_url' , None ) : 
~~~ config_file = cliconfig . get_cluster_config_file ( cluster ) 
~~ if 'config_property' in cl_args : 
~~~ cluster_role_env = ( cl_args [ 'cluster' ] , cl_args [ 'role' ] , cl_args [ 'environ' ] ) 
config . server_mode_cluster_role_env ( cluster_role_env , client_confs ) 
cluster_tuple = config . defaults_cluster_role_env ( cluster_role_env ) 
~~ new_cl_args = dict ( ) 
new_cl_args [ 'cluster' ] = cluster_tuple [ 0 ] 
new_cl_args [ 'role' ] = cluster_tuple [ 1 ] 
new_cl_args [ 'environ' ] = cluster_tuple [ 2 ] 
new_cl_args [ 'service_url' ] = client_confs [ cluster ] [ 'service_url' ] . rstrip ( '/' ) 
new_cl_args [ 'deploy_mode' ] = config . SERVER_MODE 
cl_args . update ( new_cl_args ) 
return cl_args 
~~ def direct_deployment_mode ( command , parser , cluster , cl_args ) : 
~~~ config_path = cl_args [ 'config_path' ] 
override_config_file = config . parse_override_config_and_write_file ( cl_args [ 'config_property' ] ) 
~~~ subparser = config . get_subparser ( parser , command ) 
print ( subparser . format_help ( ) ) 
return dict ( ) 
~~ if not cdefs . check_direct_mode_cluster_definition ( cluster , config_path ) : 
~~ config_path = config . get_heron_cluster_conf_dir ( cluster , config_path ) 
config . direct_mode_cluster_role_env ( cluster_role_env , config_path ) 
new_cl_args [ 'config_path' ] = config_path 
new_cl_args [ 'override_config_file' ] = override_config_file 
new_cl_args [ 'deploy_mode' ] = config . DIRECT_MODE 
~~ def extract_common_args ( command , parser , cl_args ) : 
~~~ cluster_role_env = cl_args . pop ( 'cluster/[role]/[env]' ) 
~~ ~~ new_cl_args = dict ( ) 
cluster_tuple = config . get_cluster_role_env ( cluster_role_env ) 
new_cl_args [ 'submit_user' ] = getpass . getuser ( ) 
~~ def execute ( handlers , local_commands ) : 
check_environment ( ) 
parser = create_parser ( handlers ) 
if len ( sys . argv [ 1 : ] ) == 0 : 
~~ sys . argv = config . insert_bool_values ( sys . argv ) 
~~~ args , unknown_args = parser . parse_known_args ( ) 
~~ except ValueError as ex : 
~~ command_line_args = vars ( args ) 
log . set_logging_level ( command_line_args ) 
command = command_line_args [ 'subcommand' ] 
is_local_command = command in local_commands 
if command == 'version' : 
~~~ results = run ( handlers , command , parser , command_line_args , unknown_args ) 
return 0 if result . is_successful ( results ) else 1 
~~ if not is_local_command : 
~~~ log . set_logging_level ( command_line_args ) 
command_line_args = extract_common_args ( command , parser , command_line_args ) 
command_line_args = deployment_mode ( command , parser , command_line_args ) 
if not command_line_args : 
~~ if command_line_args [ 'deploy_mode' ] == config . DIRECT_MODE and command != "version" : 
~~~ cleaned_up_files . append ( command_line_args [ 'override_config_file' ] ) 
atexit . register ( cleanup , cleaned_up_files ) 
start = time . time ( ) 
results = run ( handlers , command , parser , command_line_args , unknown_args ) 
if not is_local_command : 
~~~ result . render ( results ) 
~~ end = time . time ( ) 
~~~ sys . stdout . flush ( ) 
~~ return 0 if result . is_successful ( results ) else 1 
metric_names = self . get_required_arguments_metricnames ( ) 
interval = int ( self . get_argument ( constants . PARAM_INTERVAL , default = - 1 ) ) 
metrics = yield tornado . gen . Task ( 
self . getComponentMetrics , 
topology . tmaster , component , metric_names , instances , interval ) 
self . write_success_response ( metrics ) 
~~ ~~ def getComponentMetrics ( self , 
tmaster , 
componentName , 
metricNames , 
interval , 
metricRequest . component_name = componentName 
~~ ~~ for metricName in metricNames : 
~~ metricRequest . interval = interval 
ret [ "interval" ] = metricResponse . interval 
ret [ "component" ] = componentName 
ret [ "metrics" ] = { } 
value = im . value 
if metricname not in ret [ "metrics" ] : 
~~~ ret [ "metrics" ] [ metricname ] = { } 
~~ ret [ "metrics" ] [ metricname ] [ instance ] = value 
~~ ~~ raise tornado . gen . Return ( ret ) 
~~ def floorTimestamps ( self , start , end , timeline ) : 
for timestamp , value in timeline . items ( ) : 
~~~ ts = timestamp / 60 * 60 
if start <= ts <= end : 
~~~ ret [ ts ] = value 
~~ ~~ return ret 
~~ def setDefault ( self , constant , start , end ) : 
starttime = start / 60 * 60 
if starttime < start : 
~~~ starttime += 60 
~~ endtime = end / 60 * 60 
while starttime <= endtime : 
~~~ if starttime not in self . timeline or self . timeline [ starttime ] == 0 : 
~~~ self . timeline [ starttime ] = constant 
~~ starttime += 60 
~~ ~~ def initialize ( self , config , context ) : 
if SlidingWindowBolt . WINDOW_DURATION_SECS in config : 
~~~ self . window_duration = int ( config [ SlidingWindowBolt . WINDOW_DURATION_SECS ] ) 
~~ if SlidingWindowBolt . WINDOW_SLIDEINTERVAL_SECS in config : 
~~~ self . slide_interval = int ( config [ SlidingWindowBolt . WINDOW_SLIDEINTERVAL_SECS ] ) 
~~~ self . slide_interval = self . window_duration 
~~ if self . slide_interval > self . window_duration : 
~~ config [ api_constants . TOPOLOGY_TICK_TUPLE_FREQ_SECS ] = str ( self . slide_interval ) 
self . current_tuples = deque ( ) 
if hasattr ( self , 'saved_state' ) : 
~~~ if 'tuples' in self . saved_state : 
~~~ self . current_tuples = self . saved_state [ 'tuples' ] 
~~ ~~ ~~ def process ( self , tup ) : 
curtime = int ( time . time ( ) ) 
self . current_tuples . append ( ( tup , curtime ) ) 
self . _expire ( curtime ) 
~~ def process_tick ( self , tup ) : 
window_info = WindowContext ( curtime - self . window_duration , curtime ) 
tuple_batch = [ ] 
for ( tup , tm ) in self . current_tuples : 
~~~ tuple_batch . append ( tup ) 
~~ self . processWindow ( window_info , tuple_batch ) 
if TumblingWindowBolt . WINDOW_DURATION_SECS in config : 
~~~ self . window_duration = int ( config [ TumblingWindowBolt . WINDOW_DURATION_SECS ] ) 
~~ config [ api_constants . TOPOLOGY_TICK_TUPLE_FREQ_SECS ] = str ( self . window_duration ) 
~~ ~~ ~~ def process_tick ( self , tup ) : 
self . processWindow ( window_info , list ( self . current_tuples ) ) 
for tup in self . current_tuples : 
~~~ self . ack ( tup ) 
~~ self . current_tuples . clear ( ) 
self . set_header ( "Content-Disposition" , "attachment" ) 
if not utils . check_path ( path ) : 
~~ if path is None or not os . path . isfile ( path ) : 
~~ length = int ( 4 * 1024 * 1024 ) 
offset = int ( 0 ) 
~~~ data = utils . read_chunk ( path , offset = offset , length = length , escape_data = False ) 
if self . connection_closed or 'data' not in data or len ( data [ 'data' ] ) < length : 
~~ offset += length 
self . write ( data [ 'data' ] ) 
~~ if 'data' in data : 
~~~ self . write ( data [ 'data' ] ) 
~~ self . finish ( ) 
~~ def getStmgrsRegSummary ( self , tmaster , callback = None ) : 
~~ reg_request = tmaster_pb2 . StmgrsRegistrationSummaryRequest ( ) 
request_str = reg_request . SerializeToString ( ) 
url = "http://{0}:{1}/stmgrsregistrationsummary" . format ( host , port ) 
~~ reg_response = tmaster_pb2 . StmgrsRegistrationSummaryResponse ( ) 
reg_response . ParseFromString ( result . body ) 
for stmgr in reg_response . registered_stmgrs : 
~~~ ret [ stmgr ] = True 
~~ for stmgr in reg_response . absent_stmgrs : 
~~~ ret [ stmgr ] = False 
runtime_state = topology_info [ "runtime_state" ] 
runtime_state [ "topology_version" ] = topology_info [ "metadata" ] [ "release_version" ] 
reg_summary = yield tornado . gen . Task ( self . getStmgrsRegSummary , topology . tmaster ) 
for stmgr , reg in reg_summary . items ( ) : 
~~~ runtime_state [ "stmgrs" ] . setdefault ( stmgr , { } ) [ "is_registered" ] = reg 
~~ self . write_success_response ( runtime_state ) 
~~ ~~ def atomic_write_file ( path , content ) : 
tmp_file = get_tmp_filename ( ) 
with open ( tmp_file , 'w' ) as f : 
~~~ f . write ( content ) 
f . flush ( ) 
os . fsync ( f . fileno ( ) ) 
~~ os . rename ( tmp_file , path ) 
~~ def setup ( executor ) : 
def signal_handler ( signal_to_handle , frame ) : 
executor . stop_state_manager_watches ( ) 
sys . exit ( signal_to_handle ) 
~~ def cleanup ( ) : 
for pid in executor . processes_to_monitor . keys ( ) : 
~~~ os . kill ( pid , signal . SIGTERM ) 
~~ time . sleep ( 5 ) 
os . killpg ( 0 , signal . SIGTERM ) 
~~ shardid = executor . shard 
log . configure ( logfile = 'heron-executor-%s.stdout' % shardid ) 
pid = os . getpid ( ) 
sid = os . getsid ( pid ) 
if pid < > sid : 
signal . signal ( signal . SIGTERM , signal_handler ) 
atexit . register ( cleanup ) 
shell_env = os . environ . copy ( ) 
shell_env [ "PEX_ROOT" ] = os . path . join ( os . path . abspath ( '.' ) , ".pex" ) 
executor = HeronExecutor ( sys . argv , shell_env ) 
executor . initialize ( ) 
start ( executor ) 
~~ def init_from_parsed_args ( self , parsed_args ) : 
self . shard = parsed_args . shard 
self . topology_name = parsed_args . topology_name 
self . topology_id = parsed_args . topology_id 
self . topology_defn_file = parsed_args . topology_defn_file 
self . state_manager_connection = parsed_args . state_manager_connection 
self . state_manager_root = parsed_args . state_manager_root 
self . state_manager_config_file = parsed_args . state_manager_config_file 
self . tmaster_binary = parsed_args . tmaster_binary 
self . stmgr_binary = parsed_args . stmgr_binary 
self . metrics_manager_classpath = parsed_args . metrics_manager_classpath 
self . metricscache_manager_classpath = parsed_args . metricscache_manager_classpath 
self . instance_jvm_opts = base64 . b64decode ( parsed_args . instance_jvm_opts . lstrip ( \ ) . 
rstrip ( \ ) . replace ( '(61)' , '=' ) . replace ( '&equals;' , '=' ) ) 
self . classpath = parsed_args . classpath 
if is_docker_environment ( ) : 
~~~ self . master_host = os . environ . get ( 'HOST' ) if 'HOST' in os . environ else socket . gethostname ( ) 
~~~ self . master_host = socket . gethostname ( ) 
~~ self . master_port = parsed_args . master_port 
self . tmaster_controller_port = parsed_args . tmaster_controller_port 
self . tmaster_stats_port = parsed_args . tmaster_stats_port 
self . heron_internals_config_file = parsed_args . heron_internals_config_file 
self . override_config_file = parsed_args . override_config_file 
self . component_ram_map = map ( lambda x : { x . split ( ':' ) [ 0 ] : 
int ( x . split ( ':' ) [ 1 ] ) } , parsed_args . component_ram_map . split ( ',' ) ) 
self . component_ram_map = functools . reduce ( lambda x , y : dict ( x . items ( ) + y . items ( ) ) , self . component_ram_map ) 
self . component_jvm_opts = { } 
component_jvm_opts_in_json = base64 . b64decode ( parsed_args . component_jvm_opts . 
lstrip ( \ ) . rstrip ( \ ) . replace ( '(61)' , '=' ) . replace ( '&equals;' , '=' ) ) 
if component_jvm_opts_in_json != "" : 
~~~ for ( k , v ) in json . loads ( component_jvm_opts_in_json ) . items ( ) : 
~~~ self . component_jvm_opts [ base64 . b64decode ( k ) ] = base64 . b64decode ( v ) 
~~ ~~ self . pkg_type = parsed_args . pkg_type 
self . topology_binary_file = parsed_args . topology_binary_file 
self . heron_java_home = parsed_args . heron_java_home 
self . shell_port = parsed_args . shell_port 
self . heron_shell_binary = parsed_args . heron_shell_binary 
self . metrics_manager_port = parsed_args . metrics_manager_port 
self . metricscache_manager_master_port = parsed_args . metricscache_manager_master_port 
self . metricscache_manager_stats_port = parsed_args . metricscache_manager_stats_port 
self . cluster = parsed_args . cluster 
self . role = parsed_args . role 
self . environment = parsed_args . environment 
self . instance_classpath = parsed_args . instance_classpath 
self . metrics_sinks_config_file = parsed_args . metrics_sinks_config_file 
self . scheduler_classpath = parsed_args . scheduler_classpath 
self . scheduler_port = parsed_args . scheduler_port 
self . python_instance_binary = parsed_args . python_instance_binary 
self . cpp_instance_binary = parsed_args . cpp_instance_binary 
self . is_stateful_topology = ( parsed_args . is_stateful . lower ( ) == 'true' ) 
self . checkpoint_manager_classpath = parsed_args . checkpoint_manager_classpath 
self . checkpoint_manager_port = parsed_args . checkpoint_manager_port 
self . checkpoint_manager_ram = parsed_args . checkpoint_manager_ram 
self . stateful_config_file = parsed_args . stateful_config_file 
self . metricscache_manager_mode = parsed_args . metricscache_manager_mode if parsed_args . metricscache_manager_mode else "disabled" 
self . health_manager_mode = parsed_args . health_manager_mode 
self . health_manager_classpath = '%s:%s' % ( self . scheduler_classpath , parsed_args . health_manager_classpath ) 
self . jvm_remote_debugger_ports = parsed_args . jvm_remote_debugger_ports . split ( "," ) if parsed_args . jvm_remote_debugger_ports else None 
~~ def parse_args ( args ) : 
parser = argparse . ArgumentParser ( ) 
parser . add_argument ( "--shard" , type = int , required = True ) 
parser . add_argument ( "--topology-name" , required = True ) 
parser . add_argument ( "--topology-id" , required = True ) 
parser . add_argument ( "--topology-defn-file" , required = True ) 
parser . add_argument ( "--state-manager-connection" , required = True ) 
parser . add_argument ( "--state-manager-root" , required = True ) 
parser . add_argument ( "--state-manager-config-file" , required = True ) 
parser . add_argument ( "--tmaster-binary" , required = True ) 
parser . add_argument ( "--stmgr-binary" , required = True ) 
parser . add_argument ( "--metrics-manager-classpath" , required = True ) 
parser . add_argument ( "--instance-jvm-opts" , required = True ) 
parser . add_argument ( "--classpath" , required = True ) 
parser . add_argument ( "--master-port" , required = True ) 
parser . add_argument ( "--tmaster-controller-port" , required = True ) 
parser . add_argument ( "--tmaster-stats-port" , required = True ) 
parser . add_argument ( "--heron-internals-config-file" , required = True ) 
parser . add_argument ( "--override-config-file" , required = True ) 
parser . add_argument ( "--component-ram-map" , required = True ) 
parser . add_argument ( "--component-jvm-opts" , required = True ) 
parser . add_argument ( "--pkg-type" , required = True ) 
parser . add_argument ( "--topology-binary-file" , required = True ) 
parser . add_argument ( "--heron-java-home" , required = True ) 
parser . add_argument ( "--shell-port" , required = True ) 
parser . add_argument ( "--heron-shell-binary" , required = True ) 
parser . add_argument ( "--metrics-manager-port" , required = True ) 
parser . add_argument ( "--cluster" , required = True ) 
parser . add_argument ( "--role" , required = True ) 
parser . add_argument ( "--environment" , required = True ) 
parser . add_argument ( "--instance-classpath" , required = True ) 
parser . add_argument ( "--metrics-sinks-config-file" , required = True ) 
parser . add_argument ( "--scheduler-classpath" , required = True ) 
parser . add_argument ( "--scheduler-port" , required = True ) 
parser . add_argument ( "--python-instance-binary" , required = True ) 
parser . add_argument ( "--cpp-instance-binary" , required = True ) 
parser . add_argument ( "--metricscache-manager-classpath" , required = True ) 
parser . add_argument ( "--metricscache-manager-master-port" , required = True ) 
parser . add_argument ( "--metricscache-manager-stats-port" , required = True ) 
parser . add_argument ( "--metricscache-manager-mode" , required = False ) 
parser . add_argument ( "--is-stateful" , required = True ) 
parser . add_argument ( "--checkpoint-manager-classpath" , required = True ) 
parser . add_argument ( "--checkpoint-manager-port" , required = True ) 
parser . add_argument ( "--checkpoint-manager-ram" , type = long , required = True ) 
parser . add_argument ( "--stateful-config-file" , required = True ) 
parser . add_argument ( "--health-manager-mode" , required = True ) 
parser . add_argument ( "--health-manager-classpath" , required = True ) 
parser . add_argument ( "--jvm-remote-debugger-ports" , required = False , 
parsed_args , unknown_args = parser . parse_known_args ( args [ 1 : ] ) 
if unknown_args : 
parser . print_help ( ) 
~~ return parsed_args 
self . run_command_or_exit ( create_folders ) 
self . run_command_or_exit ( chmod_logs_dir ) 
chmod_x_binaries = [ self . tmaster_binary , self . stmgr_binary , self . heron_shell_binary ] 
for binary in chmod_x_binaries : 
~~~ stat_result = os . stat ( binary ) [ stat . ST_MODE ] 
if not stat_result & stat . S_IXOTH : 
self . run_command_or_exit ( chmod_binary ) 
~~ ~~ log_pid_for_process ( get_heron_executor_process_name ( self . shard ) , os . getpid ( ) ) 
~~ def _get_metricsmgr_cmd ( self , metricsManagerId , sink_config_file , port ) : 
metricsmgr_main_class = 'org.apache.heron.metricsmgr.MetricsManager' 
metricsmgr_cmd = [ os . path . join ( self . heron_java_home , 'bin/java' ) , 
'-Xmx1024M' , 
'-XX:+PrintCommandLineFlags' , 
'-verbosegc' , 
'-XX:+PrintGCDetails' , 
'-XX:+PrintGCTimeStamps' , 
'-XX:+PrintGCDateStamps' , 
'-XX:+PrintGCCause' , 
'-XX:+UseGCLogFileRotation' , 
'-XX:NumberOfGCLogFiles=5' , 
'-XX:GCLogFileSize=100M' , 
'-XX:+PrintPromotionFailure' , 
'-XX:+PrintTenuringDistribution' , 
'-XX:+PrintHeapAtGC' , 
'-XX:+HeapDumpOnOutOfMemoryError' , 
'-XX:+UseConcMarkSweepGC' , 
'-Xloggc:log-files/gc.metricsmgr.log' , 
'-Djava.net.preferIPv4Stack=true' , 
'-cp' , 
self . metrics_manager_classpath , 
metricsmgr_main_class , 
'--id=' + metricsManagerId , 
'--port=' + str ( port ) , 
'--topology=' + self . topology_name , 
'--cluster=' + self . cluster , 
'--role=' + self . role , 
'--environment=' + self . environment , 
'--topology-id=' + self . topology_id , 
'--system-config-file=' + self . heron_internals_config_file , 
'--override-config-file=' + self . override_config_file , 
'--sink-config-file=' + sink_config_file ] 
return Command ( metricsmgr_cmd , self . shell_env ) 
~~ def _get_metrics_cache_cmd ( self ) : 
metricscachemgr_main_class = 'org.apache.heron.metricscachemgr.MetricsCacheManager' 
metricscachemgr_cmd = [ os . path . join ( self . heron_java_home , 'bin/java' ) , 
'-Xloggc:log-files/gc.metricscache.log' , 
self . metricscache_manager_classpath , 
metricscachemgr_main_class , 
"--metricscache_id" , 'metricscache-0' , 
"--master_port" , self . metricscache_manager_master_port , 
"--stats_port" , self . metricscache_manager_stats_port , 
"--topology_name" , self . topology_name , 
"--topology_id" , self . topology_id , 
"--system_config_file" , self . heron_internals_config_file , 
"--override_config_file" , self . override_config_file , 
"--sink_config_file" , self . metrics_sinks_config_file , 
"--cluster" , self . cluster , 
"--role" , self . role , 
"--environment" , self . environment ] 
return Command ( metricscachemgr_cmd , self . shell_env ) 
~~ def _get_healthmgr_cmd ( self ) : 
healthmgr_main_class = 'org.apache.heron.healthmgr.HealthManager' 
healthmgr_cmd = [ os . path . join ( self . heron_java_home , 'bin/java' ) , 
'-Xloggc:log-files/gc.healthmgr.log' , 
'-cp' , self . health_manager_classpath , 
healthmgr_main_class , 
"--environment" , self . environment , 
"--metricsmgr_port" , self . metrics_manager_port ] 
return Command ( healthmgr_cmd , self . shell_env ) 
~~ def _get_tmaster_processes ( self ) : 
tmaster_cmd_lst = [ 
self . tmaster_binary , 
'--topology_name=%s' % self . topology_name , 
'--topology_id=%s' % self . topology_id , 
'--zkhostportlist=%s' % self . state_manager_connection , 
'--zkroot=%s' % self . state_manager_root , 
'--myhost=%s' % self . master_host , 
'--master_port=%s' % str ( self . master_port ) , 
'--controller_port=%s' % str ( self . tmaster_controller_port ) , 
'--stats_port=%s' % str ( self . tmaster_stats_port ) , 
'--config_file=%s' % self . heron_internals_config_file , 
'--override_config_file=%s' % self . override_config_file , 
'--metrics_sinks_yaml=%s' % self . metrics_sinks_config_file , 
'--metricsmgr_port=%s' % str ( self . metrics_manager_port ) , 
'--ckptmgr_port=%s' % str ( self . checkpoint_manager_port ) ] 
tmaster_env = self . shell_env . copy ( ) if self . shell_env is not None else { } 
tmaster_cmd = Command ( tmaster_cmd_lst , tmaster_env ) 
if os . environ . get ( 'ENABLE_HEAPCHECK' ) is not None : 
~~~ tmaster_cmd . env . update ( { 
'LD_PRELOAD' : "/usr/lib/libtcmalloc.so" , 
'HEAPCHECK' : "normal" 
~~ retval [ "heron-tmaster" ] = tmaster_cmd 
if self . metricscache_manager_mode . lower ( ) != "disabled" : 
~~~ retval [ "heron-metricscache" ] = self . _get_metrics_cache_cmd ( ) 
~~ if self . health_manager_mode . lower ( ) != "disabled" : 
~~~ retval [ "heron-healthmgr" ] = self . _get_healthmgr_cmd ( ) 
~~ retval [ self . metricsmgr_ids [ 0 ] ] = self . _get_metricsmgr_cmd ( 
self . metricsmgr_ids [ 0 ] , 
self . metrics_sinks_config_file , 
self . metrics_manager_port ) 
if self . is_stateful_topology : 
~~~ retval . update ( self . _get_ckptmgr_process ( ) ) 
~~ def _get_streaming_processes ( self ) : 
instance_plans = self . _get_instance_plans ( self . packing_plan , self . shard ) 
instance_info = [ ] 
for instance_plan in instance_plans : 
~~~ global_task_id = instance_plan . task_id 
component_index = instance_plan . component_index 
component_name = instance_plan . component_name 
instance_id = "container_%s_%s_%d" % ( str ( self . shard ) , component_name , global_task_id ) 
instance_info . append ( ( instance_id , component_name , global_task_id , component_index ) ) 
~~ stmgr_cmd_lst = [ 
self . stmgr_binary , 
'--topologydefn_file=%s' % self . topology_defn_file , 
'--stmgr_id=%s' % self . stmgr_ids [ self . shard ] , 
'--instance_ids=%s' % ',' . join ( map ( lambda x : x [ 0 ] , instance_info ) ) , 
'--data_port=%s' % str ( self . master_port ) , 
'--local_data_port=%s' % str ( self . tmaster_controller_port ) , 
'--shell_port=%s' % str ( self . shell_port ) , 
'--ckptmgr_port=%s' % str ( self . checkpoint_manager_port ) , 
'--ckptmgr_id=%s' % self . ckptmgr_ids [ self . shard ] , 
'--metricscachemgr_mode=%s' % self . metricscache_manager_mode . lower ( ) ] 
stmgr_env = self . shell_env . copy ( ) if self . shell_env is not None else { } 
stmgr_cmd = Command ( stmgr_cmd_lst , stmgr_env ) 
~~~ stmgr_cmd . env . update ( { 
~~ retval [ self . stmgr_ids [ self . shard ] ] = stmgr_cmd 
retval [ self . metricsmgr_ids [ self . shard ] ] = self . _get_metricsmgr_cmd ( 
self . metricsmgr_ids [ self . shard ] , 
self . metrics_manager_port 
~~ if self . pkg_type == 'jar' or self . pkg_type == 'tar' : 
~~~ retval . update ( self . _get_java_instance_cmd ( instance_info ) ) 
~~ elif self . pkg_type == 'pex' : 
~~~ retval . update ( self . _get_python_instance_cmd ( instance_info ) ) 
~~ elif self . pkg_type == 'so' : 
~~~ retval . update ( self . _get_cpp_instance_cmd ( instance_info ) ) 
~~ elif self . pkg_type == 'dylib' : 
~~ def _get_ckptmgr_process ( self ) : 
ckptmgr_main_class = 'org.apache.heron.ckptmgr.CheckpointManager' 
ckptmgr_ram_mb = self . checkpoint_manager_ram / ( 1024 * 1024 ) 
ckptmgr_cmd = [ os . path . join ( self . heron_java_home , "bin/java" ) , 
'-Xms%dM' % ckptmgr_ram_mb , 
'-Xmx%dM' % ckptmgr_ram_mb , 
'-Xloggc:log-files/gc.ckptmgr.log' , 
self . checkpoint_manager_classpath , 
ckptmgr_main_class , 
'-t' + self . topology_name , 
'-i' + self . topology_id , 
'-c' + self . ckptmgr_ids [ self . shard ] , 
'-p' + self . checkpoint_manager_port , 
'-f' + self . stateful_config_file , 
'-o' + self . override_config_file , 
'-g' + self . heron_internals_config_file ] 
retval [ self . ckptmgr_ids [ self . shard ] ] = Command ( ckptmgr_cmd , self . shell_env ) 
return retval 
~~ def _get_instance_plans ( self , packing_plan , container_id ) : 
this_container_plan = None 
for container_plan in packing_plan . container_plans : 
~~~ if container_plan . id == container_id : 
~~~ this_container_plan = container_plan 
~~ ~~ if this_container_plan is None : 
~~ return this_container_plan . instance_plans 
~~ def _get_heron_support_processes ( self ) : 
retval [ self . heron_shell_ids [ self . shard ] ] = Command ( [ 
'%s' % self . heron_shell_binary , 
'--port=%s' % self . shell_port , 
'--log_file_prefix=%s/heron-shell-%s.log' % ( self . log_dir , self . shard ) , 
'--secret=%s' % self . topology_id ] , self . shell_env ) 
~~ def _wait_process_std_out_err ( self , name , process ) : 
proc . stream_process_stdout ( process , stdout_log_fn ( name ) ) 
process . wait ( ) 
~~ def _start_processes ( self , commands ) : 
processes_to_monitor = { } 
for ( name , command ) in commands . items ( ) : 
~~~ p = self . _run_process ( name , command ) 
processes_to_monitor [ p . pid ] = ProcessInfo ( p , name , command ) 
log_pid_for_process ( name , p . pid ) 
~~ with self . process_lock : 
~~~ self . processes_to_monitor . update ( processes_to_monitor ) 
~~ ~~ def start_process_monitor ( self ) : 
~~~ if len ( self . processes_to_monitor ) > 0 : 
~~~ ( pid , status ) = os . wait ( ) 
with self . process_lock : 
~~~ if pid in self . processes_to_monitor . keys ( ) : 
~~~ old_process_info = self . processes_to_monitor [ pid ] 
name = old_process_info . name 
command = old_process_info . command 
self . _wait_process_std_out_err ( name , old_process_info . process ) 
if os . path . isfile ( "core.%d" % pid ) : 
~~ if old_process_info . attempts >= self . max_runs : 
~~ time . sleep ( self . interval_between_runs ) 
p = self . _run_process ( name , command ) 
del self . processes_to_monitor [ pid ] 
self . processes_to_monitor [ p . pid ] = ProcessInfo ( p , name , command , old_process_info . attempts + 1 ) 
~~ ~~ ~~ ~~ ~~ def get_commands_to_run ( self ) : 
if len ( self . packing_plan . container_plans ) == 0 : 
~~ if self . _get_instance_plans ( self . packing_plan , self . shard ) is None and self . shard != 0 : 
~~~ retval = { } 
retval [ 'heron-shell' ] = Command ( [ 
~~ if self . shard == 0 : 
~~~ commands = self . _get_tmaster_processes ( ) 
~~~ self . _untar_if_needed ( ) 
commands = self . _get_streaming_processes ( ) 
~~ commands . update ( self . _get_heron_support_processes ( ) ) 
return commands 
~~ def get_command_changes ( self , current_commands , updated_commands ) : 
commands_to_kill = { } 
commands_to_keep = { } 
commands_to_start = { } 
for current_name , current_command in current_commands . items ( ) : 
~~~ if current_name in updated_commands . keys ( ) and current_command == updated_commands [ current_name ] and not current_name . startswith ( 'stmgr-' ) : 
~~~ commands_to_keep [ current_name ] = current_command 
~~~ commands_to_kill [ current_name ] = current_command 
~~ ~~ for updated_name , updated_command in updated_commands . items ( ) : 
~~~ if updated_name not in commands_to_keep . keys ( ) : 
~~~ commands_to_start [ updated_name ] = updated_command 
~~ ~~ return commands_to_kill , commands_to_keep , commands_to_start 
~~ def launch ( self ) : 
~~~ current_commands = dict ( map ( ( lambda process : ( process . name , process . command ) ) , 
self . processes_to_monitor . values ( ) ) ) 
updated_commands = self . get_commands_to_run ( ) 
commands_to_kill , commands_to_keep , commands_to_start = self . get_command_changes ( current_commands , updated_commands ) 
self . _kill_processes ( commands_to_kill ) 
self . _start_processes ( commands_to_start ) 
( len ( commands_to_kill ) , len ( commands_to_keep ) , 
len ( commands_to_start ) , len ( self . processes_to_monitor ) ) ) 
~~ ~~ def start_state_manager_watches ( self ) : 
statemgr_config = StateMgrConfig ( ) 
statemgr_config . set_state_locations ( configloader . load_state_manager_locations ( 
self . cluster , state_manager_config_file = self . state_manager_config_file , 
overrides = { "heron.statemgr.connection.string" : self . state_manager_connection } ) ) 
~~~ self . state_managers = statemanagerfactory . get_all_state_managers ( statemgr_config ) 
for state_manager in self . state_managers : 
~~ def on_packing_plan_watch ( state_manager , new_packing_plan ) : 
( self . shard , str ( self . packing_plan ) , str ( new_packing_plan ) ) ) 
if self . packing_plan != new_packing_plan : 
% self . shard ) 
self . update_packing_plan ( new_packing_plan ) 
self . launch ( ) 
~~~ Log . info ( 
~~ ~~ for state_manager in self . state_managers : 
~~~ onPackingPlanWatch = functools . partial ( on_packing_plan_watch , state_manager ) 
state_manager . get_packing_plan ( self . topology_name , onPackingPlanWatch ) 
str ( state_manager ) ) 
~~ ~~ def run ( self , name , config , builder ) : 
if not isinstance ( name , str ) : 
~~ if not isinstance ( config , Config ) : 
~~ if not isinstance ( builder , Builder ) : 
~~ bldr = TopologyBuilder ( name = name ) 
builder . build ( bldr ) 
bldr . set_config ( config . _api_config ) 
bldr . build_and_submit ( ) 
~~ def _modules_to_main ( modList ) : 
if not modList : 
~~ main = sys . modules [ '__main__' ] 
for modname in modList : 
~~~ if isinstance ( modname , str ) : 
~~~ mod = __import__ ( modname ) 
~~~ sys . stderr . write ( 
print_exec ( sys . stderr ) 
~~~ setattr ( main , mod . __name__ , mod ) 
~~ ~~ ~~ ~~ def _fill_function ( func , globalsn , defaults , dictn , module ) : 
func . __globals__ . update ( globalsn ) 
func . __defaults__ = defaults 
func . __dict__ = dictn 
func . __module__ = module 
return func 
~~ def _make_skel_func ( code , closures , base_globals = None ) : 
closure = _reconstruct_closure ( closures ) if closures else None 
if base_globals is None : 
~~~ base_globals = { } 
~~ base_globals [ '__builtins__' ] = __builtins__ 
return types . FunctionType ( code , base_globals , None , None , closure ) 
~~ def _load_class ( cls , d ) : 
for k , v in d . items ( ) : 
~~~ if isinstance ( k , tuple ) : 
~~~ typ , k = k 
if typ == 'property' : 
~~~ v = property ( * v ) 
~~ elif typ == 'staticmethod' : 
~~ elif typ == 'classmethod' : 
~~~ v = classmethod ( v ) 
~~ ~~ setattr ( cls , k , v ) 
~~ return cls 
~~ def save_module ( self , obj ) : 
self . modules . add ( obj ) 
self . save_reduce ( subimport , ( obj . __name__ , ) , obj = obj ) 
~~ def save_function ( self , obj , name = None ) : 
write = self . write 
~~~ name = obj . __name__ 
~~~ modname = pickle . whichmodule ( obj , name ) 
~~~ modname = None 
~~~ themodule = sys . modules [ modname ] 
~~~ modname = '__main__' 
~~ if modname == '__main__' : 
~~~ themodule = None 
~~ if themodule : 
~~~ self . modules . add ( themodule ) 
if getattr ( themodule , name , None ) is obj : 
~~~ return self . save_global ( obj , name ) 
~~ ~~ if islambda ( obj ) or obj . __code__ . co_filename == '<stdin>' or themodule is None : 
~~~ self . save_function_tuple ( obj ) 
~~~ klass = getattr ( themodule , name , None ) 
if klass is None or klass is not obj : 
~~ ~~ if obj . __dict__ : 
~~~ self . save ( _restore_attr ) 
write ( pickle . MARK + pickle . GLOBAL + modname + '\\n' + name + '\\n' ) 
self . memoize ( obj ) 
self . save ( obj . __dict__ ) 
write ( pickle . TUPLE + pickle . REDUCE ) 
~~~ write ( pickle . GLOBAL + modname + '\\n' + name + '\\n' ) 
~~ ~~ def save_function_tuple ( self , func ) : 
save = self . save 
code , f_globals , defaults , closure , dct , base_globals = self . extract_func_data ( func ) 
save ( _make_skel_func ) 
save ( ( code , closure , base_globals ) ) 
write ( pickle . REDUCE ) 
self . memoize ( func ) 
save ( f_globals ) 
save ( defaults ) 
save ( dct ) 
save ( func . __module__ ) 
write ( pickle . TUPLE ) 
~~ if not hasattr ( obj , 'name' ) or not hasattr ( obj , 'mode' ) : 
~~ if obj is sys . stdout : 
~~~ return self . save_reduce ( getattr , ( sys , 'stdout' ) , obj = obj ) 
~~ if obj is sys . stderr : 
~~~ return self . save_reduce ( getattr , ( sys , 'stderr' ) , obj = obj ) 
~~ if obj is sys . stdin : 
~~ if hasattr ( obj , 'isatty' ) and obj . isatty ( ) : 
~~ if 'r' not in obj . mode : 
~~ name = obj . name 
~~~ fsize = os . stat ( name ) . st_size 
~~ if obj . closed : 
~~~ retval = pystringIO . StringIO ( "" ) 
retval . close ( ) 
~~~ tmpfile = file ( name ) 
tst = tmpfile . read ( 1 ) 
~~ except IOError : 
~~ tmpfile . close ( ) 
if tst != '' : 
~~~ raise pickle . PicklingError ( 
contents = tmpfile . read ( ) 
tmpfile . close ( ) 
~~ retval = pystringIO . StringIO ( contents ) 
curloc = obj . tell ( ) 
retval . seek ( curloc ) 
~~ retval . name = name 
self . save ( retval ) 
~~ def tail ( filename , n ) : 
size = os . path . getsize ( filename ) 
with open ( filename , "rb" ) as f : 
~~~ fm = mmap . mmap ( f . fileno ( ) , 0 , mmap . MAP_SHARED , mmap . PROT_READ ) 
~~~ for i in xrange ( size - 1 , - 1 , - 1 ) : 
~~~ if fm [ i ] == '\\n' : 
~~~ n -= 1 
if n == - 1 : 
~~ ~~ ~~ return fm [ i + 1 if i else 0 : ] . splitlines ( ) 
~~~ fm . close ( ) 
~~ ~~ ~~ def get_serializer ( context ) : 
cluster_config = context . get_cluster_config ( ) 
serializer_clsname = cluster_config . get ( constants . TOPOLOGY_SERIALIZER_CLASSNAME , None ) 
if serializer_clsname is None : 
~~~ return PythonSerializer ( ) 
~~~ topo_pex_path = context . get_topology_pex_path ( ) 
pex_loader . load_pex ( topo_pex_path ) 
serializer_cls = pex_loader . import_and_get_class ( topo_pex_path , serializer_clsname ) 
serializer = serializer_cls ( ) 
return serializer 
% ( serializer_clsname , str ( e ) ) ) 
~~ ~~ ~~ def _run_once ( self ) : 
~~~ self . do_wait ( ) 
self . _execute_wakeup_tasks ( ) 
self . _trigger_timers ( ) 
Log . error ( traceback . format_exc ( ) ) 
self . should_exit = True 
~~ ~~ def register_timer_task_in_sec ( self , task , second ) : 
second_in_float = float ( second ) 
expiration = time . time ( ) + second_in_float 
heappush ( self . timer_tasks , ( expiration , task ) ) 
~~ def _get_next_timeout_interval ( self ) : 
if len ( self . timer_tasks ) == 0 : 
~~~ return sys . maxsize 
~~~ next_timeout_interval = self . timer_tasks [ 0 ] [ 0 ] - time . time ( ) 
return next_timeout_interval 
~~ ~~ def _execute_wakeup_tasks ( self ) : 
size = len ( self . wakeup_tasks ) 
~~~ self . wakeup_tasks [ i ] ( ) 
~~ ~~ def _trigger_timers ( self ) : 
current = time . time ( ) 
while len ( self . timer_tasks ) > 0 and ( self . timer_tasks [ 0 ] [ 0 ] - current <= 0 ) : 
~~~ task = heappop ( self . timer_tasks ) [ 1 ] 
task ( ) 
~~ ~~ def post ( self ) : 
def status_finish ( ret ) : 
~~~ self . set_status ( ret ) 
~~ def kill_parent ( ) : 
~~~ status_finish ( 200 ) 
os . killpg ( os . getppid ( ) , signal . SIGTERM ) 
~~ logger = logging . getLogger ( __file__ ) 
data = dict ( urlparse . parse_qsl ( self . request . body ) ) 
sharedSecret = data . get ( 'secret' ) 
if sharedSecret != options . secret : 
~~~ status_finish ( 403 ) 
~~ instanceId = data . get ( 'instance_id_to_restart' ) 
if instanceId : 
~~~ filepath = instanceId + '.pid' 
~~~ kill_parent ( ) 
~~~ fh = open ( filepath ) 
firstLine = int ( fh . readline ( ) ) 
fh . close ( ) 
os . kill ( firstLine , signal . SIGTERM ) 
status_finish ( 200 ) 
status_finish ( 422 ) 
~~ ~~ def execute_query ( self , tmaster , query_string , start , end ) : 
if not tmaster : 
~~ self . tmaster = tmaster 
root = self . parse_query_string ( query_string ) 
metrics = yield root . execute ( self . tracker , self . tmaster , start , end ) 
raise tornado . gen . Return ( metrics ) 
~~ def find_closing_braces ( self , query ) : 
if query [ 0 ] != '(' : 
~~ num_open_braces = 0 
for i in range ( len ( query ) ) : 
~~~ c = query [ i ] 
if c == '(' : 
~~~ num_open_braces += 1 
~~ elif c == ')' : 
~~~ num_open_braces -= 1 
~~ if num_open_braces == 0 : 
~~ def get_sub_parts ( self , query ) : 
num_open_braces = 0 
delimiter = ',' 
last_starting_index = 0 
~~~ if query [ i ] == '(' : 
~~ elif query [ i ] == ')' : 
~~ elif query [ i ] == delimiter and num_open_braces == 0 : 
~~~ parts . append ( query [ last_starting_index : i ] . strip ( ) ) 
last_starting_index = i + 1 
~~ ~~ parts . append ( query [ last_starting_index : ] . strip ( ) ) 
return parts 
~~ def parse_query_string ( self , query ) : 
if not query : 
~~ if query [ 0 ] == '(' : 
~~~ index = self . find_closing_braces ( query ) 
if index != len ( query ) - 1 : 
~~~ return self . parse_query_string ( query [ 1 : - 1 ] ) 
~~ ~~ start_index = query . find ( "(" ) 
if start_index < 0 : 
~~~ constant = float ( query ) 
return constant 
~~ ~~ token = query [ : start_index ] 
if token not in self . operators : 
~~ rest_of_the_query = query [ start_index : ] 
braces_end_index = self . find_closing_braces ( rest_of_the_query ) 
if braces_end_index != len ( rest_of_the_query ) - 1 : 
~~ parts = self . get_sub_parts ( rest_of_the_query [ 1 : - 1 ] ) 
if token == "TS" : 
~~~ return self . operators [ token ] ( parts ) 
~~ children = [ ] 
for part in parts : 
~~~ children . append ( self . parse_query_string ( part ) ) 
~~ node = self . operators [ token ] ( children ) 
return node 
start_time = self . get_argument_starttime ( ) 
end_time = self . get_argument_endtime ( ) 
self . validateInterval ( start_time , end_time ) 
query = self . get_argument_query ( ) 
metrics = yield tornado . gen . Task ( self . executeMetricsQuery , 
topology . tmaster , query , int ( start_time ) , int ( end_time ) ) 
~~ ~~ def executeMetricsQuery ( self , tmaster , queryString , start_time , end_time , callback = None ) : 
query = Query ( self . tracker ) 
metrics = yield query . execute_query ( tmaster , queryString , start_time , end_time ) 
ret [ "timeline" ] = [ ] 
~~~ tl = { 
"data" : metric . timeline 
if metric . instance : 
~~~ tl [ "instance" ] = metric . instance 
~~ ret [ "timeline" ] . append ( tl ) 
'help' , 
'help-command' , 
default = 'help' , 
parser . set_defaults ( subcommand = 'help' ) 
return SimpleResult ( Status . Ok ) 
return SimpleResult ( Status . InvocationError ) 
~~ ~~ def emit ( self , tup , stream = Stream . DEFAULT_STREAM_ID , 
anchors = None , direct_task = None , need_task_ids = False ) : 
~~ ~~ if anchors is not None : 
~~~ merged_roots = set ( ) 
for tup in [ t for t in anchors if isinstance ( t , HeronTuple ) and t . roots is not None ] : 
~~~ merged_roots . update ( tup . roots ) 
~~ for rt in merged_roots : 
~~~ to_add = data_tuple . roots . add ( ) 
to_add . CopyFrom ( rt ) 
self . bolt_metrics . serialize_data_tuple ( stream , serialize_latency_ns ) 
super ( BoltInstance , self ) . admit_data_tuple ( stream_id = stream , data_tuple = data_tuple , 
self . bolt_metrics . update_emit_count ( stream ) 
~~ ~~ def process_incoming_tuples ( self ) : 
if self . output_helper . is_out_queue_available ( ) : 
~~~ self . _read_tuples_and_execute ( ) 
self . output_helper . send_out_tuples ( ) 
~~~ self . bolt_metrics . update_out_queue_full_count ( ) 
~~ ~~ def ack ( self , tup ) : 
if not isinstance ( tup , HeronTuple ) : 
~~ if self . acking_enabled : 
~~~ ack_tuple = tuple_pb2 . AckTuple ( ) 
ack_tuple . ackedtuple = int ( tup . id ) 
tuple_size_in_bytes = 0 
for rt in tup . roots : 
~~~ to_add = ack_tuple . roots . add ( ) 
tuple_size_in_bytes += rt . ByteSize ( ) 
~~ super ( BoltInstance , self ) . admit_control_tuple ( ack_tuple , tuple_size_in_bytes , True ) 
~~ process_latency_ns = ( time . time ( ) - tup . creation_time ) * system_constants . SEC_TO_NS 
self . pplan_helper . context . invoke_hook_bolt_ack ( tup , process_latency_ns ) 
self . bolt_metrics . acked_tuple ( tup . stream , tup . component , process_latency_ns ) 
~~ def fail ( self , tup ) : 
~~~ fail_tuple = tuple_pb2 . AckTuple ( ) 
fail_tuple . ackedtuple = int ( tup . id ) 
~~~ to_add = fail_tuple . roots . add ( ) 
~~ super ( BoltInstance , self ) . admit_control_tuple ( fail_tuple , tuple_size_in_bytes , False ) 
~~ fail_latency_ns = ( time . time ( ) - tup . creation_time ) * system_constants . SEC_TO_NS 
self . pplan_helper . context . invoke_hook_bolt_fail ( tup , fail_latency_ns ) 
self . bolt_metrics . failed_tuple ( tup . stream , tup . component , fail_latency_ns ) 
~~ def execute ( handlers ) : 
'standalone' , 
add_help = True 
cli_args . add_titles ( parser ) 
parser_action = parser . add_subparsers ( ) 
parser_cluster = parser_action . add_parser ( 
Action . CLUSTER , 
add_help = True , 
formatter_class = argparse . RawTextHelpFormatter , 
parser_cluster . set_defaults ( action = Action . CLUSTER ) 
parser_set = parser_action . add_parser ( 
Action . SET , 
formatter_class = argparse . RawTextHelpFormatter 
parser_set . set_defaults ( action = Action . SET ) 
parser_template = parser_action . add_parser ( 
Action . TEMPLATE , 
parser_template . set_defaults ( action = Action . TEMPLATE ) 
parser_cluster . add_argument ( 
TYPE , 
choices = { Cluster . START , Cluster . STOP } , 
parser_template . add_argument ( 
choices = { "configs" } , 
parser_get = parser_action . add_parser ( 
Action . GET , 
parser_get . set_defaults ( action = Action . GET ) 
parser_get . add_argument ( 
choices = { Get . SERVICE_URL , Get . HERON_TRACKER_URL , Get . HERON_UI_URL } , 
parser_info = parser_action . add_parser ( 
Action . INFO , 
parser_info . set_defaults ( action = Action . INFO ) 
add_additional_args ( [ parser_set , parser_cluster , parser_template , parser_get , parser_info ] ) 
parser . set_defaults ( subcommand = 'standalone' ) 
action = cl_args [ "action" ] 
if action == Action . SET : 
~~~ call_editor ( get_inventory_file ( cl_args ) ) 
update_config_files ( cl_args ) 
~~ elif action == Action . CLUSTER : 
~~~ action_type = cl_args [ "type" ] 
if action_type == Cluster . START : 
~~~ start_cluster ( cl_args ) 
~~ elif action_type == Cluster . STOP : 
~~~ stop_cluster ( cl_args ) 
~~ ~~ elif action == Action . TEMPLATE : 
~~~ update_config_files ( cl_args ) 
~~ elif action == Action . GET : 
if action_type == Get . SERVICE_URL : 
~~~ print get_service_url ( cl_args ) 
~~ elif action_type == Get . HERON_UI_URL : 
~~~ print get_heron_ui_url ( cl_args ) 
~~ elif action_type == Get . HERON_TRACKER_URL : 
~~~ print get_heron_tracker_url ( cl_args ) 
~~ ~~ elif action == Action . INFO : 
~~~ print_cluster_info ( cl_args ) 
~~ def template_slave_hcl ( cl_args , masters ) : 
slave_config_template = "%s/standalone/templates/slave.template.hcl" % cl_args [ "config_path" ] 
slave_config_actual = "%s/standalone/resources/slave.hcl" % cl_args [ "config_path" ] 
masters_in_quotes = [ \ % master for master in masters ] 
template_file ( slave_config_template , slave_config_actual , 
~~ def template_scheduler_yaml ( cl_args , masters ) : 
single_master = masters [ 0 ] 
scheduler_config_actual = "%s/standalone/scheduler.yaml" % cl_args [ "config_path" ] 
scheduler_config_template = "%s/standalone/templates/scheduler.template.yaml" % cl_args [ "config_path" ] 
template_file ( scheduler_config_template , scheduler_config_actual , 
{ "<scheduler_uri>" : "http://%s:4646" % single_master } ) 
~~ def template_uploader_yaml ( cl_args , masters ) : 
uploader_config_template = "%s/standalone/templates/uploader.template.yaml" % cl_args [ "config_path" ] 
uploader_config_actual = "%s/standalone/uploader.yaml" % cl_args [ "config_path" ] 
template_file ( uploader_config_template , uploader_config_actual , 
{ "<http_uploader_uri>" : "http://%s:9000/api/v1/file/upload" % single_master } ) 
~~ def template_apiserver_hcl ( cl_args , masters , zookeepers ) : 
apiserver_config_template = "%s/standalone/templates/apiserver.template.hcl" % cl_args [ "config_path" ] 
apiserver_config_actual = "%s/standalone/resources/apiserver.hcl" % cl_args [ "config_path" ] 
replacements = { 
"<heron_apiserver_hostname>" : \ % get_hostname ( single_master , cl_args ) , 
"<heron_apiserver_executable>" : \ 
% config . get_heron_bin_dir ( ) 
if is_self ( single_master ) 
else \ 
% get_remote_home ( single_master , cl_args ) , 
"<zookeeper_host:zookeeper_port>" : "," . join ( 
[ '%s' % zk if ":" in zk else '%s:2181' % zk for zk in zookeepers ] ) , 
"<scheduler_uri>" : "http://%s:4646" % single_master 
template_file ( apiserver_config_template , apiserver_config_actual , replacements ) 
~~ def template_statemgr_yaml ( cl_args , zookeepers ) : 
statemgr_config_file_template = "%s/standalone/templates/statemgr.template.yaml" % cl_args [ "config_path" ] 
statemgr_config_file_actual = "%s/standalone/statemgr.yaml" % cl_args [ "config_path" ] 
template_file ( statemgr_config_file_template , statemgr_config_file_actual , 
{ "<zookeeper_host:zookeeper_port>" : "," . join ( 
[ \ % zk if ":" in zk else \ % zk for zk in zookeepers ] ) } ) 
~~ def template_heron_tools_hcl ( cl_args , masters , zookeepers ) : 
heron_tools_hcl_template = "%s/standalone/templates/heron_tools.template.hcl" % cl_args [ "config_path" ] 
heron_tools_hcl_actual = "%s/standalone/resources/heron_tools.hcl" % cl_args [ "config_path" ] 
template_file ( heron_tools_hcl_template , heron_tools_hcl_actual , 
"<heron_tracker_executable>" : \ % config . get_heron_bin_dir ( ) , 
"<heron_tools_hostname>" : \ % get_hostname ( single_master , cl_args ) , 
"<heron_ui_executable>" : \ % config . get_heron_bin_dir ( ) 
~~ def print_cluster_info ( cl_args ) : 
parsed_roles = read_and_parse_roles ( cl_args ) 
masters = list ( parsed_roles [ Role . MASTERS ] ) 
slaves = list ( parsed_roles [ Role . SLAVES ] ) 
zookeepers = list ( parsed_roles [ Role . ZOOKEEPERS ] ) 
cluster = list ( parsed_roles [ Role . CLUSTER ] ) 
info = OrderedDict ( ) 
info [ 'numNodes' ] = len ( cluster ) 
info [ 'nodes' ] = cluster 
roles = OrderedDict ( ) 
roles [ 'masters' ] = masters 
roles [ 'slaves' ] = slaves 
roles [ 'zookeepers' ] = zookeepers 
urls = OrderedDict ( ) 
urls [ 'serviceUrl' ] = get_service_url ( cl_args ) 
urls [ 'heronUi' ] = get_heron_ui_url ( cl_args ) 
urls [ 'heronTracker' ] = get_heron_tracker_url ( cl_args ) 
info [ 'roles' ] = roles 
info [ 'urls' ] = urls 
print json . dumps ( info , indent = 2 ) 
~~ def add_additional_args ( parsers ) : 
for parser in parsers : 
~~~ cli_args . add_verbose ( parser ) 
cli_args . add_config ( parser ) 
'--heron-dir' , 
default = config . get_heron_dir ( ) , 
~~ ~~ def stop_cluster ( cl_args ) : 
roles = read_and_parse_roles ( cl_args ) 
masters = roles [ Role . MASTERS ] 
slaves = roles [ Role . SLAVES ] 
dist_nodes = masters . union ( slaves ) 
if masters : 
~~~ single_master = list ( masters ) [ 0 ] 
jobs = get_jobs ( cl_args , single_master ) 
for job in jobs : 
~~~ job_id = job [ "ID" ] 
delete_job ( cl_args , job_id , single_master ) 
~~ ~~ except : 
Log . debug ( sys . exc_info ( ) [ 0 ] ) 
~~ ~~ for node in dist_nodes : 
if not is_self ( node ) : 
cmd = ssh_remote_execute ( cmd , node , cl_args ) 
~~ Log . debug ( cmd ) 
pid = subprocess . Popen ( cmd , 
stderr = subprocess . PIPE ) 
return_code = pid . wait ( ) 
output = pid . communicate ( ) 
~~~ cmd = ssh_remote_execute ( cmd , node , cl_args ) 
~~ ~~ def start_cluster ( cl_args ) : 
zookeepers = roles [ Role . ZOOKEEPERS ] 
Log . info ( "Roles:" ) 
if not masters : 
sys . exit ( - 1 ) 
~~ if not slaves : 
~~ if not zookeepers : 
~~ update_config_files ( cl_args ) 
dist_nodes = list ( masters . union ( slaves ) ) 
if not ( len ( dist_nodes ) == 1 and is_self ( dist_nodes [ 0 ] ) ) : 
~~~ distribute_package ( roles , cl_args ) 
~~ start_master_nodes ( masters , cl_args ) 
start_slave_nodes ( slaves , cl_args ) 
start_api_server ( masters , cl_args ) 
start_heron_tools ( masters , cl_args ) 
~~ def start_heron_tools ( masters , cl_args ) : 
single_master = list ( masters ) [ 0 ] 
wait_for_master_to_start ( single_master ) 
if not is_self ( single_master ) : 
~~~ cmd = ssh_remote_execute ( cmd , single_master , cl_args ) 
if return_code != 0 : 
~~ wait_for_job_to_start ( single_master , "heron-tools" ) 
~~ def distribute_package ( roles , cl_args ) : 
tar_file = tempfile . NamedTemporaryFile ( suffix = ".tmp" ) . name 
make_tarfile ( tar_file , cl_args [ "heron_dir" ] ) 
scp_package ( tar_file , dist_nodes , cl_args ) 
~~ def wait_for_master_to_start ( single_master ) : 
~~~ r = requests . get ( "http://%s:4646/v1/status/leader" % single_master ) 
if r . status_code == 200 : 
~~~ Log . debug ( sys . exc_info ( ) [ 0 ] ) 
if i > 10 : 
~~ ~~ i = i + 1 
~~ ~~ def wait_for_job_to_start ( single_master , job ) : 
~~~ r = requests . get ( "http://%s:4646/v1/job/%s" % ( single_master , job ) ) 
if r . status_code == 200 and r . json ( ) [ "Status" ] == "running" : 
~~~ raise RuntimeError ( ) 
if i > 20 : 
~~ ~~ def scp_package ( package_file , destinations , cl_args ) : 
pids = [ ] 
for dest in destinations : 
~~~ if is_self ( dest ) : 
file_path = "/tmp/heron.tar.gz" 
dest_file_path = "%s:%s" % ( dest , file_path ) 
ssh_remote_execute ( remote_cmd , dest , cl_args ) ) 
Log . debug ( cmd ) 
pids . append ( { "pid" : pid , "dest" : dest } ) 
~~ errors = [ ] 
for entry in pids : 
~~~ pid = entry [ "pid" ] 
~~ ~~ if errors : 
~~~ for error in errors : 
~~~ Log . error ( error ) 
~~ sys . exit ( - 1 ) 
~~ def make_tarfile ( output_filename , source_dir ) : 
with tarfile . open ( output_filename , "w:gz" ) as tar : 
~~~ tar . add ( source_dir , arcname = os . path . basename ( source_dir ) ) 
~~ ~~ def start_master_nodes ( masters , cl_args ) : 
for master in masters : 
if not is_self ( master ) : 
~~~ cmd = ssh_remote_execute ( cmd , master , cl_args ) 
pids . append ( { "pid" : pid , "dest" : master } ) 
~~ def start_slave_nodes ( slaves , cl_args ) : 
for slave in slaves : 
if not is_self ( slave ) : 
~~~ cmd = ssh_remote_execute ( cmd , slave , cl_args ) 
pids . append ( { "pid" : pid , "dest" : slave } ) 
~~ def read_and_parse_roles ( cl_args ) : 
roles = dict ( ) 
with open ( get_inventory_file ( cl_args ) , 'r' ) as stream : 
~~~ roles = yaml . load ( stream ) 
~~ except yaml . YAMLError as exc : 
~~ ~~ if Role . ZOOKEEPERS not in roles or not roles [ Role . ZOOKEEPERS ] : 
~~ if Role . CLUSTER not in roles or not roles [ Role . CLUSTER ] : 
~~ roles [ Role . MASTERS ] = set ( [ roles [ Role . CLUSTER ] [ 0 ] ] ) 
roles [ Role . SLAVES ] = set ( roles [ Role . CLUSTER ] ) 
roles [ Role . ZOOKEEPERS ] = set ( roles [ Role . ZOOKEEPERS ] ) 
roles [ Role . CLUSTER ] = set ( roles [ Role . CLUSTER ] ) 
return roles 
~~ def read_file ( file_path ) : 
lines = [ ] 
with open ( file_path , "r" ) as tf : 
~~~ lines = [ line . strip ( "\\n" ) for line in tf . readlines ( ) if not line . startswith ( "#" ) ] 
lines = [ line for line in lines if line ] 
~~ return lines 
~~ def call_editor ( file_path ) : 
EDITOR = os . environ . get ( 'EDITOR' , 'vim' ) 
with open ( file_path , 'r+' ) as tf : 
~~~ call ( [ EDITOR , tf . name ] ) 
~~ ~~ def get_remote_home ( host , cl_args ) : 
if not is_self ( host ) : 
~~~ cmd = ssh_remote_execute ( cmd , host , cl_args ) 
~~ pid = subprocess . Popen ( cmd , 
~~ return output [ 0 ] . strip ( "\\n" ) 
~~ def get_hostname ( ip_addr , cl_args ) : 
if is_self ( ip_addr ) : 
~~~ return get_self_hostname ( ) 
~~ cmd = "hostname" 
ssh_cmd = ssh_remote_execute ( cmd , ip_addr , cl_args ) 
pid = subprocess . Popen ( ssh_cmd , 
~~ def is_self ( addr ) : 
ips = [ ] 
for i in netifaces . interfaces ( ) : 
~~~ entry = netifaces . ifaddresses ( i ) 
if netifaces . AF_INET in entry : 
~~~ for ipv4 in entry [ netifaces . AF_INET ] : 
~~~ if "addr" in ipv4 : 
~~~ ips . append ( ipv4 [ "addr" ] ) 
~~ ~~ ~~ ~~ return addr in ips or addr == get_self_hostname ( ) 
~~ def log ( self , message , level = None ) : 
if level is None : 
~~~ _log_level = logging . INFO 
~~~ if level == "trace" or level == "debug" : 
~~~ _log_level = logging . DEBUG 
~~ elif level == "info" : 
~~ elif level == "warn" : 
~~~ _log_level = logging . WARNING 
~~ elif level == "error" : 
~~~ _log_level = logging . ERROR 
~~ ~~ self . logger . log ( _log_level , message ) 
~~ def load_py_instance ( self , is_spout ) : 
~~~ if is_spout : 
~~~ spout_proto = self . pplan_helper . get_my_spout ( ) 
py_classpath = spout_proto . comp . class_name 
~~~ bolt_proto = self . pplan_helper . get_my_bolt ( ) 
py_classpath = bolt_proto . comp . class_name 
~~ pex_loader . load_pex ( self . pplan_helper . topology_pex_abs_path ) 
spbl_class = pex_loader . import_and_get_class ( self . pplan_helper . topology_pex_abs_path , 
py_classpath ) 
~~~ spbl = "spout" if is_spout else "bolt" 
self . logger . error ( traceback . format_exc ( ) ) 
~~ return spbl_class 
if not cluster or not execution_state or not environ : 
~~ topo_role = execution_state . role 
if not topo_role : 
~~ if role and role != topo_role : 
~~ if topo_role not in ret [ cluster ] : 
~~~ ret [ cluster ] [ topo_role ] = { } 
~~ if environ not in ret [ cluster ] [ topo_role ] : 
~~~ ret [ cluster ] [ topo_role ] [ environ ] = [ ] 
~~ ret [ cluster ] [ topo_role ] [ environ ] . append ( topology . name ) 
~~ self . write_success_response ( ret ) 
~~ def dereference_symlinks ( src ) : 
while os . path . islink ( src ) : 
~~~ src = os . path . join ( os . path . dirname ( src ) , os . readlink ( src ) ) 
~~ return src 
clusters = [ statemgr . name for statemgr in self . tracker . state_managers ] 
self . write_success_response ( clusters ) 
~~ def get_cluster_role_env_topologies ( cluster , role , env ) : 
return _get_topologies ( cluster , role = role , env = env ) 
~~ def get_execution_state ( cluster , environ , topology , role = None ) : 
params = dict ( cluster = cluster , environ = environ , topology = topology ) 
if role is not None : 
~~~ params [ 'role' ] = role 
~~ request_url = tornado . httputil . url_concat ( create_url ( EXECUTION_STATE_URL_FMT ) , params ) 
raise tornado . gen . Return ( ( yield fetch_url_as_json ( request_url ) ) ) 
~~ def get_logical_plan ( cluster , environ , topology , role = None ) : 
~~ request_url = tornado . httputil . url_concat ( 
create_url ( LOGICALPLAN_URL_FMT ) , params ) 
~~ def get_comps ( cluster , environ , topology , role = None ) : 
lplan = yield fetch_url_as_json ( request_url ) 
comps = lplan [ 'spouts' ] . keys ( ) + lplan [ 'bolts' ] . keys ( ) 
raise tornado . gen . Return ( comps ) 
~~ def get_instances ( cluster , environ , topology , role = None ) : 
create_url ( PHYSICALPLAN_URL_FMT ) , params ) 
pplan = yield fetch_url_as_json ( request_url ) 
instances = pplan [ 'instances' ] . keys ( ) 
raise tornado . gen . Return ( instances ) 
~~ def get_physical_plan ( cluster , environ , topology , role = None ) : 
~~ def get_scheduler_location ( cluster , environ , topology , role = None ) : 
create_url ( SCHEDULER_LOCATION_URL_FMT ) , params ) 
~~ def get_component_exceptionsummary ( cluster , environ , topology , component , role = None ) : 
params = dict ( 
component = component ) 
create_url ( EXCEPTION_SUMMARY_URL_FMT ) , params ) 
~~ def get_component_exceptions ( cluster , environ , topology , component , role = None ) : 
create_url ( EXCEPTIONS_URL_FMT ) , params ) 
~~ def get_comp_instance_metrics ( cluster , environ , topology , component , 
metrics , instances , time_range , role = None ) : 
create_url ( METRICS_URL_FMT ) , params ) 
all_instances = instances if isinstance ( instances , list ) else [ instances ] 
for _ , metric_name in metrics . items ( ) : 
~~~ request_url = tornado . httputil . url_concat ( request_url , dict ( metricname = metric_name [ 0 ] ) ) 
~~ for i in all_instances : 
~~~ request_url = tornado . httputil . url_concat ( request_url , dict ( instance = i ) ) 
~~ request_url = tornado . httputil . url_concat ( request_url , dict ( interval = time_range [ 1 ] ) ) 
~~ def get_comp_metrics ( cluster , environ , topology , component , 
instances , metricnames , time_range , role = None ) : 
for metric_name in metricnames : 
~~~ request_url = tornado . httputil . url_concat ( request_url , dict ( metricname = metric_name ) ) 
~~ for instance in instances : 
~~~ request_url = tornado . httputil . url_concat ( request_url , dict ( instance = instance ) ) 
~~ def get_metrics ( cluster , environment , topology , timerange , query , role = None ) : 
environ = environment , 
starttime = timerange [ 0 ] , 
endtime = timerange [ 1 ] , 
query = query ) 
create_url ( METRICS_QUERY_URL_FMT ) , params 
~~ def get_comp_metrics_timeline ( cluster , environ , topology , component , 
~~ request_url = tornado . httputil . url_concat ( create_url ( METRICS_TIMELINE_URL_FMT ) , params ) 
~~~ request_url = tornado . httputil . url_concat ( request_url , dict ( role = role ) ) 
~~ for metric_name in metricnames : 
request_url , dict ( starttime = time_range [ 0 ] , endtime = time_range [ 1 ] ) ) 
~~ def get_topology_info ( cluster , environ , topology , role = None ) : 
topology = topology ) 
~~ request_url = tornado . httputil . url_concat ( create_url ( INFO_URL_FMT ) , params ) 
~~ def get_instance_pid ( cluster , environ , topology , instance , role = None ) : 
instance = instance ) 
~~ request_url = tornado . httputil . url_concat ( create_url ( PID_URL_FMT ) , params ) 
~~ def get_instance_jstack ( cluster , environ , topology , instance , role = None ) : 
create_url ( JSTACK_URL_FMT ) , params ) 
~~ def get_instance_mem_histogram ( cluster , environ , topology , instance , role = None ) : 
create_url ( HISTOGRAM_URL_FMT ) , params ) 
~~ def run_instance_jmap ( cluster , environ , topology , instance , role = None ) : 
create_url ( JMAP_URL_FMT ) , params ) 
~~ raise tornado . gen . Return ( ( yield fetch_url_as_json ( request_url ) ) ) 
~~ def get_container_file_download_url ( cluster , environ , topology , container , 
path , role = None ) : 
path = path ) 
create_url ( FILE_DOWNLOAD_URL_FMT ) , params ) 
~~ return request_url 
~~ def get_container_file_data ( cluster , environ , topology , container , 
path , offset , length , role = None ) : 
offset = offset , 
length = length ) 
create_url ( FILE_DATA_URL_FMT ) , params ) 
~~ def get_filestats ( cluster , environ , topology , container , path , role = None ) : 
~~ request_url = tornado . httputil . url_concat ( create_url ( FILESTATS_URL_FMT ) , params ) 
~~ def fetch ( self , cluster , metric , topology , component , instance , timerange , environ = None ) : 
components = [ component ] if component != "*" else ( yield get_comps ( cluster , environ , topology ) ) 
futures = [ ] 
for comp in components : 
~~~ query = self . get_query ( metric , comp , instance ) 
future = get_metrics ( cluster , environ , topology , timerange , query ) 
futures . append ( future ) 
timelines = [ ] 
for result in results : 
~~~ timelines . extend ( result [ "timeline" ] ) 
~~ result = self . get_metric_response ( timerange , timelines , False ) 
raise tornado . gen . Return ( result ) 
~~ def fetch_max ( self , cluster , metric , topology , component , instance , timerange , environ = None ) : 
max_query = "MAX(%s)" % query 
future = get_metrics ( cluster , environ , topology , timerange , max_query ) 
data = self . compute_max ( results ) 
result = self . get_metric_response ( timerange , data , True ) 
~~ def fetch_backpressure ( self , cluster , metric , topology , component , instance , timerange , is_max , environ = None ) : 
instances = yield get_instances ( cluster , environ , topology ) 
if component != "*" : 
~~~ filtered_inst = [ instance for instance in instances if instance . split ( "_" ) [ 2 ] == component ] 
~~~ filtered_inst = instances 
~~ futures_dict = { } 
for inst in filtered_inst : 
~~~ query = queries . get ( metric ) . format ( inst ) 
futures_dict [ inst ] = get_metrics ( cluster , environ , topology , timerange , query ) 
~~ res = yield futures_dict 
if not is_max : 
~~~ timelines = [ ] 
for key in res : 
~~~ result = res [ key ] 
if len ( result [ "timeline" ] ) > 0 : 
~~~ result [ "timeline" ] [ 0 ] [ "instance" ] = key 
~~ timelines . extend ( result [ "timeline" ] ) 
~~ result = self . get_metric_response ( timerange , timelines , is_max ) 
~~~ data = self . compute_max ( res . values ( ) ) 
result = self . get_metric_response ( timerange , data , is_max ) 
~~ raise tornado . gen . Return ( result ) 
~~ def compute_max ( self , multi_ts ) : 
if len ( multi_ts ) > 0 and len ( multi_ts [ 0 ] [ "timeline" ] ) > 0 : 
~~~ keys = multi_ts [ 0 ] [ "timeline" ] [ 0 ] [ "data" ] . keys ( ) 
timelines = ( [ res [ "timeline" ] [ 0 ] [ "data" ] [ key ] for key in keys ] for res in multi_ts ) 
values = ( max ( v ) for v in zip ( * timelines ) ) 
return dict ( zip ( keys , values ) ) 
~~ return { } 
~~ def get_metric_response ( self , timerange , data , isMax ) : 
if isMax : 
status = "success" , 
result = dict ( timeline = [ dict ( data = data ) ] ) 
result = dict ( timeline = data ) 
~~ def get_query ( self , metric , component , instance ) : 
q = queries . get ( metric ) 
return q . format ( component , instance ) 
~~ def to_table ( result ) : 
max_count = 20 
table , count = [ ] , 0 
for role , envs_topos in result . items ( ) : 
~~~ for env , topos in envs_topos . items ( ) : 
~~~ for topo in topos : 
~~~ count += 1 
if count > max_count : 
~~~ table . append ( [ role , env , topo ] ) 
~~ ~~ ~~ ~~ header = [ 'role' , 'env' , 'topology' ] 
rest_count = 0 if count <= max_count else count - max_count 
return table , header , rest_count 
~~ def show_cluster ( cl_args , cluster ) : 
~~~ result = tracker_access . get_cluster_topologies ( cluster ) 
if not result : 
~~ result = result [ cluster ] 
~~ table , header , rest_count = to_table ( result ) 
if rest_count : 
~~ print ( tabulate ( table , headers = header ) ) 
~~ def show_cluster_role ( cl_args , cluster , role ) : 
~~~ result = tracker_access . get_cluster_role_topologies ( cluster , role ) 
~~ def show_cluster_role_env ( cl_args , cluster , role , env ) : 
~~~ result = tracker_access . get_cluster_role_env_topologies ( cluster , role , env ) 
location = cl_args [ 'cluster/[role]/[env]' ] . split ( '/' ) 
if len ( location ) == 1 : 
~~~ return show_cluster ( cl_args , * location ) 
~~ elif len ( location ) == 2 : 
~~~ return show_cluster_role ( cl_args , * location ) 
~~ elif len ( location ) == 3 : 
~~~ return show_cluster_role_env ( cl_args , * location ) 
~~ ~~ def heron_class ( class_name , lib_jars , extra_jars = None , args = None , java_defines = None ) : 
if extra_jars is None : 
~~~ extra_jars = [ ] 
~~ if args is None : 
~~~ args = [ ] 
~~ if java_defines is None : 
~~~ java_defines = [ ] 
~~ java_opts = [ '-D' + opt for opt in java_defines ] 
all_args = [ config . get_java_path ( ) , "-client" , "-Xmx1g" ] + java_opts + [ "-cp" , config . get_classpath ( extra_jars + lib_jars ) ] 
all_args += [ class_name ] + list ( args ) 
heron_env = os . environ . copy ( ) 
heron_env [ 'HERON_OPTIONS' ] = opts . get_heron_config ( ) 
process = subprocess . Popen ( all_args , env = heron_env , stdout = subprocess . PIPE , 
stderr = subprocess . PIPE , bufsize = 1 ) 
return ProcessResult ( process ) 
~~ def heron_tar ( class_name , topology_tar , arguments , tmpdir_root , java_defines ) : 
tmpdir = tempfile . mkdtemp ( dir = tmpdir_root , prefix = 'tmp' ) 
with contextlib . closing ( tarfile . open ( topology_tar ) ) as tar : 
~~~ tar . extractall ( path = tmpdir ) 
~~ topology_jar = os . path . basename ( topology_tar ) . replace ( ".tar.gz" , "" ) . replace ( ".tar" , "" ) + ".jar" 
extra_jars = [ 
os . path . join ( tmpdir , topology_jar ) , 
os . path . join ( tmpdir , "*" ) , 
os . path . join ( tmpdir , "libs/*" ) 
lib_jars = config . get_heron_libs ( jars . topology_jars ( ) ) 
return heron_class ( class_name , lib_jars , extra_jars , arguments , java_defines ) 
~~ def get ( self , cluster , environ , topology , comp_name ) : 
comp_names = [ ] 
if comp_name == "All" : 
~~~ lplan = yield access . get_logical_plan ( cluster , environ , topology ) 
if not lplan : 
~~~ self . write ( dict ( ) ) 
~~ if not 'spouts' in lplan or not 'bolts' in lplan : 
~~ comp_names = lplan [ 'spouts' ] . keys ( ) 
comp_names . extend ( lplan [ 'bolts' ] . keys ( ) ) 
~~~ comp_names = [ comp_name ] 
~~ exception_infos = dict ( ) 
for comp_name in comp_names : 
~~~ exception_infos [ comp_name ] = yield access . get_component_exceptionsummary ( 
cluster , environ , topology , comp_name ) 
~~ aggregate_exceptions = dict ( ) 
for comp_name , exception_logs in exception_infos . items ( ) : 
~~~ for exception_log in exception_logs : 
~~~ class_name = exception_log [ 'class_name' ] 
if class_name != '' : 
~~~ if not class_name in aggregate_exceptions : 
~~~ aggregate_exceptions [ class_name ] = 0 
~~ aggregate_exceptions [ class_name ] += int ( exception_log [ 'count' ] ) 
~~ ~~ ~~ aggregate_exceptions_table = [ ] 
for key in aggregate_exceptions : 
~~~ aggregate_exceptions_table . append ( [ key , str ( aggregate_exceptions [ key ] ) ] ) 
~~ result = dict ( 
executiontime = time . time ( ) - start_time , 
result = aggregate_exceptions_table ) 
self . write ( result ) 
topologies = yield access . get_topologies_states ( ) 
for cluster , cluster_value in topologies . items ( ) : 
~~~ result [ cluster ] = dict ( ) 
for environ , environ_value in cluster_value . items ( ) : 
~~~ result [ cluster ] [ environ ] = dict ( ) 
for topology , topology_value in environ_value . items ( ) : 
~~~ if "jobname" not in topology_value or topology_value [ "jobname" ] is None : 
~~ if "submission_time" in topology_value : 
~~~ topology_value [ "submission_time" ] = topology_value [ "submission_time" ] 
~~~ topology_value [ "submission_time" ] = '-' 
~~ result [ cluster ] [ environ ] [ topology ] = topology_value 
~~ ~~ ~~ self . write ( result ) 
lplan = yield access . get_logical_plan ( cluster , environ , topology ) 
result = dict ( 
message = "" , 
version = common . VERSION , 
result = lplan 
pplan = yield access . get_physical_plan ( cluster , environ , topology ) 
result_map = dict ( 
result = pplan 
self . write ( result_map ) 
~~ def get ( self , cluster , environ , topology , component ) : 
futures = yield access . get_component_exceptions ( cluster , environ , topology , component ) 
status = 'success' , 
result = futures ) 
self . write ( json . dumps ( result_map ) ) 
~~ def get ( self , cluster , environ , topology , instance ) : 
host = pplan [ 'stmgrs' ] [ pplan [ 'instances' ] [ instance ] [ 'stmgrId' ] ] [ 'host' ] 
result = json . loads ( ( yield access . get_instance_pid ( 
cluster , environ , topology , instance ) ) ) 
host , 
tornado . escape . xhtml_escape ( result [ 'command' ] ) , 
tornado . escape . xhtml_escape ( result [ 'stdout' ] ) ) ) 
~~ def add_context ( self , err_context , succ_context = None ) : 
self . err_context = err_context 
self . succ_context = succ_context 
~~ def renderProcessStdErr ( self , stderr_line ) : 
retcode = self . process . poll ( ) 
if retcode is not None and status_type ( retcode ) == Status . InvocationError : 
~~~ self . _do_log ( Log . error , stderr_line ) 
~~~ self . _do_print ( sys . stderr , stderr_line ) 
~~ ~~ def renderProcessStdOut ( self , stdout ) : 
assert self . status is not None 
if self . status == Status . Ok : 
~~~ self . _do_log ( Log . info , stdout ) 
~~ elif self . status == Status . HeronError : 
~~~ self . _do_log ( Log . error , stdout ) 
~~ elif self . status == Status . DryRun : 
~~~ self . _do_print ( sys . stdout , stdout ) 
~~ elif self . status == Status . InvocationError : 
~~ ~~ def is_host_port_reachable ( self ) : 
for hostport in self . hostportlist : 
~~~ socket . create_connection ( hostport , StateManager . TIMEOUT_SECONDS ) 
% ( self . name , hostport [ 0 ] , hostport [ 1 ] ) ) 
~~ def pick_unused_port ( self ) : 
s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) 
s . bind ( ( '127.0.0.1' , 0 ) ) 
_ , port = s . getsockname ( ) 
s . close ( ) 
return port 
~~ def establish_ssh_tunnel ( self ) : 
localportlist = [ ] 
for ( host , port ) in self . hostportlist : 
~~~ localport = self . pick_unused_port ( ) 
self . tunnel . append ( subprocess . Popen ( 
( 'ssh' , self . tunnelhost , '-NL127.0.0.1:%d:%s:%d' % ( localport , host , port ) ) ) ) 
localportlist . append ( ( '127.0.0.1' , localport ) ) 
~~ return localportlist 
~~ def delete_topology_from_zk ( self , topologyName ) : 
self . delete_pplan ( topologyName ) 
self . delete_execution_state ( topologyName ) 
self . delete_topology ( topologyName ) 
~~ def monitor ( self ) : 
def trigger_watches_based_on_files ( watchers , path , directory , ProtoClass ) : 
for topology , callbacks in watchers . items ( ) : 
~~~ file_path = os . path . join ( path , topology ) 
data = "" 
if os . path . exists ( file_path ) : 
~~~ with open ( os . path . join ( path , topology ) ) as f : 
~~ ~~ if topology not in directory or data != directory [ topology ] : 
~~~ proto_object = ProtoClass ( ) 
proto_object . ParseFromString ( data ) 
for callback in callbacks : 
~~~ callback ( proto_object ) 
~~ directory [ topology ] = data 
~~ ~~ ~~ while not self . monitoring_thread_stop_signal : 
~~~ topologies_path = self . get_topologies_path ( ) 
if os . path . isdir ( topologies_path ) : 
~~~ topologies = list ( filter ( 
lambda f : os . path . isfile ( os . path . join ( topologies_path , f ) ) , 
os . listdir ( topologies_path ) ) ) 
~~ if set ( topologies ) != set ( self . topologies_directory ) : 
~~~ for callback in self . topologies_watchers : 
~~~ callback ( topologies ) 
~~ ~~ self . topologies_directory = topologies 
trigger_watches_based_on_files ( 
self . topology_watchers , topologies_path , self . topologies_directory , Topology ) 
execution_state_path = os . path . dirname ( self . get_execution_state_path ( "" ) ) 
self . execution_state_watchers , execution_state_path , 
self . execution_state_directory , ExecutionState ) 
packing_plan_path = os . path . dirname ( self . get_packing_plan_path ( "" ) ) 
self . packing_plan_watchers , packing_plan_path , self . packing_plan_directory , PackingPlan ) 
pplan_path = os . path . dirname ( self . get_pplan_path ( "" ) ) 
self . pplan_watchers , pplan_path , 
self . pplan_directory , PhysicalPlan ) 
tmaster_path = os . path . dirname ( self . get_tmaster_path ( "" ) ) 
self . tmaster_watchers , tmaster_path , 
self . tmaster_directory , TMasterLocation ) 
scheduler_location_path = os . path . dirname ( self . get_scheduler_location_path ( "" ) ) 
self . scheduler_location_watchers , scheduler_location_path , 
self . scheduler_location_directory , SchedulerLocation ) 
self . event . wait ( timeout = 5 ) 
~~ ~~ def get_topologies ( self , callback = None ) : 
~~~ self . topologies_watchers . append ( callback ) 
return filter ( lambda f : os . path . isfile ( os . path . join ( topologies_path , f ) ) , 
os . listdir ( topologies_path ) ) 
~~~ self . topology_watchers [ topologyName ] . append ( callback ) 
~~~ topology_path = self . get_topology_path ( topologyName ) 
with open ( topology_path ) as f : 
topology = Topology ( ) 
~~ ~~ ~~ def get_packing_plan ( self , topologyName , callback = None ) : 
~~~ self . packing_plan_watchers [ topologyName ] . append ( callback ) 
~~~ packing_plan_path = self . get_packing_plan_path ( topologyName ) 
with open ( packing_plan_path ) as f : 
packing_plan = PackingPlan ( ) 
~~ ~~ ~~ def get_pplan ( self , topologyName , callback = None ) : 
~~~ self . pplan_watchers [ topologyName ] . append ( callback ) 
~~~ pplan_path = self . get_pplan_path ( topologyName ) 
with open ( pplan_path ) as f : 
pplan = PhysicalPlan ( ) 
return pplan 
~~ ~~ ~~ def get_execution_state ( self , topologyName , callback = None ) : 
~~~ self . execution_state_watchers [ topologyName ] . append ( callback ) 
~~~ execution_state_path = self . get_execution_state_path ( topologyName ) 
with open ( execution_state_path ) as f : 
executionState = ExecutionState ( ) 
return executionState 
~~ ~~ ~~ def get_tmaster ( self , topologyName , callback = None ) : 
~~~ self . tmaster_watchers [ topologyName ] . append ( callback ) 
~~~ tmaster_path = self . get_tmaster_path ( topologyName ) 
with open ( tmaster_path ) as f : 
tmaster = TMasterLocation ( ) 
return tmaster 
~~ ~~ ~~ def get_scheduler_location ( self , topologyName , callback = None ) : 
~~~ self . scheduler_location_watchers [ topologyName ] . append ( callback ) 
~~~ scheduler_location_path = self . get_scheduler_location_path ( topologyName ) 
with open ( scheduler_location_path ) as f : 
scheduler_location = SchedulerLocation ( ) 
return scheduler_location 
~~ ~~ ~~ def get ( self , pid ) : 
body = utils . str_cmd ( [ 'jmap' , '-histo' , pid ] , None , None ) 
self . write ( json . dumps ( body ) ) 
~~ def create_socket_options ( ) : 
opt_list = [ const . INSTANCE_NETWORK_WRITE_BATCH_SIZE_BYTES , 
const . INSTANCE_NETWORK_WRITE_BATCH_TIME_MS , 
const . INSTANCE_NETWORK_READ_BATCH_SIZE_BYTES , 
const . INSTANCE_NETWORK_READ_BATCH_TIME_MS , 
const . INSTANCE_NETWORK_OPTIONS_SOCKET_RECEIVED_BUFFER_SIZE_BYTES , 
const . INSTANCE_NETWORK_OPTIONS_SOCKET_SEND_BUFFER_SIZE_BYTES ] 
~~~ value_lst = [ int ( sys_config [ opt ] ) for opt in opt_list ] 
sock_opt = SocketOptions ( * value_lst ) 
return sock_opt 
~~ except ValueError as e : 
~~ ~~ def class_dict_to_specs ( mcs , class_dict ) : 
specs = { } 
for name , spec in class_dict . items ( ) : 
~~~ if isinstance ( spec , HeronComponentSpec ) : 
~~~ if spec . name is None : 
~~~ spec . name = name 
~~ if spec . name in specs : 
~~~ specs [ spec . name ] = spec 
~~ ~~ ~~ return specs 
~~ def class_dict_to_topo_config ( mcs , class_dict ) : 
topo_config = { } 
topo_config . update ( mcs . DEFAULT_TOPOLOGY_CONFIG ) 
for name , custom_config in class_dict . items ( ) : 
~~~ if name == 'config' and isinstance ( custom_config , dict ) : 
~~~ sanitized_dict = mcs . _sanitize_config ( custom_config ) 
topo_config . update ( sanitized_dict ) 
~~ ~~ return topo_config 
~~ def init_topology ( mcs , classname , class_dict ) : 
if classname == 'Topology' : 
~~ heron_options = TopologyType . get_heron_options_from_env ( ) 
initial_state = heron_options . get ( "cmdline.topology.initial.state" , "RUNNING" ) 
tmp_directory = heron_options . get ( "cmdline.topologydefn.tmpdirectory" ) 
if tmp_directory is None : 
~~ topology_name = heron_options . get ( "cmdline.topology.name" , classname ) 
topology_id = topology_name + str ( uuid . uuid4 ( ) ) 
topology = topology_pb2 . Topology ( ) 
topology . id = topology_id 
topology . name = topology_name 
topology . state = topology_pb2 . TopologyState . Value ( initial_state ) 
topology . topology_config . CopyFrom ( TopologyType . get_topology_config_protobuf ( class_dict ) ) 
TopologyType . add_bolts_and_spouts ( topology , class_dict ) 
class_dict [ 'topology_name' ] = topology_name 
class_dict [ 'topology_id' ] = topology_id 
class_dict [ 'protobuf_topology' ] = topology 
class_dict [ 'topologydefn_tmpdir' ] = tmp_directory 
class_dict [ 'heron_runtime_options' ] = heron_options 
~~ def get_heron_options_from_env ( ) : 
heron_options_raw = os . environ . get ( "HERON_OPTIONS" ) 
if heron_options_raw is None : 
~~ options = { } 
~~~ key , sep , value = option_line . partition ( "=" ) 
if sep : 
~~~ options [ key ] = value 
~~ ~~ return options 
~~ def add_spec ( self , * specs ) : 
for spec in specs : 
~~~ if not isinstance ( spec , HeronComponentSpec ) : 
% str ( spec ) ) 
~~ if spec . name is None : 
~~ if spec . name == "config" : 
~~ if spec . name in self . _specs : 
~~ self . _specs [ spec . name ] = spec 
~~ ~~ def add_spout ( self , name , spout_cls , par , config = None , optional_outputs = None ) : 
spout_spec = spout_cls . spec ( name = name , par = par , config = config , 
optional_outputs = optional_outputs ) 
self . add_spec ( spout_spec ) 
return spout_spec 
~~ def add_bolt ( self , name , bolt_cls , par , inputs , config = None , optional_outputs = None ) : 
bolt_spec = bolt_cls . spec ( name = name , par = par , inputs = inputs , config = config , 
self . add_spec ( bolt_spec ) 
return bolt_spec 
~~ def set_config ( self , config ) : 
if not isinstance ( config , dict ) : 
~~ self . _topology_config = config 
~~ def build_and_submit ( self ) : 
class_dict = self . _construct_topo_class_dict ( ) 
topo_cls = TopologyType ( self . topology_name , ( Topology , ) , class_dict ) 
topo_cls . write ( ) 
~~ def fetch_url_as_json ( fetch_url , default_value = None ) : 
if default_value is None : 
~~~ default_value = dict ( ) 
ret = default_value 
http_response = yield tornado . httpclient . AsyncHTTPClient ( ) . fetch ( fetch_url ) 
if http_response . error : 
raise tornado . gen . Return ( ret ) 
~~ response = json . loads ( http_response . body ) 
if not 'result' in response : 
~~ ret = response [ 'result' ] 
execution = 1000 * response [ 'executiontime' ] 
end = time . time ( ) 
duration = 1000 * ( end - start ) 
~~ def queries_map ( ) : 
qs = _all_metric_queries ( ) 
return dict ( zip ( qs [ 0 ] , qs [ 1 ] ) + zip ( qs [ 2 ] , qs [ 3 ] ) ) 
~~ def get_clusters ( ) : 
instance = tornado . ioloop . IOLoop . instance ( ) 
~~~ return instance . run_sync ( lambda : API . get_clusters ( ) ) 
~~ ~~ def get_logical_plan ( cluster , env , topology , role ) : 
~~~ return instance . run_sync ( lambda : API . get_logical_plan ( cluster , env , topology , role ) ) 
~~ ~~ def get_topology_info ( * args ) : 
~~~ return instance . run_sync ( lambda : API . get_topology_info ( * args ) ) 
~~ ~~ def get_component_metrics ( component , cluster , env , topology , role ) : 
all_queries = metric_queries ( ) 
~~~ result = get_topology_metrics ( cluster , env , topology , component , [ ] , 
all_queries , [ 0 , - 1 ] , role ) 
return result [ "metrics" ] 
~~ ~~ def configure ( level = logging . INFO , logfile = None ) : 
for handler in Log . handlers : 
~~~ if isinstance ( handler , logging . StreamHandler ) : 
~~~ Log . handlers . remove ( handler ) 
~~ ~~ Log . setLevel ( level ) 
formatter = logging . Formatter ( fmt = log_format , datefmt = date_format ) 
file_handler = logging . FileHandler ( logfile ) 
file_handler . setFormatter ( formatter ) 
Log . addHandler ( file_handler ) 
formatter = colorlog . ColoredFormatter ( fmt = log_format , datefmt = date_format ) 
stream_handler = logging . StreamHandler ( ) 
stream_handler . setFormatter ( formatter ) 
Log . addHandler ( stream_handler ) 
~~ ~~ def init_rotating_logger ( level , logfile , max_files , max_bytes ) : 
logging . basicConfig ( ) 
root_logger = logging . getLogger ( ) 
root_logger . setLevel ( level ) 
handler = RotatingFileHandler ( logfile , maxBytes = max_bytes , backupCount = max_files ) 
handler . setFormatter ( logging . Formatter ( fmt = log_format , datefmt = date_format ) ) 
root_logger . addHandler ( handler ) 
for handler in root_logger . handlers : 
if isinstance ( handler , logging . StreamHandler ) : 
root_logger . handlers . remove ( handler ) 
~~ ~~ ~~ def set_logging_level ( cl_args ) : 
if 'verbose' in cl_args and cl_args [ 'verbose' ] : 
~~~ configure ( logging . DEBUG ) 
~~~ configure ( logging . INFO ) 
~~ ~~ def _get_spout ( self ) : 
spout = topology_pb2 . Spout ( ) 
spout . comp . CopyFrom ( self . _get_base_component ( ) ) 
self . _add_out_streams ( spout ) 
return spout 
~~ def _get_bolt ( self ) : 
bolt = topology_pb2 . Bolt ( ) 
bolt . comp . CopyFrom ( self . _get_base_component ( ) ) 
self . _add_in_streams ( bolt ) 
self . _add_out_streams ( bolt ) 
return bolt 
~~ def _get_base_component ( self ) : 
comp = topology_pb2 . Component ( ) 
comp . name = self . name 
comp . spec = topology_pb2 . ComponentObjectSpec . Value ( "PYTHON_CLASS_NAME" ) 
comp . class_name = self . python_class_path 
comp . config . CopyFrom ( self . _get_comp_config ( ) ) 
return comp 
~~ def _get_comp_config ( self ) : 
proto_config = topology_pb2 . Config ( ) 
key = proto_config . kvs . add ( ) 
key . key = TOPOLOGY_COMPONENT_PARALLELISM 
key . value = str ( self . parallelism ) 
key . type = topology_pb2 . ConfigValueType . Value ( "STRING_VALUE" ) 
if self . custom_config is not None : 
~~~ sanitized = self . _sanitize_config ( self . custom_config ) 
for key , value in sanitized . items ( ) : 
~~~ if isinstance ( value , str ) : 
~~~ kvs = proto_config . kvs . add ( ) 
kvs . key = key 
kvs . value = value 
kvs . type = topology_pb2 . ConfigValueType . Value ( "STRING_VALUE" ) 
kvs . serialized_value = default_serializer . serialize ( value ) 
kvs . type = topology_pb2 . ConfigValueType . Value ( "PYTHON_SERIALIZED_VALUE" ) 
~~ ~~ ~~ return proto_config 
~~ def _sanitize_config ( custom_config ) : 
if not isinstance ( custom_config , dict ) : 
% str ( type ( custom_config ) ) ) 
~~ sanitized = { } 
for key , value in custom_config . items ( ) : 
~~~ if not isinstance ( key , str ) : 
% ( str ( type ( key ) ) , str ( key ) ) ) 
~~ if isinstance ( value , bool ) : 
~~~ sanitized [ key ] = "true" if value else "false" 
~~ elif isinstance ( value , ( str , int , float ) ) : 
~~~ sanitized [ key ] = str ( value ) 
~~~ sanitized [ key ] = value 
~~ ~~ return sanitized 
~~ def _add_in_streams ( self , bolt ) : 
if self . inputs is None : 
~~ input_dict = self . _sanitize_inputs ( ) 
for global_streamid , gtype in input_dict . items ( ) : 
~~~ in_stream = bolt . inputs . add ( ) 
in_stream . stream . CopyFrom ( self . _get_stream_id ( global_streamid . component_id , 
global_streamid . stream_id ) ) 
if isinstance ( gtype , Grouping . FIELDS ) : 
~~~ in_stream . gtype = gtype . gtype 
in_stream . grouping_fields . CopyFrom ( self . _get_stream_schema ( gtype . fields ) ) 
~~ elif isinstance ( gtype , Grouping . CUSTOM ) : 
in_stream . custom_grouping_object = gtype . python_serialized 
in_stream . type = topology_pb2 . CustomGroupingObjectType . Value ( "PYTHON_OBJECT" ) 
~~~ in_stream . gtype = gtype 
~~ ~~ ~~ def _sanitize_inputs ( self ) : 
~~ if isinstance ( self . inputs , dict ) : 
~~~ for key , grouping in self . inputs . items ( ) : 
~~~ if not Grouping . is_grouping_sane ( grouping ) : 
~~ if isinstance ( key , HeronComponentSpec ) : 
~~~ if key . name is None : 
~~ global_streamid = GlobalStreamId ( key . name , Stream . DEFAULT_STREAM_ID ) 
ret [ global_streamid ] = grouping 
~~ elif isinstance ( key , GlobalStreamId ) : 
~~~ ret [ key ] = grouping 
~~ ~~ ~~ elif isinstance ( self . inputs , ( list , tuple ) ) : 
~~~ for input_obj in self . inputs : 
~~~ if isinstance ( input_obj , HeronComponentSpec ) : 
~~~ if input_obj . name is None : 
~~ global_streamid = GlobalStreamId ( input_obj . name , Stream . DEFAULT_STREAM_ID ) 
ret [ global_streamid ] = Grouping . SHUFFLE 
~~ elif isinstance ( input_obj , GlobalStreamId ) : 
~~~ ret [ input_obj ] = Grouping . SHUFFLE 
~~ def _add_out_streams ( self , spbl ) : 
if self . outputs is None : 
~~ output_map = self . _sanitize_outputs ( ) 
for stream_id , out_fields in output_map . items ( ) : 
~~~ out_stream = spbl . outputs . add ( ) 
out_stream . stream . CopyFrom ( self . _get_stream_id ( self . name , stream_id ) ) 
out_stream . schema . CopyFrom ( self . _get_stream_schema ( out_fields ) ) 
~~ ~~ def _sanitize_outputs ( self ) : 
~~ if not isinstance ( self . outputs , ( list , tuple ) ) : 
% str ( type ( self . outputs ) ) ) 
~~ for output in self . outputs : 
~~~ if not isinstance ( output , ( str , Stream ) ) : 
~~ if isinstance ( output , str ) : 
~~~ if Stream . DEFAULT_STREAM_ID not in ret : 
~~~ ret [ Stream . DEFAULT_STREAM_ID ] = list ( ) 
~~ ret [ Stream . DEFAULT_STREAM_ID ] . append ( output ) 
~~~ if output . stream_id == Stream . DEFAULT_STREAM_ID and Stream . DEFAULT_STREAM_ID in ret : 
~~~ ret [ Stream . DEFAULT_STREAM_ID ] . extend ( output . fields ) 
~~~ ret [ output . stream_id ] = output . fields 
~~ ~~ ~~ return ret 
~~ def get_out_streamids ( self ) : 
~~~ return set ( ) 
~~ ret_lst = [ ] 
for output in self . outputs : 
~~ ret_lst . append ( Stream . DEFAULT_STREAM_ID if isinstance ( output , str ) else output . stream_id ) 
~~ return set ( ret_lst ) 
~~ def _get_stream_id ( comp_name , stream_id ) : 
proto_stream_id = topology_pb2 . StreamId ( ) 
proto_stream_id . id = stream_id 
proto_stream_id . component_name = comp_name 
return proto_stream_id 
~~ def _get_stream_schema ( fields ) : 
stream_schema = topology_pb2 . StreamSchema ( ) 
for field in fields : 
~~~ key = stream_schema . keys . add ( ) 
key . key = field 
key . type = topology_pb2 . Type . Value ( "OBJECT" ) 
~~ return stream_schema 
~~ def component_id ( self ) : 
if isinstance ( self . _component_id , HeronComponentSpec ) : 
~~~ if self . _component_id . name is None : 
~~ return self . _component_id . name 
~~ elif isinstance ( self . _component_id , str ) : 
~~~ return self . _component_id 
% ( str ( type ( self . _component_id ) ) , str ( self . _component_id ) ) ) 
~~ ~~ def write_error ( self , status_code , ** kwargs ) : 
if "exc_info" in kwargs : 
~~~ exc_info = kwargs [ "exc_info" ] 
error = exc_info [ 1 ] 
self . render ( "error.html" , errormessage = errormessage ) 
~~~ errormessage = "%s" % ( status_code ) 
~~ ~~ def register_metric ( self , name , metric , time_bucket_in_sec ) : 
collector = self . get_metrics_collector ( ) 
collector . register_metric ( name , metric , time_bucket_in_sec ) 
~~ def get_sources ( self , component_id ) : 
if component_id in self . inputs : 
~~~ ret = { } 
for istream in self . inputs . get ( component_id ) : 
~~~ key = StreamId ( id = istream . stream . id , component_name = istream . stream . component_name ) 
ret [ key ] = istream . gtype 
~~ ~~ def get_component_tasks ( self , component_id ) : 
ret = [ ] 
for task_id , comp_id in self . task_to_component_map . items ( ) : 
~~~ if comp_id == component_id : 
~~~ ret . append ( task_id ) 
~~ def add_task_hook ( self , task_hook ) : 
if not isinstance ( task_hook , ITaskHook ) : 
% str ( type ( task_hook ) ) ) 
~~ self . task_hooks . append ( task_hook ) 
~~ def get_metrics_collector ( self ) : 
if self . metrics_collector is None or not isinstance ( self . metrics_collector , MetricsCollector ) : 
~~ return self . metrics_collector 
~~ def invoke_hook_prepare ( self ) : 
for task_hook in self . task_hooks : 
~~~ task_hook . prepare ( self . get_cluster_config ( ) , self ) 
~~ ~~ def invoke_hook_emit ( self , values , stream_id , out_tasks ) : 
if len ( self . task_hooks ) > 0 : 
~~~ emit_info = EmitInfo ( values = values , stream_id = stream_id , 
task_id = self . get_task_id ( ) , out_tasks = out_tasks ) 
~~~ task_hook . emit ( emit_info ) 
~~ ~~ ~~ def invoke_hook_spout_ack ( self , message_id , complete_latency_ns ) : 
~~~ spout_ack_info = SpoutAckInfo ( message_id = message_id , 
spout_task_id = self . get_task_id ( ) , 
complete_latency_ms = complete_latency_ns * 
system_constants . NS_TO_MS ) 
~~~ task_hook . spout_ack ( spout_ack_info ) 
~~ ~~ ~~ def invoke_hook_spout_fail ( self , message_id , fail_latency_ns ) : 
~~~ spout_fail_info = SpoutFailInfo ( message_id = message_id , 
fail_latency_ms = fail_latency_ns * system_constants . NS_TO_MS ) 
~~~ task_hook . spout_fail ( spout_fail_info ) 
~~ ~~ ~~ def invoke_hook_bolt_execute ( self , heron_tuple , execute_latency_ns ) : 
~~~ bolt_execute_info = BoltExecuteInfo ( heron_tuple = heron_tuple , 
executing_task_id = self . get_task_id ( ) , 
execute_latency_ms = execute_latency_ns * system_constants . NS_TO_MS ) 
~~~ task_hook . bolt_execute ( bolt_execute_info ) 
~~ ~~ ~~ def invoke_hook_bolt_ack ( self , heron_tuple , process_latency_ns ) : 
~~~ bolt_ack_info = BoltAckInfo ( heron_tuple = heron_tuple , 
acking_task_id = self . get_task_id ( ) , 
process_latency_ms = process_latency_ns * system_constants . NS_TO_MS ) 
~~~ task_hook . bolt_ack ( bolt_ack_info ) 
~~ ~~ ~~ def invoke_hook_bolt_fail ( self , heron_tuple , fail_latency_ns ) : 
~~~ bolt_fail_info = BoltFailInfo ( heron_tuple = heron_tuple , 
failing_task_id = self . get_task_id ( ) , 
~~~ task_hook . bolt_fail ( bolt_fail_info ) 
~~ ~~ ~~ def create_parser ( subparsers ) : 
'submit' , 
cli_args . add_cluster_role_env ( parser ) 
cli_args . add_topology_file ( parser ) 
cli_args . add_topology_class ( parser ) 
cli_args . add_deactive_deploy ( parser ) 
cli_args . add_dry_run ( parser ) 
cli_args . add_extra_launch_classpath ( parser ) 
cli_args . add_release_yaml_file ( parser ) 
cli_args . add_system_property ( parser ) 
cli_args . add_verbose ( parser ) 
parser . set_defaults ( subcommand = 'submit' ) 
~~ def launch_a_topology ( cl_args , tmp_dir , topology_file , topology_defn_file , topology_name ) : 
topology_pkg_path = config . normalized_class_path ( os . path . join ( tmp_dir , 'topology.tar.gz' ) ) 
release_yaml_file = cl_args [ 'release_yaml_file' ] 
config_path = cl_args [ 'config_path' ] 
tar_pkg_files = [ topology_file , topology_defn_file ] 
generated_config_files = [ release_yaml_file , cl_args [ 'override_config_file' ] ] 
config . create_tar ( topology_pkg_path , tar_pkg_files , config_path , generated_config_files ) 
args = [ 
"--config_path" , config_path , 
"--release_file" , release_yaml_file , 
"--topology_package" , topology_pkg_path , 
"--topology_defn" , topology_defn_file , 
~~~ args . append ( "--verbose" ) 
~~ if cl_args [ "dry_run" ] : 
~~~ args . append ( "--dry_run" ) 
if "dry_run_format" in cl_args : 
~~~ args += [ "--dry_run_format" , cl_args [ "dry_run_format" ] ] 
~~ ~~ lib_jars = config . get_heron_libs ( 
jars . scheduler_jars ( ) + jars . uploader_jars ( ) + jars . statemgr_jars ( ) + jars . packing_jars ( ) 
extra_jars = cl_args [ 'extra_launch_classpath' ] . split ( ':' ) 
main_class = 'org.apache.heron.scheduler.SubmitterMain' 
res = execute . heron_class ( 
class_name = main_class , 
lib_jars = lib_jars , 
extra_jars = extra_jars , 
args = args , 
java_defines = [ ] ) 
res . add_context ( err_ctxt , succ_ctxt ) 
~~ def launch_topology_server ( cl_args , topology_file , topology_defn_file , topology_name ) : 
service_apiurl = cl_args [ 'service_url' ] + rest . ROUTE_SIGNATURES [ 'submit' ] [ 1 ] 
service_method = rest . ROUTE_SIGNATURES [ 'submit' ] [ 0 ] 
data = dict ( 
name = topology_name , 
cluster = cl_args [ 'cluster' ] , 
role = cl_args [ 'role' ] , 
environment = cl_args [ 'environ' ] , 
user = cl_args [ 'submit_user' ] , 
Log . info ( "" + str ( cl_args ) ) 
if 'config_property' in cl_args : 
~~~ overrides = config . parse_override_config ( cl_args [ 'config_property' ] ) 
~~ if overrides : 
~~~ data . update ( overrides ) 
~~~ data [ "dry_run" ] = True 
~~ files = dict ( 
definition = open ( topology_defn_file , 'rb' ) , 
topology = open ( topology_file , 'rb' ) , 
~~~ r = service_method ( service_apiurl , data = data , files = files ) 
ok = r . status_code is requests . codes . ok 
created = r . status_code is requests . codes . created 
s = Status . Ok if created or ok else Status . HeronError 
if s is Status . HeronError : 
~~ elif ok : 
~~~ print ( r . json ( ) . get ( "response" ) ) 
return SimpleResult ( Status . HeronError , err_ctxt , succ_ctxt ) 
~~ return SimpleResult ( s , err_ctxt , succ_ctxt ) 
~~ def launch_topologies ( cl_args , topology_file , tmp_dir ) : 
defn_files = glob . glob ( tmp_dir + '/*.defn' ) 
if len ( defn_files ) == 0 : 
for defn_file in defn_files : 
~~~ topology_defn = topology_pb2 . Topology ( ) 
~~~ handle = open ( defn_file , "rb" ) 
topology_defn . ParseFromString ( handle . read ( ) ) 
handle . close ( ) 
return SimpleResult ( Status . HeronError , err_context ) 
if cl_args [ 'deploy_mode' ] == config . SERVER_MODE : 
~~~ res = launch_topology_server ( 
cl_args , topology_file , defn_file , topology_defn . name ) 
~~~ res = launch_a_topology ( 
cl_args , tmp_dir , topology_file , defn_file , topology_defn . name ) 
~~ results . append ( res ) 
~~ def submit_fatjar ( cl_args , unknown_args , tmp_dir ) : 
topology_file = cl_args [ 'topology-file-name' ] 
main_class = cl_args [ 'topology-class-name' ] 
lib_jars = config . get_heron_libs ( jars . topology_jars ( ) ) , 
extra_jars = [ topology_file ] , 
args = tuple ( unknown_args ) , 
java_defines = cl_args [ 'topology_main_jvm_property' ] ) 
result . render ( res ) 
if not result . is_successful ( res ) : 
res . add_context ( err_context ) 
~~ results = launch_topologies ( cl_args , topology_file , tmp_dir ) 
~~ def submit_tar ( cl_args , unknown_args , tmp_dir ) : 
java_defines = cl_args [ 'topology_main_jvm_property' ] 
res = execute . heron_tar ( 
main_class , 
topology_file , 
tuple ( unknown_args ) , 
tmp_dir , 
java_defines ) 
~~ return launch_topologies ( cl_args , topology_file , tmp_dir ) 
if urlparse . urlparse ( topology_file ) . scheme : 
~~~ cl_args [ 'topology-file-name' ] = download ( topology_file , cl_args [ 'cluster' ] ) 
~~ if not os . path . isfile ( topology_file ) : 
return SimpleResult ( Status . InvocationError , err_context ) 
~~ jar_type = topology_file . endswith ( ".jar" ) 
tar_type = topology_file . endswith ( ".tar" ) or topology_file . endswith ( ".tar.gz" ) 
pex_type = topology_file . endswith ( ".pex" ) 
cpp_type = topology_file . endswith ( ".dylib" ) or topology_file . endswith ( ".so" ) 
if not ( jar_type or tar_type or pex_type or cpp_type ) : 
~~~ _ , ext_name = os . path . splitext ( topology_file ) 
~~ if cl_args [ 'extra_launch_classpath' ] : 
~~~ valid_classpath = classpath . valid_java_classpath ( cl_args [ 'extra_launch_classpath' ] ) 
if not valid_classpath : 
~~ ~~ tmp_dir = tempfile . mkdtemp ( ) 
opts . cleaned_up_files . append ( tmp_dir ) 
if cl_args [ 'deploy_deactivated' ] : 
~~~ initial_state = topology_pb2 . TopologyState . Name ( topology_pb2 . PAUSED ) 
~~~ initial_state = topology_pb2 . TopologyState . Name ( topology_pb2 . RUNNING ) 
~~ opts . set_config ( 'cmdline.topologydefn.tmpdirectory' , tmp_dir ) 
opts . set_config ( 'cmdline.topology.initial.state' , initial_state ) 
opts . set_config ( 'cmdline.topology.role' , cl_args [ 'role' ] ) 
opts . set_config ( 'cmdline.topology.environment' , cl_args [ 'environ' ] ) 
if not cl_args [ 'release_yaml_file' ] : 
~~~ cl_args [ 'release_yaml_file' ] = config . get_heron_release_file ( ) 
~~ if jar_type : 
~~~ return submit_fatjar ( cl_args , unknown_args , tmp_dir ) 
~~ elif tar_type : 
~~~ return submit_tar ( cl_args , unknown_args , tmp_dir ) 
~~ elif cpp_type : 
~~~ return submit_cpp ( cl_args , unknown_args , tmp_dir ) 
~~~ return submit_pex ( cl_args , unknown_args , tmp_dir ) 
container = self . get_argument ( constants . PARAM_CONTAINER ) 
path = self . get_argument ( constants . PARAM_PATH ) 
offset = self . get_argument_offset ( ) 
length = self . get_argument_length ( ) 
stmgr_id = "stmgr-" + container 
stmgr = topology_info [ "physical_plan" ] [ "stmgrs" ] [ stmgr_id ] 
host = stmgr [ "host" ] 
shell_port = stmgr [ "shell_port" ] 
file_data_url = "http://%s:%d/filedata/%s?offset=%s&length=%s" % ( host , shell_port , path , offset , length ) 
http_client = tornado . httpclient . AsyncHTTPClient ( ) 
response = yield http_client . fetch ( file_data_url ) 
self . write_success_response ( json . loads ( response . body ) ) 
~~ ~~ def setup ( self , context ) : 
myindex = context . get_partition_index ( ) 
self . _files_to_consume = self . _files [ myindex : : context . get_num_partitions ( ) ] 
self . _lines_to_consume = self . _get_next_lines ( ) 
self . _emit_count = 0 
~~ def add_config ( parser ) : 
default_config_path = config . get_heron_conf_dir ( ) 
'--config-path' , 
metavar = \ + default_config_path + \ , 
default = os . path . join ( config . get_heron_dir ( ) , default_config_path ) ) 
~~ def add_verbose ( parser ) : 
'--verbose' , 
metavar = \ , 
type = bool , 
~~ def add_tracker_url ( parser ) : 
metavar = \ + DEFAULT_TRACKER_URL + \ , 
type = str , default = DEFAULT_TRACKER_URL ) 
~~ def hex_escape ( bin_str ) : 
return '' . join ( ch if ch in printable else r'0x{0:02x}' . format ( ord ( ch ) ) for ch in bin_str ) 
~~ def make_shell_endpoint ( topologyInfo , instance_id ) : 
pplan = topologyInfo [ "physical_plan" ] 
stmgrId = pplan [ "instances" ] [ instance_id ] [ "stmgrId" ] 
host = pplan [ "stmgrs" ] [ stmgrId ] [ "host" ] 
shell_port = pplan [ "stmgrs" ] [ stmgrId ] [ "shell_port" ] 
return "http://%s:%d" % ( host , shell_port ) 
~~ def make_shell_logfiles_url ( host , shell_port , _ , instance_id = None ) : 
if not shell_port : 
~~ if not instance_id : 
~~~ return "http://%s:%d/browse/log-files" % ( host , shell_port ) 
~~~ return "http://%s:%d/file/log-files/%s.log.0" % ( host , shell_port , instance_id ) 
~~ ~~ def make_shell_logfile_data_url ( host , shell_port , instance_id , offset , length ) : 
return "http://%s:%d/filedata/log-files/%s.log.0?offset=%s&length=%s" % ( host , shell_port , instance_id , offset , length ) 
~~ def cygpath ( x ) : 
command = [ 'cygpath' , '-wp' , x ] 
p = subprocess . Popen ( command , stdout = subprocess . PIPE ) 
output , _ = p . communicate ( ) 
lines = output . split ( "\\n" ) 
return lines [ 0 ] 
~~ def get_heron_tracker_dir ( ) : 
path = "/" . join ( os . path . realpath ( __file__ ) . split ( '/' ) [ : - 8 ] ) 
~~ def parse_config_file ( config_file ) : 
expanded_config_file_path = os . path . expanduser ( config_file ) 
if not os . path . lexists ( expanded_config_file_path ) : 
~~ configs = { } 
with open ( expanded_config_file_path , 'r' ) as f : 
~~~ configs = yaml . load ( f ) 
~~ return configs 
~~ def _handle_register_response ( self , response ) : 
if response . status . status != common_pb2 . StatusCode . Value ( "OK" ) : 
self . is_registered = True 
if response . HasField ( "pplan" ) : 
self . _handle_assignment_message ( response . pplan ) 
~~ ~~ def _handle_assignment_message ( self , pplan ) : 
self . heron_instance_cls . handle_assignment_msg ( pplan ) 
~~ def decode_packet ( packet ) : 
if not packet . is_complete : 
~~ data = packet . data 
len_typename = HeronProtocol . unpack_int ( data [ : 4 ] ) 
data = data [ 4 : ] 
typename = data [ : len_typename ] 
data = data [ len_typename : ] 
reqid = REQID . unpack ( data [ : REQID . REQID_SIZE ] ) 
data = data [ REQID . REQID_SIZE : ] 
len_msg = HeronProtocol . unpack_int ( data [ : 4 ] ) 
serialized_msg = data [ : len_msg ] 
return typename , reqid , serialized_msg 
~~ def create_packet ( reqid , message ) : 
assert message . IsInitialized ( ) 
packet = '' 
typename = message . DESCRIPTOR . full_name 
datasize = HeronProtocol . get_size_to_pack_string ( typename ) + REQID . REQID_SIZE + HeronProtocol . get_size_to_pack_message ( message ) 
packet += HeronProtocol . pack_int ( datasize ) 
packet += HeronProtocol . pack_int ( len ( typename ) ) 
packet += typename 
packet += reqid . pack ( ) 
packet += HeronProtocol . pack_int ( message . ByteSize ( ) ) 
packet += message . SerializeToString ( ) 
return OutgoingPacket ( packet ) 
~~ def send ( self , dispatcher ) : 
if self . sent_complete : 
~~ sent = dispatcher . send ( self . to_send ) 
self . to_send = self . to_send [ sent : ] 
~~ def create_packet ( header , data ) : 
packet = IncomingPacket ( ) 
packet . header = header 
packet . data = data 
if len ( header ) == HeronProtocol . HEADER_SIZE : 
~~~ packet . is_header_read = True 
if len ( data ) == packet . get_datasize ( ) : 
~~~ packet . is_complete = True 
~~ ~~ return packet 
~~ def read ( self , dispatcher ) : 
~~~ if not self . is_header_read : 
~~~ to_read = HeronProtocol . HEADER_SIZE - len ( self . header ) 
self . header += dispatcher . recv ( to_read ) 
if len ( self . header ) == HeronProtocol . HEADER_SIZE : 
~~~ self . is_header_read = True 
~~ ~~ if self . is_header_read and not self . is_complete : 
~~~ to_read = self . get_datasize ( ) - len ( self . data ) 
self . data += dispatcher . recv ( to_read ) 
if len ( self . data ) == self . get_datasize ( ) : 
~~~ self . is_complete = True 
~~ ~~ ~~ except socket . error as e : 
~~~ if e . errno == socket . errno . EAGAIN or e . errno == socket . errno . EWOULDBLOCK : 
~~ ~~ ~~ def generate ( ) : 
data_bytes = bytearray ( random . getrandbits ( 8 ) for i in range ( REQID . REQID_SIZE ) ) 
return REQID ( data_bytes ) 
'restart' , 
'container-id' , 
default = - 1 , 
parser . set_defaults ( subcommand = 'restart' ) 
container_id = cl_args [ 'container-id' ] 
~~~ dict_extra_args = { "container_id" : str ( container_id ) } 
~~~ list_extra_args = [ "--container_id" , str ( container_id ) ] 
~~ ~~ def get_heron_config ( ) : 
opt_list = [ ] 
for ( key , value ) in config_opts . items ( ) : 
~~~ opt_list . append ( '%s=%s' % ( key , value ) ) 
return all_opts 
~~ def yaml_config_reader ( config_path ) : 
if not config_path . endswith ( ".yaml" ) : 
~~ with open ( config_path , 'r' ) as f : 
~~~ config = yaml . load ( f ) 
~~ return config 
~~ def handle_new_tuple_set_2 ( self , hts2 ) : 
if self . my_pplan_helper is None or self . my_instance is None : 
~~~ hts = tuple_pb2 . HeronTupleSet ( ) 
if hts2 . HasField ( 'control' ) : 
~~~ hts . control . CopyFrom ( hts2 . control ) 
~~~ hdts = tuple_pb2 . HeronDataTupleSet ( ) 
hdts . stream . CopyFrom ( hts2 . data . stream ) 
~~~ for trunk in hts2 . data . tuples : 
~~~ added_tuple = hdts . tuples . add ( ) 
added_tuple . ParseFromString ( trunk ) 
~~ hts . data . CopyFrom ( hdts ) 
~~ self . in_stream . offer ( hts ) 
if self . my_pplan_helper . is_topology_running ( ) : 
~~~ self . my_instance . py_class . process_incoming_tuples ( ) 
~~ ~~ ~~ def handle_initiate_stateful_checkpoint ( self , ckptmsg ) : 
self . in_stream . offer ( ckptmsg ) 
~~ ~~ def handle_start_stateful_processing ( self , start_msg ) : 
self . is_stateful_started = True 
self . start_instance_if_possible ( ) 
~~ def handle_restore_instance_state ( self , restore_msg ) : 
if self . is_stateful_started : 
~~~ self . my_instance . py_class . stop ( ) 
self . my_instance . py_class . clear_collector ( ) 
self . is_stateful_started = False 
~~ self . in_stream . clear ( ) 
self . out_stream . clear ( ) 
if self . stateful_state is not None : 
~~~ self . stateful_state . clear ( ) 
~~ if restore_msg . state . state is not None and restore_msg . state . state : 
~~~ self . stateful_state = self . serializer . deserialize ( restore_msg . state . state ) 
~~ if self . stateful_state is None : 
~~~ self . stateful_state = HashMapState ( ) 
resp = ckptmgr_pb2 . RestoreInstanceStateResponse ( ) 
resp . status . status = common_pb2 . StatusCode . Value ( "OK" ) 
resp . checkpoint_id = restore_msg . state . checkpoint_id 
self . _stmgr_client . send_message ( resp ) 
~~ def send_buffered_messages ( self ) : 
while not self . out_stream . is_empty ( ) and self . _stmgr_client . is_registered : 
~~~ tuple_set = self . out_stream . poll ( ) 
if isinstance ( tuple_set , tuple_pb2 . HeronTupleSet ) : 
~~~ tuple_set . src_task_id = self . my_pplan_helper . my_task_id 
self . gateway_metrics . update_sent_packet ( tuple_set . ByteSize ( ) ) 
~~ self . _stmgr_client . send_message ( tuple_set ) 
~~ ~~ def _handle_state_change_msg ( self , new_helper ) : 
assert self . my_pplan_helper is not None 
assert self . my_instance is not None and self . my_instance . py_class is not None 
if self . my_pplan_helper . get_topology_state ( ) != new_helper . get_topology_state ( ) : 
~~~ self . my_pplan_helper = new_helper 
if new_helper . is_topology_running ( ) : 
~~~ if not self . is_instance_started : 
~~~ self . start_instance_if_possible ( ) 
~~ self . my_instance . py_class . invoke_activate ( ) 
~~ elif new_helper . is_topology_paused ( ) : 
~~~ self . my_instance . py_class . invoke_deactivate ( ) 
~~ ~~ def handle_assignment_msg ( self , pplan ) : 
new_helper = PhysicalPlanHelper ( pplan , self . instance . instance_id , 
self . topo_pex_file_abs_path ) 
if self . my_pplan_helper is not None and ( self . my_pplan_helper . my_component_name != new_helper . my_component_name or 
self . my_pplan_helper . my_task_id != new_helper . my_task_id ) : 
~~ new_helper . set_topology_context ( self . metrics_collector ) 
if self . my_pplan_helper is None : 
self . _handle_assignment_msg ( new_helper ) 
self . my_pplan_helper . get_topology_state ( ) , new_helper . get_topology_state ( ) ) 
self . _handle_state_change_msg ( new_helper ) 
~~ ~~ def check_output_schema ( self , stream_id , tup ) : 
size = self . _output_schema . get ( stream_id , None ) 
% ( self . my_component_name , stream_id ) ) 
~~ elif size != len ( tup ) : 
~~ ~~ def get_topology_config ( self ) : 
if self . pplan . topology . HasField ( "topology_config" ) : 
~~~ return self . _get_dict_from_config ( self . pplan . topology . topology_config ) 
~~ ~~ def set_topology_context ( self , metrics_collector ) : 
cluster_config = self . get_topology_config ( ) 
cluster_config . update ( self . _get_dict_from_config ( self . my_component . config ) ) 
task_to_component_map = self . _get_task_to_comp_map ( ) 
self . context = TopologyContextImpl ( cluster_config , self . pplan . topology , task_to_component_map , 
self . my_task_id , metrics_collector , 
self . topology_pex_abs_path ) 
~~ def _get_dict_from_config ( topology_config ) : 
for kv in topology_config . kvs : 
~~~ if kv . HasField ( "value" ) : 
~~~ assert kv . type == topology_pb2 . ConfigValueType . Value ( "STRING_VALUE" ) 
if PhysicalPlanHelper . _is_number ( kv . value ) : 
~~~ config [ kv . key ] = PhysicalPlanHelper . _get_number ( kv . value ) 
~~ elif kv . value . lower ( ) in ( "true" , "false" ) : 
~~~ config [ kv . key ] = True if kv . value . lower ( ) == "true" else False 
~~ ~~ elif kv . HasField ( "serialized_value" ) and kv . type == topology_pb2 . ConfigValueType . Value ( "PYTHON_SERIALIZED_VALUE" ) : 
~~~ config [ kv . key ] = default_serializer . deserialize ( kv . serialized_value ) 
~~~ assert kv . HasField ( "type" ) 
% ( str ( kv ) , str ( kv . type ) ) ) 
~~ def _setup_custom_grouping ( self , topology ) : 
for i in range ( len ( topology . bolts ) ) : 
~~~ for in_stream in topology . bolts [ i ] . inputs : 
~~~ if in_stream . stream . component_name == self . my_component_name and in_stream . gtype == topology_pb2 . Grouping . Value ( "CUSTOM" ) : 
~~~ if in_stream . type == topology_pb2 . CustomGroupingObjectType . Value ( "PYTHON_OBJECT" ) : 
~~~ custom_grouping_obj = default_serializer . deserialize ( in_stream . custom_grouping_object ) 
if isinstance ( custom_grouping_obj , str ) : 
~~~ pex_loader . load_pex ( self . topology_pex_abs_path ) 
grouping_cls = pex_loader . import_and_get_class ( self . topology_pex_abs_path , custom_grouping_obj ) 
custom_grouping_obj = grouping_cls ( ) 
~~ assert isinstance ( custom_grouping_obj , ICustomGrouping ) 
self . custom_grouper . add ( in_stream . stream . id , 
self . _get_taskids_for_component ( topology . bolts [ i ] . comp . name ) , 
custom_grouping_obj , 
self . my_component_name ) 
~~ elif in_stream . type == topology_pb2 . CustomGroupingObjectType . Value ( "JAVA_OBJECT" ) : 
~~ ~~ ~~ ~~ ~~ def add ( self , stream_id , task_ids , grouping , source_comp_name ) : 
if stream_id not in self . targets : 
~~~ self . targets [ stream_id ] = [ ] 
~~ self . targets [ stream_id ] . append ( Target ( task_ids , grouping , source_comp_name ) ) 
~~ def prepare ( self , context ) : 
for stream_id , targets in self . targets . items ( ) : 
~~~ for target in targets : 
~~~ target . prepare ( context , stream_id ) 
~~ ~~ ~~ def choose_tasks ( self , stream_id , values ) : 
~~ ret = [ ] 
for target in self . targets [ stream_id ] : 
~~~ ret . extend ( target . choose_tasks ( values ) ) 
~~ def prepare ( self , context , stream_id ) : 
self . grouping . prepare ( context , self . source_comp_name , stream_id , self . task_ids ) 
~~ def choose_tasks ( self , values ) : 
ret = self . grouping . choose_tasks ( values ) 
if not isinstance ( ret , list ) : 
~~~ for i in ret : 
~~~ if not isinstance ( i , int ) : 
~~ if i not in self . task_ids : 
~~ ~~ def add_config ( parser ) : 
default = os . path . join ( config . get_heron_dir ( ) , default_config_path ) , 
'--config-property' , 
metavar = 'PROPERTY=VALUE' , 
default = [ ] , 
~~ def add_dry_run ( parser ) : 
default_format = 'table' 
resp_formats = [ 'raw' , 'table' , 'colored_table' , 'json' ] 
def dry_run_resp_format ( value ) : 
~~~ if value not in resp_formats : 
% ( value , available_options ) ) 
'--dry-run' , 
'--dry-run-format' , 
metavar = 'DRY_RUN_FORMAT' , 
default = 'colored_table' if sys . stdout . isatty ( ) else 'table' , 
type = dry_run_resp_format , 
~~ def read_server_mode_cluster_definition ( cluster , cl_args ) : 
client_confs [ cluster ] = cliconfig . cluster_config ( cluster ) 
if cl_args . get ( 'service_url' , None ) : 
~~~ client_confs [ cluster ] [ 'service_url' ] = cl_args [ 'service_url' ] 
~~ return client_confs 
~~ def check_direct_mode_cluster_definition ( cluster , config_path ) : 
config_path = config . get_heron_cluster_conf_dir ( cluster , config_path ) 
~~ t = Template ( utils . get_asset ( "browse.html" ) ) 
args = dict ( 
listing = utils . get_listing ( path ) , 
format_prefix = utils . format_prefix , 
stat = stat , 
get_stat = utils . get_stat , 
os = os , 
css = utils . get_asset ( "bootstrap.css" ) 
~~ def format_mode ( sres ) : 
mode = sres . st_mode 
root = ( mode & 0o700 ) >> 6 
group = ( mode & 0o070 ) >> 3 
user = ( mode & 0o7 ) 
def stat_type ( md ) : 
if stat . S_ISDIR ( md ) : 
~~~ return 'd' 
~~ elif stat . S_ISSOCK ( md ) : 
~~~ return 's' 
~~~ return '-' 
~~ ~~ def triple ( md ) : 
return '%c%c%c' % ( 
'r' if md & 0b100 else '-' , 
'w' if md & 0b010 else '-' , 
'x' if md & 0b001 else '-' ) 
~~ return '' . join ( [ stat_type ( mode ) , triple ( root ) , triple ( group ) , triple ( user ) ] ) 
~~ def format_mtime ( mtime ) : 
now = datetime . now ( ) 
dt = datetime . fromtimestamp ( mtime ) 
dt . strftime ( '%b' ) , dt . day , 
dt . year if dt . year != now . year else dt . strftime ( '%H:%M' ) ) 
~~ def format_prefix ( filename , sres ) : 
~~~ pwent = pwd . getpwuid ( sres . st_uid ) 
user = pwent . pw_name 
~~~ user = sres . st_uid 
~~~ grent = grp . getgrgid ( sres . st_gid ) 
group = grent . gr_name 
~~~ group = sres . st_gid 
format_mode ( sres ) , 
sres . st_nlink , 
user , 
group , 
sres . st_size , 
format_mtime ( sres . st_mtime ) , 
~~ def get_listing ( path ) : 
if path != "." : 
~~~ listing = sorted ( [ '..' ] + os . listdir ( path ) ) 
~~~ listing = sorted ( os . listdir ( path ) ) 
~~ return listing 
~~ def get_stat ( path , filename ) : 
return os . stat ( os . path . join ( path , filename ) ) 
~~ def read_chunk ( filename , offset = - 1 , length = - 1 , escape_data = False ) : 
~~~ length = int ( length ) 
offset = int ( offset ) 
~~ if not os . path . isfile ( filename ) : 
~~~ fstat = os . stat ( filename ) 
~~ if offset == - 1 : 
~~~ offset = fstat . st_size 
~~ if length == - 1 : 
~~~ length = fstat . st_size - offset 
~~ with open ( filename , "r" ) as fp : 
~~~ fp . seek ( offset ) 
~~~ data = fp . read ( length ) 
~~ ~~ if data : 
~~~ data = _escape_data ( data ) if escape_data else data 
return dict ( offset = offset , length = len ( data ) , data = data ) 
~~ return dict ( offset = offset , length = 0 ) 
~~ def pipe ( prev_proc , to_cmd ) : 
stdin = None if prev_proc is None else prev_proc . stdout 
process = subprocess . Popen ( to_cmd , 
stdin = stdin ) 
if prev_proc is not None : 
~~ def str_cmd ( cmd , cwd , env ) : 
process = subprocess . Popen ( cmd , stdout = subprocess . PIPE , 
stderr = subprocess . PIPE , cwd = cwd , env = env ) 
stdout_builder , stderr_builder = proc . async_stdout_stderr_builder ( process ) 
stdout , stderr = stdout_builder . result ( ) , stderr_builder . result ( ) 
~~ def chain ( cmd_list ) : 
chained_proc = functools . reduce ( pipe , [ None ] + cmd_list ) 
stdout_builder = proc . async_stdout_builder ( chained_proc ) 
chained_proc . wait ( ) 
'command' : command , 
'stdout' : stdout_builder . result ( ) 
metrics_parser = subparsers . add_parser ( 
'metrics' , 
args . add_cluster_role_env ( metrics_parser ) 
args . add_topology_name ( metrics_parser ) 
args . add_verbose ( metrics_parser ) 
args . add_tracker_url ( metrics_parser ) 
args . add_config ( metrics_parser ) 
args . add_component_name ( metrics_parser ) 
metrics_parser . set_defaults ( subcommand = 'metrics' ) 
containers_parser = subparsers . add_parser ( 
'containers' , 
args . add_cluster_role_env ( containers_parser ) 
args . add_topology_name ( containers_parser ) 
args . add_verbose ( containers_parser ) 
args . add_tracker_url ( containers_parser ) 
args . add_config ( containers_parser ) 
args . add_container_id ( containers_parser ) 
containers_parser . set_defaults ( subcommand = 'containers' ) 
~~ def parse_topo_loc ( cl_args ) : 
~~~ topo_loc = cl_args [ 'cluster/[role]/[env]' ] . split ( '/' ) 
topo_name = cl_args [ 'topology-name' ] 
topo_loc . append ( topo_name ) 
if len ( topo_loc ) != 4 : 
~~ return topo_loc 
~~ ~~ def to_table ( metrics ) : 
all_queries = tracker_access . metric_queries ( ) 
m = tracker_access . queries_map ( ) 
names = metrics . values ( ) [ 0 ] . keys ( ) 
for n in names : 
~~~ info = [ n ] 
for field in all_queries : 
~~~ info . append ( str ( metrics [ field ] [ n ] ) ) 
~~ ~~ stats . append ( info ) 
return stats , header 
~~ def run_metrics ( command , parser , cl_args , unknown_args ) : 
~~~ result = tracker_access . get_topology_info ( cluster , env , topology , role ) 
spouts = result [ 'physical_plan' ] [ 'spouts' ] . keys ( ) 
bolts = result [ 'physical_plan' ] [ 'bolts' ] . keys ( ) 
components = spouts + bolts 
cname = cl_args [ 'component' ] 
if cname : 
~~~ if cname in components : 
~~~ components = [ cname ] 
~~ ~~ ~~ except Exception : 
~~ cresult = [ ] 
~~~ metrics = tracker_access . get_component_metrics ( comp , cluster , env , topology , role ) 
~~ stat , header = to_table ( metrics ) 
cresult . append ( ( comp , stat , header ) ) 
~~ for i , ( comp , stat , header ) in enumerate ( cresult ) : 
~~~ if i != 0 : 
~~~ print ( '' ) 
print ( tabulate ( stat , headers = header ) ) 
~~ def run_bolts ( command , parser , cl_args , unknown_args ) : 
bolt_name = cl_args [ 'bolt' ] 
if bolt_name : 
~~~ if bolt_name in bolts : 
~~~ bolts = [ bolt_name ] 
~~ bolts_result = [ ] 
for bolt in bolts : 
~~~ metrics = tracker_access . get_component_metrics ( bolt , cluster , env , topology , role ) 
stat , header = to_table ( metrics ) 
bolts_result . append ( ( bolt , stat , header ) ) 
~~ ~~ for i , ( bolt , stat , header ) in enumerate ( bolts_result ) : 
~~ def run_containers ( command , parser , cl_args , unknown_args ) : 
container_id = cl_args [ 'id' ] 
~~ containers = result [ 'physical_plan' ] [ 'stmgrs' ] 
all_bolts , all_spouts = set ( ) , set ( ) 
for _ , bolts in result [ 'physical_plan' ] [ 'bolts' ] . items ( ) : 
~~~ all_bolts = all_bolts | set ( bolts ) 
~~ for _ , spouts in result [ 'physical_plan' ] [ 'spouts' ] . items ( ) : 
~~~ all_spouts = all_spouts | set ( spouts ) 
~~ stmgrs = containers . keys ( ) 
stmgrs . sort ( ) 
if container_id is not None : 
~~~ normalized_cid = container_id - 1 
if normalized_cid < 0 : 
~~ stmgrs = [ stmgrs [ normalized_cid ] ] 
~~ ~~ table = [ ] 
for sid , name in enumerate ( stmgrs ) : 
~~~ cid = sid + 1 
host = containers [ name ] [ "host" ] 
port = containers [ name ] [ "port" ] 
pid = containers [ name ] [ "pid" ] 
instances = containers [ name ] [ "instance_ids" ] 
bolt_nums = len ( [ instance for instance in instances if instance in all_bolts ] ) 
spout_nums = len ( [ instance for instance in instances if instance in all_spouts ] ) 
table . append ( [ cid , host , port , pid , bolt_nums , spout_nums , len ( instances ) ] ) 
~~ headers = [ "container" , "host" , "port" , "pid" , "#bolt" , "#spout" , "#instance" ] 
sys . stdout . flush ( ) 
print ( tabulate ( table , headers = headers ) ) 
~~ def define_options ( address , port , tracker_url , base_url ) : 
define ( "address" , default = address ) 
define ( "port" , default = port ) 
define ( "tracker_url" , default = tracker_url ) 
define ( "base_url" , default = base_url ) 
log . configure ( logging . DEBUG ) 
tornado . log . enable_pretty_logging ( ) 
( parser , child_parser ) = args . create_parsers ( ) 
( parsed_args , remaining ) = parser . parse_known_args ( ) 
~~~ r = child_parser . parse_args ( args = remaining , namespace = parsed_args ) 
namespace = vars ( r ) 
if 'version' in namespace : 
~~~ common_config . print_build_info ( zipped_pex = True ) 
~~ parser . exit ( ) 
~~ command_line_args = vars ( parsed_args ) 
command_line_args [ 'port' ] , command_line_args [ 'base_url' ] ) 
define_options ( command_line_args [ 'address' ] , 
command_line_args [ 'port' ] , 
command_line_args [ 'tracker_url' ] , 
command_line_args [ 'base_url' ] ) 
http_server = tornado . httpserver . HTTPServer ( Application ( command_line_args [ 'base_url' ] ) ) 
http_server . listen ( command_line_args [ 'port' ] , address = command_line_args [ 'address' ] ) 
def signal_handler ( signum , frame ) : 
~~~ print ( '\\n' , end = '' ) 
tornado . ioloop . IOLoop . instance ( ) . stop ( ) 
~~ signal . signal ( signal . SIGINT , signal_handler ) 
tornado . ioloop . IOLoop . instance ( ) . start ( ) 
~~ def spec ( cls , name = None , inputs = None , par = 1 , config = None , optional_outputs = None ) : 
python_class_path = "%s.%s" % ( cls . __module__ , cls . __name__ ) 
if hasattr ( cls , 'outputs' ) : 
~~~ _outputs = copy . copy ( cls . outputs ) 
~~~ _outputs = [ ] 
~~ if optional_outputs is not None : 
~~~ assert isinstance ( optional_outputs , ( list , tuple ) ) 
for out in optional_outputs : 
~~~ assert isinstance ( out , ( str , Stream ) ) 
_outputs . append ( out ) 
~~ ~~ return HeronComponentSpec ( name , python_class_path , is_spout = False , par = par , 
inputs = inputs , outputs = _outputs , config = config ) 
~~ def emit ( self , tup , stream = Stream . DEFAULT_STREAM_ID , 
self . delegate . emit ( tup , stream , anchors , direct_task , need_task_ids ) 
'clusters' , 
args . add_tracker_url ( parser ) 
parser . set_defaults ( subcommand = 'clusters' ) 
~~~ clusters = tracker_access . get_clusters ( ) 
for cluster in clusters : 
~~ def get_time_ranges ( ranges ) : 
now = int ( time . time ( ) ) 
time_slots = dict ( ) 
for key , value in ranges . items ( ) : 
~~~ time_slots [ key ] = ( now - value [ 0 ] , now - value [ 1 ] , value [ 2 ] ) 
~~ return ( now , time_slots ) 
default_config_file = os . path . join ( 
utils . get_heron_tracker_conf_dir ( ) , constants . DEFAULT_CONFIG_FILE ) 
'--config-file' , 
metavar = \ + default_config_file + \ , 
default = default_config_file ) 
'--type' , 
choices = [ "file" , "zookeeper" ] ) 
'--name' , 
'--rootpath' , 
'--tunnelhost' , 
'--hostport' , 
default = constants . DEFAULT_PORT ) 
action = 'store_true' ) 
~~ def create_parsers ( ) : 
parser = add_titles ( parser ) 
parser = add_arguments ( parser ) 
ya_parser = argparse . ArgumentParser ( 
parents = [ parser ] , 
formatter_class = SubcommandHelpFormatter , 
subparsers = ya_parser . add_subparsers ( 
help_parser = subparsers . add_parser ( 
help_parser . set_defaults ( help = True ) 
subparsers . add_parser ( 
return parser , ya_parser 
( parser , _ ) = create_parsers ( ) 
( args , remaining ) = parser . parse_known_args ( ) 
if remaining == [ 'help' ] : 
parser . exit ( ) 
~~ elif remaining == [ 'version' ] : 
~~~ common_config . print_build_info ( ) 
~~ elif remaining != [ ] : 
~~ namespace = vars ( args ) 
log . set_logging_level ( namespace ) 
define_options ( namespace [ 'port' ] , namespace [ 'config_file' ] ) 
config = Config ( create_tracker_config ( namespace ) ) 
application = Application ( config ) 
application . stop ( ) 
if namespace [ "config_file" ] : 
http_server = tornado . httpserver . HTTPServer ( application ) 
http_server . listen ( namespace [ 'port' ] ) 
~~ def make_tuple ( stream , tuple_key , values , roots = None ) : 
component_name = stream . component_name 
stream_id = stream . id 
gen_task = roots [ 0 ] . taskid if roots is not None and len ( roots ) > 0 else None 
return HeronTuple ( id = str ( tuple_key ) , component = component_name , stream = stream_id , 
task = gen_task , values = values , creation_time = time . time ( ) , roots = roots ) 
~~ def make_tick_tuple ( ) : 
return HeronTuple ( id = TupleHelper . TICK_TUPLE_ID , component = TupleHelper . TICK_SOURCE_COMPONENT , 
stream = TupleHelper . TICK_TUPLE_ID , task = None , values = None , 
creation_time = time . time ( ) , roots = None ) 
~~ def make_root_tuple_info ( stream_id , tuple_id ) : 
key = random . getrandbits ( TupleHelper . MAX_SFIXED64_RAND_BITS ) 
return RootTupleInfo ( stream_id = stream_id , tuple_id = tuple_id , 
insertion_time = time . time ( ) , key = key ) 
pass 
~~ def ParseNolintSuppressions ( filename , raw_line , linenum , error ) : 
matched = Search ( r'\\bNOLINT(NEXTLINE)?\\b(\\([^)]+\\))?' , raw_line ) 
if matched : 
~~~ if matched . group ( 1 ) : 
~~~ suppressed_line = linenum + 1 
~~~ suppressed_line = linenum 
~~ category = matched . group ( 2 ) 
~~~ _error_suppressions . setdefault ( None , set ( ) ) . add ( suppressed_line ) 
~~~ if category . startswith ( '(' ) and category . endswith ( ')' ) : 
~~~ category = category [ 1 : - 1 ] 
if category in _ERROR_CATEGORIES : 
~~~ _error_suppressions . setdefault ( category , set ( ) ) . add ( suppressed_line ) 
~~ elif category not in _LEGACY_ERROR_CATEGORIES : 
~~~ error ( filename , linenum , 'readability/nolint' , 5 , 
~~ ~~ ~~ ~~ ~~ def ProcessGlobalSuppresions ( lines ) : 
~~~ if _SEARCH_C_FILE . search ( line ) : 
~~~ for category in _DEFAULT_C_SUPPRESSED_CATEGORIES : 
~~~ _global_error_suppressions [ category ] = True 
~~ ~~ if _SEARCH_KERNEL_FILE . search ( line ) : 
~~~ for category in _DEFAULT_KERNEL_SUPPRESSED_CATEGORIES : 
~~ ~~ ~~ ~~ def IsErrorSuppressedByNolint ( category , linenum ) : 
return ( _global_error_suppressions . get ( category , False ) or 
linenum in _error_suppressions . get ( category , set ( ) ) or 
linenum in _error_suppressions . get ( None , set ( ) ) ) 
~~ def Match ( pattern , s ) : 
if pattern not in _regexp_compile_cache : 
~~~ _regexp_compile_cache [ pattern ] = sre_compile . compile ( pattern ) 
~~ return _regexp_compile_cache [ pattern ] . match ( s ) 
~~ def ReplaceAll ( pattern , rep , s ) : 
~~ return _regexp_compile_cache [ pattern ] . sub ( rep , s ) 
~~ def Search ( pattern , s ) : 
~~ return _regexp_compile_cache [ pattern ] . search ( s ) 
~~ def _ShouldPrintError ( category , confidence , linenum ) : 
if IsErrorSuppressedByNolint ( category , linenum ) : 
~~ if confidence < _cpplint_state . verbose_level : 
~~ is_filtered = False 
for one_filter in _Filters ( ) : 
~~~ if one_filter . startswith ( '-' ) : 
~~~ if category . startswith ( one_filter [ 1 : ] ) : 
~~~ is_filtered = True 
~~ ~~ elif one_filter . startswith ( '+' ) : 
~~~ is_filtered = False 
~~ ~~ if is_filtered : 
~~ def IsCppString ( line ) : 
return ( ( line . count ( \ ) - line . count ( r\ ) - line . count ( "\ ) ) & 1 ) == 1 
~~ def CleanseRawStrings ( raw_lines ) : 
delimiter = None 
lines_without_raw_strings = [ ] 
for line in raw_lines : 
~~~ if delimiter : 
~~~ end = line . find ( delimiter ) 
if end >= 0 : 
~~~ leading_space = Match ( r'^(\\s*)\\S' , line ) 
line = leading_space . group ( 1 ) + \ + line [ end + len ( delimiter ) : ] 
~~~ line = \ 
~~ ~~ while delimiter is None : 
~~~ matched = Match ( r\ , line ) 
if ( matched and 
not Match ( r\ , 
matched . group ( 1 ) ) ) : 
~~~ delimiter = ')' + matched . group ( 2 ) + \ 
end = matched . group ( 3 ) . find ( delimiter ) 
~~~ line = ( matched . group ( 1 ) + \ + 
matched . group ( 3 ) [ end + len ( delimiter ) : ] ) 
~~~ line = matched . group ( 1 ) + \ 
~~ ~~ lines_without_raw_strings . append ( line ) 
~~ return lines_without_raw_strings 
~~ def FindNextMultiLineCommentStart ( lines , lineix ) : 
while lineix < len ( lines ) : 
~~~ if lines [ lineix ] . strip ( ) . startswith ( '/*' ) : 
~~~ if lines [ lineix ] . strip ( ) . find ( '*/' , 2 ) < 0 : 
~~~ return lineix 
~~ ~~ lineix += 1 
~~ return len ( lines ) 
~~ def FindNextMultiLineCommentEnd ( lines , lineix ) : 
~~~ if lines [ lineix ] . strip ( ) . endswith ( '*/' ) : 
~~ lineix += 1 
~~ def RemoveMultiLineCommentsFromRange ( lines , begin , end ) : 
for i in range ( begin , end ) : 
~~~ lines [ i ] = '/**/' 
~~ ~~ def RemoveMultiLineComments ( filename , lines , error ) : 
lineix = 0 
~~~ lineix_begin = FindNextMultiLineCommentStart ( lines , lineix ) 
if lineix_begin >= len ( lines ) : 
~~ lineix_end = FindNextMultiLineCommentEnd ( lines , lineix_begin ) 
if lineix_end >= len ( lines ) : 
~~~ error ( filename , lineix_begin + 1 , 'readability/multiline_comment' , 5 , 
~~ RemoveMultiLineCommentsFromRange ( lines , lineix_begin , lineix_end + 1 ) 
lineix = lineix_end + 1 
~~ ~~ def CleanseComments ( line ) : 
commentpos = line . find ( '//' ) 
if commentpos != - 1 and not IsCppString ( line [ : commentpos ] ) : 
~~~ line = line [ : commentpos ] . rstrip ( ) 
~~ return _RE_PATTERN_CLEANSE_LINE_C_COMMENTS . sub ( '' , line ) 
~~ def FindEndOfExpressionInLine ( line , startpos , stack ) : 
for i in xrange ( startpos , len ( line ) ) : 
~~~ char = line [ i ] 
if char in '([{' : 
~~~ stack . append ( char ) 
~~ elif char == '<' : 
~~~ if i > 0 and line [ i - 1 ] == '<' : 
~~~ if stack and stack [ - 1 ] == '<' : 
~~~ stack . pop ( ) 
if not stack : 
~~~ return ( - 1 , None ) 
~~ ~~ ~~ elif i > 0 and Search ( r'\\boperator\\s*$' , line [ 0 : i ] ) : 
~~~ stack . append ( '<' ) 
~~ ~~ elif char in ')]}' : 
~~~ while stack and stack [ - 1 ] == '<' : 
~~ if not stack : 
~~ if ( ( stack [ - 1 ] == '(' and char == ')' ) or 
( stack [ - 1 ] == '[' and char == ']' ) or 
( stack [ - 1 ] == '{' and char == '}' ) ) : 
~~~ return ( i + 1 , None ) 
~~ ~~ elif char == '>' : 
~~~ if ( i > 0 and 
( line [ i - 1 ] == '-' or Search ( r'\\boperator\\s*$' , line [ 0 : i - 1 ] ) ) ) : 
~~ if stack : 
~~~ if stack [ - 1 ] == '<' : 
~~ ~~ ~~ ~~ elif char == ';' : 
~~ ~~ ~~ return ( - 1 , stack ) 
~~ def CloseExpression ( clean_lines , linenum , pos ) : 
line = clean_lines . elided [ linenum ] 
if ( line [ pos ] not in '({[<' ) or Match ( r'<[<=]' , line [ pos : ] ) : 
~~~ return ( line , clean_lines . NumLines ( ) , - 1 ) 
~~ ( end_pos , stack ) = FindEndOfExpressionInLine ( line , pos , [ ] ) 
if end_pos > - 1 : 
~~~ return ( line , linenum , end_pos ) 
~~ while stack and linenum < clean_lines . NumLines ( ) - 1 : 
~~~ linenum += 1 
( end_pos , stack ) = FindEndOfExpressionInLine ( line , 0 , stack ) 
~~ ~~ return ( line , clean_lines . NumLines ( ) , - 1 ) 
~~ def FindStartOfExpressionInLine ( line , endpos , stack ) : 
i = endpos 
while i >= 0 : 
if char in ')]}' : 
~~ elif char == '>' : 
( line [ i - 1 ] == '-' or 
Match ( r'\\s>=\\s' , line [ i - 1 : ] ) or 
Search ( r'\\boperator\\s*$' , line [ 0 : i ] ) ) ) : 
~~~ i -= 1 
~~~ stack . append ( '>' ) 
~~ ~~ elif char == '<' : 
~~~ if stack and stack [ - 1 ] == '>' : 
~~~ return ( i , None ) 
~~ ~~ ~~ ~~ elif char in '([{' : 
~~~ while stack and stack [ - 1 ] == '>' : 
~~ if ( ( char == '(' and stack [ - 1 ] == ')' ) or 
( char == '[' and stack [ - 1 ] == ']' ) or 
( char == '{' and stack [ - 1 ] == '}' ) ) : 
~~ ~~ elif char == ';' : 
~~ ~~ i -= 1 
~~ return ( - 1 , stack ) 
~~ def ReverseCloseExpression ( clean_lines , linenum , pos ) : 
if line [ pos ] not in ')}]>' : 
~~~ return ( line , 0 , - 1 ) 
~~ ( start_pos , stack ) = FindStartOfExpressionInLine ( line , pos , [ ] ) 
if start_pos > - 1 : 
~~~ return ( line , linenum , start_pos ) 
~~ while stack and linenum > 0 : 
~~~ linenum -= 1 
( start_pos , stack ) = FindStartOfExpressionInLine ( line , len ( line ) - 1 , stack ) 
~~ ~~ return ( line , 0 , - 1 ) 
~~ def CheckForCopyright ( filename , lines , error ) : 
for line in range ( 1 , min ( len ( lines ) , 11 ) ) : 
~~~ if re . search ( r'Copyright' , lines [ line ] , re . I ) : break 
~~~ error ( filename , 0 , 'legal/copyright' , 5 , 
~~ ~~ def GetIndentLevel ( line ) : 
if indent : 
~~~ return len ( indent . group ( 1 ) ) 
~~ ~~ def GetHeaderGuardCPPVariable ( filename ) : 
filename = re . sub ( r'_flymake\\.h$' , '.h' , filename ) 
filename = re . sub ( r'/\\.flymake/([^/]*)$' , r'/\\1' , filename ) 
filename = filename . replace ( 'C++' , 'cpp' ) . replace ( 'c++' , 'cpp' ) 
fileinfo = FileInfo ( filename ) 
file_path_from_root = fileinfo . RepositoryName ( ) 
if _root : 
~~~ suffix = os . sep 
if suffix == '\\\\' : 
~~~ suffix += '\\\\' 
~~ file_path_from_root = re . sub ( '^' + _root + suffix , '' , file_path_from_root ) 
~~ return re . sub ( r'[^a-zA-Z0-9]' , '_' , file_path_from_root ) . upper ( ) + '_' 
~~ def CheckForHeaderGuard ( filename , clean_lines , error ) : 
raw_lines = clean_lines . lines_without_raw_strings 
for i in raw_lines : 
~~~ if Search ( r'//\\s*NOLINT\\(build/header_guard\\)' , i ) : 
~~ ~~ for i in raw_lines : 
~~~ if Search ( r'^\\s*#pragma\\s+once' , i ) : 
~~ ~~ cppvar = GetHeaderGuardCPPVariable ( filename ) 
ifndef = '' 
ifndef_linenum = 0 
define = '' 
endif = '' 
endif_linenum = 0 
for linenum , line in enumerate ( raw_lines ) : 
~~~ linesplit = line . split ( ) 
if len ( linesplit ) >= 2 : 
~~~ if not ifndef and linesplit [ 0 ] == '#ifndef' : 
~~~ ifndef = linesplit [ 1 ] 
ifndef_linenum = linenum 
~~ if not define and linesplit [ 0 ] == '#define' : 
~~~ define = linesplit [ 1 ] 
~~ ~~ if line . startswith ( '#endif' ) : 
~~~ endif = line 
endif_linenum = linenum 
~~ ~~ if not ifndef or not define or ifndef != define : 
~~~ error ( filename , 0 , 'build/header_guard' , 5 , 
cppvar ) 
~~ if ifndef != cppvar : 
~~~ error_level = 0 
if ifndef != cppvar + '_' : 
~~~ error_level = 5 
~~ ParseNolintSuppressions ( filename , raw_lines [ ifndef_linenum ] , ifndef_linenum , 
error ) 
error ( filename , ifndef_linenum , 'build/header_guard' , error_level , 
~~ ParseNolintSuppressions ( filename , raw_lines [ endif_linenum ] , endif_linenum , 
match = Match ( r'#endif\\s*//\\s*' + cppvar + r'(_)?\\b' , endif ) 
~~~ if match . group ( 1 ) == '_' : 
~~~ error ( filename , endif_linenum , 'build/header_guard' , 0 , 
\ % cppvar ) 
~~ no_single_line_comments = True 
for i in xrange ( 1 , len ( raw_lines ) - 1 ) : 
~~~ line = raw_lines [ i ] 
if Match ( r\ , line ) : 
~~~ no_single_line_comments = False 
~~ ~~ if no_single_line_comments : 
~~~ match = Match ( r'#endif\\s*/\\*\\s*' + cppvar + r'(_)?\\s*\\*/' , endif ) 
~~ ~~ error ( filename , endif_linenum , 'build/header_guard' , 5 , 
~~ def CheckHeaderFileIncluded ( filename , include_state , error ) : 
if Search ( _TEST_FILE_SUFFIX , fileinfo . BaseName ( ) ) : 
~~ for ext in GetHeaderExtensions ( ) : 
~~~ basefilename = filename [ 0 : len ( filename ) - len ( fileinfo . Extension ( ) ) ] 
headerfile = basefilename + '.' + ext 
if not os . path . exists ( headerfile ) : 
~~ headername = FileInfo ( headerfile ) . RepositoryName ( ) 
first_include = None 
for section_list in include_state . include_list : 
~~~ for f in section_list : 
~~~ if headername in f [ 0 ] or f [ 0 ] in headername : 
~~ if not first_include : 
~~~ first_include = f [ 1 ] 
~~ ~~ ~~ error ( filename , first_include , 'build/include' , 5 , 
headername ) ) 
~~ ~~ def CheckForBadCharacters ( filename , lines , error ) : 
for linenum , line in enumerate ( lines ) : 
~~~ if unicode_escape_decode ( '\\ufffd' ) in line : 
~~~ error ( filename , linenum , 'readability/utf8' , 5 , 
~~ if '\\0' in line : 
~~ ~~ ~~ def CheckForNewlineAtEOF ( filename , lines , error ) : 
if len ( lines ) < 3 or lines [ - 2 ] : 
~~~ error ( filename , len ( lines ) - 2 , 'whitespace/ending_newline' , 5 , 
~~ ~~ def CheckForMultilineCommentsAndStrings ( filename , clean_lines , linenum , error ) : 
line = line . replace ( '\\\\\\\\' , '' ) 
if line . count ( '/*' ) > line . count ( '*/' ) : 
~~~ error ( filename , linenum , 'readability/multiline_comment' , 5 , 
~~ if ( line . count ( \ ) - line . count ( \ ) ) % 2 : 
~~~ error ( filename , linenum , 'readability/multiline_string' , 5 , 
~~ ~~ def CheckPosixThreading ( filename , clean_lines , linenum , error ) : 
for single_thread_func , multithread_safe_func , pattern in _THREADING_LIST : 
~~~ if Search ( pattern , line ) : 
~~~ error ( filename , linenum , 'runtime/threadsafe_fn' , 2 , 
~~ ~~ ~~ def CheckVlogArguments ( filename , clean_lines , linenum , error ) : 
if Search ( r'\\bVLOG\\((INFO|ERROR|WARNING|DFATAL|FATAL)\\)' , line ) : 
~~~ error ( filename , linenum , 'runtime/vlog' , 5 , 
~~ ~~ def CheckInvalidIncrement ( filename , clean_lines , linenum , error ) : 
if _RE_PATTERN_INVALID_INCREMENT . match ( line ) : 
~~~ error ( filename , linenum , 'runtime/invalid_increment' , 5 , 
~~ ~~ def CheckSpacingForFunctionCall ( filename , clean_lines , linenum , error ) : 
for pattern in ( r'\\bif\\s*\\((.*)\\)\\s*{' , 
r'\\bfor\\s*\\((.*)\\)\\s*{' , 
r'\\bwhile\\s*\\((.*)\\)\\s*[{;]' , 
r'\\bswitch\\s*\\((.*)\\)\\s*{' ) : 
~~~ match = Search ( pattern , line ) 
not Search ( r'\\b(if|for|while|switch|return|new|delete|catch|sizeof)\\b' , 
fncall ) and 
~~~ error ( filename , linenum , 'whitespace/parens' , 4 , 
~~ elif Search ( r'\\(\\s+(?!(\\s*\\\\)|\\()' , fncall ) : 
~~~ error ( filename , linenum , 'whitespace/parens' , 2 , 
~~ if ( Search ( r'\\w\\s+\\(' , fncall ) and 
not Search ( r'_{0,2}asm_{0,2}\\s+_{0,2}volatile_{0,2}\\s+\\(' , fncall ) and 
not Search ( r'#\\s*define|typedef|using\\s+\\w+\\s*=' , fncall ) and 
not Search ( r'\\w\\s+\\((\\w+::)*\\*\\w+\\)\\(' , fncall ) and 
not Search ( r'\\bcase\\s+\\(' , fncall ) ) : 
~~~ if Search ( r'\\boperator_*\\b' , line ) : 
~~~ error ( filename , linenum , 'whitespace/parens' , 0 , 
~~ ~~ if Search ( r'[^)]\\s+\\)\\s*[^{\\s]' , fncall ) : 
~~~ if Search ( r'^\\s+\\)' , fncall ) : 
~~ ~~ ~~ ~~ def CheckForFunctionLengths ( filename , clean_lines , linenum , 
function_state , error ) : 
lines = clean_lines . lines 
line = lines [ linenum ] 
joined_line = '' 
starting_func = False 
match_result = Match ( regexp , line ) 
if match_result : 
~~~ function_name = match_result . group ( 1 ) . split ( ) [ - 1 ] 
if function_name == 'TEST' or function_name == 'TEST_F' or ( 
not Match ( r'[A-Z_]+$' , function_name ) ) : 
~~~ starting_func = True 
~~ ~~ if starting_func : 
~~~ body_found = False 
for start_linenum in range ( linenum , clean_lines . NumLines ( ) ) : 
~~~ start_line = lines [ start_linenum ] 
~~~ body_found = True 
~~ elif Search ( r'{' , start_line ) : 
function = Search ( r'((\\w|:)*)\\(' , line ) . group ( 1 ) 
~~~ parameter_regexp = Search ( r'(\\(.*\\))' , joined_line ) 
~~~ function += parameter_regexp . group ( 1 ) 
~~~ function += '()' 
~~ function_state . Begin ( function ) 
~~ ~~ if not body_found : 
~~~ error ( filename , linenum , 'readability/fn_size' , 5 , 
~~~ function_state . Check ( error , filename , linenum ) 
function_state . End ( ) 
~~ elif not Match ( r'^\\s*$' , line ) : 
~~~ function_state . Count ( ) 
~~ ~~ def CheckComment ( line , filename , linenum , next_line_start , error ) : 
if commentpos != - 1 : 
~~~ if re . sub ( r'\\\\.' , '' , line [ 0 : commentpos ] ) . count ( \ ) % 2 == 0 : 
( ( commentpos >= 1 and 
line [ commentpos - 1 ] not in string . whitespace ) or 
( commentpos >= 2 and 
line [ commentpos - 2 ] not in string . whitespace ) ) ) : 
~~~ error ( filename , linenum , 'whitespace/comments' , 2 , 
~~ comment = line [ commentpos : ] 
match = _RE_PATTERN_TODO . match ( comment ) 
~~~ leading_whitespace = match . group ( 1 ) 
if len ( leading_whitespace ) > 1 : 
~~~ error ( filename , linenum , 'whitespace/todo' , 2 , 
~~ username = match . group ( 2 ) 
if not username : 
~~~ error ( filename , linenum , 'readability/todo' , 2 , 
~~ middle_whitespace = match . group ( 3 ) 
not Match ( r'(///|//\\!)(\\s+|$)' , comment ) ) : 
~~~ error ( filename , linenum , 'whitespace/comments' , 4 , 
~~ ~~ ~~ ~~ def CheckAccess ( filename , clean_lines , linenum , nesting_state , error ) : 
matched = Match ( ( r'\\s*(DISALLOW_COPY_AND_ASSIGN|' 
r'DISALLOW_IMPLICIT_CONSTRUCTORS)' ) , line ) 
if not matched : 
~~ if nesting_state . stack and isinstance ( nesting_state . stack [ - 1 ] , _ClassInfo ) : 
~~~ if nesting_state . stack [ - 1 ] . access != 'private' : 
~~~ error ( filename , linenum , 'readability/constructors' , 3 , 
~~ ~~ def CheckSpacing ( filename , clean_lines , linenum , nesting_state , error ) : 
raw = clean_lines . lines_without_raw_strings 
line = raw [ linenum ] 
if ( IsBlankLine ( line ) and 
not nesting_state . InNamespaceBody ( ) and 
not nesting_state . InExternC ( ) ) : 
~~~ elided = clean_lines . elided 
prev_line = elided [ linenum - 1 ] 
prevbrace = prev_line . rfind ( '{' ) 
if prevbrace != - 1 and prev_line [ prevbrace : ] . find ( '}' ) == - 1 : 
~~~ exception = False 
~~~ search_position = linenum - 2 
while ( search_position >= 0 
~~~ search_position -= 1 
~~ exception = ( search_position >= 0 
prev_line ) 
~~ if not exception : 
~~~ error ( filename , linenum , 'whitespace/blank_line' , 2 , 
~~ ~~ if linenum + 1 < clean_lines . NumLines ( ) : 
~~~ next_line = raw [ linenum + 1 ] 
if ( next_line 
and Match ( r'\\s*}' , next_line ) 
~~~ error ( filename , linenum , 'whitespace/blank_line' , 3 , 
~~ ~~ matched = Match ( r'\\s*(public|protected|private):' , prev_line ) 
\ % matched . group ( 1 ) ) 
~~ ~~ next_line_start = 0 
if linenum + 1 < clean_lines . NumLines ( ) : 
next_line_start = len ( next_line ) - len ( next_line . lstrip ( ) ) 
~~ CheckComment ( line , filename , linenum , next_line_start , error ) 
if Search ( r'\\w\\s+\\[' , line ) and not Search ( r'(?:delete|return)\\s+\\[' , line ) : 
~~~ error ( filename , linenum , 'whitespace/braces' , 5 , 
~~~ error ( filename , linenum , 'whitespace/forcolon' , 2 , 
~~ ~~ def CheckParenthesisSpacing ( filename , clean_lines , linenum , error ) : 
~~~ error ( filename , linenum , 'whitespace/parens' , 5 , 
~~ match = Search ( r'\\b(if|for|while|switch)\\s*' 
line ) 
~~~ if len ( match . group ( 2 ) ) != len ( match . group ( 4 ) ) : 
~~~ if not ( match . group ( 3 ) == ';' and 
len ( match . group ( 2 ) ) == 1 + len ( match . group ( 4 ) ) or 
~~ ~~ if len ( match . group ( 2 ) ) not in [ 0 , 1 ] : 
match . group ( 1 ) ) 
~~ ~~ ~~ def CheckCommaSpacing ( filename , clean_lines , linenum , error ) : 
if ( Search ( r',[^,\\s]' , ReplaceAll ( r'\\boperator\\s*,\\s*\\(' , 'F(' , line ) ) and 
Search ( r',[^,\\s]' , raw [ linenum ] ) ) : 
~~~ error ( filename , linenum , 'whitespace/comma' , 3 , 
~~ if Search ( r';[^\\s};\\\\)/]' , line ) : 
~~~ error ( filename , linenum , 'whitespace/semicolon' , 3 , 
~~ ~~ def _IsType ( clean_lines , nesting_state , expr ) : 
last_word = Match ( r'^.*(\\b\\S+)$' , expr ) 
if last_word : 
~~~ token = last_word . group ( 1 ) 
~~~ token = expr 
~~ if _TYPES . match ( token ) : 
~~ typename_pattern = ( r'\\b(?:typename|class|struct)\\s+' + re . escape ( token ) + 
r'\\b' ) 
block_index = len ( nesting_state . stack ) - 1 
while block_index >= 0 : 
~~~ if isinstance ( nesting_state . stack [ block_index ] , _NamespaceInfo ) : 
~~ last_line = nesting_state . stack [ block_index ] . starting_linenum 
next_block_start = 0 
if block_index > 0 : 
~~~ next_block_start = nesting_state . stack [ block_index - 1 ] . starting_linenum 
~~ first_line = last_line 
while first_line >= next_block_start : 
~~~ if clean_lines . elided [ first_line ] . find ( 'template' ) >= 0 : 
~~ first_line -= 1 
~~ if first_line < next_block_start : 
~~~ block_index -= 1 
~~ for i in xrange ( first_line , last_line + 1 , 1 ) : 
~~~ if Search ( typename_pattern , clean_lines . elided [ i ] ) : 
~~ ~~ block_index -= 1 
~~ def CheckBracesSpacing ( filename , clean_lines , linenum , nesting_state , error ) : 
~~~ leading_text = match . group ( 1 ) 
( endline , endlinenum , endpos ) = CloseExpression ( 
clean_lines , linenum , len ( match . group ( 1 ) ) ) 
trailing_text = '' 
if endpos > - 1 : 
~~~ trailing_text = endline [ endpos : ] 
~~ for offset in xrange ( endlinenum + 1 , 
min ( endlinenum + 3 , clean_lines . NumLines ( ) - 1 ) ) : 
~~~ trailing_text += clean_lines . elided [ offset ] 
~~ if ( not Match ( r'^[\\s}]*[{.;,)<>\\]:]' , trailing_text ) 
and not _IsType ( clean_lines , nesting_state , leading_text ) ) : 
~~ ~~ if Search ( r'}else' , line ) : 
~~ if Search ( r':\\s*;\\s*$' , line ) : 
~~~ error ( filename , linenum , 'whitespace/semicolon' , 5 , 
~~ elif Search ( r'^\\s*;\\s*$' , line ) : 
~~ elif ( Search ( r'\\s+;\\s*$' , line ) and 
not Search ( r'\\bfor\\b' , line ) ) : 
~~ ~~ def IsDecltype ( clean_lines , linenum , column ) : 
( text , _ , start_col ) = ReverseCloseExpression ( clean_lines , linenum , column ) 
if start_col < 0 : 
~~ if Search ( r'\\bdecltype\\s*$' , text [ 0 : start_col ] ) : 
~~ def CheckSectionSpacing ( filename , clean_lines , class_info , linenum , error ) : 
if ( class_info . last_line - class_info . starting_linenum <= 24 or 
linenum <= class_info . starting_linenum ) : 
~~ matched = Match ( r'\\s*(public|protected|private):' , clean_lines . lines [ linenum ] ) 
~~~ prev_line = clean_lines . lines [ linenum - 1 ] 
if ( not IsBlankLine ( prev_line ) and 
not Search ( r'\\b(class|struct)\\b' , prev_line ) and 
not Search ( r'\\\\$' , prev_line ) ) : 
~~~ end_class_head = class_info . starting_linenum 
for i in range ( class_info . starting_linenum , linenum ) : 
~~~ if Search ( r'\\{\\s*$' , clean_lines . lines [ i ] ) : 
~~~ end_class_head = i 
~~ ~~ if end_class_head < linenum - 1 : 
~~ ~~ ~~ ~~ def GetPreviousNonBlankLine ( clean_lines , linenum ) : 
prevlinenum = linenum - 1 
while prevlinenum >= 0 : 
~~~ prevline = clean_lines . elided [ prevlinenum ] 
~~~ return ( prevline , prevlinenum ) 
~~ prevlinenum -= 1 
~~ return ( '' , - 1 ) 
~~ def CheckTrailingSemicolon ( filename , clean_lines , linenum , error ) : 
match = Match ( r'^(.*\\)\\s*)\\{' , line ) 
~~~ closing_brace_pos = match . group ( 1 ) . rfind ( ')' ) 
opening_parenthesis = ReverseCloseExpression ( 
clean_lines , linenum , closing_brace_pos ) 
if opening_parenthesis [ 2 ] > - 1 : 
~~~ line_prefix = opening_parenthesis [ 0 ] [ 0 : opening_parenthesis [ 2 ] ] 
macro = Search ( r'\\b([A-Z_][A-Z0-9_]*)\\s*$' , line_prefix ) 
func = Match ( r'^(.*\\])\\s*$' , line_prefix ) 
if ( ( macro and 
macro . group ( 1 ) not in ( 
'TEST' , 'TEST_F' , 'MATCHER' , 'MATCHER_P' , 'TYPED_TEST' , 
'EXCLUSIVE_LOCKS_REQUIRED' , 'SHARED_LOCKS_REQUIRED' , 
'LOCKS_EXCLUDED' , 'INTERFACE_DEF' ) ) or 
( func and not Search ( r'\\boperator\\s*\\[\\s*\\]' , func . group ( 1 ) ) ) or 
Search ( r'\\b(?:struct|union)\\s+alignas\\s*$' , line_prefix ) or 
Search ( r'\\bdecltype$' , line_prefix ) or 
Search ( r'\\s+=\\s*$' , line_prefix ) ) : 
~~~ match = None 
~~ ~~ if ( match and 
opening_parenthesis [ 1 ] > 1 and 
Search ( r'\\]\\s*$' , clean_lines . elided [ opening_parenthesis [ 1 ] - 1 ] ) ) : 
~~~ match = Match ( r'^(.*(?:else|\\)\\s*const)\\s*)\\{' , line ) 
~~~ prevline = GetPreviousNonBlankLine ( clean_lines , linenum ) [ 0 ] 
if prevline and Search ( r'[;{}]\\s*$' , prevline ) : 
~~~ match = Match ( r'^(\\s*)\\{' , line ) 
~~ ~~ ~~ if match : 
~~~ ( endline , endlinenum , endpos ) = CloseExpression ( 
if endpos > - 1 and Match ( r'^\\s*;' , endline [ endpos : ] ) : 
~~~ raw_lines = clean_lines . raw_lines 
ParseNolintSuppressions ( filename , raw_lines [ endlinenum - 1 ] , endlinenum - 1 , 
ParseNolintSuppressions ( filename , raw_lines [ endlinenum ] , endlinenum , 
error ( filename , endlinenum , 'readability/braces' , 4 , 
~~ ~~ ~~ def CheckEmptyBlockBody ( filename , clean_lines , linenum , error ) : 
matched = Match ( r'\\s*(for|while|if)\\s*\\(' , line ) 
~~~ ( end_line , end_linenum , end_pos ) = CloseExpression ( 
clean_lines , linenum , line . find ( '(' ) ) 
if end_pos >= 0 and Match ( r';' , end_line [ end_pos : ] ) : 
~~~ if matched . group ( 1 ) == 'if' : 
~~~ error ( filename , end_linenum , 'whitespace/empty_conditional_body' , 5 , 
~~~ error ( filename , end_linenum , 'whitespace/empty_loop_body' , 5 , 
~~ ~~ if end_pos >= 0 and matched . group ( 1 ) == 'if' : 
~~~ opening_linenum = end_linenum 
opening_line_fragment = end_line [ end_pos : ] 
while not Search ( r'^\\s*\\{' , opening_line_fragment ) : 
~~~ if Search ( r'^(?!\\s*$)' , opening_line_fragment ) : 
~~ opening_linenum += 1 
if opening_linenum == len ( clean_lines . elided ) : 
~~ opening_line_fragment = clean_lines . elided [ opening_linenum ] 
~~ opening_line = clean_lines . elided [ opening_linenum ] 
opening_pos = opening_line_fragment . find ( '{' ) 
if opening_linenum == end_linenum : 
~~~ opening_pos += end_pos 
~~ ( closing_line , closing_linenum , closing_pos ) = CloseExpression ( 
clean_lines , opening_linenum , opening_pos ) 
if closing_pos < 0 : 
~~ if ( clean_lines . raw_lines [ opening_linenum ] != 
CleanseComments ( clean_lines . raw_lines [ opening_linenum ] ) ) : 
~~ if closing_linenum > opening_linenum : 
~~~ bodylist = list ( opening_line [ opening_pos + 1 : ] ) 
bodylist . extend ( clean_lines . raw_lines [ opening_linenum + 1 : closing_linenum ] ) 
bodylist . append ( clean_lines . elided [ closing_linenum ] [ : closing_pos - 1 ] ) 
body = '\\n' . join ( bodylist ) 
~~~ body = opening_line [ opening_pos + 1 : closing_pos - 1 ] 
~~ if not _EMPTY_CONDITIONAL_BODY_PATTERN . search ( body ) : 
~~ current_linenum = closing_linenum 
current_line_fragment = closing_line [ closing_pos : ] 
while Search ( r'^\\s*$|^(?=\\s*else)' , current_line_fragment ) : 
~~~ if Search ( r'^(?=\\s*else)' , current_line_fragment ) : 
~~ current_linenum += 1 
if current_linenum == len ( clean_lines . elided ) : 
~~ current_line_fragment = clean_lines . elided [ current_linenum ] 
~~ error ( filename , end_linenum , 'whitespace/empty_if_body' , 4 , 
~~ ~~ ~~ def FindCheckMacro ( line ) : 
for macro in _CHECK_MACROS : 
~~~ i = line . find ( macro ) 
if i >= 0 : 
~~~ matched = Match ( r'^(.*\\b' + macro + r'\\s*)\\(' , line ) 
~~ return ( macro , len ( matched . group ( 1 ) ) ) 
~~ ~~ return ( None , - 1 ) 
~~ def CheckCheck ( filename , clean_lines , linenum , error ) : 
lines = clean_lines . elided 
( check_macro , start_pos ) = FindCheckMacro ( lines [ linenum ] ) 
if not check_macro : 
~~ ( last_line , end_line , end_pos ) = CloseExpression ( 
clean_lines , linenum , start_pos ) 
if end_pos < 0 : 
~~ if not Match ( r'\\s*;' , last_line [ end_pos : ] ) : 
~~ if linenum == end_line : 
~~~ expression = lines [ linenum ] [ start_pos + 1 : end_pos - 1 ] 
~~~ expression = lines [ linenum ] [ start_pos + 1 : ] 
for i in xrange ( linenum + 1 , end_line ) : 
~~~ expression += lines [ i ] 
~~ expression += last_line [ 0 : end_pos - 1 ] 
~~ lhs = '' 
rhs = '' 
operator = None 
while expression : 
~~~ matched = Match ( r'^\\s*(<<|<<=|>>|>>=|->\\*|->|&&|\\|\\||' 
r'==|!=|>=|>|<=|<|\\()(.*)$' , expression ) 
~~~ token = matched . group ( 1 ) 
if token == '(' : 
~~~ expression = matched . group ( 2 ) 
( end , _ ) = FindEndOfExpressionInLine ( expression , 0 , [ '(' ] ) 
if end < 0 : 
~~ lhs += '(' + expression [ 0 : end ] 
expression = expression [ end : ] 
~~ elif token in ( '&&' , '||' ) : 
~~ elif token in ( '<<' , '<<=' , '>>' , '>>=' , '->*' , '->' ) : 
~~~ lhs += token 
expression = matched . group ( 2 ) 
~~~ operator = token 
rhs = matched . group ( 2 ) 
~~~ matched = Match ( r'^([^-=!<>()&|]+)(.*)$' , expression ) 
~~~ matched = Match ( r'^(\\s*\\S)(.*)$' , expression ) 
~~ ~~ lhs += matched . group ( 1 ) 
~~ ~~ if not ( lhs and operator and rhs ) : 
~~ if rhs . find ( '&&' ) > - 1 or rhs . find ( '||' ) > - 1 : 
~~ lhs = lhs . strip ( ) 
rhs = rhs . strip ( ) 
match_constant = r\ 
if Match ( match_constant , lhs ) or Match ( match_constant , rhs ) : 
~~~ error ( filename , linenum , 'readability/check' , 2 , 
_CHECK_REPLACEMENT [ check_macro ] [ operator ] , 
check_macro , operator ) ) 
~~ ~~ def CheckAltTokens ( filename , clean_lines , linenum , error ) : 
if Match ( r'^\\s*#' , line ) : 
~~ if line . find ( '/*' ) >= 0 or line . find ( '*/' ) >= 0 : 
~~ for match in _ALT_TOKEN_REPLACEMENT_PATTERN . finditer ( line ) : 
~~~ error ( filename , linenum , 'readability/alt_tokens' , 2 , 
_ALT_TOKEN_REPLACEMENT [ match . group ( 1 ) ] , match . group ( 1 ) ) ) 
~~ ~~ def GetLineWidth ( line ) : 
if isinstance ( line , unicode ) : 
~~~ width = 0 
for uc in unicodedata . normalize ( 'NFC' , line ) : 
~~~ if unicodedata . east_asian_width ( uc ) in ( 'W' , 'F' ) : 
~~~ width += 2 
~~ elif not unicodedata . combining ( uc ) : 
~~~ width += 1 
~~ ~~ return width 
~~~ return len ( line ) 
~~ ~~ def _DropCommonSuffixes ( filename ) : 
for suffix in itertools . chain ( 
( '%s.%s' % ( test_suffix . lstrip ( '_' ) , ext ) 
for test_suffix , ext in itertools . product ( _test_suffixes , GetNonHeaderExtensions ( ) ) ) , 
( '%s.%s' % ( suffix , ext ) 
for suffix , ext in itertools . product ( [ 'inl' , 'imp' , 'internal' ] , GetHeaderExtensions ( ) ) ) ) : 
~~~ if ( filename . endswith ( suffix ) and len ( filename ) > len ( suffix ) and 
filename [ - len ( suffix ) - 1 ] in ( '-' , '_' ) ) : 
~~~ return filename [ : - len ( suffix ) - 1 ] 
~~ ~~ return os . path . splitext ( filename ) [ 0 ] 
~~ def _ClassifyInclude ( fileinfo , include , is_system ) : 
is_cpp_h = include in _CPP_HEADERS 
if is_system and os . path . splitext ( include ) [ 1 ] in [ '.hpp' , '.hxx' , '.h++' ] : 
~~~ is_system = False 
~~ if is_system : 
~~~ if is_cpp_h : 
~~~ return _CPP_SYS_HEADER 
~~~ return _C_SYS_HEADER 
~~ ~~ target_dir , target_base = ( 
os . path . split ( _DropCommonSuffixes ( fileinfo . RepositoryName ( ) ) ) ) 
include_dir , include_base = os . path . split ( _DropCommonSuffixes ( include ) ) 
target_dir_pub = os . path . normpath ( target_dir + '/../public' ) 
target_dir_pub = target_dir_pub . replace ( '\\\\' , '/' ) 
if target_base == include_base and ( 
include_dir == target_dir or 
include_dir == target_dir_pub ) : 
~~~ return _LIKELY_MY_HEADER 
~~ target_first_component = _RE_FIRST_COMPONENT . match ( target_base ) 
include_first_component = _RE_FIRST_COMPONENT . match ( include_base ) 
if ( target_first_component and include_first_component and 
target_first_component . group ( 0 ) == 
include_first_component . group ( 0 ) ) : 
~~~ return _POSSIBLE_MY_HEADER 
~~ return _OTHER_HEADER 
~~ def CheckIncludeLine ( filename , clean_lines , linenum , include_state , error ) : 
line = clean_lines . lines [ linenum ] 
match = Match ( r\ , line ) 
if match and not _THIRD_PARTY_HEADERS_PATTERN . match ( match . group ( 1 ) ) : 
~~~ error ( filename , linenum , 'build/include_subdir' , 4 , 
~~ match = _RE_PATTERN_INCLUDE . search ( line ) 
~~~ include = match . group ( 2 ) 
is_system = ( match . group ( 1 ) == '<' ) 
duplicate_line = include_state . FindHeader ( include ) 
if duplicate_line >= 0 : 
~~~ error ( filename , linenum , 'build/include' , 4 , 
\ % 
( include , filename , duplicate_line ) ) 
~~ for extension in GetNonHeaderExtensions ( ) : 
~~~ if ( include . endswith ( '.' + extension ) and 
os . path . dirname ( fileinfo . RepositoryName ( ) ) != os . path . dirname ( include ) ) : 
~~ ~~ if not _THIRD_PARTY_HEADERS_PATTERN . match ( include ) : 
~~~ include_state . include_list [ - 1 ] . append ( ( include , linenum ) ) 
error_message = include_state . CheckNextIncludeOrder ( 
_ClassifyInclude ( fileinfo , include , is_system ) ) 
if error_message : 
~~~ error ( filename , linenum , 'build/include_order' , 4 , 
( error_message , fileinfo . BaseName ( ) ) ) 
~~ canonical_include = include_state . CanonicalizeAlphabeticalOrder ( include ) 
if not include_state . IsInAlphabeticalOrder ( 
clean_lines , linenum , canonical_include ) : 
~~~ error ( filename , linenum , 'build/include_alpha' , 4 , 
\ % include ) 
~~ include_state . SetLastHeader ( canonical_include ) 
~~ ~~ ~~ def _GetTextInside ( text , start_pattern ) : 
matching_punctuation = { '(' : ')' , '{' : '}' , '[' : ']' } 
closing_punctuation = set ( itervalues ( matching_punctuation ) ) 
match = re . search ( start_pattern , text , re . M ) 
~~ start_position = match . end ( 0 ) 
assert start_position > 0 , ( 
assert text [ start_position - 1 ] in matching_punctuation , ( 
punctuation_stack = [ matching_punctuation [ text [ start_position - 1 ] ] ] 
position = start_position 
while punctuation_stack and position < len ( text ) : 
~~~ if text [ position ] == punctuation_stack [ - 1 ] : 
~~~ punctuation_stack . pop ( ) 
~~ elif text [ position ] in closing_punctuation : 
~~ elif text [ position ] in matching_punctuation : 
~~~ punctuation_stack . append ( matching_punctuation [ text [ position ] ] ) 
~~ position += 1 
~~ if punctuation_stack : 
~~ return text [ start_position : position - 1 ] 
~~ def CheckLanguage ( filename , clean_lines , linenum , file_extension , 
include_state , nesting_state , error ) : 
if not line : 
~~~ CheckIncludeLine ( filename , clean_lines , linenum , include_state , error ) 
~~ match = Match ( r'^\\s*#\\s*(if|ifdef|ifndef|elif|else|endif)\\b' , line ) 
~~~ include_state . ResetSection ( match . group ( 1 ) ) 
~~ CheckCasts ( filename , clean_lines , linenum , error ) 
CheckGlobalStatic ( filename , clean_lines , linenum , error ) 
CheckPrintf ( filename , clean_lines , linenum , error ) 
if file_extension in GetHeaderExtensions ( ) : 
~~~ error ( filename , linenum , 'runtime/int' , 4 , 
~~ ~~ if Search ( r'\\boperator\\s*&\\s*\\(\\s*\\)' , line ) : 
~~~ error ( filename , linenum , 'runtime/operator' , 4 , 
~~ if Search ( r'\\}\\s*if\\s*\\(' , line ) : 
~~~ error ( filename , linenum , 'readability/braces' , 4 , 
~~ printf_args = _GetTextInside ( line , r'(?i)\\b(string)?printf\\s*\\(' ) 
if printf_args : 
~~~ match = Match ( r'([\\w.\\->()]+)$' , printf_args ) 
if match and match . group ( 1 ) != '__VA_ARGS__' : 
~~~ function_name = re . search ( r'\\b((?:string)?printf)\\s*\\(' , 
line , re . I ) . group ( 1 ) 
error ( filename , linenum , 'runtime/printf' , 4 , 
% ( function_name , match . group ( 1 ) ) ) 
~~ ~~ match = Search ( r'memset\\s*\\(([^,]*),\\s*([^,]*),\\s*0\\s*\\)' , line ) 
if match and not Match ( r"^\ , match . group ( 2 ) ) : 
~~~ error ( filename , linenum , 'runtime/memset' , 4 , 
% ( match . group ( 1 ) , match . group ( 2 ) ) ) 
~~~ if Search ( r'\\bliterals\\b' , line ) : 
~~~ error ( filename , linenum , 'build/namespaces_literals' , 5 , 
~~~ error ( filename , linenum , 'build/namespaces' , 5 , 
if ( match and match . group ( 2 ) != 'return' and match . group ( 2 ) != 'delete' and 
match . group ( 3 ) . find ( ']' ) == - 1 ) : 
~~~ tokens = re . split ( r'\\s|\\+|\\-|\\*|\\/|<<|>>]' , match . group ( 3 ) ) 
is_const = True 
skip_next = False 
for tok in tokens : 
~~~ if skip_next : 
~~~ skip_next = False 
~~ if Search ( r'sizeof\\(.+\\)' , tok ) : continue 
if Search ( r'arraysize\\(\\w+\\)' , tok ) : continue 
tok = tok . lstrip ( '(' ) 
tok = tok . rstrip ( ')' ) 
if not tok : continue 
if Match ( r'\\d+' , tok ) : continue 
if Match ( r'0[xX][0-9a-fA-F]+' , tok ) : continue 
if Match ( r'k[A-Z0-9]\\w*' , tok ) : continue 
if Match ( r'(.+::)?k[A-Z0-9]\\w*' , tok ) : continue 
if Match ( r'(.+::)?[A-Z][A-Z0-9_]*' , tok ) : continue 
if tok . startswith ( 'sizeof' ) : 
~~~ skip_next = True 
~~ is_const = False 
~~ if not is_const : 
~~~ error ( filename , linenum , 'runtime/arrays' , 1 , 
"(\ ) 
~~ ~~ if ( file_extension in GetHeaderExtensions ( ) 
and Search ( r'\\bnamespace\\s*{' , line ) 
and line [ - 1 ] != '\\\\' ) : 
~~~ error ( filename , linenum , 'build/namespaces' , 4 , 
'https://google-styleguide.googlecode.com/svn/trunk/cppguide.xml#Namespaces' 
~~ ~~ def CheckGlobalStatic ( filename , clean_lines , linenum , error ) : 
if linenum + 1 < clean_lines . NumLines ( ) and not Search ( r'[;({]' , line ) : 
~~~ line += clean_lines . elided [ linenum + 1 ] . strip ( ) 
~~ match = Match ( 
r'([a-zA-Z0-9_:]+)\\b(.*)' , 
if ( match and 
not Search ( r'\\bstring\\b(\\s+const)?\\s*[\\*\\&]\\s*(const\\s+)?\\w' , line ) and 
not Search ( r'\\boperator\\W' , line ) and 
not Match ( r\ , match . group ( 4 ) ) ) : 
~~~ if Search ( r'\\bconst\\b' , line ) : 
~~~ error ( filename , linenum , 'runtime/string' , 4 , 
( match . group ( 1 ) , match . group ( 2 ) or '' , match . group ( 3 ) ) ) 
~~ ~~ if ( Search ( r'\\b([A-Za-z0-9_]*_)\\(\\1\\)' , line ) or 
Search ( r'\\b([A-Za-z0-9_]*_)\\(CHECK_NOTNULL\\(\\1\\)\\)' , line ) ) : 
~~~ error ( filename , linenum , 'runtime/init' , 4 , 
~~ ~~ def CheckPrintf ( filename , clean_lines , linenum , error ) : 
match = Search ( r'snprintf\\s*\\(([^,]*),\\s*([0-9]*)\\s*,' , line ) 
if match and match . group ( 2 ) != '0' : 
~~~ error ( filename , linenum , 'runtime/printf' , 3 , 
~~ if Search ( r'\\bsprintf\\s*\\(' , line ) : 
~~~ error ( filename , linenum , 'runtime/printf' , 5 , 
~~ match = Search ( r'\\b(strcpy|strcat)\\s*\\(' , line ) 
~~~ error ( filename , linenum , 'runtime/printf' , 4 , 
~~ ~~ def IsDerivedFunction ( clean_lines , linenum ) : 
for i in xrange ( linenum , max ( - 1 , linenum - 10 ) , - 1 ) : 
~~~ match = Match ( r'^([^()]*\\w+)\\(' , clean_lines . elided [ i ] ) 
~~~ line , _ , closing_paren = CloseExpression ( 
clean_lines , i , len ( match . group ( 1 ) ) ) 
return ( closing_paren >= 0 and 
Search ( r'\\boverride\\b' , line [ closing_paren : ] ) ) 
~~ def IsOutOfLineMethodDefinition ( clean_lines , linenum ) : 
~~~ if Match ( r'^([^()]*\\w+)\\(' , clean_lines . elided [ i ] ) : 
~~~ return Match ( r'^[^()]*\\w+::\\w+\\(' , clean_lines . elided [ i ] ) is not None 
~~ def IsInitializerList ( clean_lines , linenum ) : 
for i in xrange ( linenum , 1 , - 1 ) : 
~~~ line = clean_lines . elided [ i ] 
if i == linenum : 
~~~ remove_function_body = Match ( r'^(.*)\\{\\s*$' , line ) 
if remove_function_body : 
~~~ line = remove_function_body . group ( 1 ) 
~~ ~~ if Search ( r'\\s:\\s*\\w+[({]' , line ) : 
~~ if Search ( r'\\}\\s*,\\s*$' , line ) : 
~~ if Search ( r'[{};]\\s*$' , line ) : 
~~ def CheckForNonConstReference ( filename , clean_lines , linenum , 
nesting_state , error ) : 
if '&' not in line : 
~~ if IsDerivedFunction ( clean_lines , linenum ) : 
~~ if IsOutOfLineMethodDefinition ( clean_lines , linenum ) : 
~~ if linenum > 1 : 
~~~ previous = None 
if Match ( r'\\s*::(?:[\\w<>]|::)+\\s*&\\s*\\S' , line ) : 
~~~ previous = Search ( r'\\b((?:const\\s*)?(?:[\\w<>]|::)+[\\w<>])\\s*$' , 
clean_lines . elided [ linenum - 1 ] ) 
~~ elif Match ( r'\\s*[a-zA-Z_]([\\w<>]|::)+\\s*&\\s*\\S' , line ) : 
~~~ previous = Search ( r'\\b((?:const\\s*)?(?:[\\w<>]|::)+::)\\s*$' , 
~~ if previous : 
~~~ line = previous . group ( 1 ) + line . lstrip ( ) 
~~~ endpos = line . rfind ( '>' ) 
~~~ ( _ , startline , startpos ) = ReverseCloseExpression ( 
clean_lines , linenum , endpos ) 
if startpos > - 1 and startline < linenum : 
~~~ line = '' 
for i in xrange ( startline , linenum + 1 ) : 
~~~ line += clean_lines . elided [ i ] . strip ( ) 
~~ ~~ ~~ ~~ ~~ if ( nesting_state . previous_stack_top and 
not ( isinstance ( nesting_state . previous_stack_top , _ClassInfo ) or 
isinstance ( nesting_state . previous_stack_top , _NamespaceInfo ) ) ) : 
~~ if linenum > 0 : 
~~~ for i in xrange ( linenum - 1 , max ( 0 , linenum - 10 ) , - 1 ) : 
~~~ previous_line = clean_lines . elided [ i ] 
if not Search ( r'[),]\\s*$' , previous_line ) : 
~~ if Match ( r'^\\s*:\\s+\\S' , previous_line ) : 
~~ ~~ ~~ if Search ( r'\\\\\\s*$' , line ) : 
~~ if IsInitializerList ( clean_lines , linenum ) : 
~~ whitelisted_functions = ( r'(?:[sS]wap(?:<\\w:+>)?|' 
r'operator\\s*[<>][<>]|' 
r'static_assert|COMPILE_ASSERT' 
r')\\s*\\(' ) 
if Search ( whitelisted_functions , line ) : 
~~ elif not Search ( r'\\S+\\([^)]*$' , line ) : 
~~~ for i in xrange ( 2 ) : 
~~~ if ( linenum > i and 
Search ( whitelisted_functions , clean_lines . elided [ linenum - i - 1 ] ) ) : 
for parameter in re . findall ( _RE_PATTERN_REF_PARAM , decls ) : 
~~~ if ( not Match ( _RE_PATTERN_CONST_REF_PARAM , parameter ) and 
not Match ( _RE_PATTERN_REF_STREAM_PARAM , parameter ) ) : 
~~~ error ( filename , linenum , 'runtime/references' , 2 , 
~~ ~~ ~~ def CheckCasts ( filename , clean_lines , linenum , error ) : 
match = Search ( 
r'(\\bnew\\s+(?:const\\s+)?|\\S<\\s*(?:const\\s+)?)?\\b' 
r'(int|float|double|bool|char|int32|uint32|int64|uint64)' 
r'(\\([^)].*)' , line ) 
expecting_function = ExpectingFunctionArgs ( clean_lines , linenum ) 
if match and not expecting_function : 
~~~ matched_type = match . group ( 2 ) 
matched_new_or_template = match . group ( 1 ) 
if Match ( r'\\([^()]+\\)\\s*\\[' , match . group ( 3 ) ) : 
~~ matched_funcptr = match . group ( 3 ) 
if ( matched_new_or_template is None and 
not ( matched_funcptr and 
matched_funcptr ) or 
matched_funcptr . startswith ( '(*)' ) ) ) and 
not Match ( r'\\s*using\\s+\\S+\\s*=\\s*' + matched_type , line ) and 
not Search ( r'new\\(\\S+\\)\\s*' + matched_type , line ) ) : 
~~~ error ( filename , linenum , 'readability/casting' , 4 , 
matched_type ) 
~~ ~~ if not expecting_function : 
~~~ CheckCStyleCast ( filename , clean_lines , linenum , 'static_cast' , 
r'\\((int|float|double|bool|char|u?int(16|32|64))\\)' , error ) 
~~ if CheckCStyleCast ( filename , clean_lines , linenum , 'const_cast' , 
r\ , error ) : 
~~~ CheckCStyleCast ( filename , clean_lines , linenum , 'reinterpret_cast' , 
r'\\((\\w+\\s?\\*+\\s?)\\)' , error ) 
~~ match = Search ( 
r'(?:[^\\w]&\\(([^)*][^)]*)\\)[\\w(])|' 
r'(?:[^\\w]&(static|dynamic|down|reinterpret)_cast\\b)' , line ) 
~~~ parenthesis_error = False 
match = Match ( r'^(.*&(?:static|dynamic|down|reinterpret)_cast\\b)<' , line ) 
~~~ _ , y1 , x1 = CloseExpression ( clean_lines , linenum , len ( match . group ( 1 ) ) ) 
if x1 >= 0 and clean_lines . elided [ y1 ] [ x1 ] == '(' : 
~~~ _ , y2 , x2 = CloseExpression ( clean_lines , y1 , x1 ) 
if x2 >= 0 : 
~~~ extended_line = clean_lines . elided [ y2 ] [ x2 : ] 
if y2 < clean_lines . NumLines ( ) - 1 : 
~~~ extended_line += clean_lines . elided [ y2 + 1 ] 
~~ if Match ( r'\\s*(?:->|\\[)' , extended_line ) : 
~~~ parenthesis_error = True 
~~ ~~ ~~ ~~ if parenthesis_error : 
~~~ error ( filename , linenum , 'runtime/casting' , 4 , 
~~ ~~ ~~ def CheckCStyleCast ( filename , clean_lines , linenum , cast_type , pattern , error ) : 
match = Search ( pattern , line ) 
~~ context = line [ 0 : match . start ( 1 ) - 1 ] 
if Match ( r'.*\\b(?:sizeof|alignof|alignas|[_A-Z][_A-Z0-9]*)\\s*$' , context ) : 
~~~ for i in xrange ( linenum - 1 , max ( 0 , linenum - 5 ) , - 1 ) : 
~~~ context = clean_lines . elided [ i ] + context 
~~ ~~ if Match ( r'.*\\b[_A-Z][_A-Z0-9]*\\s*\\((?:\\([^()]*\\)|[^()])*$' , context ) : 
~~ remainder = line [ match . end ( 0 ) : ] 
if Match ( r'^\\s*(?:;|const\\b|throw\\b|final\\b|override\\b|[=>{),]|->)' , 
remainder ) : 
~~ error ( filename , linenum , 'readability/casting' , 4 , 
( cast_type , match . group ( 1 ) ) ) 
~~ def ExpectingFunctionArgs ( clean_lines , linenum ) : 
return ( Match ( r'^\\s*MOCK_(CONST_)?METHOD\\d+(_T)?\\(' , line ) or 
( linenum >= 2 and 
( Match ( r'^\\s*MOCK_(?:CONST_)?METHOD\\d+(?:_T)?\\((?:\\S+,)?\\s*$' , 
clean_lines . elided [ linenum - 1 ] ) or 
Match ( r'^\\s*MOCK_(?:CONST_)?METHOD\\d+(?:_T)?\\(\\s*$' , 
clean_lines . elided [ linenum - 2 ] ) or 
Search ( r'\\bstd::m?function\\s*\\<\\s*$' , 
clean_lines . elided [ linenum - 1 ] ) ) ) ) 
~~ def FilesBelongToSameModule ( filename_cc , filename_h ) : 
fileinfo_cc = FileInfo ( filename_cc ) 
if not fileinfo_cc . Extension ( ) . lstrip ( '.' ) in GetNonHeaderExtensions ( ) : 
~~~ return ( False , '' ) 
~~ fileinfo_h = FileInfo ( filename_h ) 
if not fileinfo_h . Extension ( ) . lstrip ( '.' ) in GetHeaderExtensions ( ) : 
~~ filename_cc = filename_cc [ : - ( len ( fileinfo_cc . Extension ( ) ) ) ] 
matched_test_suffix = Search ( _TEST_FILE_SUFFIX , fileinfo_cc . BaseName ( ) ) 
if matched_test_suffix : 
~~~ filename_cc = filename_cc [ : - len ( matched_test_suffix . group ( 1 ) ) ] 
~~ filename_cc = filename_cc . replace ( '/public/' , '/' ) 
filename_cc = filename_cc . replace ( '/internal/' , '/' ) 
filename_h = filename_h [ : - ( len ( fileinfo_h . Extension ( ) ) ) ] 
if filename_h . endswith ( '-inl' ) : 
~~~ filename_h = filename_h [ : - len ( '-inl' ) ] 
~~ filename_h = filename_h . replace ( '/public/' , '/' ) 
filename_h = filename_h . replace ( '/internal/' , '/' ) 
files_belong_to_same_module = filename_cc . endswith ( filename_h ) 
common_path = '' 
if files_belong_to_same_module : 
~~~ common_path = filename_cc [ : - len ( filename_h ) ] 
~~ return files_belong_to_same_module , common_path 
~~ def UpdateIncludeState ( filename , include_dict , io = codecs ) : 
headerfile = None 
~~~ headerfile = io . open ( filename , 'r' , 'utf8' , 'replace' ) 
~~ linenum = 0 
for line in headerfile : 
clean_line = CleanseComments ( line ) 
match = _RE_PATTERN_INCLUDE . search ( clean_line ) 
include_dict . setdefault ( include , linenum ) 
~~ def CheckMakePairUsesDeduction ( filename , clean_lines , linenum , error ) : 
match = _RE_PATTERN_EXPLICIT_MAKEPAIR . search ( line ) 
~~~ error ( filename , linenum , 'build/explicit_make_pair' , 
~~ ~~ def CheckRedundantVirtual ( filename , clean_lines , linenum , error ) : 
virtual = Match ( r'^(.*)(\\bvirtual\\b)(.*)$' , line ) 
if not virtual : return 
if ( Search ( r'\\b(public|protected|private)\\s+$' , virtual . group ( 1 ) ) or 
Match ( r'^\\s+(public|protected|private)\\b' , virtual . group ( 3 ) ) ) : 
~~ if Match ( r'^.*[^:]:[^:].*$' , line ) : return 
end_col = - 1 
end_line = - 1 
start_col = len ( virtual . group ( 2 ) ) 
for start_line in xrange ( linenum , min ( linenum + 3 , clean_lines . NumLines ( ) ) ) : 
~~~ line = clean_lines . elided [ start_line ] [ start_col : ] 
parameter_list = Match ( r'^([^(]*)\\(' , line ) 
if parameter_list : 
~~~ ( _ , end_line , end_col ) = CloseExpression ( 
clean_lines , start_line , start_col + len ( parameter_list . group ( 1 ) ) ) 
~~ start_col = 0 
~~ if end_col < 0 : 
~~ for i in xrange ( end_line , min ( end_line + 3 , clean_lines . NumLines ( ) ) ) : 
~~~ line = clean_lines . elided [ i ] [ end_col : ] 
match = Search ( r'\\b(override|final)\\b' , line ) 
~~~ error ( filename , linenum , 'readability/inheritance' , 4 , 
( \ 
\ % match . group ( 1 ) ) ) 
~~ end_col = 0 
if Search ( r'[^\\w]\\s*$' , line ) : 
~~ ~~ ~~ def CheckRedundantOverrideOrFinal ( filename , clean_lines , linenum , error ) : 
declarator_end = line . rfind ( ')' ) 
if declarator_end >= 0 : 
~~~ fragment = line [ declarator_end : ] 
~~~ if linenum > 1 and clean_lines . elided [ linenum - 1 ] . rfind ( ')' ) >= 0 : 
~~~ fragment = line 
~~ ~~ if Search ( r'\\boverride\\b' , fragment ) and Search ( r'\\bfinal\\b' , fragment ) : 
\ ) ) 
~~ ~~ def IsBlockInNameSpace ( nesting_state , is_forward_declaration ) : 
if is_forward_declaration : 
~~~ return len ( nesting_state . stack ) >= 1 and ( 
isinstance ( nesting_state . stack [ - 1 ] , _NamespaceInfo ) ) 
~~ return ( len ( nesting_state . stack ) > 1 and 
nesting_state . stack [ - 1 ] . check_namespace_indentation and 
isinstance ( nesting_state . stack [ - 2 ] , _NamespaceInfo ) ) 
~~ def ShouldCheckNamespaceIndentation ( nesting_state , is_namespace_indent_item , 
raw_lines_no_comments , linenum ) : 
is_forward_declaration = IsForwardClassDeclaration ( raw_lines_no_comments , 
linenum ) 
if not ( is_namespace_indent_item or is_forward_declaration ) : 
~~ if IsMacroDefinition ( raw_lines_no_comments , linenum ) : 
~~ return IsBlockInNameSpace ( nesting_state , is_forward_declaration ) 
~~ def FlagCxx14Features ( filename , clean_lines , linenum , error ) : 
include = Match ( r\ , line ) 
if include and include . group ( 1 ) in ( 'scoped_allocator' , 'shared_mutex' ) : 
~~~ error ( filename , linenum , 'build/c++14' , 5 , 
~~ ~~ def ProcessFileData ( filename , file_extension , lines , error , 
extra_check_functions = None ) : 
include_state = _IncludeState ( ) 
function_state = _FunctionState ( ) 
nesting_state = NestingState ( ) 
ResetNolintSuppressions ( ) 
CheckForCopyright ( filename , lines , error ) 
ProcessGlobalSuppresions ( lines ) 
RemoveMultiLineComments ( filename , lines , error ) 
clean_lines = CleansedLines ( lines ) 
~~~ CheckForHeaderGuard ( filename , clean_lines , error ) 
~~ for line in range ( clean_lines . NumLines ( ) ) : 
~~~ ProcessLine ( filename , file_extension , clean_lines , line , 
include_state , function_state , nesting_state , error , 
extra_check_functions ) 
FlagCxx11Features ( filename , clean_lines , line , error ) 
~~ nesting_state . CheckCompletedBlocks ( filename , error ) 
CheckForIncludeWhatYouUse ( filename , clean_lines , include_state , error ) 
if _IsSourceExtension ( file_extension ) : 
~~~ CheckHeaderFileIncluded ( filename , include_state , error ) 
~~ CheckForBadCharacters ( filename , lines , error ) 
CheckForNewlineAtEOF ( filename , lines , error ) 
~~ def ProcessConfigOverrides ( filename ) : 
abs_filename = os . path . abspath ( filename ) 
cfg_filters = [ ] 
keep_looking = True 
while keep_looking : 
~~~ abs_path , base_name = os . path . split ( abs_filename ) 
if not base_name : 
~~ cfg_file = os . path . join ( abs_path , "CPPLINT.cfg" ) 
abs_filename = abs_path 
if not os . path . isfile ( cfg_file ) : 
~~~ with open ( cfg_file ) as file_handle : 
~~~ for line in file_handle : 
if not line . strip ( ) : 
~~ name , _ , val = line . partition ( '=' ) 
name = name . strip ( ) 
val = val . strip ( ) 
~~~ keep_looking = False 
~~ elif name == 'filter' : 
~~~ cfg_filters . append ( val ) 
~~ elif name == 'exclude_files' : 
~~~ if base_name : 
~~~ pattern = re . compile ( val ) 
if pattern . match ( base_name ) : 
~~~ _cpplint_state . PrintInfo ( \ 
( filename , cfg_file , base_name , val ) ) 
~~ ~~ ~~ elif name == 'linelength' : 
~~~ global _line_length 
~~~ _line_length = int ( val ) 
~~ ~~ elif name == 'extensions' : 
~~~ global _valid_extensions 
~~~ extensions = [ ext . strip ( ) for ext in val . split ( ',' ) ] 
_valid_extensions = set ( extensions ) 
\ % ( val , ) ) 
~~ ~~ elif name == 'headers' : 
~~~ global _header_extensions 
_header_extensions = set ( extensions ) 
~~ ~~ elif name == 'root' : 
~~~ global _root 
_root = val 
~~~ _cpplint_state . PrintError ( 
( name , cfg_file ) ) 
~~ ~~ ~~ ~~ except IOError : 
keep_looking = False 
~~ ~~ for cfg_filter in reversed ( cfg_filters ) : 
~~~ _AddFilters ( cfg_filter ) 
~~ def ProcessFile ( filename , vlevel , extra_check_functions = None ) : 
_SetVerboseLevel ( vlevel ) 
_BackupFilters ( ) 
if not ProcessConfigOverrides ( filename ) : 
~~~ _RestoreFilters ( ) 
~~ lf_lines = [ ] 
crlf_lines = [ ] 
~~~ if filename == '-' : 
~~~ lines = codecs . StreamReaderWriter ( sys . stdin , 
codecs . getreader ( 'utf8' ) , 
codecs . getwriter ( 'utf8' ) , 
'replace' ) . read ( ) . split ( '\\n' ) 
~~~ lines = codecs . open ( filename , 'r' , 'utf8' , 'replace' ) . read ( ) . split ( '\\n' ) 
~~ for linenum in range ( len ( lines ) - 1 ) : 
~~~ if lines [ linenum ] . endswith ( '\\r' ) : 
~~~ lines [ linenum ] = lines [ linenum ] . rstrip ( '\\r' ) 
crlf_lines . append ( linenum + 1 ) 
~~~ lf_lines . append ( linenum + 1 ) 
~~ ~~ ~~ except IOError : 
_RestoreFilters ( ) 
~~ file_extension = filename [ filename . rfind ( '.' ) + 1 : ] 
if filename != '-' and file_extension not in GetAllExtensions ( ) : 
~~~ bazel_gen_files = set ( [ 
"external/local_config_cc/libtool" , 
"external/local_config_cc/make_hashed_objlist.py" , 
"external/local_config_cc/wrapped_ar" , 
"external/local_config_cc/wrapped_clang" , 
"external/local_config_cc/xcrunwrapper.sh" , 
if not filename in bazel_gen_files : 
~~~ ProcessFileData ( filename , file_extension , lines , Error , 
if lf_lines and crlf_lines : 
~~~ for linenum in crlf_lines : 
~~~ Error ( filename , linenum , 'whitespace/newline' , 1 , 
~~ ~~ ~~ _RestoreFilters ( ) 
~~ def PrintCategories ( ) : 
~~ def ParseArguments ( args ) : 
~~~ ( opts , filenames ) = getopt . getopt ( args , '' , [ 'help' , 'output=' , 'verbose=' , 
'counting=' , 
'filter=' , 
'root=' , 
'repository=' , 
'linelength=' , 
'extensions=' , 
'exclude=' , 
'headers=' , 
'quiet' , 
'recursive' ] ) 
~~ except getopt . GetoptError : 
~~ verbosity = _VerboseLevel ( ) 
output_format = _OutputFormat ( ) 
filters = '' 
counting_style = '' 
recursive = False 
for ( opt , val ) in opts : 
~~~ if opt == '--help' : 
~~~ PrintUsage ( None ) 
~~ elif opt == '--output' : 
~~~ if val not in ( 'emacs' , 'vs7' , 'eclipse' , 'junit' ) : 
~~ output_format = val 
~~ elif opt == '--verbose' : 
~~~ verbosity = int ( val ) 
~~ elif opt == '--filter' : 
~~~ filters = val 
if not filters : 
~~~ PrintCategories ( ) 
~~ ~~ elif opt == '--counting' : 
~~~ if val not in ( 'total' , 'toplevel' , 'detailed' ) : 
~~ counting_style = val 
~~ elif opt == '--root' : 
~~ elif opt == '--repository' : 
~~~ global _repository 
_repository = val 
~~ elif opt == '--linelength' : 
~~ ~~ elif opt == '--exclude' : 
~~~ global _excludes 
if not _excludes : 
~~~ _excludes = set ( ) 
~~ _excludes . update ( glob . glob ( val ) ) 
~~ elif opt == '--extensions' : 
~~~ _valid_extensions = set ( val . split ( ',' ) ) 
~~ ~~ elif opt == '--headers' : 
~~~ _header_extensions = set ( val . split ( ',' ) ) 
~~ ~~ elif opt == '--recursive' : 
~~~ recursive = True 
~~ elif opt == '--quiet' : 
~~~ global _quiet 
_quiet = True 
~~ ~~ if not filenames : 
~~ if recursive : 
~~~ filenames = _ExpandDirectories ( filenames ) 
~~ if _excludes : 
~~~ filenames = _FilterExcludedFiles ( filenames ) 
~~ _SetOutputFormat ( output_format ) 
_SetVerboseLevel ( verbosity ) 
_SetFilters ( filters ) 
_SetCountingStyle ( counting_style ) 
return filenames 
~~ def _ExpandDirectories ( filenames ) : 
expanded = set ( ) 
for filename in filenames : 
~~~ if not os . path . isdir ( filename ) : 
~~~ expanded . add ( filename ) 
~~ for root , _ , files in os . walk ( filename ) : 
~~~ for loopfile in files : 
~~~ fullname = os . path . join ( root , loopfile ) 
if fullname . startswith ( '.' + os . path . sep ) : 
~~~ fullname = fullname [ len ( '.' + os . path . sep ) : ] 
~~ expanded . add ( fullname ) 
~~ ~~ ~~ filtered = [ ] 
for filename in expanded : 
~~~ if os . path . splitext ( filename ) [ 1 ] [ 1 : ] in GetAllExtensions ( ) : 
~~~ filtered . append ( filename ) 
~~ ~~ return filtered 
~~ def _FilterExcludedFiles ( filenames ) : 
exclude_paths = [ os . path . abspath ( f ) for f in _excludes ] 
return [ f for f in filenames if os . path . abspath ( f ) not in exclude_paths ] 
~~ def FindHeader ( self , header ) : 
for section_list in self . include_list : 
~~~ if f [ 0 ] == header : 
~~~ return f [ 1 ] 
~~ ~~ ~~ return - 1 
~~ def ResetSection ( self , directive ) : 
self . _section = self . _INITIAL_SECTION 
self . _last_header = '' 
if directive in ( 'if' , 'ifdef' , 'ifndef' ) : 
~~~ self . include_list . append ( [ ] ) 
~~ elif directive in ( 'else' , 'elif' ) : 
~~~ self . include_list [ - 1 ] = [ ] 
~~ ~~ def IsInAlphabeticalOrder ( self , clean_lines , linenum , header_path ) : 
if ( self . _last_header > header_path and 
Match ( r'^\\s*#\\s*include\\b' , clean_lines . elided [ linenum - 1 ] ) ) : 
~~ def CheckNextIncludeOrder ( self , header_type ) : 
( self . _TYPE_NAMES [ header_type ] , 
self . _SECTION_NAMES [ self . _section ] ) ) 
last_section = self . _section 
if header_type == _C_SYS_HEADER : 
~~~ if self . _section <= self . _C_SECTION : 
~~~ self . _section = self . _C_SECTION 
~~~ self . _last_header = '' 
return error_message 
~~ ~~ elif header_type == _CPP_SYS_HEADER : 
~~~ if self . _section <= self . _CPP_SECTION : 
~~~ self . _section = self . _CPP_SECTION 
~~ ~~ elif header_type == _LIKELY_MY_HEADER : 
~~~ if self . _section <= self . _MY_H_SECTION : 
~~~ self . _section = self . _MY_H_SECTION 
~~~ self . _section = self . _OTHER_H_SECTION 
~~ ~~ elif header_type == _POSSIBLE_MY_HEADER : 
~~~ assert header_type == _OTHER_HEADER 
self . _section = self . _OTHER_H_SECTION 
~~ if last_section != self . _section : 
~~ return '' 
~~ def SetVerboseLevel ( self , level ) : 
last_verbose_level = self . verbose_level 
self . verbose_level = level 
return last_verbose_level 
~~ def AddFilters ( self , filters ) : 
for filt in filters . split ( ',' ) : 
~~~ clean_filt = filt . strip ( ) 
if clean_filt : 
~~~ self . filters . append ( clean_filt ) 
~~ ~~ for filt in self . filters : 
~~~ if not ( filt . startswith ( '+' ) or filt . startswith ( '-' ) ) : 
~~ ~~ ~~ def IncrementErrorCount ( self , category ) : 
self . error_count += 1 
if self . counting in ( 'toplevel' , 'detailed' ) : 
~~~ if self . counting != 'detailed' : 
~~~ category = category . split ( '/' ) [ 0 ] 
~~ if category not in self . errors_by_category : 
~~~ self . errors_by_category [ category ] = 0 
~~ self . errors_by_category [ category ] += 1 
~~ ~~ def PrintErrorCounts ( self ) : 
for category , count in sorted ( iteritems ( self . errors_by_category ) ) : 
( category , count ) ) 
~~ if self . error_count > 0 : 
~~ ~~ def Begin ( self , function_name ) : 
self . in_a_function = True 
self . lines_in_function = 0 
self . current_function = function_name 
~~ def RepositoryName ( self ) : 
fullname = self . FullName ( ) 
if os . path . exists ( fullname ) : 
~~~ project_dir = os . path . dirname ( fullname ) 
if _repository : 
~~~ repo = FileInfo ( _repository ) . FullName ( ) 
root_dir = project_dir 
while os . path . exists ( root_dir ) : 
~~~ if os . path . normcase ( root_dir ) == os . path . normcase ( repo ) : 
~~~ return os . path . relpath ( fullname , root_dir ) . replace ( '\\\\' , '/' ) 
~~ one_up_dir = os . path . dirname ( root_dir ) 
if one_up_dir == root_dir : 
~~ root_dir = one_up_dir 
~~ ~~ if os . path . exists ( os . path . join ( project_dir , ".svn" ) ) : 
~~~ root_dir = project_dir 
one_up_dir = os . path . dirname ( root_dir ) 
while os . path . exists ( os . path . join ( one_up_dir , ".svn" ) ) : 
~~~ root_dir = os . path . dirname ( root_dir ) 
one_up_dir = os . path . dirname ( one_up_dir ) 
~~ prefix = os . path . commonprefix ( [ root_dir , project_dir ] ) 
return fullname [ len ( prefix ) + 1 : ] 
~~ root_dir = current_dir = os . path . dirname ( fullname ) 
while current_dir != os . path . dirname ( current_dir ) : 
~~~ if ( os . path . exists ( os . path . join ( current_dir , ".git" ) ) or 
os . path . exists ( os . path . join ( current_dir , ".hg" ) ) or 
os . path . exists ( os . path . join ( current_dir , ".svn" ) ) ) : 
~~~ root_dir = current_dir 
~~ current_dir = os . path . dirname ( current_dir ) 
~~ if ( os . path . exists ( os . path . join ( root_dir , ".git" ) ) or 
os . path . exists ( os . path . join ( root_dir , ".hg" ) ) or 
os . path . exists ( os . path . join ( root_dir , ".svn" ) ) ) : 
~~~ prefix = os . path . commonprefix ( [ root_dir , project_dir ] ) 
~~ ~~ return fullname 
~~ def Split ( self ) : 
googlename = self . RepositoryName ( ) 
project , rest = os . path . split ( googlename ) 
return ( project , ) + os . path . splitext ( rest ) 
~~ def _CollapseStrings ( elided ) : 
if _RE_PATTERN_INCLUDE . match ( elided ) : 
~~~ return elided 
~~ elided = _RE_PATTERN_CLEANSE_LINE_ESCAPES . sub ( '' , elided ) 
collapsed = '' 
~~~ match = Match ( r\ , elided ) 
~~~ collapsed += elided 
~~ head , quote , tail = match . groups ( ) 
if quote == \ : 
~~~ second_quote = tail . find ( \ ) 
if second_quote >= 0 : 
~~~ collapsed += head + \ 
elided = tail [ second_quote + 1 : ] 
~~~ if Search ( r'\\b(?:0[bBxX]?|[1-9])[0-9a-fA-F]*$' , head ) : 
~~~ match_literal = Match ( r'^((?:\\'?[0-9a-zA-Z_])*)(.*)$' , "\ + tail ) 
collapsed += head + match_literal . group ( 1 ) . replace ( "\ , '' ) 
elided = match_literal . group ( 2 ) 
~~~ second_quote = tail . find ( '\\'' ) 
~~~ collapsed += head + "\ 
~~ ~~ ~~ ~~ return collapsed 
~~ def CheckEnd ( self , filename , clean_lines , linenum , error ) : 
line = clean_lines . raw_lines [ linenum ] 
if ( linenum - self . starting_linenum < 10 
and not Match ( r'^\\s*};*\\s*(//|/\\*).*\\bnamespace\\b' , line ) ) : 
~~ if self . name : 
~~~ if not Match ( ( r'^\\s*};*\\s*(//|/\\*).*\\bnamespace\\s+' + 
re . escape ( self . name ) + r'[\\*/\\.\\\\\\s]*$' ) , 
line ) : 
~~~ error ( filename , linenum , 'readability/namespace' , 5 , 
self . name ) 
~~~ if not Match ( r'^\\s*};*\\s*(//|/\\*).*\\bnamespace[\\*/\\.\\\\\\s]*$' , line ) : 
~~ ~~ ~~ ~~ def InTemplateArgumentList ( self , clean_lines , linenum , pos ) : 
while linenum < clean_lines . NumLines ( ) : 
~~~ line = clean_lines . elided [ linenum ] 
match = Match ( r'^[^{};=\\[\\]\\.<>]*(.)' , line [ pos : ] ) 
pos = 0 
~~ token = match . group ( 1 ) 
pos += len ( match . group ( 0 ) ) 
if token in ( '{' , '}' , ';' ) : return False 
if token in ( '>' , '=' , '[' , ']' , '.' ) : return True 
if token != '<' : 
~~~ pos += 1 
if pos >= len ( line ) : 
~~ ( _ , end_line , end_pos ) = CloseExpression ( clean_lines , linenum , pos - 1 ) 
~~ linenum = end_line 
pos = end_pos 
~~ def UpdatePreprocessor ( self , line ) : 
if Match ( r'^\\s*#\\s*(if|ifdef|ifndef)\\b' , line ) : 
~~~ self . pp_stack . append ( _PreprocessorInfo ( copy . deepcopy ( self . stack ) ) ) 
~~ elif Match ( r'^\\s*#\\s*(else|elif)\\b' , line ) : 
~~~ if self . pp_stack : 
~~~ if not self . pp_stack [ - 1 ] . seen_else : 
~~~ self . pp_stack [ - 1 ] . seen_else = True 
self . pp_stack [ - 1 ] . stack_before_else = copy . deepcopy ( self . stack ) 
~~ self . stack = copy . deepcopy ( self . pp_stack [ - 1 ] . stack_before_if ) 
~~ ~~ elif Match ( r'^\\s*#\\s*endif\\b' , line ) : 
~~~ if self . pp_stack [ - 1 ] . seen_else : 
~~~ self . stack = self . pp_stack [ - 1 ] . stack_before_else 
~~ self . pp_stack . pop ( ) 
~~ ~~ ~~ def Update ( self , filename , clean_lines , linenum , error ) : 
if self . stack : 
~~~ self . previous_stack_top = self . stack [ - 1 ] 
~~~ self . previous_stack_top = None 
~~ self . UpdatePreprocessor ( line ) 
~~~ inner_block = self . stack [ - 1 ] 
depth_change = line . count ( '(' ) - line . count ( ')' ) 
inner_block . open_parentheses += depth_change 
if inner_block . inline_asm in ( _NO_ASM , _END_ASM ) : 
~~~ if ( depth_change != 0 and 
inner_block . open_parentheses == 1 and 
_MATCH_ASM . match ( line ) ) : 
~~~ inner_block . inline_asm = _INSIDE_ASM 
~~~ inner_block . inline_asm = _NO_ASM 
~~ ~~ elif ( inner_block . inline_asm == _INSIDE_ASM and 
inner_block . open_parentheses == 0 ) : 
~~~ inner_block . inline_asm = _END_ASM 
~~ ~~ while True : 
~~~ namespace_decl_match = Match ( r'^\\s*namespace\\b\\s*([:\\w]+)?(.*)$' , line ) 
if not namespace_decl_match : 
~~ new_namespace = _NamespaceInfo ( namespace_decl_match . group ( 1 ) , linenum ) 
self . stack . append ( new_namespace ) 
line = namespace_decl_match . group ( 2 ) 
if line . find ( '{' ) != - 1 : 
~~~ new_namespace . seen_open_brace = True 
line = line [ line . find ( '{' ) + 1 : ] 
~~ ~~ class_decl_match = Match ( 
r'^(\\s*(?:template\\s*<[\\w\\s<>,:=]*>\\s*)?' 
r'(class|struct)\\s+(?:[A-Z_]+\\s+)*(\\w+(?:::\\w+)*))' 
r'(.*)$' , line ) 
if ( class_decl_match and 
( not self . stack or self . stack [ - 1 ] . open_parentheses == 0 ) ) : 
~~~ end_declaration = len ( class_decl_match . group ( 1 ) ) 
if not self . InTemplateArgumentList ( clean_lines , linenum , end_declaration ) : 
~~~ self . stack . append ( _ClassInfo ( 
class_decl_match . group ( 3 ) , class_decl_match . group ( 2 ) , 
clean_lines , linenum ) ) 
line = class_decl_match . group ( 4 ) 
~~ ~~ if not self . SeenOpenBrace ( ) : 
~~~ self . stack [ - 1 ] . CheckBegin ( filename , clean_lines , linenum , error ) 
~~ if self . stack and isinstance ( self . stack [ - 1 ] , _ClassInfo ) : 
~~~ classinfo = self . stack [ - 1 ] 
access_match = Match ( 
r'^(.*)\\b(public|private|protected|signals)(\\s+(?:slots\\s*)?)?' 
r':(?:[^:]|$)' , 
if access_match : 
~~~ classinfo . access = access_match . group ( 2 ) 
indent = access_match . group ( 1 ) 
if ( len ( indent ) != classinfo . class_indent + 1 and 
Match ( r'^\\s*$' , indent ) ) : 
~~~ if classinfo . is_struct : 
~~ slots = '' 
if access_match . group ( 3 ) : 
~~~ slots = access_match . group ( 3 ) 
~~ error ( filename , linenum , 'whitespace/indent' , 3 , 
access_match . group ( 2 ) , slots , parent ) ) 
~~ ~~ ~~ while True : 
~~~ matched = Match ( r'^[^{;)}]*([{;)}])(.*)$' , line ) 
~~ token = matched . group ( 1 ) 
if token == '{' : 
~~~ if not self . SeenOpenBrace ( ) : 
~~~ self . stack [ - 1 ] . seen_open_brace = True 
~~ elif Match ( r\ , line ) : 
~~~ self . stack . append ( _ExternCInfo ( linenum ) ) 
~~~ self . stack . append ( _BlockInfo ( linenum , True ) ) 
if _MATCH_ASM . match ( line ) : 
~~~ self . stack [ - 1 ] . inline_asm = _BLOCK_ASM 
~~ ~~ ~~ elif token == ';' or token == ')' : 
~~~ self . stack . pop ( ) 
~~~ if self . stack : 
~~~ self . stack [ - 1 ] . CheckEnd ( filename , clean_lines , linenum , error ) 
self . stack . pop ( ) 
~~ ~~ line = matched . group ( 2 ) 
~~ ~~ def InnermostClass ( self ) : 
for i in range ( len ( self . stack ) , 0 , - 1 ) : 
~~~ classinfo = self . stack [ i - 1 ] 
if isinstance ( classinfo , _ClassInfo ) : 
~~~ return classinfo 
~~ def CheckCompletedBlocks ( self , filename , error ) : 
for obj in self . stack : 
~~~ if isinstance ( obj , _ClassInfo ) : 
~~~ error ( filename , obj . starting_linenum , 'build/class' , 5 , 
obj . name ) 
~~ elif isinstance ( obj , _NamespaceInfo ) : 
~~~ error ( filename , obj . starting_linenum , 'build/namespaces' , 5 , 
~~ ~~ ~~ def map ( self , map_function ) : 
from heronpy . streamlet . impl . mapbolt import MapStreamlet 
map_streamlet = MapStreamlet ( map_function , self ) 
self . _add_child ( map_streamlet ) 
return map_streamlet 
~~ def flat_map ( self , flatmap_function ) : 
from heronpy . streamlet . impl . flatmapbolt import FlatMapStreamlet 
fm_streamlet = FlatMapStreamlet ( flatmap_function , self ) 
self . _add_child ( fm_streamlet ) 
return fm_streamlet 
~~ def filter ( self , filter_function ) : 
from heronpy . streamlet . impl . filterbolt import FilterStreamlet 
filter_streamlet = FilterStreamlet ( filter_function , self ) 
self . _add_child ( filter_streamlet ) 
return filter_streamlet 
~~ def repartition ( self , num_partitions , repartition_function = None ) : 
from heronpy . streamlet . impl . repartitionbolt import RepartitionStreamlet 
if repartition_function is None : 
~~~ repartition_function = lambda x : x 
~~ repartition_streamlet = RepartitionStreamlet ( num_partitions , repartition_function , self ) 
self . _add_child ( repartition_streamlet ) 
return repartition_streamlet 
~~ def clone ( self , num_clones ) : 
retval = [ ] 
for i in range ( num_clones ) : 
~~~ retval . append ( self . repartition ( self . get_num_partitions ( ) ) ) 
~~ def reduce_by_window ( self , window_config , reduce_function ) : 
from heronpy . streamlet . impl . reducebywindowbolt import ReduceByWindowStreamlet 
reduce_streamlet = ReduceByWindowStreamlet ( window_config , reduce_function , self ) 
self . _add_child ( reduce_streamlet ) 
return reduce_streamlet 
~~ def union ( self , other_streamlet ) : 
from heronpy . streamlet . impl . unionbolt import UnionStreamlet 
union_streamlet = UnionStreamlet ( self , other_streamlet ) 
self . _add_child ( union_streamlet ) 
other_streamlet . _add_child ( union_streamlet ) 
return union_streamlet 
~~ def transform ( self , transform_operator ) : 
from heronpy . streamlet . impl . transformbolt import TransformStreamlet 
transform_streamlet = TransformStreamlet ( transform_operator , self ) 
self . _add_child ( transform_streamlet ) 
return transform_streamlet 
~~ def log ( self ) : 
from heronpy . streamlet . impl . logbolt import LogStreamlet 
log_streamlet = LogStreamlet ( self ) 
self . _add_child ( log_streamlet ) 
~~ def consume ( self , consume_function ) : 
from heronpy . streamlet . impl . consumebolt import ConsumeStreamlet 
consume_streamlet = ConsumeStreamlet ( consume_function , self ) 
self . _add_child ( consume_streamlet ) 
~~ def join ( self , join_streamlet , window_config , join_function ) : 
from heronpy . streamlet . impl . joinbolt import JoinStreamlet , JoinBolt 
join_streamlet_result = JoinStreamlet ( JoinBolt . INNER , window_config , 
join_function , self , join_streamlet ) 
self . _add_child ( join_streamlet_result ) 
join_streamlet . _add_child ( join_streamlet_result ) 
return join_streamlet_result 
~~ def outer_right_join ( self , join_streamlet , window_config , join_function ) : 
join_streamlet_result = JoinStreamlet ( JoinBolt . OUTER_RIGHT , window_config , 
~~ def outer_left_join ( self , join_streamlet , window_config , join_function ) : 
join_streamlet_result = JoinStreamlet ( JoinBolt . OUTER_LEFT , window_config , 
~~ def outer_join ( self , join_streamlet , window_config , join_function ) : 
join_streamlet_result = JoinStreamlet ( JoinBolt . OUTER , window_config , 
~~ def reduce_by_key_and_window ( self , window_config , reduce_function ) : 
from heronpy . streamlet . impl . reducebykeyandwindowbolt import ReduceByKeyAndWindowStreamlet 
reduce_streamlet = ReduceByKeyAndWindowStreamlet ( window_config , reduce_function , self ) 
~~ def _default_stage_name_calculator ( self , prefix , existing_stage_names ) : 
index = 1 
calculated_name = "" 
~~~ calculated_name = prefix + "-" + str ( index ) 
if calculated_name not in existing_stage_names : 
~~~ return calculated_name 
topology_names = self . get_arguments ( constants . PARAM_TOPOLOGY ) 
if len ( topology_names ) > 1 : 
~~~ if not clusters : 
self . write_error_response ( message ) 
~~ if not environs : 
topology_name = topology . name 
~~ if topology_names and topology_name not in topology_names : 
~~ ret [ cluster ] [ environ ] [ topology_name ] = topology . get_machines ( ) 
~~ def _async_stream_process_output ( process , stream_fn , handler ) : 
logging_thread = Thread ( target = stream_fn , args = ( process , handler , ) ) 
logging_thread . daemon = True 
logging_thread . start ( ) 
return logging_thread 
~~ def pick ( dirname , pattern ) : 
file_list = fnmatch . filter ( os . listdir ( dirname ) , pattern ) 
return file_list [ 0 ] if file_list else None 
metrics = yield tornado . gen . Task ( metricstimeline . getMetricsTimeline , 
topology . tmaster , component , metric_names , 
instances , int ( start_time ) , int ( end_time ) ) 
~~ ~~ def create_parser ( ) : 
prog = 'heron-explorer' , 
epilog = help_epilog , 
clusters . create_parser ( subparsers ) 
logicalplan . create_parser ( subparsers ) 
physicalplan . create_parser ( subparsers ) 
topologies . create_parser ( subparsers ) 
help . create_parser ( subparsers ) 
version . create_parser ( subparsers ) 
~~ def run ( command , * args ) : 
if command == 'clusters' : 
~~~ return clusters . run ( command , * args ) 
~~ elif command == 'topologies' : 
~~~ return topologies . run ( command , * args ) 
~~ elif command == 'containers' : 
~~~ return physicalplan . run_containers ( command , * args ) 
~~ elif command == 'metrics' : 
~~~ return physicalplan . run_metrics ( command , * args ) 
~~ elif command == 'components' : 
~~~ return logicalplan . run_components ( command , * args ) 
~~ elif command == 'spouts' : 
~~~ return logicalplan . run_spouts ( command , * args ) 
~~ elif command == 'bolts' : 
~~~ return logicalplan . run_bolts ( command , * args ) 
~~ elif command == 'help' : 
~~~ return help . run ( command , * args ) 
~~ elif command == 'version' : 
~~~ return version . run ( command , * args ) 
~~~ cluster_role_env = cl_args [ 'cluster/[role]/[env]' ] 
~~ cluster = config . get_heron_cluster ( cluster_role_env ) 
new_cl_args = dict ( ) 
~~~ cluster_tuple = config . parse_cluster_role_env ( cluster_role_env , config_path ) 
~~ cl_args . update ( new_cl_args ) 
~~ def main ( args ) : 
parser = create_parser ( ) 
if not args : 
~~ all_args = parse . insert_bool_values ( args ) 
args , unknown_args = parser . parse_known_args ( args = all_args ) 
command_line_args = vars ( args ) 
command_line_args [ 'help-command' ] = command 
command = 'help' 
~~ if command not in [ 'help' , 'version' ] : 
~~~ opts . set_tracker_url ( command_line_args ) 
if command not in [ 'topologies' , 'clusters' ] : 
~~~ command_line_args = extract_common_args ( command , parser , command_line_args ) 
~~ if not command_line_args : 
~~ start = time . time ( ) 
ret = run ( command , parser , command_line_args , unknown_args ) 
if command != 'help' : 
~~ return 0 if ret else 1 
~~ def correspond ( text ) : 
if text : 
~~~ subproc . stdin . write ( text ) 
~~ subproc . stdin . flush ( ) 
return get_lines ( ) 
~~ def _normalize ( c ) : 
if isinstance ( c , int ) : 
~~~ return bytes ( [ c ] ) 
~~ elif isinstance ( c , str ) : 
~~~ return bytes ( [ ord ( c ) ] ) 
~~~ return c 
~~ ~~ def _set_perms ( self , perms ) : 
self . _perms = perms 
~~ def access_ok ( self , access ) : 
for c in access : 
~~~ if c not in self . perms : 
~~ def _in_range ( self , index ) : 
if isinstance ( index , slice ) : 
~~~ in_range = index . start < index . stop and index . start >= self . start and index . stop <= self . end 
~~~ in_range = index >= self . start and index <= self . end 
~~ return in_range 
~~ def _get_offset ( self , index ) : 
if not self . _in_range ( index ) : 
~~ if isinstance ( index , slice ) : 
~~~ index = slice ( index . start - self . start , index . stop - self . start ) 
~~~ index -= self . start 
~~ def _ceil ( self , address ) : 
return ( ( ( address - 1 ) + self . page_size ) & ~ self . page_mask ) & self . memory_mask 
~~ def _search ( self , size , start = None , counter = 0 ) : 
assert size & self . page_mask == 0 
if start is None : 
~~~ end = { 32 : 0xf8000000 , 64 : 0x0000800000000000 } [ self . memory_bit_size ] 
start = end - size 
~~~ if start > self . memory_size - size : 
~~~ start = self . memory_size - size 
~~ end = start + size 
~~ consecutive_free = 0 
for p in range ( self . _page ( end - 1 ) , - 1 , - 1 ) : 
~~~ if p not in self . _page2map : 
~~~ consecutive_free += 0x1000 
~~~ consecutive_free = 0 
~~ if consecutive_free >= size : 
~~~ return p << self . page_bit_size 
~~ counter += 1 
if counter >= self . memory_size // self . page_size : 
~~ ~~ return self . _search ( size , self . memory_size - size , counter ) 
~~ def mmapFile ( self , addr , size , perms , filename , offset = 0 ) : 
assert size > 0 
self . cpu . _publish ( 'will_map_memory' , addr , size , perms , filename , offset ) 
if addr is not None : 
addr = self . _floor ( addr ) 
~~ size = self . _ceil ( size ) 
addr = self . _search ( size , addr ) 
for i in range ( self . _page ( addr ) , self . _page ( addr + size ) ) : 
~~ m = FileMap ( addr , size , perms , filename , offset ) 
self . _add ( m ) 
self . cpu . _publish ( 'did_map_memory' , addr , size , perms , filename , offset , addr ) 
return addr 
~~ def mmap ( self , addr , size , perms , data_init = None , name = None ) : 
self . cpu . _publish ( 'will_map_memory' , addr , size , perms , None , None ) 
~~ m = AnonMap ( start = addr , size = size , perms = perms , data_init = data_init , name = name ) 
self . cpu . _publish ( 'did_map_memory' , addr , size , perms , None , None , addr ) 
~~ def map_containing ( self , address ) : 
page_offset = self . _page ( address ) 
if page_offset not in self . _page2map : 
~~ return self . _page2map [ page_offset ] 
~~ def mappings ( self ) : 
for m in self . maps : 
~~~ if isinstance ( m , AnonMap ) : 
~~~ result . append ( ( m . start , m . end , m . perms , 0 , '' ) ) 
~~ elif isinstance ( m , FileMap ) : 
~~~ result . append ( ( m . start , m . end , m . perms , m . _offset , m . _filename ) ) 
~~~ result . append ( ( m . start , m . end , m . perms , 0 , m . name ) ) 
~~ ~~ return sorted ( result ) 
~~ def _maps_in_range ( self , start , end ) : 
addr = start 
while addr < end : 
~~~ if addr not in self : 
~~~ addr += self . page_size 
~~~ m = self . _page2map [ self . _page ( addr ) ] 
yield m 
addr = m . end 
~~ ~~ ~~ def munmap ( self , start , size ) : 
start = self . _floor ( start ) 
end = self . _ceil ( start + size ) 
self . cpu . _publish ( 'will_unmap_memory' , start , size ) 
for m in self . _maps_in_range ( start , end ) : 
~~~ self . _del ( m ) 
head , tail = m . split ( start ) 
middle , tail = tail . split ( end ) 
assert middle is not None 
if head : 
~~~ self . _add ( head ) 
~~ if tail : 
~~~ self . _add ( tail ) 
~~ ~~ self . cpu . _publish ( 'did_unmap_memory' , start , size ) 
~~ def pop_record_writes ( self ) : 
lst = self . _recording_stack . pop ( ) 
if self . _recording_stack : 
~~~ self . _recording_stack [ - 1 ] . extend ( lst ) 
~~ return lst 
~~ def munmap ( self , start , size ) : 
for addr in range ( start , start + size ) : 
~~~ if len ( self . _symbols ) == 0 : 
~~ if addr in self . _symbols : 
~~~ del self . _symbols [ addr ] 
~~ ~~ super ( ) . munmap ( start , size ) 
~~ def read ( self , address , size , force = False ) : 
size = self . _get_size ( size ) 
assert not issymbolic ( size ) 
if issymbolic ( address ) : 
~~~ assert solver . check ( self . constraints ) 
~~~ solutions = self . _try_get_solutions ( address , size , 'r' , force = force ) 
assert len ( solutions ) > 0 
~~ except TooManySolutions as e : 
~~~ m , M = solver . minmax ( self . constraints , address ) 
crashing_condition = True 
for start , end , perms , offset , name in self . mappings ( ) : 
~~~ if start <= M + size and end >= m : 
~~~ if 'r' in perms : 
~~~ crashing_condition = Operators . AND ( Operators . OR ( ( address + size ) . ult ( start ) , address . uge ( end ) ) , crashing_condition ) 
~~ ~~ ~~ if solver . can_be_true ( self . constraints , crashing_condition ) : 
~~~ raise InvalidSymbolicMemoryAccess ( address , 'r' , size , crashing_condition ) 
condition = False 
for base in e . solutions : 
~~~ condition = Operators . OR ( address == base , condition ) 
~~ from . state import ForkState 
~~ condition = False 
for base in solutions : 
~~ result = [ ] 
for offset in range ( size ) : 
~~~ for base in solutions : 
~~~ addr_value = base + offset 
byte = Operators . ORD ( self . map_containing ( addr_value ) [ addr_value ] ) 
if addr_value in self . _symbols : 
~~~ for condition , value in self . _symbols [ addr_value ] : 
~~~ byte = Operators . ITEBV ( 8 , condition , Operators . ORD ( value ) , byte ) 
~~ ~~ if len ( result ) > offset : 
~~~ result [ offset ] = Operators . ITEBV ( 8 , address == base , byte , result [ offset ] ) 
~~~ result . append ( byte ) 
~~ assert len ( result ) == offset + 1 
~~ ~~ return list ( map ( Operators . CHR , result ) ) 
~~~ result = list ( map ( Operators . ORD , super ( ) . read ( address , size , force ) ) ) 
~~~ if address + offset in self . _symbols : 
~~~ for condition , value in self . _symbols [ address + offset ] : 
~~~ if condition is True : 
~~~ result [ offset ] = Operators . ORD ( value ) 
~~~ result [ offset ] = Operators . ITEBV ( 8 , condition , Operators . ORD ( value ) , result [ offset ] ) 
~~ ~~ ~~ ~~ return list ( map ( Operators . CHR , result ) ) 
~~ ~~ def write ( self , address , value , force = False ) : 
size = len ( value ) 
~~~ solutions = self . _try_get_solutions ( address , size , 'w' , force = force ) 
~~~ condition = base == address 
self . _symbols . setdefault ( base + offset , [ ] ) . append ( ( condition , value [ offset ] ) ) 
~~~ for offset in range ( size ) : 
~~~ if issymbolic ( value [ offset ] ) : 
~~~ if not self . access_ok ( address + offset , 'w' , force ) : 
~~~ raise InvalidMemoryAccess ( address + offset , 'w' ) 
~~ self . _symbols [ address + offset ] = [ ( True , value [ offset ] ) ] 
~~~ del self . _symbols [ address + offset ] 
~~ super ( ) . write ( address + offset , [ value [ offset ] ] , force ) 
~~ ~~ ~~ ~~ def _try_get_solutions ( self , address , size , access , max_solutions = 0x1000 , force = False ) : 
assert issymbolic ( address ) 
solutions = solver . get_all_values ( self . constraints , address , maxcnt = max_solutions ) 
crashing_condition = False 
~~~ if not self . access_ok ( slice ( base , base + size ) , access , force ) : 
~~~ crashing_condition = Operators . OR ( address == base , crashing_condition ) 
~~ ~~ if solver . can_be_true ( self . constraints , crashing_condition ) : 
~~~ raise InvalidSymbolicMemoryAccess ( address , access , size , crashing_condition ) 
~~ return solutions 
map = AnonMap ( addr , size , perms ) 
self . _add ( map ) 
~~~ addr = self . _floor ( addr ) 
with open ( filename , 'rb' ) as f : 
~~ fdata = fdata [ offset : ] 
fdata = fdata . ljust ( size , b'\\0' ) 
~~~ Memory . write ( self , addr + i , chr ( fdata [ i ] ) , force = True ) 
~~ def _import_concrete_memory ( self , from_addr , to_addr ) : 
~~~ span = interval_intersection ( m . start , m . end , from_addr , to_addr ) 
if span is None : 
~~ start , stop = span 
for addr in range ( start , stop ) : 
~~~ if addr in self . backed_by_symbolic_store : 
~~ self . backing_array [ addr ] = Memory . read ( self , addr , 1 ) [ 0 ] 
self . backed_by_symbolic_store . add ( addr ) 
~~ ~~ ~~ def scan_mem ( self , data_to_find ) : 
if isinstance ( data_to_find , bytes ) : 
~~~ data_to_find = [ bytes ( [ c ] ) for c in data_to_find ] 
~~ for mapping in sorted ( self . maps ) : 
~~~ for ptr in mapping : 
~~~ if ptr + len ( data_to_find ) >= mapping . end : 
~~ candidate = mapping [ ptr : ptr + len ( data_to_find ) ] 
if issymbolic ( candidate [ 0 ] ) : 
~~ if candidate == data_to_find : 
~~~ yield ptr 
~~ ~~ ~~ ~~ def _reg_name ( self , reg_id ) : 
if reg_id >= X86_REG_ENDING : 
~~ cs_reg_name = self . cpu . instruction . reg_name ( reg_id ) 
if cs_reg_name is None or cs_reg_name . lower ( ) == '(invalid)' : 
~~ return self . cpu . _regfile . _alias ( cs_reg_name . upper ( ) ) 
~~ def values_from ( self , base ) : 
word_bytes = self . _cpu . address_bit_size // 8 
~~~ yield base 
base += word_bytes 
~~ ~~ def get_argument_values ( self , model , prefix_args ) : 
spec = inspect . getfullargspec ( model ) 
if spec . varargs : 
~~ nargs = len ( spec . args ) - len ( prefix_args ) 
if inspect . ismethod ( model ) : 
~~~ nargs -= 1 
~~ def resolve_argument ( arg ) : 
~~~ if isinstance ( arg , str ) : 
~~~ return self . _cpu . read_register ( arg ) 
~~~ return self . _cpu . read_int ( arg ) 
~~ ~~ descriptors = self . get_arguments ( ) 
argument_iter = map ( resolve_argument , descriptors ) 
if isvariadic ( model ) : 
~~~ arguments = prefix_args + ( argument_iter , ) 
~~~ arguments = prefix_args + tuple ( islice ( argument_iter , nargs ) ) 
~~ def invoke ( self , model , prefix_args = None ) : 
prefix_args = prefix_args or ( ) 
arguments = self . get_argument_values ( model , prefix_args ) 
~~~ result = model ( * arguments ) 
~~ except ConcretizeArgument as e : 
~~~ assert e . argnum >= len ( prefix_args ) , "Can\ 
idx = e . argnum - len ( prefix_args ) 
descriptors = self . get_arguments ( ) 
src = next ( islice ( descriptors , idx , idx + 1 ) ) 
if isinstance ( src , str ) : 
~~~ raise ConcretizeRegister ( self . _cpu , src , msg ) 
~~~ raise ConcretizeMemory ( self . _cpu . memory , src , self . _cpu . address_bit_size , msg ) 
~~~ if result is not None : 
~~~ self . write_result ( result ) 
~~ self . ret ( ) 
~~ def write_register ( self , register , value ) : 
self . _publish ( 'will_write_register' , register , value ) 
value = self . _regfile . write ( register , value ) 
self . _publish ( 'did_write_register' , register , value ) 
~~ def read_register ( self , register ) : 
self . _publish ( 'will_read_register' , register ) 
value = self . _regfile . read ( register ) 
self . _publish ( 'did_read_register' , register , value ) 
~~ def emulate_until ( self , target : int ) : 
self . _concrete = True 
self . _break_unicorn_at = target 
if self . emu : 
~~~ self . emu . _stop_at = target 
~~ ~~ def write_int ( self , where , expression , size = None , force = False ) : 
~~~ size = self . address_bit_size 
~~ assert size in SANE_SIZES 
self . _publish ( 'will_write_memory' , where , expression , size ) 
data = [ Operators . CHR ( Operators . EXTRACT ( expression , offset , 8 ) ) for offset in range ( 0 , size , 8 ) ] 
self . _memory . write ( where , data , force ) 
self . _publish ( 'did_write_memory' , where , expression , size ) 
~~ def _raw_read ( self , where : int , size = 1 ) -> bytes : 
map = self . memory . map_containing ( where ) 
start = map . _get_offset ( where ) 
mapType = type ( map ) 
if mapType is FileMap : 
~~~ end = map . _get_offset ( where + size ) 
if end > map . _mapped_size : 
~~ raw_data = map . _data [ map . _get_offset ( where ) : min ( end , map . _mapped_size ) ] 
if len ( raw_data ) < end : 
~~~ raw_data += b'\\x00' * ( end - len ( raw_data ) ) 
~~ data = b'' 
for offset in sorted ( map . _overlay . keys ( ) ) : 
~~~ data += raw_data [ len ( data ) : offset ] 
data += map . _overlay [ offset ] 
~~ data += raw_data [ len ( data ) : ] 
~~ elif mapType is AnonMap : 
~~~ data = bytes ( map . _data [ start : start + size ] ) 
~~~ data = b'' . join ( self . memory [ where : where + size ] ) 
~~ def read_int ( self , where , size = None , force = False ) : 
self . _publish ( 'will_read_memory' , where , size ) 
data = self . _memory . read ( where , size // 8 , force ) 
assert ( 8 * len ( data ) ) == size 
value = Operators . CONCAT ( size , * map ( Operators . ORD , reversed ( data ) ) ) 
self . _publish ( 'did_read_memory' , where , value , size ) 
~~ def write_bytes ( self , where , data , force = False ) : 
mp = self . memory . map_containing ( where ) 
can_write_raw = type ( mp ) is AnonMap and isinstance ( data , ( str , bytes ) ) and ( mp . end - mp . start + 1 ) >= len ( data ) >= 1024 and not issymbolic ( data ) and self . _concrete 
if can_write_raw : 
offset = mp . _get_offset ( where ) 
if isinstance ( data , str ) : 
~~~ data = bytes ( data . encode ( 'utf-8' ) ) 
~~ mp . _data [ offset : offset + len ( data ) ] = data 
self . _publish ( 'did_write_memory' , where , data , 8 * len ( data ) ) 
~~~ for i in range ( len ( data ) ) : 
~~~ self . write_int ( where + i , Operators . ORD ( data [ i ] ) , 8 , force ) 
~~ ~~ ~~ def read_bytes ( self , where , size , force = False ) : 
~~~ result . append ( Operators . CHR ( self . read_int ( where + i , 8 , force ) ) ) 
~~ def write_string ( self , where , string , max_length = None , force = False ) : 
if max_length is not None : 
~~~ string = string [ : max_length - 1 ] 
~~ self . write_bytes ( where , string + '\\x00' , force ) 
~~ def read_string ( self , where , max_length = None , force = False ) : 
s = io . BytesIO ( ) 
~~~ c = self . read_int ( where , 8 , force ) 
if issymbolic ( c ) or c == 0 : 
~~ if max_length is not None : 
~~~ if max_length == 0 : 
~~ max_length = max_length - 1 
~~ s . write ( Operators . CHR ( c ) ) 
where += 1 
~~ return s . getvalue ( ) . decode ( ) 
~~ def push_bytes ( self , data , force = False ) : 
self . STACK -= len ( data ) 
self . write_bytes ( self . STACK , data , force ) 
return self . STACK 
~~ def pop_bytes ( self , nbytes , force = False ) : 
data = self . read_bytes ( self . STACK , nbytes , force = force ) 
self . STACK += nbytes 
~~ def push_int ( self , value , force = False ) : 
self . STACK -= self . address_bit_size // 8 
self . write_int ( self . STACK , value , force = force ) 
~~ def pop_int ( self , force = False ) : 
value = self . read_int ( self . STACK , force = force ) 
self . STACK += self . address_bit_size // 8 
~~ def decode_instruction ( self , pc ) : 
if pc in self . _instruction_cache : 
~~~ return self . _instruction_cache [ pc ] 
~~ text = b'' 
for address in range ( pc , pc + self . max_instr_width ) : 
~~~ if not self . memory . access_ok ( address , 'x' ) : 
~~ c = self . memory [ address ] 
if issymbolic ( c ) : 
~~~ if isinstance ( self . memory , LazySMemory ) : 
~~~ vals = visitors . simplify_array_select ( c ) 
c = bytes ( [ vals [ 0 ] ] ) 
~~ except visitors . ArraySelectSimplifier . ExpressionNotSimple : 
~~~ c = struct . pack ( 'B' , solver . get_value ( self . memory . constraints , c ) ) 
~~ ~~ elif isinstance ( c , Constant ) : 
~~~ c = bytes ( [ c . value ] ) 
raise ConcretizeMemory ( self . memory , 
address = pc , 
size = 8 * self . max_instr_width , 
policy = 'INSTRUCTION' ) 
~~ ~~ text += c 
~~ code = text . ljust ( self . max_instr_width , b'\\x00' ) 
~~~ insn = self . disasm . disassemble_instruction ( code , pc ) 
~~ except StopIteration as e : 
~~~ raise DecodeException ( pc , code ) 
~~ if not self . memory . access_ok ( slice ( pc , pc + insn . size ) , 'x' ) : 
raise InvalidMemoryAccess ( pc , 'x' ) 
~~ insn . operands = self . _wrap_operands ( insn . operands ) 
self . _instruction_cache [ pc ] = insn 
return insn 
~~ def execute ( self ) : 
if issymbolic ( self . PC ) : 
~~~ raise ConcretizeRegister ( self , 'PC' , policy = 'ALL' ) 
~~ if not self . memory . access_ok ( self . PC , 'x' ) : 
~~~ raise InvalidMemoryAccess ( self . PC , 'x' ) 
~~ self . _publish ( 'will_decode_instruction' , self . PC ) 
insn = self . decode_instruction ( self . PC ) 
self . _last_pc = self . PC 
self . _publish ( 'will_execute_instruction' , self . PC , insn ) 
if insn . address != self . PC : 
~~ name = self . canonicalize_instruction_name ( insn ) 
if logger . level == logging . DEBUG : 
~~~ logger . debug ( self . render_instruction ( insn ) ) 
for l in self . render_registers ( ) : 
~~~ register_logger . debug ( l ) 
~~~ if self . _concrete and 'SYSCALL' in name : 
~~~ self . emu . sync_unicorn_to_manticore ( ) 
~~ if self . _concrete and 'SYSCALL' not in name : 
~~~ self . emulate ( insn ) 
if self . PC == self . _break_unicorn_at : 
self . _break_unicorn_at = None 
self . _concrete = False 
~~~ implementation = getattr ( self , name , None ) 
if implementation is not None : 
~~~ implementation ( * insn . operands ) 
insn . address , text_bytes , insn . mnemonic , insn . op_str ) 
self . backup_emulate ( insn ) 
~~ ~~ ~~ except ( Interruption , Syscall ) as e : 
~~~ e . on_handled = lambda : self . _publish_instruction_as_executed ( insn ) 
~~~ self . _publish_instruction_as_executed ( insn ) 
~~ ~~ def _publish_instruction_as_executed ( self , insn ) : 
self . _icount += 1 
self . _publish ( 'did_execute_instruction' , self . _last_pc , self . PC , insn ) 
~~ def emulate ( self , insn ) : 
if self . _concrete : 
~~~ self . concrete_emulate ( insn ) 
~~~ self . backup_emulate ( insn ) 
~~ ~~ def concrete_emulate ( self , insn ) : 
if not self . emu : 
~~~ self . emu = ConcreteUnicornEmulator ( self ) 
self . emu . _stop_at = self . _break_unicorn_at 
~~~ self . emu . emulate ( insn ) 
~~ except unicorn . UcError as e : 
~~~ if e . errno == unicorn . UC_ERR_INSN_INVALID : 
~~ raise InstructionEmulationError ( str ( e ) ) 
~~ ~~ def backup_emulate ( self , insn ) : 
if not hasattr ( self , 'backup_emu' ) : 
~~~ self . backup_emu = UnicornEmulator ( self ) 
~~~ self . backup_emu . emulate ( insn ) 
~~~ del self . backup_emu 
~~ ~~ def viz_trace ( view ) : 
tv = TraceVisualizer ( view , None ) 
if tv . workspace is None : 
~~~ tv . workspace = get_workspace ( ) 
~~ tv . visualize ( ) 
~~ def viz_live_trace ( view ) : 
tv = TraceVisualizer ( view , None , live = True ) 
~~ tv . live_update = True 
tv . visualize ( ) 
~~ def visualize ( self ) : 
if os . path . isfile ( self . workspace ) : 
~~~ t = threading . Thread ( target = self . highlight_from_file , 
args = ( self . workspace , ) ) 
~~ elif os . path . isdir ( self . workspace ) : 
~~~ t = threading . Thread ( target = self . highlight_from_dir , 
~~ t . start ( ) 
~~ def t_TOKEN ( t ) : 
~~~ '[a-zA-Z0-9]+' 
if re_TYPE . match ( t . value ) : 
~~~ t . type = 'TYPE' 
~~ elif re_PTR . match ( t . value ) : 
~~~ t . type = 'PTR' 
~~ elif re_NUMBER . match ( t . value ) : 
~~~ if t . value . startswith ( '0x' ) : 
~~~ t . value = t . value [ 2 : ] 
~~ t . value = int ( t . value , 16 ) 
t . type = 'NUMBER' 
~~ elif re_REGISTER . match ( t . value ) : 
~~~ t . type = 'REGISTER' 
~~ elif re_SEGMENT . match ( t . value ) : 
~~~ t . type = 'SEGMENT' 
~~~ raise Exception ( f"Unknown:<{t.value}>" ) 
~~ def p_expression_deref ( p ) : 
size = sizes [ p [ 1 ] ] 
address = p [ 4 ] 
char_list = functions [ 'read_memory' ] ( address , size ) 
value = Operators . CONCAT ( 8 * len ( char_list ) , * reversed ( map ( Operators . ORD , char_list ) ) ) 
p [ 0 ] = value 
~~ def p_expression_derefseg ( p ) : 
address = p [ 6 ] 
seg = functions [ 'read_register' ] ( p [ 3 ] ) 
base , limit , _ = functions [ 'get_descriptor' ] ( seg ) 
address = base + address 
~~ def _get_flags ( self , reg ) : 
def make_symbolic ( flag_expr ) : 
~~~ register_size = 32 if reg == 'EFLAGS' else 64 
value , offset = flag_expr 
return Operators . ITEBV ( register_size , value , 
BitVecConstant ( register_size , 1 << offset ) , 
BitVecConstant ( register_size , 0 ) ) 
~~ flags = [ ] 
for flag , offset in self . _flags . items ( ) : 
~~~ flags . append ( ( self . _registers [ flag ] , offset ) ) 
~~ if any ( issymbolic ( flag ) for flag , offset in flags ) : 
~~~ res = reduce ( operator . or_ , map ( make_symbolic , flags ) ) 
~~~ res = 0 
for flag , offset in flags : 
~~~ res += flag << offset 
~~ def _set_flags ( self , reg , res ) : 
~~~ self . write ( flag , Operators . EXTRACT ( res , offset , 1 ) ) 
~~ ~~ def push ( cpu , value , size ) : 
assert size in ( 8 , 16 , cpu . address_bit_size ) 
cpu . STACK = cpu . STACK - size // 8 
base , _ , _ = cpu . get_descriptor ( cpu . read_register ( 'SS' ) ) 
address = cpu . STACK + base 
cpu . write_int ( address , value , size ) 
~~ def pop ( cpu , size ) : 
assert size in ( 16 , cpu . address_bit_size ) 
base , _ , _ = cpu . get_descriptor ( cpu . SS ) 
value = cpu . read_int ( address , size ) 
cpu . STACK = cpu . STACK + size // 8 
~~ def invalidate_cache ( cpu , address , size ) : 
cache = cpu . instruction_cache 
~~~ if address + offset in cache : 
~~~ del cache [ address + offset ] 
~~ ~~ ~~ def CPUID ( cpu ) : 
conf = { 0x0 : ( 0x0000000d , 0x756e6547 , 0x6c65746e , 0x49656e69 ) , 
0x1 : ( 0x000306c3 , 0x05100800 , 0x7ffafbff , 0xbfebfbff ) , 
0x2 : ( 0x76035a01 , 0x00f0b5ff , 0x00000000 , 0x00c10000 ) , 
0x4 : { 0x0 : ( 0x1c004121 , 0x01c0003f , 0x0000003f , 0x00000000 ) , 
0x1 : ( 0x1c004122 , 0x01c0003f , 0x0000003f , 0x00000000 ) , 
0x2 : ( 0x1c004143 , 0x01c0003f , 0x000001ff , 0x00000000 ) , 
0x3 : ( 0x1c03c163 , 0x03c0003f , 0x00000fff , 0x00000006 ) } , 
0x7 : ( 0x00000000 , 0x00000000 , 0x00000000 , 0x00000000 ) , 
0x8 : ( 0x00000000 , 0x00000000 , 0x00000000 , 0x00000000 ) , 
0xb : { 0x0 : ( 0x00000001 , 0x00000002 , 0x00000100 , 0x00000005 ) , 
0x1 : ( 0x00000004 , 0x00000004 , 0x00000201 , 0x00000003 ) } , 
0xd : { 0x0 : ( 0x00000000 , 0x00000000 , 0x00000000 , 0x00000000 ) , 
0x1 : ( 0x00000000 , 0x00000000 , 0x00000000 , 0x00000000 ) } , 
if cpu . EAX not in conf : 
cpu . EAX , cpu . EBX , cpu . ECX , cpu . EDX = 0 , 0 , 0 , 0 
~~ if isinstance ( conf [ cpu . EAX ] , tuple ) : 
~~~ cpu . EAX , cpu . EBX , cpu . ECX , cpu . EDX = conf [ cpu . EAX ] 
~~ if cpu . ECX not in conf [ cpu . EAX ] : 
~~ cpu . EAX , cpu . EBX , cpu . ECX , cpu . EDX = conf [ cpu . EAX ] [ cpu . ECX ] 
~~ def AND ( cpu , dest , src ) : 
if src . size == 64 and src . type == 'immediate' and dest . size == 64 : 
~~~ arg1 = Operators . SEXTEND ( src . read ( ) , 32 , 64 ) 
~~~ arg1 = src . read ( ) 
~~ res = dest . write ( dest . read ( ) & arg1 ) 
cpu . _calculate_logic_flags ( dest . size , res ) 
~~ def TEST ( cpu , src1 , src2 ) : 
temp = src1 . read ( ) & src2 . read ( ) 
cpu . SF = ( temp & ( 1 << ( src1 . size - 1 ) ) ) != 0 
cpu . ZF = temp == 0 
cpu . PF = cpu . _calculate_parity_flag ( temp ) 
cpu . CF = False 
cpu . OF = False 
~~ def XOR ( cpu , dest , src ) : 
if dest == src : 
~~~ res = dest . write ( 0 ) 
~~~ res = dest . write ( dest . read ( ) ^ src . read ( ) ) 
~~ cpu . _calculate_logic_flags ( dest . size , res ) 
~~ def OR ( cpu , dest , src ) : 
res = dest . write ( dest . read ( ) | src . read ( ) ) 
~~ def AAA ( cpu ) : 
cpu . AF = Operators . OR ( cpu . AL & 0x0F > 9 , cpu . AF ) 
cpu . CF = cpu . AF 
cpu . AH = Operators . ITEBV ( 8 , cpu . AF , cpu . AH + 1 , cpu . AH ) 
cpu . AL = Operators . ITEBV ( 8 , cpu . AF , cpu . AL + 6 , cpu . AL ) 
cpu . AL = cpu . AL & 0x0f 
~~ def AAD ( cpu , imm = None ) : 
if imm is None : 
~~~ imm = 10 
~~~ imm = imm . read ( ) 
~~ cpu . AL += cpu . AH * imm 
cpu . AH = 0 
cpu . _calculate_logic_flags ( 8 , cpu . AL ) 
~~ def AAM ( cpu , imm = None ) : 
~~ cpu . AH = Operators . UDIV ( cpu . AL , imm ) 
cpu . AL = Operators . UREM ( cpu . AL , imm ) 
~~ def AAS ( cpu ) : 
if ( cpu . AL & 0x0F > 9 ) or cpu . AF == 1 : 
~~~ cpu . AX = cpu . AX - 6 
cpu . AH = cpu . AH - 1 
cpu . AF = True 
cpu . CF = True 
~~~ cpu . AF = False 
~~ cpu . AL = cpu . AL & 0x0f 
~~ def ADC ( cpu , dest , src ) : 
cpu . _ADD ( dest , src , carry = True ) 
~~ def ADD ( cpu , dest , src ) : 
cpu . _ADD ( dest , src , carry = False ) 
~~ def CMP ( cpu , src1 , src2 ) : 
arg0 = src1 . read ( ) 
arg1 = Operators . SEXTEND ( src2 . read ( ) , src2 . size , src1 . size ) 
cpu . _calculate_CMP_flags ( src1 . size , arg0 - arg1 , arg0 , arg1 ) 
~~ def CMPXCHG ( cpu , dest , src ) : 
size = dest . size 
reg_name = { 8 : 'AL' , 16 : 'AX' , 32 : 'EAX' , 64 : 'RAX' } [ size ] 
accumulator = cpu . read_register ( reg_name ) 
sval = src . read ( ) 
dval = dest . read ( ) 
cpu . write_register ( reg_name , dval ) 
dest . write ( Operators . ITEBV ( size , accumulator == dval , sval , dval ) ) 
cpu . _calculate_CMP_flags ( size , accumulator - dval , accumulator , dval ) 
~~ def CMPXCHG8B ( cpu , dest ) : 
cmp_reg_name_l = { 64 : 'EAX' , 128 : 'RAX' } [ size ] 
cmp_reg_name_h = { 64 : 'EDX' , 128 : 'RDX' } [ size ] 
src_reg_name_l = { 64 : 'EBX' , 128 : 'RBX' } [ size ] 
src_reg_name_h = { 64 : 'ECX' , 128 : 'RCX' } [ size ] 
cmph = cpu . read_register ( cmp_reg_name_h ) 
cmpl = cpu . read_register ( cmp_reg_name_l ) 
srch = cpu . read_register ( src_reg_name_h ) 
srcl = cpu . read_register ( src_reg_name_l ) 
cmp0 = Operators . CONCAT ( size , cmph , cmpl ) 
src0 = Operators . CONCAT ( size , srch , srcl ) 
arg_dest = dest . read ( ) 
cpu . ZF = arg_dest == cmp0 
dest . write ( 
Operators . ITEBV ( size , cpu . ZF , 
Operators . CONCAT ( size , srch , srcl ) , 
arg_dest ) 
cpu . write_register ( cmp_reg_name_l , Operators . ITEBV ( size // 2 , cpu . ZF , cmpl , 
Operators . EXTRACT ( arg_dest , 0 , size // 2 ) ) ) 
cpu . write_register ( cmp_reg_name_h , Operators . ITEBV ( size // 2 , cpu . ZF , cmph , 
Operators . EXTRACT ( arg_dest , size // 2 , size // 2 ) ) ) 
~~ def DAA ( cpu ) : 
cpu . AF = Operators . OR ( ( cpu . AL & 0x0f ) > 9 , cpu . AF ) 
oldAL = cpu . AL 
cpu . CF = Operators . ITE ( cpu . AF , Operators . OR ( cpu . CF , cpu . AL < oldAL ) , cpu . CF ) 
cpu . CF = Operators . OR ( ( cpu . AL & 0xf0 ) > 0x90 , cpu . CF ) 
cpu . AL = Operators . ITEBV ( 8 , cpu . CF , cpu . AL + 0x60 , cpu . AL ) 
cpu . ZF = cpu . AL == 0 
cpu . SF = ( cpu . AL & 0x80 ) != 0 
cpu . PF = cpu . _calculate_parity_flag ( cpu . AL ) 
~~ def DAS ( cpu ) : 
oldCF = cpu . CF 
cpu . AL = Operators . ITEBV ( 8 , cpu . AF , cpu . AL - 6 , cpu . AL ) 
cpu . CF = Operators . ITE ( cpu . AF , Operators . OR ( oldCF , cpu . AL > oldAL ) , cpu . CF ) 
cpu . CF = Operators . ITE ( Operators . OR ( oldAL > 0x99 , oldCF ) , True , cpu . CF ) 
cpu . AL = Operators . ITEBV ( 8 , Operators . OR ( oldAL > 0x99 , oldCF ) , cpu . AL - 0x60 , cpu . AL ) 
~~ def DIV ( cpu , src ) : 
size = src . size 
reg_name_h = { 8 : 'DL' , 16 : 'DX' , 32 : 'EDX' , 64 : 'RDX' } [ size ] 
reg_name_l = { 8 : 'AL' , 16 : 'AX' , 32 : 'EAX' , 64 : 'RAX' } [ size ] 
dividend = Operators . CONCAT ( size * 2 , 
cpu . read_register ( reg_name_h ) , 
cpu . read_register ( reg_name_l ) ) 
divisor = Operators . ZEXTEND ( src . read ( ) , size * 2 ) 
if isinstance ( divisor , int ) and divisor == 0 : 
~~~ raise DivideByZeroError ( ) 
~~ quotient = Operators . UDIV ( dividend , divisor ) 
MASK = ( 1 << size ) - 1 
if isinstance ( quotient , int ) and quotient > MASK : 
~~ remainder = Operators . UREM ( dividend , divisor ) 
cpu . write_register ( reg_name_l , Operators . EXTRACT ( quotient , 0 , size ) ) 
cpu . write_register ( reg_name_h , Operators . EXTRACT ( remainder , 0 , size ) ) 
~~ def IDIV ( cpu , src ) : 
reg_name_h = { 8 : 'AH' , 16 : 'DX' , 32 : 'EDX' , 64 : 'RDX' } [ src . size ] 
reg_name_l = { 8 : 'AL' , 16 : 'AX' , 32 : 'EAX' , 64 : 'RAX' } [ src . size ] 
dividend = Operators . CONCAT ( src . size * 2 , 
divisor = src . read ( ) 
~~ dst_size = src . size * 2 
divisor = Operators . SEXTEND ( divisor , src . size , dst_size ) 
mask = ( 1 << dst_size ) - 1 
sign_mask = 1 << ( dst_size - 1 ) 
dividend_sign = ( dividend & sign_mask ) != 0 
divisor_sign = ( divisor & sign_mask ) != 0 
if isinstance ( divisor , int ) : 
~~~ if divisor_sign : 
~~~ divisor = ( ( ~ divisor ) + 1 ) & mask 
divisor = - divisor 
~~ ~~ if isinstance ( dividend , int ) : 
~~~ if dividend_sign : 
~~~ dividend = ( ( ~ dividend ) + 1 ) & mask 
dividend = - dividend 
~~ ~~ quotient = Operators . SDIV ( dividend , divisor ) 
if ( isinstance ( dividend , int ) and 
isinstance ( dividend , int ) ) : 
~~~ remainder = dividend - ( quotient * divisor ) 
~~~ remainder = Operators . SREM ( dividend , divisor ) 
~~ cpu . write_register ( reg_name_l , Operators . EXTRACT ( quotient , 0 , src . size ) ) 
cpu . write_register ( reg_name_h , Operators . EXTRACT ( remainder , 0 , src . size ) ) 
~~ def IMUL ( cpu , * operands ) : 
dest = operands [ 0 ] 
OperandSize = dest . size 
reg_name_h = { 8 : 'AH' , 16 : 'DX' , 32 : 'EDX' , 64 : 'RDX' } [ OperandSize ] 
reg_name_l = { 8 : 'AL' , 16 : 'AX' , 32 : 'EAX' , 64 : 'RAX' } [ OperandSize ] 
arg0 = dest . read ( ) 
arg1 = None 
arg2 = None 
res = None 
if len ( operands ) == 1 : 
~~~ arg1 = cpu . read_register ( reg_name_l ) 
temp = ( Operators . SEXTEND ( arg0 , OperandSize , OperandSize * 2 ) * 
Operators . SEXTEND ( arg1 , OperandSize , OperandSize * 2 ) ) 
temp = temp & ( ( 1 << ( OperandSize * 2 ) ) - 1 ) 
cpu . write_register ( reg_name_l , 
Operators . EXTRACT ( temp , 0 , OperandSize ) ) 
cpu . write_register ( reg_name_h , 
Operators . EXTRACT ( temp , OperandSize , OperandSize ) ) 
res = Operators . EXTRACT ( temp , 0 , OperandSize ) 
~~ elif len ( operands ) == 2 : 
~~~ arg1 = operands [ 1 ] . read ( ) 
arg1 = Operators . SEXTEND ( arg1 , OperandSize , OperandSize * 2 ) 
temp = Operators . SEXTEND ( arg0 , OperandSize , OperandSize * 2 ) * arg1 
res = dest . write ( Operators . EXTRACT ( temp , 0 , OperandSize ) ) 
arg2 = operands [ 2 ] . read ( ) 
temp = ( Operators . SEXTEND ( arg1 , OperandSize , OperandSize * 2 ) * 
Operators . SEXTEND ( arg2 , operands [ 2 ] . size , OperandSize * 2 ) ) 
~~ cpu . CF = ( Operators . SEXTEND ( res , OperandSize , OperandSize * 2 ) != temp ) 
cpu . OF = cpu . CF 
~~ def INC ( cpu , dest ) : 
res = dest . write ( arg0 + 1 ) 
res &= ( 1 << dest . size ) - 1 
SIGN_MASK = 1 << ( dest . size - 1 ) 
cpu . AF = ( ( arg0 ^ 1 ) ^ res ) & 0x10 != 0 
cpu . ZF = res == 0 
cpu . SF = ( res & SIGN_MASK ) != 0 
cpu . OF = res == SIGN_MASK 
cpu . PF = cpu . _calculate_parity_flag ( res ) 
~~ def MUL ( cpu , src ) : 
reg_name_low , reg_name_high = { 8 : ( 'AL' , 'AH' ) , 
16 : ( 'AX' , 'DX' ) , 
32 : ( 'EAX' , 'EDX' ) , 
64 : ( 'RAX' , 'RDX' ) } [ size ] 
res = ( Operators . ZEXTEND ( cpu . read_register ( reg_name_low ) , 256 ) * 
Operators . ZEXTEND ( src . read ( ) , 256 ) ) 
cpu . write_register ( reg_name_low , Operators . EXTRACT ( res , 0 , size ) ) 
cpu . write_register ( reg_name_high , Operators . EXTRACT ( res , size , size ) ) 
cpu . OF = Operators . EXTRACT ( res , size , size ) != 0 
cpu . CF = cpu . OF 
~~ def NEG ( cpu , dest ) : 
source = dest . read ( ) 
res = dest . write ( - source ) 
cpu . CF = source != 0 
cpu . AF = ( res & 0x0f ) != 0x00 
~~ def SBB ( cpu , dest , src ) : 
cpu . _SUB ( dest , src , carry = True ) 
~~ def SUB ( cpu , dest , src ) : 
cpu . _SUB ( dest , src , carry = False ) 
~~ def XADD ( cpu , dest , src ) : 
MASK = ( 1 << dest . size ) - 1 
arg1 = src . read ( ) 
temp = ( arg1 + arg0 ) & MASK 
src . write ( arg0 ) 
dest . write ( temp ) 
tempCF = Operators . OR ( Operators . ULT ( temp , arg0 ) , Operators . ULT ( temp , arg1 ) ) 
cpu . CF = tempCF 
cpu . AF = ( ( arg0 ^ arg1 ) ^ temp ) & 0x10 != 0 
cpu . SF = ( temp & SIGN_MASK ) != 0 
cpu . OF = ( ( ( arg0 ^ arg1 ^ SIGN_MASK ) & ( temp ^ arg1 ) ) & SIGN_MASK ) != 0 
~~ def BSWAP ( cpu , dest ) : 
for i in range ( 0 , dest . size , 8 ) : 
~~~ parts . append ( Operators . EXTRACT ( arg0 , i , 8 ) ) 
~~ dest . write ( Operators . CONCAT ( 8 * len ( parts ) , * parts ) ) 
~~ def CMOVB ( cpu , dest , src ) : 
dest . write ( Operators . ITEBV ( dest . size , cpu . CF , src . read ( ) , dest . read ( ) ) ) 
~~ def CMOVA ( cpu , dest , src ) : 
dest . write ( Operators . ITEBV ( dest . size , Operators . AND ( cpu . CF == False , cpu . ZF == False ) , src . read ( ) , dest . read ( ) ) ) 
~~ def CMOVAE ( cpu , dest , src ) : 
dest . write ( Operators . ITEBV ( dest . size , cpu . CF == False , src . read ( ) , dest . read ( ) ) ) 
~~ def CMOVBE ( cpu , dest , src ) : 
dest . write ( Operators . ITEBV ( dest . size , Operators . OR ( cpu . CF , cpu . ZF ) , src . read ( ) , dest . read ( ) ) ) 
~~ def CMOVZ ( cpu , dest , src ) : 
dest . write ( Operators . ITEBV ( dest . size , cpu . ZF , src . read ( ) , dest . read ( ) ) ) 
~~ def CMOVNZ ( cpu , dest , src ) : 
dest . write ( Operators . ITEBV ( dest . size , cpu . ZF == False , src . read ( ) , dest . read ( ) ) ) 
~~ def CMOVP ( cpu , dest , src ) : 
dest . write ( Operators . ITEBV ( dest . size , cpu . PF , src . read ( ) , dest . read ( ) ) ) 
~~ def CMOVNP ( cpu , dest , src ) : 
dest . write ( Operators . ITEBV ( dest . size , cpu . PF == False , src . read ( ) , dest . read ( ) ) ) 
~~ def CMOVG ( cpu , dest , src ) : 
dest . write ( Operators . ITEBV ( dest . size , Operators . AND ( cpu . ZF == 0 , cpu . SF == cpu . OF ) , src . read ( ) , dest . read ( ) ) ) 
~~ def CMOVGE ( cpu , dest , src ) : 
dest . write ( Operators . ITEBV ( dest . size , ( cpu . SF ^ cpu . OF ) == 0 , src . read ( ) , dest . read ( ) ) ) 
~~ def CMOVLE ( cpu , dest , src ) : 
dest . write ( Operators . ITEBV ( dest . size , Operators . OR ( cpu . SF ^ cpu . OF , cpu . ZF ) , src . read ( ) , dest . read ( ) ) ) 
~~ def CMOVO ( cpu , dest , src ) : 
dest . write ( Operators . ITEBV ( dest . size , cpu . OF , src . read ( ) , dest . read ( ) ) ) 
~~ def CMOVNO ( cpu , dest , src ) : 
dest . write ( Operators . ITEBV ( dest . size , cpu . OF == False , src . read ( ) , dest . read ( ) ) ) 
~~ def CMOVS ( cpu , dest , src ) : 
dest . write ( Operators . ITEBV ( dest . size , cpu . SF , src . read ( ) , dest . read ( ) ) ) 
~~ def CMOVNS ( cpu , dest , src ) : 
dest . write ( Operators . ITEBV ( dest . size , cpu . SF == False , src . read ( ) , dest . read ( ) ) ) 
~~ def LAHF ( cpu ) : 
used_regs = ( cpu . SF , cpu . ZF , cpu . AF , cpu . PF , cpu . CF ) 
is_expression = any ( issymbolic ( x ) for x in used_regs ) 
def make_flag ( val , offset ) : 
~~~ if is_expression : 
~~~ return Operators . ITEBV ( 8 , val , 
BitVecConstant ( 8 , 1 << offset ) , 
BitVecConstant ( 8 , 0 ) ) 
~~~ return val << offset 
~~ ~~ cpu . AH = ( make_flag ( cpu . SF , 7 ) | 
make_flag ( cpu . ZF , 6 ) | 
make_flag ( 0 , 5 ) | 
make_flag ( cpu . AF , 4 ) | 
make_flag ( 0 , 3 ) | 
make_flag ( cpu . PF , 2 ) | 
make_flag ( 1 , 1 ) | 
make_flag ( cpu . CF , 0 ) ) 
~~ def LEA ( cpu , dest , src ) : 
dest . write ( Operators . EXTRACT ( src . address ( ) , 0 , dest . size ) ) 
~~ def MOVBE ( cpu , dest , src ) : 
temp = 0 
for pos in range ( 0 , size , 8 ) : 
~~~ temp = ( temp << 8 ) | ( arg0 & 0xff ) 
arg0 = arg0 >> 8 
~~ dest . write ( arg0 ) 
~~ def SAHF ( cpu ) : 
eflags_size = 32 
val = cpu . AH & 0xD5 | 0x02 
cpu . EFLAGS = Operators . ZEXTEND ( val , eflags_size ) 
~~ def SETA ( cpu , dest ) : 
dest . write ( Operators . ITEBV ( dest . size , Operators . OR ( cpu . CF , cpu . ZF ) == False , 1 , 0 ) ) 
~~ def SETB ( cpu , dest ) : 
dest . write ( Operators . ITEBV ( dest . size , cpu . CF , 1 , 0 ) ) 
~~ def SETBE ( cpu , dest ) : 
dest . write ( Operators . ITEBV ( dest . size , Operators . OR ( cpu . CF , cpu . ZF ) , 1 , 0 ) ) 
~~ def SETC ( cpu , dest ) : 
~~ def SETE ( cpu , dest ) : 
dest . write ( Operators . ITEBV ( dest . size , cpu . ZF , 1 , 0 ) ) 
~~ def SETGE ( cpu , dest ) : 
dest . write ( Operators . ITEBV ( dest . size , cpu . SF == cpu . OF , 1 , 0 ) ) 
~~ def SETNAE ( cpu , dest ) : 
~~ def SETNB ( cpu , dest ) : 
dest . write ( Operators . ITEBV ( dest . size , cpu . CF == False , 1 , 0 ) ) 
~~ def SETNBE ( cpu , dest ) : 
dest . write ( Operators . ITEBV ( dest . size , Operators . AND ( cpu . CF == False , cpu . ZF == False ) , 1 , 0 ) ) 
~~ def SETNG ( cpu , dest ) : 
dest . write ( Operators . ITEBV ( dest . size , Operators . OR ( cpu . ZF , cpu . SF != cpu . OF ) , 1 , 0 ) ) 
~~ def SETNLE ( cpu , dest ) : 
dest . write ( Operators . ITEBV ( dest . size , Operators . AND ( cpu . ZF == False , cpu . SF == cpu . OF ) , 1 , 0 ) ) 
~~ def SETNO ( cpu , dest ) : 
dest . write ( Operators . ITEBV ( dest . size , cpu . OF == False , 1 , 0 ) ) 
~~ def SETNS ( cpu , dest ) : 
dest . write ( Operators . ITEBV ( dest . size , cpu . SF == False , 1 , 0 ) ) 
~~ def SETNZ ( cpu , dest ) : 
dest . write ( Operators . ITEBV ( dest . size , cpu . ZF == False , 1 , 0 ) ) 
~~ def SETO ( cpu , dest ) : 
dest . write ( Operators . ITEBV ( dest . size , cpu . OF , 1 , 0 ) ) 
~~ def SETP ( cpu , dest ) : 
dest . write ( Operators . ITEBV ( dest . size , cpu . PF , 1 , 0 ) ) 
~~ def SETPE ( cpu , dest ) : 
~~ def SETPO ( cpu , dest ) : 
dest . write ( Operators . ITEBV ( dest . size , cpu . PF == False , 1 , 0 ) ) 
~~ def SETS ( cpu , dest ) : 
dest . write ( Operators . ITEBV ( dest . size , cpu . SF , 1 , 0 ) ) 
~~ def SETZ ( cpu , dest ) : 
~~ def XCHG ( cpu , dest , src ) : 
temp = dest . read ( ) 
dest . write ( src . read ( ) ) 
src . write ( temp ) 
~~ def LEAVE ( cpu ) : 
cpu . STACK = cpu . FRAME 
cpu . FRAME = cpu . pop ( cpu . address_bit_size ) 
~~ def PUSH ( cpu , src ) : 
v = src . read ( ) 
if size != 64 and size != cpu . address_bit_size // 2 : 
~~~ v = Operators . SEXTEND ( v , size , cpu . address_bit_size ) 
size = cpu . address_bit_size 
~~ cpu . push ( v , size ) 
~~ def POPF ( cpu ) : 
mask = ( 0x00000001 | 
0x00000004 | 
0x00000010 | 
0x00000040 | 
0x00000080 | 
0x00000400 | 
0x00000800 ) 
val = cpu . pop ( 16 ) 
cpu . EFLAGS = Operators . ZEXTEND ( val & mask , eflags_size ) 
~~ def POPFQ ( cpu ) : 
mask = 0x00000001 | 0x00000004 | 0x00000010 | 0x00000040 | 0x00000080 | 0x00000400 | 0x00000800 
cpu . EFLAGS = ( cpu . EFLAGS & ~ mask ) | cpu . pop ( 64 ) & mask 
~~ def INT ( cpu , op0 ) : 
if op0 . read ( ) != 0x80 : 
~~ raise Interruption ( op0 . read ( ) ) 
~~ def CALL ( cpu , op0 ) : 
proc = op0 . read ( ) 
cpu . push ( cpu . PC , cpu . address_bit_size ) 
cpu . PC = proc 
~~ def RET ( cpu , * operands ) : 
N = 0 
if len ( operands ) > 0 : 
~~~ N = operands [ 0 ] . read ( ) 
~~ cpu . PC = cpu . pop ( cpu . address_bit_size ) 
cpu . STACK += N 
~~ def JA ( cpu , target ) : 
cpu . PC = Operators . ITEBV ( cpu . address_bit_size , Operators . AND ( cpu . CF == False , cpu . ZF == False ) , target . read ( ) , cpu . PC ) 
~~ def JB ( cpu , target ) : 
cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . CF == True , target . read ( ) , cpu . PC ) 
~~ def JBE ( cpu , target ) : 
cpu . PC = Operators . ITEBV ( cpu . address_bit_size , Operators . OR ( cpu . CF , cpu . ZF ) , target . read ( ) , cpu . PC ) 
~~ def JC ( cpu , target ) : 
cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . CF , target . read ( ) , cpu . PC ) 
~~ def JCXZ ( cpu , target ) : 
cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . CX == 0 , target . read ( ) , cpu . PC ) 
~~ def JECXZ ( cpu , target ) : 
cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . ECX == 0 , target . read ( ) , cpu . PC ) 
~~ def JRCXZ ( cpu , target ) : 
cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . RCX == 0 , target . read ( ) , cpu . PC ) 
~~ def JG ( cpu , target ) : 
cpu . PC = Operators . ITEBV ( cpu . address_bit_size , Operators . AND ( cpu . ZF == False , cpu . SF == cpu . OF ) , target . read ( ) , cpu . PC ) 
~~ def JGE ( cpu , target ) : 
cpu . PC = Operators . ITEBV ( cpu . address_bit_size , ( cpu . SF == cpu . OF ) , target . read ( ) , cpu . PC ) 
~~ def JNB ( cpu , target ) : 
cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . CF == False , target . read ( ) , cpu . PC ) 
~~ def JNE ( cpu , target ) : 
cpu . PC = Operators . ITEBV ( cpu . address_bit_size , False == cpu . ZF , target . read ( ) , cpu . PC ) 
~~ def JNG ( cpu , target ) : 
cpu . PC = Operators . ITEBV ( cpu . address_bit_size , Operators . OR ( cpu . ZF , cpu . SF != cpu . OF ) , target . read ( ) , cpu . PC ) 
~~ def JNO ( cpu , target ) : 
cpu . PC = Operators . ITEBV ( cpu . address_bit_size , False == cpu . OF , target . read ( ) , cpu . PC ) 
~~ def JNP ( cpu , target ) : 
cpu . PC = Operators . ITEBV ( cpu . address_bit_size , False == cpu . PF , target . read ( ) , cpu . PC ) 
~~ def JNS ( cpu , target ) : 
cpu . PC = Operators . ITEBV ( cpu . address_bit_size , False == cpu . SF , target . read ( ) , cpu . PC ) 
~~ def JO ( cpu , target ) : 
cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . OF , target . read ( ) , cpu . PC ) 
~~ def JP ( cpu , target ) : 
cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . PF , target . read ( ) , cpu . PC ) 
~~ def JS ( cpu , target ) : 
cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . SF , target . read ( ) , cpu . PC ) 
~~ def JZ ( cpu , target ) : 
cpu . PC = Operators . ITEBV ( cpu . address_bit_size , cpu . ZF , target . read ( ) , cpu . PC ) 
~~ def LJMP ( cpu , cs_selector , target ) : 
cpu . CS = cs_selector . read ( ) 
cpu . PC = target . read ( ) 
~~ def LOOP ( cpu , dest ) : 
counter_name = { 16 : 'CX' , 32 : 'ECX' , 64 : 'RCX' } [ cpu . address_bit_size ] 
counter = cpu . write_register ( counter_name , cpu . read_register ( counter_name ) - 1 ) 
cpu . PC = Operators . ITEBV ( cpu . address_bit_size , counter == 0 , ( cpu . PC + dest . read ( ) ) & ( ( 1 << dest . size ) - 1 ) , cpu . PC + cpu . instruction . size ) 
~~ def LOOPNZ ( cpu , target ) : 
cpu . PC = Operators . ITEBV ( cpu . address_bit_size , counter != 0 , ( cpu . PC + target . read ( ) ) & ( ( 1 << target . size ) - 1 ) , cpu . PC + cpu . instruction . size ) 
~~ def RCL ( cpu , dest , src ) : 
count = src . read ( ) 
countMask = { 8 : 0x1f , 
16 : 0x1f , 
32 : 0x1f , 
64 : 0x3f } [ OperandSize ] 
tempCount = Operators . ZEXTEND ( ( count & countMask ) % ( src . size + 1 ) , OperandSize ) 
value = dest . read ( ) 
if isinstance ( tempCount , int ) and tempCount == 0 : 
~~~ new_val = value 
dest . write ( new_val ) 
~~~ carry = Operators . ITEBV ( OperandSize , cpu . CF , 1 , 0 ) 
right = value >> ( OperandSize - tempCount ) 
new_val = ( value << tempCount ) | ( carry << ( tempCount - 1 ) ) | ( right >> 1 ) 
def sf ( v , size ) : 
~~~ return ( v & ( 1 << ( size - 1 ) ) ) != 0 
~~ cpu . CF = sf ( value << ( tempCount - 1 ) , OperandSize ) 
cpu . OF = Operators . ITE ( tempCount == 1 , 
sf ( new_val , OperandSize ) != cpu . CF , 
cpu . OF ) 
~~ ~~ def RCR ( cpu , dest , src ) : 
left = value >> ( tempCount - 1 ) 
right = value << ( OperandSize - tempCount ) 
new_val = ( left >> 1 ) | ( carry << ( OperandSize - tempCount ) ) | ( right << 1 ) 
cpu . CF = Operators . ITE ( tempCount != 0 , ( left & 1 ) == 1 , cpu . CF ) 
s_MSB = ( ( new_val >> ( OperandSize - 1 ) ) & 0x1 ) == 1 
s_MSB2 = ( ( new_val >> ( OperandSize - 2 ) ) & 0x1 ) == 1 
s_MSB ^ s_MSB2 , cpu . OF ) 
~~ ~~ def ROL ( cpu , dest , src ) : 
tempCount = Operators . ZEXTEND ( ( count & countMask ) % ( OperandSize ) , OperandSize ) 
newValue = ( value << tempCount ) | ( value >> ( OperandSize - tempCount ) ) 
dest . write ( newValue ) 
cpu . CF = Operators . ITE ( tempCount != 0 , ( newValue & 1 ) == 1 , cpu . CF ) 
s_MSB = ( ( newValue >> ( OperandSize - 1 ) ) & 0x1 ) == 1 
cpu . OF = Operators . ITE ( tempCount == 1 , s_MSB ^ cpu . CF , cpu . OF ) 
~~ def SAL ( cpu , dest , src ) : 
tempCount = Operators . ZEXTEND ( count & countMask , dest . size ) 
tempDest = value = dest . read ( ) 
res = dest . write ( Operators . ITEBV ( dest . size , tempCount == 0 , tempDest , value << tempCount ) ) 
MASK = ( 1 << OperandSize ) - 1 
SIGN_MASK = 1 << ( OperandSize - 1 ) 
cpu . CF = Operators . OR ( Operators . AND ( tempCount == 0 , cpu . CF ) , Operators . AND ( tempCount != 0 , ( tempDest & ( 1 << ( OperandSize - tempCount ) ) != 0 ) ) ) 
cpu . OF = Operators . ITE ( tempCount != 0 , ( cpu . CF ) ^ ( ( ( res >> ( OperandSize - 1 ) ) & 0x1 ) == 1 ) , cpu . OF ) 
cpu . SF = Operators . OR ( Operators . AND ( tempCount == 0 , cpu . SF ) , Operators . AND ( tempCount != 0 , ( res & SIGN_MASK ) != 0 ) ) 
cpu . ZF = Operators . OR ( Operators . AND ( tempCount == 0 , cpu . ZF ) , Operators . AND ( tempCount != 0 , res == 0 ) ) 
cpu . PF = Operators . OR ( Operators . AND ( tempCount == 0 , cpu . PF ) , Operators . AND ( tempCount != 0 , cpu . _calculate_parity_flag ( res ) ) ) 
~~ def SAR ( cpu , dest , src ) : 
count = src . read ( ) & countMask 
res = Operators . SAR ( OperandSize , value , Operators . ZEXTEND ( count , OperandSize ) ) 
dest . write ( res ) 
SIGN_MASK = ( 1 << ( OperandSize - 1 ) ) 
if issymbolic ( count ) : 
~~~ cpu . CF = Operators . ITE ( Operators . AND ( count != 0 , count <= OperandSize ) , ( ( value >> Operators . ZEXTEND ( count - 1 , OperandSize ) ) & 1 ) != 0 , cpu . CF ) 
~~~ if count != 0 : 
~~~ if count > OperandSize : 
~~~ count = OperandSize 
~~ cpu . CF = Operators . EXTRACT ( value , count - 1 , 1 ) != 0 
~~ ~~ cpu . ZF = Operators . ITE ( count != 0 , res == 0 , cpu . ZF ) 
cpu . SF = Operators . ITE ( count != 0 , ( res & SIGN_MASK ) != 0 , cpu . SF ) 
cpu . OF = Operators . ITE ( count == 1 , False , cpu . OF ) 
cpu . PF = Operators . ITE ( count != 0 , cpu . _calculate_parity_flag ( res ) , cpu . PF ) 
~~ def SHR ( cpu , dest , src ) : 
count = Operators . ZEXTEND ( src . read ( ) & ( OperandSize - 1 ) , OperandSize ) 
~~~ cpu . CF = Operators . ITE ( count != 0 , 
( ( value >> Operators . ZEXTEND ( count - 1 , OperandSize ) ) & 1 ) != 0 , 
cpu . CF ) 
~~~ cpu . CF = Operators . EXTRACT ( value , count - 1 , 1 ) != 0 
cpu . OF = Operators . ITE ( count != 0 , ( ( value >> ( OperandSize - 1 ) ) & 0x1 ) == 1 , cpu . OF ) 
~~ def SHLD ( cpu , dest , src , count ) : 
tempCount = Operators . ZEXTEND ( count . read ( ) , OperandSize ) & ( OperandSize - 1 ) 
MASK = ( ( 1 << OperandSize ) - 1 ) 
t0 = ( arg0 << tempCount ) 
t1 = arg1 >> ( OperandSize - tempCount ) 
res = Operators . ITEBV ( OperandSize , tempCount == 0 , arg0 , t0 | t1 ) 
res = res & MASK 
~~~ SIGN_MASK = 1 << ( OperandSize - 1 ) 
lastbit = 0 != ( ( arg0 << ( tempCount - 1 ) ) & SIGN_MASK ) 
cpu . _set_shiftd_flags ( OperandSize , arg0 , res , lastbit , tempCount ) 
~~ ~~ def _getMemoryBit ( cpu , bitbase , bitoffset ) : 
assert bitbase . type == 'memory' 
assert bitbase . size >= bitoffset . size 
addr = bitbase . address ( ) 
offt = Operators . SEXTEND ( bitoffset . read ( ) , bitoffset . size , bitbase . size ) 
offt_is_neg = offt >= ( 1 << ( bitbase . size - 1 ) ) 
offt_in_bytes = offt // 8 
bitpos = offt % 8 
new_addr = addr + Operators . ITEBV ( bitbase . size , offt_is_neg , - offt_in_bytes , offt_in_bytes ) 
return ( new_addr , bitpos ) 
~~ def BSF ( cpu , dest , src ) : 
value = src . read ( ) 
flag = Operators . EXTRACT ( value , 0 , 1 ) == 1 
res = 0 
for pos in range ( 1 , src . size ) : 
~~~ res = Operators . ITEBV ( dest . size , flag , res , pos ) 
flag = Operators . OR ( flag , Operators . EXTRACT ( value , pos , 1 ) == 1 ) 
~~ cpu . ZF = value == 0 
dest . write ( Operators . ITEBV ( dest . size , cpu . ZF , dest . read ( ) , res ) ) 
~~ def BSR ( cpu , dest , src ) : 
flag = Operators . EXTRACT ( value , src . size - 1 , 1 ) == 1 
for pos in reversed ( range ( 0 , src . size ) ) : 
flag = Operators . OR ( flag , ( Operators . EXTRACT ( value , pos , 1 ) == 1 ) ) 
~~ cpu . PF = cpu . _calculate_parity_flag ( res ) 
cpu . ZF = value == 0 
~~ def BT ( cpu , dest , src ) : 
if dest . type == 'register' : 
~~~ cpu . CF = ( ( dest . read ( ) >> ( src . read ( ) % dest . size ) ) & 1 ) != 0 
~~ elif dest . type == 'memory' : 
~~~ addr , pos = cpu . _getMemoryBit ( dest , src ) 
base , size , ty = cpu . get_descriptor ( cpu . DS ) 
value = cpu . read_int ( addr + base , 8 ) 
cpu . CF = Operators . EXTRACT ( value , pos , 1 ) == 1 
~~ ~~ def BTC ( cpu , dest , src ) : 
~~~ value = dest . read ( ) 
pos = src . read ( ) % dest . size 
cpu . CF = value & ( 1 << pos ) == 1 << pos 
dest . write ( value ^ ( 1 << pos ) ) 
addr += base 
value = cpu . read_int ( addr , 8 ) 
value = value ^ ( 1 << pos ) 
cpu . write_int ( addr , value , 8 ) 
~~ ~~ def POPCNT ( cpu , dest , src ) : 
source = src . read ( ) 
for i in range ( src . size ) : 
~~~ count += Operators . ITEBV ( dest . size , ( source >> i ) & 1 == 1 , 1 , 0 ) 
~~ dest . write ( count ) 
cpu . SF = False 
cpu . AF = False 
cpu . PF = False 
cpu . ZF = source == 0 
~~ def CMPS ( cpu , dest , src ) : 
src_reg = { 8 : 'SI' , 32 : 'ESI' , 64 : 'RSI' } [ cpu . address_bit_size ] 
dest_reg = { 8 : 'DI' , 32 : 'EDI' , 64 : 'RDI' } [ cpu . address_bit_size ] 
base , _ , ty = cpu . get_descriptor ( cpu . DS ) 
src_addr = cpu . read_register ( src_reg ) + base 
dest_addr = cpu . read_register ( dest_reg ) + base 
arg1 = cpu . read_int ( dest_addr , size ) 
arg0 = cpu . read_int ( src_addr , size ) 
res = ( arg0 - arg1 ) & ( ( 1 << size ) - 1 ) 
cpu . _calculate_CMP_flags ( size , res , arg0 , arg1 ) 
increment = Operators . ITEBV ( cpu . address_bit_size , cpu . DF , - size // 8 , size // 8 ) 
cpu . write_register ( src_reg , cpu . read_register ( src_reg ) + increment ) 
cpu . write_register ( dest_reg , cpu . read_register ( dest_reg ) + increment ) 
~~ def LODS ( cpu , dest , src ) : 
dest . write ( arg0 ) 
~~ def MOVS ( cpu , dest , src ) : 
src_addr = src . address ( ) + base 
dest_addr = dest . address ( ) + base 
src_reg = src . mem . base 
dest_reg = dest . mem . base 
~~ def SCAS ( cpu , dest , src ) : 
dest_reg = dest . reg 
res = arg0 - arg1 
cpu . write_register ( mem_reg , cpu . read_register ( mem_reg ) + increment ) 
~~ def STOS ( cpu , dest , src ) : 
increment = Operators . ITEBV ( { 'RDI' : 64 , 'EDI' : 32 , 'DI' : 16 } [ dest_reg ] , cpu . DF , - size // 8 , size // 8 ) 
~~ def ANDN ( cpu , dest , src1 , src2 ) : 
value = ~ src1 . read ( ) & src2 . read ( ) 
dest . write ( value ) 
cpu . SF = ( value & ( 1 << dest . size ) ) != 0 
~~ def SHLX ( cpu , dest , src , count ) : 
count = count . read ( ) 
tempDest = value = src . read ( ) 
~~ def SARX ( cpu , dest , src , count ) : 
tempCount = count & countMask 
sign = value & ( 1 << ( OperandSize - 1 ) ) 
while tempCount != 0 : 
value = ( value >> 1 ) | sign 
tempCount = tempCount - 1 
~~ res = dest . write ( value ) 
~~ def PMINUB ( cpu , dest , src ) : 
dest_value = dest . read ( ) 
src_value = src . read ( ) 
result = 0 
for pos in range ( 0 , dest . size , 8 ) : 
~~~ itema = ( dest_value >> pos ) & 0xff 
itemb = ( src_value >> pos ) & 0xff 
result |= Operators . ITEBV ( dest . size , itema < itemb , itema , itemb ) << pos 
~~ dest . write ( result ) 
~~ def PXOR ( cpu , dest , src ) : 
res = dest . write ( dest . read ( ) ^ src . read ( ) ) 
~~ def _PUNPCKL ( cpu , dest , src , item_size ) : 
assert dest . size == src . size 
mask = ( 1 << item_size ) - 1 
for pos in range ( 0 , size // item_size ) : 
~~~ if count >= size : 
~~ item0 = Operators . ZEXTEND ( ( dest_value >> ( pos * item_size ) ) & mask , size ) 
item1 = Operators . ZEXTEND ( ( src_value >> ( pos * item_size ) ) & mask , size ) 
res |= item0 << count 
count += item_size 
res |= item1 << count 
~~ dest . write ( res ) 
~~ def PSHUFW ( cpu , op0 , op1 , op3 ) : 
size = op0 . size 
arg0 = op0 . read ( ) 
arg1 = op1 . read ( ) 
arg3 = Operators . ZEXTEND ( op3 . read ( ) , size ) 
assert size == 64 
arg0 |= ( ( arg1 >> ( ( arg3 >> 0 ) & 3 * 16 ) ) & 0xffff ) 
arg0 |= ( ( arg1 >> ( ( arg3 >> 2 ) & 3 * 16 ) ) & 0xffff ) << 16 
arg0 |= ( ( arg1 >> ( ( arg3 >> 4 ) & 3 * 16 ) ) & 0xffff ) << 32 
arg0 |= ( ( arg1 >> ( ( arg3 >> 6 ) & 3 * 16 ) ) & 0xffff ) << 48 
op0 . write ( arg0 ) 
~~ def PSHUFD ( cpu , op0 , op1 , op3 ) : 
order = Operators . ZEXTEND ( op3 . read ( ) , size ) 
arg0 = arg0 & 0xffffffffffffffffffffffffffffffff00000000000000000000000000000000 
arg0 |= ( ( arg1 >> ( ( ( order >> 0 ) & 3 ) * 32 ) ) & 0xffffffff ) 
arg0 |= ( ( arg1 >> ( ( ( order >> 2 ) & 3 ) * 32 ) ) & 0xffffffff ) << 32 
arg0 |= ( ( arg1 >> ( ( ( order >> 4 ) & 3 ) * 32 ) ) & 0xffffffff ) << 64 
arg0 |= ( ( arg1 >> ( ( ( order >> 6 ) & 3 ) * 32 ) ) & 0xffffffff ) << 96 
~~ def PMOVMSKB ( cpu , op0 , op1 ) : 
for i in reversed ( range ( 7 , op1 . size , 8 ) ) : 
~~~ res = ( res << 1 ) | ( ( arg1 >> i ) & 1 ) 
~~ op0 . write ( Operators . EXTRACT ( res , 0 , op0 . size ) ) 
~~ def PSRLDQ ( cpu , dest , src ) : 
temp = Operators . EXTRACT ( src . read ( ) , 0 , 8 ) 
temp = Operators . ITEBV ( src . size , temp > 15 , 16 , temp ) 
dest . write ( dest . read ( ) >> ( temp * 8 ) ) 
~~ def MOVZX ( cpu , op0 , op1 ) : 
op0 . write ( Operators . ZEXTEND ( op1 . read ( ) , op0 . size ) ) 
~~ def MOVSX ( cpu , op0 , op1 ) : 
op0 . write ( Operators . SEXTEND ( op1 . read ( ) , op1 . size , op0 . size ) ) 
~~ def CQO ( cpu ) : 
res = Operators . SEXTEND ( cpu . RAX , 64 , 128 ) 
cpu . RAX = Operators . EXTRACT ( res , 0 , 64 ) 
cpu . RDX = Operators . EXTRACT ( res , 64 , 64 ) 
~~ def CDQ ( cpu ) : 
cpu . EDX = Operators . EXTRACT ( Operators . SEXTEND ( cpu . EAX , 32 , 64 ) , 32 , 32 ) 
~~ def CWDE ( cpu ) : 
bit = Operators . EXTRACT ( cpu . AX , 15 , 1 ) 
cpu . EAX = Operators . SEXTEND ( cpu . AX , 16 , 32 ) 
cpu . EDX = Operators . SEXTEND ( bit , 1 , 32 ) 
~~ 