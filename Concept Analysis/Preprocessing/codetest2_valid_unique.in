utf-8 def verified_excel_file ( store , institute_list , temp_excel_dir ) : 
document_lines = [ ] 
written_files = 0 
today = datetime . datetime . now ( ) . strftime ( '%Y-%m-%d' ) 

for cust in institute_list : 
~~~ verif_vars = store . verified ( institute_id = cust ) 
if not verif_vars : 
~~~ continue 
~~ unique_callers = set ( ) 
for var_type , var_callers in CALLERS . items ( ) : 
~~~ for caller in var_callers : 
~~~ unique_callers . add ( caller . get ( 'id' ) ) 
~~ ~~ cust_verified = export_verified_variants ( verif_vars , unique_callers ) 
document_name = '.' . join ( [ cust , '_verified_variants' , today ] ) + '.xlsx' 
workbook = Workbook ( os . path . join ( temp_excel_dir , document_name ) ) 
Report_Sheet = workbook . add_worksheet ( ) 
row = 0 
for col , field in enumerate ( VERIFIED_VARIANTS_HEADER + list ( unique_callers ) ) : 
~~~ Report_Sheet . write ( row , col , field ) 
~~ ~~ workbook . close ( ) 
if os . path . exists ( os . path . join ( temp_excel_dir , document_name ) ) : 
~~~ written_files += 1 
~~ ~~ return written_files 
~~ def build_hpo_term ( hpo_info ) : 
try : 
~~~ hpo_id = hpo_info [ 'hpo_id' ] 
~~ except KeyError : 
~~~ description = hpo_info [ 'description' ] 
~~ hpo_obj = HpoTerm ( 
hpo_id = hpo_id , 
description = description 
) 
hgnc_ids = hpo_info . get ( 'genes' , set ( ) ) 
if hgnc_ids : 
~~~ hpo_obj [ 'genes' ] = list ( hgnc_ids ) 
~~ return hpo_obj 
~~ def export_genes ( adapter , build = '37' ) : 
for gene_obj in adapter . all_genes ( build = build ) : 
~~~ yield gene_obj 
~~ ~~ def parse_clnsig ( acc , sig , revstat , transcripts ) : 
clnsig_accsessions = [ ] 
if acc : 
~~~ try : 
~~~ acc = int ( acc ) 
~~ except ValueError : 
~~~ pass 
~~ if isinstance ( acc , int ) : 
~~~ revstat_groups = [ ] 
if revstat : 
~~~ revstat_groups = [ rev . lstrip ( '_' ) for rev in revstat . split ( ',' ) ] 
~~ sig_groups = [ ] 
if sig : 
~~~ for significance in sig . split ( '/' ) : 
~~~ splitted_word = significance . split ( '_' ) 
~~ ~~ for sign_term in sig_groups : 
~~~ clnsig_accsessions . append ( { 
'value' : sign_term , 
'accession' : int ( acc ) , 
} ) 
~~ ~~ else : 
~~~ acc_groups = acc . split ( '|' ) 
sig_groups = sig . split ( '|' ) 
revstat_groups = revstat . split ( '|' ) 
for acc_group , sig_group , revstat_group in zip ( acc_groups , sig_groups , revstat_groups ) : 
~~~ accessions = acc_group . split ( ',' ) 
significances = sig_group . split ( ',' ) 
revstats = revstat_group . split ( ',' ) 
for accession , significance , revstat in zip ( accessions , significances , revstats ) : 
'value' : int ( significance ) , 
'accession' : accession , 
'revstat' : revstat , 
~~ ~~ ~~ ~~ elif transcripts : 
~~~ clnsig = set ( ) 
for transcript in transcripts : 
~~~ for annotation in transcript . get ( 'clinsig' , [ ] ) : 
~~~ clnsig . add ( annotation ) 
~~ ~~ for annotation in clnsig : 
~~~ clnsig_accsessions . append ( { 'value' : annotation } ) 
~~ ~~ return clnsig_accsessions 
~~ def index ( context , collection_name ) : 
adapter = context . obj [ 'adapter' ] 
i = 0 
click . echo ( "collection\\tindex" ) 
for collection_name in adapter . collections ( ) : 
~~~ for index in adapter . indexes ( collection_name ) : 
~~~ click . echo ( "{0}\\t{1}" . format ( collection_name , index ) ) 
i += 1 
~~ ~~ if i == 0 : 
~~ ~~ def groups ( context , institute_id , phenotype_group , group_abbreviation , group_file , add ) : 
if group_file : 
~~~ phenotype_group = [ ] 
group_abbreviation = [ ] 
for line in group_file : 
~~~ if line . startswith ( '#' ) : 
~~ if len ( line ) < 7 : 
~~ line = line . rstrip ( ) . split ( '\\t' ) 
phenotype_group . append ( line [ 0 ] ) 
if line [ 1 ] : 
~~~ group_abbreviation . append ( line [ 1 ] ) 
~~ ~~ ~~ if not phenotype_group : 
return 
~~ if ( phenotype_group and group_abbreviation ) : 
~~~ if not len ( phenotype_group ) == len ( group_abbreviation ) : 
~~ ~~ adapter . update_institute ( 
internal_id = institute_id , 
phenotype_groups = phenotype_group , 
group_abbreviations = group_abbreviation , 
add_groups = add , 
~~ def parse_compounds ( compound_info , case_id , variant_type ) : 
compounds = [ ] 
if compound_info : 
~~~ for family_info in compound_info . split ( ',' ) : 
~~~ splitted_entry = family_info . split ( ':' ) 
if splitted_entry [ 0 ] == case_id : 
~~~ for compound in splitted_entry [ 1 ] . split ( '|' ) : 
~~~ splitted_compound = compound . split ( '>' ) 
compound_obj = { } 
compound_name = splitted_compound [ 0 ] 
compound_obj [ 'variant' ] = generate_md5_key ( compound_name . split ( '_' ) + 
[ variant_type , case_id ] ) 
~~~ compound_score = float ( splitted_compound [ 1 ] ) 
~~ except ( TypeError , IndexError ) : 
~~~ compound_score = 0.0 
~~ compound_obj [ 'score' ] = compound_score 
compound_obj [ 'display_name' ] = compound_name 
compounds . append ( compound_obj ) 
~~ ~~ ~~ ~~ return compounds 
~~ def genes ( context , build , json ) : 
result = adapter . all_genes ( build = build ) 
if json : 
~~~ click . echo ( dumps ( result ) ) 
~~ gene_string = ( "{0}\\t{1}\\t{2}\\t{3}\\t{4}" ) 
click . echo ( "#Chromosom\\tStart\\tEnd\\tHgnc_id\\tHgnc_symbol" ) 
for gene_obj in result : 
~~~ click . echo ( gene_string . format ( 
gene_obj [ 'chromosome' ] , 
gene_obj [ 'start' ] , 
gene_obj [ 'end' ] , 
gene_obj [ 'hgnc_id' ] , 
gene_obj [ 'hgnc_symbol' ] , 
) ) 
~~ ~~ def build_individual ( ind ) : 
~~~ ind_obj = dict ( 
individual_id = ind [ 'individual_id' ] 
~~ except KeyError as err : 
~~ ind_obj [ 'display_name' ] = ind . get ( 'display_name' , ind_obj [ 'individual_id' ] ) 
sex = ind . get ( 'sex' , 'unknown' ) 
~~~ int ( sex ) 
ind_obj [ 'sex' ] = str ( sex ) 
~~ except ValueError as err : 
~~~ ind_obj [ 'sex' ] = REV_SEX_MAP [ sex ] 
~~ ~~ phenotype = ind . get ( 'phenotype' , 'unknown' ) 
~~~ ped_phenotype = REV_PHENOTYPE_MAP [ phenotype ] 
if ped_phenotype == - 9 : 
~~~ ped_phenotype = 0 
~~ ind_obj [ 'phenotype' ] = ped_phenotype 
~~ ind_obj [ 'father' ] = ind . get ( 'father' ) 
ind_obj [ 'mother' ] = ind . get ( 'mother' ) 
ind_obj [ 'capture_kits' ] = ind . get ( 'capture_kits' , [ ] ) 
ind_obj [ 'bam_file' ] = ind . get ( 'bam_file' ) 
ind_obj [ 'mt_bam' ] = ind . get ( 'mt_bam' ) 
ind_obj [ 'vcf2cytosure' ] = ind . get ( 'vcf2cytosure' ) 
ind_obj [ 'confirmed_sex' ] = ind . get ( 'confirmed_sex' ) 
ind_obj [ 'confirmed_parent' ] = ind . get ( 'confirmed_parent' ) 
ind_obj [ 'predicted_ancestry' ] = ind . get ( 'predicted_ancestry' ) 
analysis_type = ind . get ( 'analysis_type' , 'unknown' ) 
if not analysis_type in ANALYSIS_TYPES : 
~~ ind_obj [ 'analysis_type' ] = analysis_type 
if 'tmb' in ind : 
~~~ ind_obj [ 'tmb' ] = ind [ 'tmb' ] 
~~ if 'msi' in ind : 
~~~ ind_obj [ 'msi' ] = ind [ 'msi' ] 
~~ if 'tumor_purity' in ind : 
~~~ ind_obj [ 'tumor_purity' ] = ind [ 'tumor_purity' ] 
~~ if 'tumor_type' in ind : 
~~~ ind_obj [ 'tumor_type' ] = ind [ 'tumor_type' ] 
~~ return ind_obj 
~~ def variants ( context , case_id , institute , force , cancer , cancer_research , sv , 
sv_research , snv , snv_research , str_clinical , chrom , start , end , hgnc_id , 
hgnc_symbol , rank_treshold ) : 
if institute : 
~~~ case_id = "{0}-{1}" . format ( institute , case_id ) 
~~ else : 
~~~ institute = case_id . split ( '-' ) [ 0 ] 
~~ case_obj = adapter . case ( case_id = case_id ) 
if case_obj is None : 
context . abort ( ) 
~~ files = [ 
{ 'category' : 'cancer' , 'variant_type' : 'clinical' , 'upload' : cancer } , 
{ 'category' : 'cancer' , 'variant_type' : 'research' , 'upload' : cancer_research } , 
{ 'category' : 'sv' , 'variant_type' : 'clinical' , 'upload' : sv } , 
{ 'category' : 'sv' , 'variant_type' : 'research' , 'upload' : sv_research } , 
{ 'category' : 'snv' , 'variant_type' : 'clinical' , 'upload' : snv } , 
{ 'category' : 'snv' , 'variant_type' : 'research' , 'upload' : snv_research } , 
{ 'category' : 'str' , 'variant_type' : 'clinical' , 'upload' : str_clinical } , 
] 
gene_obj = None 
if ( hgnc_id or hgnc_symbol ) : 
~~~ if hgnc_id : 
~~~ gene_obj = adapter . hgnc_gene ( hgnc_id ) 
~~ if hgnc_symbol : 
~~~ for res in adapter . gene_by_alias ( hgnc_symbol ) : 
~~~ gene_obj = res 
~~ ~~ if not gene_obj : 
~~ ~~ i = 0 
for file_type in files : 
~~~ variant_type = file_type [ 'variant_type' ] 
category = file_type [ 'category' ] 
if file_type [ 'upload' ] : 
~~~ i += 1 
if variant_type == 'research' : 
~~~ if not ( force or case_obj [ 'research_requested' ] ) : 
variant_type , category , case_id ) ) 
adapter . delete_variants ( case_id = case_obj [ '_id' ] , 
variant_type = variant_type , 
category = category ) 
~~~ adapter . load_variants ( 
case_obj = case_obj , 
category = category , 
rank_threshold = rank_treshold , 
chrom = chrom , 
start = start , 
end = end , 
gene_obj = gene_obj 
~~ except Exception as e : 
~~~ LOG . warning ( e ) 
~~ ~~ ~~ if i == 0 : 
~~ ~~ def case ( institute_id , case_name ) : 
institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) 
~~~ return abort ( 404 ) 
~~ return Response ( json_util . dumps ( case_obj ) , mimetype = 'application/json' ) 
~~ def variant ( institute_id , case_name , variant_id ) : 
variant_obj = store . variant ( variant_id ) 
return Response ( json_util . dumps ( variant_obj ) , mimetype = 'application/json' ) 
~~ def collections ( context ) : 
~~~ click . echo ( collection_name ) 
~~ ~~ def institute ( ctx , internal_id , display_name , sanger_recipients ) : 
adapter = ctx . obj [ 'adapter' ] 
if not internal_id : 
ctx . abort ( ) 
~~ if not display_name : 
~~~ display_name = internal_id 
~~ if sanger_recipients : 
~~~ sanger_recipients = list ( sanger_recipients ) 
~~ try : 
~~~ load_institute ( 
adapter = adapter , 
internal_id = internal_id , 
display_name = display_name , 
sanger_recipients = sanger_recipients 
~~~ logger . warning ( e ) 
~~ ~~ def institute ( context , institute_id , sanger_recipient , coverage_cutoff , frequency_cutoff , 
display_name , remove_sanger ) : 
~~~ adapter . update_institute ( 
sanger_recipient = sanger_recipient , 
coverage_cutoff = coverage_cutoff , 
frequency_cutoff = frequency_cutoff , 
remove_sanger = remove_sanger , 
~~ except Exception as err : 
~~~ LOG . warning ( err ) 
~~ ~~ def get_file_handle ( file_path ) : 
if file_path . endswith ( '.gz' ) : 
~~~ file_handle = getreader ( 'utf-8' ) ( gzip . open ( file_path , 'r' ) , errors = 'replace' ) 
~~~ file_handle = open ( file_path , 'r' , encoding = 'utf-8' ) 
~~ return file_handle 
~~ def parse_case_data ( config = None , ped = None , owner = None , vcf_snv = None , 
vcf_sv = None , vcf_cancer = None , vcf_str = None , peddy_ped = None , 
peddy_sex = None , peddy_check = None , delivery_report = None , multiqc = None ) : 
config_data = copy . deepcopy ( config ) or { } 
if 'analysis_date' not in config_data : 
~~~ config_data [ 'analysis_date' ] = datetime . datetime . now ( ) 
~~ if ped : 
~~~ family_id , samples = parse_ped ( ped ) 
config_data [ 'family' ] = family_id 
config_data [ 'samples' ] = samples 
~~ if 'owner' not in config_data : 
~~~ if not owner : 
~~~ config_data [ 'owner' ] = owner 
~~ ~~ if 'gene_panels' in config_data : 
~~~ config_data [ 'gene_panels' ] = [ panel . strip ( ) for panel in 
config_data [ 'gene_panels' ] ] 
config_data [ 'default_gene_panels' ] = [ panel . strip ( ) for panel in 
config_data [ 'default_gene_panels' ] ] 
~~ config_data [ 'peddy_ped' ] = peddy_ped or config_data . get ( 'peddy_ped' ) 
config_data [ 'peddy_sex_check' ] = peddy_sex or config_data . get ( 'peddy_sex' ) 
config_data [ 'peddy_ped_check' ] = peddy_check or config_data . get ( 'peddy_check' ) 
add_peddy_information ( config_data ) 
config_data [ 'multiqc' ] = multiqc or config_data . get ( 'multiqc' ) 
config_data [ 'vcf_snv' ] = vcf_snv if vcf_snv else config_data . get ( 'vcf_snv' ) 
config_data [ 'vcf_sv' ] = vcf_sv if vcf_sv else config_data . get ( 'vcf_sv' ) 
config_data [ 'vcf_str' ] = vcf_str if vcf_str else config_data . get ( 'vcf_str' ) 
config_data [ 'vcf_cancer' ] = vcf_cancer if vcf_cancer else config_data . get ( 'vcf_cancer' ) 
config_data [ 'delivery_report' ] = delivery_report if delivery_report else config_data . get ( 'delivery_report' ) 
config_data [ 'rank_model_version' ] = config_data . get ( 'rank_model_version' ) 
config_data [ 'rank_score_threshold' ] = config_data . get ( 'rank_score_threshold' , 0 ) 
config_data [ 'track' ] = config_data . get ( 'track' , 'rare' ) 
if config_data [ 'vcf_cancer' ] : 
~~~ config_data [ 'track' ] = 'cancer' 
~~ return config_data 
~~ def add_peddy_information ( config_data ) : 
ped_info = { } 
ped_check = { } 
sex_check = { } 
relations = [ ] 
if config_data . get ( 'peddy_ped' ) : 
~~~ file_handle = open ( config_data [ 'peddy_ped' ] , 'r' ) 
for ind_info in parse_peddy_ped ( file_handle ) : 
~~~ ped_info [ ind_info [ 'sample_id' ] ] = ind_info 
~~ ~~ if config_data . get ( 'peddy_ped_check' ) : 
~~~ file_handle = open ( config_data [ 'peddy_ped_check' ] , 'r' ) 
for pair_info in parse_peddy_ped_check ( file_handle ) : 
~~~ ped_check [ ( pair_info [ 'sample_a' ] , pair_info [ 'sample_b' ] ) ] = pair_info 
~~ ~~ if config_data . get ( 'peddy_sex_check' ) : 
~~~ file_handle = open ( config_data [ 'peddy_sex_check' ] , 'r' ) 
for ind_info in parse_peddy_sex_check ( file_handle ) : 
~~~ sex_check [ ind_info [ 'sample_id' ] ] = ind_info 
~~ ~~ if not ped_info : 
~~~ return 
~~ analysis_inds = { } 
for ind in config_data [ 'samples' ] : 
~~~ ind_id = ind [ 'sample_id' ] 
analysis_inds [ ind_id ] = ind 
~~ for ind_id in analysis_inds : 
~~~ ind = analysis_inds [ ind_id ] 
if ind_id in ped_info : 
~~~ ind [ 'predicted_ancestry' ] = ped_info [ ind_id ] . get ( 'ancestry-prediction' , 'UNKNOWN' ) 
~~ if ind_id in sex_check : 
~~~ if sex_check [ ind_id ] [ 'error' ] : 
~~~ ind [ 'confirmed_sex' ] = False 
~~~ ind [ 'confirmed_sex' ] = True 
~~ ~~ for parent in [ 'mother' , 'father' ] : 
~~~ if ind [ parent ] != '0' : 
~~~ for pair in ped_check : 
~~~ if ( ind_id in pair and ind [ parent ] in pair ) : 
~~~ if ped_check [ pair ] [ 'parent_error' ] : 
~~~ analysis_inds [ ind [ parent ] ] [ 'confirmed_parent' ] = False 
~~~ if 'confirmed_parent' not in analysis_inds [ ind [ parent ] ] : 
~~~ analysis_inds [ ind [ parent ] ] [ 'confirmed_parent' ] = True 
~~ ~~ ~~ ~~ ~~ ~~ ~~ ~~ def parse_individual ( sample ) : 
ind_info = { } 
if 'sample_id' not in sample : 
~~ sample_id = sample [ 'sample_id' ] 
if 'sex' not in sample : 
~~ sex = sample [ 'sex' ] 
if sex not in REV_SEX_MAP : 
~~~ log . warning ( "\ 
~~ if 'phenotype' not in sample : 
% sample_id ) 
~~ phenotype = sample [ 'phenotype' ] 
if phenotype not in REV_PHENOTYPE_MAP : 
~~ ind_info [ 'individual_id' ] = sample_id 
ind_info [ 'display_name' ] = sample . get ( 'sample_name' , sample [ 'sample_id' ] ) 
ind_info [ 'sex' ] = sex 
ind_info [ 'phenotype' ] = phenotype 
ind_info [ 'father' ] = sample . get ( 'father' ) 
ind_info [ 'mother' ] = sample . get ( 'mother' ) 
ind_info [ 'confirmed_parent' ] = sample . get ( 'confirmed_parent' ) 
ind_info [ 'confirmed_sex' ] = sample . get ( 'confirmed_sex' ) 
ind_info [ 'predicted_ancestry' ] = sample . get ( 'predicted_ancestry' ) 
bam_file = sample . get ( 'bam_path' ) 
if bam_file : 
~~~ ind_info [ 'bam_file' ] = bam_file 
~~ mt_bam = sample . get ( 'mt_bam' ) 
if mt_bam : 
~~~ ind_info [ 'mt_bam' ] = mt_bam 
~~ analysis_type = sample . get ( 'analysis_type' ) 
if analysis_type : 
~~~ ind_info [ 'analysis_type' ] = analysis_type 
~~ ind_info [ 'capture_kits' ] = ( [ sample . get ( 'capture_kit' ) ] 
if 'capture_kit' in sample else [ ] ) 
vcf2cytosure = sample . get ( 'vcf2cytosure' ) 
if vcf2cytosure : 
~~~ ind_info [ 'vcf2cytosure' ] = vcf2cytosure 
~~ tumor_type = sample . get ( 'tumor_type' ) 
if tumor_type : 
~~~ ind_info [ 'tumor_type' ] = tumor_type 
~~ tumor_mutational_burden = sample . get ( 'tmb' ) 
if tumor_mutational_burden : 
~~~ ind_info [ 'tmb' ] = tumor_mutational_burden 
~~ msi = sample . get ( 'msi' ) 
if msi : 
~~~ ind_info [ 'msi' ] = msi 
~~ tumor_purity = sample . get ( 'tumor_purity' ) 
if tumor_purity : 
~~~ ind_info [ 'tumor_purity' ] = tumor_purity 
~~ return ind_info 
~~ def parse_individuals ( samples ) : 
individuals = [ ] 
if len ( samples ) == 0 : 
~~ ind_ids = set ( ) 
for sample_info in samples : 
~~~ parsed_ind = parse_individual ( sample_info ) 
individuals . append ( parsed_ind ) 
ind_ids . add ( parsed_ind [ 'individual_id' ] ) 
~~ for parsed_ind in individuals : 
~~~ father = parsed_ind [ 'father' ] 
if ( father and father != '0' ) : 
~~~ if father not in ind_ids : 
~~ ~~ mother = parsed_ind [ 'mother' ] 
if ( mother and mother != '0' ) : 
~~~ if mother not in ind_ids : 
~~ ~~ ~~ return individuals 
~~ def parse_case ( config ) : 
if 'owner' not in config : 
~~ if 'family' not in config : 
~~ individuals = parse_individuals ( config [ 'samples' ] ) 
case_data = { 
'owner' : config [ 'owner' ] , 
'collaborators' : [ config [ 'owner' ] ] , 
'case_id' : config [ 'family' ] , 
'display_name' : config . get ( 'family_name' , config [ 'family' ] ) , 
'genome_build' : config . get ( 'human_genome_build' ) , 
'rank_model_version' : config . get ( 'rank_model_version' ) , 
'rank_score_threshold' : config . get ( 'rank_score_threshold' , 0 ) , 
'analysis_date' : config [ 'analysis_date' ] , 
'individuals' : individuals , 
'vcf_files' : { 
'vcf_snv' : config . get ( 'vcf_snv' ) , 
'vcf_sv' : config . get ( 'vcf_sv' ) , 
'vcf_str' : config . get ( 'vcf_str' ) , 
'vcf_cancer' : config . get ( 'vcf_cancer' ) , 
'vcf_snv_research' : config . get ( 'vcf_snv_research' ) , 
'vcf_sv_research' : config . get ( 'vcf_sv_research' ) , 
'vcf_cancer_research' : config . get ( 'vcf_cancer_research' ) , 
} , 
'default_panels' : config . get ( 'default_gene_panels' , [ ] ) , 
'gene_panels' : config . get ( 'gene_panels' , [ ] ) , 
'assignee' : config . get ( 'assignee' ) , 
'peddy_ped' : config . get ( 'peddy_ped' ) , 
'peddy_sex' : config . get ( 'peddy_sex' ) , 
'peddy_check' : config . get ( 'peddy_check' ) , 
'delivery_report' : config . get ( 'delivery_report' ) , 
'multiqc' : config . get ( 'multiqc' ) , 
'track' : config . get ( 'track' , 'rare' ) , 
} 
if 'madeline' in config : 
~~~ mad_path = Path ( config [ 'madeline' ] ) 
if not mad_path . exists ( ) : 
~~ with mad_path . open ( 'r' ) as in_handle : 
~~~ case_data [ 'madeline_info' ] = in_handle . read ( ) 
~~ ~~ if ( case_data [ 'vcf_files' ] [ 'vcf_cancer' ] or case_data [ 'vcf_files' ] [ 'vcf_cancer_research' ] ) : 
~~~ case_data [ 'track' ] = 'cancer' 
~~ return case_data 
~~ def parse_ped ( ped_stream , family_type = 'ped' ) : 
pedigree = FamilyParser ( ped_stream , family_type = family_type ) 
if len ( pedigree . families ) != 1 : 
~~ family_id = list ( pedigree . families . keys ( ) ) [ 0 ] 
family = pedigree . families [ family_id ] 
samples = [ { 
'sample_id' : ind_id , 
'father' : individual . father , 
'mother' : individual . mother , 
'sex' : SEX_MAP [ individual . sex ] , 
'phenotype' : PHENOTYPE_MAP [ int ( individual . phenotype ) ] , 
} for ind_id , individual in family . individuals . items ( ) ] 
return family_id , samples 
~~ def build_evaluation ( variant_specific , variant_id , user_id , user_name , 
institute_id , case_id , classification , criteria ) : 
criteria = criteria or [ ] 
evaluation_obj = dict ( 
variant_specific = variant_specific , 
variant_id = variant_id , 
institute_id = institute_id , 
case_id = case_id , 
classification = classification , 
user_id = user_id , 
user_name = user_name , 
created_at = datetime . datetime . now ( ) , 
criteria_objs = [ ] 
for info in criteria : 
~~~ criteria_obj = { } 
criteria_obj [ 'term' ] = info [ 'term' ] 
if 'comment' in info : 
~~~ criteria_obj [ 'comment' ] = info [ 'comment' ] 
~~ if 'links' in info : 
~~~ criteria_obj [ 'links' ] = info [ 'links' ] 
~~ criteria_objs . append ( criteria_obj ) 
~~ evaluation_obj [ 'criteria' ] = criteria_objs 
return evaluation_obj 
~~ def mt_report ( context , case_id , test , outpath = None ) : 
LOG . info ( \ . format ( case_id ) ) 
query = { 'chrom' : 'MT' } 
case_obj = adapter . case ( case_id = case_id ) 
if not case_obj : 
~~~ LOG . warning ( \ . format ( case_id ) ) 
~~ samples = case_obj . get ( 'individuals' ) 
mt_variants = list ( adapter . variants ( case_id = case_id , query = query , nr_of_variants = - 1 , sort_key = 'position' ) ) 
if not mt_variants : 
~~ today = datetime . datetime . now ( ) . strftime ( '%Y-%m-%d' ) 
if not outpath : 
~~~ outpath = str ( os . getcwd ( ) ) 
~~ written_files = 0 
for sample in samples : 
~~~ sample_id = sample [ 'individual_id' ] 
sample_lines = export_mt_variants ( variants = mt_variants , sample_id = sample_id ) 
document_name = '.' . join ( [ case_obj [ 'display_name' ] , sample_id , today ] ) + '.xlsx' 
workbook = Workbook ( os . path . join ( outpath , document_name ) ) 
if test and sample_lines and workbook : 
continue 
~~ row = 0 
for col , field in enumerate ( MT_EXPORT_HEADER ) : 
if os . path . exists ( os . path . join ( outpath , document_name ) ) : 
~~ ~~ if test : 
~~ return written_files 
~~ def build_genotype ( gt_call ) : 
gt_obj = dict ( 
sample_id = gt_call [ 'individual_id' ] , 
display_name = gt_call [ 'display_name' ] , 
genotype_call = gt_call [ 'genotype_call' ] , 
allele_depths = [ gt_call [ 'ref_depth' ] , gt_call [ 'alt_depth' ] ] , 
read_depth = gt_call [ 'read_depth' ] , 
genotype_quality = gt_call [ 'genotype_quality' ] 
return gt_obj 
~~ def is_pathogenic ( pvs , ps_terms , pm_terms , pp_terms ) : 
if pvs : 
~~~ if ps_terms : 
~~~ return True 
~~ if pm_terms : 
~~~ if pp_terms : 
~~ if len ( pm_terms ) >= 2 : 
~~ ~~ if len ( pp_terms ) >= 2 : 
~~ ~~ if ps_terms : 
~~~ if len ( ps_terms ) >= 2 : 
~~~ if len ( pm_terms ) >= 3 : 
~~ elif len ( pm_terms ) >= 2 : 
~~~ if len ( pp_terms ) >= 2 : 
~~ ~~ elif len ( pp_terms ) >= 4 : 
~~ ~~ ~~ return False 
~~ def is_likely_pathogenic ( pvs , ps_terms , pm_terms , pp_terms ) : 
~~~ if pm_terms : 
~~ if len ( pp_terms ) >= 2 : 
~~ ~~ if pm_terms : 
~~ ~~ return False 
~~ def is_likely_benign ( bs_terms , bp_terms ) : 
if bs_terms : 
~~~ if bp_terms : 
~~ ~~ if len ( bp_terms ) >= 2 : 
~~ return False 
~~ def get_acmg ( acmg_terms ) : 
prediction = 'uncertain_significance' 
pvs = False 
ps_terms = [ ] 
pm_terms = [ ] 
pp_terms = [ ] 
ba = False 
bs_terms = [ ] 
bp_terms = [ ] 
for term in acmg_terms : 
~~~ if term . startswith ( 'PVS' ) : 
~~~ pvs = True 
~~ elif term . startswith ( 'PS' ) : 
~~~ ps_terms . append ( term ) 
~~ elif term . startswith ( 'PM' ) : 
~~~ pm_terms . append ( term ) 
~~ elif term . startswith ( 'PP' ) : 
~~~ pp_terms . append ( term ) 
~~ elif term . startswith ( 'BA' ) : 
~~~ ba = True 
~~ elif term . startswith ( 'BS' ) : 
~~~ bs_terms . append ( term ) 
~~ elif term . startswith ( 'BP' ) : 
~~~ bp_terms . append ( term ) 
~~ ~~ pathogenic = is_pathogenic ( pvs , ps_terms , pm_terms , pp_terms ) 
likely_pathogenic = is_likely_pathogenic ( pvs , ps_terms , pm_terms , pp_terms ) 
benign = is_benign ( ba , bs_terms ) 
likely_benign = is_likely_benign ( bs_terms , bp_terms ) 
if ( pathogenic or likely_pathogenic ) : 
~~~ if ( benign or likely_benign ) : 
~~~ prediction = 'uncertain_significance' 
~~ elif pathogenic : 
~~~ prediction = 'pathogenic' 
~~~ prediction = 'likely_pathogenic' 
~~~ if benign : 
~~~ prediction = 'benign' 
~~ if likely_benign : 
~~~ prediction = 'likely_benign' 
~~ ~~ return prediction 
~~ def add_gene_info ( self , variant_obj , gene_panels = None ) : 
gene_panels = gene_panels or [ ] 
variant_obj [ 'has_refseq' ] = False 
extra_info = { } 
for panel_obj in gene_panels : 
~~~ for gene_info in panel_obj [ 'genes' ] : 
~~~ hgnc_id = gene_info [ 'hgnc_id' ] 
if hgnc_id not in extra_info : 
~~~ extra_info [ hgnc_id ] = [ ] 
~~ extra_info [ hgnc_id ] . append ( gene_info ) 
~~ ~~ for variant_gene in variant_obj . get ( 'genes' , [ ] ) : 
~~~ hgnc_id = variant_gene [ 'hgnc_id' ] 
hgnc_gene = self . hgnc_gene ( hgnc_id ) 
if not hgnc_gene : 
~~ transcripts_dict = { } 
for transcript in hgnc_gene . get ( 'transcripts' , [ ] ) : 
~~~ tx_id = transcript [ 'ensembl_transcript_id' ] 
transcripts_dict [ tx_id ] = transcript 
~~ hgnc_gene [ 'transcripts_dict' ] = transcripts_dict 
if hgnc_gene . get ( 'incomplete_penetrance' ) : 
~~~ variant_gene [ 'omim_penetrance' ] = True 
~~ panel_info = extra_info . get ( hgnc_id , [ ] ) 
disease_associated = set ( ) 
disease_associated_no_version = set ( ) 
manual_penetrance = False 
mosaicism = False 
manual_inheritance = set ( ) 
for gene_info in panel_info : 
~~~ for tx in gene_info . get ( 'disease_associated_transcripts' , [ ] ) : 
~~~ stripped = re . sub ( r'\\.[0-9]' , '' , tx ) 
disease_associated_no_version . add ( stripped ) 
disease_associated . add ( tx ) 
~~ if gene_info . get ( 'reduced_penetrance' ) : 
~~~ manual_penetrance = True 
~~ if gene_info . get ( 'mosaicism' ) : 
~~~ mosaicism = True 
~~ manual_inheritance . update ( gene_info . get ( 'inheritance_models' , [ ] ) ) 
~~ variant_gene [ 'disease_associated_transcripts' ] = list ( disease_associated ) 
variant_gene [ 'manual_penetrance' ] = manual_penetrance 
variant_gene [ 'mosaicism' ] = mosaicism 
variant_gene [ 'manual_inheritance' ] = list ( manual_inheritance ) 
for transcript in variant_gene . get ( 'transcripts' , [ ] ) : 
~~~ tx_id = transcript [ 'transcript_id' ] 
if not tx_id in transcripts_dict : 
~~ hgnc_transcript = transcripts_dict [ tx_id ] 
if hgnc_transcript . get ( 'is_primary' ) : 
~~~ transcript [ 'is_primary' ] = True 
~~ if not hgnc_transcript . get ( 'refseq_id' ) : 
~~ refseq_id = hgnc_transcript [ 'refseq_id' ] 
transcript [ 'refseq_id' ] = refseq_id 
variant_obj [ 'has_refseq' ] = True 
if refseq_id in disease_associated_no_version : 
~~~ transcript [ 'is_disease_associated' ] = True 
~~ transcript [ 'refseq_identifiers' ] = hgnc_transcript . get ( 'refseq_identifiers' , [ ] ) 
~~ variant_gene [ 'common' ] = hgnc_gene 
variant_gene [ 'disease_terms' ] = self . disease_terms ( hgnc_id ) 
~~ return variant_obj 
~~ def variants ( self , case_id , query = None , variant_ids = None , category = 'snv' , 
nr_of_variants = 10 , skip = 0 , sort_key = 'variant_rank' ) : 
if variant_ids : 
~~~ nr_of_variants = len ( variant_ids ) 
~~ elif nr_of_variants == - 1 : 
~~~ nr_of_variants = skip + nr_of_variants 
~~ mongo_query = self . build_query ( case_id , query = query , 
variant_ids = variant_ids , 
sorting = [ ] 
if sort_key == 'variant_rank' : 
~~~ sorting = [ ( 'variant_rank' , pymongo . ASCENDING ) ] 
~~ if sort_key == 'rank_score' : 
~~~ sorting = [ ( 'rank_score' , pymongo . DESCENDING ) ] 
~~ if sort_key == 'position' : 
~~~ sorting = [ ( 'position' , pymongo . ASCENDING ) ] 
~~ result = self . variant_collection . find ( 
mongo_query , 
skip = skip , 
limit = nr_of_variants 
) . sort ( sorting ) 
return result 
~~ def sanger_variants ( self , institute_id = None , case_id = None ) : 
query = { 'validation' : { '$exists' : True } } 
if institute_id : 
~~~ query [ 'institute_id' ] = institute_id 
~~ if case_id : 
~~~ query [ 'case_id' ] = case_id 
~~ return self . variant_collection . find ( query ) 
~~ def variant ( self , document_id , gene_panels = None , case_id = None ) : 
query = { } 
if case_id : 
query [ 'variant_id' ] = document_id 
~~~ query [ '_id' ] = document_id 
~~ variant_obj = self . variant_collection . find_one ( query ) 
if variant_obj : 
~~~ variant_obj = self . add_gene_info ( variant_obj , gene_panels ) 
if variant_obj [ 'chromosome' ] in [ 'X' , 'Y' ] : 
~~~ variant_obj [ 'is_par' ] = is_par ( variant_obj [ 'chromosome' ] , 
variant_obj [ 'position' ] ) 
~~ ~~ return variant_obj 
~~ def gene_variants ( self , query = None , 
category = 'snv' , variant_type = [ 'clinical' ] , 
nr_of_variants = 50 , skip = 0 ) : 
mongo_variant_query = self . build_variant_query ( query = query , 
category = category , variant_type = variant_type ) 
sorting = [ ( 'rank_score' , pymongo . DESCENDING ) ] 
if nr_of_variants == - 1 : 
mongo_variant_query 
) . sort ( sorting ) . skip ( skip ) . limit ( nr_of_variants ) 
~~ def verified ( self , institute_id ) : 
query = { 
'verb' : 'validate' , 
'institute' : institute_id , 
res = [ ] 
validate_events = self . event_collection . find ( query ) 
for validated in list ( validate_events ) : 
~~~ case_id = validated [ 'case' ] 
var_obj = self . variant ( case_id = case_id , document_id = validated [ 'variant_id' ] ) 
case_obj = self . case ( case_id = case_id ) 
if not case_obj or not var_obj : 
~~ var_obj [ 'case_obj' ] = { 
'display_name' : case_obj [ 'display_name' ] , 
'individuals' : case_obj [ 'individuals' ] 
res . append ( var_obj ) 
~~ return res 
~~ def get_causatives ( self , institute_id , case_id = None ) : 
causatives = [ ] 
~~~ case_obj = self . case_collection . find_one ( 
{ "_id" : case_id } 
causatives = [ causative for causative in case_obj [ 'causatives' ] ] 
~~ elif institute_id : 
~~~ query = self . case_collection . aggregate ( [ 
{ '$match' : { 'collaborators' : institute_id , 'causatives' : { '$exists' : True } } } , 
{ '$unwind' : '$causatives' } , 
{ '$group' : { '_id' : '$causatives' } } 
] ) 
causatives = [ item [ '_id' ] for item in query ] 
~~ return causatives 
~~ def check_causatives ( self , case_obj = None , institute_obj = None ) : 
institute_id = case_obj [ 'owner' ] if case_obj else institute_obj [ '_id' ] 
institute_causative_variant_ids = self . get_causatives ( institute_id ) 
if len ( institute_causative_variant_ids ) == 0 : 
~~~ return [ ] 
~~ if case_obj : 
~~~ case_causative_ids = set ( case_obj . get ( 'causatives' , [ ] ) ) 
institute_causative_variant_ids = list ( 
set ( institute_causative_variant_ids ) . difference ( case_causative_ids ) 
~~ query = self . variant_collection . find ( 
{ '_id' : { '$in' : institute_causative_variant_ids } } , 
{ 'variant_id' : 1 } 
positional_variant_ids = [ item [ 'variant_id' ] for item in query ] 
filters = { 'variant_id' : { '$in' : positional_variant_ids } } 
if case_obj : 
~~~ filters [ 'case_id' ] = case_obj [ '_id' ] 
~~~ filters [ 'institute' ] = institute_obj [ '_id' ] 
~~ return self . variant_collection . find ( filters ) 
~~ def other_causatives ( self , case_obj , variant_obj ) : 
variant_id = variant_obj [ 'display_name' ] . rsplit ( '_' , 1 ) [ 0 ] 
institute_causatives = self . get_causatives ( variant_obj [ 'institute' ] ) 
for causative_id in institute_causatives : 
~~~ other_variant = self . variant ( causative_id ) 
if not other_variant : 
~~ not_same_case = other_variant [ 'case_id' ] != case_obj [ '_id' ] 
same_variant = other_variant [ 'display_name' ] . startswith ( variant_id ) 
if not_same_case and same_variant : 
~~~ yield other_variant 
~~ ~~ ~~ def delete_variants ( self , case_id , variant_type , category = None ) : 
category = category or '' 
query = { 'case_id' : case_id , 'variant_type' : variant_type } 
if category : 
~~~ query [ 'category' ] = category 
~~ result = self . variant_collection . delete_many ( query ) 
~~ def overlapping ( self , variant_obj ) : 
category = 'snv' if variant_obj [ 'category' ] == 'sv' else 'sv' 
'$and' : [ 
{ 'case_id' : variant_obj [ 'case_id' ] } , 
{ 'category' : category } , 
{ 'hgnc_ids' : { '$in' : variant_obj [ 'hgnc_ids' ] } } 
sort_key = [ ( 'rank_score' , pymongo . DESCENDING ) ] 
variants = self . variant_collection . find ( query ) . sort ( sort_key ) . limit ( 30 ) 
return variants 
~~ def evaluated_variants ( self , case_id ) : 
{ 'case_id' : case_id } , 
{ 
'$or' : [ 
{ 'acmg_classification' : { '$exists' : True } } , 
{ 'manual_rank' : { '$exists' : True } } , 
{ 'dismiss_variant' : { '$exists' : True } } , 
] , 
variants = { } 
for var in self . variant_collection . find ( query ) : 
~~~ variants [ var [ 'variant_id' ] ] = self . add_gene_info ( var ) 
~~ event_query = { 
{ 'case' : case_id } , 
{ 'category' : 'variant' } , 
{ 'verb' : 'comment' } , 
comment_variants = { event [ 'variant_id' ] for event in self . event_collection . find ( event_query ) } 
for var_id in comment_variants : 
~~~ if var_id in variants : 
~~ variant_obj = self . variant ( var_id , case_id = case_id ) 
if not variant_obj : 
~~ variant_obj [ 'is_commented' ] = True 
variants [ var_id ] = variant_obj 
~~ return variants . values ( ) 
~~ def get_region_vcf ( self , case_obj , chrom = None , start = None , end = None , 
gene_obj = None , variant_type = 'clinical' , category = 'snv' , 
rank_threshold = None ) : 
rank_threshold = rank_threshold or - 100 
variant_file = None 
if variant_type == 'clinical' : 
~~~ if category == 'snv' : 
~~~ variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_snv' ) 
~~ elif category == 'sv' : 
~~~ variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_sv' ) 
~~ elif category == 'str' : 
~~~ variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_str' ) 
~~ ~~ elif variant_type == 'research' : 
~~~ variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_snv_research' ) 
~~~ variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_sv_research' ) 
~~ ~~ if not variant_file : 
~~ vcf_obj = VCF ( variant_file ) 
region = "" 
if gene_obj : 
~~~ chrom = gene_obj [ 'chromosome' ] 
start = gene_obj [ 'start' ] 
end = gene_obj [ 'end' ] 
~~ if chrom : 
~~~ if ( start and end ) : 
~~~ region = "{0}:{1}-{2}" . format ( chrom , start , end ) 
~~~ region = "{0}" . format ( chrom ) 
~~~ rank_threshold = rank_threshold or 5 
~~ with tempfile . NamedTemporaryFile ( mode = 'w' , delete = False ) as temp : 
~~~ file_name = str ( pathlib . Path ( temp . name ) ) 
for header_line in vcf_obj . raw_header . split ( '\\n' ) : 
~~~ if len ( header_line ) > 3 : 
~~~ temp . write ( header_line + '\\n' ) 
~~ ~~ for variant in vcf_obj ( region ) : 
~~~ temp . write ( str ( variant ) ) 
~~ ~~ return file_name 
~~ def sample_variants ( self , variants , sample_name , category = 'snv' ) : 
{ '_id' : { '$in' : variants } } , 
{ 'samples' : { 
'$elemMatch' : { 'display_name' : sample_name , 'genotype_call' : { '$regex' : has_allele } } 
} } 
result = self . variant_collection . find ( query ) 
~~ def get_connection ( host = 'localhost' , port = 27017 , username = None , password = None , 
uri = None , mongodb = None , authdb = None , timeout = 20 , * args , ** kwargs ) : 
authdb = authdb or mongodb 
if uri is None : 
~~~ if username and password : 
~~~ uri = ( "mongodb://{}:{}@{}:{}/{}" 
. format ( quote_plus ( username ) , quote_plus ( password ) , host , port , authdb ) ) 
log_uri = ( "mongodb://{}:****@{}:{}/{}" 
. format ( quote_plus ( username ) , host , port , authdb ) ) 
~~~ log_uri = uri = "mongodb://%s:%s" % ( host , port ) 
~~~ client = MongoClient ( uri , serverSelectionTimeoutMS = timeout ) 
~~ except ServerSelectionTimeoutError as err : 
raise ConnectionFailure 
return client 
~~ def set_submission_objects ( form_fields ) : 
variant_objs = get_objects_from_form ( variant_ids , form_fields , 'variant' ) 
casedata_objs = get_objects_from_form ( variant_ids , form_fields , 'casedata' ) 
return ( variant_objs , casedata_objs ) 
~~ def get_objects_from_form ( variant_ids , form_fields , object_type ) : 
submission_fields = [ ] 
if object_type == 'variant' : 
~~~ submission_fields = CLINVAR_HEADER 
~~~ submission_fields = CASEDATA_HEADER 
~~ submission_objects = [ ] 
if object_type == 'casedata' and 'casedata_' + variant_id not in form_fields : 
~~ subm_obj [ 'csv_type' ] = object_type 
subm_obj [ 'case_id' ] = form_fields . get ( 'case_id' ) 
subm_obj [ 'category' ] = form_fields . get ( 'category@' + variant_id ) 
~~~ field_value = form_fields . get ( key + '@' + variant_id ) 
if field_value and not field_value == '-' : 
~~~ refseq_raw = field_value . split ( '|' ) 
subm_obj [ 'ref_seq' ] = refseq_raw [ 0 ] 
subm_obj [ 'hgvs' ] = refseq_raw [ 1 ] 
~~~ subm_obj [ key ] = field_value 
~~ ~~ ~~ if object_type == 'casedata' : 
~~~ subm_obj [ '_id' ] = str ( subm_obj [ 'case_id' ] ) + '_' + variant_id + '_' + str ( subm_obj [ 'individual_id' ] ) 
~~~ subm_obj [ '_id' ] = str ( subm_obj [ 'case_id' ] ) + '_' + variant_id 
~~ submission_objects . append ( subm_obj ) 
~~ return submission_objects 
~~ def get_submission_variants ( form_fields ) : 
clinvars = [ ] 
if 'all_vars' in form_fields : 
~~~ for field , value in form_fields . items ( ) : 
~~~ if field . startswith ( 'local_id' ) : 
~~~ clinvars . append ( form_fields [ field ] . replace ( 'local_id@' , '' ) ) 
~~ ~~ ~~ else : 
~~ return clinvars 
~~ def clinvar_submission_header ( submission_objs , csv_type ) : 
if csv_type == 'variant_data' : 
~~~ complete_header = CLINVAR_HEADER 
~~~ complete_header = CASEDATA_HEADER 
~~~ custom_header [ header_key ] = header_value 
~~ ~~ ~~ ~~ return custom_header 
~~ def clinvar_submission_lines ( submission_objs , submission_header ) : 
submission_lines = [ ] 
~~~ csv_line = [ ] 
~~~ csv_line . append ( \ + submission_obj . get ( header_key ) + \ ) 
~~~ csv_line . append ( \ ) 
~~ ~~ submission_lines . append ( ',' . join ( csv_line ) ) 
~~ return submission_lines 
~~ def load_transcripts ( adapter , transcripts_lines = None , build = '37' , ensembl_genes = None ) : 
ensembl_genes = ensembl_genes or adapter . ensembl_genes ( build ) 
if transcripts_lines is None : 
~~~ transcripts_lines = fetch_ensembl_transcripts ( build = build ) 
~~ transcripts_dict = parse_transcripts ( transcripts_lines ) 
for ens_tx_id in list ( transcripts_dict ) : 
~~~ parsed_tx = transcripts_dict [ ens_tx_id ] 
ens_gene_id = parsed_tx [ 'ensembl_gene_id' ] 
gene_obj = ensembl_genes . get ( ens_gene_id ) 
if not gene_obj : 
~~~ transcripts_dict . pop ( ens_tx_id ) 
~~ parsed_tx [ 'hgnc_id' ] = gene_obj [ 'hgnc_id' ] 
parsed_tx [ 'primary_transcripts' ] = set ( gene_obj . get ( 'primary_transcripts' , [ ] ) ) 
~~ ref_seq_transcripts = 0 
nr_primary_transcripts = 0 
nr_transcripts = len ( transcripts_dict ) 
transcript_objs = [ ] 
~~~ for tx_data in bar : 
~~~ tx_data [ 'is_primary' ] = False 
primary_transcripts = tx_data [ 'primary_transcripts' ] 
refseq_identifier = None 
refseq_identifiers = [ ] 
for category in TRANSCRIPT_CATEGORIES : 
~~~ identifiers = tx_data [ category ] 
if not identifiers : 
~~ for refseq_id in identifiers : 
~~~ refseq_identifiers . append ( refseq_id ) 
ref_seq_transcripts += 1 
if refseq_id in primary_transcripts : 
~~~ refseq_identifier = refseq_id 
tx_data [ 'is_primary' ] = True 
nr_primary_transcripts += 1 
~~ if not refseq_identifier : 
~~ ~~ ~~ if refseq_identifier : 
~~~ tx_data [ 'refseq_id' ] = refseq_identifier 
~~ if refseq_identifiers : 
~~~ tx_data [ 'refseq_identifiers' ] = refseq_identifiers 
~~ tx_obj = build_transcript ( tx_data , build ) 
transcript_objs . append ( tx_obj ) 
if len ( transcript_objs ) > 0 : 
~~~ adapter . load_transcript_bulk ( transcript_objs ) 
return transcript_objs 
~~ def panel ( context , path , date , display_name , version , panel_type , panel_id , institute , omim , api_key , panel_app ) : 
institute = institute or 'cust000' 
if omim : 
~~~ api_key = api_key or context . obj . get ( 'omim_api_key' ) 
if not api_key : 
~~ if adapter . gene_panel ( panel_id = 'OMIM-AUTO' ) : 
~~~ adapter . load_omim_panel ( api_key , institute = institute ) 
~~~ LOG . error ( err ) 
~~ ~~ if panel_app : 
~~~ load_panel_app ( adapter , panel_id , institute = institute ) 
~~ if ( omim or panel_app ) : 
~~ if path is None : 
~~~ load_panel ( path , adapter , date , display_name , version , panel_type , panel_id , institute ) 
~~ ~~ def build_exon ( exon_info , build = '37' ) : 
~~~ chrom = exon_info [ 'chrom' ] 
~~~ start = int ( exon_info [ 'start' ] ) 
~~ except TypeError : 
~~~ end = int ( exon_info [ 'end' ] ) 
~~~ rank = int ( exon_info [ 'rank' ] ) 
~~~ exon_id = exon_info [ 'exon_id' ] 
~~~ transcript = exon_info [ 'transcript' ] 
~~~ hgnc_id = int ( exon_info [ 'hgnc_id' ] ) 
~~ exon_obj = Exon ( 
exon_id = exon_id , 
rank = rank , 
transcript = transcript , 
hgnc_id = hgnc_id , 
build = build , 
return exon_obj 
~~ def panel ( context , panel_id , version ) : 
panel_objs = adapter . gene_panels ( panel_id = panel_id , version = version ) 
if panel_objs . count ( ) == 0 : 
~~ for panel_obj in panel_objs : 
~~~ adapter . delete_panel ( panel_obj ) 
~~ ~~ def index ( context ) : 
for collection in adapter . db . collection_names ( ) : 
~~~ adapter . db [ collection ] . drop_indexes ( ) 
~~ def user ( context , mail ) : 
user_obj = adapter . user ( mail ) 
if not user_obj : 
~~~ adapter . delete_user ( mail ) 
~~ ~~ def genes ( context , build ) : 
if build : 
adapter . drop_genes ( ) 
~~ ~~ def exons ( context , build ) : 
adapter . drop_exons ( build ) 
~~ def case ( context , institute , case_id , display_name ) : 
if not ( case_id or display_name ) : 
~~ if display_name : 
~~~ if not institute : 
~~ case_id = "{0}-{1}" . format ( institute , display_name ) 
case = adapter . delete_case ( 
institute_id = institute , 
display_name = display_name 
if case . deleted_count == 1 : 
~~~ adapter . delete_variants ( case_id = case_id , variant_type = 'clinical' ) 
adapter . delete_variants ( case_id = case_id , variant_type = 'research' ) 
~~ ~~ def individuals ( context , institute , causatives , case_id ) : 
~~~ case = adapter . case ( case_id = case_id ) 
if case : 
~~~ cases = [ case ] 
~~~ cases = [ case_obj for case_obj in 
adapter . cases ( 
collaborator = institute , 
has_causatives = causatives ) ] 
if len ( cases ) == 0 : 
~~ individuals = ( ind_obj for case_obj in cases for ind_obj in case_obj [ 'individuals' ] ) 
~~ click . echo ( "#case_id\\tind_id\\tdisplay_name\\tsex\\tphenotype\\tmother\\tfather" ) 
for case in cases : 
~~~ for ind_obj in case [ 'individuals' ] : 
~~~ ind_info = [ 
case [ '_id' ] , ind_obj [ 'individual_id' ] , 
ind_obj [ 'display_name' ] , SEX_MAP [ int ( ind_obj [ 'sex' ] ) ] , 
PHENOTYPE_MAP [ ind_obj [ 'phenotype' ] ] , ind_obj [ 'mother' ] , 
ind_obj [ 'father' ] 
click . echo ( '\\t' . join ( ind_info ) ) 
~~ ~~ ~~ def hpo_terms ( case_obj ) : 
features = [ ] 
case_features = case_obj . get ( 'phenotype_terms' ) 
if case_features : 
~~~ for feature in case_features : 
~~~ feature_obj = { 
"id" : feature . get ( 'phenotype_id' ) , 
"label" : feature . get ( 'feature' ) , 
"observed" : "yes" 
features . append ( feature_obj ) 
~~ ~~ return features 
~~ def omim_terms ( case_obj ) : 
disorders = [ ] 
if case_disorders : 
~~~ for disorder in case_disorders : 
~~~ disorder_obj = { 
"id" : ':' . join ( [ 'MIM' , str ( disorder ) ] ) 
disorders . append ( disorder_obj ) 
~~ ~~ return disorders 
~~ def genomic_features ( store , case_obj , sample_name , genes_only ) : 
g_features = [ ] 
build = case_obj [ 'genome_build' ] 
if not build in [ '37' , '38' ] : 
~~~ build = 'GRCh37' 
~~~ build = 'GRCh' + build 
~~ individual_pinned_snvs = list ( store . sample_variants ( variants = case_obj . get ( 'suspects' ) , 
sample_name = sample_name ) ) 
gene_set = set ( ) 
for var in individual_pinned_snvs : 
~~~ hgnc_genes = var . get ( 'hgnc_ids' ) 
if not hgnc_genes : 
~~ for hgnc_id in hgnc_genes : 
~~~ gene_obj = store . hgnc_gene ( hgnc_id ) 
~~ g_feature = { 
'gene' : { 'id' : gene_obj . get ( 'hgnc_symbol' ) } 
~~~ gene_set . add ( hgnc_id ) 
g_features . append ( g_feature ) 
~~ g_feature [ 'variant' ] = { 
'referenceName' : var [ 'chromosome' ] , 
'start' : var [ 'position' ] , 
'end' : var [ 'end' ] , 
'assembly' : build , 
'referenceBases' : var [ 'reference' ] , 
'alternateBases' : var [ 'alternative' ] , 
'shareVariantLevelData' : True 
zygosity = None 
for zyg in zygosities : 
~~~ zygosity = zyg [ 'genotype_call' ] . count ( '1' ) + zyg [ 'genotype_call' ] . count ( '2' ) 
~~ ~~ g_feature [ 'zygosity' ] = zygosity 
~~ ~~ return g_features 
~~ def parse_matches ( patient_id , match_objs ) : 
parsed_matches = [ ] 
for match_obj in match_objs : 
~~~ milliseconds_date = match_obj [ 'created' ] [ '$date' ] 
mdate = datetime . datetime . fromtimestamp ( milliseconds_date / 1000.0 ) 
match_type = 'external' 
matching_patients = [ ] 
parsed_match = { 
'match_date' : mdate 
if match_obj [ 'data' ] [ 'patient' ] [ 'id' ] == patient_id : 
for node_result in match_results : 
~~~ if match_obj [ 'match_type' ] == 'internal' : 
~~~ match_type = 'internal' 
~~ for patient in node_result [ 'patients' ] : 
~~~ match_patient = { 
'patient_id' : patient [ 'patient' ] [ 'id' ] , 
'score' : patient [ 'score' ] , 
'patient' : patient [ 'patient' ] , 
'node' : node_result [ 'node' ] 
matching_patients . append ( match_patient ) 
~~~ m_patient = match_obj [ 'data' ] [ 'patient' ] 
contact_institution = m_patient [ 'contact' ] . get ( 'institution' ) 
~~ score = None 
for res in match_obj [ 'results' ] : 
~~~ for patient in res [ 'patients' ] : 
if patient [ 'patient' ] [ 'id' ] == patient_id : 
~~~ score = patient [ 'score' ] 
match_patient = { 
'patient_id' : m_patient [ 'id' ] , 
'score' : score , 
'patient' : m_patient , 
'node' : res [ 'node' ] 
~~ ~~ ~~ ~~ parsed_match [ 'match_type' ] = match_type 
parsed_match [ 'patients' ] = matching_patients 
parsed_matches . append ( parsed_match ) 
~~ parsed_matches = sorted ( parsed_matches , key = lambda k : k [ 'match_date' ] , reverse = True ) 
return parsed_matches 
~~ def cases ( context , institute , display_name , case_id , nr_variants , variants_treshold ) : 
models = [ ] 
~~~ case_obj = adapter . case ( case_id = case_id ) 
~~~ models . append ( case_obj ) 
~~~ models = adapter . cases ( collaborator = institute , name_query = display_name ) 
models = [ case_obj for case_obj in models ] 
~~ if not models : 
~~ header = [ 'case_id' , 'display_name' , 'institute' ] 
if variants_treshold : 
nr_variants = True 
~~ if nr_variants : 
header . append ( 'clinical' ) 
header . append ( 'research' ) 
~~ click . echo ( "#" + '\\t' . join ( header ) ) 
for model in models : 
~~~ output_str = "{:<12}\\t{:<12}\\t{:<12}" 
output_values = [ model [ '_id' ] , model [ 'display_name' ] , model [ 'owner' ] ] 
if nr_variants : 
~~~ output_str += "\\t{:<12}\\t{:<12}" 
nr_clinical = 0 
nr_research = 0 
variants = adapter . variant_collection . find ( { 'case_id' : model [ '_id' ] } ) 
for i , var in enumerate ( variants , 1 ) : 
~~~ if var [ 'variant_type' ] == 'clinical' : 
~~~ nr_clinical += 1 
~~~ nr_research += 1 
~~ ~~ output_values . extend ( [ nr_clinical , nr_research ] ) 
if variants_treshold and i < variants_treshold : 
~~ ~~ click . echo ( output_str . format ( * output_values ) ) 
~~ ~~ def load_user ( user_email ) : 
user_obj = store . user ( user_email ) 
user_inst = LoginUser ( user_obj ) if user_obj else None 
return user_inst 
~~ def login ( ) : 
if 'next' in request . args : 
~~~ session [ 'next_url' ] = request . args [ 'next' ] 
~~ if current_app . config . get ( 'GOOGLE' ) : 
~~~ callback_url = url_for ( '.authorized' , _external = True ) 
return google . authorize ( callback = callback_url ) 
~~ user_email = request . args . get ( 'email' ) 
if user_obj is None : 
return redirect ( url_for ( 'public.index' ) ) 
~~ return perform_login ( user_obj ) 
~~ def case_mme_update ( self , case_obj , user_obj , mme_subm_obj ) : 
created = None 
patient_ids = [ ] 
updated = datetime . now ( ) 
if 'mme_submission' in case_obj and case_obj [ 'mme_submission' ] : 
~~~ created = case_obj [ 'mme_submission' ] [ 'created_at' ] 
~~~ created = updated 
~~ patients = [ resp [ 'patient' ] for resp in mme_subm_obj . get ( 'server_responses' ) ] 
subm_obj = { 
'created_at' : created , 
'updated_at' : updated , 
'sex' : mme_subm_obj [ 'sex' ] , 
'features' : mme_subm_obj [ 'features' ] , 
'disorders' : mme_subm_obj [ 'disorders' ] , 
'genes_only' : mme_subm_obj [ 'genes_only' ] 
case_obj [ 'mme_submission' ] = subm_obj 
updated_case = self . update_case ( case_obj ) 
institute_obj = self . institute ( case_obj [ 'owner' ] ) 
for individual in case_obj [ 'individuals' ] : 
~~~ self . create_event ( institute = institute_obj , case = case_obj , user = user_obj , 
link = '' , category = 'case' , verb = 'mme_add' , subject = individual [ 'display_name' ] , 
level = 'specific' ) 
~~ ~~ return updated_case 
~~ def case_mme_delete ( self , case_obj , user_obj ) : 
link = '' , category = 'case' , verb = 'mme_remove' , subject = individual [ 'display_name' ] , 
~~ ~~ case_obj [ 'mme_submission' ] = None 
return updated_case 
~~ def build_institute ( internal_id , display_name , sanger_recipients = None , 
coverage_cutoff = None , frequency_cutoff = None ) : 
institute_obj = Institute ( 
sanger_recipients = sanger_recipients , 
frequency_cutoff = frequency_cutoff 
for key in list ( institute_obj ) : 
~~~ if institute_obj [ key ] is None : 
~~~ institute_obj . pop ( key ) 
~~ ~~ return institute_obj 
~~ def delete_event ( self , event_id ) : 
if not isinstance ( event_id , ObjectId ) : 
~~~ event_id = ObjectId ( event_id ) 
~~ self . event_collection . delete_one ( { '_id' : event_id } ) 
~~ def create_event ( self , institute , case , user , link , category , verb , 
subject , level = 'specific' , variant = None , content = None , 
panel = None ) : 
variant = variant or { } 
event = dict ( 
institute = institute [ '_id' ] , 
case = case [ '_id' ] , 
user_id = user [ '_id' ] , 
user_name = user [ 'name' ] , 
link = link , 
verb = verb , 
subject = subject , 
level = level , 
variant_id = variant . get ( 'variant_id' ) , 
content = content , 
panel = panel , 
created_at = datetime . now ( ) , 
updated_at = datetime . now ( ) , 
self . event_collection . insert_one ( event ) 
return event 
~~ def events ( self , institute , case = None , variant_id = None , level = None , 
comments = False , panel = None ) : 
if variant_id : 
~~~ if comments : 
institute [ '_id' ] , case [ '_id' ] , variant_id ) ) 
'category' : 'variant' , 
'variant_id' : variant_id , 
'verb' : 'comment' , 
'level' : 'global' 
'institute' : institute [ '_id' ] , 
'case' : case [ '_id' ] , 
'level' : 'specific' 
~~~ query [ 'institute' ] = institute [ '_id' ] 
query [ 'category' ] = 'variant' 
query [ 'variant_id' ] = variant_id 
query [ 'case' ] = case [ '_id' ] 
if panel : 
~~~ query [ 'panel' ] = panel 
~~~ query [ 'category' ] = 'case' 
~~~ query [ 'case' ] = case [ '_id' ] 
~~ if comments : 
~~~ query [ 'verb' ] = 'comment' 
~~ ~~ ~~ return self . event_collection . find ( query ) . sort ( 'created_at' , pymongo . DESCENDING ) 
~~ def user_events ( self , user_obj = None ) : 
query = dict ( user_id = user_obj [ '_id' ] ) if user_obj else dict ( ) 
return self . event_collection . find ( query ) 
~~ def add_phenotype ( self , institute , case , user , link , hpo_term = None , 
omim_term = None , is_group = False ) : 
hpo_results = [ ] 
~~~ if hpo_term : 
~~~ hpo_results = [ hpo_term ] 
~~ elif omim_term : 
disease_obj = self . disease_term ( omim_term ) 
if disease_obj : 
~~~ for hpo_term in disease_obj . get ( 'hpo_terms' , [ ] ) : 
~~~ hpo_results . append ( hpo_term ) 
~~ ~~ except ValueError as e : 
~~~ raise e 
~~ existing_terms = set ( term [ 'phenotype_id' ] for term in 
case . get ( 'phenotype_terms' , [ ] ) ) 
updated_case = case 
phenotype_terms = [ ] 
for hpo_term in hpo_results : 
hpo_obj = self . hpo_term ( hpo_term ) 
if hpo_obj is None : 
~~ phenotype_id = hpo_obj [ '_id' ] 
description = hpo_obj [ 'description' ] 
if phenotype_id not in existing_terms : 
~~~ phenotype_term = dict ( phenotype_id = phenotype_id , feature = description ) 
phenotype_terms . append ( phenotype_term ) 
self . create_event ( 
institute = institute , 
case = case , 
user = user , 
category = 'case' , 
verb = 'add_phenotype' , 
subject = case [ 'display_name' ] , 
content = phenotype_id 
~~ if is_group : 
~~~ updated_case = self . case_collection . find_one_and_update ( 
{ '_id' : case [ '_id' ] } , 
'$addToSet' : { 
'phenotype_terms' : { '$each' : phenotype_terms } , 
'phenotype_groups' : { '$each' : phenotype_terms } , 
return_document = pymongo . ReturnDocument . AFTER 
~~ def remove_phenotype ( self , institute , case , user , link , phenotype_id , 
is_group = False ) : 
if is_group : 
'$pull' : { 
'phenotype_terms' : { 'phenotype_id' : phenotype_id } , 
'phenotype_groups' : { 'phenotype_id' : phenotype_id } , 
verb = 'remove_phenotype' , 
subject = case [ 'display_name' ] 
~~ def comment ( self , institute , case , user , link , variant = None , 
content = "" , comment_level = "specific" ) : 
if not comment_level in COMMENT_LEVELS : 
~~ if variant : 
comment_level , variant [ 'display_name' ] ) ) 
comment = self . create_event ( 
category = 'variant' , 
verb = 'comment' , 
level = comment_level , 
variant = variant , 
subject = variant [ 'display_name' ] , 
content = content 
case [ 'display_name' ] ) ) 
~~ return comment 
~~ def parse_genotypes ( variant , individuals , individual_positions ) : 
genotypes = [ ] 
for ind in individuals : 
~~~ pos = individual_positions [ ind [ 'individual_id' ] ] 
genotypes . append ( parse_genotype ( variant , ind , pos ) ) 
~~ return genotypes 
~~ def parse_genotype ( variant , ind , pos ) : 
gt_call = { } 
ind_id = ind [ 'individual_id' ] 
gt_call [ 'individual_id' ] = ind_id 
gt_call [ 'display_name' ] = ind [ 'display_name' ] 
genotype = variant . genotypes [ pos ] 
ref_call = genotype [ 0 ] 
alt_call = genotype [ 1 ] 
gt_call [ 'genotype_call' ] = '/' . join ( [ GENOTYPE_MAP [ ref_call ] , 
GENOTYPE_MAP [ alt_call ] ] ) 
paired_end_alt = None 
paired_end_ref = None 
split_read_alt = None 
split_read_ref = None 
if 'PE' in variant . FORMAT : 
~~~ value = int ( variant . format ( 'PE' ) [ pos ] ) 
if not value < 0 : 
~~~ paired_end_alt = value 
~~ ~~ if 'PR' in variant . FORMAT : 
~~~ values = variant . format ( 'PR' ) [ pos ] 
~~~ alt_value = int ( values [ 1 ] ) 
ref_value = int ( values [ 0 ] ) 
if not alt_value < 0 : 
~~~ paired_end_alt = alt_value 
~~ if not ref_value < 0 : 
~~~ paired_end_ref = ref_value 
~~ ~~ except ValueError as r : 
~~ ~~ if 'SR' in variant . FORMAT : 
~~~ values = variant . format ( 'SR' ) [ pos ] 
alt_value = 0 
ref_value = 0 
if len ( values ) == 1 : 
~~~ alt_value = int ( values [ 0 ] ) 
~~ elif len ( values ) == 2 : 
~~ if not alt_value < 0 : 
~~~ split_read_alt = alt_value 
~~~ split_read_ref = ref_value 
~~ ~~ if 'DV' in variant . FORMAT : 
~~~ values = variant . format ( 'DV' ) [ pos ] 
alt_value = int ( values [ 0 ] ) 
~~ ~~ if 'DR' in variant . FORMAT : 
~~~ values = variant . format ( 'DR' ) [ pos ] 
~~ ~~ if 'RV' in variant . FORMAT : 
~~~ values = variant . format ( 'RV' ) [ pos ] 
~~ ~~ if 'RR' in variant . FORMAT : 
~~~ values = variant . format ( 'RR' ) [ pos ] 
if not ref_value < 0 : 
~~ ~~ alt_depth = int ( variant . gt_alt_depths [ pos ] ) 
if alt_depth == - 1 : 
~~~ if 'VD' in variant . FORMAT : 
~~~ alt_depth = int ( variant . format ( 'VD' ) [ pos ] [ 0 ] ) 
~~ if ( paired_end_alt != None or split_read_alt != None ) : 
~~~ alt_depth = 0 
if paired_end_alt : 
~~~ alt_depth += paired_end_alt 
~~ if split_read_alt : 
~~~ alt_depth += split_read_alt 
~~ ~~ ~~ gt_call [ 'alt_depth' ] = alt_depth 
ref_depth = int ( variant . gt_ref_depths [ pos ] ) 
if ref_depth == - 1 : 
~~~ if ( paired_end_ref != None or split_read_ref != None ) : 
~~~ ref_depth = 0 
if paired_end_ref : 
~~~ ref_depth += paired_end_ref 
~~ if split_read_ref : 
~~~ ref_depth += split_read_ref 
~~ ~~ ~~ gt_call [ 'ref_depth' ] = ref_depth 
alt_frequency = float ( variant . gt_alt_freqs [ pos ] ) 
if alt_frequency == - 1 : 
~~~ if 'AF' in variant . FORMAT : 
~~~ alt_frequency = float ( variant . format ( 'AF' ) [ pos ] [ 0 ] ) 
~~ ~~ read_depth = int ( variant . gt_depths [ pos ] ) 
if read_depth == - 1 : 
~~~ if 'DP' in variant . FORMAT : 
~~~ read_depth = int ( variant . format ( 'DP' ) [ pos ] [ 0 ] ) 
~~ elif ( alt_depth != - 1 or ref_depth != - 1 ) : 
~~~ read_depth = 0 
if alt_depth != - 1 : 
~~~ read_depth += alt_depth 
~~ if ref_depth != - 1 : 
~~ ~~ ~~ gt_call [ 'read_depth' ] = read_depth 
gt_call [ 'alt_frequency' ] = alt_frequency 
gt_call [ 'genotype_quality' ] = int ( variant . gt_quals [ pos ] ) 
return gt_call 
~~ def is_par ( chromosome , position , build = '37' ) : 
chrom_match = CHR_PATTERN . match ( chromosome ) 
chrom = chrom_match . group ( 2 ) 
if not chrom in [ 'X' , 'Y' ] : 
~~~ return False 
~~ if PAR_COORDINATES [ build ] [ chrom ] . search ( position ) : 
~~ def check_coordinates ( chromosome , pos , coordinates ) : 
if chrom != coordinates [ 'chrom' ] : 
~~ if ( pos >= coordinates [ 'start' ] and pos <= coordinates [ 'end' ] ) : 
~~ def export_panels ( adapter , panels , versions = None , build = '37' ) : 
if versions and ( len ( versions ) != len ( panels ) ) : 
~~ headers = [ ] 
build_string = ( "##genome_build={}" ) 
headers . append ( build_string . format ( build ) ) 
header_string = ( "##gene_panel={0},version={1},updated_at={2},display_name={3}" ) 
contig_string = ( "##contig={0}" ) 
bed_string = ( "{0}\\t{1}\\t{2}\\t{3}\\t{4}" ) 
panel_geneids = set ( ) 
chromosomes_found = set ( ) 
hgnc_geneobjs = [ ] 
for i , panel_id in enumerate ( panels ) : 
~~~ version = None 
if versions : 
~~~ version = versions [ i ] 
~~ panel_obj = adapter . gene_panel ( panel_id , version = version ) 
if not panel_obj : 
~~ headers . append ( header_string . format ( 
panel_obj [ 'panel_name' ] , 
panel_obj [ 'version' ] , 
panel_obj [ 'date' ] . date ( ) , 
panel_obj [ 'display_name' ] , 
for gene_obj in panel_obj [ 'genes' ] : 
~~~ panel_geneids . add ( gene_obj [ 'hgnc_id' ] ) 
~~ ~~ gene_objs = adapter . hgncid_to_gene ( build = build ) 
for hgnc_id in panel_geneids : 
~~~ hgnc_geneobj = gene_objs . get ( hgnc_id ) 
if hgnc_geneobj is None : 
~~ chrom = hgnc_geneobj [ 'chromosome' ] 
start = hgnc_geneobj [ 'start' ] 
chrom_int = CHROMOSOME_INTEGERS . get ( chrom ) 
if not chrom_int : 
~~ hgnc_geneobjs . append ( ( chrom_int , start , hgnc_geneobj ) ) 
chromosomes_found . add ( chrom ) 
~~ hgnc_geneobjs . sort ( key = lambda tup : ( tup [ 0 ] , tup [ 1 ] ) ) 
for chrom in CHROMOSOMES : 
~~~ if chrom in chromosomes_found : 
~~~ headers . append ( contig_string . format ( chrom ) ) 
~~ ~~ headers . append ( "#chromosome\\tgene_start\\tgene_stop\\thgnc_id\\thgnc_symbol" ) 
for header in headers : 
~~~ yield header 
~~ for hgnc_gene in hgnc_geneobjs : 
~~~ gene_obj = hgnc_gene [ - 1 ] 
gene_line = bed_string . format ( gene_obj [ 'chromosome' ] , gene_obj [ 'start' ] , 
gene_obj [ 'end' ] , gene_obj [ 'hgnc_id' ] , 
gene_obj [ 'hgnc_symbol' ] ) 
yield gene_line 
~~ ~~ def export_gene_panels ( adapter , panels , version = None ) : 
if version and len ( panels ) > 1 : 
~~ bed_string = ( "{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}" ) 
headers = [ ] 
panel_geneobjs = dict ( ) 
for panel_id in panels : 
~~~ panel_obj = adapter . gene_panel ( panel_id , version = version ) 
~~ for gene_obj in panel_obj [ 'genes' ] : 
~~~ panel_geneobjs [ gene_obj [ 'hgnc_id' ] ] = gene_obj 
~~ ~~ if len ( panel_geneobjs ) == 0 : 
~~ headers . append ( '#hgnc_id\\thgnc_symbol\\tdisease_associated_transcripts\\t' 
'reduced_penetrance\\tmosaicism\\tdatabase_entry_version' ) 
~~ for hgnc_id in panel_geneobjs : 
~~~ gene_obj = panel_geneobjs [ hgnc_id ] 
gene_line = bed_string . format ( 
gene_obj [ 'hgnc_id' ] , gene_obj [ 'symbol' ] , 
',' . join ( gene_obj . get ( 'disease_associated_transcripts' , [ ] ) ) , 
gene_obj . get ( 'reduced_penetrance' , '' ) , 
gene_obj . get ( 'mosaicism' , '' ) , 
gene_obj . get ( 'database_entry_version' , '' ) , 
~~ ~~ def hpo_terms ( ) : 
if request . method == 'GET' : 
~~~ data = controllers . hpo_terms ( store = store , limit = 100 ) 
return data 
~~~ search_term = request . form . get ( 'hpo_term' ) 
limit = request . form . get ( 'limit' ) 
data = controllers . hpo_terms ( store = store , query = search_term , limit = limit ) 
return dict ( data , query = search_term , limit = limit ) 
~~ ~~ def transcripts ( context , build ) : 
header = [ "#Chrom\\tStart\\tEnd\\tTranscript\\tRefSeq\\tHgncID" ] 
for line in header : 
~~~ click . echo ( line ) 
~~ transcript_string = ( "{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}" ) 
for tx_obj in export_transcripts ( adapter ) : 
~~~ click . echo ( transcript_string . format ( 
tx_obj [ 'chrom' ] , 
tx_obj [ 'start' ] , 
tx_obj [ 'end' ] , 
tx_obj [ 'ensembl_transcript_id' ] , 
tx_obj . get ( 'refseq_id' , '' ) , 
tx_obj [ 'hgnc_id' ] , 
start = datetime . now ( ) 
nr_exons = adapter . exons ( build = build ) . count ( ) 
if nr_exons : 
adapter . drop_exons ( build = build ) 
~~ ensembl_exons = fetch_ensembl_exons ( build = build ) 
load_exons ( adapter , ensembl_exons , build ) 
adapter . update_indexes ( ) 
~~ def intervals ( context , build ) : 
intervals = adapter . get_coding_intervals ( build ) 
nr_intervals = 0 
longest = 0 
~~~ for iv in intervals [ chrom ] : 
~~~ iv_len = iv . end - iv . begin 
if iv_len > longest : 
~~~ longest = iv_len 
~~ ~~ int_nr = len ( intervals . get ( chrom , [ ] ) ) 
click . echo ( "{0}\\t{1}" . format ( chrom , int_nr ) ) 
nr_intervals += int_nr 
~~ def region ( context , hgnc_id , case_id , chromosome , start , end ) : 
load_region ( 
adapter = adapter , case_id = case_id , hgnc_id = hgnc_id , chrom = chromosome , start = start , end = end 
~~ def build_user ( user_info ) : 
~~~ email = user_info [ 'email' ] 
~~~ name = user_info [ 'name' ] 
~~ user_obj = User ( email = email , name = name ) 
if 'roles' in user_info : 
~~~ user_obj [ 'roles' ] = user_info [ 'roles' ] 
~~ if 'location' in user_info : 
~~~ user_obj [ 'location' ] = user_info [ 'location' ] 
~~ if 'institutes' in user_info : 
~~~ user_obj [ 'institutes' ] = user_info [ 'institutes' ] 
~~ return user_obj 
~~ def parse_reqs ( req_path = './requirements.txt' ) : 
install_requires = [ ] 
with io . open ( os . path . join ( here , 'requirements.txt' ) , encoding = 'utf-8' ) as handle : 
~~~ lines = ( line . strip ( ) for line in handle 
if line . strip ( ) and not line . startswith ( '#' ) ) 
for line in lines : 
~~~ if line . startswith ( '-r' ) : 
~~~ install_requires += parse_reqs ( req_path = line [ 3 : ] ) 
~~~ install_requires . append ( line ) 
~~ ~~ ~~ return install_requires 
~~ def existing_gene ( store , panel_obj , hgnc_id ) : 
existing_genes = { gene [ 'hgnc_id' ] : gene for gene in panel_obj [ 'genes' ] } 
return existing_genes . get ( hgnc_id ) 
~~ def update_panel ( store , panel_name , csv_lines , option ) : 
new_genes = [ ] 
panel_obj = store . gene_panel ( panel_name ) 
if panel_obj is None : 
~~~ return None 
~~ except SyntaxError as error : 
~~~ flash ( error . args [ 0 ] , 'danger' ) 
return None 
~~ if option == 'replace' : 
~~~ for gene in panel_obj [ 'genes' ] : 
~~~ gene [ 'hgnc_symbol' ] = gene [ 'symbol' ] 
store . add_pending ( panel_obj , gene , action = 'delete' , info = None ) 
~~ ~~ for new_gene in new_genes : 
~~~ if not new_gene [ 'hgnc_id' ] : 
~~ gene_obj = store . hgnc_gene ( new_gene [ 'hgnc_id' ] ) 
if gene_obj is None : 
~~ if new_gene [ 'hgnc_symbol' ] and gene_obj [ 'hgnc_symbol' ] != new_gene [ 'hgnc_symbol' ] : 
gene_obj [ 'hgnc_symbol' ] , new_gene [ 'hgnc_symbol' ] ) , 'warning' ) 
~~ info_data = { 
'disease_associated_transcripts' : new_gene [ 'transcripts' ] , 
'reduced_penetrance' : new_gene [ 'reduced_penetrance' ] , 
'mosaicism' : new_gene [ 'mosaicism' ] , 
'inheritance_models' : new_gene [ 'inheritance_models' ] , 
'database_entry_version' : new_gene [ 'database_entry_version' ] , 
~~~ action = 'add' 
~~~ existing_genes = { gene [ 'hgnc_id' ] for gene in panel_obj [ 'genes' ] } 
action = 'edit' if gene_obj [ 'hgnc_id' ] in existing_genes else 'add' 
~~ store . add_pending ( panel_obj , gene_obj , action = action , info = info_data ) 
~~ return panel_obj 
~~ def new_panel ( store , institute_id , panel_name , display_name , csv_lines ) : 
institute_obj = store . institute ( institute_id ) 
if institute_obj is None : 
~~ panel_obj = store . gene_panel ( panel_name ) 
if panel_obj : 
panel_obj [ 'display_name' ] ) ) 
~~~ new_genes = parse_genes ( csv_lines ) 
panel_id = None 
~~~ panel_data = build_panel ( dict ( 
panel_name = panel_name , 
institute = institute_obj [ '_id' ] , 
version = 1.0 , 
date = dt . datetime . now ( ) , 
genes = new_genes , 
) , store ) 
panel_id = store . add_gene_panel ( panel_data ) 
~~ return panel_id 
~~ def panel_export ( store , panel_obj ) : 
panel_obj [ 'institute' ] = store . institute ( panel_obj [ 'institute' ] ) 
full_name = "{}({})" . format ( panel_obj [ 'display_name' ] , panel_obj [ 'version' ] ) 
panel_obj [ 'name_and_version' ] = full_name 
return dict ( panel = panel_obj ) 
~~ def archive_info ( database : Database , archive_case : dict ) -> dict : 
data = { 
'collaborators' : archive_case [ 'collaborators' ] , 
'synopsis' : archive_case . get ( 'synopsis' ) , 
'assignees' : [ ] , 
'suspects' : [ ] , 
'causatives' : [ ] , 
'phenotype_terms' : [ ] , 
'phenotype_groups' : [ ] , 
if archive_case . get ( 'assignee' ) : 
~~~ archive_user = database . user . find_one ( { '_id' : archive_case [ 'assignee' ] } ) 
data [ 'assignee' ] . append ( archive_user [ 'email' ] ) 
~~ for key in [ 'suspects' , 'causatives' ] : 
~~~ for variant_id in archive_case . get ( key , [ ] ) : 
~~~ archive_variant = database . variant . find_one ( { '_id' : variant_id } ) 
data [ key ] . append ( { 
'chromosome' : archive_variant [ 'chromosome' ] , 
'position' : archive_variant [ 'position' ] , 
'reference' : archive_variant [ 'reference' ] , 
'alternative' : archive_variant [ 'alternative' ] , 
'variant_type' : archive_variant [ 'variant_type' ] , 
~~ ~~ for key in [ 'phenotype_terms' , 'phenotype_groups' ] : 
~~~ for archive_term in archive_case . get ( key , [ ] ) : 
~~~ data [ key ] . append ( { 
'phenotype_id' : archive_term [ 'phenotype_id' ] , 
'feature' : archive_term [ 'feature' ] , 
~~ ~~ return data 
~~ def migrate_case ( adapter : MongoAdapter , scout_case : dict , archive_data : dict ) : 
collaborators = list ( set ( scout_case [ 'collaborators' ] + archive_data [ 'collaborators' ] ) ) 
if collaborators != scout_case [ 'collaborators' ] : 
scout_case [ 'collaborators' ] = collaborators 
~~ if len ( scout_case . get ( 'assignees' , [ ] ) ) == 0 : 
~~~ scout_user = adapter . user ( archive_data [ 'assignee' ] ) 
if scout_user : 
~~~ scout_case [ 'assignees' ] = [ archive_data [ 'assignee' ] ] 
~~~ LOG . warning ( f"{archive_data[\ ) 
~~ ~~ for key in [ 'suspects' , 'causatives' ] : 
~~~ scout_case [ key ] = scout_case . get ( key , [ ] ) 
for archive_variant in archive_data [ key ] : 
~~~ variant_id = get_variantid ( archive_variant , scout_case [ '_id' ] ) 
scout_variant = adapter . variant ( variant_id ) 
if scout_variant : 
~~~ if scout_variant [ '_id' ] in scout_case [ key ] : 
~~~ LOG . info ( f"{scout_variant[\ ) 
scout_variant [ key ] . append ( scout_variant [ '_id' ] ) 
~~~ LOG . warning ( f"{scout_variant[\ ) 
scout_variant [ key ] . append ( variant_id ) 
~~ ~~ ~~ if not scout_case . get ( 'synopsis' ) : 
~~~ scout_case [ 'synopsis' ] = archive_data [ 'synopsis' ] 
~~ scout_case [ 'is_migrated' ] = True 
adapter . case_collection . find_one_and_replace ( 
{ '_id' : scout_case [ '_id' ] } , 
scout_case , 
scout_institute = adapter . institute ( scout_case [ 'owner' ] ) 
scout_user = adapter . user ( 'mans.magnusson@scilifelab.se' ) 
for key in [ 'phenotype_terms' , 'phenotype_groups' ] : 
~~~ for archive_term in archive_data [ key ] : 
~~~ adapter . add_phenotype ( 
institute = scout_institute , 
case = scout_case , 
user = scout_user , 
link = f"/{scout_case[\ , 
hpo_term = archive_term [ 'phenotype_id' ] , 
is_group = key == 'phenotype_groups' , 
~~ ~~ ~~ def migrate ( uri : str , archive_uri : str , case_id : str , dry : bool , force : bool ) : 
scout_client = MongoClient ( uri ) 
scout_database = scout_client [ uri . rsplit ( '/' , 1 ) [ - 1 ] ] 
scout_adapter = MongoAdapter ( database = scout_database ) 
scout_case = scout_adapter . case ( case_id ) 
if not force and scout_case . get ( 'is_migrated' ) : 
~~ archive_client = MongoClient ( archive_uri ) 
archive_database = archive_client [ archive_uri . rsplit ( '/' , 1 ) [ - 1 ] ] 
archive_case = archive_database . case . find_one ( { 
'owner' : scout_case [ 'owner' ] , 
'display_name' : scout_case [ 'display_name' ] 
archive_data = archive_info ( archive_database , archive_case ) 
if dry : 
~~~ print ( ruamel . yaml . safe_dump ( archive_data ) ) 
~~ ~~ def research ( context , case_id , institute , force ) : 
~~~ splitted_case = case_id . split ( '-' ) 
if len ( splitted_case ) > 1 : 
~~~ institute_obj = adapter . institute ( splitted_case [ 0 ] ) 
if institute_obj : 
~~~ institute = institute_obj [ '_id' ] 
case_id = splitted_case [ 1 ] 
~~ ~~ ~~ case_obj = adapter . case ( institute_id = institute , case_id = case_id ) 
~~~ case_objs = [ case_obj ] 
~~~ case_objs = adapter . cases ( research_requested = True ) 
~~ default_threshold = 8 
files = False 
for case_obj in case_objs : 
~~~ if force or case_obj [ 'research_requested' ] : 
~~~ if case_obj [ 'vcf_files' ] . get ( 'vcf_snv_research' ) : 
~~~ files = True 
variant_type = 'research' , 
category = 'snv' ) 
adapter . load_variants ( 
category = 'snv' , 
rank_threshold = default_threshold , 
~~ if case_obj [ 'vcf_files' ] . get ( 'vcf_sv_research' ) : 
category = 'sv' ) 
category = 'sv' , 
~~ if case_obj [ 'vcf_files' ] . get ( 'vcf_cancer_research' ) : 
category = 'cancer' ) 
category = 'cancer' , 
~~ if not files : 
~~ case_obj [ 'is_research' ] = True 
case_obj [ 'research_requested' ] = False 
adapter . update_case ( case_obj ) 
~~ ~~ ~~ def load_hgnc ( adapter , genes = None , ensembl_lines = None , hgnc_lines = None , exac_lines = None , mim2gene_lines = None , 
genemap_lines = None , hpo_lines = None , transcripts_lines = None , build = '37' , omim_api_key = '' ) : 
gene_objs = load_hgnc_genes ( 
genes = genes , 
ensembl_lines = ensembl_lines , 
hgnc_lines = hgnc_lines , 
exac_lines = exac_lines , 
mim2gene_lines = mim2gene_lines , 
genemap_lines = genemap_lines , 
hpo_lines = hpo_lines , 
omim_api_key = omim_api_key , 
ensembl_genes = { } 
for gene_obj in gene_objs : 
~~~ ensembl_genes [ gene_obj [ 'ensembl_id' ] ] = gene_obj 
~~ transcript_objs = load_transcripts ( 
transcripts_lines = transcripts_lines , 
ensembl_genes = ensembl_genes ) 
~~ def load_hgnc_genes ( adapter , genes = None , ensembl_lines = None , hgnc_lines = None , exac_lines = None , mim2gene_lines = None , 
genemap_lines = None , hpo_lines = None , build = '37' , omim_api_key = '' ) : 
gene_objects = list ( ) 
if not genes : 
~~~ if ensembl_lines is None : 
~~~ ensembl_lines = fetch_ensembl_genes ( build = build ) 
~~ hgnc_lines = hgnc_lines or fetch_hgnc ( ) 
exac_lines = exac_lines or fetch_exac_constraint ( ) 
if not ( mim2gene_lines and genemap_lines ) : 
~~~ if not omim_api_key : 
~~ mim_files = fetch_mim_files ( omim_api_key , mim2genes = True , genemap2 = True ) 
mim2gene_lines = mim_files [ 'mim2genes' ] 
genemap_lines = mim_files [ 'genemap2' ] 
~~ if not hpo_lines : 
~~~ hpo_files = fetch_hpo_files ( hpogenes = True ) 
hpo_lines = hpo_files [ 'hpogenes' ] 
~~ genes = link_genes ( 
hpo_lines = hpo_lines 
~~ non_existing = 0 
nr_genes = len ( genes ) 
~~~ for gene_data in bar : 
~~~ if not gene_data . get ( 'chromosome' ) : 
non_existing += 1 
~~ gene_obj = build_hgnc_gene ( gene_data , build = build ) 
gene_objects . append ( gene_obj ) 
adapter . load_hgnc_bulk ( gene_objects ) 
return gene_objects 
~~ def hpo ( context , term , description ) : 
if term : 
~~~ term = term . upper ( ) 
if not term . startswith ( 'HP:' ) : 
~~~ while len ( term ) < 7 : 
~~~ term = '0' + term 
~~ term = 'HP:' + term 
hpo_terms = adapter . hpo_terms ( hpo_term = term ) 
~~ elif description : 
~~~ sorted_terms = sorted ( adapter . hpo_terms ( query = description ) , key = itemgetter ( 'hpo_number' ) ) 
for term in sorted_terms : 
~~~ term . pop ( 'genes' ) 
~~ context . abort ( ) 
~~~ hpo_terms = adapter . hpo_terms ( ) 
~~ if hpo_terms . count ( ) == 0 : 
~~ click . echo ( "hpo_id\\tdescription\\tnr_genes" ) 
for hpo_obj in hpo_terms : 
~~~ click . echo ( "{0}\\t{1}\\t{2}" . format ( 
hpo_obj [ 'hpo_id' ] , 
hpo_obj [ 'description' ] , 
len ( hpo_obj . get ( 'genes' , [ ] ) ) 
~~ ~~ def build_gene ( gene , hgncid_to_gene = None ) : 
hgncid_to_gene = hgncid_to_gene or { } 
gene_obj = dict ( ) 
hgnc_id = int ( gene [ 'hgnc_id' ] ) 
gene_obj [ 'hgnc_id' ] = hgnc_id 
hgnc_gene = hgncid_to_gene . get ( hgnc_id ) 
inheritance = set ( ) 
hgnc_transcripts = [ ] 
if hgnc_gene : 
~~~ gene_obj [ 'hgnc_symbol' ] = hgnc_gene [ 'hgnc_symbol' ] 
gene_obj [ 'ensembl_id' ] = hgnc_gene [ 'ensembl_id' ] 
gene_obj [ 'description' ] = hgnc_gene [ 'description' ] 
if hgnc_gene . get ( 'inheritance_models' ) : 
~~~ gene_obj [ 'inheritance' ] = hgnc_gene [ 'inheritance_models' ] 
~~ ~~ transcripts = [ ] 
for transcript in gene [ 'transcripts' ] : 
~~~ transcript_obj = build_transcript ( transcript ) 
transcripts . append ( transcript_obj ) 
~~ gene_obj [ 'transcripts' ] = transcripts 
functional_annotation = gene . get ( 'most_severe_consequence' ) 
if functional_annotation : 
~~~ if not functional_annotation in SO_TERM_KEYS : 
~~~ gene_obj [ 'functional_annotation' ] = functional_annotation 
~~ ~~ region_annotation = gene . get ( 'region_annotation' ) 
if region_annotation : 
~~~ if not region_annotation in FEATURE_TYPES : 
~~~ gene_obj [ 'region_annotation' ] = region_annotation 
~~ ~~ sift_prediction = gene . get ( 'most_severe_sift' ) 
if sift_prediction : 
~~~ if not sift_prediction in CONSEQUENCE : 
~~~ gene_obj [ 'sift_prediction' ] = sift_prediction 
~~ ~~ polyphen_prediction = gene . get ( 'most_severe_polyphen' ) 
if polyphen_prediction : 
~~~ if not polyphen_prediction in CONSEQUENCE : 
~~~ gene_obj [ 'polyphen_prediction' ] = polyphen_prediction 
~~ ~~ gene_obj [ 'hgvs_identifier' ] = gene [ 'hgvs_identifier' ] 
gene_obj [ 'canonical_transcript' ] = gene [ 'canonical_transcript' ] 
gene_obj [ 'exon' ] = gene [ 'exon' ] 
return gene_obj 
~~ def create_app ( config_file = None , config = None ) : 
app = Flask ( __name__ ) 
app . config . from_pyfile ( 'config.py' ) 
app . jinja_env . add_extension ( 'jinja2.ext.do' ) 
if config : 
~~~ app . config . update ( config ) 
~~ if config_file : 
~~~ app . config . from_pyfile ( config_file ) 
~~ app . mme_nodes = mme_nodes ( app . config . get ( 'MME_URL' ) , app . config . get ( 'MME_TOKEN' ) ) 
app . config [ "JSON_SORT_KEYS" ] = False 
current_log_level = logger . getEffectiveLevel ( ) 
coloredlogs . install ( level = 'DEBUG' if app . debug else current_log_level ) 
configure_extensions ( app ) 
register_blueprints ( app ) 
register_filters ( app ) 
if not ( app . debug or app . testing ) and app . config . get ( 'MAIL_USERNAME' ) : 
~~~ configure_email_logging ( app ) 
~~ @ app . before_request 
def check_user ( ) : 
~~~ if not app . config . get ( 'LOGIN_DISABLED' ) and request . endpoint : 
~~~ static_endpoint = 'static' in request . endpoint or 'report' in request . endpoint 
public_endpoint = getattr ( app . view_functions [ request . endpoint ] , 
'is_public' , False ) 
relevant_endpoint = not ( static_endpoint or public_endpoint ) 
if relevant_endpoint and not current_user . is_authenticated : 
~~~ next_url = "{}?{}" . format ( request . path , request . query_string . decode ( ) ) 
login_url = url_for ( 'login.login' , next = next_url ) 
return redirect ( login_url ) 
~~ ~~ ~~ return app 
~~ def configure_extensions ( app ) : 
extensions . toolbar . init_app ( app ) 
extensions . bootstrap . init_app ( app ) 
extensions . mongo . init_app ( app ) 
extensions . store . init_app ( app ) 
extensions . login_manager . init_app ( app ) 
extensions . oauth . init_app ( app ) 
extensions . mail . init_app ( app ) 
Markdown ( app ) 
if app . config . get ( 'SQLALCHEMY_DATABASE_URI' ) : 
~~~ configure_coverage ( app ) 
~~ if app . config . get ( 'LOQUSDB_SETTINGS' ) : 
~~~ extensions . loqusdb . init_app ( app ) 
~~ ~~ def register_blueprints ( app ) : 
app . register_blueprint ( public . public_bp ) 
app . register_blueprint ( genes . genes_bp ) 
app . register_blueprint ( cases . cases_bp ) 
app . register_blueprint ( login . login_bp ) 
app . register_blueprint ( variants . variants_bp ) 
app . register_blueprint ( panels . panels_bp ) 
app . register_blueprint ( dashboard . dashboard_bp ) 
app . register_blueprint ( api . api_bp ) 
app . register_blueprint ( alignviewers . alignviewers_bp ) 
app . register_blueprint ( phenotypes . hpo_bp ) 
app . register_blueprint ( institutes . overview ) 
~~ def configure_email_logging ( app ) : 
import logging 
from scout . log import TlsSMTPHandler 
mail_handler = TlsSMTPHandler ( 
mailhost = app . config [ 'MAIL_SERVER' ] , 
fromaddr = app . config [ 'MAIL_USERNAME' ] , 
toaddrs = app . config [ 'ADMINS' ] , 
credentials = ( app . config [ 'MAIL_USERNAME' ] , app . config [ 'MAIL_PASSWORD' ] ) 
mail_handler . setLevel ( logging . ERROR ) 
mail_handler . setFormatter ( logging . Formatter ( 
app . logger . addHandler ( mail_handler ) 
~~ def configure_coverage ( app ) : 
app . config [ 'SQLALCHEMY_TRACK_MODIFICATIONS' ] = True if app . debug else False 
if chanjo_api : 
~~~ chanjo_api . init_app ( app ) 
configure_template_filters ( app ) 
app . register_blueprint ( report_bp , url_prefix = '/reports' ) 
~~ babel = Babel ( app ) 
@ babel . localeselector 
def get_locale ( ) : 
accept_languages = current_app . config . get ( 'ACCEPT_LANGUAGES' , [ 'en' ] ) 
session_language = request . args . get ( 'lang' ) 
if session_language in accept_languages : 
return session_language 
~~ user_language = current_app . config . get ( 'REPORT_LANGUAGE' ) 
if user_language : 
~~~ return user_language 
~~ return request . accept_languages . best_match ( accept_languages ) 
~~ ~~ def aliases ( context , build , symbol ) : 
if symbol : 
~~~ alias_genes = { } 
res = adapter . gene_by_alias ( symbol , build = build ) 
for gene_obj in res : 
~~~ hgnc_id = gene_obj [ 'hgnc_id' ] 
hgnc_symbol = gene_obj [ 'hgnc_symbol' ] 
for alias in gene_obj [ 'aliases' ] : 
~~~ true_id = None 
if alias == hgnc_symbol : 
~~~ true_id = hgnc_id 
~~ if alias in alias_genes : 
~~~ alias_genes [ alias ] [ 'ids' ] . add ( hgnc_id ) 
if true_id : 
~~~ alias_genes [ alias ] [ 'true' ] = hgnc_id 
~~~ alias_genes [ alias ] = { 
'true' : hgnc_id , 
'ids' : set ( [ hgnc_id ] ) 
~~ ~~ ~~ ~~ else : 
~~~ alias_genes = adapter . genes_by_alias ( build = build ) 
~~ if len ( alias_genes ) == 0 : 
~~ click . echo ( "#hgnc_symbol\\ttrue_id\\thgnc_ids" ) 
for alias_symbol in alias_genes : 
~~~ info = alias_genes [ alias_symbol ] 
click . echo ( "{0}\\t{1}\\t{2}\\t" . format ( 
alias_symbol , 
( alias_genes [ alias_symbol ] [ 'true' ] or 'None' ) , 
~~ ~~ def build_gene ( gene_info , adapter ) : 
symbol = gene_info . get ( 'hgnc_symbol' ) 
if not hgnc_id : 
~~~ raise KeyError ( ) 
~~ gene_obj = dict ( hgnc_id = hgnc_id ) 
~~ hgnc_gene = adapter . hgnc_gene ( hgnc_id ) 
if hgnc_gene is None : 
~~ gene_obj [ 'symbol' ] = hgnc_gene [ 'hgnc_symbol' ] 
if symbol != gene_obj [ 'symbol' ] : 
( hgnc_gene [ 'hgnc_symbol' ] , hgnc_id , symbol ) ) 
~~ if gene_info . get ( 'transcripts' ) : 
~~~ gene_obj [ 'disease_associated_transcripts' ] = gene_info [ 'transcripts' ] 
~~~ gene_obj [ 'reduced_penetrance' ] = True 
~~~ gene_obj [ 'mosaicism' ] = True 
~~ if gene_info . get ( 'database_entry_version' ) : 
~~~ gene_obj [ 'database_entry_version' ] = gene_info [ 'database_entry_version' ] 
~~ if gene_info . get ( 'inheritance_models' ) : 
~~~ for model in gene_info [ 'inheritance_models' ] : 
~~~ if model == 'AR' : 
~~~ gene_obj [ 'ar' ] = True 
~~ if model == 'AD' : 
~~~ gene_obj [ 'ad' ] = True 
~~ if model == 'MT' : 
~~~ gene_obj [ 'mt' ] = True 
~~ if model == 'XR' : 
~~~ gene_obj [ 'xr' ] = True 
~~ if model == 'XD' : 
~~~ gene_obj [ 'xd' ] = True 
~~ if model == 'X' : 
~~~ gene_obj [ 'x' ] = True 
~~ if model == 'Y' : 
~~~ gene_obj [ 'y' ] = True 
~~ ~~ ~~ return gene_obj 
~~ def build_panel ( panel_info , adapter ) : 
panel_name = panel_info . get ( 'panel_id' , panel_info . get ( 'panel_name' ) ) 
if not panel_name : 
~~ panel_obj = dict ( panel_name = panel_name ) 
~~~ institute_id = panel_info [ 'institute' ] 
~~ if adapter . institute ( institute_id ) is None : 
~~ panel_obj [ 'institute' ] = panel_info [ 'institute' ] 
panel_obj [ 'version' ] = float ( panel_info [ 'version' ] ) 
~~~ panel_obj [ 'date' ] = panel_info [ 'date' ] 
~~ panel_obj [ 'display_name' ] = panel_info . get ( 'display_name' , panel_obj [ 'panel_name' ] ) 
gene_objs = [ ] 
fail = False 
for gene_info in panel_info . get ( 'genes' , [ ] ) : 
~~~ gene_obj = build_gene ( gene_info , adapter ) 
gene_objs . append ( gene_obj ) 
~~ except IntegrityError as err : 
fail = True 
~~ ~~ if fail : 
~~ panel_obj [ 'genes' ] = gene_objs 
return panel_obj 
~~ def verified ( context , collaborator , test , outpath = None ) : 
collaborator = collaborator or 'cust000' 
verified_vars = adapter . verified ( institute_id = collaborator ) 
if not verified_vars : 
~~ document_lines = export_verified_variants ( verified_vars ) 
document_name = '.' . join ( [ 'verified_variants' , collaborator , today ] ) + '.xlsx' 
if test and document_lines : 
return written_files 
~~ if not outpath : 
~~ workbook = Workbook ( os . path . join ( outpath , document_name ) ) 
for col , field in enumerate ( VERIFIED_VARIANTS_HEADER ) : 
written_files += 1 
~~ def variants ( context , collaborator , document_id , case_id , json ) : 
variants = export_variants ( 
adapter , 
collaborator , 
document_id = document_id , 
case_id = case_id 
~~~ click . echo ( dumps ( [ var for var in variants ] ) ) 
~~ vcf_header = VCF_HEADER 
~~~ vcf_header [ - 1 ] = vcf_header [ - 1 ] + "\\tFORMAT" 
~~~ vcf_header [ - 1 ] = vcf_header [ - 1 ] + "\\t" + individual [ 'individual_id' ] 
~~ ~~ for line in vcf_header : 
~~ for variant_obj in variants : 
~~~ variant_string = get_vcf_entry ( variant_obj , case_id = case_id ) 
click . echo ( variant_string ) 
~~ ~~ def get_vcf_entry ( variant_obj , case_id = None ) : 
if variant_obj [ 'category' ] == 'snv' : 
~~~ var_type = 'TYPE' 
~~~ var_type = 'SVTYPE' 
~~ info_field = ';' . join ( 
[ 
'END=' + str ( variant_obj [ 'end' ] ) , 
var_type + '=' + variant_obj [ 'sub_category' ] . upper ( ) 
variant_string = "{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}" . format ( 
variant_obj [ 'chromosome' ] , 
variant_obj [ 'position' ] , 
variant_obj [ 'dbsnp_id' ] , 
variant_obj [ 'reference' ] , 
variant_obj [ 'alternative' ] , 
variant_obj [ 'quality' ] , 
';' . join ( variant_obj [ 'filters' ] ) , 
info_field 
~~~ variant_string += "\\tGT" 
for sample in variant_obj [ 'samples' ] : 
~~~ variant_string += "\\t" + sample [ 'genotype_call' ] 
~~ ~~ return variant_string 
~~ def serve ( context , config , host , port , debug , livereload ) : 
pymongo_config = dict ( 
MONGO_HOST = context . obj [ 'host' ] , 
MONGO_PORT = context . obj [ 'port' ] , 
MONGO_DBNAME = context . obj [ 'mongodb' ] , 
MONGO_USERNAME = context . obj [ 'username' ] , 
MONGO_PASSWORD = context . obj [ 'password' ] , 
valid_connection = check_connection ( 
host = pymongo_config [ 'MONGO_HOST' ] , 
port = pymongo_config [ 'MONGO_PORT' ] , 
username = pymongo_config [ 'MONGO_USERNAME' ] , 
password = pymongo_config [ 'MONGO_PASSWORD' ] , 
authdb = context . obj [ 'authdb' ] , 
if not valid_connection : 
~~ config = os . path . abspath ( config ) if config else None 
app = create_app ( config = pymongo_config , config_file = config ) 
if livereload : 
~~~ server = Server ( app . wsgi_app ) 
server . serve ( host = host , port = port , debug = debug ) 
~~~ app . run ( host = host , port = port , debug = debug ) 
~~ ~~ def generate_md5_key ( list_of_arguments ) : 
for arg in list_of_arguments : 
~~~ if not isinstance ( arg , string_types ) : 
~~ ~~ hash = hashlib . md5 ( ) 
return hash . hexdigest ( ) 
~~ def init_app ( self , app ) : 
host = app . config . get ( 'MONGO_HOST' , 'localhost' ) 
port = app . config . get ( 'MONGO_PORT' , 27017 ) 
dbname = app . config [ 'MONGO_DBNAME' ] 
self . setup ( app . config [ 'MONGO_DATABASE' ] ) 
~~ def setup ( self , database ) : 
self . db = database 
self . hgnc_collection = database . hgnc_gene 
self . user_collection = database . user 
self . whitelist_collection = database . whitelist 
self . institute_collection = database . institute 
self . event_collection = database . event 
self . case_collection = database . case 
self . panel_collection = database . gene_panel 
self . hpo_term_collection = database . hpo_term 
self . disease_term_collection = database . disease_term 
self . variant_collection = database . variant 
self . acmg_collection = database . acmg 
self . clinvar_collection = database . clinvar 
self . clinvar_submission_collection = database . clinvar_submission 
self . exon_collection = database . exon 
self . transcript_collection = database . transcript 
~~ def index ( context , update ) : 
if update : 
~~~ adapter . update_indexes ( ) 
~~~ adapter . load_indexes ( ) 
~~ ~~ def database ( context , institute_name , user_name , user_mail , api_key ) : 
api_key = api_key or context . obj . get ( 'omim_api_key' ) 
~~ institute_name = institute_name or context . obj [ 'institute_name' ] 
user_name = user_name or context . obj [ 'user_name' ] 
user_mail = user_mail or context . obj [ 'user_mail' ] 
setup_scout ( 
institute_id = institute_name , 
user_mail = user_mail , 
api_key = api_key 
~~ def demo ( context ) : 
institute_name = context . obj [ 'institute_name' ] 
user_name = context . obj [ 'user_name' ] 
user_mail = context . obj [ 'user_mail' ] 
demo = True 
~~ def setup ( context , institute , user_mail , user_name ) : 
context . obj [ 'institute_name' ] = institute 
context . obj [ 'user_name' ] = user_name 
context . obj [ 'user_mail' ] = user_mail 
if context . invoked_subcommand == 'demo' : 
context . obj [ 'mongodb' ] = 'scout-demo' 
~~~ client = get_connection ( 
host = context . obj [ 'host' ] , 
port = context . obj [ 'port' ] , 
username = context . obj [ 'username' ] , 
password = context . obj [ 'password' ] , 
mongodb = context . obj [ 'mongodb' ] 
~~ except ConnectionFailure : 
~~~ context . abort ( ) 
database = client [ context . obj [ 'mongodb' ] ] 
database . test . find_one ( ) 
mongo_adapter = MongoAdapter ( database ) 
context . obj [ 'adapter' ] = mongo_adapter 
~~ def institutes ( context , institute_id , json ) : 
~~~ institute_objs = [ ] 
institute_obj = adapter . institute ( institute_id ) 
if not institute_obj : 
~~ institute_objs . append ( institute_obj ) 
~~~ institute_objs = [ ins_obj for ins_obj in adapter . institutes ( ) ] 
~~ if len ( institute_objs ) == 0 : 
~~ header = '' 
if not json : 
~~~ for key in institute_objs [ 0 ] . keys ( ) : 
~~~ header = header + "{0}\\t" . format ( key ) 
~~ click . echo ( header ) 
~~ for institute_obj in institute_objs : 
~~~ if json : 
~~~ click . echo ( institute_obj ) 
~~ row = '' 
for value in institute_obj . values ( ) : 
~~~ row = row + "{0}\\t" . format ( value ) 
~~ click . echo ( row ) 
~~ ~~ def parse_genetic_models ( models_info , case_id ) : 
genetic_models = [ ] 
if models_info : 
~~~ for family_info in models_info . split ( ',' ) : 
~~~ splitted_info = family_info . split ( ':' ) 
if splitted_info [ 0 ] == case_id : 
~~~ genetic_models = splitted_info [ 1 ] . split ( '|' ) 
~~ ~~ ~~ return genetic_models 
~~ def panels ( context , institute ) : 
panel_objs = adapter . gene_panels ( institute_id = institute ) 
~~ click . echo ( "#panel_name\\tversion\\tnr_genes\\tdate" ) 
for panel_obj in panel_objs : 
~~~ click . echo ( "{0}\\t{1}\\t{2}\\t{3}" . format ( 
str ( panel_obj [ 'version' ] ) , 
len ( panel_obj [ 'genes' ] ) , 
str ( panel_obj [ 'date' ] . strftime ( '%Y-%m-%d' ) ) 
~~ ~~ def add_institute ( self , institute_obj ) : 
internal_id = institute_obj [ 'internal_id' ] 
display_name = institute_obj [ 'internal_id' ] 
if self . institute ( institute_id = internal_id ) : 
. format ( display_name ) ) 
display_name ) ) 
insert_info = self . institute_collection . insert_one ( institute_obj ) 
~~ def update_institute ( self , internal_id , sanger_recipient = None , coverage_cutoff = None , 
frequency_cutoff = None , display_name = None , remove_sanger = None , 
phenotype_groups = None , group_abbreviations = None , add_groups = None ) : 
add_groups = add_groups or False 
institute_obj = self . institute ( internal_id ) 
~~ updates = { } 
updated_institute = institute_obj 
if sanger_recipient : 
~~~ user_obj = self . user ( sanger_recipient ) 
internal_id , sanger_recipient ) ) 
updates [ '$push' ] = { 'sanger_recipients' : remove_sanger } 
~~ if remove_sanger : 
remove_sanger , internal_id ) ) 
updates [ '$pull' ] = { 'sanger_recipients' : remove_sanger } 
~~ if coverage_cutoff : 
internal_id , coverage_cutoff ) ) 
updates [ '$set' ] = { 'coverage_cutoff' : coverage_cutoff } 
~~ if frequency_cutoff : 
internal_id , frequency_cutoff ) ) 
if not '$set' in updates : 
~~~ updates [ '$set' ] = { } 
~~ updates [ '$set' ] = { 'frequency_cutoff' : frequency_cutoff } 
internal_id , display_name ) ) 
~~ updates [ '$set' ] = { 'display_name' : display_name } 
~~ if phenotype_groups : 
~~~ if group_abbreviations : 
~~~ group_abbreviations = list ( group_abbreviations ) 
~~ existing_groups = { } 
if add_groups : 
~~~ existing_groups = institute_obj . get ( 'phenotype_groups' , PHENOTYPE_GROUPS ) 
~~ for i , hpo_term in enumerate ( phenotype_groups ) : 
~~~ hpo_obj = self . hpo_term ( hpo_term ) 
if not hpo_obj : 
~~ hpo_id = hpo_obj [ 'hpo_id' ] 
abbreviation = None 
if group_abbreviations : 
~~~ abbreviation = group_abbreviations [ i ] 
~~ existing_groups [ hpo_term ] = { 'name' : description , 'abbr' : abbreviation } 
~~ updates [ '$set' ] = { 'phenotype_groups' : existing_groups } 
~~ if updates : 
~~~ if not '$set' in updates : 
~~ updates [ '$set' ] [ 'updated_at' ] = datetime . now ( ) 
updated_institute = self . institute_collection . find_one_and_update ( 
{ '_id' : internal_id } , updates , return_document = pymongo . ReturnDocument . AFTER ) 
~~ return updated_institute 
~~ def institute ( self , institute_id ) : 
institute_obj = self . institute_collection . find_one ( { 
'_id' : institute_id 
~~ return institute_obj 
~~ def institutes ( self , institute_ids = None ) : 
if institute_ids : 
~~~ query [ '_id' ] = { '$in' : institute_ids } 
return self . institute_collection . find ( query ) 
~~ def match_date ( date ) : 
if re . match ( date_pattern , date ) : 
~~ def get_date ( date , date_format = None ) : 
date_obj = datetime . datetime . now ( ) 
if date : 
~~~ if date_format : 
~~~ date_obj = datetime . datetime . strptime ( date , date_format ) 
~~~ if match_date ( date ) : 
~~~ if len ( date . split ( '-' ) ) == 3 : 
~~~ date = date . split ( '-' ) 
~~ elif len ( date . split ( '.' ) ) == 3 : 
~~~ date = date . split ( '.' ) 
~~~ date = date . split ( '/' ) 
~~ date_obj = datetime . datetime ( * ( int ( number ) for number in date ) ) 
~~ ~~ ~~ return date_obj 
~~ def hpo_genes ( context , hpo_term ) : 
header = [ "#Gene_id\\tCount" ] 
if not hpo_term : 
~~ for line in header : 
~~ for term in adapter . generate_hpo_gene_list ( * hpo_term ) : 
~~~ click . echo ( "{0}\\t{1}" . format ( term [ 0 ] , term [ 1 ] ) ) 
~~ ~~ def parse_genes ( transcripts ) : 
genes_to_transcripts = { } 
genes = [ ] 
hgvs_identifier = None 
canonical_transcript = None 
exon = None 
~~~ hgnc_id = transcript [ 'hgnc_id' ] 
hgnc_symbol = transcript [ 'hgnc_symbol' ] 
if ( transcript [ 'is_canonical' ] and transcript . get ( 'coding_sequence_name' ) ) : 
~~~ hgvs_identifier = transcript . get ( 'coding_sequence_name' ) 
canonical_transcript = transcript [ 'transcript_id' ] 
exon = transcript [ 'exon' ] 
~~ if hgnc_id : 
~~~ if hgnc_id in genes_to_transcripts : 
~~~ genes_to_transcripts [ hgnc_id ] . append ( transcript ) 
~~~ genes_to_transcripts [ hgnc_id ] = [ transcript ] 
~~~ if hgnc_symbol : 
~~~ if hgnc_symbol in genes_to_transcripts : 
~~~ genes_to_transcripts [ hgnc_symbol ] . append ( transcript ) 
~~~ genes_to_transcripts [ hgnc_symbol ] = [ transcript ] 
~~ ~~ ~~ ~~ for gene_id in genes_to_transcripts : 
~~~ gene_transcripts = genes_to_transcripts [ gene_id ] 
most_severe_consequence = None 
most_severe_rank = float ( 'inf' ) 
most_severe_transcript = None 
most_severe_region = None 
most_severe_sift = None 
most_severe_polyphen = None 
for transcript in gene_transcripts : 
for consequence in transcript [ 'functional_annotations' ] : 
~~~ new_rank = SO_TERMS [ consequence ] [ 'rank' ] 
if new_rank < most_severe_rank : 
~~~ most_severe_rank = new_rank 
most_severe_consequence = consequence 
most_severe_transcript = transcript 
most_severe_sift = transcript [ 'sift_prediction' ] 
most_severe_polyphen = transcript [ 'polyphen_prediction' ] 
most_severe_region = SO_TERMS [ consequence ] [ 'region' ] 
~~ ~~ ~~ gene = { 
'transcripts' : gene_transcripts , 
'most_severe_transcript' : most_severe_transcript , 
'most_severe_consequence' : most_severe_consequence , 
'most_severe_sift' : most_severe_sift , 
'most_severe_polyphen' : most_severe_polyphen , 
'hgnc_id' : hgnc_id , 
'hgnc_symbol' : hgnc_symbol , 
'region_annotation' : most_severe_region , 
'hgvs_identifier' : transcript [ 'coding_sequence_name' ] , 
'canonical_transcript' : transcript [ 'transcript_id' ] , 
'exon' : transcript [ 'exon' ] , 
genes . append ( gene ) 
~~ return genes 
~~ def parse_rank_score ( rank_score_entry , case_id ) : 
rank_score = None 
if rank_score_entry : 
~~~ for family_info in rank_score_entry . split ( ',' ) : 
if case_id == splitted_info [ 0 ] : 
~~~ rank_score = float ( splitted_info [ 1 ] ) 
~~ ~~ ~~ return rank_score 
~~ def user ( context , institute_id , user_name , user_mail , admin ) : 
institutes = [ ] 
for institute in institute_id : 
~~~ institute_obj = adapter . institute ( institute_id = institute ) 
~~ institutes . append ( institute ) 
~~ roles = [ ] 
if admin : 
roles . append ( 'admin' ) 
~~ user_info = dict ( email = user_mail . lower ( ) , name = user_name , roles = roles , institutes = institutes ) 
user_obj = build_user ( user_info ) 
~~~ adapter . add_user ( user_obj ) 
~~ ~~ def parse_transcripts ( raw_transcripts , allele = None ) : 
for entry in raw_transcripts : 
~~~ transcript = { } 
functional_annotations = entry . get ( 'CONSEQUENCE' , '' ) . split ( '&' ) 
transcript [ 'functional_annotations' ] = functional_annotations 
transcript_id = entry . get ( 'FEATURE' , '' ) . split ( ':' ) [ 0 ] 
transcript [ 'transcript_id' ] = transcript_id 
hgnc_id = entry . get ( 'HGNC_ID' ) 
if hgnc_id : 
~~~ hgnc_id = hgnc_id . split ( ':' ) [ - 1 ] 
transcript [ 'hgnc_id' ] = int ( hgnc_id ) 
~~~ transcript [ 'hgnc_id' ] = None 
~~ hgnc_symbol = entry . get ( 'SYMBOL' ) 
if hgnc_symbol : 
~~~ transcript [ 'hgnc_symbol' ] = hgnc_symbol 
~~~ transcript [ 'hgnc_symbol' ] = None 
~~ transcript [ 'protein_id' ] = entry . get ( 'ENSP' ) 
polyphen_prediction = entry . get ( 'POLYPHEN' ) 
prediction_term = 'unknown' 
~~~ prediction_term = polyphen_prediction . split ( '(' ) [ 0 ] 
~~ transcript [ 'polyphen_prediction' ] = prediction_term 
sift_prediction = entry . get ( 'SIFT' ) 
if not sift_prediction : 
~~~ sift_prediction = entry . get ( 'SIFT_PRED' ) 
~~ if sift_prediction : 
~~~ prediction_term = sift_prediction . split ( '(' ) [ 0 ] 
~~ transcript [ 'sift_prediction' ] = prediction_term 
transcript [ 'swiss_prot' ] = entry . get ( 'SWISSPROT' ) or 'unknown' 
if entry . get ( 'DOMAINS' , None ) : 
~~~ pfam_domains = entry [ 'DOMAINS' ] . split ( '&' ) 
for annotation in pfam_domains : 
~~~ annotation = annotation . split ( ':' ) 
domain_name = annotation [ 0 ] 
domain_id = annotation [ 1 ] 
if domain_name == 'Pfam_domain' : 
~~~ transcript [ 'pfam_domain' ] = domain_id 
~~ elif domain_name == 'PROSITE_profiles' : 
~~~ transcript [ 'prosite_profile' ] = domain_id 
~~ elif domain_name == 'SMART_domains' : 
~~~ transcript [ 'smart_domain' ] = domain_id 
~~ ~~ ~~ coding_sequence_entry = entry . get ( 'HGVSC' , '' ) . split ( ':' ) 
protein_sequence_entry = entry . get ( 'HGVSP' , '' ) . split ( ':' ) 
coding_sequence_name = None 
if len ( coding_sequence_entry ) > 1 : 
~~~ coding_sequence_name = coding_sequence_entry [ - 1 ] 
~~ transcript [ 'coding_sequence_name' ] = coding_sequence_name 
protein_sequence_name = None 
if len ( protein_sequence_entry ) > 1 : 
~~~ protein_sequence_name = protein_sequence_entry [ - 1 ] 
~~ transcript [ 'protein_sequence_name' ] = protein_sequence_name 
transcript [ 'biotype' ] = entry . get ( 'BIOTYPE' ) 
transcript [ 'exon' ] = entry . get ( 'EXON' ) 
transcript [ 'intron' ] = entry . get ( 'INTRON' ) 
if entry . get ( 'STRAND' ) : 
~~~ if entry [ 'STRAND' ] == '1' : 
~~~ transcript [ 'strand' ] = '+' 
~~ elif entry [ 'STRAND' ] == '-1' : 
~~~ transcript [ 'strand' ] = '-' 
~~~ transcript [ 'strand' ] = None 
~~ functional = [ ] 
regional = [ ] 
for annotation in functional_annotations : 
~~~ functional . append ( annotation ) 
regional . append ( SO_TERMS [ annotation ] [ 'region' ] ) 
~~ transcript [ 'functional_annotations' ] = functional 
transcript [ 'region_annotations' ] = regional 
transcript [ 'is_canonical' ] = ( entry . get ( 'CANONICAL' ) == 'YES' ) 
cadd_phred = entry . get ( 'CADD_PHRED' ) 
if cadd_phred : 
~~~ transcript [ 'cadd' ] = float ( cadd_phred ) 
~~ thousandg_freqs = [ ] 
gnomad_freqs = [ ] 
~~~ for key in entry : 
~~~ if not key . endswith ( 'AF' ) : 
~~ value = entry [ key ] 
if not value : 
~~ if ( key == 'AF' or key == '1000GAF' ) : 
~~~ transcript [ 'thousand_g_maf' ] = float ( value ) 
~~ if key == 'GNOMAD_AF' : 
~~~ transcript [ 'gnomad_maf' ] = float ( value ) 
~~ if key == 'EXAC_MAX_AF' : 
~~~ transcript [ 'exac_max' ] = float ( value ) 
transcript [ 'exac_maf' ] = float ( value ) 
~~ if 'GNOMAD' in key : 
~~~ gnomad_freqs . append ( float ( value ) ) 
~~~ thousandg_freqs . append ( float ( value ) ) 
~~ ~~ if thousandg_freqs : 
~~~ transcript [ 'thousandg_max' ] = max ( thousandg_freqs ) 
~~ if gnomad_freqs : 
~~~ transcript [ 'gnomad_max' ] = max ( gnomad_freqs ) 
~~ ~~ except Exception as err : 
~~ clinsig = entry . get ( 'CLIN_SIG' ) 
if clinsig : 
~~~ transcript [ 'clinsig' ] = clinsig . split ( '&' ) 
~~ transcript [ 'dbsnp' ] = [ ] 
transcript [ 'cosmic' ] = [ ] 
variant_ids = entry . get ( 'EXISTING_VARIATION' ) 
~~~ for variant_id in variant_ids . split ( '&' ) : 
~~~ if variant_id . startswith ( 'rs' ) : 
~~~ transcript [ 'dbsnp' ] . append ( variant_id ) 
~~ elif variant_id . startswith ( 'COSM' ) : 
~~~ transcript [ 'cosmic' ] . append ( int ( variant_id [ 4 : ] ) ) 
~~ ~~ ~~ yield transcript 
~~ ~~ def check_connection ( host = 'localhost' , port = 27017 , username = None , password = None , 
authdb = None , max_delay = 1 ) : 
#mongodb://[username:password@]host1[:port1][,host2[:port2],...[,hostN[:portN]]][/[database][?options]] 
if username and password : 
client = MongoClient ( uri , serverSelectionTimeoutMS = max_delay ) 
~~~ client . server_info ( ) 
~~ except ( ServerSelectionTimeoutError , OperationFailure ) as err : 
return False 
~~ return True 
uri = app . config . get ( "MONGO_URI" , None ) 
db_name = app . config . get ( "MONGO_DBNAME" , 'scout' ) 
host = app . config . get ( "MONGO_HOST" , 'localhost' ) , 
port = app . config . get ( "MONGO_PORT" , 27017 ) , 
username = app . config . get ( "MONGO_USERNAME" , None ) , 
password = app . config . get ( "MONGO_PASSWORD" , None ) , 
uri = uri , 
mongodb = db_name 
~~ app . config [ "MONGO_DATABASE" ] = client [ db_name ] 
app . config [ 'MONGO_CLIENT' ] = client 
~~ def institutes ( ) : 
institute_objs = user_institutes ( store , current_user ) 
for ins_obj in institute_objs : 
~~~ sanger_recipients = [ ] 
for user_mail in ins_obj . get ( 'sanger_recipients' , [ ] ) : 
~~~ user_obj = store . user ( user_mail ) 
~~ sanger_recipients . append ( user_obj [ 'name' ] ) 
~~ institutes . append ( 
'display_name' : ins_obj [ 'display_name' ] , 
'internal_id' : ins_obj [ '_id' ] , 
'coverage_cutoff' : ins_obj . get ( 'coverage_cutoff' , 'None' ) , 
'sanger_recipients' : sanger_recipients , 
'frequency_cutoff' : ins_obj . get ( 'frequency_cutoff' , 'None' ) , 
'phenotype_groups' : ins_obj . get ( 'phenotype_groups' , PHENOTYPE_GROUPS ) 
~~ data = dict ( institutes = institutes ) 
return render_template ( 
'overview/institutes.html' , ** data ) 
~~ def load_delivery_report ( adapter : MongoAdapter , 
report_path : str , 
case_id : str , 
update : bool = False ) : 
case_obj = adapter . case ( 
~~ if not case_obj . get ( 'delivery_report' ) : 
~~~ _put_report_in_case_root ( case_obj , report_path ) 
~~~ if update : 
'overwrite' ) 
return adapter . replace_case ( case_obj ) 
~~ def build_transcript ( transcript , build = '37' ) : 
transcript_id = transcript [ 'transcript_id' ] 
transcript_obj = dict ( 
transcript_id = transcript_id 
transcript_obj [ 'hgnc_id' ] = transcript [ 'hgnc_id' ] 
if transcript . get ( 'protein_id' ) : 
~~~ transcript_obj [ 'protein_id' ] = transcript [ 'protein_id' ] 
~~ if transcript . get ( 'sift_prediction' ) : 
~~~ transcript_obj [ 'sift_prediction' ] = transcript [ 'sift_prediction' ] 
~~ if transcript . get ( 'polyphen_prediction' ) : 
~~~ transcript_obj [ 'polyphen_prediction' ] = transcript [ 'polyphen_prediction' ] 
~~ if transcript . get ( 'swiss_prot' ) : 
~~~ transcript_obj [ 'swiss_prot' ] = transcript [ 'swiss_prot' ] 
~~ if transcript . get ( 'pfam_domain' ) : 
~~~ transcript_obj [ 'pfam_domain' ] = transcript . get ( 'pfam_domain' ) 
~~ if transcript . get ( 'prosite_profile' ) : 
~~~ transcript_obj [ 'prosite_profile' ] = transcript . get ( 'prosite_profile' ) 
~~ if transcript . get ( 'smart_domain' ) : 
~~~ transcript_obj [ 'smart_domain' ] = transcript . get ( 'smart_domain' ) 
~~ if transcript . get ( 'biotype' ) : 
~~~ transcript_obj [ 'biotype' ] = transcript . get ( 'biotype' ) 
~~ if transcript . get ( 'functional_annotations' ) : 
~~~ transcript_obj [ 'functional_annotations' ] = transcript [ 'functional_annotations' ] 
~~ if transcript . get ( 'region_annotations' ) : 
~~~ transcript_obj [ 'region_annotations' ] = transcript [ 'region_annotations' ] 
~~ if transcript . get ( 'exon' ) : 
~~~ transcript_obj [ 'exon' ] = transcript . get ( 'exon' ) 
~~ if transcript . get ( 'intron' ) : 
~~~ transcript_obj [ 'intron' ] = transcript . get ( 'intron' ) 
~~ if transcript . get ( 'strand' ) : 
~~~ transcript_obj [ 'strand' ] = transcript . get ( 'strand' ) 
~~ if transcript . get ( 'coding_sequence_name' ) : 
~~~ transcript_obj [ 'coding_sequence_name' ] = transcript [ 'coding_sequence_name' ] 
~~ if transcript . get ( 'protein_sequence_name' ) : 
~~~ transcript_obj [ 'protein_sequence_name' ] = transcript [ 'protein_sequence_name' ] 
~~ transcript_obj [ 'is_canonical' ] = transcript . get ( 'is_canonical' , False ) 
return transcript_obj 
~~ def update_user ( self , user_obj ) : 
updated_user = self . user_collection . find_one_and_replace ( 
{ '_id' : user_obj [ '_id' ] } , 
user_obj , 
return updated_user 
~~ def add_user ( self , user_obj ) : 
if not '_id' in user_obj : 
~~~ user_obj [ '_id' ] = user_obj [ 'email' ] 
~~~ self . user_collection . insert_one ( user_obj ) 
~~ except DuplicateKeyError as err : 
~~ def users ( self , institute = None ) : 
query = { 'institutes' : { '$in' : [ institute ] } } 
~~ res = self . user_collection . find ( query ) 
return res 
~~ def user ( self , email ) : 
user_obj = self . user_collection . find_one ( { '_id' : email } ) 
return user_obj 
~~ def delete_user ( self , email ) : 
user_obj = self . user_collection . delete_one ( { '_id' : email } ) 
~~ def build_compound ( compound ) : 
compound_obj = dict ( 
variant = compound [ 'variant' ] , 
display_name = compound [ 'display_name' ] , 
combined_score = float ( compound [ 'score' ] ) 
return compound_obj 
~~ def remote_static ( ) : 
file_path = request . args . get ( 'file' ) 
range_header = request . headers . get ( 'Range' , None ) 
if not range_header and file_path . endswith ( '.bam' ) : 
~~~ return abort ( 500 ) 
~~ new_resp = send_file_partial ( file_path ) 
return new_resp 
~~ def pileup ( ) : 
vcf_file = request . args . get ( 'vcf' ) 
bam_files = request . args . getlist ( 'bam' ) 
bai_files = request . args . getlist ( 'bai' ) 
samples = request . args . getlist ( 'sample' ) 
alignments = [ { 'bam' : bam , 'bai' : bai , 'sample' : sample } 
for bam , bai , sample in zip ( bam_files , bai_files , samples ) ] 
position = { 
'contig' : request . args [ 'contig' ] , 
'start' : request . args [ 'start' ] , 
'stop' : request . args [ 'stop' ] 
genome = current_app . config . get ( 'PILEUP_GENOME' ) 
if genome : 
~~~ if not os . path . isfile ( genome ) : 
genome = None 
exons = current_app . config . get ( 'PILEUP_EXONS' ) 
if exons : 
~~~ if not os . path . isfile ( exons ) : 
position [ 'contig' ] , position [ 'start' ] , position [ 'stop' ] ) ) 
return render_template ( 'alignviewers/pileup.html' , alignments = alignments , 
position = position , vcf_file = vcf_file , 
genome = genome , exons = exons ) 
~~ def igv ( ) : 
chrom = request . args . get ( 'contig' ) 
if chrom == 'MT' : 
~~~ chrom = 'M' 
~~ start = request . args . get ( 'start' ) 
stop = request . args . get ( 'stop' ) 
locus = "chr{0}:{1}-{2}" . format ( chrom , start , stop ) 
chromosome_build = request . args . get ( 'build' ) 
display_obj = { } 
fastaURL = '' 
indexURL = '' 
cytobandURL = '' 
gene_track_format = '' 
gene_track_URL = '' 
gene_track_indexURL = '' 
if chromosome_build == "GRCh38" or chrom == 'M' : 
~~~ fastaURL = 'https://s3.amazonaws.com/igv.broadinstitute.org/genomes/seq/hg38/hg38.fa' 
indexURL = 'https://s3.amazonaws.com/igv.broadinstitute.org/genomes/seq/hg38/hg38.fa.fai' 
cytobandURL = 'https://s3.amazonaws.com/igv.broadinstitute.org/annotations/hg38/cytoBandIdeo.txt' 
gene_track_format = 'gtf' 
gene_track_URL = 'https://s3.amazonaws.com/igv.broadinstitute.org/annotations/hg38/genes/Homo_sapiens.GRCh38.80.sorted.gtf.gz' 
gene_track_indexURL = 'https://s3.amazonaws.com/igv.broadinstitute.org/annotations/hg38/genes/Homo_sapiens.GRCh38.80.sorted.gtf.gz.tbi' 
~~~ fastaURL = 'https://s3.amazonaws.com/igv.broadinstitute.org/genomes/seq/hg19/hg19.fasta' 
indexURL = 'https://s3.amazonaws.com/igv.broadinstitute.org/genomes/seq/hg19/hg19.fasta.fai' 
cytobandURL = 'https://s3.amazonaws.com/igv.broadinstitute.org/genomes/seq/hg19/cytoBand.txt' 
gene_track_format = 'bed' 
gene_track_URL = 'https://s3.amazonaws.com/igv.broadinstitute.org/annotations/hg19/genes/refGene.hg19.bed.gz' 
gene_track_indexURL = 'https://s3.amazonaws.com/igv.broadinstitute.org/annotations/hg19/genes/refGene.hg19.bed.gz.tbi' 
~~ display_obj [ 'reference_track' ] = { 
'fastaURL' : fastaURL , 
'indexURL' : indexURL , 
'cytobandURL' : cytobandURL 
display_obj [ 'genes_track' ] = { 
'name' : 'Genes' , 
'type' : 'annotation' , 
'format' : gene_track_format , 
'sourceType' : 'file' , 
'url' : gene_track_URL , 
'indexURL' : gene_track_indexURL , 
'displayMode' : 'EXPANDED' 
sample_tracks = [ ] 
counter = 0 
~~~ if bam_files [ counter ] : 
~~~ sample_tracks . append ( { 'name' : sample , 'url' : bam_files [ counter ] , 
'indexURL' : bai_files [ counter ] , 
'height' : 700 , 'maxHeight' : 2000 } ) 
~~ counter += 1 
~~ display_obj [ 'sample_tracks' ] = sample_tracks 
if request . args . get ( 'center_guide' ) : 
~~~ display_obj [ 'display_center_guide' ] = True 
~~~ display_obj [ 'display_center_guide' ] = False 
~~ return render_template ( 'alignviewers/igv_viewer.html' , locus = locus , ** display_obj ) 
~~ def build_disease_term ( disease_info , alias_genes = { } ) : 
~~~ disease_nr = int ( disease_info [ 'mim_number' ] ) 
~~ disease_id = "{0}:{1}" . format ( 'OMIM' , disease_nr ) 
~~~ description = disease_info [ 'description' ] 
~~ disease_obj = DiseaseTerm ( 
disease_id = disease_id , 
disease_nr = disease_nr , 
description = description , 
source = 'OMIM' 
inheritance_models = disease_info . get ( 'inheritance' ) 
if inheritance_models : 
~~~ disease_obj [ 'inheritance' ] = list ( inheritance_models ) 
~~ hgnc_ids = set ( ) 
for hgnc_symbol in disease_info . get ( 'hgnc_symbols' , [ ] ) : 
~~~ if hgnc_symbol in alias_genes : 
~~~ if alias_genes [ hgnc_symbol ] [ 'true' ] : 
~~~ hgnc_ids . add ( alias_genes [ hgnc_symbol ] [ 'true' ] ) 
~~~ for hgnc_id in alias_genes [ hgnc_symbol ] [ 'ids' ] : 
~~~ hgnc_ids . add ( hgnc_id ) 
~~ ~~ disease_obj [ 'genes' ] = list ( hgnc_ids ) 
if 'hpo_terms' in disease_info : 
~~~ disease_obj [ 'hpo_terms' ] = list ( disease_info [ 'hpo_terms' ] ) 
~~ return disease_obj 
~~ def load_exons ( adapter , exon_lines , build = '37' , ensembl_genes = None ) : 
hgnc_id_transcripts = adapter . id_transcripts_by_gene ( build = build ) 
if isinstance ( exon_lines , DataFrame ) : 
~~~ exons = parse_ensembl_exon_request ( exon_lines ) 
nr_exons = exon_lines . shape [ 0 ] 
~~~ exons = parse_ensembl_exons ( exon_lines ) 
nr_exons = 1000000 
~~ start_insertion = datetime . now ( ) 
loaded_exons = 0 
~~~ for exon in bar : 
~~~ ensg_id = exon [ 'gene' ] 
enst_id = exon [ 'transcript' ] 
gene_obj = ensembl_genes . get ( ensg_id ) 
~~ hgnc_id = gene_obj [ 'hgnc_id' ] 
if not enst_id in hgnc_id_transcripts [ hgnc_id ] : 
~~ exon [ 'hgnc_id' ] = hgnc_id 
exon_obj = build_exon ( exon , build ) 
adapter . load_exon ( exon_obj ) 
loaded_exons += 1 
~~ def parse_variant ( variant , case , variant_type = 'clinical' , 
rank_results_header = None , vep_header = None , 
individual_positions = None , category = None ) : 
rank_results_header = rank_results_header or [ ] 
vep_header = vep_header or [ ] 
parsed_variant = { } 
case_id = case [ '_id' ] 
if '-' in case_id : 
genmod_key = case [ 'display_name' ] 
~~~ genmod_key = case [ '_id' ] 
~~ chrom_match = CHR_PATTERN . match ( variant . CHROM ) 
if variant . ALT : 
~~~ alt = variant . ALT [ 0 ] 
~~ elif not variant . ALT and category == "str" : 
~~~ alt = '.' 
~~ parsed_variant [ 'ids' ] = parse_ids ( 
pos = variant . POS , 
ref = variant . REF , 
alt = alt , 
parsed_variant [ 'case_id' ] = case_id 
parsed_variant [ 'variant_type' ] = variant_type 
if not category : 
~~~ category = variant . var_type 
if category == 'indel' : 
~~~ category = 'snv' 
~~ if category == 'snp' : 
~~ ~~ parsed_variant [ 'category' ] = category 
parsed_variant [ 'reference' ] = variant . REF 
if len ( variant . ALT ) > 1 : 
~~ parsed_variant [ 'alternative' ] = alt 
parsed_variant [ 'quality' ] = variant . QUAL 
if variant . FILTER : 
~~~ parsed_variant [ 'filters' ] = variant . FILTER . split ( ';' ) 
~~~ parsed_variant [ 'filters' ] = [ 'PASS' ] 
~~ parsed_variant [ 'dbsnp_id' ] = variant . ID 
parsed_variant [ 'mate_id' ] = None 
parsed_variant [ 'chromosome' ] = chrom 
coordinates = parse_coordinates ( variant , category ) 
parsed_variant [ 'position' ] = coordinates [ 'position' ] 
parsed_variant [ 'sub_category' ] = coordinates [ 'sub_category' ] 
parsed_variant [ 'mate_id' ] = coordinates [ 'mate_id' ] 
parsed_variant [ 'end' ] = coordinates [ 'end' ] 
parsed_variant [ 'length' ] = coordinates [ 'length' ] 
parsed_variant [ 'end_chrom' ] = coordinates [ 'end_chrom' ] 
parsed_variant [ 'cytoband_start' ] = coordinates [ 'cytoband_start' ] 
parsed_variant [ 'cytoband_end' ] = coordinates [ 'cytoband_end' ] 
rank_score = parse_rank_score ( variant . INFO . get ( 'RankScore' , '' ) , genmod_key ) 
parsed_variant [ 'rank_score' ] = rank_score or 0 
if individual_positions and case [ 'individuals' ] : 
~~~ parsed_variant [ 'samples' ] = parse_genotypes ( variant , case [ 'individuals' ] , 
individual_positions ) 
~~~ parsed_variant [ 'samples' ] = [ ] 
~~ compounds = parse_compounds ( compound_info = variant . INFO . get ( 'Compounds' ) , 
case_id = genmod_key , 
variant_type = variant_type ) 
if compounds : 
~~~ parsed_variant [ 'compounds' ] = compounds 
~~ genetic_models = parse_genetic_models ( variant . INFO . get ( 'GeneticModels' ) , genmod_key ) 
if genetic_models : 
~~~ parsed_variant [ 'genetic_models' ] = genetic_models 
~~ azlength = variant . INFO . get ( 'AZLENGTH' ) 
if azlength : 
~~~ parsed_variant [ 'azlength' ] = int ( azlength ) 
~~ azqual = variant . INFO . get ( 'AZQUAL' ) 
if azqual : 
~~~ parsed_variant [ 'azqual' ] = float ( azqual ) 
~~ repeat_id = variant . INFO . get ( 'REPID' ) 
if repeat_id : 
~~~ parsed_variant [ 'str_repid' ] = str ( repeat_id ) 
~~ repeat_unit = variant . INFO . get ( 'RU' ) 
if repeat_unit : 
~~~ parsed_variant [ 'str_ru' ] = str ( repeat_unit ) 
~~ repeat_ref = variant . INFO . get ( 'REF' ) 
if repeat_ref : 
~~~ parsed_variant [ 'str_ref' ] = int ( repeat_ref ) 
~~ repeat_len = variant . INFO . get ( 'RL' ) 
if repeat_len : 
~~~ parsed_variant [ 'str_len' ] = int ( repeat_len ) 
~~ str_status = variant . INFO . get ( 'STR_STATUS' ) 
if str_status : 
~~~ parsed_variant [ 'str_status' ] = str ( str_status ) 
~~ raw_transcripts = [ ] 
if vep_header : 
~~~ vep_info = variant . INFO . get ( 'CSQ' ) 
if vep_info : 
~~~ raw_transcripts = ( dict ( zip ( vep_header , transcript_info . split ( '|' ) ) ) 
for transcript_info in vep_info . split ( ',' ) ) 
~~ ~~ parsed_transcripts = [ ] 
dbsnp_ids = set ( ) 
cosmic_ids = set ( ) 
for parsed_transcript in parse_transcripts ( raw_transcripts , parsed_variant [ 'alternative' ] ) : 
~~~ parsed_transcripts . append ( parsed_transcript ) 
for dbsnp in parsed_transcript . get ( 'dbsnp' , [ ] ) : 
~~~ dbsnp_ids . add ( dbsnp ) 
~~ for cosmic in parsed_transcript . get ( 'cosmic' , [ ] ) : 
~~~ cosmic_ids . add ( cosmic ) 
~~ ~~ cosmic_tag = variant . INFO . get ( 'COSMIC' ) 
if cosmic_tag : 
~~~ cosmic_ids . add ( cosmic_tag [ 4 : ] ) 
~~ if ( dbsnp_ids and not parsed_variant [ 'dbsnp_id' ] ) : 
~~~ parsed_variant [ 'dbsnp_id' ] = ';' . join ( dbsnp_ids ) 
~~ if cosmic_ids : 
~~~ parsed_variant [ 'cosmic_ids' ] = list ( cosmic_ids ) 
~~ gene_info = parse_genes ( parsed_transcripts ) 
parsed_variant [ 'genes' ] = gene_info 
hgnc_ids = set ( [ ] ) 
for gene in parsed_variant [ 'genes' ] : 
~~~ hgnc_ids . add ( gene [ 'hgnc_id' ] ) 
~~ parsed_variant [ 'hgnc_ids' ] = list ( hgnc_ids ) 
if variant . INFO . get ( 'CLNACC' ) : 
~~~ acc = variant . INFO . get ( 'CLNACC' ) 
~~~ acc = variant . INFO . get ( 'CLNVID' ) 
~~ clnsig_predictions = parse_clnsig ( 
acc = acc , 
sig = variant . INFO . get ( 'CLNSIG' ) , 
revstat = variant . INFO . get ( 'CLNREVSTAT' ) , 
transcripts = parsed_transcripts 
if clnsig_predictions : 
~~~ parsed_variant [ 'clnsig' ] = clnsig_predictions 
~~ frequencies = parse_frequencies ( variant , parsed_transcripts ) 
parsed_variant [ 'frequencies' ] = frequencies 
local_obs_old = variant . INFO . get ( 'Obs' ) 
if local_obs_old : 
~~~ parsed_variant [ 'local_obs_old' ] = int ( local_obs_old ) 
~~ local_obs_hom_old = variant . INFO . get ( 'Hom' ) 
if local_obs_hom_old : 
~~~ parsed_variant [ 'local_obs_hom_old' ] = int ( local_obs_hom_old ) 
~~ cadd = parse_cadd ( variant , parsed_transcripts ) 
if cadd : 
~~~ parsed_variant [ 'cadd_score' ] = cadd 
~~ spidex = variant . INFO . get ( 'SPIDEX' ) 
if spidex : 
~~~ parsed_variant [ 'spidex' ] = float ( spidex ) 
~~ parsed_variant [ 'conservation' ] = parse_conservations ( variant ) 
parsed_variant [ 'callers' ] = parse_callers ( variant , category = category ) 
rank_result = variant . INFO . get ( 'RankResult' ) 
if rank_result : 
~~~ results = [ int ( i ) for i in rank_result . split ( '|' ) ] 
parsed_variant [ 'rank_result' ] = dict ( zip ( rank_results_header , results ) ) 
~~ sv_frequencies = parse_sv_frequencies ( variant ) 
for key in sv_frequencies : 
~~~ parsed_variant [ 'frequencies' ] [ key ] = sv_frequencies [ key ] 
~~ mvl_tag = variant . INFO . get ( 'MSK_MVL' ) 
if mvl_tag : 
~~~ parsed_variant [ 'mvl_tag' ] = True 
~~ return parsed_variant 
~~ def compounds ( context , case_id ) : 
case_obj = adapter . case ( case_id ) 
~~~ adapter . update_case_compounds ( case_obj ) 
~~ ~~ def add_gene_links ( gene_obj , build = 37 ) : 
~~~ build = int ( build ) 
~~~ build = 37 
gene_obj [ 'hgnc_link' ] = genenames ( hgnc_id ) 
gene_obj [ 'omim_link' ] = omim ( hgnc_id ) 
if not 'ensembl_id' in gene_obj : 
~~~ ensembl_id = gene_obj . get ( 'common' , { } ) . get ( 'ensembl_id' ) 
~~~ ensembl_id = gene_obj [ 'ensembl_id' ] 
~~ ensembl_37_link = ensembl ( ensembl_id , build = 37 ) 
ensembl_38_link = ensembl ( ensembl_id , build = 38 ) 
gene_obj [ 'ensembl_37_link' ] = ensembl_37_link 
gene_obj [ 'ensembl_38_link' ] = ensembl_38_link 
gene_obj [ 'ensembl_link' ] = ensembl_37_link 
if build == 38 : 
~~~ gene_obj [ 'ensembl_link' ] = ensembl_38_link 
~~ gene_obj [ 'hpa_link' ] = hpa ( ensembl_id ) 
gene_obj [ 'string_link' ] = string ( ensembl_id ) 
gene_obj [ 'reactome_link' ] = reactome ( ensembl_id ) 
gene_obj [ 'clingen_link' ] = clingen ( hgnc_id ) 
gene_obj [ 'expression_atlas_link' ] = expression_atlas ( ensembl_id ) 
gene_obj [ 'exac_link' ] = exac ( ensembl_id ) 
gene_obj [ 'entrez_link' ] = entrez ( gene_obj . get ( 'entrez_id' ) ) 
gene_obj [ 'omim_link' ] = omim ( gene_obj . get ( 'omim_id' ) ) 
gene_obj [ 'ppaint_link' ] = ppaint ( gene_obj [ 'hgnc_symbol' ] ) 
gene_obj [ 'vega_link' ] = vega ( gene_obj . get ( 'vega_id' ) ) 
gene_obj [ 'ucsc_link' ] = ucsc ( gene_obj . get ( 'ucsc_id' ) ) 
~~ def hgnc ( ctx , hgnc_symbol , hgnc_id , build ) : 
if not ( hgnc_symbol or hgnc_id ) : 
~~~ result = adapter . hgnc_gene ( hgnc_id , build = build ) 
if result : 
~~~ hgnc_symbol = result [ 'hgnc_symbol' ] 
~~ ~~ result = adapter . hgnc_genes ( hgnc_symbol , build = build ) 
if result . count ( ) == 0 : 
~~~ click . echo ( "#hgnc_id\\thgnc_symbol\\taliases\\ttranscripts" ) 
for gene in result : 
gene [ 'hgnc_id' ] , 
gene [ 'hgnc_symbol' ] , 
~~ ~~ ~~ def parse_hgnc_line ( line , header ) : 
hgnc_gene = { } 
line = line . rstrip ( ) . split ( '\\t' ) 
raw_info = dict ( zip ( header , line ) ) 
if 'Withdrawn' in raw_info [ 'status' ] : 
~~~ return hgnc_gene 
~~ hgnc_symbol = raw_info [ 'symbol' ] 
hgnc_gene [ 'hgnc_symbol' ] = hgnc_symbol 
hgnc_gene [ 'hgnc_id' ] = int ( raw_info [ 'hgnc_id' ] . split ( ':' ) [ - 1 ] ) 
hgnc_gene [ 'description' ] = raw_info [ 'name' ] 
aliases = set ( [ hgnc_symbol , hgnc_symbol . upper ( ) ] ) 
previous_names = raw_info [ 'prev_symbol' ] 
if previous_names : 
~~~ for alias in previous_names . strip ( \ ) . split ( '|' ) : 
~~~ aliases . add ( alias ) 
~~ ~~ alias_symbols = raw_info [ 'alias_symbol' ] 
if alias_symbols : 
~~~ for alias in alias_symbols . strip ( \ ) . split ( '|' ) : 
~~ ~~ hgnc_gene [ 'previous_symbols' ] = list ( aliases ) 
hgnc_gene [ 'ensembl_gene_id' ] = raw_info . get ( 'ensembl_gene_id' ) 
omim_id = raw_info . get ( 'omim_id' ) 
if omim_id : 
~~~ hgnc_gene [ 'omim_id' ] = int ( omim_id . strip ( \ ) . split ( '|' ) [ 0 ] ) 
~~~ hgnc_gene [ 'omim_id' ] = None 
~~ entrez_id = hgnc_gene [ 'entrez_id' ] = raw_info . get ( 'entrez_id' ) 
if entrez_id : 
~~~ hgnc_gene [ 'entrez_id' ] = int ( entrez_id ) 
~~~ hgnc_gene [ 'entrez_id' ] = None 
~~ ref_seq = raw_info . get ( 'refseq_accession' ) 
if ref_seq : 
~~~ hgnc_gene [ 'ref_seq' ] = ref_seq . strip ( \ ) . split ( '|' ) 
~~~ hgnc_gene [ 'ref_seq' ] = [ ] 
~~ uniprot_ids = raw_info . get ( 'uniprot_ids' ) 
if uniprot_ids : 
~~~ hgnc_gene [ 'uniprot_ids' ] = uniprot_ids . strip ( \ ) . split ( '|' ) 
~~~ hgnc_gene [ 'uniprot_ids' ] = [ ] 
~~ ucsc_id = raw_info . get ( 'ucsc_id' ) 
if ucsc_id : 
~~~ hgnc_gene [ 'ucsc_id' ] = ucsc_id 
~~~ hgnc_gene [ 'ucsc_id' ] = None 
~~ vega_id = raw_info . get ( 'vega_id' ) 
if vega_id : 
~~~ hgnc_gene [ 'vega_id' ] = vega_id 
~~~ hgnc_gene [ 'vega_id' ] = None 
~~ return hgnc_gene 
~~ def parse_hgnc_genes ( lines ) : 
header = [ ] 
for index , line in enumerate ( lines ) : 
~~~ if index == 0 : 
~~~ header = line . split ( '\\t' ) 
~~ elif len ( line ) > 1 : 
~~~ hgnc_gene = parse_hgnc_line ( line = line , header = header ) 
~~~ yield hgnc_gene 
~~ ~~ ~~ ~~ def create_submission ( self , user_id , institute_id ) : 
submission_obj = { 
'status' : 'open' , 
'created_at' : datetime . now ( ) , 
'user_id' : user_id , 
'institute_id' : institute_id 
result = self . clinvar_submission_collection . insert_one ( submission_obj ) 
return result . inserted_id 
~~ def delete_submission ( self , submission_id ) : 
submission_obj = self . clinvar_submission_collection . find_one ( { '_id' : ObjectId ( submission_id ) } ) 
submission_variants = submission_obj . get ( 'variant_data' ) 
submission_casedata = submission_obj . get ( 'case_data' ) 
submission_objects = [ ] 
if submission_variants and submission_casedata : 
~~~ submission_objects = submission_variants + submission_casedata 
~~ elif submission_variants : 
~~~ submission_objects = submission_variants 
~~ elif submission_casedata : 
~~~ submission_objects = submission_casedata 
~~ result = self . clinvar_collection . delete_many ( { '_id' : { "$in" : submission_objects } } ) 
deleted_objects = result . deleted_count 
result = self . clinvar_submission_collection . delete_one ( { '_id' : ObjectId ( submission_id ) } ) 
deleted_submissions = result . deleted_count 
return deleted_objects , deleted_submissions 
~~ def get_open_clinvar_submission ( self , user_id , institute_id ) : 
query = dict ( user_id = user_id , institute_id = institute_id , status = 'open' ) 
submission = self . clinvar_submission_collection . find_one ( query ) 
if submission is None : 
~~~ submission_id = self . create_submission ( user_id , institute_id ) 
submission = self . clinvar_submission_collection . find_one ( { '_id' : submission_id } ) 
~~ return submission 
~~ def update_clinvar_id ( self , clinvar_id , submission_id ) : 
updated_submission = self . clinvar_submission_collection . find_one_and_update ( { '_id' : ObjectId ( submission_id ) } , { '$set' : { 'clinvar_subm_id' : clinvar_id , 'updated_at' : datetime . now ( ) } } , upsert = True , return_document = pymongo . ReturnDocument . AFTER ) 
return updated_submission 
~~ def get_clinvar_id ( self , submission_id ) : 
return clinvar_subm_id 
~~ def add_to_submission ( self , submission_id , submission_objects ) : 
for var_obj in submission_objects [ 0 ] : 
~~~ result = self . clinvar_collection . insert_one ( var_obj ) 
self . clinvar_submission_collection . update_one ( { '_id' : submission_id } , { '$push' : { 'variant_data' : str ( result . inserted_id ) } } , upsert = True ) 
~~ except pymongo . errors . DuplicateKeyError : 
~~ ~~ if submission_objects [ 1 ] : 
~~~ for case_obj in submission_objects [ 1 ] : 
~~~ result = self . clinvar_collection . insert_one ( case_obj ) 
self . clinvar_submission_collection . update_one ( { '_id' : submission_id } , { '$push' : { 'case_data' : str ( result . inserted_id ) } } , upsert = True ) 
~~ ~~ ~~ updated_submission = self . clinvar_submission_collection . find_one_and_update ( { '_id' : submission_id } , { '$set' : { 'updated_at' : datetime . now ( ) } } , return_document = pymongo . ReturnDocument . AFTER ) 
~~ def update_clinvar_submission_status ( self , user_id , submission_id , status ) : 
LOG . info ( \ , submission_id ) 
~~~ self . clinvar_submission_collection . update_many ( 
{ 'user_id' : user_id } , 
{ '$set' : 
{ 'status' : 'closed' , 'updated_at' : datetime . now ( ) } 
~~ updated_submission = self . clinvar_submission_collection . find_one_and_update ( 
{ '_id' : ObjectId ( submission_id ) } , 
{ 'status' : status , 'updated_at' : datetime . now ( ) } 
~~ def clinvar_submissions ( self , user_id , institute_id ) : 
query = dict ( user_id = user_id , institute_id = institute_id ) 
results = list ( self . clinvar_submission_collection . find ( query ) ) 
submissions = [ ] 
for result in results : 
~~~ submission = { } 
submission [ '_id' ] = result . get ( '_id' ) 
submission [ 'status' ] = result . get ( 'status' ) 
submission [ 'user_id' ] = result . get ( 'user_id' ) 
submission [ 'institute_id' ] = result . get ( 'institute_id' ) 
submission [ 'created_at' ] = result . get ( 'created_at' ) 
submission [ 'updated_at' ] = result . get ( 'updated_at' ) 
if 'clinvar_subm_id' in result : 
~~~ submission [ 'clinvar_subm_id' ] = result [ 'clinvar_subm_id' ] 
~~ if result . get ( 'variant_data' ) : 
~~~ submission [ 'variant_data' ] = self . clinvar_collection . find ( { '_id' : { "$in" : result [ 'variant_data' ] } } ) 
~~ if result . get ( 'case_data' ) : 
~~~ submission [ 'case_data' ] = self . clinvar_collection . find ( { '_id' : { "$in" : result [ 'case_data' ] } } ) 
~~ submissions . append ( submission ) 
~~ return submissions 
~~ def clinvar_objs ( self , submission_id , key_id ) : 
submission = self . clinvar_submission_collection . find_one ( { '_id' : ObjectId ( submission_id ) } ) 
if submission . get ( key_id ) : 
~~~ clinvar_obj_ids = list ( submission . get ( key_id ) ) 
clinvar_objects = self . clinvar_collection . find ( { '_id' : { "$in" : clinvar_obj_ids } } ) 
return list ( clinvar_objects ) 
~~ ~~ def delete_clinvar_object ( self , object_id , object_type , submission_id ) : 
result = '' 
if object_type == 'variant_data' : 
~~~ self . clinvar_submission_collection . find_one_and_update ( { '_id' : ObjectId ( submission_id ) } , { '$pull' : { 'variant_data' : object_id } } ) 
variant_object = self . clinvar_collection . find_one ( { '_id' : object_id } ) 
result = self . clinvar_collection . delete_many ( { 'linking_id' : linking_id } ) 
~~~ result = self . clinvar_collection . delete_one ( { '_id' : object_id } ) 
~~ self . clinvar_submission_collection . find_one_and_update ( { '_id' : ObjectId ( submission_id ) } , { '$pull' : { 'case_data' : object_id } } ) 
updated_submission = self . clinvar_submission_collection . find_one_and_update ( { '_id' : submission_id } , { '$set' : { 'updated_at' : datetime . now ( ) } } , return_document = pymongo . ReturnDocument . AFTER ) 
~~ def case_to_clinVars ( self , case_id ) : 
query = dict ( case_id = case_id , csv_type = 'variant' ) 
clinvar_objs = list ( self . clinvar_collection . find ( query ) ) 
submitted_vars = { } 
for clinvar in clinvar_objs : 
~~~ submitted_vars [ clinvar . get ( 'local_id' ) ] = clinvar 
~~ return submitted_vars 
~~ def parse_hpo_phenotype ( hpo_line ) : 
hpo_line = hpo_line . rstrip ( ) . split ( '\\t' ) 
hpo_info = { } 
hpo_info [ 'hpo_id' ] = hpo_line [ 0 ] 
hpo_info [ 'description' ] = hpo_line [ 1 ] 
hpo_info [ 'hgnc_symbol' ] = hpo_line [ 3 ] 
return hpo_info 
~~ def parse_hpo_gene ( hpo_line ) : 
if not len ( hpo_line ) > 3 : 
~~~ return { } 
~~ hpo_line = hpo_line . rstrip ( ) . split ( '\\t' ) 
hpo_info [ 'hgnc_symbol' ] = hpo_line [ 1 ] 
hpo_info [ 'description' ] = hpo_line [ 2 ] 
hpo_info [ 'hpo_id' ] = hpo_line [ 3 ] 
~~ def parse_hpo_disease ( hpo_line ) : 
disease = hpo_line [ 0 ] . split ( ':' ) 
hpo_info [ 'source' ] = disease [ 0 ] 
hpo_info [ 'disease_nr' ] = int ( disease [ 1 ] ) 
hpo_info [ 'hgnc_symbol' ] = None 
hpo_info [ 'hpo_term' ] = None 
if len ( hpo_line ) >= 3 : 
~~~ hpo_info [ 'hgnc_symbol' ] = hpo_line [ 2 ] 
if len ( hpo_line ) >= 4 : 
~~~ hpo_info [ 'hpo_term' ] = hpo_line [ 3 ] 
~~ ~~ return hpo_info 
~~ def parse_hpo_phenotypes ( hpo_lines ) : 
hpo_terms = { } 
for index , line in enumerate ( hpo_lines ) : 
~~~ if index > 0 and len ( line ) > 0 : 
~~~ hpo_info = parse_hpo_phenotype ( line ) 
hpo_term = hpo_info [ 'hpo_id' ] 
hgnc_symbol = hpo_info [ 'hgnc_symbol' ] 
if hpo_term in hpo_terms : 
~~~ hpo_terms [ hpo_term ] [ 'hgnc_symbols' ] . append ( hgnc_symbol ) 
~~~ hpo_terms [ hpo_term ] = { 
'hpo_id' : hpo_term , 
'description' : hpo_info [ 'description' ] , 
'hgnc_symbols' : [ hgnc_symbol ] 
return hpo_terms 
~~ def parse_hpo_diseases ( hpo_lines ) : 
diseases = { } 
~~ if not len ( line ) > 3 : 
~~ disease_info = parse_hpo_disease ( line ) 
if not disease_info : 
~~ disease_nr = disease_info [ 'disease_nr' ] 
hgnc_symbol = disease_info [ 'hgnc_symbol' ] 
hpo_term = disease_info [ 'hpo_term' ] 
source = disease_info [ 'source' ] 
disease_id = "{0}:{1}" . format ( source , disease_nr ) 
if disease_id not in diseases : 
~~~ diseases [ disease_id ] = { 
'disease_nr' : disease_nr , 
'source' : source , 
'hgnc_symbols' : set ( ) , 
'hpo_terms' : set ( ) , 
~~~ diseases [ disease_id ] [ 'hgnc_symbols' ] . add ( hgnc_symbol ) 
~~ if hpo_term : 
~~~ diseases [ disease_id ] [ 'hpo_terms' ] . add ( hpo_term ) 
return diseases 
~~ def parse_hpo_to_genes ( hpo_lines ) : 
for line in hpo_lines : 
~~~ if line . startswith ( '#' ) or len ( line ) < 1 : 
hpo_id = line [ 0 ] 
hgnc_symbol = line [ 3 ] 
yield { 
'hpo_id' : hpo_id , 
'hgnc_symbol' : hgnc_symbol 
~~ ~~ def parse_hpo_genes ( hpo_lines ) : 
genes = { } 
~~ if len ( line ) < 5 : 
~~ gene_info = parse_hpo_gene ( line ) 
hgnc_symbol = gene_info [ 'hgnc_symbol' ] 
description = gene_info [ 'description' ] 
if hgnc_symbol not in genes : 
~~~ genes [ hgnc_symbol ] = { 
~~ gene = genes [ hgnc_symbol ] 
~~~ gene [ 'incomplete_penetrance' ] = True 
~~~ gene [ 'ad' ] = True 
~~~ gene [ 'ar' ] = True 
~~~ gene [ 'mt' ] = True 
~~~ gene [ 'xd' ] = True 
~~~ gene [ 'xr' ] = True 
~~~ gene [ 'x' ] = True 
~~~ gene [ 'y' ] = True 
return genes 
~~ def get_incomplete_penetrance_genes ( hpo_lines ) : 
genes = parse_hpo_genes ( hpo_lines ) 
incomplete_penetrance_genes = set ( ) 
for hgnc_symbol in genes : 
~~~ if genes [ hgnc_symbol ] . get ( 'incomplete_penetrance' ) : 
~~~ incomplete_penetrance_genes . add ( hgnc_symbol ) 
~~ ~~ return incomplete_penetrance_genes 
~~ def parse_hpo_obo ( hpo_lines ) : 
term = { } 
~~~ if len ( line ) == 0 : 
~~ line = line . rstrip ( ) 
if line == '[Term]' : 
~~~ if term : 
~~~ yield term 
~~ term = { } 
~~ elif line . startswith ( 'id' ) : 
~~~ term [ 'hpo_id' ] = line [ 4 : ] 
~~ elif line . startswith ( 'name' ) : 
~~~ term [ 'description' ] = line [ 6 : ] 
~~ elif line . startswith ( 'alt_id' ) : 
~~~ if 'aliases' not in term : 
~~~ term [ 'aliases' ] = [ ] 
~~ term [ 'aliases' ] . append ( line [ 8 : ] ) 
~~ elif line . startswith ( 'is_a' ) : 
~~~ if 'ancestors' not in term : 
~~~ term [ 'ancestors' ] = [ ] 
~~ term [ 'ancestors' ] . append ( line [ 6 : 16 ] ) 
~~ ~~ if term : 
~~ ~~ def genes ( ) : 
query = request . args . get ( 'query' , '' ) 
if '|' in query : 
return redirect ( url_for ( '.gene' , hgnc_id = hgnc_id ) ) 
~~ gene_q = store . all_genes ( ) . limit ( 20 ) 
return dict ( genes = gene_q ) 
~~ def gene ( hgnc_id = None , hgnc_symbol = None ) : 
~~~ query = store . hgnc_genes ( hgnc_symbol ) 
if query . count ( ) == 1 : 
~~~ hgnc_id = query . first ( ) [ 'hgnc_id' ] 
~~~ return redirect ( url_for ( '.genes' , query = hgnc_symbol ) ) 
~~ ~~ try : 
~~~ genes = controllers . gene ( store , hgnc_id ) 
~~ except ValueError as error : 
~~ def api_genes ( ) : 
query = request . args . get ( 'query' ) 
json_out = controllers . genes_to_json ( store , query ) 
return jsonify ( json_out ) 
~~ def check_panels ( adapter , panels , default_panels = None ) : 
default_panels = default_panels or [ ] 
panels_exist = True 
for panel in default_panels : 
~~~ if panel not in panels : 
panels_exist = False 
~~ ~~ for panel in panels : 
~~~ if not adapter . gene_panel ( panel ) : 
~~ ~~ return panels_exist 
~~ def load_region ( adapter , case_id , hgnc_id = None , chrom = None , start = None , end = None ) : 
~~ chrom = gene_obj [ 'chromosome' ] 
adapter . load_variants ( case_obj = case_obj , variant_type = 'clinical' , 
category = 'snv' , chrom = chrom , start = start , end = end ) 
vcf_sv_file = case_obj [ 'vcf_files' ] . get ( 'vcf_sv' ) 
if vcf_sv_file : 
category = 'sv' , chrom = chrom , start = start , end = end ) 
~~ vcf_str_file = case_obj [ 'vcf_files' ] . get ( 'vcf_str' ) 
if vcf_str_file : 
category = 'str' , chrom = chrom , start = start , end = end ) 
~~ if case_obj [ 'is_research' ] : 
adapter . load_variants ( case_obj = case_obj , variant_type = 'research' , 
vcf_sv_research = case_obj [ 'vcf_files' ] . get ( 'vcf_sv_research' ) 
if vcf_sv_research : 
~~ ~~ ~~ def load_scout ( adapter , config , ped = None , update = False ) : 
if not check_panels ( adapter , config . get ( 'gene_panels' , [ ] ) , 
config . get ( 'default_gene_panels' ) ) : 
~~ case_obj = adapter . load_case ( config , update = update ) 
return case_obj 
~~ def templated ( template = None ) : 
def decorator ( f ) : 
~~~ @ wraps ( f ) 
def decorated_function ( * args , ** kwargs ) : 
~~~ template_name = template 
if template_name is None : 
~~~ template_name = request . endpoint . replace ( '.' , '/' ) + '.html' 
~~ context = f ( * args , ** kwargs ) 
if context is None : 
~~~ context = { } 
~~ elif not isinstance ( context , dict ) : 
~~~ return context 
~~ return render_template ( template_name , ** context ) 
~~ return decorated_function 
~~ return decorator 
~~ def institute_and_case ( store , institute_id , case_name = None ) : 
if institute_obj is None and institute_id != 'favicon.ico' : 
~~~ flash ( "Can\ . format ( institute_id ) , 'warning' ) 
return abort ( 404 ) 
~~ if case_name : 
~~~ if case_name : 
~~~ case_obj = store . case ( institute_id = institute_id , display_name = case_name ) 
~~ ~~ ~~ if not current_user . is_admin : 
~~~ if institute_id not in current_user . institutes : 
~~~ if not case_name or not any ( inst_id in case_obj [ 'collaborators' ] for inst_id in 
current_user . institutes ) : 
return abort ( 403 ) 
~~ ~~ ~~ if case_name : 
~~~ return institute_obj , case_obj 
~~~ return institute_obj 
~~ ~~ def user_institutes ( store , login_user ) : 
if login_user . is_admin : 
~~~ institutes = store . institutes ( ) 
~~~ institutes = [ store . institute ( inst_id ) for inst_id in login_user . institutes ] 
~~ return institutes 
~~ def get_hgnc_id ( gene_info , adapter ) : 
hgnc_id = gene_info . get ( 'hgnc_id' ) 
hgnc_symbol = gene_info . get ( 'hgnc_symbol' ) 
true_id = None 
~~~ true_id = int ( hgnc_id ) 
~~~ gene_result = adapter . hgnc_genes ( hgnc_symbol ) 
if gene_result . count ( ) == 0 : 
~~ for gene in gene_result : 
~~~ if hgnc_symbol . upper ( ) == gene . hgnc_symbol . upper ( ) : 
~~~ true_id = gene . hgnc_id 
~~ ~~ if not gene_info [ 'hgnc_id' ] : 
~~ ~~ return true_id 
~~ def panel ( context , panel , version , update_date , update_version ) : 
panel_obj = adapter . gene_panel ( panel , version = version ) 
~~ date_obj = None 
if update_date : 
~~~ date_obj = get_date ( update_date ) 
~~ ~~ update_panel ( 
panel , 
panel_version = panel_obj [ 'version' ] , 
new_version = update_version , 
new_date = date_obj 
~~ def diseases ( context , api_key ) : 
~~~ mim_files = fetch_mim_files ( api_key , genemap2 = True ) 
adapter . disease_term_collection . drop ( ) 
load_disease_terms ( 
genemap_lines = mim_files [ 'genemap2' ] , 
~~ def load_hpo ( adapter , disease_lines , hpo_disease_lines = None , hpo_lines = None , hpo_gene_lines = None ) : 
alias_genes = adapter . genes_by_alias ( ) 
if not hpo_lines : 
~~~ hpo_lines = fetch_hpo_terms ( ) 
~~ if not hpo_gene_lines : 
~~~ hpo_gene_lines = fetch_hpo_to_genes ( ) 
~~ if not hpo_disease_lines : 
~~~ hpo_disease_lines = fetch_hpo_phenotype_to_terms ( ) 
~~ load_hpo_terms ( adapter , hpo_lines , hpo_gene_lines , alias_genes ) 
load_disease_terms ( adapter , disease_lines , alias_genes , hpo_disease_lines ) 
~~ def load_hpo_terms ( adapter , hpo_lines = None , hpo_gene_lines = None , alias_genes = None ) : 
for term in parse_hpo_obo ( hpo_lines ) : 
~~~ hpo_terms [ term [ 'hpo_id' ] ] = term 
~~ if not alias_genes : 
~~~ alias_genes = adapter . genes_by_alias ( ) 
for hpo_to_symbol in parse_hpo_to_genes ( hpo_gene_lines ) : 
~~~ hgnc_symbol = hpo_to_symbol [ 'hgnc_symbol' ] 
hpo_id = hpo_to_symbol [ 'hpo_id' ] 
gene_info = alias_genes . get ( hgnc_symbol ) 
if not gene_info : 
~~ hgnc_id = gene_info [ 'true' ] 
if hpo_id not in hpo_terms : 
~~ hpo_term = hpo_terms [ hpo_id ] 
if not 'genes' in hpo_term : 
~~~ hpo_term [ 'genes' ] = set ( ) 
~~ hpo_term [ 'genes' ] . add ( hgnc_id ) 
~~ start_time = datetime . now ( ) 
nr_terms = len ( hpo_terms ) 
hpo_bulk = [ ] 
~~~ for hpo_info in bar : 
~~~ hpo_bulk . append ( build_hpo_term ( hpo_info ) ) 
~~ if len ( hpo_bulk ) > 10000 : 
~~~ adapter . load_hpo_bulk ( hpo_bulk ) 
~~ ~~ if hpo_bulk : 
~~ def load_disease_terms ( adapter , genemap_lines , genes = None , hpo_disease_lines = None ) : 
~~~ genes = adapter . genes_by_alias ( ) 
~~ disease_terms = get_mim_phenotypes ( genemap_lines = genemap_lines ) 
if not hpo_disease_lines : 
~~ hpo_diseases = parse_hpo_diseases ( hpo_disease_lines ) 
start_time = datetime . now ( ) 
nr_diseases = None 
for nr_diseases , disease_number in enumerate ( disease_terms ) : 
~~~ disease_info = disease_terms [ disease_number ] 
disease_id = "OMIM:{0}" . format ( disease_number ) 
if disease_id in hpo_diseases : 
~~~ hpo_terms = hpo_diseases [ disease_id ] [ 'hpo_terms' ] 
if hpo_terms : 
~~~ disease_info [ 'hpo_terms' ] = hpo_terms 
~~ ~~ disease_obj = build_disease_term ( disease_info , genes ) 
adapter . load_disease_term ( disease_obj ) 
~~ def parse_frequencies ( variant , transcripts ) : 
frequencies = { } 
thousand_genomes_keys = [ '1000GAF' ] 
thousand_genomes_max_keys = [ '1000G_MAX_AF' ] 
exac_keys = [ 'EXACAF' ] 
exac_max_keys = [ 'ExAC_MAX_AF' , 'EXAC_MAX_AF' ] 
gnomad_keys = [ 'GNOMADAF' , 'GNOMAD_AF' ] 
gnomad_max_keys = [ 'GNOMADAF_POPMAX' , 'GNOMADAF_MAX' ] 
for test_key in thousand_genomes_keys : 
~~~ thousand_g = parse_frequency ( variant , test_key ) 
if thousand_g : 
~~~ frequencies [ 'thousand_g' ] = thousand_g 
break 
~~ ~~ for test_key in thousand_genomes_max_keys : 
~~~ thousand_g_max = parse_frequency ( variant , test_key ) 
if thousand_g_max : 
~~~ frequencies [ 'thousand_g_max' ] = thousand_g_max 
~~ ~~ for test_key in exac_keys : 
~~~ exac = parse_frequency ( variant , test_key ) 
if exac : 
~~~ frequencies [ 'exac' ] = exac 
~~ ~~ for test_key in exac_max_keys : 
~~~ exac_max = parse_frequency ( variant , test_key ) 
if exac_max : 
~~~ frequencies [ 'exac_max' ] = exac_max 
~~ ~~ for test_key in gnomad_keys : 
~~~ gnomad = parse_frequency ( variant , test_key ) 
if gnomad : 
~~~ frequencies [ 'gnomad' ] = gnomad 
~~ ~~ for test_key in gnomad_max_keys : 
~~~ gnomad_max = parse_frequency ( variant , test_key ) 
if gnomad_max : 
~~~ frequencies [ 'gnomad_max' ] = gnomad_max 
~~ ~~ if not frequencies : 
~~~ for transcript in transcripts : 
~~~ exac = transcript . get ( 'exac_maf' ) 
exac_max = transcript . get ( 'exac_max' ) 
thousand_g = transcript . get ( 'thousand_g_maf' ) 
thousandg_max = transcript . get ( 'thousandg_max' ) 
gnomad = transcript . get ( 'gnomad_maf' ) 
gnomad_max = transcript . get ( 'gnomad_max' ) 
~~ if exac_max : 
~~ if thousand_g : 
~~ if thousandg_max : 
~~~ frequencies [ 'thousand_g_max' ] = thousandg_max 
~~ if gnomad : 
~~ if gnomad_max : 
~~ ~~ ~~ thousand_g_left = parse_frequency ( variant , 'left_1000GAF' ) 
if thousand_g_left : 
~~~ frequencies [ 'thousand_g_left' ] = thousand_g_left 
~~ thousand_g_right = parse_frequency ( variant , 'right_1000GAF' ) 
if thousand_g_right : 
~~~ frequencies [ 'thousand_g_right' ] = thousand_g_right 
~~ return frequencies 
~~ def parse_frequency ( variant , info_key ) : 
raw_annotation = variant . INFO . get ( info_key ) 
raw_annotation = None if raw_annotation == '.' else raw_annotation 
frequency = float ( raw_annotation ) if raw_annotation else None 
return frequency 
~~ def parse_sv_frequencies ( variant ) : 
frequency_keys = [ 
'clingen_cgh_benignAF' , 
'clingen_cgh_benign' , 
'clingen_cgh_pathogenicAF' , 
'clingen_cgh_pathogenic' , 
'clingen_ngi' , 
'clingen_ngiAF' , 
'swegen' , 
'swegenAF' , 
'decipherAF' , 
'decipher' 
sv_frequencies = { } 
for key in frequency_keys : 
~~~ value = variant . INFO . get ( key , 0 ) 
if 'AF' in key : 
~~~ value = float ( value ) 
~~~ value = int ( value ) 
~~ if value > 0 : 
~~~ sv_frequencies [ key ] = value 
~~ ~~ return sv_frequencies 
~~ def users ( context ) : 
user_objs = adapter . users ( ) 
if user_objs . count ( ) == 0 : 
~~ click . echo ( "#name\\temail\\troles\\tinstitutes" ) 
for user_obj in user_objs : 
~~~ click . echo ( "{0}\\t{1}\\t{2}\\t{3}\\t" . format ( 
user_obj [ 'name' ] , 
user_obj . get ( 'mail' , user_obj [ '_id' ] ) , 
~~ ~~ def load_case ( adapter , case_obj , update = False ) : 
existing_case = adapter . case ( case_obj [ '_id' ] ) 
if existing_case : 
~~~ adapter . update_case ( case_obj ) 
~~~ adapter . add_case ( case_obj ) 
~~ return case_obj 
~~ def build_hgnc_gene ( gene_info , build = '37' ) : 
~~~ hgnc_id = int ( gene_info [ 'hgnc_id' ] ) 
~~~ hgnc_symbol = gene_info [ 'hgnc_symbol' ] 
~~~ ensembl_id = gene_info [ 'ensembl_gene_id' ] 
~~~ chromosome = gene_info [ 'chromosome' ] 
~~~ start = int ( gene_info [ 'start' ] ) 
~~ except TypeError as err : 
~~~ end = int ( gene_info [ 'end' ] ) 
~~ gene_obj = HgncGene ( 
hgnc_symbol = hgnc_symbol , 
ensembl_id = ensembl_id , 
chrom = chromosome , 
if gene_info . get ( 'description' ) : 
~~~ gene_obj [ 'description' ] = gene_info [ 'description' ] 
~~ if gene_info . get ( 'previous_symbols' ) : 
~~~ gene_obj [ 'aliases' ] = gene_info [ 'previous_symbols' ] 
~~ if gene_info . get ( 'entrez_id' ) : 
~~~ gene_obj [ 'entrez_id' ] = int ( gene_info [ 'entrez_id' ] ) 
~~ if gene_info . get ( 'omim_id' ) : 
~~~ gene_obj [ 'omim_id' ] = int ( gene_info [ 'omim_id' ] ) 
~~ if gene_info . get ( 'pli_score' ) : 
~~~ gene_obj [ 'pli_score' ] = float ( gene_info [ 'pli_score' ] ) 
~~ if gene_info . get ( 'ref_seq' ) : 
~~~ gene_obj [ 'primary_transcripts' ] = gene_info [ 'ref_seq' ] 
~~ if gene_info . get ( 'ucsc_id' ) : 
~~~ gene_obj [ 'ucsc_id' ] = gene_info [ 'ucsc_id' ] 
~~ if gene_info . get ( 'uniprot_ids' ) : 
~~~ gene_obj [ 'uniprot_ids' ] = gene_info [ 'uniprot_ids' ] 
~~ if gene_info . get ( 'vega_id' ) : 
~~~ gene_obj [ 'vega_id' ] = gene_info [ 'vega_id' ] 
~~ if gene_info . get ( 'incomplete_penetrance' ) : 
~~~ gene_obj [ 'incomplete_penetrance' ] = True 
~~~ gene_obj [ 'inheritance_models' ] = gene_info [ 'inheritance_models' ] 
~~ phenotype_objs = [ ] 
for phenotype_info in gene_info . get ( 'phenotypes' , [ ] ) : 
~~~ phenotype_objs . append ( build_phenotype ( phenotype_info ) ) 
~~ if phenotype_objs : 
~~~ gene_obj [ 'phenotypes' ] = phenotype_objs 
~~ for key in list ( gene_obj ) : 
~~~ if gene_obj [ key ] is None : 
~~~ gene_obj . pop ( key ) 
~~ ~~ return gene_obj 
~~ def load_panel ( self , parsed_panel ) : 
panel_obj = build_panel ( parsed_panel , self ) 
self . add_gene_panel ( panel_obj ) 
~~ def load_omim_panel ( self , api_key , institute = None ) : 
existing_panel = self . gene_panel ( panel_id = 'OMIM-AUTO' ) 
if not existing_panel : 
version = 1.0 
~~ if existing_panel : 
~~~ version = float ( math . floor ( existing_panel [ 'version' ] ) + 1 ) 
~~~ mim_files = fetch_mim_files ( api_key = api_key , genemap2 = True , mim2genes = True ) 
~~~ raise err 
~~ date_string = None 
for line in mim_files [ 'genemap2' ] : 
~~~ if 'Generated' in line : 
~~~ date_string = line . split ( ':' ) [ - 1 ] . lstrip ( ) . rstrip ( ) 
~~ ~~ date_obj = get_date ( date_string ) 
if existing_panel : 
~~~ if existing_panel [ 'date' ] == date_obj : 
~~ ~~ panel_data = { } 
panel_data [ 'path' ] = None 
panel_data [ 'type' ] = 'clinical' 
panel_data [ 'date' ] = date_obj 
panel_data [ 'panel_id' ] = 'OMIM-AUTO' 
panel_data [ 'institute' ] = institute or 'cust002' 
panel_data [ 'version' ] = version 
panel_data [ 'display_name' ] = 'OMIM-AUTO' 
panel_data [ 'genes' ] = [ ] 
alias_genes = self . genes_by_alias ( ) 
genes = get_omim_panel_genes ( 
genemap2_lines = mim_files [ 'genemap2' ] , 
mim2gene_lines = mim_files [ 'mim2genes' ] , 
alias_genes = alias_genes , 
for gene in genes : 
~~~ panel_data [ 'genes' ] . append ( gene ) 
~~ panel_obj = build_panel ( panel_data , self ) 
~~~ new_genes = self . compare_mim_panels ( existing_panel , panel_obj ) 
if new_genes : 
~~~ self . update_mim_version ( new_genes , panel_obj , old_version = existing_panel [ 'version' ] ) 
~~ ~~ self . add_gene_panel ( panel_obj ) 
~~ def compare_mim_panels ( self , existing_panel , new_panel ) : 
existing_genes = set ( [ gene [ 'hgnc_id' ] for gene in existing_panel [ 'genes' ] ] ) 
new_genes = set ( [ gene [ 'hgnc_id' ] for gene in new_panel [ 'genes' ] ] ) 
return new_genes . difference ( existing_genes ) 
~~ def update_mim_version ( self , new_genes , new_panel , old_version ) : 
version = new_panel [ 'version' ] 
for gene in new_panel [ 'genes' ] : 
~~~ gene_symbol = gene [ 'hgnc_id' ] 
if gene_symbol in new_genes : 
~~~ gene [ 'database_entry_version' ] = version 
~~ gene [ 'database_entry_version' ] = old_version 
~~ return 
~~ def add_gene_panel ( self , panel_obj ) : 
panel_name = panel_obj [ 'panel_name' ] 
panel_version = panel_obj [ 'version' ] 
display_name = panel_obj . get ( 'display_name' , panel_name ) 
if self . gene_panel ( panel_name , panel_version ) : 
display_name , panel_version 
result = self . panel_collection . insert_one ( panel_obj ) 
~~ def panel ( self , panel_id ) : 
if not isinstance ( panel_id , ObjectId ) : 
~~~ panel_id = ObjectId ( panel_id ) 
~~ panel_obj = self . panel_collection . find_one ( { '_id' : panel_id } ) 
~~ def delete_panel ( self , panel_obj ) : 
res = self . panel_collection . delete_one ( { '_id' : panel_obj [ '_id' ] } ) 
~~ def gene_panel ( self , panel_id , version = None ) : 
query = { 'panel_name' : panel_id } 
if version : 
panel_id , version 
query [ 'version' ] = version 
return self . panel_collection . find_one ( query ) 
res = self . panel_collection . find ( query ) . sort ( 'version' , - 1 ) 
if res . count ( ) > 0 : 
~~~ return res [ 0 ] 
~~ ~~ ~~ def gene_panels ( self , panel_id = None , institute_id = None , version = None ) : 
if panel_id : 
~~~ query [ 'panel_name' ] = panel_id 
~~~ query [ 'version' ] = version 
~~ ~~ if institute_id : 
~~~ query [ 'institute' ] = institute_id 
~~ return self . panel_collection . find ( query ) 
~~ def gene_to_panels ( self , case_obj ) : 
gene_dict = { } 
for panel_info in case_obj . get ( 'panels' , [ ] ) : 
~~~ panel_name = panel_info [ 'panel_name' ] 
panel_version = panel_info [ 'version' ] 
panel_obj = self . gene_panel ( panel_name , version = panel_version ) 
~~ for gene in panel_obj [ 'genes' ] : 
~~~ hgnc_id = gene [ 'hgnc_id' ] 
if hgnc_id not in gene_dict : 
~~~ gene_dict [ hgnc_id ] = set ( [ panel_name ] ) 
~~ gene_dict [ hgnc_id ] . add ( panel_name ) 
return gene_dict 
~~ def update_panel ( self , panel_obj , version = None , date_obj = None ) : 
date = panel_obj [ 'date' ] 
panel_obj [ 'version' ] , version ) ) 
panel_obj [ 'version' ] = version 
if date_obj : 
~~~ date = date_obj 
~~~ date = date_obj or dt . datetime . now ( ) 
~~ panel_obj [ 'date' ] = date 
updated_panel = self . panel_collection . find_one_and_replace ( 
{ '_id' : panel_obj [ '_id' ] } , 
panel_obj , 
return updated_panel 
~~ def add_pending ( self , panel_obj , hgnc_gene , action , info = None ) : 
valid_actions = [ 'add' , 'delete' , 'edit' ] 
if action not in valid_actions : 
~~ info = info or { } 
pending_action = { 
'hgnc_id' : hgnc_gene [ 'hgnc_id' ] , 
'action' : action , 
'info' : info , 
'symbol' : hgnc_gene [ 'hgnc_symbol' ] , 
updated_panel = self . panel_collection . find_one_and_update ( 
'pending' : pending_action 
~~ def apply_pending ( self , panel_obj , version ) : 
updates = { } 
new_panel = deepcopy ( panel_obj ) 
new_panel [ 'pending' ] = [ ] 
new_panel [ 'date' ] = dt . datetime . now ( ) 
info_fields = [ 'disease_associated_transcripts' , 'inheritance_models' , 'reduced_penetrance' , 
'mosaicism' , 'database_entry_version' , 'comment' ] 
for update in panel_obj . get ( 'pending' , [ ] ) : 
~~~ hgnc_id = update [ 'hgnc_id' ] 
if update [ 'action' ] != 'add' : 
~~~ updates [ hgnc_id ] = update 
~~ info = update . get ( 'info' , { } ) 
gene_obj = { 
'symbol' : update [ 'symbol' ] 
for field in info_fields : 
~~~ if field in info : 
~~~ gene_obj [ field ] = info [ field ] 
~~ ~~ new_genes . append ( gene_obj ) 
if hgnc_id not in updates : 
~~~ new_genes . append ( gene ) 
~~ current_update = updates [ hgnc_id ] 
action = current_update [ 'action' ] 
info = current_update [ 'info' ] 
if action == 'delete' : 
~~ elif action == 'edit' : 
~~~ for field in info_fields : 
~~~ gene [ field ] = info [ field ] 
~~ ~~ new_genes . append ( gene ) 
~~ ~~ new_panel [ 'genes' ] = new_genes 
new_panel [ 'version' ] = float ( version ) 
inserted_id = None 
if new_panel [ 'version' ] == panel_obj [ 'version' ] : 
~~~ result = self . panel_collection . find_one_and_replace ( 
new_panel , 
inserted_id = result [ '_id' ] 
~~~ new_panel . pop ( '_id' ) 
panel_obj [ 'is_archived' ] = True 
self . update_panel ( panel_obj = panel_obj , date_obj = panel_obj [ 'date' ] ) 
inserted_id = self . panel_collection . insert_one ( new_panel ) . inserted_id 
~~ return inserted_id 
~~ def clinical_symbols ( self , case_obj ) : 
panel_ids = [ panel [ 'panel_id' ] for panel in case_obj [ 'panels' ] ] 
query = self . panel_collection . aggregate ( [ 
{ '$match' : { '_id' : { '$in' : panel_ids } } } , 
{ '$unwind' : '$genes' } , 
{ '$group' : { '_id' : '$genes.symbol' } } 
return set ( item [ '_id' ] for item in query ) 
~~ def cases ( context , case_id , institute , reruns , finished , causatives , research_requested , 
is_research , status , json ) : 
~~~ models = adapter . cases ( collaborator = institute , reruns = reruns , 
finished = finished , has_causatives = causatives , 
research_requested = research_requested , 
is_research = is_research , status = status ) 
if len ( models ) == 0 : 
~~ ~~ if json : 
~~~ click . echo ( dumps ( models ) ) 
~~ for model in models : 
~~~ pp ( model ) 
~~ ~~ def emit ( self , record ) : 
~~~ import smtplib 
~~~ from email . utils import formatdate 
~~ except ImportError : 
~~~ formatdate = self . date_time 
~~ port = self . mailport 
if not port : 
~~~ port = smtplib . SMTP_PORT 
~~ smtp = smtplib . SMTP ( self . mailhost , port ) 
msg = self . format ( record ) 
self . fromaddr , 
',' . join ( self . toaddrs ) , 
self . getSubject ( record ) , 
formatdate ( ) , msg 
if self . username : 
smtp . login ( self . username , self . password ) 
~~ smtp . sendmail ( self . fromaddr , self . toaddrs , msg ) 
smtp . quit ( ) 
~~ except ( KeyboardInterrupt , SystemExit ) : 
~~~ raise 
~~ except : 
~~~ self . handleError ( record ) 
~~ ~~ def indexes ( self , collection = None ) : 
indexes = [ ] 
for collection_name in self . collections ( ) : 
~~~ if collection and collection != collection_name : 
~~ for index_name in self . db [ collection_name ] . index_information ( ) : 
~~~ if index_name != '_id_' : 
~~~ indexes . append ( index_name ) 
~~ ~~ ~~ return indexes 
~~ def load_indexes ( self ) : 
for collection_name in INDEXES : 
~~~ existing_indexes = self . indexes ( collection_name ) 
indexes = INDEXES [ collection_name ] 
for index in indexes : 
~~~ index_name = index . document . get ( 'name' ) 
if index_name in existing_indexes : 
self . db [ collection_name ] . drop_index ( index_name ) 
collection_name , 
self . db [ collection_name ] . create_indexes ( indexes ) 
~~ ~~ def update_indexes ( self ) : 
nr_updated = 0 
if index_name not in existing_indexes : 
~~~ nr_updated += 1 
~~ ~~ ~~ if nr_updated == 0 : 
~~ ~~ def drop_indexes ( self ) : 
self . db [ collection_name ] . drop_indexes ( ) 
~~ ~~ def build_variant_query ( self , query = None , category = 'snv' , variant_type = [ 'clinical' ] ) : 
query = query or { } 
mongo_variant_query = { } 
if query . get ( 'hgnc_symbols' ) : 
~~~ mongo_variant_query [ 'hgnc_symbols' ] = { '$in' : query [ 'hgnc_symbols' ] } 
~~ mongo_variant_query [ 'variant_type' ] = { '$in' : variant_type } 
mongo_variant_query [ 'category' ] = category 
rank_score = query . get ( 'rank_score' ) or 15 
mongo_variant_query [ 'rank_score' ] = { '$gte' : rank_score } 
return mongo_variant_query 
~~ def build_query ( self , case_id , query = None , variant_ids = None , category = 'snv' ) : 
mongo_query = { } 
gene_query = None 
for criterion in FUNDAMENTAL_CRITERIA : 
~~~ if criterion == 'case_id' : 
mongo_query [ 'case_id' ] = case_id 
~~ elif criterion == 'variant_ids' and variant_ids : 
mongo_query [ 'variant_id' ] = { '$in' : variant_ids } 
~~ elif criterion == 'category' : 
mongo_query [ 'category' ] = category 
~~ elif criterion == 'variant_type' : 
~~~ mongo_query [ 'variant_type' ] = query . get ( 'variant_type' , 'clinical' ) 
~~ elif criterion in [ 'hgnc_symbols' , 'gene_panels' ] and gene_query is None : 
~~~ gene_query = self . gene_filter ( query , mongo_query ) 
~~~ self . coordinate_filter ( query , mongo_query ) 
~~ ~~ primary_terms = False 
secondary_terms = False 
for term in PRIMARY_CRITERIA : 
~~~ if query . get ( term ) : 
~~~ primary_terms = True 
~~ ~~ for term in SECONDARY_CRITERIA : 
~~~ secondary_terms = True 
~~ ~~ if primary_terms is True : 
~~~ clinsign_filter = self . clinsig_query ( query , mongo_query ) 
~~ if secondary_terms is True : 
~~~ secondary_filter = self . secondary_query ( query , mongo_query ) 
if primary_terms is False : 
~~~ if gene_query : 
~~~ mongo_query [ '$and' ] = [ { '$or' : gene_query } , { '$and' : secondary_filter } ] 
~~~ mongo_query [ '$and' ] = secondary_filter 
~~~ if query . get ( 'clinsig_confident_always_returned' ) == True : 
~~~ mongo_query [ '$and' ] = [ 
{ '$or' : gene_query } , 
{ '$and' : secondary_filter } , clinsign_filter 
~~~ mongo_query [ '$or' ] = [ { '$and' : secondary_filter } , clinsign_filter ] 
~~~ secondary_filter . append ( clinsign_filter ) 
if gene_query : 
~~~ mongo_query [ 'clnsig' ] = clinsign_filter [ 'clnsig' ] 
~~~ mongo_query [ '$and' ] = [ { '$or' : gene_query } ] 
return mongo_query 
~~ def clinsig_query ( self , query , mongo_query ) : 
trusted_revision_level = [ 'mult' , 'single' , 'exp' , 'guideline' ] 
rank = [ ] 
str_rank = [ ] 
clnsig_query = { } 
for item in query [ 'clinsig' ] : 
~~~ rank . append ( int ( item ) ) 
rank . append ( CLINSIG_MAP [ int ( item ) ] ) 
str_rank . append ( CLINSIG_MAP [ int ( item ) ] ) 
~~ if query . get ( 'clinsig_confident_always_returned' ) == True : 
clnsig_query = { "clnsig" : 
'$elemMatch' : { 
{ 'value' : { '$in' : rank } } , 
{ 'revstat' : { '$in' : trusted_revision_level } } 
{ 'value' : re . compile ( '|' . join ( str_rank ) ) } , 
{ 'revstat' : re . compile ( '|' . join ( trusted_revision_level ) ) } 
clnsig_query = { 
"clnsig" : 
{ 'value' : re . compile ( '|' . join ( str_rank ) ) } 
~~ return clnsig_query 
~~ def coordinate_filter ( self , query , mongo_query ) : 
chromosome = query [ 'chrom' ] 
mongo_query [ 'chromosome' ] = chromosome 
if ( query . get ( 'start' ) and query . get ( 'end' ) ) : 
~~~ mongo_query [ 'position' ] = { '$lte' : int ( query [ 'end' ] ) } 
mongo_query [ 'end' ] = { '$gte' : int ( query [ 'start' ] ) } 
~~ return mongo_query 
~~ def gene_filter ( self , query , mongo_query ) : 
gene_query = [ ] 
if query . get ( 'hgnc_symbols' ) and query . get ( 'gene_panels' ) : 
~~~ gene_query . append ( { 'hgnc_symbols' : { '$in' : query [ 'hgnc_symbols' ] } } ) 
gene_query . append ( { 'panels' : { '$in' : query [ 'gene_panels' ] } } ) 
mongo_query [ '$or' ] = gene_query 
~~~ if query . get ( 'hgnc_symbols' ) : 
~~~ hgnc_symbols = query [ 'hgnc_symbols' ] 
mongo_query [ 'hgnc_symbols' ] = { '$in' : hgnc_symbols } 
~~ if query . get ( 'gene_panels' ) : 
~~~ gene_panels = query [ 'gene_panels' ] 
mongo_query [ 'panels' ] = { '$in' : gene_panels } 
~~ ~~ return gene_query 
~~ def secondary_query ( self , query , mongo_query , secondary_filter = None ) : 
mongo_secondary_query = [ ] 
for criterion in SECONDARY_CRITERIA : 
~~~ if not query . get ( criterion ) : 
~~ if criterion == 'gnomad_frequency' : 
~~~ gnomad = query . get ( 'gnomad_frequency' ) 
if gnomad == '-1' : 
~~~ mongo_query [ 'gnomad_frequency' ] = { '$exists' : False } 
~~~ mongo_secondary_query . append ( 
'gnomad_frequency' : { '$lt' : float ( gnomad ) } 
'gnomad_frequency' : { '$exists' : False } 
~~ if criterion == 'local_obs' : 
~~~ local_obs = query . get ( 'local_obs' ) 
mongo_secondary_query . append ( { 
{ 'local_obs_old' : None } , 
{ 'local_obs_old' : { '$lt' : local_obs + 1 } } , 
~~ if criterion in [ 'clingen_ngi' , 'swegen' ] : 
~~~ mongo_secondary_query . append ( { 
{ criterion : { '$exists' : False } } , 
{ criterion : { '$lt' : query [ criterion ] + 1 } } , 
~~ if criterion == 'spidex_human' : 
~~~ spidex_human = query [ 'spidex_human' ] 
spidex_query_or_part = [ ] 
if ( 'not_reported' in spidex_human ) : 
~~~ spidex_query_or_part . append ( { 'spidex' : { '$exists' : False } } ) 
~~ for spidex_level in SPIDEX_HUMAN : 
~~~ if ( spidex_level in spidex_human ) : 
~~~ spidex_query_or_part . append ( { '$or' : [ 
{ '$and' : [ { 'spidex' : { '$gt' : SPIDEX_HUMAN [ spidex_level ] [ 'neg' ] [ 0 ] } } , 
{ 'spidex' : { '$lt' : SPIDEX_HUMAN [ spidex_level ] [ 'neg' ] [ 1 ] } } ] } , 
{ '$and' : [ { 'spidex' : { '$gt' : SPIDEX_HUMAN [ spidex_level ] [ 'pos' ] [ 0 ] } } , 
{ 'spidex' : { '$lt' : SPIDEX_HUMAN [ spidex_level ] [ 'pos' ] [ 1 ] } } ] } ] } ) 
~~ ~~ mongo_secondary_query . append ( { '$or' : spidex_query_or_part } ) 
~~ if criterion == 'cadd_score' : 
~~~ cadd = query [ 'cadd_score' ] 
cadd_query = { 'cadd_score' : { '$gt' : float ( cadd ) } } 
if query . get ( 'cadd_inclusive' ) is True : 
~~~ cadd_query = { 
cadd_query , 
{ 'cadd_score' : { '$exists' : False } } 
] } 
~~ mongo_secondary_query . append ( cadd_query ) 
~~ if criterion in [ 'genetic_models' , 'functional_annotations' , 'region_annotations' ] : 
~~~ criterion_values = query [ criterion ] 
if criterion == 'genetic_models' : 
~~~ mongo_secondary_query . append ( { criterion : { '$in' : criterion_values } } ) 
~~~ mongo_secondary_query . append ( { '.' . join ( [ 'genes' , criterion [ : - 1 ] ] ) : { '$in' : criterion_values } } ) 
~~ if criterion == 'size' : 
~~~ size = query [ 'size' ] 
size_query = { 'length' : { '$gt' : int ( size ) } } 
if query . get ( 'size_shorter' ) : 
~~~ size_query = { 
{ 'length' : { '$lt' : int ( size ) } } , 
{ 'length' : { '$exists' : False } } 
~~ mongo_secondary_query . append ( size_query ) 
~~ if criterion == 'svtype' : 
~~~ svtype = query [ 'svtype' ] 
mongo_secondary_query . append ( { 'sub_category' : { '$in' : svtype } } ) 
~~ if criterion == 'decipher' : 
~~~ mongo_query [ 'decipher' ] = { '$exists' : True } 
~~ if criterion == 'depth' : 
'tumor.read_depth' : { 
'$gt' : query . get ( 'depth' ) , 
~~ if criterion == 'alt_count' : 
'tumor.alt_depth' : { 
'$gt' : query . get ( 'alt_count' ) , 
~~ if criterion == 'control_frequency' : 
'normal.alt_freq' : { 
'$lt' : float ( query . get ( 'control_frequency' ) ) , 
~~ if criterion == 'mvl_tag' : 
'mvl_tag' : { 
'$exists' : True , 
~~ ~~ return mongo_secondary_query 
~~ def wipe ( ctx ) : 
db_name = ctx . obj [ 'mongodb' ] 
~~~ ctx . obj [ 'client' ] . drop_database ( db_name ) 
~~ def parse_panel ( csv_stream ) : 
reader = csv . DictReader ( csv_stream , delimiter = ';' , quoting = csv . QUOTE_NONE ) 
for gene_row in reader : 
~~~ if not gene_row [ 'HGNC_IDnumber' ] . strip ( ) . isdigit ( ) : 
~~ transcripts_raw = gene_row . get ( 'Disease_associated_transcript' ) 
if transcripts_raw : 
~~~ transcripts_list = [ tx . split ( ':' , 1 ) [ - 1 ] . strip ( ) for tx in transcripts_raw . split ( ',' ) ] 
~~~ transcripts_list = [ ] 
~~ models_raw = gene_row . get ( 'Genetic_disease_model' ) 
models_list = [ model . strip ( ) for model in models_raw . split ( ',' ) ] if models_raw else [ ] 
panel_gene = dict ( 
symbol = gene_row [ 'HGNC_symbol' ] . strip ( ) if gene_row . get ( 'HGNC_symbol' ) else None , 
hgnc_id = int ( gene_row [ 'HGNC_IDnumber' ] . strip ( ) ) , 
disease_associated_transcripts = transcripts_list , 
reduced_penetrance = True if gene_row . get ( 'Reduced_penetrance' ) else None , 
mosaicism = True if gene_row . get ( 'Mosaicism' ) else None , 
inheritance_models = models_list , 
database_entry_version = gene_row . get ( 'Database_entry_version' ) , 
genes . append ( panel_gene ) 
~~ def build_clnsig ( clnsig_info ) : 
clnsig_obj = dict ( 
value = clnsig_info [ 'value' ] , 
accession = clnsig_info . get ( 'accession' ) , 
revstat = clnsig_info . get ( 'revstat' ) 
return clnsig_obj 
~~ def load_hgnc_bulk ( self , gene_objs ) : 
~~~ result = self . hgnc_collection . insert_many ( gene_objs ) 
~~ except ( DuplicateKeyError , BulkWriteError ) as err : 
~~~ raise IntegrityError ( err ) 
~~ return result 
~~ def load_transcript_bulk ( self , transcript_objs ) : 
~~~ result = self . transcript_collection . insert_many ( transcript_objs ) 
~~ def load_exon_bulk ( self , exon_objs ) : 
~~~ result = self . exon_collection . insert_many ( transcript_objs ) 
~~ def hgnc_gene ( self , hgnc_identifier , build = '37' ) : 
~~~ build = '37' 
~~ query = { } 
~~~ hgnc_identifier = int ( hgnc_identifier ) 
query [ 'hgnc_id' ] = hgnc_identifier 
~~~ query [ 'hgnc_symbol' ] = hgnc_identifier 
~~ query [ 'build' ] = build 
gene_obj = self . hgnc_collection . find_one ( query ) 
~~ transcripts = [ ] 
tx_objs = self . transcripts ( build = build , hgnc_id = gene_obj [ 'hgnc_id' ] ) 
if tx_objs . count ( ) > 0 : 
~~~ for tx in tx_objs : 
~~~ transcripts . append ( tx ) 
~~ ~~ gene_obj [ 'transcripts' ] = transcripts 
~~ def hgnc_id ( self , hgnc_symbol , build = '37' ) : 
query = { 'hgnc_symbol' : hgnc_symbol , 'build' : build } 
projection = { 'hgnc_id' : 1 , '_id' : 0 } 
res = self . hgnc_collection . find ( query , projection ) 
~~~ return res [ 0 ] [ 'hgnc_id' ] 
~~ ~~ def hgnc_genes ( self , hgnc_symbol , build = '37' , search = False ) : 
if search : 
~~~ full_query = self . hgnc_collection . find ( { 
{ 'aliases' : hgnc_symbol } , 
{ 'hgnc_id' : int ( hgnc_symbol ) if hgnc_symbol . isdigit ( ) else None } , 
'build' : build 
if full_query . count ( ) != 0 : 
~~~ return full_query 
~~ return self . hgnc_collection . find ( { 
'aliases' : { '$regex' : hgnc_symbol , '$options' : 'i' } , 
~~ return self . hgnc_collection . find ( { 'build' : build , 'aliases' : hgnc_symbol } ) 
~~ def all_genes ( self , build = '37' ) : 
return self . hgnc_collection . find ( { 'build' : build } ) . sort ( 'chromosome' , 1 ) 
~~ def nr_genes ( self , build = None ) : 
~~ return self . hgnc_collection . find ( { 'build' : build } ) . count ( ) 
~~ def drop_genes ( self , build = None ) : 
self . hgnc_collection . delete_many ( { 'build' : build } ) 
self . hgnc_collection . drop ( ) 
~~ ~~ def drop_transcripts ( self , build = None ) : 
self . transcript_collection . delete_many ( { 'build' : build } ) 
self . transcript_collection . drop ( ) 
~~ ~~ def drop_exons ( self , build = None ) : 
self . exon_collection . delete_many ( { 'build' : build } ) 
self . exon_collection . drop ( ) 
~~ ~~ def ensembl_transcripts ( self , build = '37' ) : 
ensembl_transcripts = { } 
for transcript_obj in self . transcript_collection . find ( { 'build' : build } ) : 
~~~ enst_id = transcript_obj [ 'transcript_id' ] 
ensembl_transcripts [ enst_id ] = transcript_obj 
return ensembl_transcripts 
~~ def hgncsymbol_to_gene ( self , build = '37' , genes = None ) : 
hgnc_dict = { } 
~~~ genes = self . hgnc_collection . find ( { 'build' : build } ) 
~~ for gene_obj in genes : 
~~~ hgnc_dict [ gene_obj [ 'hgnc_symbol' ] ] = gene_obj 
return hgnc_dict 
~~ def gene_by_alias ( self , symbol , build = '37' ) : 
res = self . hgnc_collection . find ( { 'hgnc_symbol' : symbol , 'build' : build } ) 
if res . count ( ) == 0 : 
~~~ res = self . hgnc_collection . find ( { 'aliases' : symbol , 'build' : build } ) 
~~ def genes_by_alias ( self , build = '37' , genes = None ) : 
alias_genes = { } 
~~ for gene in genes : 
hgnc_symbol = gene [ 'hgnc_symbol' ] 
for alias in gene [ 'aliases' ] : 
~~ ~~ ~~ return alias_genes 
~~ def get_id_transcripts ( self , hgnc_id , build = '37' ) : 
transcripts = self . transcripts ( build = build , hgnc_id = hgnc_id ) 
identifier_transcripts = set ( ) 
longest = None 
nr = [ ] 
xm = [ ] 
for tx in transcripts : 
~~~ enst_id = tx [ 'transcript_id' ] 
if not longest : 
~~~ longest = enst_id 
~~ refseq_id = tx . get ( 'refseq_id' ) 
if not refseq_id : 
~~ if 'NM' in refseq_id : 
~~~ identifier_transcripts . add ( enst_id ) 
~~ elif 'NR' in refseq_id : 
~~~ nr . append ( enst_id ) 
~~ elif 'XM' in refseq_id : 
~~~ xm . append ( enst_id ) 
~~ ~~ if identifier_transcripts : 
~~~ return identifier_transcripts 
~~ if nr : 
~~~ return set ( [ nr [ 0 ] ] ) 
~~ if xm : 
~~~ return set ( [ xm [ 0 ] ] ) 
~~ return set ( [ longest ] ) 
~~ def transcripts_by_gene ( self , build = '37' ) : 
hgnc_transcripts = { } 
for transcript in self . transcript_collection . find ( { 'build' : build } ) : 
if not hgnc_id in hgnc_transcripts : 
~~~ hgnc_transcripts [ hgnc_id ] = [ ] 
~~ hgnc_transcripts [ hgnc_id ] . append ( transcript ) 
~~ return hgnc_transcripts 
~~ def id_transcripts_by_gene ( self , build = '37' ) : 
hgnc_id_transcripts = { } 
for gene_obj in self . hgnc_collection . find ( { 'build' : build } ) : 
id_transcripts = self . get_id_transcripts ( hgnc_id = hgnc_id , build = build ) 
hgnc_id_transcripts [ hgnc_id ] = id_transcripts 
~~ return hgnc_id_transcripts 
~~ def ensembl_genes ( self , build = '37' ) : 
~~~ ensg_id = gene_obj [ 'ensembl_id' ] 
hgnc_id = gene_obj [ 'hgnc_id' ] 
genes [ ensg_id ] = gene_obj 
~~ def transcripts ( self , build = '37' , hgnc_id = None ) : 
query = { 'build' : build } 
~~~ query [ 'hgnc_id' ] = hgnc_id 
~~ return self . transcript_collection . find ( query ) 
~~ def to_hgnc ( self , hgnc_alias , build = '37' ) : 
result = self . hgnc_genes ( hgnc_symbol = hgnc_alias , build = build ) 
~~~ for gene in result : 
~~~ return gene [ 'hgnc_symbol' ] 
~~ ~~ def add_hgnc_id ( self , genes ) : 
genes_by_alias = self . genes_by_alias ( ) 
~~~ id_info = genes_by_alias . get ( gene [ 'hgnc_symbol' ] ) 
if not id_info : 
~~ gene [ 'hgnc_id' ] = id_info [ 'true' ] 
if not id_info [ 'true' ] : 
~~~ if len ( id_info [ 'ids' ] ) > 1 : 
~~ gene [ 'hgnc_id' ] = ',' . join ( [ str ( hgnc_id ) for hgnc_id in id_info [ 'ids' ] ] ) 
~~ ~~ ~~ def get_coding_intervals ( self , build = '37' , genes = None ) : 
intervals = { } 
~~~ genes = self . all_genes ( build = build ) 
for i , hgnc_obj in enumerate ( genes ) : 
~~~ chrom = hgnc_obj [ 'chromosome' ] 
start = max ( ( hgnc_obj [ 'start' ] - 5000 ) , 1 ) 
end = hgnc_obj [ 'end' ] + 5000 
if chrom not in intervals : 
~~~ intervals [ chrom ] = intervaltree . IntervalTree ( ) 
intervals [ chrom ] . addi ( start , end , i ) 
~~ res = intervals [ chrom ] . search ( start , end ) 
if not res : 
~~~ intervals [ chrom ] . addi ( start , end , i ) 
~~ for interval in res : 
~~~ if interval . begin < start : 
~~~ start = interval . begin 
~~ if interval . end > end : 
~~~ end = interval . end 
~~ intervals [ chrom ] . remove ( interval ) 
~~ intervals [ chrom ] . addi ( start , end , i ) 
~~ return intervals 
~~ def load_exons ( self , exons , genes = None , build = '37' ) : 
genes = genes or self . ensembl_genes ( build ) 
for exon in exons : 
~~~ exon_obj = build_exon ( exon , genes ) 
if not exon_obj : 
~~ res = self . exon_collection . insert_one ( exon_obj ) 
~~ ~~ def exons ( self , hgnc_id = None , transcript_id = None , build = None ) : 
~~~ query [ 'build' ] = build 
~~ if transcript_id : 
~~~ query [ 'transcript_id' ] = transcript_id 
~~ return self . exon_collection . find ( query ) 
~~ def omim ( context , api_key , institute ) : 
~~ institute_obj = adapter . institute ( institute ) 
~~ ~~ def index ( ) : 
institutes_count = ( ( institute_obj , store . cases ( collaborator = institute_obj [ '_id' ] ) . count ( ) ) 
for institute_obj in institute_objs if institute_obj ) 
return dict ( institutes = institutes_count ) 
~~ def cases ( institute_id ) : 
institute_obj = institute_and_case ( store , institute_id ) 
limit = 100 
if request . args . get ( 'limit' ) : 
~~~ limit = int ( request . args . get ( 'limit' ) ) 
~~ skip_assigned = request . args . get ( 'skip_assigned' ) 
is_research = request . args . get ( 'is_research' ) 
all_cases = store . cases ( collaborator = institute_id , name_query = query , 
skip_assigned = skip_assigned , is_research = is_research ) 
data = controllers . cases ( store , all_cases , limit ) 
sanger_unevaluated = controllers . get_sanger_unevaluated ( store , institute_id , current_user . email ) 
if len ( sanger_unevaluated ) > 0 : 
~~~ data [ 'sanger_unevaluated' ] = sanger_unevaluated 
~~ return dict ( institute = institute_obj , skip_assigned = skip_assigned , 
is_research = is_research , query = query , ** data ) 
~~ def case ( institute_id , case_name ) : 
data = controllers . case ( store , institute_obj , case_obj ) 
return dict ( institute = institute_obj , case = case_obj , ** data ) 
~~ def matchmaker_matches ( institute_id , case_name ) : 
user_obj = store . user ( current_user . email ) 
if 'mme_submitter' not in user_obj [ 'roles' ] : 
return redirect ( request . referrer ) 
~~ mme_base_url = current_app . config . get ( 'MME_URL' ) 
mme_token = current_app . config . get ( 'MME_TOKEN' ) 
if not mme_base_url or not mme_token : 
~~ institute_obj , case_obj = institute_and_case ( store , institute_id , case_name ) 
data = controllers . mme_matches ( case_obj , institute_obj , mme_base_url , mme_token ) 
if data and data . get ( 'server_errors' ) : 
~~ elif not data : 
~~~ data = { 
'institute' : institute_obj , 
'case' : case_obj 
~~ return data 
~~ def matchmaker_match ( institute_id , case_name , target ) : 
mme_accepts = current_app . config . get ( 'MME_ACCEPTS' ) 
nodes = current_app . mme_nodes 
if not mme_base_url or not mme_token or not mme_accepts : 
~~ match_results = controllers . mme_match ( case_obj , target , mme_base_url , mme_token , nodes , mme_accepts ) 
ok_responses = 0 
for match_results in match_results : 
~~~ match_results [ 'status_code' ] == 200 
ok_responses += 1 
~~ if ok_responses : 
~~ return redirect ( request . referrer ) 
~~ def matchmaker_add ( institute_id , case_name ) : 
causatives = False 
features = False 
if case_obj . get ( 'suspects' ) and len ( case_obj . get ( 'suspects' ) ) > 3 : 
~~ elif case_obj . get ( 'suspects' ) : 
~~~ causatives = True 
~~ if case_obj . get ( 'phenotype_terms' ) : 
~~~ features = True 
~~ mme_save_options = [ 'sex' , 'features' , 'disorders' ] 
for index , item in enumerate ( mme_save_options ) : 
~~~ if item in request . form : 
mme_save_options [ index ] = True 
~~~ mme_save_options [ index ] = False 
~~ ~~ genomic_features = request . form . get ( 'genomicfeatures' ) 
if genomic_features == 'variants' : 
~~ if ( not case_obj . get ( 'suspects' ) and not mme_save_options [ 1 ] ) or ( causatives is False and features is False ) : 
~~ user_obj = store . user ( current_user . email ) 
mme_base_url = current_app . config . get ( 'MME_URL' ) 
if not mme_base_url or not mme_accepts or not mme_token : 
~~ add_result = controllers . mme_add ( store = store , user_obj = user_obj , case_obj = case_obj , 
add_gender = mme_save_options [ 0 ] , add_features = mme_save_options [ 1 ] , 
add_disorders = mme_save_options [ 2 ] , genes_only = genes_only , 
mme_base_url = mme_base_url , mme_accepts = mme_accepts , mme_token = mme_token ) 
n_succes_response = 0 
n_inserted = 0 
n_updated = 0 
category = 'warning' 
for resp in add_result [ 'server_responses' ] : 
~~~ message = resp . get ( 'message' ) 
if resp . get ( 'status_code' ) == 200 : 
~~~ n_succes_response += 1 
~~~ n_updated += 1 
~~~ n_inserted += 1 
~~ ~~ if n_inserted or n_updated : 
~~~ category = 'success' 
store . case_mme_update ( case_obj = case_obj , user_obj = user_obj , mme_subm_obj = add_result ) 
n_inserted , n_updated , len ( add_result . get ( 'server_responses' ) ) - n_succes_response ) , category ) 
~~ def matchmaker_delete ( institute_id , case_name ) : 
~~ delete_result = controllers . mme_delete ( case_obj , mme_base_url , mme_token ) 
n_deleted = 0 
for resp in delete_result : 
~~~ if resp [ 'status_code' ] == 200 : 
~~~ n_deleted += 1 
~~~ flash ( resp [ 'message' ] , category ) 
~~ ~~ if n_deleted : 
store . case_mme_delete ( case_obj = case_obj , user_obj = user_obj ) 
~~ def gene_variants ( institute_id ) : 
page = int ( request . form . get ( 'page' , 1 ) ) 
if ( request . method == "POST" ) : 
~~~ form = GeneVariantFiltersForm ( request . form ) 
~~~ form = GeneVariantFiltersForm ( request . args ) 
~~ variant_type = form . data . get ( 'variant_type' , 'clinical' ) 
hgnc_symbols = [ ] 
non_clinical_symbols = [ ] 
not_found_symbols = [ ] 
not_found_ids = [ ] 
data = { } 
if ( form . hgnc_symbols . data ) and len ( form . hgnc_symbols . data ) > 0 : 
~~~ is_clinical = form . data . get ( 'variant_type' , 'clinical' ) == 'clinical' 
clinical_symbols = store . clinical_symbols ( case_obj ) if is_clinical else None 
for hgnc_symbol in form . hgnc_symbols . data : 
~~~ if hgnc_symbol . isdigit ( ) : 
~~~ hgnc_gene = store . hgnc_gene ( int ( hgnc_symbol ) ) 
~~~ not_found_ids . append ( hgnc_symbol ) 
~~~ hgnc_symbols . append ( hgnc_gene [ 'hgnc_symbol' ] ) 
~~ ~~ elif store . hgnc_genes ( hgnc_symbol ) . count ( ) == 0 : 
~~~ not_found_symbols . append ( hgnc_symbol ) 
~~ elif is_clinical and ( hgnc_symbol not in clinical_symbols ) : 
~~~ non_clinical_symbols . append ( hgnc_symbol ) 
~~~ hgnc_symbols . append ( hgnc_symbol ) 
~~ ~~ if ( not_found_ids ) : 
~~ if ( not_found_symbols ) : 
~~ if ( non_clinical_symbols ) : 
~~ form . hgnc_symbols . data = hgnc_symbols 
variants_query = store . gene_variants ( query = form . data , category = 'snv' , 
data = controllers . gene_variants ( store , variants_query , page ) 
~~ return dict ( institute = institute_obj , form = form , page = page , ** data ) 
~~ def case_synopsis ( institute_id , case_name ) : 
new_synopsis = request . form . get ( 'synopsis' ) 
controllers . update_synopsis ( store , institute_obj , case_obj , user_obj , new_synopsis ) 
~~ def case_report ( institute_id , case_name ) : 
data = controllers . case_report_content ( store , institute_obj , case_obj ) 
return dict ( institute = institute_obj , case = case_obj , format = 'html' , ** data ) 
~~ def pdf_case_report ( institute_id , case_name ) : 
if current_app . config . get ( 'SQLALCHEMY_DATABASE_URI' ) : 
~~~ data [ 'coverage_report' ] = controllers . coverage_report_contents ( store , institute_obj , case_obj , request . url_root ) 
~~ if case_obj . get ( 'madeline_info' ) is not None : 
~~~ with open ( os . path . join ( cases_bp . static_folder , 'madeline.svg' ) , 'w' ) as temp_madeline : 
~~~ temp_madeline . write ( case_obj [ 'madeline_info' ] ) 
~~ ~~ html_report = render_template ( 'cases/case_report.html' , institute = institute_obj , case = case_obj , format = 'pdf' , ** data ) 
return render_pdf ( HTML ( string = html_report ) , download_filename = case_obj [ 'display_name' ] + '_' + datetime . datetime . now ( ) . strftime ( "%Y-%m-%d" ) + '_scout.pdf' ) 
~~ def case_diagnosis ( institute_id , case_name ) : 
link = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) 
level = 'phenotype' if 'phenotype' in request . form else 'gene' 
omim_id = request . form [ 'omim_id' ] 
remove = True if request . args . get ( 'remove' ) == 'yes' else False 
store . diagnose ( institute_obj , case_obj , user_obj , link , level = level , 
omim_id = omim_id , remove = remove ) 
~~ def phenotypes ( institute_id , case_name , phenotype_id = None ) : 
case_url = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) 
is_group = request . args . get ( 'is_group' ) == 'yes' 
if phenotype_id : 
~~~ store . remove_phenotype ( institute_obj , case_obj , user_obj , case_url , 
phenotype_id , is_group = is_group ) 
~~~ phenotype_term = request . form [ 'hpo_term' ] 
if phenotype_term . startswith ( 'HP:' ) or len ( phenotype_term ) == 7 : 
store . add_phenotype ( institute_obj , case_obj , user_obj , case_url , 
hpo_term = hpo_term , is_group = is_group ) 
~~~ store . add_phenotype ( institute_obj , case_obj , user_obj , case_url , 
omim_term = phenotype_term ) 
~~ ~~ except ValueError : 
~~ ~~ return redirect ( case_url ) 
~~ def phenotypes_actions ( institute_id , case_name ) : 
action = request . form [ 'action' ] 
hpo_ids = request . form . getlist ( 'hpo_id' ) 
if action == 'DELETE' : 
~~~ for hpo_id in hpo_ids : 
~~~ store . remove_phenotype ( institute_obj , case_obj , user_obj , case_url , hpo_id ) 
~~ ~~ elif action == 'PHENOMIZER' : 
~~~ if len ( hpo_ids ) == 0 : 
~~~ hpo_ids = [ term [ 'phenotype_id' ] for term in case_obj . get ( 'phenotype_terms' , [ ] ) ] 
~~ username = current_app . config [ 'PHENOMIZER_USERNAME' ] 
password = current_app . config [ 'PHENOMIZER_PASSWORD' ] 
diseases = controllers . hpo_diseases ( username , password , hpo_ids ) 
return render_template ( 'cases/diseases.html' , diseases = diseases , 
institute = institute_obj , case = case_obj ) 
~~ elif action == 'GENES' : 
~~~ hgnc_symbols = set ( ) 
for raw_symbols in request . form . getlist ( 'genes' ) : 
~~~ if raw_symbols : 
raw_symbols . split ( '|' ) ) 
~~ ~~ store . update_dynamic_gene_list ( case_obj , hgnc_symbols = hgnc_symbols ) 
~~ elif action == 'GENERATE' : 
~~ results = store . generate_hpo_gene_list ( * hpo_ids ) 
hpo_count = int ( request . form . get ( 'min_match' ) or 1 ) 
hgnc_ids = [ result [ 0 ] for result in results if result [ 1 ] >= hpo_count ] 
store . update_dynamic_gene_list ( case_obj , hgnc_ids = hgnc_ids , phenotype_ids = hpo_ids ) 
~~ return redirect ( case_url ) 
~~ def events ( institute_id , case_name , event_id = None ) : 
link = request . form . get ( 'link' ) 
content = request . form . get ( 'content' ) 
variant_id = request . args . get ( 'variant_id' ) 
if event_id : 
~~~ store . delete_event ( event_id ) 
~~~ if variant_id : 
~~~ variant_obj = store . variant ( variant_id ) 
level = request . form . get ( 'level' , 'specific' ) 
store . comment ( institute_obj , case_obj , user_obj , link , 
variant = variant_obj , content = content , comment_level = level ) 
~~~ store . comment ( institute_obj , case_obj , user_obj , link , content = content ) 
~~ ~~ return redirect ( request . referrer ) 
~~ def status ( institute_id , case_name ) : 
status = request . form . get ( 'status' , case_obj [ 'status' ] ) 
if status == 'archive' : 
~~~ store . archive_case ( institute_obj , case_obj , user_obj , status , link ) 
~~~ store . update_status ( institute_obj , case_obj , user_obj , status , link ) 
~~ def assign ( institute_id , case_name , user_id = None ) : 
if user_id : 
~~~ user_obj = store . user ( user_id ) 
~~~ user_obj = store . user ( current_user . email ) 
~~ if request . form . get ( 'action' ) == 'DELETE' : 
~~~ store . unassign ( institute_obj , case_obj , user_obj , link ) 
~~~ store . assign ( institute_obj , case_obj , user_obj , link ) 
~~ def hpoterms ( ) : 
if query is None : 
~~ terms = sorted ( store . hpo_terms ( query = query ) , key = itemgetter ( 'hpo_number' ) ) 
json_terms = [ 
'id' : term [ '_id' ] 
} for term in terms [ : 7 ] ] 
return jsonify ( json_terms ) 
~~ def pin_variant ( institute_id , case_name , variant_id ) : 
link = url_for ( 'variants.variant' , institute_id = institute_id , case_name = case_name , 
variant_id = variant_id ) 
if request . form [ 'action' ] == 'ADD' : 
~~~ store . pin_variant ( institute_obj , case_obj , user_obj , link , variant_obj ) 
~~ elif request . form [ 'action' ] == 'DELETE' : 
~~~ store . unpin_variant ( institute_obj , case_obj , user_obj , link , variant_obj ) 
~~ return redirect ( request . referrer or link ) 
~~ def mark_validation ( institute_id , case_name , variant_id ) : 
validate_type = request . form [ 'type' ] or None 
store . validate ( institute_obj , case_obj , user_obj , link , variant_obj , validate_type ) 
return redirect ( request . referrer or link ) 
~~ def mark_causative ( institute_id , case_name , variant_id ) : 
~~~ store . mark_causative ( institute_obj , case_obj , user_obj , link , variant_obj ) 
~~~ store . unmark_causative ( institute_obj , case_obj , user_obj , link , variant_obj ) 
~~ case_url = url_for ( '.case' , institute_id = institute_id , case_name = case_name ) 
return redirect ( case_url ) 
~~ def check_case ( institute_id , case_name ) : 
store . case_collection . find_one_and_update ( { '_id' : case_obj [ '_id' ] } , { '$set' : { 'needs_check' : False } } ) 
~~ def delivery_report ( institute_id , case_name ) : 
if case_obj . get ( 'delivery_report' ) is None : 
~~ date_str = request . args . get ( 'date' ) 
if date_str : 
~~~ delivery_report = None 
analysis_date = parse_date ( date_str ) 
for analysis_data in case_obj [ 'analyses' ] : 
~~~ if analysis_data [ 'date' ] == analysis_date : 
~~~ delivery_report = analysis_data [ 'delivery_report' ] 
~~ ~~ if delivery_report is None : 
~~~ delivery_report = case_obj [ 'delivery_report' ] 
~~ out_dir = os . path . dirname ( delivery_report ) 
filename = os . path . basename ( delivery_report ) 
return send_from_directory ( out_dir , filename ) 
~~ def share ( institute_id , case_name ) : 
collaborator_id = request . form [ 'collaborator' ] 
revoke_access = 'revoke' in request . form 
if revoke_access : 
~~~ store . unshare ( institute_obj , case_obj , collaborator_id , user_obj , link ) 
~~~ store . share ( institute_obj , case_obj , collaborator_id , user_obj , link ) 
~~ def rerun ( institute_id , case_name ) : 
sender = current_app . config [ 'MAIL_USERNAME' ] 
recipient = current_app . config [ 'TICKET_SYSTEM_EMAIL' ] 
controllers . rerun ( store , mail , current_user , institute_id , case_name , sender , 
recipient ) 
~~ def research ( institute_id , case_name ) : 
store . open_research ( institute_obj , case_obj , user_obj , link ) 
~~ def cohorts ( institute_id , case_name ) : 
cohort_tag = request . form [ 'cohort_tag' ] 
if request . args . get ( 'remove' ) == 'yes' : 
~~~ store . remove_cohort ( institute_obj , case_obj , user_obj , link , cohort_tag ) 
~~~ store . add_cohort ( institute_obj , case_obj , user_obj , link , cohort_tag ) 
~~ def default_panels ( institute_id , case_name ) : 
panel_ids = request . form . getlist ( 'panel_ids' ) 
controllers . update_default_panels ( store , current_user , institute_id , case_name , panel_ids ) 
~~ def vcf2cytosure ( institute_id , case_name , individual_id ) : 
( display_name , vcf2cytosure ) = controllers . vcf2cytosure ( store , 
institute_id , case_name , individual_id ) 
outdir = os . path . abspath ( os . path . dirname ( vcf2cytosure ) ) 
filename = os . path . basename ( vcf2cytosure ) 
attachment_filename = display_name + ".vcf2cytosure.cgh" 
return send_from_directory ( outdir , filename , 
attachment_filename = attachment_filename , 
as_attachment = True ) 
~~ def multiqc ( institute_id , case_name ) : 
data = controllers . multiqc ( store , institute_id , case_name ) 
if data [ 'case' ] . get ( 'multiqc' ) is None : 
~~ out_dir = os . path . abspath ( os . path . dirname ( data [ 'case' ] [ 'multiqc' ] ) ) 
filename = os . path . basename ( data [ 'case' ] [ 'multiqc' ] ) 
~~ def update_panels ( context , mongodb , username , password , authdb , host , port , loglevel , config ) : 
coloredlogs . install ( level = loglevel ) 
mongo_config = { } 
cli_config = { } 
with open ( config , 'r' ) as in_handle : 
~~~ cli_config = yaml . load ( in_handle ) 
~~ ~~ mongo_config [ 'mongodb' ] = ( mongodb or cli_config . get ( 'mongodb' ) or 'scout' ) 
mongo_config [ 'host' ] = ( host or cli_config . get ( 'host' ) or 'localhost' ) 
mongo_config [ 'port' ] = ( port or cli_config . get ( 'port' ) or 27017 ) 
mongo_config [ 'username' ] = username or cli_config . get ( 'username' ) 
mongo_config [ 'password' ] = password or cli_config . get ( 'password' ) 
mongo_config [ 'authdb' ] = authdb or cli_config . get ( 'authdb' ) or mongo_config [ 'mongodb' ] 
mongo_config [ 'omim_api_key' ] = cli_config . get ( 'omim_api_key' ) 
host = mongo_config [ 'host' ] , 
port = mongo_config [ 'port' ] , 
username = mongo_config [ 'username' ] , 
password = mongo_config [ 'password' ] , 
authdb = mongo_config [ 'authdb' ] , 
~~~ client = get_connection ( ** mongo_config ) 
~~ database = client [ mongo_config [ 'mongodb' ] ] 
mongo_config [ 'client' ] = client 
adapter = MongoAdapter ( database ) 
requests = [ ] 
for case_obj in adapter . case_collection . find ( ) : 
~~~ gene_to_panels = adapter . gene_to_panels ( case_obj ) 
variants = adapter . variant_collection . find ( { 
'case_id' : case_obj [ '_id' ] , 
'category' : 'snv' , 
'variant_type' : 'clinical' , 
for variant_obj in variants : 
~~~ panel_names = set ( ) 
for hgnc_id in variant_obj [ 'hgnc_ids' ] : 
~~~ gene_panels = gene_to_panels . get ( hgnc_id , set ( ) ) 
panel_names = panel_names . union ( gene_panels ) 
~~ if panel_names : 
~~~ operation = pymongo . UpdateOne ( 
{ '_id' : variant_obj [ '_id' ] } , 
'$set' : { 
'panels' : list ( panel_names ) 
requests . append ( operation ) 
~~ if len ( requests ) > 5000 : 
~~~ adapter . variant_collection . bulk_write ( requests , ordered = False ) 
~~ ~~ if requests : 
~~ ~~ ~~ def cases ( store , case_query , limit = 100 ) : 
case_groups = { status : [ ] for status in CASE_STATUSES } 
for case_obj in case_query . limit ( limit ) : 
~~~ analysis_types = set ( ind [ 'analysis_type' ] for ind in case_obj [ 'individuals' ] ) 
case_obj [ 'analysis_types' ] = list ( analysis_types ) 
case_obj [ 'assignees' ] = [ store . user ( user_email ) for user_email in 
case_obj . get ( 'assignees' , [ ] ) ] 
case_groups [ case_obj [ 'status' ] ] . append ( case_obj ) 
case_obj [ 'is_rerun' ] = len ( case_obj . get ( 'analyses' , [ ] ) ) > 0 
case_obj [ 'clinvar_variants' ] = store . case_to_clinVars ( case_obj [ '_id' ] ) 
case_obj [ 'display_track' ] = TRACKS [ case_obj . get ( 'track' , 'rare' ) ] 
~~ data = { 
'cases' : [ ( status , case_groups [ status ] ) for status in CASE_STATUSES ] , 
'found_cases' : case_query . count ( ) , 
'limit' : limit , 
~~ def case ( store , institute_obj , case_obj ) : 
case_obj [ 'individual_ids' ] = [ ] 
~~~ sex = int ( individual . get ( 'sex' , 0 ) ) 
~~~ sex = 0 
~~ individual [ 'sex_human' ] = SEX_MAP [ sex ] 
pheno_map = PHENOTYPE_MAP 
if case_obj . get ( 'track' , 'rare' ) == 'cancer' : 
~~~ pheno_map = CANCER_PHENOTYPE_MAP 
~~ individual [ 'phenotype_human' ] = pheno_map . get ( individual [ 'phenotype' ] ) 
case_obj [ 'individual_ids' ] . append ( individual [ 'individual_id' ] ) 
~~ case_obj [ 'assignees' ] = [ store . user ( user_email ) for user_email in 
suspects = [ store . variant ( variant_id ) or variant_id for variant_id in 
case_obj . get ( 'suspects' , [ ] ) ] 
causatives = [ store . variant ( variant_id ) or variant_id for variant_id in 
case_obj . get ( 'causatives' , [ ] ) ] 
distinct_genes = set ( ) 
case_obj [ 'panel_names' ] = [ ] 
~~~ if not panel_info . get ( 'is_default' ) : 
~~ panel_obj = store . gene_panel ( panel_info [ 'panel_name' ] , version = panel_info . get ( 'version' ) ) 
distinct_genes . update ( [ gene [ 'hgnc_id' ] for gene in panel_obj . get ( 'genes' , [ ] ) ] ) 
case_obj [ 'panel_names' ] . append ( full_name ) 
~~ case_obj [ 'default_genes' ] = list ( distinct_genes ) 
for hpo_term in itertools . chain ( case_obj . get ( 'phenotype_groups' , [ ] ) , 
case_obj . get ( 'phenotype_terms' , [ ] ) ) : 
~~~ hpo_term [ 'hpo_link' ] = ( "http://compbio.charite.de/hpoweb/showterm?id={}" 
. format ( hpo_term [ 'phenotype_id' ] ) ) 
~~ o_collaborators = [ ] 
for collab_id in case_obj [ 'collaborators' ] : 
~~~ if collab_id != case_obj [ 'owner' ] and store . institute ( collab_id ) : 
~~~ o_collaborators . append ( store . institute ( collab_id ) ) 
~~ ~~ case_obj [ 'o_collaborators' ] = [ ( collab_obj [ '_id' ] , collab_obj [ 'display_name' ] ) for 
collab_obj in o_collaborators ] 
irrelevant_ids = ( 'cust000' , institute_obj [ '_id' ] ) 
collab_ids = [ ( collab [ '_id' ] , collab [ 'display_name' ] ) for collab in store . institutes ( ) if 
( collab [ '_id' ] not in irrelevant_ids ) and 
( collab [ '_id' ] not in case_obj [ 'collaborators' ] ) ] 
events = list ( store . events ( institute_obj , case = case_obj ) ) 
for event in events : 
~~~ event [ 'verb' ] = VERBS_MAP [ event [ 'verb' ] ] 
~~ case_obj [ 'clinvar_variants' ] = store . case_to_clinVars ( case_obj [ '_id' ] ) 
pheno_groups = institute_obj . get ( 'phenotype_groups' ) or PHENOTYPE_GROUPS 
'status_class' : STATUS_MAP . get ( case_obj [ 'status' ] ) , 
'other_causatives' : store . check_causatives ( case_obj = case_obj ) , 
'comments' : store . events ( institute_obj , case = case_obj , comments = True ) , 
'hpo_groups' : pheno_groups , 
'events' : events , 
'suspects' : suspects , 
'causatives' : causatives , 
'collaborators' : collab_ids , 
'cohort_tags' : COHORT_TAGS , 
~~ def case_report_content ( store , institute_obj , case_obj ) : 
variant_types = { 
'causatives_detailed' : 'causatives' , 
'suspects_detailed' : 'suspects' , 
'classified_detailed' : 'acmg_classification' , 
'tagged_detailed' : 'manual_rank' , 
'dismissed_detailed' : 'dismiss_variant' , 
'commented_detailed' : 'is_commented' , 
data = case_obj 
for individual in data [ 'individuals' ] : 
individual [ 'phenotype_human' ] = PHENOTYPE_MAP . get ( individual [ 'phenotype' ] ) 
~~ data [ 'comments' ] = store . events ( institute_obj , case = case_obj , comments = True ) 
data [ 'manual_rank_options' ] = MANUAL_RANK_OPTIONS 
data [ 'dismissed_options' ] = DISMISS_VARIANT_OPTIONS 
data [ 'genetic_models' ] = dict ( GENETIC_MODELS ) 
evaluated_variants = { } 
for vt in variant_types : 
~~~ evaluated_variants [ vt ] = [ ] 
~~ for var_type in [ 'causatives' , 'suspects' ] : 
~~~ vt = '_' . join ( [ var_type , 'detailed' ] ) 
for var_id in case_obj . get ( var_type , [ ] ) : 
~~~ variant_obj = store . variant ( var_id ) 
~~ evaluated_variants [ vt ] . append ( variant_obj ) 
~~ ~~ for var_obj in store . evaluated_variants ( case_id = case_obj [ '_id' ] ) : 
~~~ for vt in variant_types : 
~~~ keyword = variant_types [ vt ] 
if keyword in var_obj : 
~~~ evaluated_variants [ vt ] . append ( var_obj ) 
~~ ~~ ~~ for var_type in evaluated_variants : 
~~~ decorated_variants = [ ] 
for var_obj in evaluated_variants [ var_type ] : 
~~~ if var_obj [ 'category' ] == 'snv' : 
~~~ decorated_info = variant_decorator ( 
store = store , 
institute_obj = institute_obj , 
variant_id = None , 
variant_obj = var_obj , 
add_case = False , 
add_other = False , 
get_overlapping = False 
~~~ decorated_info = sv_variant ( 
institute_id = institute_obj [ '_id' ] , 
case_name = case_obj [ 'display_name' ] , 
~~ decorated_variants . append ( decorated_info [ 'variant' ] ) 
~~ data [ var_type ] = decorated_variants 
~~ def coverage_report_contents ( store , institute_obj , case_obj , base_url ) : 
request_data = { } 
request_data [ 'sample_id' ] = [ ind [ 'individual_id' ] for ind in case_obj [ 'individuals' ] ] 
panel_names = [ ] 
panel_names . append ( full_name ) 
request_data [ 'panel_name' ] = panel_names 
request_data [ 'level' ] = institute_obj . get ( 'coverage_cutoff' , 15 ) 
resp = requests . get ( base_url + 'reports/report' , params = request_data ) 
soup = BeautifulSoup ( resp . text ) 
for tag in soup . find_all ( 'a' ) : 
~~~ tag . replaceWith ( '' ) 
~~ coverage_data = '' . join ( [ '%s' % x for x in soup . body . contents ] ) 
return coverage_data 
~~ def clinvar_submissions ( store , user_id , institute_id ) : 
submissions = list ( store . clinvar_submissions ( user_id , institute_id ) ) 
return submissions 
~~ def mt_excel_files ( store , case_obj , temp_excel_dir ) : 
samples = case_obj . get ( 'individuals' ) 
mt_variants = list ( store . variants ( case_id = case_obj [ '_id' ] , query = query , nr_of_variants = - 1 , sort_key = 'position' ) ) 
~~ def update_synopsis ( store , institute_obj , case_obj , user_obj , new_synopsis ) : 
if case_obj [ 'synopsis' ] != new_synopsis : 
~~~ link = url_for ( 'cases.case' , institute_id = institute_obj [ '_id' ] , 
case_name = case_obj [ 'display_name' ] ) 
store . update_synopsis ( institute_obj , case_obj , user_obj , link , 
content = new_synopsis ) 
~~ ~~ def hpo_diseases ( username , password , hpo_ids , p_value_treshold = 1 ) : 
~~~ results = query_phenomizer . query ( username , password , * hpo_ids ) 
diseases = [ result for result in results 
if result [ 'p_value' ] <= p_value_treshold ] 
~~ except SystemExit : 
~~ ~~ def rerun ( store , mail , current_user , institute_id , case_name , sender , recipient ) : 
link = url_for ( 'cases.case' , institute_id = institute_id , case_name = case_name ) 
store . request_rerun ( institute_obj , case_obj , user_obj , link ) 
case = case_obj [ 'display_name' ] , case_id = case_obj [ '_id' ] , 
name = user_obj [ 'name' ] . encode ( ) ) 
. format ( case_obj [ 'display_name' ] ) ) , 
html = html , sender = sender , recipients = [ recipient ] , 
cc = [ user_obj [ 'email' ] ] ) 
mail . send ( msg ) 
~~ def update_default_panels ( store , current_user , institute_id , case_name , panel_ids ) : 
panel_objs = [ store . panel ( panel_id ) for panel_id in panel_ids ] 
store . update_default_panels ( institute_obj , case_obj , user_obj , link , panel_objs ) 
~~ def vcf2cytosure ( store , institute_id , case_name , individual_id ) : 
~~~ if individual [ 'individual_id' ] == individual_id : 
~~~ individual_obj = individual 
~~ ~~ return ( individual_obj [ 'display_name' ] , individual_obj [ 'vcf2cytosure' ] ) 
~~ def gene_variants ( store , variants_query , page = 1 , per_page = 50 ) : 
variant_count = variants_query . count ( ) 
skip_count = per_page * max ( page - 1 , 0 ) 
more_variants = True if variant_count > ( skip_count + per_page ) else False 
variant_res = variants_query . skip ( skip_count ) . limit ( per_page ) 
my_institutes = list ( inst [ '_id' ] for inst in user_institutes ( store , current_user ) ) 
variants = [ ] 
for variant_obj in variant_res : 
~~~ if variant_obj [ 'institute' ] not in my_institutes : 
~~ variant_case_obj = store . case ( case_id = variant_obj [ 'case_id' ] ) 
if not variant_case_obj : 
~~ case_display_name = variant_case_obj . get ( 'display_name' ) 
variant_obj [ 'case_display_name' ] = case_display_name 
genome_build = variant_case_obj . get ( 'genome_build' , '37' ) 
if genome_build not in [ '37' , '38' ] : 
~~~ genome_build = '37' 
~~ variant_genes = variant_obj . get ( 'genes' ) 
if variant_genes is not None : 
~~~ for gene_obj in variant_genes : 
~~~ if not gene_obj [ 'hgnc_id' ] : 
~~ if gene_obj . get ( 'hgnc_symbol' ) is None or gene_obj . get ( 'description' ) is None : 
~~~ hgnc_gene = store . hgnc_gene ( gene_obj [ 'hgnc_id' ] , build = genome_build ) 
~~ gene_obj [ 'hgnc_symbol' ] = hgnc_gene [ 'hgnc_symbol' ] 
~~ ~~ ~~ gene_ids = [ ] 
gene_symbols = [ ] 
hgvs_c = [ ] 
hgvs_p = [ ] 
variant_genes = variant_obj . get ( 'genes' ) 
~~~ functional_annotation = '' 
for gene_obj in variant_genes : 
gene_symbol = gene ( store , hgnc_id ) [ 'symbol' ] 
gene_ids . append ( hgnc_id ) 
gene_symbols . append ( gene_symbol ) 
hgvs_nucleotide = '-' 
transcripts_list = gene_obj . get ( 'transcripts' ) 
for transcript_obj in transcripts_list : 
~~~ if transcript_obj . get ( 'is_canonical' ) and transcript_obj . get ( 'is_canonical' ) is True : 
~~~ hgvs_nucleotide = str ( transcript_obj . get ( 'coding_sequence_name' ) ) 
hgvs_protein = str ( transcript_obj . get ( 'protein_sequence_name' ) ) 
~~ ~~ hgvs_c . append ( hgvs_nucleotide ) 
hgvs_p . append ( hgvs_protein ) 
~~ if len ( gene_symbols ) == 1 : 
~~~ if ( hgvs_p [ 0 ] != "None" ) : 
~~~ hgvs = hgvs_p [ 0 ] 
~~ elif ( hgvs_c [ 0 ] != "None" ) : 
~~~ hgvs = hgvs_c [ 0 ] 
~~~ hgvs = "-" 
~~ variant_obj [ 'hgvs' ] = hgvs 
~~ variant_obj . update ( get_predictions ( variant_genes ) ) 
~~ variants . append ( variant_obj ) 
~~ return { 
'variants' : variants , 
'more_variants' : more_variants , 
~~ def multiqc ( store , institute_id , case_name ) : 
return dict ( 
institute = institute_obj , 
case = case_obj , 
~~ def get_sanger_unevaluated ( store , institute_id , user_id ) : 
sanger_ordered_by_case = store . sanger_ordered ( institute_id , user_id ) 
unevaluated = [ ] 
for item in sanger_ordered_by_case : 
~~~ case_id = item [ '_id' ] 
case_obj = store . case ( case_id = case_id ) 
~~ case_display_name = case_obj . get ( 'display_name' ) 
varid_list = item [ 'vars' ] 
unevaluated_by_case = { } 
unevaluated_by_case [ case_display_name ] = [ ] 
for var_id in varid_list : 
~~~ variant_obj = store . variant ( document_id = var_id , case_id = case_id ) 
if variant_obj is None or variant_obj . get ( 'sanger_ordered' ) is None or variant_obj . get ( 'sanger_ordered' ) is False : 
~~ validation = variant_obj . get ( 'validation' , 'not_evaluated' ) 
~~ unevaluated_by_case [ case_display_name ] . append ( variant_obj [ '_id' ] ) 
~~ if len ( unevaluated_by_case [ case_display_name ] ) > 0 : 
~~~ unevaluated . append ( unevaluated_by_case ) 
~~ ~~ return unevaluated 
~~ def mme_add ( store , user_obj , case_obj , add_gender , add_features , add_disorders , genes_only , 
mme_base_url , mme_accepts , mme_token ) : 
~~ url = '' . join ( [ mme_base_url , '/patient/add' ] ) 
contact_info = { 
'name' : user_obj [ 'name' ] , 
'href' : '' . join ( [ 'mailto:' , user_obj [ 'email' ] ] ) , 
~~~ features = hpo_terms ( case_obj ) 
~~~ disorders = omim_terms ( case_obj ) 
~~ server_responses = [ ] 
submitted_info = { 
'contact' : contact_info , 
'sex' : add_gender , 
'features' : features , 
'disorders' : disorders , 
'genes_only' : genes_only , 
'patient_id' : [ ] 
for individual in case_obj . get ( 'individuals' ) : 
~~ patient = { 
'label' : '.' . join ( [ case_obj [ 'display_name' ] , individual . get ( 'display_name' ) ] ) , 
'disorders' : disorders 
if add_gender : 
~~~ if individual [ 'sex' ] == '1' : 
~~~ patient [ 'sex' ] = 'MALE' 
~~~ patient [ 'sex' ] = 'FEMALE' 
~~ ~~ if case_obj . get ( 'suspects' ) : 
~~~ g_features = genomic_features ( store , case_obj , individual . get ( 'display_name' ) , genes_only ) 
patient [ 'genomicFeatures' ] = g_features 
~~ resp = matchmaker_request ( url = url , token = mme_token , method = 'POST' , content_type = mme_accepts , 
accept = 'application/json' , data = { 'patient' : patient } ) 
server_responses . append ( { 
'patient' : patient , 
'message' : resp . get ( 'message' ) , 
'status_code' : resp . get ( 'status_code' ) 
~~ submitted_info [ 'server_responses' ] = server_responses 
return submitted_info 
~~ def mme_delete ( case_obj , mme_base_url , mme_token ) : 
server_responses = [ ] 
~~ for patient in case_obj [ 'mme_submission' ] [ 'patients' ] : 
~~~ patient_id = patient [ 'id' ] 
url = '' . join ( [ mme_base_url , '/patient/delete/' , patient_id ] ) 
resp = matchmaker_request ( url = url , token = mme_token , method = 'DELETE' , ) 
'patient_id' : patient_id , 
~~ return server_responses 
~~ def mme_matches ( case_obj , institute_obj , mme_base_url , mme_token ) : 
'case' : case_obj , 
'server_errors' : [ ] 
matches = { } 
if not case_obj . get ( 'mme_submission' ) : 
matches [ patient_id ] = None 
url = '' . join ( [ mme_base_url , '/matches/' , patient_id ] ) 
server_resp = matchmaker_request ( url = url , token = mme_token , method = 'GET' ) 
~~~ pat_matches = [ ] 
if server_resp . get ( 'matches' ) : 
~~~ pat_matches = parse_matches ( patient_id , server_resp [ 'matches' ] ) 
~~ matches [ patient_id ] = pat_matches 
data [ 'server_errors' ] . append ( server_resp [ 'message' ] ) 
~~ ~~ data [ 'matches' ] = matches 
~~ def mme_match ( case_obj , match_type , mme_base_url , mme_token , nodes = None , mme_accepts = None ) : 
query_patients = [ ] 
url = None 
query_patients = case_obj [ 'mme_submission' ] [ 'patients' ] 
if match_type == 'internal' : 
~~~ url = '' . join ( [ mme_base_url , '/match' ] ) 
for patient in query_patients : 
~~~ json_resp = matchmaker_request ( url = url , token = mme_token , method = 'POST' , 
content_type = mme_accepts , accept = mme_accepts , data = { 'patient' : patient } ) 
resp_obj = { 
'patient_id' : patient [ 'id' ] , 
'results' : json_resp . get ( 'results' ) , 
'status_code' : json_resp . get ( 'status_code' ) , 
server_responses . append ( resp_obj ) 
~~~ query_patients = [ patient [ 'id' ] for patient in query_patients ] 
node_ids = [ node [ 'id' ] for node in nodes ] 
~~~ node_ids = [ match_type ] 
~~ for patient in query_patients : 
~~~ for node in node_ids : 
~~~ url = '' . join ( [ mme_base_url , '/match/external/' , patient , '?node=' , node ] ) 
json_resp = matchmaker_request ( url = url , token = mme_token , method = 'POST' ) 
'server' : node , 
'patient_id' : patient , 
~~ ~~ ~~ return server_responses 
~~ def build_variant ( variant , institute_id , gene_to_panels = None , 
hgncid_to_gene = None , sample_info = None ) : 
gene_to_panels = gene_to_panels or { } 
sample_info = sample_info or { } 
variant_obj = dict ( 
_id = variant [ 'ids' ] [ 'document_id' ] , 
document_id = variant [ 'ids' ] [ 'document_id' ] , 
variant_id = variant [ 'ids' ] [ 'variant_id' ] , 
display_name = variant [ 'ids' ] [ 'display_name' ] , 
variant_type = variant [ 'variant_type' ] , 
case_id = variant [ 'case_id' ] , 
chromosome = variant [ 'chromosome' ] , 
reference = variant [ 'reference' ] , 
alternative = variant [ 'alternative' ] , 
institute = institute_id , 
variant_obj [ 'missing_data' ] = False 
variant_obj [ 'position' ] = int ( variant [ 'position' ] ) 
variant_obj [ 'rank_score' ] = float ( variant [ 'rank_score' ] ) 
end = variant . get ( 'end' ) 
if end : 
~~~ variant_obj [ 'end' ] = int ( end ) 
~~ length = variant . get ( 'length' ) 
if length : 
~~~ variant_obj [ 'length' ] = int ( length ) 
~~ variant_obj [ 'simple_id' ] = variant [ 'ids' ] . get ( 'simple_id' ) 
variant_obj [ 'quality' ] = float ( variant [ 'quality' ] ) if variant [ 'quality' ] else None 
variant_obj [ 'filters' ] = variant [ 'filters' ] 
variant_obj [ 'dbsnp_id' ] = variant . get ( 'dbsnp_id' ) 
variant_obj [ 'cosmic_ids' ] = variant . get ( 'cosmic_ids' ) 
variant_obj [ 'category' ] = variant [ 'category' ] 
variant_obj [ 'sub_category' ] = variant . get ( 'sub_category' ) 
if 'mate_id' in variant : 
~~~ variant_obj [ 'mate_id' ] = variant [ 'mate_id' ] 
~~ if 'cytoband_start' in variant : 
~~~ variant_obj [ 'cytoband_start' ] = variant [ 'cytoband_start' ] 
~~ if 'cytoband_end' in variant : 
~~~ variant_obj [ 'cytoband_end' ] = variant [ 'cytoband_end' ] 
~~ if 'end_chrom' in variant : 
~~~ variant_obj [ 'end_chrom' ] = variant [ 'end_chrom' ] 
~~ if 'str_ru' in variant : 
~~~ variant_obj [ 'str_ru' ] = variant [ 'str_ru' ] 
~~ if 'str_repid' in variant : 
~~~ variant_obj [ 'str_repid' ] = variant [ 'str_repid' ] 
~~ if 'str_ref' in variant : 
~~~ variant_obj [ 'str_ref' ] = variant [ 'str_ref' ] 
~~ if 'str_len' in variant : 
~~~ variant_obj [ 'str_len' ] = variant [ 'str_len' ] 
~~ if 'str_status' in variant : 
~~~ variant_obj [ 'str_status' ] = variant [ 'str_status' ] 
~~ gt_types = [ ] 
for sample in variant . get ( 'samples' , [ ] ) : 
~~~ gt_call = build_genotype ( sample ) 
gt_types . append ( gt_call ) 
if sample_info : 
if sample_info [ sample_id ] == 'case' : 
~~~ key = 'tumor' 
~~~ key = 'normal' 
~~ variant_obj [ key ] = { 
'alt_depth' : sample [ 'alt_depth' ] , 
'ref_depth' : sample [ 'ref_depth' ] , 
'read_depth' : sample [ 'read_depth' ] , 
'alt_freq' : sample [ 'alt_frequency' ] , 
'ind_id' : sample_id 
~~ ~~ variant_obj [ 'samples' ] = gt_types 
if 'genetic_models' in variant : 
~~~ variant_obj [ 'genetic_models' ] = variant [ 'genetic_models' ] 
~~ compounds = [ ] 
for compound in variant . get ( 'compounds' , [ ] ) : 
~~~ compound_obj = build_compound ( compound ) 
~~ if compounds : 
~~~ variant_obj [ 'compounds' ] = compounds 
~~ genes = [ ] 
for index , gene in enumerate ( variant . get ( 'genes' , [ ] ) ) : 
~~~ if gene . get ( 'hgnc_id' ) : 
~~~ gene_obj = build_gene ( gene , hgncid_to_gene ) 
genes . append ( gene_obj ) 
if index > 30 : 
~~~ variant_obj [ 'missing_data' ] = True 
~~ ~~ ~~ if genes : 
~~~ variant_obj [ 'genes' ] = genes 
~~ if 'hgnc_ids' in variant : 
~~~ variant_obj [ 'hgnc_ids' ] = [ hgnc_id for hgnc_id in variant [ 'hgnc_ids' ] if hgnc_id ] 
~~ hgnc_symbols = [ ] 
~~~ gene_obj = hgncid_to_gene . get ( hgnc_id ) 
~~~ hgnc_symbols . append ( gene_obj [ 'hgnc_symbol' ] ) 
~~ ~~ if hgnc_symbols : 
~~~ variant_obj [ 'hgnc_symbols' ] = hgnc_symbols 
~~ panel_names = set ( ) 
~~~ variant_obj [ 'panels' ] = list ( panel_names ) 
~~ clnsig_objects = [ ] 
for entry in variant . get ( 'clnsig' , [ ] ) : 
~~~ clnsig_obj = build_clnsig ( entry ) 
clnsig_objects . append ( clnsig_obj ) 
~~ if clnsig_objects : 
~~~ variant_obj [ 'clnsig' ] = clnsig_objects 
~~ call_info = variant . get ( 'callers' , { } ) 
for caller in call_info : 
~~~ if call_info [ caller ] : 
~~~ variant_obj [ caller ] = call_info [ caller ] 
~~ ~~ conservation_info = variant . get ( 'conservation' , { } ) 
if conservation_info . get ( 'phast' ) : 
~~~ variant_obj [ 'phast_conservation' ] = conservation_info [ 'phast' ] 
~~ if conservation_info . get ( 'gerp' ) : 
~~~ variant_obj [ 'gerp_conservation' ] = conservation_info [ 'gerp' ] 
~~ if conservation_info . get ( 'phylop' ) : 
~~~ variant_obj [ 'phylop_conservation' ] = conservation_info [ 'phylop' ] 
~~ if variant . get ( 'azlength' ) : 
~~~ variant_obj [ 'azlength' ] = variant [ 'azlength' ] 
~~ if variant . get ( 'azqual' ) : 
~~~ variant_obj [ 'azqual' ] = variant [ 'azqual' ] 
~~ frequencies = variant . get ( 'frequencies' , { } ) 
if frequencies . get ( 'thousand_g' ) : 
~~~ variant_obj [ 'thousand_genomes_frequency' ] = float ( frequencies [ 'thousand_g' ] ) 
~~ if frequencies . get ( 'thousand_g_max' ) : 
~~~ variant_obj [ 'max_thousand_genomes_frequency' ] = float ( frequencies [ 'thousand_g_max' ] ) 
~~ if frequencies . get ( 'exac' ) : 
~~~ variant_obj [ 'exac_frequency' ] = float ( frequencies [ 'exac' ] ) 
~~ if frequencies . get ( 'exac_max' ) : 
~~~ variant_obj [ 'max_exac_frequency' ] = float ( frequencies [ 'exac_max' ] ) 
~~ if frequencies . get ( 'gnomad' ) : 
~~~ variant_obj [ 'gnomad_frequency' ] = float ( frequencies [ 'gnomad' ] ) 
~~ if frequencies . get ( 'gnomad_max' ) : 
~~~ variant_obj [ 'max_gnomad_frequency' ] = float ( frequencies [ 'gnomad_max' ] ) 
~~ if frequencies . get ( 'thousand_g_left' ) : 
~~~ variant_obj [ 'thousand_genomes_frequency_left' ] = float ( frequencies [ 'thousand_g_left' ] ) 
~~ if frequencies . get ( 'thousand_g_right' ) : 
~~~ variant_obj [ 'thousand_genomes_frequency_right' ] = float ( frequencies [ 'thousand_g_right' ] ) 
~~ if variant . get ( 'local_obs_old' ) : 
~~~ variant_obj [ 'local_obs_old' ] = variant [ 'local_obs_old' ] 
~~ if variant . get ( 'local_obs_hom_old' ) : 
~~~ variant_obj [ 'local_obs_hom_old' ] = variant [ 'local_obs_hom_old' ] 
~~ if frequencies . get ( 'clingen_cgh_benign' ) : 
~~~ variant_obj [ 'clingen_cgh_benign' ] = frequencies [ 'clingen_cgh_benign' ] 
~~ if frequencies . get ( 'clingen_cgh_pathogenic' ) : 
~~~ variant_obj [ 'clingen_cgh_pathogenic' ] = frequencies [ 'clingen_cgh_pathogenic' ] 
~~ if frequencies . get ( 'clingen_ngi' ) : 
~~~ variant_obj [ 'clingen_ngi' ] = frequencies [ 'clingen_ngi' ] 
~~ if frequencies . get ( 'swegen' ) : 
~~~ variant_obj [ 'swegen' ] = frequencies [ 'swegen' ] 
~~ if frequencies . get ( 'decipher' ) : 
~~~ variant_obj [ 'decipher' ] = frequencies [ 'decipher' ] 
~~ elif frequencies . get ( 'decipherAF' ) : 
~~~ variant_obj [ 'decipher' ] = frequencies [ 'decipherAF' ] 
~~ if variant . get ( 'cadd_score' ) : 
~~~ variant_obj [ 'cadd_score' ] = variant [ 'cadd_score' ] 
~~ if variant . get ( 'spidex' ) : 
~~~ variant_obj [ 'spidex' ] = variant [ 'spidex' ] 
~~ rank_results = [ ] 
for category in variant . get ( 'rank_result' , [ ] ) : 
~~~ rank_result = { 
'category' : category , 
'score' : variant [ 'rank_result' ] [ category ] 
rank_results . append ( rank_result ) 
~~ if rank_results : 
~~~ variant_obj [ 'rank_score_results' ] = rank_results 
~~ if variant . get ( 'mvl_tag' ) : 
~~~ variant_obj [ 'mvl_tag' ] = True 
~~ def genes ( context , build , api_key ) : 
~~~ mim_files = fetch_mim_files ( api_key , mim2genes = True , morbidmap = True , genemap2 = True ) 
adapter . drop_genes ( build ) 
adapter . drop_transcripts ( build ) 
hpo_genes = fetch_hpo_genes ( ) 
~~~ builds = [ build ] 
~~~ builds = [ '37' , '38' ] 
~~ hgnc_lines = fetch_hgnc ( ) 
exac_lines = fetch_exac_constraint ( ) 
for build in builds : 
~~~ ensembl_genes = fetch_ensembl_genes ( build = build ) 
hgnc_genes = load_hgnc_genes ( 
ensembl_lines = ensembl_genes , 
hpo_lines = hpo_genes , 
for gene_obj in hgnc_genes : 
ensembl_genes [ ensembl_id ] = gene_obj 
~~ ensembl_transcripts = fetch_ensembl_transcripts ( build = build ) 
transcripts = load_transcripts ( adapter , ensembl_transcripts , build , ensembl_genes ) 
~~ adapter . update_indexes ( ) 
~~ def parse_callers ( variant , category = 'snv' ) : 
relevant_callers = CALLERS [ category ] 
callers = { caller [ 'id' ] : None for caller in relevant_callers } 
raw_info = variant . INFO . get ( 'set' ) 
if raw_info : 
~~~ info = raw_info . split ( '-' ) 
for call in info : 
~~~ if call == 'FilteredInAll' : 
~~~ for caller in callers : 
~~~ callers [ caller ] = 'Filtered' 
~~ ~~ elif call == 'Intersection' : 
~~~ callers [ caller ] = 'Pass' 
~~ ~~ elif 'filterIn' in call : 
~~~ if caller in call : 
~~ ~~ ~~ elif call in set ( callers . keys ( ) ) : 
~~~ callers [ call ] = 'Pass' 
~~ ~~ ~~ other_info = variant . INFO . get ( 'FOUND_IN' ) 
if other_info : 
~~~ for info in other_info . split ( ',' ) : 
~~~ called_by = info . split ( '|' ) [ 0 ] 
callers [ called_by ] = 'Pass' 
~~ ~~ return callers 
~~ def parse_header_format ( description ) : 
description = description . strip ( \ ) 
keyword = 'Format:' 
before_keyword , keyword , after_keyword = description . partition ( keyword ) 
return after_keyword . strip ( ) 
~~ def parse_vep_header ( vcf_obj ) : 
vep_header = [ ] 
if 'CSQ' in vcf_obj : 
~~~ csq_info = vcf_obj [ 'CSQ' ] 
format_info = parse_header_format ( csq_info [ 'Description' ] ) 
vep_header = [ key . upper ( ) for key in format_info . split ( '|' ) ] 
~~ return vep_header 
~~ def build_transcript ( transcript_info , build = '37' ) : 
~~~ transcript_id = transcript_info [ 'ensembl_transcript_id' ] 
~~ build = build 
is_primary = transcript_info . get ( 'is_primary' , False ) 
refseq_id = transcript_info . get ( 'refseq_id' ) 
refseq_identifiers = transcript_info . get ( 'refseq_identifiers' ) 
~~~ chrom = transcript_info [ 'chrom' ] 
~~~ start = int ( transcript_info [ 'transcript_start' ] ) 
~~~ end = int ( transcript_info [ 'transcript_end' ] ) 
~~~ hgnc_id = int ( transcript_info [ 'hgnc_id' ] ) 
~~ transcript_obj = HgncTranscript ( 
transcript_id = transcript_id , 
is_primary = is_primary , 
refseq_id = refseq_id , 
refseq_identifiers = refseq_identifiers , 
build = build 
for key in list ( transcript_obj ) : 
~~~ if transcript_obj [ key ] is None : 
~~~ transcript_obj . pop ( key ) 
~~ ~~ return transcript_obj 
~~ def load_institute ( adapter , internal_id , display_name , sanger_recipients = None ) : 
institute_obj = build_institute ( 
adapter . add_institute ( institute_obj ) 
~~ def parse_cadd ( variant , transcripts ) : 
cadd = 0 
cadd_keys = [ 'CADD' , 'CADD_PHRED' ] 
for key in cadd_keys : 
~~~ cadd = variant . INFO . get ( key , 0 ) 
~~~ return float ( cadd ) 
~~ ~~ for transcript in transcripts : 
~~~ cadd_entry = transcript . get ( 'cadd' ) 
if ( cadd_entry and cadd_entry > cadd ) : 
~~~ cadd = cadd_entry 
~~ ~~ return cadd 
~~ def case ( context , vcf , vcf_sv , vcf_cancer , vcf_str , owner , ped , update , config , 
no_variants , peddy_ped , peddy_sex , peddy_check ) : 
if config is None and ped is None : 
~~ config_raw = yaml . load ( config ) if config else { } 
~~~ config_data = parse_case_data ( 
config = config_raw , 
ped = ped , 
owner = owner , 
vcf_snv = vcf , 
vcf_sv = vcf_sv , 
vcf_str = vcf_str , 
vcf_cancer = vcf_cancer , 
peddy_ped = peddy_ped , 
peddy_sex = peddy_sex , 
peddy_check = peddy_check 
~~ except SyntaxError as err : 
~~~ case_obj = adapter . load_case ( config_data , update ) 
LOG . warning ( err ) 
~~ ~~ def update_variant ( self , variant_obj ) : 
new_variant = self . variant_collection . find_one_and_replace ( 
variant_obj , 
return new_variant 
~~ def update_variant_rank ( self , case_obj , variant_type = 'clinical' , category = 'snv' ) : 
variants = self . variant_collection . find ( { 
'variant_type' : variant_type , 
} ) . sort ( 'rank_score' , pymongo . DESCENDING ) 
for index , var_obj in enumerate ( variants ) : 
~~~ if len ( requests ) > 5000 : 
~~~ self . variant_collection . bulk_write ( requests , ordered = False ) 
~~ except BulkWriteError as err : 
raise err 
~~ ~~ operation = pymongo . UpdateOne ( 
{ '_id' : var_obj [ '_id' ] } , 
'variant_rank' : index + 1 , 
~~ def update_variant_compounds ( self , variant , variant_objs = None ) : 
compound_objs = [ ] 
~~~ not_loaded = True 
if variant_objs : 
~~~ variant_obj = variant_objs . get ( compound [ 'variant' ] ) 
~~~ variant_obj = self . variant_collection . find_one ( { '_id' : compound [ 'variant' ] } ) 
~~ if variant_obj : 
~~~ not_loaded = False 
compound [ 'rank_score' ] = variant_obj [ 'rank_score' ] 
for gene in variant_obj . get ( 'genes' , [ ] ) : 
~~~ gene_obj = { 
'hgnc_id' : gene [ 'hgnc_id' ] , 
'hgnc_symbol' : gene . get ( 'hgnc_symbol' ) , 
'region_annotation' : gene . get ( 'region_annotation' ) , 
'functional_annotation' : gene . get ( 'functional_annotation' ) , 
compound [ 'genes' ] = gene_objs 
~~ ~~ compound [ 'not_loaded' ] = not_loaded 
compound_objs . append ( compound ) 
~~ return compound_objs 
~~ def update_compounds ( self , variants ) : 
for var_id in variants : 
~~~ variant_obj = variants [ var_id ] 
if not variant_obj . get ( 'compounds' ) : 
~~ updated_compounds = self . update_variant_compounds ( variant_obj , variants ) 
variant_obj [ 'compounds' ] = updated_compounds 
~~ def update_mongo_compound_variants ( self , bulk ) : 
for var_id in bulk : 
~~~ var_obj = bulk [ var_id ] 
if not var_obj . get ( 'compounds' ) : 
~~ operation = pymongo . UpdateOne ( 
'compounds' : var_obj [ 'compounds' ] 
~~ if not requests : 
~~ ~~ def update_case_compounds ( self , case_obj , build = '37' ) : 
case_id = case_obj [ '_id' ] 
categories = set ( ) 
variant_types = set ( ) 
for file_type in FILE_TYPE_MAP : 
~~~ if case_obj . get ( 'vcf_files' , { } ) . get ( file_type ) : 
~~~ categories . add ( FILE_TYPE_MAP [ file_type ] [ 'category' ] ) 
variant_types . add ( FILE_TYPE_MAP [ file_type ] [ 'variant_type' ] ) 
~~ ~~ coding_intervals = self . get_coding_intervals ( build = build ) 
~~~ intervals = coding_intervals . get ( chrom , IntervalTree ( ) ) 
for var_type in variant_types : 
~~~ for category in categories : 
chrom , var_type , category , case_id ) ) 
'variant_type' : var_type , 
'chrom' : chrom , 
variant_objs = self . variants ( 
query = query , 
nr_of_variants = - 1 , 
sort_key = 'position' 
bulk = { } 
current_region = None 
special = False 
for var_obj in variant_objs : 
~~~ var_id = var_obj [ '_id' ] 
var_chrom = var_obj [ 'chromosome' ] 
var_start = var_obj [ 'position' ] 
var_end = var_obj [ 'end' ] + 1 
update_bulk = True 
new_region = None 
genomic_regions = coding_intervals . get ( var_chrom , IntervalTree ( ) ) . search ( var_start , var_end ) 
if genomic_regions : 
~~~ new_region = genomic_regions . pop ( ) . data 
~~ if new_region and ( new_region == current_region ) : 
~~~ update_bulk = False 
~~ current_region = new_region 
if update_bulk and bulk : 
~~~ self . update_compounds ( bulk ) 
self . update_mongo_compound_variants ( bulk ) 
~~ if new_region : 
~~~ bulk [ var_id ] = var_obj 
~~ ~~ if not bulk : 
~~ self . update_compounds ( bulk ) 
~~ def load_variant ( self , variant_obj ) : 
~~~ result = self . variant_collection . insert_one ( variant_obj ) 
~~ def upsert_variant ( self , variant_obj ) : 
result = self . variant_collection . find_one_and_update ( 
'compounds' : variant_obj . get ( 'compounds' , [ ] ) 
variant = self . variant_collection . find_one ( { '_id' : variant_obj [ '_id' ] } ) 
~~ def load_variant_bulk ( self , variants ) : 
if not len ( variants ) > 0 : 
~~~ result = self . variant_collection . insert_many ( variants ) 
~~~ for var_obj in variants : 
~~~ self . upsert_variant ( var_obj ) 
~~ ~~ ~~ return 
~~ def _load_variants ( self , variants , variant_type , case_obj , individual_positions , rank_threshold , 
institute_id , build = None , rank_results_header = None , vep_header = None , 
category = 'snv' , sample_info = None ) : 
build = build or '37' 
genes = [ gene_obj for gene_obj in self . all_genes ( build = build ) ] 
gene_to_panels = self . gene_to_panels ( case_obj ) 
hgncid_to_gene = self . hgncid_to_gene ( genes = genes ) 
genomic_intervals = self . get_coding_intervals ( genes = genes ) 
start_insertion = datetime . now ( ) 
start_five_thousand = datetime . now ( ) 
nr_variants = 0 
nr_inserted = 0 
inserted = 1 
nr_bulks = 0 
for nr_variants , variant in enumerate ( variants ) : 
~~~ mt_variant = 'MT' in variant . CHROM 
rank_score = parse_rank_score ( variant . INFO . get ( 'RankScore' ) , case_obj [ '_id' ] ) 
if ( rank_score is None ) or ( rank_score > rank_threshold ) or mt_variant : 
~~~ nr_inserted += 1 
parsed_variant = parse_variant ( 
rank_results_header = rank_results_header , 
vep_header = vep_header , 
individual_positions = individual_positions , 
variant_obj = build_variant ( 
variant = parsed_variant , 
gene_to_panels = gene_to_panels , 
hgncid_to_gene = hgncid_to_gene , 
sample_info = sample_info 
var_chrom = variant_obj [ 'chromosome' ] 
var_start = variant_obj [ 'position' ] 
var_end = variant_obj [ 'end' ] + 1 
var_id = variant_obj [ '_id' ] 
load = True 
genomic_regions = genomic_intervals . get ( var_chrom , IntervalTree ( ) ) . search ( var_start , var_end ) 
if new_region == current_region : 
~~~ load = False 
~~~ if not current_region : 
~~ if len ( bulk ) > 10000 : 
~~~ load = True 
~~ ~~ if load : 
~~~ if current_region : 
~~~ self . load_variant_bulk ( list ( bulk . values ( ) ) ) 
nr_bulks += 1 
~~ except IntegrityError as error : 
~~ bulk = { } 
bulk [ var_id ] = variant_obj 
if ( nr_variants != 0 and nr_variants % 5000 == 0 ) : 
( datetime . now ( ) - start_five_thousand ) ) 
~~ if ( nr_inserted != 0 and ( nr_inserted * inserted ) % ( 1000 * inserted ) == 0 ) : 
inserted += 1 
~~ ~~ ~~ if current_region : 
~~ self . load_variant_bulk ( list ( bulk . values ( ) ) ) 
datetime . now ( ) - start_insertion ) ) 
~~~ nr_variants += 1 
return nr_inserted 
~~ def load_variants ( self , case_obj , variant_type = 'clinical' , category = 'snv' , 
rank_threshold = None , chrom = None , start = None , end = None , 
gene_obj = None , build = '37' ) : 
institute_id = self . institute ( institute_id = case_obj [ 'owner' ] ) [ '_id' ] 
variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_str' ) 
~~ elif category == 'cancer' : 
~~~ variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_cancer' ) 
~~~ variant_file = case_obj [ 'vcf_files' ] . get ( 'vcf_cancer_research' ) 
~~~ vcf_obj = VCF ( variant_file ) 
var = next ( vcf_obj ) 
~~ except StopIteration as err : 
rank_results_header = parse_rank_results_header ( vcf_obj ) 
vep_header = parse_vep_header ( vcf_obj ) 
individual_positions = { } 
for i , ind in enumerate ( vcf_obj . samples ) : 
~~~ individual_positions [ ind ] = i 
~~ sample_info = { } 
if category == 'cancer' : 
~~~ for ind in case_obj [ 'individuals' ] : 
~~~ if ind [ 'phenotype' ] == 2 : 
~~~ sample_info [ ind [ 'individual_id' ] ] = 'case' 
~~~ sample_info [ ind [ 'individual_id' ] ] = 'control' 
~~ ~~ ~~ region = "" 
start = max ( gene_obj [ 'start' ] - 5000 , 0 ) 
end = gene_obj [ 'end' ] + 5000 
~~~ rank_threshold = rank_threshold or - 1000 
if not ( start and end ) : 
~~ region = "{0}:{1}-{2}" . format ( chrom , start , end ) 
~~~ rank_threshold = rank_threshold or 0 
~~ variants = vcf_obj ( region ) 
~~~ nr_inserted = self . _load_variants ( 
variants = variants , 
rank_threshold = rank_threshold , 
~~ except Exception as error : 
self . delete_variants ( case_obj [ '_id' ] , variant_type ) 
raise error 
~~ self . update_variant_rank ( case_obj , variant_type , category = category ) 
~~ def assign ( self , institute , case , user , link ) : 
. format ( user [ 'name' ] . encode ( 'utf-8' ) , case [ 'display_name' ] ) ) 
verb = 'assign' , 
. format ( case [ 'display_name' ] , user [ 'name' ] ) ) 
updated_case = self . case_collection . find_one_and_update ( 
{ '$addToSet' : { 'assignees' : user [ '_id' ] } } , 
~~ def share ( self , institute , case , collaborator_id , user , link ) : 
if collaborator_id in case . get ( 'collaborators' , [ ] ) : 
~~ self . create_event ( 
verb = 'share' , 
subject = collaborator_id 
'$push' : { 'collaborators' : collaborator_id } 
~~ def diagnose ( self , institute , case , user , link , level , omim_id , remove = False ) : 
if level == 'phenotype' : 
~~~ case_key = 'diagnosis_phenotypes' 
~~ elif level == 'gene' : 
~~~ case_key = 'diagnosis_genes' 
~~ diagnosis_list = case . get ( case_key , [ ] ) 
omim_number = int ( omim_id . split ( ':' ) [ - 1 ] ) 
updated_case = None 
if remove and omim_number in diagnosis_list : 
{ '$pull' : { case_key : omim_number } } , 
~~ elif omim_number not in diagnosis_list : 
{ '$push' : { case_key : omim_number } } , 
~~ if updated_case : 
~~~ self . create_event ( 
verb = 'update_diagnosis' , 
content = omim_id 
~~ return updated_case 
~~ def mark_checked ( self , institute , case , user , link , 
unmark = False ) : 
. format ( case [ 'display_name' ] ) ) 
verb = 'check_case' , 
subject = status 
. format ( case [ 'display_name' ] , status ) ) 
analysis_checked = False if unmark else True 
'$set' : { 'analysis_checked' : analysis_checked } 
~~ def update_default_panels ( self , institute_obj , case_obj , user_obj , link , panel_objs ) : 
user = user_obj , 
verb = 'update_default_panels' , 
subject = case_obj [ 'display_name' ] , 
panel_ids = [ panel [ '_id' ] for panel in panel_objs ] 
for existing_panel in case_obj [ 'panels' ] : 
~~~ if existing_panel [ 'panel_id' ] in panel_ids : 
~~~ existing_panel [ 'is_default' ] = True 
~~~ existing_panel [ 'is_default' ] = False 
~~ ~~ updated_case = self . case_collection . find_one_and_update ( 
{ '_id' : case_obj [ '_id' ] } , 
'$set' : { 'panels' : case_obj [ 'panels' ] } 
~~ def order_verification ( self , institute , case , user , link , variant ) : 
updated_variant = self . variant_collection . find_one_and_update ( 
{ '_id' : variant [ '_id' ] } , 
{ '$set' : { 'sanger_ordered' : True } } , 
verb = 'sanger' , 
return updated_variant 
~~ def sanger_ordered ( self , institute_id = None , user_id = None ) : 
query = { '$match' : { 
{ 'verb' : 'sanger' } , 
~~~ query [ '$match' ] [ '$and' ] . append ( { 'institute' : institute_id } ) 
~~ if user_id : 
~~~ query [ '$match' ] [ '$and' ] . append ( { 'user_id' : user_id } ) 
~~ results = self . event_collection . aggregate ( [ 
query , 
{ '$group' : { 
'_id' : "$case" , 
'vars' : { '$addToSet' : '$variant_id' } 
sanger_ordered = [ item for item in results ] 
return sanger_ordered 
~~ def validate ( self , institute , case , user , link , variant , validate_type ) : 
if not validate_type in SANGER_OPTIONS : 
~~ updated_variant = self . variant_collection . find_one_and_update ( 
{ '$set' : { 'validation' : validate_type } } , 
verb = 'validate' , 
~~ def mark_causative ( self , institute , case , user , link , variant ) : 
display_name = variant [ 'display_name' ] 
display_name , case [ 'display_name' ] ) ) 
'$push' : { 'causatives' : variant [ '_id' ] } , 
'$set' : { 'status' : 'solved' } 
verb = 'mark_causative' , 
~~ def update_dismiss_variant ( self , institute , case , user , link , variant , 
dismiss_variant ) : 
verb = 'dismiss_variant' , 
if dismiss_variant : 
. format ( dismiss_variant , variant [ 'display_name' ] ) ) 
action = '$set' 
. format ( variant [ 'dismiss_variant' ] , variant [ 'display_name' ] ) ) 
action = '$unset' 
{ action : { 'dismiss_variant' : dismiss_variant } } , 
~~ def update_acmg ( self , institute_obj , case_obj , user_obj , link , variant_obj , acmg_str ) : 
verb = 'acmg' , 
variant = variant_obj , 
subject = variant_obj [ 'display_name' ] , 
if acmg_str is None : 
~~~ updated_variant = self . variant_collection . find_one_and_update ( 
{ '$unset' : { 'acmg_classification' : 1 } } , 
{ '$set' : { 'acmg_classification' : REV_ACMG_MAP [ acmg_str ] } } , 
~~ def parse_ids ( chrom , pos , ref , alt , case_id , variant_type ) : 
ids = { } 
pos = str ( pos ) 
ids [ 'simple_id' ] = parse_simple_id ( chrom , pos , ref , alt ) 
ids [ 'variant_id' ] = parse_variant_id ( chrom , pos , ref , alt , variant_type ) 
ids [ 'display_name' ] = parse_display_name ( chrom , pos , ref , alt , variant_type ) 
ids [ 'document_id' ] = parse_document_id ( chrom , pos , ref , alt , variant_type , case_id ) 
return ids 
~~ def parse_simple_id ( chrom , pos , ref , alt ) : 
return '_' . join ( [ chrom , pos , ref , alt ] ) 
~~ def parse_variant_id ( chrom , pos , ref , alt , variant_type ) : 
return generate_md5_key ( [ chrom , pos , ref , alt , variant_type ] ) 
~~ def parse_display_name ( chrom , pos , ref , alt , variant_type ) : 
return '_' . join ( [ chrom , pos , ref , alt , variant_type ] ) 
~~ def parse_document_id ( chrom , pos , ref , alt , variant_type , case_id ) : 
return generate_md5_key ( [ chrom , pos , ref , alt , variant_type , case_id ] ) 
~~ def convert ( context , panel ) : 
new_header = [ "hgnc_id" , "hgnc_symbol" , "disease_associated_transcripts" , 
"reduced_penetrance" , "genetic_disease_models" , "mosaicism" , 
"database_entry_version" ] 
genes = parse_genes ( panel ) 
adapter . add_hgnc_id ( genes ) 
click . echo ( "#{0}" . format ( '\\t' . join ( new_header ) ) ) 
~~~ print_info = [ ] 
for head in new_header : 
~~~ print_info . append ( str ( gene [ head ] ) if gene . get ( head ) else '' ) 
~~ click . echo ( '\\t' . join ( print_info ) ) 
~~ ~~ ~~ def get_variantid ( variant_obj , family_id ) : 
new_id = parse_document_id ( 
chrom = variant_obj [ 'chromosome' ] , 
pos = str ( variant_obj [ 'position' ] ) , 
ref = variant_obj [ 'reference' ] , 
alt = variant_obj [ 'alternative' ] , 
variant_type = variant_obj [ 'variant_type' ] , 
case_id = family_id , 
return new_id 
~~ def cases ( self , owner = None , collaborator = None , query = None , skip_assigned = False , 
has_causatives = False , reruns = False , finished = False , 
research_requested = False , is_research = False , status = None , 
phenotype_terms = False , pinned = False , cohort = False , name_query = None , 
yield_query = False ) : 
if collaborator and owner : 
~~~ collaborator = None 
~~ if collaborator : 
query [ 'collaborators' ] = collaborator 
~~ if owner : 
query [ 'owner' ] = owner 
~~ if skip_assigned : 
~~~ query [ 'assignees' ] = { '$exists' : False } 
~~ if has_causatives : 
~~~ query [ 'causatives' ] = { '$exists' : True , '$ne' : [ ] } 
~~ if reruns : 
~~~ query [ 'rerun_requested' ] = True 
~~ if status : 
~~~ query [ 'status' ] = status 
~~ elif finished : 
~~~ query [ 'status' ] = { '$in' : [ 'solved' , 'archived' ] } 
~~ if research_requested : 
~~~ query [ 'research_requested' ] = True 
~~ if is_research : 
~~~ query [ 'is_research' ] = { '$exists' : True , '$eq' : True } 
~~ if phenotype_terms : 
~~~ query [ 'phenotype_terms' ] = { '$exists' : True , '$ne' : [ ] } 
~~ if pinned : 
~~~ query [ 'suspects' ] = { '$exists' : True , '$ne' : [ ] } 
~~ if cohort : 
~~~ query [ 'cohorts' ] = { '$exists' : True , '$ne' : [ ] } 
~~ if name_query : 
users = self . user_collection . find ( { 'name' : { '$regex' : name_query , '$options' : 'i' } } ) 
if users . count ( ) > 0 : 
~~~ query [ 'assignees' ] = { '$in' : [ user [ 'email' ] for user in users ] } 
~~ elif name_query . startswith ( 'HP:' ) : 
if name_value : 
~~~ query [ 'phenotype_terms.phenotype_id' ] = name_query 
~~~ query [ '$or' ] = [ { 'phenotype_terms' : { '$size' : 0 } } , { 'phenotype_terms' : { '$exists' : False } } ] 
~~ ~~ elif name_query . startswith ( 'PG:' ) : 
~~~ phenotype_group_query = name_query . replace ( 'PG:' , 'HP:' ) 
query [ 'phenotype_groups.phenotype_id' ] = phenotype_group_query 
~~~ query [ '$or' ] = [ { 'phenotype_groups' : { '$size' : 0 } } , { 'phenotype_groups' : { '$exists' : False } } ] 
~~ ~~ elif name_query . startswith ( 'synopsis:' ) : 
~~~ if name_value : 
~~~ query [ '$text' ] = { '$search' : name_value } 
~~~ query [ 'synopsis' ] = '' 
~~ ~~ elif name_query . startswith ( 'cohort:' ) : 
~~~ query [ 'cohorts' ] = name_value 
~~ elif name_query . startswith ( 'panel:' ) : 
~~~ query [ 'panels' ] = { '$elemMatch' : { 'panel_name' : name_value , 
'is_default' : True } } 
~~ elif name_query . startswith ( 'status:' ) : 
~~~ status_query = name_query . replace ( 'status:' , '' ) 
query [ 'status' ] = status_query 
~~ elif name_query . startswith ( 'is_research' ) : 
~~~ query [ '$or' ] = [ 
{ 'display_name' : { '$regex' : name_query } } , 
{ 'individuals.display_name' : { '$regex' : name_query } } , 
~~ ~~ if yield_query : 
~~~ return query 
return self . case_collection . find ( query ) . sort ( 'updated_at' , - 1 ) 
~~ def nr_cases ( self , institute_id = None ) : 
~~~ query [ 'collaborators' ] = institute_id 
nr_cases = self . case_collection . find ( query ) . count ( ) 
return nr_cases 
~~ def update_dynamic_gene_list ( self , case , hgnc_symbols = None , hgnc_ids = None , 
phenotype_ids = None , build = '37' ) : 
dynamic_gene_list = [ ] 
res = self . hgnc_collection . find ( { 'hgnc_id' : { '$in' : hgnc_ids } , 'build' : build } ) 
~~ elif hgnc_symbols : 
for symbol in hgnc_symbols : 
~~~ for gene_obj in self . gene_by_alias ( symbol = symbol , build = build ) : 
~~~ res . append ( gene_obj ) 
~~ ~~ ~~ for gene_obj in res : 
~~~ dynamic_gene_list . append ( 
'hgnc_symbol' : gene_obj [ 'hgnc_symbol' ] , 
'hgnc_id' : gene_obj [ 'hgnc_id' ] , 
'description' : gene_obj [ 'description' ] , 
{ '$set' : { 'dynamic_gene_list' : dynamic_gene_list , 
'dynamic_panel_phenotypes' : phenotype_ids or [ ] } } , 
~~ def case ( self , case_id = None , institute_id = None , display_name = None ) : 
~~~ query [ '_id' ] = case_id 
~~~ if not ( institute_id and display_name ) : 
query [ 'owner' ] = institute_id 
query [ 'display_name' ] = display_name 
~~ return self . case_collection . find_one ( query ) 
~~ def delete_case ( self , case_id = None , institute_id = None , display_name = None ) : 
~~ result = self . case_collection . delete_one ( query ) 
~~ def load_case ( self , config_data , update = False ) : 
institute_obj = self . institute ( config_data [ 'owner' ] ) 
~~ parsed_case = parse_case ( config = config_data ) 
case_obj = build_case ( parsed_case , self ) 
old_caseid = '-' . join ( [ case_obj [ 'owner' ] , case_obj [ 'display_name' ] ] ) 
old_case = self . case ( old_caseid ) 
if old_case : 
self . update_caseid ( old_case , case_obj [ '_id' ] ) 
update = True 
~~ existing_case = self . case ( case_obj [ '_id' ] ) 
if existing_case and not update : 
{ 'file_name' : 'vcf_snv' , 'variant_type' : 'clinical' , 'category' : 'snv' } , 
{ 'file_name' : 'vcf_sv' , 'variant_type' : 'clinical' , 'category' : 'sv' } , 
{ 'file_name' : 'vcf_cancer' , 'variant_type' : 'clinical' , 'category' : 'cancer' } , 
{ 'file_name' : 'vcf_str' , 'variant_type' : 'clinical' , 'category' : 'str' } 
~~~ for vcf_file in files : 
~~~ if not case_obj [ 'vcf_files' ] . get ( vcf_file [ 'file_name' ] ) : 
~~~ LOG . debug ( "didn\ . format ( vcf_file [ 'file_name' ] ) ) 
~~ variant_type = vcf_file [ 'variant_type' ] 
category = vcf_file [ 'category' ] 
~~~ self . delete_variants ( 
case_id = case_obj [ '_id' ] , 
category = category 
~~ self . load_variants ( 
rank_threshold = case_obj . get ( 'rank_score_threshold' , 0 ) , 
~~ ~~ except ( IntegrityError , ValueError , ConfigError , KeyError ) as error : 
~~~ LOG . warning ( error ) 
~~ if existing_case and update : 
~~~ self . update_case ( case_obj ) 
self . _add_case ( case_obj ) 
~~ def _add_case ( self , case_obj ) : 
if self . case ( case_obj [ '_id' ] ) : 
~~ return self . case_collection . insert_one ( case_obj ) 
~~ def update_case ( self , case_obj ) : 
old_case = self . case_collection . find_one ( 
{ '_id' : case_obj [ '_id' ] } 
'collaborators' : { '$each' : case_obj [ 'collaborators' ] } , 
'analyses' : { 
'date' : old_case [ 'analysis_date' ] , 
'delivery_report' : old_case . get ( 'delivery_report' ) 
'analysis_date' : case_obj [ 'analysis_date' ] , 
'delivery_report' : case_obj . get ( 'delivery_report' ) , 
'individuals' : case_obj [ 'individuals' ] , 
'updated_at' : datetime . datetime . now ( ) , 
'rerun_requested' : False , 
'panels' : case_obj . get ( 'panels' , [ ] ) , 
'genome_build' : case_obj . get ( 'genome_build' , '37' ) , 
'genome_version' : case_obj . get ( 'genome_version' ) , 
'rank_model_version' : case_obj . get ( 'rank_model_version' ) , 
'madeline_info' : case_obj . get ( 'madeline_info' ) , 
'vcf_files' : case_obj . get ( 'vcf_files' ) , 
'has_svvariants' : case_obj . get ( 'has_svvariants' ) , 
'has_strvariants' : case_obj . get ( 'has_strvariants' ) , 
'is_research' : case_obj . get ( 'is_research' , False ) , 
'research_requested' : case_obj . get ( 'research_requested' , False ) , 
'multiqc' : case_obj . get ( 'multiqc' ) , 
'mme_submission' : case_obj . get ( 'mme_submission' ) , 
~~ def replace_case ( self , case_obj ) : 
case_obj [ 'updated_at' ] = datetime . datetime . now ( ) , 
updated_case = self . case_collection . find_one_and_replace ( 
case_obj , 
~~ def update_caseid ( self , case_obj , family_id ) : 
new_case = deepcopy ( case_obj ) 
new_case [ '_id' ] = family_id 
for case_variants in [ 'suspects' , 'causatives' ] : 
~~~ new_variantids = [ ] 
for variant_id in case_obj . get ( case_variants , [ ] ) : 
~~~ case_variant = self . variant ( variant_id ) 
if not case_variant : 
~~ new_variantid = get_variantid ( case_variant , family_id ) 
new_variantids . append ( new_variantid ) 
~~ new_case [ case_variants ] = new_variantids 
~~ for acmg_obj in self . acmg_collection . find ( { 'case_id' : case_obj [ '_id' ] } ) : 
acmg_variant = self . variant ( acmg_obj [ 'variant_specific' ] ) 
new_specific_id = get_variantid ( acmg_variant , family_id ) 
self . acmg_collection . find_one_and_update ( 
{ '_id' : acmg_obj [ '_id' ] } , 
{ '$set' : { 'case_id' : family_id , 'variant_specific' : new_specific_id } } , 
~~ institute_obj = self . institute ( case_obj [ 'owner' ] ) 
for event_obj in self . events ( institute_obj , case = case_obj ) : 
self . event_collection . find_one_and_update ( 
{ '_id' : event_obj [ '_id' ] } , 
{ '$set' : { 'case' : family_id } } , 
~~ self . case_collection . insert_one ( new_case ) 
self . case_collection . find_one_and_delete ( { '_id' : case_obj [ '_id' ] } ) 
return new_case 
~~ def submit_evaluation ( self , variant_obj , user_obj , institute_obj , case_obj , link , criteria ) : 
variant_specific = variant_obj [ '_id' ] 
variant_id = variant_obj [ 'variant_id' ] 
user_id = user_obj [ '_id' ] 
user_name = user_obj . get ( 'name' , user_obj [ '_id' ] ) 
institute_id = institute_obj [ '_id' ] 
evaluation_terms = [ evluation_info [ 'term' ] for evluation_info in criteria ] 
classification = get_acmg ( evaluation_terms ) 
evaluation_obj = build_evaluation ( 
criteria = criteria 
self . _load_evaluation ( evaluation_obj ) 
self . update_acmg ( institute_obj , case_obj , user_obj , link , variant_obj , classification ) 
return classification 
~~ def get_evaluations ( self , variant_obj ) : 
query = dict ( variant_id = variant_obj [ 'variant_id' ] ) 
res = self . acmg_collection . find ( query ) . sort ( [ ( 'created_at' , pymongo . DESCENDING ) ] ) 
~~ def parse_transcripts ( transcript_lines ) : 
if isinstance ( transcript_lines , DataFrame ) : 
~~~ transcripts = parse_ensembl_transcript_request ( transcript_lines ) 
~~~ transcripts = parse_ensembl_transcripts ( transcript_lines ) 
~~ parsed_transcripts = { } 
~~~ tx_id = tx [ 'ensembl_transcript_id' ] 
ens_gene_id = tx [ 'ensembl_gene_id' ] 
if not tx_id in parsed_transcripts : 
~~~ tx_info = { 
'chrom' : tx [ 'chrom' ] , 
'transcript_start' : tx [ 'transcript_start' ] , 
'transcript_end' : tx [ 'transcript_end' ] , 
'mrna' : set ( ) , 
'mrna_predicted' : set ( ) , 
'nc_rna' : set ( ) , 
'ensembl_gene_id' : ens_gene_id , 
'ensembl_transcript_id' : tx_id , 
parsed_transcripts [ tx_id ] = tx_info 
~~ tx_info = parsed_transcripts [ tx_id ] 
if tx . get ( 'refseq_mrna_predicted' ) : 
~~~ tx_info [ 'mrna_predicted' ] . add ( tx [ 'refseq_mrna_predicted' ] ) 
~~ if tx . get ( 'refseq_mrna' ) : 
~~~ tx_info [ 'mrna' ] . add ( tx [ 'refseq_mrna' ] ) 
~~ if tx . get ( 'refseq_ncrna' ) : 
~~~ tx_info [ 'nc_rna' ] . add ( tx [ 'refseq_ncrna' ] ) 
~~ ~~ return parsed_transcripts 
~~ def parse_ensembl_gene_request ( result ) : 
for index , row in result . iterrows ( ) : 
~~~ ensembl_info = { } 
if type ( row [ 'hgnc_symbol' ] ) is float : 
~~ ensembl_info [ 'chrom' ] = row [ 'chromosome_name' ] 
ensembl_info [ 'gene_start' ] = int ( row [ 'start_position' ] ) 
ensembl_info [ 'gene_end' ] = int ( row [ 'end_position' ] ) 
ensembl_info [ 'ensembl_gene_id' ] = row [ 'ensembl_gene_id' ] 
ensembl_info [ 'hgnc_symbol' ] = row [ 'hgnc_symbol' ] 
hgnc_id = row [ 'hgnc_id' ] 
if type ( hgnc_id ) is float : 
~~~ hgnc_id = int ( hgnc_id ) 
~~~ hgnc_id = int ( hgnc_id . split ( ':' ) [ - 1 ] ) 
~~ ensembl_info [ 'hgnc_id' ] = hgnc_id 
yield ensembl_info 
~~ ~~ def parse_ensembl_transcript_request ( result ) : 
keys = [ 
'chrom' , 
'ensembl_gene_id' , 
'ensembl_transcript_id' , 
'transcript_start' , 
'transcript_end' , 
'refseq_mrna' , 
'refseq_mrna_predicted' , 
'refseq_ncrna' , 
ensembl_info [ 'chrom' ] = str ( row [ 'chromosome_name' ] ) 
ensembl_info [ 'ensembl_transcript_id' ] = row [ 'ensembl_transcript_id' ] 
ensembl_info [ 'transcript_start' ] = int ( row [ 'transcript_start' ] ) 
ensembl_info [ 'transcript_end' ] = int ( row [ 'transcript_end' ] ) 
for key in keys [ - 3 : ] : 
~~~ if type ( row [ key ] ) is float : 
~~~ ensembl_info [ key ] = None 
~~~ ensembl_info [ key ] = row [ key ] 
~~ ~~ yield ensembl_info 
~~ ~~ def parse_ensembl_line ( line , header ) : 
header = [ head . lower ( ) for head in header ] 
ensembl_info = { } 
for word in raw_info : 
~~~ value = raw_info [ word ] 
~~ if 'chromosome' in word : 
~~~ ensembl_info [ 'chrom' ] = value 
~~ if 'gene' in word : 
~~~ if 'id' in word : 
~~~ ensembl_info [ 'ensembl_gene_id' ] = value 
~~ elif 'start' in word : 
~~~ ensembl_info [ 'gene_start' ] = int ( value ) 
~~ elif 'end' in word : 
~~~ ensembl_info [ 'gene_end' ] = int ( value ) 
~~~ ensembl_info [ 'hgnc_symbol' ] = value 
~~~ ensembl_info [ 'hgnc_id' ] = int ( value . split ( ':' ) [ - 1 ] ) 
~~ if 'transcript' in word : 
~~~ ensembl_info [ 'ensembl_transcript_id' ] = value 
~~~ ensembl_info [ 'transcript_start' ] = int ( value ) 
~~~ ensembl_info [ 'transcript_end' ] = int ( value ) 
~~ ~~ if 'exon' in word : 
~~~ if 'start' in word : 
~~~ ensembl_info [ 'exon_start' ] = int ( value ) 
~~~ ensembl_info [ 'exon_end' ] = int ( value ) 
~~ elif 'rank' in word : 
~~~ ensembl_info [ 'exon_rank' ] = int ( value ) 
~~ ~~ if 'utr' in word : 
~~~ if '5' in word : 
~~~ ensembl_info [ 'utr_5_start' ] = int ( value ) 
~~ elif '3' in word : 
~~~ ensembl_info [ 'utr_3_start' ] = int ( value ) 
~~ ~~ elif 'end' in word : 
~~~ ensembl_info [ 'utr_5_end' ] = int ( value ) 
~~~ ensembl_info [ 'utr_3_end' ] = int ( value ) 
~~ ~~ ~~ if 'strand' in word : 
~~~ ensembl_info [ 'strand' ] = int ( value ) 
~~ if 'refseq' in word : 
~~~ if 'mrna' in word : 
~~~ if 'predicted' in word : 
~~~ ensembl_info [ 'refseq_mrna_predicted' ] = value 
~~~ ensembl_info [ 'refseq_mrna' ] = value 
~~ ~~ if 'ncrna' in word : 
~~~ ensembl_info [ 'refseq_ncrna' ] = value 
~~ ~~ ~~ return ensembl_info 
~~ def parse_ensembl_genes ( lines ) : 
~~~ header = line . rstrip ( ) . split ( '\\t' ) 
~~ yield parse_ensembl_line ( line , header ) 
~~ ~~ def parse_ensembl_exons ( lines ) : 
~~ exon_info = parse_ensembl_line ( line , header ) 
chrom = exon_info [ 'chrom' ] 
start = exon_info [ 'exon_start' ] 
end = exon_info [ 'exon_end' ] 
transcript = exon_info [ 'ensembl_transcript_id' ] 
gene = exon_info [ 'ensembl_gene_id' ] 
rank = exon_info [ 'exon_rank' ] 
strand = exon_info [ 'strand' ] 
if strand == 1 : 
~~~ start = max ( start , exon_info . get ( 'utr_5_end' ) or - 1 ) 
end = min ( end , exon_info . get ( 'utr_3_start' ) or float ( 'inf' ) ) 
~~ elif strand == - 1 : 
~~~ start = max ( start , exon_info . get ( 'utr_3_end' ) or - 1 ) 
end = min ( end , exon_info . get ( 'utr_5_start' ) or float ( 'inf' ) ) 
~~ exon_id = "-" . join ( [ chrom , str ( start ) , str ( end ) ] ) 
if start > end : 
"exon_id" : exon_id , 
"chrom" : chrom , 
"start" : start , 
"end" : end , 
"transcript" : transcript , 
"gene" : gene , 
"rank" : rank , 
yield data 
~~ ~~ def parse_ensembl_exon_request ( result ) : 
'gene' , 
'transcript' , 
'exon_id' , 
'exon_chrom_start' , 
'exon_chrom_end' , 
'5_utr_start' , 
'5_utr_end' , 
'3_utr_start' , 
'3_utr_end' , 
'strand' , 
'rank' 
result [ "5\ ] , 
result [ "3\ ] , 
result [ "Strand" ] , 
~~~ ensembl_info = dict ( zip ( keys , res ) ) 
if ensembl_info [ 'strand' ] == 1 : 
~~~ start = max ( ensembl_info [ 'exon_chrom_start' ] , ensembl_info [ '5_utr_end' ] or - 1 ) 
end = min ( ensembl_info [ 'exon_chrom_end' ] , ensembl_info [ '3_utr_start' ] or float ( 'inf' ) ) 
~~ elif ensembl_info [ 'strand' ] == - 1 : 
~~~ start = max ( ensembl_info [ 'exon_chrom_start' ] , ensembl_info [ '3_utr_end' ] or - 1 ) 
end = min ( ensembl_info [ 'exon_chrom_end' ] , ensembl_info [ '5_utr_start' ] or float ( 'inf' ) ) 
~~ ensembl_info [ 'start' ] = start 
ensembl_info [ 'end' ] = end 
~~ ~~ def init_log ( logger , filename = None , loglevel = None ) : 
formatter = logging . Formatter ( template ) 
if loglevel : 
~~~ logger . setLevel ( getattr ( logging , loglevel ) ) 
~~ console = logging . StreamHandler ( ) 
console . setLevel ( 'WARNING' ) 
console . setFormatter ( formatter ) 
if filename : 
~~~ file_handler = logging . FileHandler ( filename , encoding = 'utf-8' ) 
~~~ file_handler . setLevel ( getattr ( logging , loglevel ) ) 
~~ file_handler . setFormatter ( formatter ) 
logger . addHandler ( file_handler ) 
~~~ if loglevel : 
~~~ console . setLevel ( getattr ( logging , loglevel ) ) 
~~ ~~ logger . addHandler ( console ) 
~~ def parse_omim_line ( line , header ) : 
omim_info = dict ( zip ( header , line . split ( '\\t' ) ) ) 
return omim_info 
~~ def parse_genemap2 ( lines ) : 
for i , line in enumerate ( lines ) : 
~~~ line = line . rstrip ( ) 
if line . startswith ( '#' ) : 
~~~ if i < 10 : 
~~~ header = line [ 2 : ] . split ( '\\t' ) 
~~ ~~ continue 
~~ parsed_entry = parse_omim_line ( line , header ) 
parsed_entry [ 'raw' ] = line 
~~ parsed_entry [ 'hgnc_symbols' ] = gene_symbols 
if not hgnc_symbol and gene_symbols : 
~~~ hgnc_symbol = gene_symbols [ 0 ] 
~~ parsed_entry [ 'hgnc_symbol' ] = hgnc_symbol 
gene_inheritance = set ( ) 
parsed_phenotypes = [ ] 
for phenotype_info in parsed_entry . get ( 'Phenotypes' , '' ) . split ( ';' ) : 
~~~ if not phenotype_info : 
~~ phenotype_info = phenotype_info . lstrip ( ) 
phenotype_status = OMIM_STATUS_MAP . get ( phenotype_info [ 0 ] , 'established' ) 
if phenotype_status == 'nondisease' : 
~~ phenotype_description = "" 
splitted_info = phenotype_info . split ( ',' ) 
for i , text in enumerate ( splitted_info ) : 
~~~ match = entry_pattern . search ( text ) 
if not match : 
~~~ phenotype_description += text 
~~~ mimnr_match = mimnr_pattern . search ( phenotype_info ) 
if mimnr_match : 
~~~ phenotype_mim = int ( mimnr_match . group ( ) ) 
~~~ phenotype_mim = parsed_entry [ 'mim_number' ] 
phenotype_description += text [ : - 4 ] 
~~ break 
~~ ~~ inheritance = set ( ) 
inheritance_text = ',' . join ( splitted_info [ i : ] ) 
for term in mim_inheritance_terms : 
~~~ if term in inheritance_text : 
~~~ inheritance . add ( TERMS_MAPPER [ term ] ) 
gene_inheritance . add ( TERMS_MAPPER [ term ] ) 
~~ ~~ parsed_phenotypes . append ( 
'mim_number' : phenotype_mim , 
'inheritance' : inheritance , 
'description' : phenotype_description . strip ( '?\\{\\}' ) , 
'status' : phenotype_status , 
~~ parsed_entry [ 'phenotypes' ] = parsed_phenotypes 
parsed_entry [ 'inheritance' ] = gene_inheritance 
yield parsed_entry 
~~ ~~ def parse_mim2gene ( lines ) : 
header = [ "mim_number" , "entry_type" , "entrez_gene_id" , "hgnc_symbol" , "ensembl_gene_id" ] 
~~ if not len ( line ) > 0 : 
parsed_entry = parse_omim_line ( line , header ) 
parsed_entry [ 'mim_number' ] = int ( parsed_entry [ 'mim_number' ] ) 
if 'hgnc_symbol' in parsed_entry : 
~~~ parsed_entry [ 'hgnc_symbol' ] = parsed_entry [ 'hgnc_symbol' ] 
~~ if parsed_entry . get ( 'entrez_gene_id' ) : 
~~~ parsed_entry [ 'entrez_gene_id' ] = int ( parsed_entry [ 'entrez_gene_id' ] ) 
~~ if parsed_entry . get ( 'ensembl_gene_id' ) : 
~~~ ensembl_info = parsed_entry [ 'ensembl_gene_id' ] . split ( ',' ) 
parsed_entry [ 'ensembl_gene_id' ] = ensembl_info [ 0 ] . strip ( ) 
if len ( ensembl_info ) > 1 : 
~~~ parsed_entry [ 'ensembl_transcript_id' ] = ensembl_info [ 1 ] . strip ( ) 
~~ ~~ yield parsed_entry 
~~ ~~ def parse_omim_morbid ( lines ) : 
~~~ yield parse_omim_line ( line , header ) 
~~ ~~ ~~ def parse_mim_titles ( lines ) : 
header = [ 'prefix' , 'mim_number' , 'preferred_title' , 'alternative_title' , 'included_title' ] 
if not line . startswith ( '#' ) : 
~~~ parsed_entry = parse_omim_line ( line , header ) 
parsed_entry [ 'preferred_title' ] = parsed_entry [ 'preferred_title' ] . split ( ';' ) [ 0 ] 
~~ ~~ ~~ def get_mim_genes ( genemap_lines , mim2gene_lines ) : 
hgnc_genes = { } 
gene_nr = 0 
no_hgnc = 0 
for entry in parse_mim2gene ( mim2gene_lines ) : 
~~~ if 'gene' in entry [ 'entry_type' ] : 
~~~ mim_nr = entry [ 'mim_number' ] 
gene_nr += 1 
if not 'hgnc_symbol' in entry : 
~~~ no_hgnc += 1 
~~~ genes [ mim_nr ] = entry 
for entry in parse_genemap2 ( genemap_lines ) : 
~~~ mim_number = entry [ 'mim_number' ] 
inheritance = entry [ 'inheritance' ] 
phenotype_info = entry [ 'phenotypes' ] 
hgnc_symbol = entry [ 'hgnc_symbol' ] 
hgnc_symbols = entry [ 'hgnc_symbols' ] 
if mim_number in genes : 
~~~ genes [ mim_number ] [ 'inheritance' ] = inheritance 
genes [ mim_number ] [ 'phenotypes' ] = phenotype_info 
genes [ mim_number ] [ 'hgnc_symbols' ] = hgnc_symbols 
~~ ~~ for mim_nr in genes : 
~~~ gene_info = genes [ mim_nr ] 
if hgnc_symbol in hgnc_genes : 
~~~ existing_info = hgnc_genes [ hgnc_symbol ] 
if not existing_info [ 'phenotypes' ] : 
~~~ hgnc_genes [ hgnc_symbol ] = gene_info 
~~ ~~ return hgnc_genes 
~~ def get_mim_phenotypes ( genemap_lines ) : 
phenotype_mims = set ( ) 
phenotypes_found = { } 
~~~ hgnc_symbol = entry [ 'hgnc_symbol' ] 
for phenotype in entry [ 'phenotypes' ] : 
~~~ mim_nr = phenotype [ 'mim_number' ] 
if mim_nr in phenotypes_found : 
~~~ phenotype_entry = phenotypes_found [ mim_nr ] 
phenotype_entry [ 'inheritance' ] = phenotype_entry [ 'inheritance' ] . union ( phenotype [ 'inheritance' ] ) 
phenotype_entry [ 'hgnc_symbols' ] . add ( hgnc_symbol ) 
~~~ phenotype [ 'hgnc_symbols' ] = set ( [ hgnc_symbol ] ) 
phenotypes_found [ mim_nr ] = phenotype 
~~ ~~ ~~ return phenotypes_found 
~~ def cli ( context , morbid , genemap , mim2gene , mim_titles , phenotypes ) : 
from scout . utils . handle import get_file_handle 
from pprint import pprint as pp 
if morbid : 
~~~ morbid_handle = get_file_handle ( morbid ) 
~~ if genemap : 
~~~ genemap_handle = get_file_handle ( genemap ) 
~~ if mim2gene : 
~~~ mim2gene_handle = get_file_handle ( mim2gene ) 
~~ if mim_titles : 
~~~ mimtitles_handle = get_file_handle ( mim_titles ) 
~~ mim_genes = get_mim_genes ( genemap_handle , mim2gene_handle ) 
for entry in mim_genes : 
~~~ if entry == 'C10orf11' : 
~~~ pp ( mim_genes [ entry ] ) 
~~ ~~ context . abort ( ) 
if phenotypes : 
~~~ if not genemap : 
~~ phenotypes = get_mim_phenotypes ( genemap_handle ) 
for i , mim_term in enumerate ( phenotypes ) : 
genes = get_mim_genes ( genemap_handle , mim2gene_handle ) 
~~~ if hgnc_symbol == 'OPA1' : 
~~~ print ( genes [ hgnc_symbol ] ) 
~~ ~~ ~~ def convert_number ( string ) : 
res = None 
if isint ( string ) : 
~~~ res = int ( string ) 
~~ elif isfloat ( string ) : 
~~~ res = float ( string ) 
~~ def case ( context , case_id , case_name , institute , collaborator , vcf , vcf_sv , 
vcf_cancer , vcf_research , vcf_sv_research , vcf_cancer_research , peddy_ped , 
reupload_sv , rankscore_treshold , rankmodel_version ) : 
if not case_id : 
~~~ if not ( case_name and institute ) : 
context . abort 
~~ case_id = "{0}-{1}" . format ( institute , case_name ) 
~~ case_obj = adapter . case ( case_id ) 
~~ case_changed = False 
if collaborator : 
~~~ if not adapter . institute ( collaborator ) : 
~~ if not collaborator in case_obj [ 'collaborators' ] : 
~~~ case_changed = True 
case_obj [ 'collaborators' ] . append ( collaborator ) 
~~ ~~ if vcf : 
case_obj [ 'vcf_files' ] [ 'vcf_snv' ] = vcf 
case_changed = True 
~~ if vcf_sv : 
case_obj [ 'vcf_files' ] [ 'vcf_sv' ] = vcf_sv 
~~ if vcf_cancer : 
case_obj [ 'vcf_files' ] [ 'vcf_cancer' ] = vcf_cancer 
~~ if vcf_research : 
case_obj [ 'vcf_files' ] [ 'vcf_research' ] = vcf_research 
~~ if vcf_sv_research : 
case_obj [ 'vcf_files' ] [ 'vcf_sv_research' ] = vcf_sv_research 
~~ if vcf_cancer_research : 
case_obj [ 'vcf_files' ] [ 'vcf_cancer_research' ] = vcf_cancer_research 
~~ if case_changed : 
~~ if reupload_sv : 
updates = { 'needs_check' : True } 
if rankscore_treshold : 
~~~ updates [ 'sv_rank_model_version' ] = rankmodel_version 
~~~ updates [ 'vcf_files.vcf_sv' ] = vcf_sv 
~~~ updates [ 'vcf_files.vcf_sv_research' ] = vcf_sv_research 
~~ updated_case = adapter . case_collection . find_one_and_update ( 
{ '_id' : case_id } , 
{ '$set' : updates 
rankscore_treshold = rankscore_treshold or updated_case . get ( "rank_score_threshold" , 5 ) 
if updated_case [ 'vcf_files' ] . get ( 'vcf_sv' ) : 
~~~ adapter . delete_variants ( case_id , variant_type = 'clinical' , category = 'sv' ) 
adapter . load_variants ( updated_case , variant_type = 'clinical' , 
category = 'sv' , rank_threshold = rankscore_treshold ) 
~~ if updated_case [ 'vcf_files' ] . get ( 'vcf_sv_research' ) : 
~~~ adapter . delete_variants ( case_id , variant_type = 'research' , category = 'sv' ) 
if updated_case . get ( 'is_research' ) : 
~~~ adapter . load_variants ( updated_case , variant_type = 'research' , 
user_mail = 'clark.kent@mail.com' , api_key = None , demo = False ) : 
for collection_name in adapter . db . collection_names ( ) : 
~~~ if not collection_name . startswith ( 'system' ) : 
adapter . db . drop_collection ( collection_name ) 
##################################################################### 
display_name = institute_id , 
sanger_recipients = [ user_mail ] 
user_obj = dict ( 
_id = user_mail , 
email = user_mail , 
name = user_name , 
roles = [ 'admin' ] , 
institutes = [ institute_id ] 
adapter . add_user ( user_obj ) 
if not demo : 
~~ mim2gene_lines = mim_files [ 'mim2genes' ] 
hpo_gene_lines = fetch_hpo_genes ( ) 
hgnc_lines = fetch_hgnc ( ) 
~~~ mim2gene_lines = [ line for line in get_file_handle ( mim2gene_reduced_path ) ] 
genemap_lines = [ line for line in get_file_handle ( genemap2_reduced_path ) ] 
hpo_gene_lines = [ line for line in get_file_handle ( hpogenes_reduced_path ) ] 
hgnc_lines = [ line for line in get_file_handle ( hgnc_reduced_path ) ] 
exac_lines = [ line for line in get_file_handle ( exac_reduced_path ) ] 
~~ builds = [ '37' , '38' ] 
~~~ if not demo : 
~~~ ensembl_genes = get_file_handle ( genes37_reduced_path ) 
~~ hgnc_genes = load_hgnc_genes ( 
hpo_lines = hpo_gene_lines , 
~~ if not demo : 
~~~ ensembl_transcripts = fetch_ensembl_transcripts ( build = build ) 
~~~ ensembl_transcripts = get_file_handle ( transcripts37_reduced_path ) 
~~ transcripts = load_transcripts ( adapter , ensembl_transcripts , build , ensembl_genes ) 
~~ hpo_terms_handle = None 
hpo_to_genes_handle = None 
hpo_disease_handle = None 
if demo : 
~~~ hpo_terms_handle = get_file_handle ( hpoterms_reduced_path ) 
hpo_to_genes_handle = get_file_handle ( hpo_to_genes_reduced_path ) 
hpo_disease_handle = get_file_handle ( hpo_phenotype_to_terms_reduced_path ) 
~~ load_hpo ( 
hpo_lines = hpo_terms_handle , 
hpo_gene_lines = hpo_to_genes_handle , 
disease_lines = genemap_lines , 
hpo_disease_lines = hpo_disease_handle 
~~~ parsed_panel = parse_gene_panel ( 
path = panel_path , 
institute = 'cust000' , 
panel_id = 'panel1' , 
adapter . load_panel ( parsed_panel ) 
case_handle = get_file_handle ( load_path ) 
case_data = yaml . load ( case_handle ) 
adapter . load_case ( case_data ) 
adapter . load_indexes ( ) 
~~ def export_transcripts ( adapter , build = '37' ) : 
for tx_obj in adapter . transcripts ( build = build ) : 
~~~ yield tx_obj 
~~ ~~ def get_panel_info ( panel_lines = None , panel_id = None , institute = None , version = None , date = None , 
display_name = None ) : 
panel_info = { 
'panel_id' : panel_id , 
'institute' : institute , 
'version' : version , 
'date' : date , 
'display_name' : display_name , 
if panel_lines : 
~~~ for line in panel_lines : 
if not line . startswith ( '##' ) : 
~~~ break 
~~ info = line [ 2 : ] . split ( '=' ) 
field = info [ 0 ] 
value = info [ 1 ] 
if not panel_info . get ( field ) : 
~~~ panel_info [ field ] = value 
~~ ~~ ~~ panel_info [ 'date' ] = get_date ( panel_info [ 'date' ] ) 
return panel_info 
~~ def parse_gene ( gene_info ) : 
gene = { } 
identifier = None 
hgnc_id = None 
~~~ if 'hgnc_id' in gene_info : 
~~ elif 'hgnc_idnumber' in gene_info : 
~~~ hgnc_id = int ( gene_info [ 'hgnc_idnumber' ] ) 
~~ elif 'hgncid' in gene_info : 
~~~ hgnc_id = int ( gene_info [ 'hgncid' ] ) 
~~ gene [ 'hgnc_id' ] = hgnc_id 
identifier = hgnc_id 
hgnc_symbol = None 
if 'hgnc_symbol' in gene_info : 
~~ elif 'hgncsymbol' in gene_info : 
~~~ hgnc_symbol = gene_info [ 'hgncsymbol' ] 
~~ elif 'symbol' in gene_info : 
~~~ hgnc_symbol = gene_info [ 'symbol' ] 
~~ gene [ 'hgnc_symbol' ] = hgnc_symbol 
if not identifier : 
~~~ identifier = hgnc_symbol 
~~ ~~ gene [ 'identifier' ] = identifier 
transcripts = "" 
if 'disease_associated_transcripts' in gene_info : 
~~~ transcripts = gene_info [ 'disease_associated_transcripts' ] 
~~ elif 'disease_associated_transcript' in gene_info : 
~~~ transcripts = gene_info [ 'disease_associated_transcript' ] 
~~ elif 'transcripts' in gene_info : 
~~~ transcripts = gene_info [ 'transcripts' ] 
~~ gene [ 'transcripts' ] = [ 
transcript . strip ( ) for transcript in 
transcripts . split ( ',' ) if transcript 
models = "" 
if 'genetic_disease_models' in gene_info : 
~~~ models = gene_info [ 'genetic_disease_models' ] 
~~ elif 'genetic_disease_model' in gene_info : 
~~~ models = gene_info [ 'genetic_disease_model' ] 
~~ elif 'inheritance_models' in gene_info : 
~~~ models = gene_info [ 'inheritance_models' ] 
~~ elif 'genetic_inheritance_models' in gene_info : 
~~~ models = gene_info [ 'genetic_inheritance_models' ] 
~~ gene [ 'inheritance_models' ] = [ 
model . strip ( ) for model in models . split ( ',' ) 
if model . strip ( ) in VALID_MODELS 
gene [ 'mosaicism' ] = True if gene_info . get ( 'mosaicism' ) else False 
gene [ 'reduced_penetrance' ] = True if gene_info . get ( 'reduced_penetrance' ) else False 
gene [ 'database_entry_version' ] = gene_info . get ( 'database_entry_version' ) 
return gene 
~~ def parse_genes ( gene_lines ) : 
hgnc_identifiers = set ( ) 
delimiter = '\\t' 
for i , line in enumerate ( gene_lines ) : 
if not len ( line ) > 0 : 
~~ if line . startswith ( '#' ) : 
~~~ if not line . startswith ( '##' ) : 
~~~ line_length = 0 
delimiter = None 
for alt in delimiters : 
~~~ head_line = line . split ( alt ) 
if len ( head_line ) > line_length : 
~~~ line_length = len ( head_line ) 
delimiter = alt 
~~ ~~ header = [ word . lower ( ) for word in line [ 1 : ] . split ( delimiter ) ] 
~~~ if i == 0 : 
~~ ~~ if ( 'hgnc' in line or 'HGNC' in line ) : 
~~~ header = [ word . lower ( ) for word in line . split ( delimiter ) ] 
~~ if line . split ( delimiter ) [ 0 ] . isdigit ( ) : 
~~~ header = [ 'hgnc_id' ] 
~~~ header = [ 'hgnc_symbol' ] 
~~ ~~ splitted_line = line . split ( delimiter ) 
gene_info = dict ( zip ( header , splitted_line ) ) 
info_found = False 
for key in gene_info : 
~~~ if gene_info [ key ] : 
~~~ info_found = True 
~~ ~~ if not info_found : 
~~~ gene = parse_gene ( gene_info ) 
~~ identifier = gene . pop ( 'identifier' ) 
if not identifier in hgnc_identifiers : 
~~~ hgnc_identifiers . add ( identifier ) 
~~ ~~ ~~ return genes 
~~ def parse_gene_panel ( path , institute = 'cust000' , panel_id = 'test' , panel_type = 'clinical' , date = datetime . now ( ) , 
version = 1.0 , display_name = None , genes = None ) : 
gene_panel = { } 
gene_panel [ 'path' ] = path 
gene_panel [ 'type' ] = panel_type 
gene_panel [ 'date' ] = date 
gene_panel [ 'panel_id' ] = panel_id 
gene_panel [ 'institute' ] = institute 
version = version or 1.0 
gene_panel [ 'version' ] = float ( version ) 
gene_panel [ 'display_name' ] = display_name or panel_id 
if not path : 
~~~ panel_handle = genes 
~~~ panel_handle = get_file_handle ( gene_panel [ 'path' ] ) 
~~ gene_panel [ 'genes' ] = parse_genes ( gene_lines = panel_handle ) 
return gene_panel 
~~ def parse_panel_app_gene ( app_gene , hgnc_map ) : 
gene_info = { } 
confidence_level = app_gene [ 'LevelOfConfidence' ] 
if not confidence_level == 'HighEvidence' : 
~~~ return gene_info 
~~ hgnc_symbol = app_gene [ 'GeneSymbol' ] 
hgnc_ids = get_correct_ids ( hgnc_symbol , hgnc_map ) 
if not hgnc_ids : 
return gene_info 
~~ if len ( hgnc_ids ) > 1 : 
~~ gene_info [ 'hgnc_symbol' ] = hgnc_symbol 
for hgnc_id in hgnc_ids : 
~~~ gene_info [ 'hgnc_id' ] = hgnc_id 
~~ gene_info [ 'reduced_penetrance' ] = INCOMPLETE_PENETRANCE_MAP . get ( app_gene [ 'Penetrance' ] ) 
inheritance_models = [ ] 
for model in MODELS_MAP . get ( app_gene [ 'ModeOfInheritance' ] , [ ] ) : 
~~~ inheritance_models . append ( model ) 
~~ gene_info [ 'inheritance_models' ] = inheritance_models 
~~ def parse_panel_app_panel ( panel_info , hgnc_map , institute = 'cust000' , panel_type = 'clinical' ) : 
date_format = "%Y-%m-%dT%H:%M:%S.%f" 
gene_panel [ 'version' ] = float ( panel_info [ 'version' ] ) 
gene_panel [ 'date' ] = get_date ( panel_info [ 'Created' ] [ : - 1 ] , date_format = date_format ) 
gene_panel [ 'display_name' ] = panel_info [ 'SpecificDiseaseName' ] 
gene_panel [ 'panel_type' ] = panel_type 
gene_panel [ 'genes' ] = [ ] 
nr_low_confidence = 1 
nr_genes = 0 
for nr_genes , gene in enumerate ( panel_info [ 'Genes' ] , 1 ) : 
~~~ gene_info = parse_panel_app_gene ( gene , hgnc_map ) 
~~~ nr_low_confidence += 1 
~~ gene_panel [ 'genes' ] . append ( gene_info ) 
~~ def get_omim_panel_genes ( genemap2_lines , mim2gene_lines , alias_genes ) : 
parsed_genes = get_mim_genes ( genemap2_lines , mim2gene_lines ) 
STATUS_TO_ADD = set ( [ 'established' , 'provisional' ] ) 
for hgnc_symbol in parsed_genes : 
~~~ gene = parsed_genes [ hgnc_symbol ] 
keep = False 
for phenotype_info in gene . get ( 'phenotypes' , [ ] ) : 
~~~ if phenotype_info [ 'status' ] in STATUS_TO_ADD : 
~~~ keep = True 
~~ ~~ if keep : 
~~~ hgnc_id_info = alias_genes . get ( hgnc_symbol ) 
if not hgnc_id_info : 
~~~ for symbol in gene . get ( 'hgnc_symbols' , [ ] ) : 
~~~ if symbol in alias_genes : 
~~~ hgnc_id_info = alias_genes [ symbol ] 
~~ ~~ ~~ if hgnc_id_info : 
~~~ yield { 
'hgnc_id' : hgnc_id_info [ 'true' ] , 
~~ ~~ ~~ except KeyError : 
~~ ~~ ~~ def diseases ( context ) : 
disease_objs = adapter . disease_terms ( ) 
nr_diseases = disease_objs . count ( ) 
if nr_diseases == 0 : 
~~~ click . echo ( "Disease" ) 
for disease_obj in adapter . disease_terms ( ) : 
~~~ click . echo ( "{0}" . format ( disease_obj [ '_id' ] ) ) 
~~ ~~ def hpo ( context ) : 
adapter . hpo_term_collection . drop ( ) 
load_hpo_terms ( adapter ) 
~~ def users ( store ) : 
user_objs = list ( store . users ( ) ) 
total_events = store . user_events ( ) . count ( ) 
~~~ if user_obj . get ( 'institutes' ) : 
~~~ user_obj [ 'institutes' ] = [ store . institute ( inst_id ) for inst_id in user_obj . get ( 'institutes' ) ] 
~~~ user_obj [ 'institutes' ] = [ ] 
~~ user_obj [ 'events' ] = store . user_events ( user_obj ) . count ( ) 
user_obj [ 'events_rank' ] = event_rank ( user_obj [ 'events' ] ) 
~~ return dict ( 
users = sorted ( user_objs , key = lambda user : - user [ 'events' ] ) , 
total_events = total_events , 
~~ def parse_conservations ( variant ) : 
conservations = { } 
conservations [ 'gerp' ] = parse_conservation ( 
variant , 
'dbNSFP_GERP___RS' 
conservations [ 'phast' ] = parse_conservation ( 
'dbNSFP_phastCons100way_vertebrate' 
conservations [ 'phylop' ] = parse_conservation ( 
'dbNSFP_phyloP100way_vertebrate' 
return conservations 
~~ def parse_conservation ( variant , info_key ) : 
raw_score = variant . INFO . get ( info_key ) 
conservations = [ ] 
if raw_score : 
~~~ if isinstance ( raw_score , numbers . Number ) : 
~~~ raw_score = ( raw_score , ) 
~~ for score in raw_score : 
~~~ if score >= CONSERVATION [ info_key ] [ 'conserved_min' ] : 
~~~ conservations . append ( 'Conserved' ) 
~~~ conservations . append ( 'NotConserved' ) 
~~ ~~ ~~ return conservations 
~~ def get_dashboard_info ( adapter , institute_id = None , slice_query = None ) : 
if institute_id == 'None' : 
~~~ institute_id = None 
~~ general_sliced_info = get_general_case_info ( adapter , institute_id = institute_id , 
slice_query = slice_query ) 
total_sliced_cases = general_sliced_info [ 'total_cases' ] 
data = { 'total_cases' : total_sliced_cases } 
if total_sliced_cases == 0 : 
~~~ return data 
~~ data [ 'pedigree' ] = [ ] 
for ped_info in general_sliced_info [ 'pedigree' ] . values ( ) : 
~~~ ped_info [ 'percent' ] = ped_info [ 'count' ] / total_sliced_cases 
data [ 'pedigree' ] . append ( ped_info ) 
~~ data [ 'cases' ] = get_case_groups ( adapter , total_sliced_cases , 
institute_id = institute_id , slice_query = slice_query ) 
data [ 'analysis_types' ] = get_analysis_types ( adapter , total_sliced_cases , 
overview = [ 
'count' : general_sliced_info [ 'phenotype_cases' ] , 
'percent' : general_sliced_info [ 'phenotype_cases' ] / total_sliced_cases , 
'count' : general_sliced_info [ 'causative_cases' ] , 
'percent' : general_sliced_info [ 'causative_cases' ] / total_sliced_cases , 
'count' : general_sliced_info [ 'pinned_cases' ] , 
'percent' : general_sliced_info [ 'pinned_cases' ] / total_sliced_cases , 
'count' : general_sliced_info [ 'cohort_cases' ] , 
'percent' : general_sliced_info [ 'cohort_cases' ] / total_sliced_cases , 
general_info = get_general_case_info ( adapter , institute_id = institute_id ) 
total_cases = general_info [ 'total_cases' ] 
sliced_case_ids = general_sliced_info [ 'case_ids' ] 
verified_query = { 
~~~ verified_query [ 'institute' ] = institute_id 
~~ sliced_validation_cases = set ( ) 
sliced_validated_cases = set ( ) 
validated_tp = set ( ) 
validated_fp = set ( ) 
validate_events = adapter . event_collection . find ( verified_query ) 
for validate_event in list ( validate_events ) : 
~~~ case_id = validate_event . get ( 'case' ) 
var_obj = adapter . variant ( case_id = case_id , document_id = validate_event [ 'variant_id' ] ) 
~~~ var_valid_orders += 1 
if case_id in sliced_case_ids : 
~~ validation = var_obj . get ( 'validation' ) 
~~~ if case_id in sliced_case_ids : 
~~~ sliced_validated_cases . add ( case_id ) 
~~~ validated_tp . add ( var_obj [ '_id' ] ) 
~~~ validated_fp . add ( var_obj [ '_id' ] ) 
~~ ~~ ~~ ~~ n_validation_cases = len ( sliced_validation_cases ) 
n_validated_cases = len ( sliced_validated_cases ) 
overview . append ( 
'count' : n_validation_cases , 
'percent' : n_validation_cases / total_sliced_cases , 
'count' : n_validated_cases , 
'percent' : n_validated_cases / total_sliced_cases , 
data [ 'overview' ] = overview 
nr_validated = len ( validated_tp ) + len ( validated_fp ) 
variants . append ( 
'count' : var_valid_orders , 
'percent' : 1 
percent_validated_tp = 0 
percent_validated_fp = 0 
if var_valid_orders : 
~~~ percent_validated_tp = len ( validated_tp ) / var_valid_orders 
percent_validated_fp = len ( validated_fp ) / var_valid_orders 
~~ variants . append ( 
'count' : len ( validated_tp ) , 
'percent' : percent_validated_tp , 
'count' : len ( validated_fp ) , 
'percent' : percent_validated_fp , 
data [ 'variants' ] = variants 
~~ def get_general_case_info ( adapter , institute_id = None , slice_query = None ) : 
general = { } 
name_query = slice_query 
cases = adapter . cases ( owner = institute_id , name_query = name_query ) 
phenotype_cases = 0 
causative_cases = 0 
pinned_cases = 0 
cohort_cases = 0 
pedigree = { 
1 : { 
'title' : 'Single' , 
'count' : 0 
2 : { 
'title' : 'Duo' , 
3 : { 
'title' : 'Trio' , 
'many' : { 
'title' : 'Many' , 
case_ids = set ( ) 
total_cases = 0 
for total_cases , case in enumerate ( cases , 1 ) : 
~~~ if institute_id : 
~~~ case_ids . add ( case [ '_id' ] ) 
~~ if case . get ( 'phenotype_terms' ) : 
~~~ phenotype_cases += 1 
~~ if case . get ( 'causatives' ) : 
~~~ causative_cases += 1 
~~ if case . get ( 'suspects' ) : 
~~~ pinned_cases += 1 
~~ if case . get ( 'cohorts' ) : 
~~~ cohort_cases += 1 
~~ nr_individuals = len ( case . get ( 'individuals' , [ ] ) ) 
if nr_individuals == 0 : 
~~ if nr_individuals > 3 : 
~~~ pedigree [ 'many' ] [ 'count' ] += 1 
~~~ pedigree [ nr_individuals ] [ 'count' ] += 1 
~~ ~~ general [ 'total_cases' ] = total_cases 
general [ 'phenotype_cases' ] = phenotype_cases 
general [ 'causative_cases' ] = causative_cases 
general [ 'pinned_cases' ] = pinned_cases 
general [ 'cohort_cases' ] = cohort_cases 
general [ 'pedigree' ] = pedigree 
general [ 'case_ids' ] = case_ids 
return general 
~~ def get_case_groups ( adapter , total_cases , institute_id = None , slice_query = None ) : 
cases = [ { 'status' : 'all' , 'count' : total_cases , 'percent' : 1 } ] 
pipeline = [ ] 
group = { '$group' : { '_id' : '$status' , 'count' : { '$sum' : 1 } } } 
subquery = { } 
if institute_id and slice_query : 
~~~ subquery = adapter . cases ( owner = institute_id , name_query = slice_query , 
yield_query = True ) 
~~~ subquery = adapter . cases ( owner = institute_id , yield_query = True ) 
~~ elif slice_query : 
~~~ subquery = adapter . cases ( name_query = slice_query , yield_query = True ) 
~~ query = { '$match' : subquery } if subquery else { } 
if query : 
~~~ pipeline . append ( query ) 
~~ pipeline . append ( group ) 
res = adapter . case_collection . aggregate ( pipeline ) 
for status_group in res : 
~~~ cases . append ( { 'status' : status_group [ '_id' ] , 
'count' : status_group [ 'count' ] , 
'percent' : status_group [ 'count' ] / total_cases } ) 
~~ return cases 
~~ def get_analysis_types ( adapter , total_cases , institute_id = None , slice_query = None ) : 
~~ query = { '$match' : subquery } 
~~ pipeline . append ( { '$unwind' : '$individuals' } ) 
pipeline . append ( { '$group' : { '_id' : '$individuals.analysis_type' , 'count' : { '$sum' : 1 } } } ) 
analysis_query = adapter . case_collection . aggregate ( pipeline ) 
analysis_types = [ { 'name' : group [ '_id' ] , 'count' : group [ 'count' ] } for group in analysis_query ] 
return analysis_types 
~~ def load_hpo_term ( self , hpo_obj ) : 
~~~ self . hpo_term_collection . insert_one ( hpo_obj ) 
~~ def load_hpo_bulk ( self , hpo_bulk ) : 
~~~ result = self . hpo_term_collection . insert_many ( hpo_bulk ) 
~~ def hpo_term ( self , hpo_id ) : 
return self . hpo_term_collection . find_one ( { '_id' : hpo_id } ) 
~~ def hpo_terms ( self , query = None , hpo_term = None , text = None , limit = None ) : 
query_dict = { } 
search_term = None 
~~~ query_dict = { '$or' : 
{ 'hpo_id' : { '$regex' : query , '$options' : 'i' } } , 
{ 'description' : { '$regex' : query , '$options' : 'i' } } , 
search_term = query 
~~ elif text : 
~~~ new_string = '' 
~~~ new_string += word 
~~~ new_string += \ . format ( word ) 
query_dict [ '$text' ] = { '$search' : new_string } 
search_term = text 
~~ elif hpo_term : 
~~~ query_dict [ 'hpo_id' ] = hpo_term 
search_term = hpo_term 
~~ limit = limit or int ( 10e10 ) 
res = self . hpo_term_collection . find ( query_dict ) . limit ( limit ) . sort ( 'hpo_number' , ASCENDING ) 
~~ def disease_term ( self , disease_identifier ) : 
~~~ disease_identifier = int ( disease_identifier ) 
query [ 'disease_nr' ] = disease_identifier 
~~~ query [ '_id' ] = disease_identifier 
~~ return self . disease_term_collection . find_one ( query ) 
~~ def disease_terms ( self , hgnc_id = None ) : 
query [ 'genes' ] = hgnc_id 
~~ return list ( self . disease_term_collection . find ( query ) ) 
~~ def load_disease_term ( self , disease_obj ) : 
~~~ self . disease_term_collection . insert_one ( disease_obj ) 
~~ def generate_hpo_gene_list ( self , * hpo_terms ) : 
for term in hpo_terms : 
~~~ hpo_obj = self . hpo_term ( term ) 
if hpo_obj : 
~~~ for hgnc_id in hpo_obj [ 'genes' ] : 
~~~ if hgnc_id in genes : 
~~~ genes [ hgnc_id ] += 1 
~~~ genes [ hgnc_id ] = 1 
~~ ~~ sorted_genes = sorted ( genes . items ( ) , key = operator . itemgetter ( 1 ) , reverse = True ) 
return sorted_genes 
~~ def cmd_tool ( args = None ) : 
from argparse import ArgumentParser 
parser . add_argument ( '-p' , action = 'store' , default = 'ank' , dest = 'what_to_plot' , type = str , 
help = \ ) 
parser . add_argument ( 'filename' , type = str , 
parser . add_argument ( '-b' , action = 'store' , default = None , dest = 'f_start' , type = float , 
parser . add_argument ( '-e' , action = 'store' , default = None , dest = 'f_stop' , type = float , 
parser . add_argument ( '-B' , action = 'store' , default = None , dest = 't_start' , type = int , 
parser . add_argument ( '-E' , action = 'store' , default = None , dest = 't_stop' , type = int , 
parser . add_argument ( '-i' , action = 'store_true' , default = False , dest = 'info_only' , 
parser . add_argument ( '-a' , action = 'store_true' , default = False , dest = 'average' , 
parser . add_argument ( '-s' , action = 'store' , default = '' , dest = 'plt_filename' , type = str , 
parser . add_argument ( '-S' , action = 'store_true' , default = False , dest = 'save_only' , 
parser . add_argument ( '-D' , action = 'store_false' , default = True , dest = 'blank_dc' , 
parser . add_argument ( '-c' , action = 'store_true' , default = False , dest = 'calibrate_band_pass' , 
args = parser . parse_args ( ) 
filename = args . filename 
load_data = not args . info_only 
wtp = args . what_to_plot 
if not wtp or 's' in wtp : 
~~~ if args . t_start == None : 
~~~ t_start = 0 
~~~ t_start = args . t_start 
~~ t_stop = t_start + 1 
if args . average : 
~~~ t_start = None 
t_stop = None 
t_stop = args . t_stop 
~~ if args . info_only : 
~~~ args . blank_dc = False 
args . calibrate_band_pass = False 
~~ fil = Filterbank ( filename , f_start = args . f_start , f_stop = args . f_stop , 
t_start = t_start , t_stop = t_stop , 
load_data = load_data , blank_dc = args . blank_dc , 
cal_band_pass = args . calibrate_band_pass ) 
fil . info ( ) 
if not args . info_only : 
#try: 
# 
~~~ if args . what_to_plot == "w" : 
~~~ plt . figure ( "waterfall" , figsize = ( 8 , 6 ) ) 
fil . plot_waterfall ( f_start = args . f_start , f_stop = args . f_stop ) 
~~ elif args . what_to_plot == "s" : 
~~~ plt . figure ( "Spectrum" , figsize = ( 8 , 6 ) ) 
fil . plot_spectrum ( logged = True , f_start = args . f_start , f_stop = args . f_stop , t = 'all' ) 
~~ elif args . what_to_plot == "mm" : 
fil . plot_spectrum_min_max ( logged = True , f_start = args . f_start , f_stop = args . f_stop , t = 'all' ) 
~~ elif args . what_to_plot == "k" : 
~~~ plt . figure ( "kurtosis" , figsize = ( 8 , 6 ) ) 
fil . plot_kurtosis ( f_start = args . f_start , f_stop = args . f_stop ) 
~~ elif args . what_to_plot == "t" : 
fil . plot_time_series ( f_start = args . f_start , f_stop = args . f_stop , orientation = 'h' ) 
~~ elif args . what_to_plot == "a" : 
fil . plot_all ( logged = True , f_start = args . f_start , f_stop = args . f_stop , t = 'all' ) 
~~ elif args . what_to_plot == "ank" : 
fil . plot_all ( logged = True , f_start = args . f_start , f_stop = args . f_stop , t = 'all' , kurtosis = False ) 
~~ if args . plt_filename != '' : 
~~~ plt . savefig ( args . plt_filename ) 
~~ if not args . save_only : 
~~~ if 'DISPLAY' in os . environ . keys ( ) : 
~~~ plt . show ( ) 
~~ ~~ ~~ ~~ def read_hdf5 ( self , filename , f_start = None , f_stop = None , 
t_start = None , t_stop = None , load_data = True ) : 
self . header = { } 
self . filename = filename 
self . h5 = h5py . File ( filename ) 
for key , val in self . h5 [ b'data' ] . attrs . items ( ) : 
~~~ if six . PY3 : 
~~~ key = bytes ( key , 'ascii' ) 
~~ if key == b'src_raj' : 
~~~ self . header [ key ] = Angle ( val , unit = 'hr' ) 
~~ elif key == b'src_dej' : 
~~~ self . header [ key ] = Angle ( val , unit = 'deg' ) 
~~~ self . header [ key ] = val 
~~ ~~ self . n_ints_in_file = self . h5 [ b"data" ] . shape [ 0 ] 
i_start , i_stop , chan_start_idx , chan_stop_idx = self . _setup_freqs ( f_start = f_start , f_stop = f_stop ) 
ii_start , ii_stop , n_ints = self . _setup_time_axis ( t_start = t_start , t_stop = t_stop ) 
if load_data : 
~~~ self . data = self . h5 [ b"data" ] [ ii_start : ii_stop , : , chan_start_idx : chan_stop_idx ] 
self . file_size_bytes = os . path . getsize ( self . filename ) 
self . data = np . array ( [ 0 ] ) 
self . n_ints_in_file = 0 
~~ ~~ def _setup_freqs ( self , f_start = None , f_stop = None ) : 
f0 = self . header [ b'fch1' ] 
f_delt = self . header [ b'foff' ] 
i_start , i_stop = 0 , self . header [ b'nchans' ] 
if f_start : 
~~~ i_start = int ( ( f_start - f0 ) / f_delt ) 
~~ if f_stop : 
~~~ i_stop = int ( ( f_stop - f0 ) / f_delt ) 
~~ chan_start_idx = np . int ( i_start ) 
chan_stop_idx = np . int ( i_stop ) 
if i_start < i_stop : 
~~~ i_vals = np . arange ( chan_start_idx , chan_stop_idx ) 
~~~ i_vals = np . arange ( chan_stop_idx , chan_start_idx ) 
~~ self . freqs = f_delt * i_vals + f0 
if chan_stop_idx < chan_start_idx : 
~~~ chan_stop_idx , chan_start_idx = chan_start_idx , chan_stop_idx 
~~ return i_start , i_stop , chan_start_idx , chan_stop_idx 
~~ def _setup_time_axis ( self , t_start = None , t_stop = None ) : 
ii_start , ii_stop = 0 , self . n_ints_in_file 
if t_start : 
~~~ ii_start = t_start 
~~ if t_stop : 
~~~ ii_stop = t_stop 
~~ n_ints = ii_stop - ii_start 
t0 = self . header [ b'tstart' ] 
t_delt = self . header [ b'tsamp' ] 
self . timestamps = np . arange ( 0 , n_ints ) * t_delt / 24. / 60. / 60 + t0 
return ii_start , ii_stop , n_ints 
~~ def read_filterbank ( self , filename = None , f_start = None , f_stop = None , 
if filename is None : 
~~~ filename = self . filename 
~~~ self . filename = filename 
~~ self . header = read_header ( filename ) 
n_bits = self . header [ b'nbits' ] 
n_bytes = int ( self . header [ b'nbits' ] / 8 ) 
n_chans = self . header [ b'nchans' ] 
n_chans_selected = self . freqs . shape [ 0 ] 
n_ifs = self . header [ b'nifs' ] 
self . idx_data = len_header ( filename ) 
f = open ( filename , 'rb' ) 
f . seek ( self . idx_data ) 
filesize = os . path . getsize ( self . filename ) 
n_bytes_data = filesize - self . idx_data 
self . n_ints_in_file = calc_n_ints_in_file ( self . filename ) 
self . file_size_bytes = filesize 
f . seek ( int ( ii_start * n_bits * n_ifs * n_chans / 8 ) , 1 ) 
i0 = np . min ( ( chan_start_idx , chan_stop_idx ) ) 
i1 = np . max ( ( chan_start_idx , chan_stop_idx ) ) 
if n_bits == 2 : 
~~~ dd_type = b'uint8' 
n_chans_selected = int ( n_chans_selected / 4 ) 
~~ elif n_bytes == 4 : 
~~~ dd_type = b'float32' 
~~ elif n_bytes == 2 : 
~~~ dd_type = b'uint16' 
~~ elif n_bytes == 1 : 
~~ if load_data : 
~~~ if n_ints * n_ifs * n_chans_selected > MAX_DATA_ARRAY_SIZE : 
sys . exit ( ) 
~~ if n_bits == 2 : 
~~~ self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected * 4 ) , dtype = dd_type ) 
~~~ self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected ) , dtype = dd_type ) 
~~ for ii in range ( n_ints ) : 
for jj in range ( n_ifs ) : 
dd = np . fromfile ( f , count = n_chans_selected , dtype = dd_type ) 
~~~ dd = unpack_2to8 ( dd ) 
~~ self . data [ ii , jj ] = dd 
self . data = np . array ( [ 0 ] , dtype = dd_type ) 
~~ ~~ def compute_lst ( self ) : 
if self . header [ b'telescope_id' ] == 6 : 
~~~ self . coords = gbt_coords 
~~ elif self . header [ b'telescope_id' ] == 4 : 
~~~ self . coords = parkes_coords 
~~ if HAS_SLALIB : 
~~~ dut1 = 0.0 
mjd = self . header [ b'tstart' ] 
tellong = np . deg2rad ( self . coords [ 1 ] ) 
last = s . sla_gmst ( mjd ) - tellong + s . sla_eqeqx ( mjd ) + dut1 
if last < 0.0 : last = last + 2.0 * np . pi 
return last 
~~ ~~ def compute_lsrk ( self ) : 
ra = Angle ( self . header [ b'src_raj' ] , unit = 'hourangle' ) 
dec = Angle ( self . header [ b'src_dej' ] , unit = 'degree' ) 
mjdd = self . header [ b'tstart' ] 
rarad = ra . to ( 'radian' ) . value 
dcrad = dec . to ( 'radian' ) . value 
last = self . compute_lst ( ) 
tellat = np . deg2rad ( self . coords [ 0 ] ) 
starvect = s . sla_dcs2c ( rarad , dcrad ) 
Rgeo = s . sla_rverot ( tellat , rarad , dcrad , last ) 
evp = s . sla_evp ( mjdd , 2000.0 ) 
vcorhelio = - s . sla_dvdv ( starvect , dvh ) * 149.597870e6 
vcorbary = - s . sla_dvdv ( starvect , dvb ) * 149.597870e6 
rvlsrd = s . sla_rvlsrd ( rarad , dcrad ) 
rvlsrk = s . sla_rvlsrk ( rarad , dcrad ) 
rvgalc = s . sla_rvgalc ( rarad , dcrad ) 
totalhelio = Rgeo + vcorhelio 
totalbary = Rgeo + vcorbary 
totallsrk = totalhelio + rvlsrk 
totalgal = totalbary + rvlsrd + rvgalc 
return totallsrk 
~~ def blank_dc ( self , n_coarse_chan ) : 
if n_coarse_chan < 1 : 
~~ if not n_coarse_chan % int ( n_coarse_chan ) == 0 : 
~~ n_coarse_chan = int ( n_coarse_chan ) 
n_chan = self . data . shape [ - 1 ] 
n_chan_per_coarse = int ( n_chan / n_coarse_chan ) 
mid_chan = int ( n_chan_per_coarse / 2 ) 
for ii in range ( n_coarse_chan ) : 
~~~ ss = ii * n_chan_per_coarse 
self . data [ ... , ss + mid_chan ] = np . median ( self . data [ ... , ss + mid_chan + 5 : ss + mid_chan + 10 ] ) 
~~ ~~ def info ( self ) : 
for key , val in self . header . items ( ) : 
~~~ if key == b'src_raj' : 
~~~ val = val . to_string ( unit = u . hour , sep = ':' ) 
~~ if key == b'src_dej' : 
~~~ val = val . to_string ( unit = u . deg , sep = ':' ) 
~~ if key == b'tsamp' : 
~~~ val *= u . second 
~~ if key in ( 'foff' , 'fch1' ) : 
~~~ val *= u . MHz 
~~ if key == b'tstart' : 
~~ def generate_freqs ( self , f_start , f_stop ) : 
fch1 = self . header [ b'fch1' ] 
foff = self . header [ b'foff' ] 
i_start = int ( ( f_start - fch1 ) / foff ) 
i_stop = int ( ( f_stop - fch1 ) / foff ) 
chan_start_idx = np . int ( i_start ) 
i_vals = np . arange ( chan_stop_idx , chan_start_idx , 1 ) 
freqs = foff * i_vals + fch1 
return freqs 
~~ def _calc_extent ( self , plot_f = None , plot_t = None , MJD_time = False ) : 
plot_f_begin = plot_f [ 0 ] 
plot_f_end = plot_f [ - 1 ] + ( plot_f [ 1 ] - plot_f [ 0 ] ) 
plot_t_begin = self . timestamps [ 0 ] 
plot_t_end = self . timestamps [ - 1 ] + ( self . timestamps [ 1 ] - self . timestamps [ 0 ] ) 
if MJD_time : 
~~~ extent = ( plot_f_begin , plot_f_begin_end , plot_t_begin , plot_t_end ) 
~~~ extent = ( plot_f_begin , plot_f_end , 0.0 , ( plot_t_end - plot_t_begin ) * 24. * 60. * 60 ) 
~~ return extent 
~~ def plot_spectrum ( self , t = 0 , f_start = None , f_stop = None , logged = False , if_id = 0 , c = None , ** kwargs ) : 
if self . header [ b'nbits' ] <= 2 : 
~~~ logged = False 
t = 'all' 
~~ ax = plt . gca ( ) 
plot_f , plot_data = self . grab_data ( f_start , f_stop , if_id ) 
if self . header [ b'foff' ] < 0 : 
plot_f = plot_f [ : : - 1 ] 
~~ if isinstance ( t , int ) : 
plot_data = plot_data [ t ] 
~~ elif t == b'all' : 
if len ( plot_data . shape ) > 1 : 
~~~ plot_data = plot_data . mean ( axis = 0 ) 
~~~ plot_data = plot_data . mean ( ) 
~~ dec_fac_x = 1 
if plot_data . shape [ 0 ] > MAX_PLT_POINTS : 
~~~ dec_fac_x = int ( plot_data . shape [ 0 ] / MAX_PLT_POINTS ) 
~~ plot_data = rebin ( plot_data , dec_fac_x , 1 ) 
plot_f = rebin ( plot_f , dec_fac_x , 1 ) 
if not c : 
~~~ kwargs [ 'c' ] = '#333333' 
~~ if logged : 
plt . legend ( ) 
~~~ plt . title ( self . header [ b'source_name' ] ) 
~~~ plt . title ( self . filename ) 
~~ plt . xlim ( plot_f [ 0 ] , plot_f [ - 1 ] ) 
~~ def plot_spectrum_min_max ( self , t = 0 , f_start = None , f_stop = None , logged = False , if_id = 0 , c = None , ** kwargs ) : 
ax = plt . gca ( ) 
~~ fig_max = plot_data [ 0 ] . max ( ) 
fig_min = plot_data [ 0 ] . min ( ) 
~~~ plot_max = plot_data . max ( axis = 0 ) 
plot_min = plot_data . min ( axis = 0 ) 
plot_data = plot_data . mean ( axis = 0 ) 
~~~ plot_max = plot_data . max ( ) 
plot_min = plot_data . min ( ) 
plot_data = plot_data . mean ( ) 
plot_min = rebin ( plot_min , dec_fac_x , 1 ) 
plot_max = rebin ( plot_max , dec_fac_x , 1 ) 
if logged : 
~~~ plt . plot ( plot_f , db ( plot_data ) , "#333333" , label = 'mean' , ** kwargs ) 
plt . plot ( plot_f , db ( plot_max ) , "#e74c3c" , label = 'max' , ** kwargs ) 
plt . plot ( plot_f , db ( plot_min ) , '#3b5b92' , label = 'min' , ** kwargs ) 
~~~ plt . plot ( plot_f , plot_data , "#333333" , label = 'mean' , ** kwargs ) 
plt . plot ( plot_f , plot_max , "#e74c3c" , label = 'max' , ** kwargs ) 
plt . plot ( plot_f , plot_min , '#3b5b92' , label = 'min' , ** kwargs ) 
~~~ plt . ylim ( db ( fig_min ) , db ( fig_max ) ) 
~~ ~~ def plot_waterfall ( self , f_start = None , f_stop = None , if_id = 0 , logged = True , cb = True , MJD_time = False , ** kwargs ) : 
~~~ plot_data = db ( plot_data ) 
~~ dec_fac_x , dec_fac_y = 1 , 1 
if plot_data . shape [ 0 ] > MAX_IMSHOW_POINTS [ 0 ] : 
~~~ dec_fac_x = int ( plot_data . shape [ 0 ] / MAX_IMSHOW_POINTS [ 0 ] ) 
~~ if plot_data . shape [ 1 ] > MAX_IMSHOW_POINTS [ 1 ] : 
~~~ dec_fac_y = int ( plot_data . shape [ 1 ] / MAX_IMSHOW_POINTS [ 1 ] ) 
~~ plot_data = rebin ( plot_data , dec_fac_x , dec_fac_y ) 
~~ extent = self . _calc_extent ( plot_f = plot_f , plot_t = self . timestamps , MJD_time = MJD_time ) 
plt . imshow ( plot_data , 
aspect = 'auto' , 
origin = 'lower' , 
rasterized = True , 
interpolation = 'nearest' , 
extent = extent , 
cmap = 'viridis' , 
** kwargs 
if cb : 
~~~ plt . colorbar ( ) 
~~ ~~ def plot_time_series ( self , f_start = None , f_stop = None , if_id = 0 , logged = True , orientation = 'h' , MJD_time = False , ** kwargs ) : 
if logged and self . header [ b'nbits' ] >= 8 : 
~~ if len ( plot_data . shape ) > 1 : 
~~~ plot_data = plot_data . mean ( axis = 1 ) 
plot_t = np . linspace ( extent [ 2 ] , extent [ 3 ] , len ( self . timestamps ) ) 
~~ if 'v' in orientation : 
~~~ plt . plot ( plot_data , plot_t , ** kwargs ) 
plt . xlabel ( plabel ) 
~~~ plt . plot ( plot_t , plot_data , ** kwargs ) 
plt . xlabel ( tlabel ) 
plt . ylabel ( plabel ) 
~~ ax . autoscale ( axis = 'both' , tight = True ) 
~~ def plot_kurtosis ( self , f_start = None , f_stop = None , if_id = 0 , ** kwargs ) : 
~~~ plot_kurtosis = scipy . stats . kurtosis ( plot_data , axis = 0 , nan_policy = 'omit' ) 
~~~ plot_kurtosis = plot_data * 0.0 
~~ plt . plot ( plot_f , plot_kurtosis , ** kwargs ) 
plt . ylabel ( "Kurtosis" ) 
plt . xlim ( plot_f [ 0 ] , plot_f [ - 1 ] ) 
~~ def plot_all ( self , t = 0 , f_start = None , f_stop = None , logged = False , if_id = 0 , kurtosis = True , ** kwargs ) : 
left , width = 0.35 , 0.5 
bottom , height = 0.45 , 0.5 
width2 , height2 = 0.1125 , 0.15 
bottom2 , left2 = bottom - height2 - .025 , left - width2 - .02 
bottom3 , left3 = bottom2 - height2 - .025 , 0.075 
rect_waterfall = [ left , bottom , width , height ] 
rect_colorbar = [ left + width , bottom , .025 , height ] 
rect_spectrum = [ left , bottom2 , width , height2 ] 
rect_min_max = [ left , bottom3 , width , height2 ] 
rect_timeseries = [ left + width , bottom , width2 , height ] 
rect_kurtosis = [ left3 , bottom3 , 0.25 , height2 ] 
rect_header = [ left3 - .05 , bottom , 0.2 , height ] 
axMinMax = plt . axes ( rect_min_max ) 
self . plot_spectrum_min_max ( logged = logged , f_start = f_start , f_stop = f_stop , t = t ) 
plt . title ( '' ) 
axMinMax . yaxis . tick_right ( ) 
axMinMax . yaxis . set_label_position ( "right" ) 
axSpectrum = plt . axes ( rect_spectrum , sharex = axMinMax ) 
self . plot_spectrum ( logged = logged , f_start = f_start , f_stop = f_stop , t = t ) 
axSpectrum . yaxis . tick_right ( ) 
axSpectrum . yaxis . set_label_position ( "right" ) 
plt . xlabel ( '' ) 
plt . setp ( axSpectrum . get_xticklabels ( ) , visible = False ) 
axWaterfall = plt . axes ( rect_waterfall , sharex = axMinMax ) 
self . plot_waterfall ( f_start = f_start , f_stop = f_stop , logged = logged , cb = False ) 
plt . setp ( axWaterfall . get_xticklabels ( ) , visible = False ) 
axTimeseries = plt . axes ( rect_timeseries ) 
self . plot_time_series ( f_start = f_start , f_stop = f_stop , orientation = 'v' ) 
axTimeseries . yaxis . set_major_formatter ( nullfmt ) 
if kurtosis : 
~~~ axKurtosis = plt . axes ( rect_kurtosis ) 
self . plot_kurtosis ( f_start = f_start , f_stop = f_stop ) 
~~ axHeader = plt . axes ( rect_header ) 
1 : 'Arecibo' , 
2 : 'Ooty' , 
3 : 'Nancay' , 
4 : 'Parkes' , 
5 : 'Jodrell' , 
6 : 'GBT' , 
8 : 'Effelsberg' , 
10 : 'SRT' , 
64 : 'MeerKAT' , 
65 : 'KAT7' 
telescope = telescopes . get ( self . header [ b"telescope_id" ] , self . header [ b"telescope_id" ] ) 
for key in ( b'SRC_RAJ' , b'SRC_DEJ' , b'TSTART' , b'NCHANS' , b'NBEAMS' , b'NIFS' , b'NBITS' ) : 
foff = ( self . header [ b'foff' ] * 1e6 * u . Hz ) 
if np . abs ( foff ) > 1e6 * u . Hz : 
~~~ foff = str ( foff . to ( 'MHz' ) ) 
~~ elif np . abs ( foff ) > 1e3 * u . Hz : 
~~~ foff = str ( foff . to ( 'kHz' ) ) 
~~~ foff = str ( foff . to ( 'Hz' ) ) 
plt . text ( 0.05 , .95 , plot_header , ha = 'left' , va = 'top' , wrap = True ) 
axHeader . set_facecolor ( 'white' ) 
axHeader . xaxis . set_major_formatter ( nullfmt ) 
axHeader . yaxis . set_major_formatter ( nullfmt ) 
~~ def write_to_filterbank ( self , filename_out ) : 
with open ( filename_out , "wb" ) as fileh : 
~~~ fileh . write ( generate_sigproc_header ( self ) ) 
j = self . data 
if n_bytes == 4 : 
~~~ np . float32 ( j . ravel ( ) ) . tofile ( fileh ) 
~~~ np . int16 ( j . ravel ( ) ) . tofile ( fileh ) 
~~~ np . int8 ( j . ravel ( ) ) . tofile ( fileh ) 
~~ ~~ ~~ def write_to_hdf5 ( self , filename_out , * args , ** kwargs ) : 
if not HAS_HDF5 : 
~~ with h5py . File ( filename_out , 'w' ) as h5 : 
~~~ dset = h5 . create_dataset ( b'data' , 
data = self . data , 
compression = 'lzf' ) 
dset_mask = h5 . create_dataset ( b'mask' , 
shape = self . data . shape , 
compression = 'lzf' , 
dtype = 'uint8' ) 
dset . dims [ 0 ] . label = b"frequency" 
dset . dims [ 1 ] . label = b"feed_id" 
dset . dims [ 2 ] . label = b"time" 
dset_mask . dims [ 0 ] . label = b"frequency" 
dset_mask . dims [ 1 ] . label = b"feed_id" 
dset_mask . dims [ 2 ] . label = b"time" 
for key , value in self . header . items ( ) : 
~~~ dset . attrs [ key ] = value 
~~ ~~ ~~ def calibrate_band_pass_N1 ( self ) : 
band_pass = np . median ( self . data . squeeze ( ) , axis = 0 ) 
self . data = self . data / band_pass 
~~ def get_stokes ( cross_dat , feedtype = 'l' ) : 
if feedtype == 'l' : 
~~~ I = cross_dat [ : , 0 , : ] + cross_dat [ : , 1 , : ] 
Q = cross_dat [ : , 0 , : ] - cross_dat [ : , 1 , : ] 
U = 2 * cross_dat [ : , 2 , : ] 
V = - 2 * cross_dat [ : , 3 , : ] 
~~ elif feedtype == 'c' : 
Q = 2 * cross_dat [ : , 2 , : ] 
U = - 2 * cross_dat [ : , 3 , : ] 
V = cross_dat [ : , 1 , : ] - cross_dat [ : , 0 , : ] 
~~ I = np . expand_dims ( I , axis = 1 ) 
Q = np . expand_dims ( Q , axis = 1 ) 
U = np . expand_dims ( U , axis = 1 ) 
V = np . expand_dims ( V , axis = 1 ) 
#L=np.sqrt(np.square(Q)+np.square(U)) 
return I , Q , U , V 
~~ def convert_to_coarse ( data , chan_per_coarse ) : 
num_coarse = data . size / chan_per_coarse 
data_shaped = np . array ( np . reshape ( data , ( num_coarse , chan_per_coarse ) ) ) 
return np . mean ( data_shaped [ : , 2 : - 1 ] , axis = 1 ) 
~~ def phase_offsets ( Idat , Qdat , Udat , Vdat , tsamp , chan_per_coarse , feedtype = 'l' , ** kwargs ) : 
~~~ U_OFF , U_ON = foldcal ( Udat , tsamp , ** kwargs ) 
V_OFF , V_ON = foldcal ( Vdat , tsamp , ** kwargs ) 
Udiff = U_ON - U_OFF 
Vdiff = V_ON - V_OFF 
poffset = np . arctan2 ( - 1 * Vdiff , Udiff ) 
~~ if feedtype == 'c' : 
Q_OFF , Q_ON = foldcal ( Qdat , tsamp , ** kwargs ) 
Qdiff = Q_ON - Q_OFF 
poffset = np . arctan2 ( Udiff , Qdiff ) 
~~ coarse_p = convert_to_coarse ( poffset , chan_per_coarse ) 
y = coarse_p [ : 6 ] 
x = np . arange ( y . size ) 
m = np . polyfit ( x , y , 1 ) [ 0 ] 
for i in range ( coarse_p . size - 3 ) : 
~~~ if ( m > 0 and coarse_p [ i + 1 ] < coarse_p [ i ] ) or ( m < 0 and coarse_p [ i + 1 ] > coarse_p [ i ] ) : 
~~ ~~ return coarse_p 
~~ def gain_offsets ( Idat , Qdat , Udat , Vdat , tsamp , chan_per_coarse , feedtype = 'l' , ** kwargs ) : 
~~~ I_OFF , I_ON = foldcal ( Idat , tsamp , ** kwargs ) 
XX_ON = ( I_ON + Q_ON ) / 2 
XX_OFF = ( I_OFF + Q_OFF ) / 2 
YY_ON = ( I_ON - Q_ON ) / 2 
YY_OFF = ( I_OFF - Q_OFF ) / 2 
G = ( XX_OFF - YY_OFF ) / ( XX_OFF + YY_OFF ) 
RR_ON = ( I_ON + V_ON ) / 2 
RR_OFF = ( I_OFF + V_OFF ) / 2 
LL_ON = ( I_ON - V_ON ) / 2 
LL_OFF = ( I_OFF - V_OFF ) / 2 
G = ( RR_OFF - LL_OFF ) / ( RR_OFF + LL_OFF ) 
~~ return convert_to_coarse ( G , chan_per_coarse ) 
~~ def apply_Mueller ( I , Q , U , V , gain_offsets , phase_offsets , chan_per_coarse , feedtype = 'l' ) : 
shape = I . shape 
ax0 = I . shape [ 0 ] 
ax1 = I . shape [ 1 ] 
nchans = I . shape [ 2 ] 
ncoarse = nchans / chan_per_coarse 
I = np . reshape ( I , ( ax0 , ax1 , ncoarse , chan_per_coarse ) ) 
Q = np . reshape ( Q , ( ax0 , ax1 , ncoarse , chan_per_coarse ) ) 
U = np . reshape ( U , ( ax0 , ax1 , ncoarse , chan_per_coarse ) ) 
V = np . reshape ( V , ( ax0 , ax1 , ncoarse , chan_per_coarse ) ) 
I = np . swapaxes ( I , 2 , 3 ) 
Q = np . swapaxes ( Q , 2 , 3 ) 
U = np . swapaxes ( U , 2 , 3 ) 
V = np . swapaxes ( V , 2 , 3 ) 
a = 1 / ( 1 - gain_offsets ** 2 ) 
~~~ Icorr = a * ( I - gain_offsets * Q ) 
Qcorr = a * ( - 1 * gain_offsets * I + Q ) 
I = None 
Q = None 
~~~ Icorr = a * ( I - gain_offsets * V ) 
Vcorr = a * ( - 1 * gain_offsets * I + V ) 
V = None 
~~ if feedtype == 'l' : 
~~~ Ucorr = U * np . cos ( phase_offsets ) - V * np . sin ( phase_offsets ) 
Vcorr = U * np . sin ( phase_offsets ) + V * np . cos ( phase_offsets ) 
U = None 
~~~ Qcorr = Q * np . cos ( phase_offsets ) + U * np . sin ( phase_offsets ) 
Ucorr = - 1 * Q * np . sin ( phase_offsets ) + U * np . cos ( phase_offsets ) 
~~ Icorr = np . reshape ( np . swapaxes ( Icorr , 2 , 3 ) , shape ) 
Qcorr = np . reshape ( np . swapaxes ( Qcorr , 2 , 3 ) , shape ) 
Ucorr = np . reshape ( np . swapaxes ( Ucorr , 2 , 3 ) , shape ) 
Vcorr = np . reshape ( np . swapaxes ( Vcorr , 2 , 3 ) , shape ) 
return Icorr , Qcorr , Ucorr , Vcorr 
~~ def calibrate_pols ( cross_pols , diode_cross , obsI = None , onefile = True , feedtype = 'l' , ** kwargs ) : 
obs = Waterfall ( diode_cross , max_load = 150 ) 
cross_dat = obs . data 
tsamp = obs . header [ 'tsamp' ] 
dio_ncoarse = obs . calc_n_coarse_chan ( ) 
dio_nchans = obs . header [ 'nchans' ] 
dio_chan_per_coarse = dio_nchans / dio_ncoarse 
obs = None 
Idat , Qdat , Udat , Vdat = get_stokes ( cross_dat , feedtype ) 
cross_dat = None 
gams = gain_offsets ( Idat , Qdat , Udat , Vdat , tsamp , dio_chan_per_coarse , feedtype , ** kwargs ) 
psis = phase_offsets ( Idat , Qdat , Udat , Vdat , tsamp , dio_chan_per_coarse , feedtype , ** kwargs ) 
Idat = None 
Qdat = None 
Udat = None 
Vdat = None 
cross_obs = Waterfall ( cross_pols , max_load = 150 ) 
obs_ncoarse = cross_obs . calc_n_coarse_chan ( ) 
obs_nchans = cross_obs . header [ 'nchans' ] 
obs_chan_per_coarse = obs_nchans / obs_ncoarse 
I , Q , U , V = get_stokes ( cross_obs . data , feedtype ) 
I , Q , U , V = apply_Mueller ( I , Q , U , V , gams , psis , obs_chan_per_coarse , feedtype ) 
if onefile == True : 
~~~ cross_obs . data [ : , 0 , : ] = np . squeeze ( I ) 
cross_obs . data [ : , 1 , : ] = np . squeeze ( Q ) 
cross_obs . data [ : , 2 , : ] = np . squeeze ( U ) 
cross_obs . data [ : , 3 , : ] = np . squeeze ( V ) 
cross_obs . write_to_fil ( cross_pols [ : - 15 ] + '.SIQUV.polcal.fil' ) 
~~ obs = Waterfall ( obs_I , max_load = 150 ) 
obs . data = I 
obs . data = Q 
obs . data = U 
obs . data = V 
~~ def fracpols ( str , ** kwargs ) : 
I , Q , U , V , L = get_stokes ( str , ** kwargs ) 
return L / I , V / I 
~~ def write_stokefils ( str , str_I , Ifil = False , Qfil = False , Ufil = False , Vfil = False , Lfil = False , ** kwargs ) : 
if Ifil : 
~~~ obs . data = I 
~~ if Qfil : 
~~~ obs . data = Q 
~~ if Ufil : 
~~~ obs . data = U 
~~ if Vfil : 
~~~ obs . data = V 
~~ if Lfil : 
~~~ obs . data = L 
obs . write_to_fil ( str [ : - 15 ] + '.L.fil' ) 
~~ ~~ def write_polfils ( str , str_I , ** kwargs ) : 
lin , circ = fracpols ( str , ** kwargs ) 
obs = Waterfall ( str_I , max_load = 150 ) 
obs . data = lin 
obs . data = circ 
obs . write_to_fil ( str [ : - 15 ] + '.circpol.fil' ) 
~~ def closest ( xarr , val ) : 
idx_closest = np . argmin ( np . abs ( np . array ( xarr ) - val ) ) 
return idx_closest 
~~ def rebin ( d , n_x , n_y = None ) : 
if d . ndim == 2 : 
~~~ if n_y is None : 
~~~ n_y = 1 
~~ if n_x is None : 
~~~ n_x = 1 
~~ d = d [ : int ( d . shape [ 0 ] // n_x ) * n_x , : int ( d . shape [ 1 ] // n_y ) * n_y ] 
d = d . reshape ( ( d . shape [ 0 ] // n_x , n_x , d . shape [ 1 ] // n_y , n_y ) ) 
d = d . mean ( axis = 3 ) 
d = d . mean ( axis = 1 ) 
~~ elif d . ndim == 1 : 
~~~ d = d [ : int ( d . shape [ 0 ] // n_x ) * n_x ] 
d = d . reshape ( ( d . shape [ 0 ] // n_x , n_x ) ) 
~~ return d 
~~ def unpack ( data , nbit ) : 
if nbit > 8 : 
~~ if 8 % nbit != 0 : 
~~ if data . dtype not in ( np . uint8 , np . int8 ) : 
~~ if nbit == 8 : 
~~ elif nbit == 4 : 
~~~ data = unpack_4to8 ( data ) 
~~ elif nbit == 2 : 
~~~ data = unpack_2to8 ( data ) 
~~ elif nbit == 1 : 
~~~ data = unpack_1to8 ( data ) 
~~ ~~ def unpack_2to8 ( data ) : 
two_eight_lookup = { 0 : 40 , 
1 : 12 , 
2 : - 12 , 
3 : - 40 } 
tmp = data . astype ( np . uint32 ) 
tmp = ( tmp | ( tmp << 12 ) ) & 0xF000F 
tmp = ( tmp | ( tmp << 6 ) ) & 0x3030303 
tmp = tmp . byteswap ( ) 
tmp = tmp . view ( 'uint8' ) 
mapped = np . array ( tmp , dtype = np . int8 ) 
for k , v in two_eight_lookup . items ( ) : 
~~~ mapped [ tmp == k ] = v 
~~ return mapped 
~~ def unpack_4to8 ( data ) : 
tmpdata = ( tmpdata | ( tmpdata << 4 ) ) & 0x0F0F 
updata = tmpdata . byteswap ( ) 
return updata . view ( data . dtype ) 
~~ def get_diff ( dio_cross , feedtype , ** kwargs ) : 
obs = Waterfall ( dio_cross , max_load = 150 ) 
freqs = obs . populate_freqs ( ) 
data = obs . data 
I , Q , U , V = get_stokes ( data , feedtype ) 
I_OFF , I_ON = foldcal ( I , tsamp , ** kwargs ) 
Q_OFF , Q_ON = foldcal ( Q , tsamp , ** kwargs ) 
U_OFF , U_ON = foldcal ( U , tsamp , ** kwargs ) 
V_OFF , V_ON = foldcal ( V , tsamp , ** kwargs ) 
Idiff = I_ON - I_OFF 
return Idiff , Qdiff , Udiff , Vdiff , freqs 
~~ def plot_Stokes_diode ( dio_cross , diff = True , feedtype = 'l' , ** kwargs ) : 
if diff == True : 
~~~ Idiff , Qdiff , Udiff , Vdiff , freqs = get_diff ( dio_cross , feedtype , ** kwargs ) 
~~~ obs = Waterfall ( dio_cross , max_load = 150 ) 
~~ if diff == True : 
~~~ plt . plot ( freqs , Idiff , 'k-' , label = 'I' ) 
plt . plot ( freqs , Qdiff , 'r-' , label = 'Q' ) 
plt . plot ( freqs , Udiff , 'g-' , label = 'U' ) 
plt . plot ( freqs , Vdiff , 'm-' , label = 'V' ) 
~~ plt . legend ( ) 
~~ def plot_calibrated_diode ( dio_cross , chan_per_coarse = 8 , feedtype = 'l' , ** kwargs ) : 
data = None 
psis = phase_offsets ( I , Q , U , V , tsamp , chan_per_coarse , feedtype , ** kwargs ) 
G = gain_offsets ( I , Q , U , V , tsamp , chan_per_coarse , feedtype , ** kwargs ) 
I , Q , U , V = apply_Mueller ( I , Q , U , V , G , psis , chan_per_coarse , feedtype ) 
plt . plot ( freqs , I_ON - I_OFF , 'k-' , label = 'I' ) 
plt . plot ( freqs , Q_ON - Q_OFF , 'r-' , label = 'Q' ) 
plt . plot ( freqs , U_ON - U_OFF , 'g-' , label = 'U' ) 
plt . plot ( freqs , V_ON - V_OFF , 'm-' , label = 'V' ) 
~~ def plot_phase_offsets ( dio_cross , chan_per_coarse = 8 , feedtype = 'l' , ax1 = None , ax2 = None , legend = True , ** kwargs ) : 
Idiff , Qdiff , Udiff , Vdiff , freqs = get_diff ( dio_cross , feedtype , ** kwargs ) 
coarse_psis = phase_offsets ( I , Q , U , V , tsamp , chan_per_coarse , feedtype , ** kwargs ) 
coarse_freqs = convert_to_coarse ( freqs , chan_per_coarse ) 
coarse_degs = np . degrees ( coarse_psis ) 
if ax2 == None : 
~~~ plt . subplot ( 211 ) 
~~~ axPsi = plt . axes ( ax2 ) 
plt . setp ( axPsi . get_xticklabels ( ) , visible = False ) 
plt . ylabel ( 'Degrees' ) 
plt . grid ( True ) 
if legend == True : 
~~~ plt . legend ( ) 
~~ if ax1 == None : 
~~~ plt . subplot ( 212 ) 
~~~ axUV = plt . axes ( ax1 ) 
~~ plt . plot ( freqs , Udiff , 'g-' , label = 'U' ) 
~~~ plt . plot ( freqs , Vdiff , 'm-' , label = 'V' ) 
~~~ plt . plot ( freqs , Qdiff , 'r-' , label = 'Q' ) 
~~ ~~ def plot_gain_offsets ( dio_cross , dio_chan_per_coarse = 8 , feedtype = 'l' , ax1 = None , ax2 = None , legend = True , ** kwargs ) : 
coarse_G = gain_offsets ( I , Q , U , V , tsamp , dio_chan_per_coarse , feedtype , ** kwargs ) 
coarse_freqs = convert_to_coarse ( freqs , dio_chan_per_coarse ) 
XX_OFF , XX_ON = foldcal ( np . expand_dims ( data [ : , 0 , : ] , axis = 1 ) , tsamp , ** kwargs ) 
YY_OFF , YY_ON = foldcal ( np . expand_dims ( data [ : , 1 , : ] , axis = 1 ) , tsamp , ** kwargs ) 
if ax1 == None : 
~~~ axG = plt . axes ( ax1 ) 
plt . setp ( axG . get_xticklabels ( ) , visible = False ) 
~~ plt . plot ( coarse_freqs , coarse_G , 'ko' , markersize = 2 ) 
~~ plt . grid ( True ) 
~~~ axXY = plt . axes ( ax2 , sharex = axG ) 
~~~ plt . plot ( freqs , XX_OFF , 'b-' , label = 'XX' ) 
plt . plot ( freqs , YY_OFF , 'r-' , label = 'YY' ) 
~~~ plt . plot ( freqs , XX_OFF , 'b-' , label = 'LL' ) 
plt . plot ( freqs , YY_OFF , 'r-' , label = 'RR' ) 
~~ ~~ def plot_diode_fold ( dio_cross , bothfeeds = True , feedtype = 'l' , min_samp = - 500 , max_samp = 7000 , legend = True , ** kwargs ) : 
tseriesI = np . squeeze ( np . mean ( I , axis = 2 ) ) 
I_OFF , I_ON , OFFints , ONints = foldcal ( I , tsamp , inds = True , ** kwargs ) 
if bothfeeds == True : 
~~~ if feedtype == 'l' : 
~~~ tseriesQ = np . squeeze ( np . mean ( Q , axis = 2 ) ) 
tseriesX = ( tseriesI + tseriesQ ) / 2 
tseriesY = ( tseriesI - tseriesQ ) / 2 
~~~ tseriesV = np . squeeze ( np . mean ( V , axis = 2 ) ) 
tseriesR = ( tseriesI + tseriesV ) / 2 
tseriesL = ( tseriesI - tseriesV ) / 2 
~~ ~~ stop = ONints [ - 1 , 1 ] 
if bothfeeds == False : 
for i in ONints : 
~~~ plt . plot ( np . arange ( i [ 0 ] , i [ 1 ] ) , np . full ( ( i [ 1 ] - i [ 0 ] ) , np . mean ( I_ON ) ) , 'r-' ) 
~~ for i in OFFints : 
~~~ plt . plot ( np . arange ( i [ 0 ] , i [ 1 ] ) , np . full ( ( i [ 1 ] - i [ 0 ] ) , np . mean ( I_OFF ) ) , 'b-' ) 
~~~ diff = np . mean ( tseriesX ) - np . mean ( tseriesY ) 
plt . plot ( tseriesX [ 0 : stop ] , 'b-' , label = 'XX' ) 
~~~ diff = np . mean ( tseriesL ) - np . mean ( tseriesR ) 
plt . plot ( tseriesL [ 0 : stop ] , 'b-' , label = 'LL' ) 
~~ ~~ if bothfeeds == False : 
~~~ lowlim = np . mean ( I_OFF ) - ( np . mean ( I_ON ) - np . mean ( I_OFF ) ) / 2 
hilim = np . mean ( I_ON ) + ( np . mean ( I_ON ) - np . mean ( I_OFF ) ) / 2 
plt . ylim ( ( lowlim , hilim ) ) 
~~ plt . xlim ( ( min_samp , max_samp ) ) 
~~ ~~ def plot_fullcalib ( dio_cross , feedtype = 'l' , ** kwargs ) : 
left , width = 0.075 , 0.435 
width2 = 0.232 
bottom2 , height2 = 0.115 , 0.0975 
rect_uncal = [ left , bottom , width , height ] 
rect_cal = [ left + width + 0.025 , bottom , width , height ] 
rect_fold = [ left , bottom2 , width2 , 0.22 ] 
rect_gain1 = [ left + width2 + 0.1 , bottom2 , width2 , height2 ] 
rect_phase1 = [ left + width2 * 2 + 0.1 * 2 , bottom2 , width2 , height2 ] 
rect_gain2 = [ left + width2 + 0.1 , bottom2 + height2 + 0.025 , width2 , height2 ] 
rect_phase2 = [ left + width2 * 2 + 0.1 * 2 , bottom2 + height2 + 0.025 , width2 , height2 ] 
#-------- 
axFold = plt . axes ( rect_fold ) 
plot_diode_fold ( dio_cross , bothfeeds = False , feedtype = feedtype , min_samp = 2000 , max_samp = 5500 , legend = False , ** kwargs ) 
plot_gain_offsets ( dio_cross , feedtype = feedtype , ax1 = rect_gain2 , ax2 = rect_gain1 , legend = False , ** kwargs ) 
plot_phase_offsets ( dio_cross , feedtype = feedtype , ax1 = rect_phase1 , ax2 = rect_phase2 , legend = False , ** kwargs ) 
plt . ylabel ( '' ) 
ax_uncal = plt . axes ( rect_uncal ) 
plot_Stokes_diode ( dio_cross , feedtype = feedtype , ** kwargs ) 
ax_cal = plt . axes ( rect_cal , sharey = ax_uncal ) 
plot_calibrated_diode ( dio_cross , feedtype = feedtype , ** kwargs ) 
plt . setp ( ax_cal . get_yticklabels ( ) , visible = False ) 
plt . savefig ( dio_cross [ : - 4 ] + '.stokescalib.png' , dpi = 2000 ) 
plt . show ( ) 
~~ def plot_diodespec ( ON_obs , OFF_obs , calflux , calfreq , spec_in , units = 'mJy' , ** kwargs ) : 
dspec = diode_spec ( ON_obs , OFF_obs , calflux , calfreq , spec_in , ** kwargs ) 
obs = Waterfall ( ON_obs , max_load = 150 ) 
chan_per_coarse = obs . header [ 'nchans' ] / obs . calc_n_coarse_chan ( ) 
plt . ion ( ) 
plt . figure ( ) 
plt . plot ( coarse_freqs , dspec ) 
~~ def cmd_tool ( ) : 
if len ( sys . argv ) == 1 : 
~~ if args . in_fname == None : 
~~ if args . out_fname == None : 
~~~ if ( args . out_format == None ) or ( args . out_format == 'h5' ) : 
~~~ if args . in_fname [ len ( args . in_fname ) - 4 : ] == '.fil' : 
~~~ args . out_fname = args . in_fname 
args . out_fname = args . out_fname . replace ( '.fil' , '_diced.h5' ) 
~~ elif args . in_fname [ len ( args . in_fname ) - 3 : ] == '.h5' : 
args . out_fname = args . out_fname . replace ( '.h5' , '_diced.h5' ) 
~~ ~~ elif args . out_format == 'fil' : 
args . out_fname = args . out_fname . replace ( '.fil' , '_diced.fil' ) 
args . out_fname = args . out_fname . replace ( '.h5' , '_diced.fil' ) 
~~ ~~ elif ( args . out_fname [ len ( args . out_fname ) - 4 : ] == '.fil' ) and ( args . out_format == 'h5' ) : 
~~ elif ( args . out_fname [ len ( args . out_fname ) - 3 : ] == '.h5' ) and ( args . out_format == 'fil' ) : 
~~ if ( args . out_fname [ len ( args . out_fname ) - 3 : ] != '.h5' ) and ( args . out_fname [ len ( args . out_fname ) - 4 : ] != '.fil' ) : 
~~ if args . f_start == None and args . f_stop == None : 
~~ if args . f_start == None : 
~~ if args . f_stop == None : 
~~ file_big = Waterfall ( args . in_fname , max_load = args . max_load ) 
f_min_file = file_big . header [ 'fch1' ] 
f_max_file = file_big . header [ 'fch1' ] + file_big . header [ 'nchans' ] * file_big . header [ 'foff' ] 
if f_max_file < f_min_file : 
~~~ f_max_file , f_min_file = f_min_file , f_max_file 
~~ FreqBWFile = f_max_file - f_min_file 
if args . f_stop < args . f_start : 
~~~ args . f_stop , args . f_start = args . f_start , args . f_stop 
~~ if args . f_start < f_max_file and args . f_start > f_min_file and args . f_stop > f_max_file : 
~~~ args . f_stop = f_max_file 
~~ if args . f_stop < f_max_file and args . f_stop > f_min_file and args . f_start < f_min_file : 
~~~ args . f_start = f_min_file 
~~ if args . f_start < f_min_file and args . f_stop > f_max_file : 
args . f_stop = f_max_file 
~~ if min ( args . f_start , args . f_stop ) < f_min_file or max ( args . f_start , args . f_stop ) > f_max_file : 
~~ f_start_real = math . floor ( ( min ( args . f_start , args . f_stop ) - f_min_file ) / stdDF ) * stdDF + f_min_file 
f_stop_real = f_max_file - math . floor ( ( f_max_file - max ( args . f_start , args . f_stop ) ) / stdDF ) * stdDF 
file_small = Waterfall ( args . in_fname , f_start = f_start_real , f_stop = f_stop_real , max_load = args . max_load ) 
if args . out_fname [ len ( args . out_fname ) - 4 : ] == '.fil' : 
~~~ file_small . write_to_fil ( args . out_fname ) 
~~ elif args . out_fname [ len ( args . out_fname ) - 3 : ] == '.h5' : 
~~~ file_small . write_to_hdf5 ( args . out_fname ) 
~~ ~~ def cmd_tool ( args = None ) : 
if not HAS_BITSHUFFLE : 
exit ( ) 
~~ filelist = glob . glob ( os . path . join ( args . dirname , '*.fil' ) ) 
for filename in filelist : 
~~~ if not os . path . exists ( filename + '.h5' ) : 
~~~ t0 = time . time ( ) 
fb = Filterbank ( filename , load_data = False ) 
data_shape = ( fb . n_ints_in_file , fb . header [ 'nifs' ] , fb . header [ 'nchans' ] ) 
data_dtype = fb . data . dtype 
print ( data_dtype ) 
block_size = 0 
h5 = h5py . File ( filename + '.h5' , 'w' ) 
h5 . attrs [ 'CLASS' ] = 'FILTERBANK' 
dset = h5 . create_dataset ( 'data' , 
shape = data_shape , 
compression = bitshuffle . h5 . H5FILTER , 
compression_opts = ( block_size , bitshuffle . h5 . H5_COMPRESS_LZ4 ) , 
dtype = data_dtype ) 
dset_mask = h5 . create_dataset ( 'mask' , 
dset . dims [ 0 ] . label = "frequency" 
dset . dims [ 1 ] . label = "feed_id" 
dset . dims [ 2 ] . label = "time" 
dset_mask . dims [ 0 ] . label = "frequency" 
dset_mask . dims [ 1 ] . label = "feed_id" 
dset_mask . dims [ 2 ] . label = "time" 
for key , value in fb . header . items ( ) : 
~~ filesize = os . path . getsize ( filename ) 
if filesize >= MAX_SIZE : 
~~~ n_int_per_read = int ( filesize / MAX_SIZE / 2 ) 
for ii in range ( 0 , n_int_per_read ) : 
fb = Filterbank ( filename , t_start = ii * n_int_per_read , t_stop = ( ii + 1 ) * n_int_per_read ) 
dset [ ii * n_int_per_read : ( ii + 1 ) * n_int_per_read ] = fb . data [ : ] 
~~~ fb = Filterbank ( filename ) 
dset [ : ] = fb . data [ : ] 
~~ h5 . close ( ) 
t1 = time . time ( ) 
~~ ~~ ~~ def open_file ( filename , f_start = None , f_stop = None , t_start = None , t_stop = None , load_data = True , max_load = 1. ) : 
if not os . path . isfile ( filename ) : 
~~~ type ( filename ) 
print ( filename ) 
~~ filename = os . path . expandvars ( os . path . expanduser ( filename ) ) 
ext = filename . split ( "." ) [ - 1 ] . strip ( ) . lower ( ) 
if six . PY3 : 
~~~ ext = bytes ( ext , 'ascii' ) 
~~ if h5py . is_hdf5 ( filename ) : 
~~~ return H5Reader ( filename , f_start = f_start , f_stop = f_stop , t_start = t_start , t_stop = t_stop , 
load_data = load_data , max_load = max_load ) 
~~ elif sigproc . is_filterbank ( filename ) : 
~~~ return FilReader ( filename , f_start = f_start , f_stop = f_stop , t_start = t_start , t_stop = t_stop , load_data = load_data , max_load = max_load ) 
~~ ~~ def _setup_selection_range ( self , f_start = None , f_stop = None , t_start = None , t_stop = None , init = False ) : 
if init is True : 
~~~ if t_start is None : 
~~~ t_start = self . t_begin 
~~ if t_stop is None : 
~~~ t_stop = self . t_end 
~~ if f_start is None : 
~~~ f_start = self . f_begin 
~~ if f_stop is None : 
~~~ f_stop = self . f_end 
~~~ if f_start is None : 
~~~ f_start = self . f_start 
~~~ f_stop = self . f_stop 
~~ if t_start is None : 
~~~ t_start = self . t_start 
~~~ t_stop = self . t_stop 
~~ ~~ if t_stop >= 0 and t_start >= 0 and t_stop < t_start : 
~~~ t_stop , t_start = t_start , t_stop 
~~ if f_stop and f_start and f_stop < f_start : 
~~~ f_stop , f_start = f_start , f_stop 
~~ if t_start >= self . t_begin and t_start < self . t_end : 
~~~ self . t_start = int ( t_start ) 
~~~ if init is False or t_start != None : 
~~ self . t_start = self . t_begin 
~~ if t_stop <= self . t_end and t_stop > self . t_begin : 
~~~ self . t_stop = int ( t_stop ) 
~~~ if init is False or t_stop : 
~~ self . t_stop = self . t_end 
~~ if f_start >= self . f_begin and f_start < self . f_end : 
~~~ self . f_start = f_start 
~~~ if init is False or f_start : 
~~ self . f_start = self . f_begin 
~~ if f_stop <= self . f_end and f_stop > self . f_begin : 
~~~ self . f_stop = f_stop 
~~~ if init is False or f_stop : 
~~ self . f_stop = self . f_end 
~~ self . selection_shape = self . _calc_selection_shape ( ) 
~~ def _setup_dtype ( self ) : 
if self . _n_bytes == 4 : 
~~~ return b'float32' 
~~ elif self . _n_bytes == 2 : 
~~~ return b'uint16' 
~~ elif self . _n_bytes == 1 : 
~~~ return b'uint8' 
return b'float32' 
~~ ~~ def _calc_selection_size ( self ) : 
n_ints = self . t_stop - self . t_start 
n_chan = ( self . f_stop - self . f_start ) / abs ( self . header [ b'foff' ] ) 
n_bytes = self . _n_bytes 
selection_size = int ( n_ints * n_chan * n_bytes ) 
return selection_size 
~~ def _calc_selection_shape ( self ) : 
n_ints = int ( self . t_stop - self . t_start ) 
n_chan = int ( np . round ( ( self . f_stop - self . f_start ) / abs ( self . header [ b'foff' ] ) ) ) 
selection_shape = ( n_ints , int ( self . header [ b'nifs' ] ) , n_chan ) 
return selection_shape 
~~ def _setup_chans ( self ) : 
~~~ f0 = self . f_end 
~~~ f0 = self . f_begin 
~~ i_start , i_stop = 0 , self . n_channels_in_file 
if self . f_start : 
~~~ i_start = np . round ( ( self . f_start - f0 ) / self . header [ b'foff' ] ) 
~~ if self . f_stop : 
~~~ i_stop = np . round ( ( self . f_stop - f0 ) / self . header [ b'foff' ] ) 
~~ self . chan_start_idx = chan_start_idx 
self . chan_stop_idx = chan_stop_idx 
~~ def _setup_freqs ( self ) : 
if self . header [ b'foff' ] > 0 : 
~~~ self . f_start = self . f_begin + self . chan_start_idx * abs ( self . header [ b'foff' ] ) 
self . f_stop = self . f_begin + self . chan_stop_idx * abs ( self . header [ b'foff' ] ) 
~~~ self . f_start = self . f_end - self . chan_stop_idx * abs ( self . header [ b'foff' ] ) 
self . f_stop = self . f_end - self . chan_start_idx * abs ( self . header [ b'foff' ] ) 
~~ ~~ def populate_timestamps ( self , update_header = False ) : 
if self . t_start : 
~~~ ii_start = self . t_start 
~~ if self . t_stop : 
~~~ ii_stop = self . t_stop 
~~ t0 = self . header [ b'tstart' ] 
if update_header : 
~~~ timestamps = ii_start * t_delt / 24. / 60. / 60. + t0 
~~~ timestamps = np . arange ( ii_start , ii_stop ) * t_delt / 24. / 60. / 60. + t0 
~~ return timestamps 
~~ def populate_freqs ( self ) : 
~~ self . _setup_chans ( ) 
i_vals = np . arange ( self . chan_start_idx , self . chan_stop_idx ) 
freqs = self . header [ b'foff' ] * i_vals + f0 
~~ def calc_n_coarse_chan ( self , chan_bw = None ) : 
nchans = int ( self . header [ b'nchans' ] ) 
if chan_bw is not None : 
~~~ bandwidth = abs ( self . f_stop - self . f_start ) 
n_coarse_chan = int ( bandwidth / chan_bw ) 
return n_coarse_chan 
~~ elif nchans >= 2 ** 20 : 
~~~ if nchans % 2 ** 20 == 0 : 
~~~ n_coarse_chan = nchans // 2 ** 20 
~~ elif self . header [ b'telescope_id' ] == 6 : 
~~~ coarse_chan_bw = 2.9296875 
bandwidth = abs ( self . f_stop - self . f_start ) 
n_coarse_chan = int ( bandwidth / coarse_chan_bw ) 
~~~ logger . warning ( "Couldn\ ) 
~~ ~~ elif self . header [ b'telescope_id' ] == 6 and nchans < 2 ** 20 : 
~~ ~~ def calc_n_blobs ( self , blob_dim ) : 
n_blobs = int ( np . ceil ( 1.0 * np . prod ( self . selection_shape ) / np . prod ( blob_dim ) ) ) 
return n_blobs 
~~ def isheavy ( self ) : 
selection_size_bytes = self . _calc_selection_size ( ) 
if selection_size_bytes > self . MAX_DATA_ARRAY_SIZE : 
~~ ~~ def read_header ( self ) : 
for key , val in self . h5 [ 'data' ] . attrs . items ( ) : 
~~ ~~ return self . header 
~~ def _find_blob_start ( self , blob_dim , n_blob ) : 
self . _setup_chans ( ) 
blob_time_start = self . t_start + blob_dim [ self . time_axis ] * n_blob 
blob_freq_start = self . chan_start_idx + ( blob_dim [ self . freq_axis ] * n_blob ) % self . selection_shape [ self . freq_axis ] 
blob_start = np . array ( [ blob_time_start , 0 , blob_freq_start ] ) 
return blob_start 
~~ def read_data ( self , f_start = None , f_stop = None , t_start = None , t_stop = None ) : 
self . _setup_selection_range ( f_start = f_start , f_stop = f_stop , t_start = t_start , t_stop = t_stop ) 
if self . isheavy ( ) : 
self . data = np . array ( [ 0 ] , dtype = self . _d_type ) 
self . _setup_freqs ( ) 
self . data = self . h5 [ "data" ] [ self . t_start : self . t_stop , : , self . chan_start_idx : self . chan_stop_idx ] 
~~ def read_blob ( self , blob_dim , n_blob = 0 ) : 
n_blobs = self . calc_n_blobs ( blob_dim ) 
if n_blob > n_blobs or n_blob < 0 : 
~~ if blob_dim [ self . time_axis ] * ( n_blob + 1 ) > self . selection_shape [ self . time_axis ] : 
~~~ updated_blob_dim = ( self . selection_shape [ self . time_axis ] - blob_dim [ self . time_axis ] * n_blob , 1 , blob_dim [ self . freq_axis ] ) 
~~~ updated_blob_dim = [ int ( i ) for i in blob_dim ] 
~~ blob_start = self . _find_blob_start ( blob_dim , n_blob ) 
blob_end = blob_start + np . array ( updated_blob_dim ) 
blob = self . h5 [ "data" ] [ int ( blob_start [ self . time_axis ] ) : int ( blob_end [ self . time_axis ] ) , 
: , 
int ( blob_start [ self . freq_axis ] ) : int ( blob_end [ self . freq_axis ] ) 
return blob 
~~ def read_header ( self , return_idxs = False ) : 
self . header = sigproc . read_header ( self . filename , return_idxs = return_idxs ) 
return self . header 
n_chans_selected = self . selection_shape [ self . freq_axis ] 
f = open ( self . filename , 'rb' ) 
f . seek ( int ( self . idx_data ) ) 
f . seek ( int ( self . t_start * self . _n_bytes * n_ifs * n_chans ) , 1 ) 
self . data = np . zeros ( ( n_ints , n_ifs , n_chans_selected ) , dtype = self . _d_type ) 
for ii in range ( n_ints ) : 
~~~ for jj in range ( n_ifs ) : 
dd = np . fromfile ( f , count = n_chans_selected , dtype = self . _d_type ) 
self . data [ ii , jj ] = dd 
f . seek ( int ( self . _n_bytes * ( n_chans - self . chan_stop_idx ) ) , 1 ) 
~~ ~~ ~~ def _find_blob_start ( self ) : 
blob_time_start = self . t_start 
blob_freq_start = self . chan_start_idx 
blob_start = blob_time_start * self . n_channels_in_file + blob_freq_start 
~~~ updated_blob_dim = ( int ( self . selection_shape [ self . time_axis ] - blob_dim [ self . time_axis ] * n_blob ) , 1 , int ( blob_dim [ self . freq_axis ] ) ) 
~~ blob_start = self . _find_blob_start ( ) 
blob = np . zeros ( updated_blob_dim , dtype = self . _d_type ) 
if self . f_start == self . f_begin and self . f_stop == self . f_end : 
~~~ blob_flat_size = np . prod ( blob_dim ) 
updated_blob_flat_size = np . prod ( updated_blob_dim ) 
with open ( self . filename , 'rb' ) as f : 
~~~ f . seek ( int ( self . idx_data + self . _n_bytes * ( blob_start + n_blob * blob_flat_size ) ) ) 
dd = np . fromfile ( f , count = updated_blob_flat_size , dtype = self . _d_type ) 
~~ if dd . shape [ 0 ] == updated_blob_flat_size : 
~~~ blob = dd . reshape ( updated_blob_dim ) 
blob = dd . reshape ( ( int ( dd . shape [ 0 ] / blob_dim [ self . freq_axis ] ) , blob_dim [ self . beam_axis ] , blob_dim [ self . freq_axis ] ) ) 
~~~ for blobt in range ( updated_blob_dim [ self . time_axis ] ) : 
~~~ with open ( self . filename , 'rb' ) as f : 
~~~ f . seek ( int ( self . idx_data + self . _n_bytes * ( blob_start + n_blob * blob_dim [ self . time_axis ] * self . n_channels_in_file + blobt * self . n_channels_in_file ) ) ) 
dd = np . fromfile ( f , count = blob_dim [ self . freq_axis ] , dtype = self . _d_type ) 
~~ blob [ blobt ] = dd 
~~ ~~ return blob 
~~ def read_all ( self , reverse = True ) : 
self . filfile . seek ( int ( self . datastart ) ) 
data = np . fromfile ( self . filfile , dtype = self . dtype ) . reshape ( self . blocksize , self . channels ) 
if reverse : 
~~~ data = data [ : , : : - 1 ] 
~~ def read_row ( self , rownumber , reverse = True ) : 
self . filfile . seek ( int ( self . datastart + self . channels * rownumber * ( int ( self . nbits / 8 ) ) ) ) 
data = np . fromfile ( self . filfile , count = self . channels , dtype = self . dtype ) . reshape ( 1 , self . channels ) 
parser . add_argument ( '-p' , action = 'store' , default = 'a' , dest = 'what_to_plot' , type = str , 
parser . add_argument ( '-H' , action = 'store_true' , default = False , dest = 'to_hdf5' , 
parser . add_argument ( '-F' , action = 'store_true' , default = False , dest = 'to_fil' , 
parser . add_argument ( '-o' , action = 'store' , default = None , dest = 'filename_out' , type = str , 
parser . add_argument ( '-l' , action = 'store' , default = None , dest = 'max_load' , type = float , 
if args is None : 
~~~ args = sys . argv [ 1 : ] 
~~ parse_args = parser . parse_args ( args ) 
filename = parse_args . filename 
load_data = not parse_args . info_only 
info_only = parse_args . info_only 
filename_out = parse_args . filename_out 
fil = Waterfall ( filename , f_start = parse_args . f_start , f_stop = parse_args . f_stop , t_start = parse_args . t_start , t_stop = parse_args . t_stop , load_data = load_data , max_load = parse_args . max_load ) 
if fil . container . isheavy ( ) or parse_args . to_hdf5 or parse_args . to_fil : 
~~~ info_only = True 
~~ if not info_only : 
~~~ print ( '' ) 
if parse_args . blank_dc : 
n_coarse_chan = fil . calc_n_coarse_chan ( ) 
fil . blank_dc ( n_coarse_chan ) 
~~ if parse_args . what_to_plot == "w" : 
fil . plot_waterfall ( f_start = parse_args . f_start , f_stop = parse_args . f_stop ) 
~~ elif parse_args . what_to_plot == "s" : 
fil . plot_spectrum ( logged = True , f_start = parse_args . f_start , f_stop = parse_args . f_stop , t = 'all' ) 
~~ elif parse_args . what_to_plot == "mm" : 
fil . plot_spectrum_min_max ( logged = True , f_start = parse_args . f_start , f_stop = parse_args . f_stop , t = 'all' ) 
~~ elif parse_args . what_to_plot == "k" : 
fil . plot_kurtosis ( f_start = parse_args . f_start , f_stop = parse_args . f_stop ) 
~~ elif parse_args . what_to_plot == "t" : 
fil . plot_time_series ( f_start = parse_args . f_start , f_stop = parse_args . f_stop , orientation = 'h' ) 
~~ elif parse_args . what_to_plot == "a" : 
fil . plot_all ( logged = True , f_start = parse_args . f_start , f_stop = parse_args . f_stop , t = 'all' ) 
~~ elif parse_args . what_to_plot == "ank" : 
fil . plot_all ( logged = True , f_start = parse_args . f_start , f_stop = parse_args . f_stop , t = 'all' , kutosis = False ) 
~~ if parse_args . plt_filename != '' : 
~~~ plt . savefig ( parse_args . plt_filename ) 
~~ if not parse_args . save_only : 
~~~ if parse_args . to_hdf5 and parse_args . to_fil : 
~~ if parse_args . to_hdf5 : 
~~~ if not filename_out : 
~~~ filename_out = filename . replace ( '.fil' , '.h5' ) 
~~ elif '.h5' not in filename_out : 
~~~ filename_out = filename_out . replace ( '.fil' , '' ) + '.h5' 
fil . write_to_hdf5 ( filename_out ) 
~~ if parse_args . to_fil : 
~~~ filename_out = filename . replace ( '.h5' , '.fil' ) 
~~ elif '.fil' not in filename_out : 
~~~ filename_out = filename_out . replace ( '.h5' , '' ) + '.fil' 
fil . write_to_fil ( filename_out ) 
~~ ~~ ~~ def read_data ( self , f_start = None , f_stop = None , t_start = None , t_stop = None ) : 
self . container . read_data ( f_start = f_start , f_stop = f_stop , t_start = t_start , t_stop = t_stop ) 
self . __load_data ( ) 
~~ def __update_header ( self ) : 
~~~ self . header [ b'fch1' ] = self . container . f_stop 
~~~ self . header [ b'fch1' ] = self . container . f_start 
~~ self . header [ b'nchans' ] = self . container . selection_shape [ self . freq_axis ] 
self . header [ b'tstart' ] = self . container . populate_timestamps ( update_header = True ) 
~~ def info ( self ) : 
for key , val in self . file_header . items ( ) : 
~~~ if key == 'src_raj' : 
~~ if key == 'src_dej' : 
~~ def write_to_fil ( self , filename_out , * args , ** kwargs ) : 
t0 = time . time ( ) 
self . __update_header ( ) 
if self . container . isheavy ( ) : 
~~~ self . __write_to_fil_heavy ( filename_out ) 
~~~ self . __write_to_fil_light ( filename_out ) 
~~ t1 = time . time ( ) 
~~ def __write_to_fil_heavy ( self , filename_out , * args , ** kwargs ) : 
chunk_dim = self . __get_chunk_dimensions ( ) 
blob_dim = self . __get_blob_dimensions ( chunk_dim ) 
n_blobs = self . container . calc_n_blobs ( blob_dim ) 
n_bytes = self . header [ b'nbits' ] / 8 
for ii in range ( 0 , n_blobs ) : 
bob = self . container . read_blob ( blob_dim , n_blob = ii ) 
with open ( filename_out , "a" ) as fileh : 
~~~ j = bob 
~~ ~~ ~~ ~~ def __write_to_fil_light ( self , filename_out , * args , ** kwargs ) : 
~~~ self . __write_to_hdf5_heavy ( filename_out ) 
~~~ self . __write_to_hdf5_light ( filename_out ) 
~~ def __write_to_hdf5_heavy ( self , filename_out , * args , ** kwargs ) : 
with h5py . File ( filename_out , 'w' ) as h5 : 
~~~ h5 . attrs [ b'CLASS' ] = b'FILTERBANK' 
h5 . attrs [ b'VERSION' ] = b'1.0' 
if HAS_BITSHUFFLE : 
~~~ bs_compression = bitshuffle . h5 . H5FILTER 
bs_compression_opts = ( block_size , bitshuffle . h5 . H5_COMPRESS_LZ4 ) 
~~~ bs_compression = None 
bs_compression_opts = None 
~~ dset = h5 . create_dataset ( 'data' , 
shape = self . selection_shape , 
chunks = chunk_dim , 
compression = bs_compression , 
compression_opts = bs_compression_opts , 
dtype = self . data . dtype ) 
~~ if blob_dim [ self . freq_axis ] < self . selection_shape [ self . freq_axis ] : 
#----- 
c_start = self . container . chan_start_idx + ii * blob_dim [ self . freq_axis ] 
t_start = self . container . t_start + ( c_start / self . selection_shape [ self . freq_axis ] ) * blob_dim [ self . time_axis ] 
t_stop = t_start + blob_dim [ self . time_axis ] 
c_start = ( c_start ) % self . selection_shape [ self . freq_axis ] 
c_stop = c_start + blob_dim [ self . freq_axis ] 
logger . debug ( t_start , t_stop , c_start , c_stop ) 
dset [ t_start : t_stop , 0 , c_start : c_stop ] = bob [ : ] 
t_start = self . container . t_start + ii * blob_dim [ self . time_axis ] 
if ( ii + 1 ) * blob_dim [ self . time_axis ] > self . n_ints_in_file : 
~~~ t_stop = self . n_ints_in_file 
~~~ t_stop = ( ii + 1 ) * blob_dim [ self . time_axis ] 
~~ dset [ t_start : t_stop ] = bob [ : ] 
~~ ~~ ~~ ~~ def __write_to_hdf5_light ( self , filename_out , * args , ** kwargs ) : 
compression_opts = bs_compression_opts ) 
shape = self . file_shape , 
~~ ~~ ~~ def __get_blob_dimensions ( self , chunk_dim ) : 
if self . selection_shape [ self . freq_axis ] > chunk_dim [ self . freq_axis ] * MAX_BLOB_MB : 
~~~ freq_axis_size = self . selection_shape [ self . freq_axis ] 
time_axis_size = 1 
time_axis_size = np . min ( [ chunk_dim [ self . time_axis ] * MAX_BLOB_MB * chunk_dim [ self . freq_axis ] / freq_axis_size , self . selection_shape [ self . time_axis ] ] ) 
~~ blob_dim = ( int ( time_axis_size ) , 1 , freq_axis_size ) 
return blob_dim 
~~ def __get_chunk_dimensions ( self ) : 
if np . abs ( self . header [ b'foff' ] ) < 1e-5 : 
return chunk_dim 
~~ elif np . abs ( self . header [ b'tsamp' ] ) < 1e-3 : 
~~ elif np . abs ( self . header [ b'foff' ] ) < 1e-2 and np . abs ( self . header [ b'foff' ] ) >= 1e-5 : 
chunk_dim = ( 1 , 1 , 512 ) 
~~ ~~ def grab_data ( self , f_start = None , f_stop = None , t_start = None , t_stop = None , if_id = 0 ) : 
self . freqs = self . populate_freqs ( ) 
self . timestamps = self . populate_timestamps ( ) 
if f_start is None : 
~~~ f_start = self . freqs [ 0 ] 
~~~ f_stop = self . freqs [ - 1 ] 
~~ i0 = np . argmin ( np . abs ( self . freqs - f_start ) ) 
i1 = np . argmin ( np . abs ( self . freqs - f_stop ) ) 
if i0 < i1 : 
~~~ plot_f = self . freqs [ i0 : i1 + 1 ] 
plot_data = np . squeeze ( self . data [ t_start : t_stop , ... , i0 : i1 + 1 ] ) 
~~~ plot_f = self . freqs [ i1 : i0 + 1 ] 
plot_data = np . squeeze ( self . data [ t_start : t_stop , ... , i1 : i0 + 1 ] ) 
~~ return plot_f , plot_data 
r = GuppiRaw ( args . filename ) 
r . print_stats ( ) 
bname = os . path . splitext ( os . path . basename ( args . filename ) ) [ 0 ] 
bname = os . path . join ( args . outdir , bname ) 
r . plot_histogram ( filename = "%s_hist.png" % bname ) 
r . plot_spectrum ( filename = "%s_spec.png" % bname ) 
~~ def read_header ( self ) : 
start_idx = self . file_obj . tell ( ) 
key , val = '' , '' 
header_dict = { } 
keep_reading = True 
first_line = self . file_obj 
~~~ while keep_reading : 
~~~ if start_idx + 80 > self . filesize : 
~~~ keep_reading = False 
~~ line = self . file_obj . read ( 80 ) 
if PYTHON3 : 
~~~ line = line . decode ( "utf-8" ) 
~~ if line . startswith ( 'END' ) : 
~~~ key , val = line . split ( '=' ) 
key , val = key . strip ( ) , val . strip ( ) 
if "\ in val : 
~~~ val = str ( val . strip ( "\ ) . strip ( ) ) 
~~ elif "." in val : 
~~~ val = float ( val ) 
~~~ val = int ( val ) 
~~ ~~ header_dict [ key ] = val 
print ( self . file_obj . read ( 512 ) ) 
raise 
~~ data_idx = self . file_obj . tell ( ) 
if "DIRECTIO" in header_dict . keys ( ) : 
~~~ if int ( header_dict [ "DIRECTIO" ] ) == 1 : 
~~~ if data_idx % 512 : 
~~~ data_idx += ( 512 - data_idx % 512 ) 
~~ ~~ ~~ self . file_obj . seek ( start_idx ) 
return header_dict , data_idx 
~~ def read_first_header ( self ) : 
self . file_obj . seek ( 0 ) 
header_dict , pos = self . read_header ( ) 
return header_dict 
~~ def get_data ( self ) : 
with self as gr : 
~~~ while True : 
~~~ yield gr . read_next_data_block_int8 ( ) 
yield None , None , None 
~~ ~~ ~~ ~~ def read_next_data_block_int8 ( self ) : 
header , data_idx = self . read_header ( ) 
self . file_obj . seek ( data_idx ) 
n_chan = int ( header [ 'OBSNCHAN' ] ) 
n_pol = int ( header [ 'NPOL' ] ) 
n_bit = int ( header [ 'NBITS' ] ) 
n_samples = int ( int ( header [ 'BLOCSIZE' ] ) / ( n_chan * n_pol * ( n_bit / 8 ) ) ) 
d = np . fromfile ( self . file_obj , count = header [ 'BLOCSIZE' ] , dtype = 'int8' ) 
if n_bit != 8 : 
~~~ d = unpack ( d , n_bit ) 
if self . _d_x . shape != d [ ... , 0 : 2 ] . shape : 
~~~ self . _d_x = np . ascontiguousarray ( np . zeros ( d [ ... , 0 : 2 ] . shape , dtype = 'int8' ) ) 
self . _d_y = np . ascontiguousarray ( np . zeros ( d [ ... , 2 : 4 ] . shape , dtype = 'int8' ) ) 
~~ self . _d_x [ : ] = d [ ... , 0 : 2 ] 
self . _d_y [ : ] = d [ ... , 2 : 4 ] 
return header , self . _d_x , self . _d_y 
~~ def read_next_data_block_int8_2x ( self ) : 
d2 = np . fromfile ( self . file_obj , count = header [ 'BLOCSIZE' ] , dtype = 'int8' ) 
d2 = d2 . reshape ( ( n_chan , n_samples , n_pol ) ) 
d = np . concatenate ( ( d , d2 ) , axis = 1 ) 
print ( d . shape ) 
if self . _d_x . shape != ( n_chan , n_samples * 2 , n_pol ) : 
~~ def read_next_data_block ( self ) : 
d = np . ascontiguousarray ( np . fromfile ( self . file_obj , count = header [ 'BLOCSIZE' ] , dtype = 'int8' ) ) 
~~ dshape = self . read_next_data_block_shape ( ) 
if self . _d . shape != d . shape : 
~~~ self . _d = np . zeros ( d . shape , dtype = 'float32' ) 
~~ self . _d [ : ] = d 
return header , self . _d [ : ] . view ( 'complex64' ) 
~~ def find_n_data_blocks ( self ) : 
header0 , data_idx0 = self . read_header ( ) 
self . file_obj . seek ( data_idx0 ) 
block_size = int ( header0 [ 'BLOCSIZE' ] ) 
n_bits = int ( header0 [ 'NBITS' ] ) 
self . file_obj . seek ( int ( header0 [ 'BLOCSIZE' ] ) , 1 ) 
n_blocks = 1 
end_found = False 
while not end_found : 
~~~ header , data_idx = self . read_header ( ) 
self . file_obj . seek ( header [ 'BLOCSIZE' ] , 1 ) 
n_blocks += 1 
~~ except EndOfFileError : 
~~~ end_found = True 
~~ ~~ self . file_obj . seek ( 0 ) 
return n_blocks 
~~ def print_stats ( self ) : 
header , data = self . read_next_data_block ( ) 
data = data . view ( 'float32' ) 
import pylab as plt 
~~ def plot_histogram ( self , filename = None ) : 
plt . figure ( "Histogram" ) 
plt . hist ( data . flatten ( ) , 65 , facecolor = '#cc0000' ) 
~~~ plt . savefig ( filename ) 
~~ plt . show ( ) 
~~ def plot_spectrum ( self , filename = None , plot_db = True ) : 
d_xx_fft = np . abs ( np . fft . fft ( data [ ... , 0 ] ) ) 
d_xx_fft = d_xx_fft . flatten ( ) 
dec_fac_x = 1 
if d_xx_fft . shape [ 0 ] > MAX_PLT_POINTS : 
~~~ dec_fac_x = d_xx_fft . shape [ 0 ] / MAX_PLT_POINTS 
~~ d_xx_fft = rebin ( d_xx_fft , dec_fac_x ) 
print ( "Plotting..." ) 
if plot_db : 
~~~ plt . plot ( 10 * np . log10 ( d_xx_fft ) ) 
~~~ plt . plot ( d_xx_fft ) 
plt . ylabel ( "Power" ) 
plt . xlabel ( "Channel" ) 
~~ plt . title ( self . filename ) 
~~ def generate_filterbank_header ( self , nchans = 1 , ) : 
gp_head = self . read_first_header ( ) 
fb_head = { } 
telescope_str = gp_head . get ( "TELESCOP" , "unknown" ) 
if telescope_str in ( 'GBT' , 'GREENBANK' ) : 
~~~ fb_head [ "telescope_id" ] = 6 
~~ elif telescope_str in ( 'PKS' , 'PARKES' ) : 
~~~ fb_head [ "telescop_id" ] = 7 
~~~ fb_head [ "telescop_id" ] = 0 
~~ fb_head [ "source_name" ] = gp_head . get ( "SRC_NAME" , "unknown" ) 
fb_head [ "az_start" ] = gp_head . get ( "AZ" , 0 ) 
fb_head [ "za_start" ] = gp_head . get ( "ZA" , 0 ) 
fb_head [ "src_raj" ] = Angle ( str ( gp_head . get ( "RA" , 0.0 ) ) + "hr" ) 
fb_head [ "src_dej" ] = Angle ( str ( gp_head . get ( "DEC" , 0.0 ) ) + "deg" ) 
fb_head [ "rawdatafile" ] = self . filename 
fb_head [ "machine_id" ] = 20 
fb_head [ "barycentric" ] = 0 
fb_head [ "pulsarcentric" ] = 0 
fb_head [ "nbits" ] = 32 
fb_head [ "tstart" ] = 0.0 
fb_head [ "tsamp" ] = 1.0 
fb_head [ "fch1" ] = 0.0 
fb_head [ "foff" ] = 187.5 / nchans 
fb_head [ "nchans" ] = nchans 
fb_head [ "nifs" ] = 1 
fb_head [ "nbeams" ] = 1 
return fb_head 
~~ def find_header_size ( filename ) : 
filfile = open ( filename , 'rb' ) 
filfile . seek ( 0 ) 
round1 = filfile . read ( 1000 ) 
headersize = round1 . find ( 'HEADER_END' ) + len ( 'HEADER_END' ) 
return headersize 
if 'bl' in local_host : 
~~ p = OptionParser ( ) 
opts , args = p . parse_args ( sys . argv [ 1 : ] ) 
file1 = args [ 0 ] 
file2 = args [ 1 ] 
#------------------------------------ 
make_batch_script ( ) 
headersize1 = find_header_size ( file1 ) 
file_size1 = os . path . getsize ( file1 ) 
#command=['tail','-c',str(file_size1-headersize1),file1,'|','md5sum'] 
command = [ './tail_sum.sh' , file1 , str ( file_size1 - headersize1 ) ] 
proc = subprocess . Popen ( command , stdout = subprocess . PIPE , stderr = subprocess . PIPE ) 
( out , err ) = proc . communicate ( ) 
check_sum1 = out . split ( ) [ 0 ] 
if err : 
#--- 
~~ out , err = reset_outs ( ) 
command = [ header_loc , file1 ] 
header1 = out 
print ( header1 ) 
out , err = reset_outs ( ) 
headersize2 = find_header_size ( file2 ) 
file_size2 = os . path . getsize ( file2 ) 
command = [ './tail_sum.sh' , file2 , str ( file_size2 - headersize2 ) ] 
check_sum2 = out . split ( ) [ 0 ] 
command = [ header_loc , file2 ] 
header2 = out 
print ( header2 ) 
if check_sum1 != check_sum2 : 
~~ os . remove ( 'tail_sum.sh' ) 
~~ def make_h5_file ( filename , out_dir = './' , new_filename = None , max_load = None ) : 
fil_file = Waterfall ( filename , max_load = max_load ) 
if not new_filename : 
~~~ new_filename = out_dir + filename . replace ( '.fil' , '.h5' ) . split ( '/' ) [ - 1 ] 
~~ if '.h5' not in new_filename : 
~~~ new_filename = new_filename + '.h5' 
~~ fil_file . write_to_hdf5 ( new_filename ) 
fileroot = args . filename . split ( '.0000.raw' ) [ 0 ] 
filelist = glob . glob ( fileroot + '*.raw' ) 
filelist = sorted ( filelist ) 
r = GuppiRaw ( filelist [ 0 ] ) 
header , data = r . read_next_data_block ( ) 
dshape = data . shape #r.read_next_data_block_shape() 
print ( dshape ) 
n_blocks_total = 0 
~~~ print ( filename ) 
r = GuppiRaw ( filename ) 
n_blocks_total += r . n_blocks 
~~ print ( n_blocks_total ) 
full_dshape = np . concatenate ( ( ( n_blocks_total , ) , dshape ) ) 
h5 = h5py . File ( fileroot + '.h5' , 'w' ) 
h5 . attrs [ 'CLASS' ] = 'GUPPIRAW' 
shape = full_dshape , 
#compression=bitshuffle.h5.H5FILTER, 
dtype = data . dtype ) 
h5_idx = 0 
for ii in range ( 0 , r . n_blocks ) : 
t2 = time . time ( ) 
dset [ h5_idx , : ] = data 
t3 = time . time ( ) 
h5_idx += 1 
for key , value in header . items ( ) : 
~~ ~~ h5 . close ( ) 
~~ ~~ def foldcal ( data , tsamp , diode_p = 0.04 , numsamps = 1000 , switch = False , inds = False ) : 
~~~ \ 
halfper = diode_p / 2.0 
ints = np . arange ( 0 , numsamps ) 
t_switch = ( onesec + ints * foldt ) 
t_switch = t_switch . astype ( 'int' ) 
ONints = np . array ( np . reshape ( t_switch [ : ] , ( numsamps / 2 , 2 ) ) ) 
OFFints = np . array ( np . reshape ( t_switch [ 1 : - 1 ] , ( numsamps / 2 - 1 , 2 ) ) ) 
av_ON = [ ] 
av_OFF = [ ] 
~~~ if i [ 1 ] != i [ 0 ] : 
~~~ av_ON . append ( np . sum ( data [ i [ 0 ] : i [ 1 ] , : , : ] , axis = 0 ) / ( i [ 1 ] - i [ 0 ] ) ) 
~~ ~~ for i in OFFints : 
~~~ av_OFF . append ( np . sum ( data [ i [ 0 ] : i [ 1 ] , : , : ] , axis = 0 ) / ( i [ 1 ] - i [ 0 ] ) ) 
~~ ~~ if switch == False : 
~~~ if inds == False : 
~~~ return np . squeeze ( np . mean ( av_ON , axis = 0 ) ) , np . squeeze ( np . mean ( av_OFF , axis = 0 ) ) 
~~~ return np . squeeze ( np . mean ( av_ON , axis = 0 ) ) , np . squeeze ( np . mean ( av_OFF , axis = 0 ) ) , ONints , OFFints 
~~ ~~ if switch == True : 
~~~ return np . squeeze ( np . mean ( av_OFF , axis = 0 ) ) , np . squeeze ( np . mean ( av_ON , axis = 0 ) ) 
~~~ return np . squeeze ( np . mean ( av_OFF , axis = 0 ) ) , np . squeeze ( np . mean ( av_ON , axis = 0 ) ) , OFFints , ONints 
~~ ~~ ~~ def integrate_chans ( spec , freqs , chan_per_coarse ) : 
spec_shaped = np . array ( np . reshape ( spec , ( num_coarse , chan_per_coarse ) ) ) 
freqs_shaped = np . array ( np . reshape ( freqs , ( num_coarse , chan_per_coarse ) ) ) 
return np . mean ( spec_shaped [ : , 1 : - 1 ] , axis = 1 ) 
~~ def integrate_calib ( name , chan_per_coarse , fullstokes = False , ** kwargs ) : 
obs = Waterfall ( name , max_load = 150 ) 
if fullstokes == False and data . shape [ 1 ] > 1 : 
~~~ data = data [ : , 0 , : ] + data [ : , 1 , : ] 
data = np . expand_dims ( data , axis = 1 ) 
~~ if fullstokes == True : 
~~~ data = data [ : , 0 , : ] 
~~ tsamp = obs . header [ 'tsamp' ] 
OFF , ON = foldcal ( data , tsamp , ** kwargs ) 
ON_int = integrate_chans ( ON , freqs , chan_per_coarse ) 
OFF_int = integrate_chans ( OFF , freqs , chan_per_coarse ) 
if np . sum ( ON_int ) < np . sum ( OFF_int ) : 
~~~ temp = ON_int 
ON_int = OFF_int 
OFF_int = temp 
~~ return OFF_int , ON_int 
~~ def get_calfluxes ( calflux , calfreq , spec_in , centerfreqs , oneflux ) : 
const = calflux / np . power ( calfreq , spec_in ) 
if oneflux == False : 
~~~ return const * np . power ( centerfreqs , spec_in ) 
~~~ return const * np . power ( np . mean ( centerfreqs ) , spec_in ) 
~~ ~~ def get_centerfreqs ( freqs , chan_per_coarse ) : 
num_coarse = freqs . size / chan_per_coarse 
freqs = np . reshape ( freqs , ( num_coarse , chan_per_coarse ) ) 
return np . mean ( freqs , axis = 1 ) 
~~ def f_ratios ( calON_obs , calOFF_obs , chan_per_coarse , ** kwargs ) : 
L_ON , H_ON = integrate_calib ( calON_obs , chan_per_coarse , ** kwargs ) 
L_OFF , H_OFF = integrate_calib ( calOFF_obs , chan_per_coarse , ** kwargs ) 
f_ON = H_ON / L_ON - 1 
f_OFF = H_OFF / L_OFF - 1 
return f_ON , f_OFF 
~~ def diode_spec ( calON_obs , calOFF_obs , calflux , calfreq , spec_in , average = True , oneflux = False , ** kwargs ) : 
obs = Waterfall ( calON_obs , max_load = 150 ) 
ncoarse = obs . calc_n_coarse_chan ( ) 
nchans = obs . header [ 'nchans' ] 
chan_per_coarse = nchans / ncoarse 
f_ON , f_OFF = f_ratios ( calON_obs , calOFF_obs , chan_per_coarse , ** kwargs ) 
centerfreqs = get_centerfreqs ( freqs , chan_per_coarse ) 
calfluxes = get_calfluxes ( calflux , calfreq , spec_in , centerfreqs , oneflux ) 
C_o = calfluxes / ( 1 / f_ON - 1 / f_OFF ) 
Tsys = C_o / f_OFF 
if average == True : 
~~~ return np . mean ( C_o ) , np . mean ( Tsys ) 
~~~ return C_o , Tsys 
~~ ~~ def get_Tsys ( calON_obs , calOFF_obs , calflux , calfreq , spec_in , oneflux = False , ** kwargs ) : 
return diode_spec ( calON_obs , calOFF_obs , calflux , calfreq , spec_in , average = False , oneflux = False , ** kwargs ) [ 1 ] 
~~ def calibrate_fluxes ( main_obs_name , dio_name , dspec , Tsys , fullstokes = False , ** kwargs ) : 
main_obs = Waterfall ( main_obs_name , max_load = 150 ) 
ncoarse = main_obs . calc_n_coarse_chan ( ) 
dio_obs = Waterfall ( dio_name , max_load = 150 ) 
dio_chan_per_coarse = dio_obs . header [ 'nchans' ] / ncoarse 
dOFF , dON = integrate_calib ( dio_name , dio_chan_per_coarse , fullstokes , ** kwargs ) 
main_dat = main_obs . data 
scale_facs = dspec / ( dON - dOFF ) 
print ( scale_facs ) 
nchans = main_obs . header [ 'nchans' ] 
obs_chan_per_coarse = nchans / ncoarse 
ax0_size = np . size ( main_dat , 0 ) 
ax1_size = np . size ( main_dat , 1 ) 
main_dat = np . reshape ( main_dat , ( ax0_size , ax1_size , ncoarse , obs_chan_per_coarse ) ) 
main_dat = np . swapaxes ( main_dat , 2 , 3 ) 
main_dat = main_dat * scale_facs 
main_dat = main_dat - Tsys 
main_dat = np . reshape ( main_dat , ( ax0_size , ax1_size , nchans ) ) 
main_obs . data = main_dat 
main_obs . write_to_filterbank ( main_obs_name [ : - 4 ] + '.fluxcal.fil' ) 
~~ def len_header ( filename ) : 
with open ( filename , 'rb' ) as f : 
~~~ header_sub_count = 0 
eoh_found = False 
while not eoh_found : 
~~~ header_sub = f . read ( 512 ) 
header_sub_count += 1 
if b'HEADER_END' in header_sub : 
~~~ idx_end = header_sub . index ( b'HEADER_END' ) + len ( b'HEADER_END' ) 
eoh_found = True 
~~ ~~ idx_end = ( header_sub_count - 1 ) * 512 + idx_end 
~~ return idx_end 
~~ def is_filterbank ( filename ) : 
with open ( filename , 'rb' ) as fh : 
~~~ is_fil = True 
~~~ keyword , value , idx = read_next_header_keyword ( fh ) 
~~~ assert keyword == b'HEADER_START' 
~~ except AssertionError : 
~~~ is_fil = False 
~~ ~~ except KeyError : 
~~ return is_fil 
~~ ~~ def read_header ( filename , return_idxs = False ) : 
~~~ header_dict = { } 
header_idxs = { } 
keyword , value , idx = read_next_header_keyword ( fh ) 
~~ while True : 
if keyword == b'HEADER_END' : 
~~~ header_dict [ keyword ] = value 
header_idxs [ keyword ] = idx 
~~ ~~ ~~ if return_idxs : 
~~~ return header_idxs 
~~~ return header_dict 
~~ ~~ def fix_header ( filename , keyword , new_value ) : 
hd = read_header ( filename ) 
hi = read_header ( filename , return_idxs = True ) 
idx = hi [ keyword ] 
dtype = header_keyword_types [ keyword ] 
dtype_to_type = { b'<l' : np . int32 , 
b'str' : bytes , 
b'<d' : np . float64 , 
b'angle' : to_sigproc_angle } 
value_dtype = dtype_to_type [ dtype ] 
if isinstance ( value_dtype , bytes ) : 
~~~ if len ( hd [ keyword ] ) == len ( new_value ) : 
~~~ val_str = np . int32 ( len ( new_value ) ) . tostring ( ) + new_value 
~~~ val_str = value_dtype ( new_value ) . tostring ( ) 
~~ with open ( filename , 'rb+' ) as fh : 
~~~ fh . seek ( idx ) 
fh . write ( val_str ) 
~~ ~~ def fil_double_to_angle ( angle ) : 
negative = ( angle < 0.0 ) 
angle = np . abs ( angle ) 
dd = np . floor ( ( angle / 10000 ) ) 
angle -= 10000 * dd 
mm = np . floor ( ( angle / 100 ) ) 
ss = angle - 100 * mm 
dd += mm / 60.0 + ss / 3600.0 
if negative : 
~~~ dd *= - 1 
~~ return dd 
~~ def to_sigproc_keyword ( keyword , value = None ) : 
keyword = bytes ( keyword ) 
if value is None : 
~~~ return np . int32 ( len ( keyword ) ) . tostring ( ) + keyword 
~~~ dtype = header_keyword_types [ keyword ] 
b'str' : str , 
if value_dtype is str : 
~~~ return np . int32 ( len ( keyword ) ) . tostring ( ) + keyword + np . int32 ( len ( value ) ) . tostring ( ) + value 
~~~ return np . int32 ( len ( keyword ) ) . tostring ( ) + keyword + value_dtype ( value ) . tostring ( ) 
~~ ~~ ~~ def generate_sigproc_header ( f ) : 
header_string = b'' 
header_string += to_sigproc_keyword ( b'HEADER_START' ) 
for keyword in f . header . keys ( ) : 
~~~ if keyword == b'src_raj' : 
~~~ header_string += to_sigproc_keyword ( b'src_raj' ) + to_sigproc_angle ( f . header [ b'src_raj' ] ) 
~~ elif keyword == b'src_dej' : 
~~~ header_string += to_sigproc_keyword ( b'src_dej' ) + to_sigproc_angle ( f . header [ b'src_dej' ] ) 
~~ elif keyword == b'az_start' or keyword == b'za_start' : 
~~~ header_string += to_sigproc_keyword ( keyword ) + np . float64 ( f . header [ keyword ] ) . tostring ( ) 
~~ elif keyword not in header_keyword_types . keys ( ) : 
~~~ header_string += to_sigproc_keyword ( keyword , f . header [ keyword ] ) 
~~ ~~ header_string += to_sigproc_keyword ( b'HEADER_END' ) 
return header_string 
~~ def to_sigproc_angle ( angle_val ) : 
x = str ( angle_val ) 
if '.' in x : 
~~~ if 'h' in x : 
~~~ d , m , s , ss = int ( x [ 0 : x . index ( 'h' ) ] ) , int ( x [ x . index ( 'h' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( '.' ) ] ) , float ( x [ x . index ( '.' ) : x . index ( 's' ) ] ) 
~~ if 'd' in x : 
~~~ d , m , s , ss = int ( x [ 0 : x . index ( 'd' ) ] ) , int ( x [ x . index ( 'd' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( '.' ) ] ) , float ( x [ x . index ( '.' ) : x . index ( 's' ) ] ) 
~~~ d , m , s = int ( x [ 0 : x . index ( 'h' ) ] ) , int ( x [ x . index ( 'h' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( 's' ) ] ) 
~~~ d , m , s = int ( x [ 0 : x . index ( 'd' ) ] ) , int ( x [ x . index ( 'd' ) + 1 : x . index ( 'm' ) ] ) , int ( x [ x . index ( 'm' ) + 1 : x . index ( 's' ) ] ) 
~~ ss = 0 
~~ num = str ( d ) . zfill ( 2 ) + str ( m ) . zfill ( 2 ) + str ( s ) . zfill ( 2 ) + '.' + str ( ss ) . split ( "." ) [ - 1 ] 
return np . float64 ( num ) . tostring ( ) 
~~ def calc_n_ints_in_file ( filename ) : 
h = read_header ( filename ) 
n_bytes = int ( h [ b'nbits' ] / 8 ) 
n_chans = h [ b'nchans' ] 
n_ifs = h [ b'nifs' ] 
idx_data = len_header ( filename ) 
f . seek ( idx_data ) 
filesize = os . path . getsize ( filename ) 
n_bytes_data = filesize - idx_data 
if h [ b'nbits' ] == 2 : 
~~~ n_ints = int ( 4 * n_bytes_data / ( n_chans * n_ifs ) ) 
~~~ n_ints = int ( n_bytes_data / ( n_bytes * n_chans * n_ifs ) ) 
~~ return n_ints 
~~ def make_fil_file ( filename , out_dir = './' , new_filename = None , max_load = None ) : 
~~~ new_filename = out_dir + filename . replace ( '.h5' , '.fil' ) . split ( '/' ) [ - 1 ] 
~~ if '.fil' not in new_filename : 
~~~ new_filename = new_filename + '.fil' 
~~ fil_file . write_to_fil ( new_filename ) 
~~ def make_rr_subparser ( subparsers , rec_type , args_and_types ) : 
sp = subparsers . add_parser ( rec_type ) 
sp . add_argument ( "name" , type = str ) 
sp . add_argument ( "ttl" , type = int , nargs = '?' ) 
sp . add_argument ( rec_type , type = str ) 
for my_spec in args_and_types : 
~~~ ( argname , argtype ) = my_spec [ : 2 ] 
if len ( my_spec ) > 2 : 
~~~ nargs = my_spec [ 2 ] 
sp . add_argument ( argname , type = argtype , nargs = nargs ) 
~~~ sp . add_argument ( argname , type = argtype ) 
~~ ~~ return sp 
~~ def make_parser ( ) : 
line_parser = ZonefileLineParser ( ) 
subparsers = line_parser . add_subparsers ( ) 
sp = subparsers . add_parser ( "$ORIGIN" ) 
sp . add_argument ( "$ORIGIN" , type = str ) 
sp = subparsers . add_parser ( "$TTL" ) 
sp . add_argument ( "$TTL" , type = int ) 
args_and_types = [ 
( "mname" , str ) , ( "rname" , str ) , ( "serial" , int ) , ( "refresh" , int ) , 
( "retry" , int ) , ( "expire" , int ) , ( "minimum" , int ) 
make_rr_subparser ( subparsers , "SOA" , args_and_types ) 
make_rr_subparser ( subparsers , "NS" , [ ( "host" , str ) ] ) 
make_rr_subparser ( subparsers , "A" , [ ( "ip" , str ) ] ) 
make_rr_subparser ( subparsers , "AAAA" , [ ( "ip" , str ) ] ) 
make_rr_subparser ( subparsers , "CNAME" , [ ( "alias" , str ) ] ) 
make_rr_subparser ( subparsers , "ALIAS" , [ ( "host" , str ) ] ) 
make_rr_subparser ( subparsers , "MX" , [ ( "preference" , str ) , ( "host" , str ) ] ) 
make_txt_subparser ( subparsers ) 
make_rr_subparser ( subparsers , "PTR" , [ ( "host" , str ) ] ) 
make_rr_subparser ( subparsers , "SRV" , [ ( "priority" , int ) , ( "weight" , int ) , ( "port" , int ) , ( "target" , str ) ] ) 
make_rr_subparser ( subparsers , "SPF" , [ ( "data" , str ) ] ) 
make_rr_subparser ( subparsers , "URI" , [ ( "priority" , int ) , ( "weight" , int ) , ( "target" , str ) ] ) 
return line_parser 
~~ def tokenize_line ( line ) : 
ret = [ ] 
escape = False 
quote = False 
tokbuf = "" 
ll = list ( line ) 
while len ( ll ) > 0 : 
~~~ c = ll . pop ( 0 ) 
if c . isspace ( ) : 
~~~ if not quote and not escape : 
~~~ if len ( tokbuf ) > 0 : 
~~~ ret . append ( tokbuf ) 
~~ tokbuf = "" 
~~ elif quote : 
~~~ tokbuf += c 
~~ elif escape : 
~~~ tokbuf = "" 
~~ continue 
~~ if c == '\\\\' : 
~~~ escape = True 
~~ elif c == \ : 
~~~ if not escape : 
~~~ if quote : 
~~~ quote = True 
~~ ~~ ~~ elif c == ';' : 
~~ ~~ tokbuf += c 
~~ return ret 
~~ def serialize ( tokens ) : 
for tok in tokens : 
~~~ tok = \ % tok 
~~ if ";" in tok : 
~~~ tok = tok . replace ( ";" , "\\;" ) 
~~ ret . append ( tok ) 
~~ def remove_comments ( text ) : 
lines = text . split ( "\\n" ) 
~~ line = serialize ( tokenize_line ( line ) ) 
ret . append ( line ) 
~~ return "\\n" . join ( ret ) 
~~ def flatten ( text ) : 
tokens = [ ] 
for l in lines : 
~~~ if len ( l ) == 0 : 
~~ capturing = False 
captured = [ ] 
flattened = [ ] 
while len ( tokens ) > 0 : 
~~~ tok = tokens . pop ( 0 ) 
if not capturing and len ( tok ) == 0 : 
~~~ if len ( captured ) > 0 : 
~~ if tok . startswith ( "(" ) : 
~~~ tok = tok . lstrip ( "(" ) 
capturing = True 
~~ if capturing and tok . endswith ( ")" ) : 
~~~ tok = tok . rstrip ( ")" ) 
capturing = False 
~~ captured . append ( tok ) 
~~ return "\\n" . join ( flattened ) 
~~ def remove_class ( text ) : 
~~~ tokens = tokenize_line ( line ) 
tokens_upper = [ t . upper ( ) for t in tokens ] 
if "IN" in tokens_upper : 
~~~ tokens . remove ( "IN" ) 
~~ elif "CS" in tokens_upper : 
~~~ tokens . remove ( "CS" ) 
~~ elif "CH" in tokens_upper : 
~~~ tokens . remove ( "CH" ) 
~~ elif "HS" in tokens_upper : 
~~~ tokens . remove ( "HS" ) 
~~ ret . append ( serialize ( tokens ) ) 
~~ def add_default_name ( text ) : 
global SUPPORTED_RECORDS 
if len ( tokens ) == 0 : 
~~ if tokens [ 0 ] in SUPPORTED_RECORDS and not tokens [ 0 ] . startswith ( "$" ) : 
~~~ tokens = [ '@' ] + tokens 
~~ def parse_line ( parser , record_token , parsed_records ) : 
if len ( record_token ) >= 2 and record_token [ 1 ] in SUPPORTED_RECORDS : 
~~~ record_token = [ record_token [ 1 ] ] + record_token 
~~ elif len ( record_token ) >= 3 and record_token [ 2 ] in SUPPORTED_RECORDS : 
~~~ record_token = [ record_token [ 2 ] ] + record_token 
if record_token [ 0 ] == "TXT" : 
~~~ record_token = record_token [ : 2 ] + [ "--ttl" ] + record_token [ 2 : ] 
~~~ rr , unmatched = parser . parse_known_args ( record_token ) 
~~ except ( SystemExit , AssertionError , InvalidLineException ) : 
~~~ raise InvalidLineException ( line ) 
~~ record_dict = rr . __dict__ 
if record_token [ 0 ] == "TXT" and len ( record_dict [ 'txt' ] ) == 1 : 
~~~ record_dict [ 'txt' ] = record_dict [ 'txt' ] [ 0 ] 
~~ record_type = None 
for key in record_dict . keys ( ) : 
~~~ if key in SUPPORTED_RECORDS and ( key . startswith ( "$" ) or record_dict [ key ] == key ) : 
~~~ record_type = key 
if record_dict [ key ] == key : 
~~~ del record_dict [ key ] 
for field in record_dict . keys ( ) : 
~~~ if record_dict [ field ] is None : 
~~~ del record_dict [ field ] 
~~ ~~ current_origin = record_dict . get ( '$ORIGIN' , parsed_records . get ( '$ORIGIN' , None ) ) 
if record_type == 'PTR' : 
~~~ record_dict [ 'fullname' ] = record_dict [ 'name' ] + '.' + current_origin 
~~ if len ( record_dict ) > 0 : 
~~~ if record_type . startswith ( "$" ) : 
~~~ record_dict_key = record_type . lower ( ) 
parsed_records [ record_dict_key ] = record_dict [ record_type ] 
parsed_records [ record_dict_key ] . append ( record_dict ) 
~~ ~~ return parsed_records 
~~ def parse_lines ( text , ignore_invalid = False ) : 
json_zone_file = defaultdict ( list ) 
record_lines = text . split ( "\\n" ) 
parser = make_parser ( ) 
for record_line in record_lines : 
~~~ record_token = tokenize_line ( record_line ) 
~~~ json_zone_file = parse_line ( parser , record_token , json_zone_file ) 
~~ except InvalidLineException : 
~~~ if ignore_invalid : 
~~ ~~ ~~ return json_zone_file 
~~ def parse_zone_file ( text , ignore_invalid = False ) : 
text = remove_comments ( text ) 
text = flatten ( text ) 
text = remove_class ( text ) 
text = add_default_name ( text ) 
json_zone_file = parse_lines ( text , ignore_invalid = ignore_invalid ) 
return json_zone_file 
~~ def make_zone_file ( json_zone_file_input , origin = None , ttl = None , template = None ) : 
if template is None : 
~~~ template = DEFAULT_TEMPLATE [ : ] 
~~ json_zone_file = copy . deepcopy ( json_zone_file_input ) 
if origin is not None : 
~~~ json_zone_file [ '$origin' ] = origin 
~~ if ttl is not None : 
~~~ json_zone_file [ '$ttl' ] = ttl 
~~ soa_records = [ json_zone_file . get ( 'soa' ) ] if json_zone_file . get ( 'soa' ) else None 
zone_file = template 
zone_file = process_origin ( json_zone_file . get ( '$origin' , None ) , zone_file ) 
zone_file = process_ttl ( json_zone_file . get ( '$ttl' , None ) , zone_file ) 
zone_file = process_soa ( soa_records , zone_file ) 
zone_file = process_ns ( json_zone_file . get ( 'ns' , None ) , zone_file ) 
zone_file = process_a ( json_zone_file . get ( 'a' , None ) , zone_file ) 
zone_file = process_aaaa ( json_zone_file . get ( 'aaaa' , None ) , zone_file ) 
zone_file = process_cname ( json_zone_file . get ( 'cname' , None ) , zone_file ) 
zone_file = process_alias ( json_zone_file . get ( 'alias' , None ) , zone_file ) 
zone_file = process_mx ( json_zone_file . get ( 'mx' , None ) , zone_file ) 
zone_file = process_ptr ( json_zone_file . get ( 'ptr' , None ) , zone_file ) 
zone_file = process_txt ( json_zone_file . get ( 'txt' , None ) , zone_file ) 
zone_file = process_srv ( json_zone_file . get ( 'srv' , None ) , zone_file ) 
zone_file = process_spf ( json_zone_file . get ( 'spf' , None ) , zone_file ) 
zone_file = process_uri ( json_zone_file . get ( 'uri' , None ) , zone_file ) 
zone_file = "\\n" . join ( 
filter ( 
lambda l : len ( l . strip ( ) ) > 0 , [ tl . strip ( ) for tl in zone_file . split ( "\\n" ) ] 
) + "\\n" 
return zone_file 
~~ def process_origin ( data , template ) : 
record = "" 
if data is not None : 
~~ return template . replace ( "{$origin}" , record ) 
~~ def process_ttl ( data , template ) : 
~~ return template . replace ( "{$ttl}" , record ) 
~~ def process_soa ( data , template ) : 
record = template [ : ] 
data = data [ 0 ] 
soadat = [ ] 
domain_fields = [ 'mname' , 'rname' ] 
param_fields = [ 'serial' , 'refresh' , 'retry' , 'expire' , 'minimum' ] 
for f in domain_fields + param_fields : 
~~ data_name = str ( data . get ( 'name' , '@' ) ) 
soadat . append ( data_name ) 
if data . get ( 'ttl' ) is not None : 
~~~ soadat . append ( str ( data [ 'ttl' ] ) ) 
~~ soadat . append ( "IN" ) 
soadat . append ( "SOA" ) 
for key in domain_fields : 
~~~ value = str ( data [ key ] ) 
soadat . append ( value ) 
~~ soadat . append ( "(" ) 
for key in param_fields : 
~~ soadat . append ( ")" ) 
record = record . replace ( "{soa}" , soa_txt ) 
~~~ record = record . replace ( "{soa}" , "" ) 
~~ return record 
~~ def quote_field ( data , field ) : 
if data is None : 
~~ data_dup = copy . deepcopy ( data ) 
for i in xrange ( 0 , len ( data_dup ) ) : 
~~~ data_dup [ i ] [ field ] = \ % data_dup [ i ] [ field ] 
data_dup [ i ] [ field ] = data_dup [ i ] [ field ] . replace ( ";" , "\\;" ) 
~~ return data_dup 
~~ def process_rr ( data , record_type , record_keys , field , template ) : 
~~~ return template . replace ( field , "" ) 
~~ if type ( record_keys ) == list : 
~~ elif type ( record_keys ) == str : 
~~~ record_keys = [ record_keys ] 
for i in xrange ( 0 , len ( data ) ) : 
~~~ for record_key in record_keys : 
~~ record_data = [ ] 
record_data . append ( str ( data [ i ] . get ( 'name' , '@' ) ) ) 
if data [ i ] . get ( 'ttl' ) is not None : 
~~~ record_data . append ( str ( data [ i ] [ 'ttl' ] ) ) 
~~ record_data . append ( record_type ) 
record_data += [ str ( data [ i ] [ record_key ] ) for record_key in record_keys ] 
~~ return template . replace ( field , record ) 
~~ def process_txt ( data , template ) : 
~~~ to_process = None 
~~~ to_process = copy . deepcopy ( data ) 
for datum in to_process : 
~~~ if isinstance ( datum [ "txt" ] , list ) : 
for entry in datum [ "txt" ] ] ) 
~~~ datum [ "txt" ] = \ % datum [ "txt" ] . replace ( ";" , "\\;" ) 
~~ ~~ ~~ return process_rr ( to_process , "TXT" , "txt" , "{txt}" , template ) 
~~ def gen_filter ( name , op , value , is_or = False ) : 
if op not in OPERATORS : 
if is_or : 
~~ def from_dict ( cls , d ) : 
if not d : 
~~ items = list ( d . items ( ) ) 
key , value = items . pop ( 0 ) 
q = cls ( key , u'=' , value ) 
for key , value in items : 
~~~ q = q & cls ( key , u'=' , value ) 
~~ return q 
~~ def query_string ( self , ** params ) : 
return SearchResult ( self , self . _api . get ( self . _href , ** params ) ) 
~~ def raw_filter ( self , filters ) : 
return SearchResult ( self , self . _api . get ( self . _href , ** { "filter[]" : filters } ) ) 
~~ def all_include_attributes ( self , attributes ) : 
self . reload ( expand = True , attributes = attributes ) 
entities = [ Entity ( self , r , attributes = attributes ) for r in self . _resources ] 
self . reload ( ) 
return entities 
~~ def _get_entity_from_href ( self , result ) : 
href_result = result [ 'href' ] 
if self . collection . _href . startswith ( href_result ) : 
~~~ return Entity ( self . collection , result , incomplete = True ) 
~~ href_match = re . match ( r"(https?://.+/api[^?]*)/([a-z_-]+)" , href_result ) 
if not href_match : 
~~ collection_name = href_match . group ( 2 ) 
entry_point = href_match . group ( 1 ) 
new_collection = Collection ( 
self . collection . api , 
"{}/{}" . format ( entry_point , collection_name ) , 
collection_name 
return Entity ( new_collection , result , incomplete = True ) 
~~ def give_another_quote ( q ) : 
for qc in QUOTES : 
~~~ if qc != q : 
~~~ return qc 
~~ ~~ def escape_filter ( o ) : 
if o is None : 
~~~ return u'NULL' 
~~ if isinstance ( o , int ) : 
~~~ return str ( o ) 
~~ if not isinstance ( o , six . string_types ) : 
~~ if not o : 
~~~ return u"\ 
~~ o = unicode_process ( o ) 
if u\ not in o : 
~~~ return u\ + o + u\ 
~~ elif u"\ not in o : 
~~~ return u"\ + o + u"\ 
~~~ first_char = o [ 0 ] 
last_char = o [ - 1 ] 
if first_char in QUOTES and last_char in QUOTES : 
~~~ if first_char == last_char : 
~~~ quote = give_another_quote ( first_char ) 
return quote + o + quote 
~~ ~~ elif first_char not in QUOTES and last_char not in QUOTES : 
~~~ if first_char in QUOTES : 
~~~ quote = give_another_quote ( last_char ) 
~~ return quote + o + quote 
~~ ~~ ~~ def serach_path ( ) : 
operating_system = get_os ( ) 
return [ os . path . expanduser ( "~/.kerncraft/iaca/{}/" . format ( operating_system ) ) , 
os . path . abspath ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) + '/iaca/{}/' . format ( 
operating_system ) ] 
~~ def find_iaca ( ) : 
requires = [ 'iaca2.2' , 'iaca2.3' , 'iaca3.0' ] 
for path in serach_path ( ) : 
~~~ path += 'bin/' 
valid = True 
for r in requires : 
~~~ if not os . path . exists ( path + r ) : 
~~~ valid = False 
~~ ~~ if valid : 
~~~ return path 
"" . format ( serach_path ( ) ) ) 
~~ def group_iterator ( group ) : 
ordered_chars = string . ascii_letters + string . digits 
tokenizer = ( '(?P<seq>[a-zA-Z0-9]-[a-zA-Z0-9])|' 
'(?P<chr>.)' ) 
for m in re . finditer ( tokenizer , group ) : 
~~~ if m . group ( 'seq' ) : 
~~~ start , sep , end = m . group ( 'seq' ) 
for i in range ( ordered_chars . index ( start ) , ordered_chars . index ( end ) + 1 ) : 
~~~ yield ordered_chars [ i ] 
~~~ yield m . group ( 'chr' ) 
~~ ~~ ~~ def register_options ( regdescr ) : 
if not regdescr : 
~~~ yield None 
~~ tokenizer = ( '\\[(?P<grp>[^]]+)\\]|' 
for u in regdescr . split ( '|' ) : 
~~~ m = re . match ( tokenizer , u ) 
if m . group ( 'grp' ) : 
~~~ current = group_iterator ( m . group ( 'grp' ) ) 
~~~ current = [ m . group ( 'chr' ) ] 
~~ for c in current : 
~~~ if u [ m . end ( ) : ] : 
~~~ for r in register_options ( u [ m . end ( ) : ] ) : 
~~~ yield c + r 
~~~ yield c 
~~ ~~ ~~ ~~ def eventstr ( event_tuple = None , event = None , register = None , parameters = None ) : 
if len ( event_tuple ) == 3 : 
~~~ event , register , parameters = event_tuple 
~~ elif len ( event_tuple ) == 2 : 
~~~ event , register = event_tuple 
~~ event_dscr = [ event , register ] 
if parameters : 
~~~ if type ( v ) is int : 
~~~ k += "={}" . format ( hex ( v ) ) 
~~ event_dscr . append ( k ) 
~~ ~~ return ":" . join ( event_dscr ) 
~~ def build_minimal_runs ( events ) : 
events = [ e for i , e in enumerate ( events ) if events . index ( e ) == i ] 
scheduled_runs = { } 
scheduled_events = [ ] 
cur_run = 0 
while len ( scheduled_events ) != len ( events ) : 
~~~ for event_tpl in events : 
~~~ event , registers , parameters = event_tpl 
if event_tpl in scheduled_events : 
~~ for possible_reg in register_options ( registers ) : 
~~~ s = scheduled_runs . setdefault ( cur_run , { } ) 
if possible_reg not in s : 
~~~ s [ possible_reg ] = ( event , possible_reg , parameters ) 
scheduled_events . append ( event_tpl ) 
~~ ~~ ~~ cur_run += 1 
~~ runs = [ list ( v . values ( ) ) for v in scheduled_runs . values ( ) ] 
return runs 
~~ def calculate_cache_access ( self ) : 
self . results = { 'misses' : self . predictor . get_misses ( ) , 
'hits' : self . predictor . get_hits ( ) , 
'evicts' : self . predictor . get_evicts ( ) , 
element_size = self . kernel . datatypes_size [ self . kernel . datatype ] 
elements_per_cacheline = int ( cacheline_size // element_size ) 
total_flops = sum ( self . kernel . _flops . values ( ) ) * elements_per_cacheline 
threads_per_core = 1 
read_offsets , write_offsets = zip ( * list ( self . kernel . compile_global_offsets ( 
iteration = range ( 0 , elements_per_cacheline ) ) ) ) 
read_offsets = set ( [ item for sublist in read_offsets if sublist is not None 
for item in sublist ] ) 
write_offsets = set ( [ item for sublist in write_offsets if sublist is not None 
write_streams = len ( write_offsets ) 
total_loads = read_streams * element_size 
bw , measurement_kernel = self . machine . get_bandwidth ( 
0 , 
write_streams , 
threads_per_core , 
cores = self . cores ) 
if total_loads == 0 : 
~~~ arith_intens = None 
performance = None 
~~~ arith_intens = float ( total_flops ) / total_loads 
performance = PrefixedUnit ( arith_intens * float ( bw ) , 'FLOP/s' ) 
'performance' : self . conv_perf ( PrefixedUnit ( performance , 'FLOP/s' ) ) , 
'bandwidth' : bw , 
~~~ total_misses = self . results [ 'misses' ] [ cache_level ] * cacheline_size 
total_evicts = self . results [ 'evicts' ] [ cache_level ] * cacheline_size 
read_streams = self . results [ 'misses' ] [ cache_level ] 
write_streams = self . results [ 'evicts' ] [ cache_level ] 
cache_level + 1 , read_streams , write_streams , threads_per_core , 
bytes_transfered = total_misses + total_evicts 
if bytes_transfered == 0 : 
~~~ arith_intens = float ( 'inf' ) 
performance = PrefixedUnit ( float ( 'inf' ) , 'FLOP/s' ) 
~~~ arith_intens = float ( total_flops ) / bytes_transfered 
'performance' : self . conv_perf ( performance ) , 
~~ ~~ return self . results 
~~ def analyze ( self ) : 
precision = 'DP' if self . kernel . datatype == 'double' else 'SP' 
self . calculate_cache_access ( ) 
~~ def conv_perf ( self , performance ) : 
clock = self . machine [ 'clock' ] 
flops_per_it = sum ( self . kernel . _flops . values ( ) ) 
it_s = performance / flops_per_it 
it_s . unit = 'It/s' 
cy_cl = clock / it_s * elements_per_cacheline 
cy_cl . unit = 'cy/CL' 
cy_it = clock / it_s 
cy_it . unit = 'cy/It' 
return { 'It/s' : it_s , 
'cy/CL' : cy_cl , 
'cy/It' : cy_it , 
'FLOP/s' : performance } 
~~ def report ( self , output_file = sys . stdout ) : 
max_perf = self . results [ 'max_perf' ] 
if self . _args and self . _args . verbose >= 3 : 
~~~ print ( '{}' . format ( pformat ( self . results ) ) , file = output_file ) 
~~ if self . _args and self . _args . verbose >= 1 : 
print ( 'Bottlenecks:' , file = output_file ) 
file = output_file ) 
print ( '--------+--------------+-----------------+-------------------+----------------------' , 
max_perf [ self . _args . unit ] ) , 
b [ 'performance' ] [ self . _args . unit ] , ** b ) , 
~~ print ( '' , file = output_file ) 
bottleneck [ 'performance' ] [ self . _args . unit ] , 
bottleneck [ 'level' ] , 
~~ ~~ def analyze ( self ) : 
self . results = self . calculate_cache_access ( ) 
~~~ iaca_analysis , asm_block = self . kernel . iaca_analysis ( 
micro_architecture = self . machine [ 'micro-architecture' ] , 
asm_block = self . asm_block , 
pointer_increment = self . pointer_increment , 
verbose = self . verbose > 2 ) 
~~ except RuntimeError as e : 
sys . exit ( 1 ) 
~~ block_throughput = iaca_analysis [ 'throughput' ] 
uops = iaca_analysis [ 'uops' ] 
iaca_output = iaca_analysis [ 'output' ] 
elements_per_block = abs ( asm_block [ 'pointer_increment' ] 
/ self . kernel . datatypes_size [ self . kernel . datatype ] ) 
block_size = elements_per_block * self . kernel . datatypes_size [ self . kernel . datatype ] 
~~ except ZeroDivisionError as e : 
~~ port_cycles = dict ( [ ( i [ 0 ] , i [ 1 ] * block_to_cl_ratio ) for i in list ( port_cycles . items ( ) ) ] ) 
uops = uops * block_to_cl_ratio 
cl_throughput = block_throughput * block_to_cl_ratio 
flops_per_element = sum ( self . kernel . _flops . values ( ) ) 
~~~ if level == 0 : 
~~ ~~ self . results . update ( { 
'uops' : uops , 
self . machine [ 'clock' ] / block_throughput * elements_per_block * flops_per_element 
* self . cores , "FLOP/s" ) ) , 
if self . verbose >= 3 : 
~~ if self . verbose >= 1 : 
~~~ print ( 'Bottlenecks:' , file = output_file ) 
cpu_perf [ self . _args . unit ] ) , 
~~~ if b is None : 
print ( '{!s}' . format ( 
{ k : v 
~~ ~~ def calculate_cache_access ( self ) : 
results = { 'dimensions' : { } } 
def sympy_compare ( a , b ) : 
~~~ c = 0 
for i in range ( min ( len ( a ) , len ( b ) ) ) : 
~~~ s = a [ i ] - b [ i ] 
if sympy . simplify ( s > 0 ) : 
~~~ c = - 1 
~~ elif sympy . simplify ( s == 0 ) : 
~~~ c = 1 
~~ if c != 0 : 
~~ ~~ return c 
~~ accesses = defaultdict ( list ) 
sympy_accesses = defaultdict ( list ) 
for var_name in self . kernel . variables : 
~~~ for r in self . kernel . sources . get ( var_name , [ ] ) : 
~~~ if r is None : 
~~ accesses [ var_name ] . append ( r ) 
sympy_accesses [ var_name ] . append ( self . kernel . access_to_sympy ( var_name , r ) ) 
~~ for w in self . kernel . destinations . get ( var_name , [ ] ) : 
~~~ if w is None : 
~~ accesses [ var_name ] . append ( w ) 
sympy_accesses [ var_name ] . append ( self . kernel . access_to_sympy ( var_name , w ) ) 
~~ accesses [ var_name ] . sort ( key = cmp_to_key ( sympy_compare ) , reverse = True ) 
~~ results [ 'accesses' ] = accesses 
results [ 'sympy_accesses' ] = sympy_accesses 
for dimension in range ( 1 , len ( list ( self . kernel . get_loop_stack ( ) ) ) + 1 ) : 
~~~ results [ 'dimensions' ] [ dimension ] = { } 
slices = defaultdict ( list ) 
slices_accesses = defaultdict ( list ) 
for var_name in accesses : 
~~~ for a in accesses [ var_name ] : 
~~~ slice_id = tuple ( [ var_name , tuple ( a [ : - dimension ] ) ] ) 
slices [ slice_id ] . append ( a ) 
slices_accesses [ slice_id ] . append ( self . kernel . access_to_sympy ( var_name , a ) ) 
~~ ~~ results [ 'dimensions' ] [ dimension ] [ 'slices' ] = slices 
results [ 'dimensions' ] [ dimension ] [ 'slices_accesses' ] = slices_accesses 
slices_distances = defaultdict ( list ) 
for k , v in slices_accesses . items ( ) : 
~~~ for i in range ( 1 , len ( v ) ) : 
~~~ slices_distances [ k ] . append ( ( v [ i ] - v [ i - 1 ] ) . simplify ( ) ) 
~~ ~~ results [ 'dimensions' ] [ dimension ] [ 'slices_distances' ] = slices_distances 
for dist in chain ( * slices_distances . values ( ) ) : 
~~~ if any ( [ s not in self . kernel . constants . keys ( ) for s in dist . free_symbols ] ) : 
~~ ~~ slices_sum = sum ( [ sum ( dists ) for dists in slices_distances . values ( ) ] ) 
results [ 'dimensions' ] [ dimension ] [ 'slices_sum' ] = slices_sum 
def FuckedUpMax ( * args ) : 
~~~ if len ( args ) == 1 : 
~~~ return args [ 0 ] 
~~ args = [ a . expand ( ) for a in args ] 
max_symbols = max ( [ len ( a . free_symbols ) for a in args ] ) 
args = list ( filter ( lambda a : len ( a . free_symbols ) == max_symbols , args ) ) 
if max_symbols == 0 : 
~~~ return sympy . Max ( * args ) 
~~ max_coeffs = 0 
for a in args : 
~~~ for s in a . free_symbols : 
~~~ max_coeffs = max ( max_coeffs , len ( sympy . Poly ( a , s ) . all_coeffs ( ) ) ) 
~~ ~~ def coeff_filter ( a ) : 
~~~ return max ( 
0 , 0 , 
* [ len ( sympy . Poly ( a , s ) . all_coeffs ( ) ) for s in a . free_symbols ] ) == max_coeffs 
~~ args = list ( filter ( coeff_filter , args ) ) 
m = sympy . Max ( * args ) 
return m 
~~ slices_max = FuckedUpMax ( sympy . Integer ( 0 ) , 
* [ FuckedUpMax ( * dists ) for dists in slices_distances . values ( ) ] ) 
results [ 'dimensions' ] [ dimension ] [ 'slices_max' ] = slices_max 
slices_count = len ( slices_accesses ) 
results [ 'dimensions' ] [ dimension ] [ 'slices_count' ] = slices_count 
cache_requirement_bytes = ( slices_sum + slices_max * slices_count ) * element_size 
results [ 'dimensions' ] [ dimension ] [ 'cache_requirement_bytes' ] = cache_requirement_bytes 
csim = self . machine . get_cachesim ( self . _args . cores ) 
results [ 'dimensions' ] [ dimension ] [ 'caches' ] = { } 
for cl in csim . levels ( with_mem = False ) : 
~~~ cache_equation = sympy . Eq ( cache_requirement_bytes , cl . size ( ) ) 
if len ( self . kernel . constants . keys ( ) ) <= 1 : 
~~~ inequality = sympy . solve ( sympy . LessThan ( cache_requirement_bytes , cl . size ( ) ) , 
* self . kernel . constants . keys ( ) ) 
~~~ inequality = sympy . LessThan ( cache_requirement_bytes , cl . size ( ) ) 
~~~ eq = sympy . solve ( inequality , * self . kernel . constants . keys ( ) , dict = True ) 
~~ except NotImplementedError : 
~~~ eq = None 
~~ results [ 'dimensions' ] [ dimension ] [ 'caches' ] [ cl . name ] = { 
'cache_size' : cl . size ( ) , 
'equation' : cache_equation , 
'lt' : inequality , 
'eq' : eq 
~~ ~~ return results 
loop_stack = list ( self . kernel . get_loop_stack ( ) ) 
if any ( [ l [ 'increment' ] != 1 for l in loop_stack ] ) : 
~~ for aref in list ( self . kernel . index_order ( ) ) : 
~~~ while aref and len ( aref [ 0 ] ) == 0 : 
~~~ aref . pop ( 0 ) 
~~ for i , idx_names in enumerate ( aref ) : 
~~~ if i >= len ( loop_stack ) or any ( [ loop_stack [ i ] [ 'index' ] != idx . name for idx in idx_names ] ) : 
~~ ~~ ~~ for arefs in chain ( chain ( * self . kernel . sources . values ( ) ) , 
chain ( * self . kernel . destinations . values ( ) ) ) : 
~~~ if not arefs : 
~~ while arefs and not arefs [ 0 ] . free_symbols : 
~~~ arefs = arefs [ 1 : ] 
~~ for i , expr in enumerate ( arefs ) : 
~~~ diff = sympy . diff ( expr , sympy . Symbol ( loop_stack [ i ] [ 'index' ] ) ) 
if diff != 0 and diff != 1 : 
~~ ~~ ~~ self . results = self . calculate_cache_access ( ) 
if self . _args and self . _args . verbose > 2 : 
~~~ pprint ( self . results ) 
~~ for dimension , lc_info in self . results [ 'dimensions' ] . items ( ) : 
for cache , lc_solution in sorted ( lc_info [ 'caches' ] . items ( ) ) : 
if lc_solution [ 'lt' ] is sympy . true : 
~~~ if lc_solution [ 'eq' ] is None : 
~~~ print ( "{}" . format ( lc_solution [ 'lt' ] ) , file = output_file ) 
~~ elif type ( lc_solution [ 'eq' ] ) is not list : 
~~~ print ( "{}" . format ( lc_solution [ 'eq' ] ) , file = output_file ) 
~~~ for solu in lc_solution [ 'eq' ] : 
~~~ for s , v in solu . items ( ) : 
~~ ~~ ~~ ~~ ~~ ~~ ~~ def clean_code ( code , comments = True , macros = False , pragmas = False ) : 
if macros or pragmas : 
~~~ lines = code . split ( '\\n' ) 
in_macro = False 
in_pragma = False 
for i in range ( len ( lines ) ) : 
~~~ l = lines [ i ] . strip ( ) 
if macros and ( l . startswith ( '#' ) and not l . startswith ( '#pragma' ) or in_macro ) : 
~~~ lines [ i ] = '' 
in_macro = l . endswith ( '\\\\' ) 
~~ if pragmas and ( l . startswith ( '#pragma' ) or in_pragma ) : 
in_pragma = l . endswith ( '\\\\' ) 
~~ ~~ code = '\\n' . join ( lines ) 
~~~ idx = 0 
comment_start = None 
while idx < len ( code ) - 1 : 
~~~ if comment_start is None and code [ idx : idx + 2 ] == '//' : 
~~~ end_idx = code . find ( '\\n' , idx ) 
code = code [ : idx ] + code [ end_idx : ] 
idx -= end_idx - idx 
~~ elif comment_start is None and code [ idx : idx + 2 ] == '/*' : 
~~~ comment_start = idx 
~~ elif comment_start is not None and code [ idx : idx + 2 ] == '*/' : 
~~~ code = ( code [ : comment_start ] + 
'\\n' * code [ comment_start : idx ] . count ( '\\n' ) + 
code [ idx + 2 : ] ) 
idx -= idx - comment_start 
~~ idx += 1 
~~ ~~ return code 
~~ def replace_id ( ast , id_name , replacement ) : 
for a in ast : 
~~~ if isinstance ( a , c_ast . ID ) and a . name == id_name : 
~~~ for attr_name in dir ( ast ) : 
~~~ if attr_name . startswith ( '__' ) or callable ( getattr ( ast , attr_name ) ) : 
~~ attr = getattr ( ast , attr_name ) 
if attr is a : 
~~~ setattr ( ast , attr_name , replacement ) 
~~ if type ( attr ) is list : 
~~~ for i , attr_element in enumerate ( attr ) : 
~~~ if attr_element is a : 
~~~ if type ( replacement ) is list : 
~~~ attr [ i : i + 1 ] = replacement 
~~~ attr [ i ] = replacement 
~~ ~~ ~~ ~~ ~~ ~~ else : 
~~~ replace_id ( a , id_name , replacement ) 
~~ ~~ ~~ def round_to_next ( x , base ) : 
return int ( base * math . ceil ( float ( x ) / base ) ) 
~~ def blocking ( indices , block_size , initial_boundary = 0 ) : 
blocks = [ ] 
for idx in indices : 
~~~ bl_idx = ( idx - initial_boundary ) // float ( block_size ) 
if bl_idx not in blocks : 
~~~ blocks . append ( bl_idx ) 
~~ ~~ blocks . sort ( ) 
return blocks 
self . results . update ( { 
'misses' : self . predictor . get_misses ( ) , 
~~ def calculate_cycles ( self ) : 
sympy . Integer ( self . kernel . bytes_per_iteration ) ) 
loads , stores = ( self . predictor . get_loads ( ) , self . predictor . get_stores ( ) ) 
~~~ read_streams = loads [ cache_level ] 
write_streams = stores [ cache_level ] 
cache_level , read_streams , write_streams , threads_per_core ) 
if duplexness == 'half-duplex' : 
~~~ cycles = float ( loads [ cache_level ] + stores [ cache_level ] ) * float ( elements_per_cacheline ) * float ( element_size ) * float ( self . machine [ 'clock' ] ) / float ( bw ) 
~~~ raise NotImplementedError ( 
~~ self . results . update ( { 
~~~ throughput = float ( throughput ) / cacheline_size 
~~~ cycles = ( loads [ cache_level ] + stores [ cache_level ] ) / float ( throughput ) 
~~ elif duplexness == 'full-duplex' : 
~~~ cycles = max ( loads [ cache_level ] / float ( throughput ) , 
stores [ cache_level ] / float ( throughput ) ) 
duplexness , cache_info [ 'name' ] ) ) 
~~ ~~ self . results [ 'cycles' ] . append ( ( cache_info [ 'level' ] , cycles ) ) 
self . results [ cache_info [ 'level' ] ] = cycles 
~~ return self . results 
self . calculate_cycles ( ) 
return self . results 
if self . verbose > 1 : 
~~ for level , cycles in self . results [ 'cycles' ] : 
level , self . conv_cy ( cycles ) [ self . _args . unit ] ) , file = output_file ) 
~~ if self . verbose > 1 : 
~~ ~~ if self . verbose > 1 : 
~~~ print ( file = output_file ) 
print ( self . report_data_transfers ( ) , file = output_file ) 
~~~ incore_analysis , asm_block = self . kernel . iaca_analysis ( 
~~ block_throughput = incore_analysis [ 'throughput' ] 
uops = incore_analysis [ 'uops' ] 
// self . kernel . datatypes_size [ self . kernel . datatype ] ) 
T_OL = max ( [ v for k , v in list ( port_cycles . items ( ) ) 
T_nOL = max ( [ v for k , v in list ( port_cycles . items ( ) ) 
if T_nOL < cl_throughput : 
~~~ T_OL = cl_throughput 
~~ self . results = { 
'T_nOL' : T_nOL , 
'T_OL' : T_OL , 
'elements_per_block' : elements_per_block , 
'pointer_increment' : asm_block [ 'pointer_increment' ] , 
~~ def conv_cy ( self , cy_cl ) : 
if not isinstance ( cy_cl , PrefixedUnit ) : 
~~~ cy_cl = PrefixedUnit ( cy_cl , '' , 'cy/CL' ) 
~~ clock = self . machine [ 'clock' ] 
it_s = clock / cy_cl * elements_per_cacheline 
performance = it_s * flops_per_it 
performance . unit = 'FLOP/s' 
cy_it = cy_cl * elements_per_cacheline 
if self . verbose > 2 : 
print ( '' , file = output_file ) 
self . results [ 'elements_per_block' ] ) , file = output_file ) 
print ( 'Uops:' , str ( self . results [ 'uops' ] ) , file = output_file ) 
self . _CPU . analyze ( ) 
self . _data . analyze ( ) 
self . results = copy . deepcopy ( self . _CPU . results ) 
self . results . update ( copy . deepcopy ( self . _data . results ) ) 
self . results [ 'T_OL' ] , 
sum ( [ self . results [ 'T_nOL' ] ] + [ i [ 1 ] for i in self . results [ 'cycles' ] ] ) ) ) 
T_MEM = self . results [ 'cycles' ] [ - 1 ] [ 1 ] 
if self . results [ 'cycles' ] [ - 1 ] [ 1 ] != 0.0 : 
~~~ utilization = [ 0 ] 
for c in range ( 1 , cores_per_numa_domain + 1 ) : 
~~~ if c * T_MEM > ( T_ECM + utilization [ c - 1 ] * ( c - 1 ) * T_MEM / 2 ) : 
~~~ utilization . append ( 1.0 ) 
~~~ utilization . append ( c * T_MEM / ( T_ECM + utilization [ c - 1 ] * ( c - 1 ) * T_MEM / 2 ) ) 
~~ ~~ utilization = utilization [ 1 : ] 
scaling_predictions = [ ] 
~~~ scaling = { 'cores' : cores , 'notes' : [ ] , 'performance' : None , 
~~~ innuma_rectp = PrefixedUnit ( 
max ( sum ( [ c [ 1 ] for c in self . results [ 'cycles' ] ] ) + self . results [ 'T_nOL' ] , 
self . results [ 'T_OL' ] ) / ( T_ECM / T_MEM ) , 
"cy/CL" ) 
~~~ innuma_rectp = PrefixedUnit ( self . results [ 'cycles' ] [ - 1 ] [ 1 ] , 'cy/CL' ) 
if 0 < cores <= cores_per_numa_domain : 
~~~ scaling [ 'performance' ] = self . _CPU . conv_cy ( 
innuma_rectp / utilization [ cores - 1 ] ) 
innuma_rectp * cores_per_numa_domain / cores ) 
~~ scaling_predictions . append ( scaling ) 
~~~ scaling_predictions = [ 
'performance' : self . _CPU . conv_cy ( T_ECM / cores ) , 
if self . _args . cores : 
~~~ self . results [ 'multi-core' ] = scaling_predictions [ self . _args . cores - 1 ] 
~~~ self . results [ 'multi-core' ] = None 
~~ ~~ def report ( self , output_file = sys . stdout ) : 
report = '' 
~~~ self . _CPU . report ( ) 
self . _data . report ( ) 
self . results [ 'T_nOL' ] , 
if self . _args . cores > 1 : 
max ( self . results [ 'T_OL' ] , self . results [ 'T_nOL' ] ) , 
self . results [ 'T_nOL' ] , self . results [ 'T_OL' ] ) ) 
for i in range ( len ( self . results [ 'cycles' ] ) ) ] ) ) 
if self . results [ 'multi-core' ] : 
self . results [ 'multi-core' ] [ 'performance' ] [ self . _args . unit ] , 
[ '{:<5.1f}' . format ( float ( s [ 'performance' ] [ self . _args . unit ] ) ) 
~~ print ( report , file = output_file ) 
if self . _args and self . _args . ecm_plot : 
fig = plt . figure ( frameon = False ) 
self . plot ( fig ) 
~~ ~~ def plot ( self , fig = None ) : 
if not fig : 
~~~ fig = plt . gcf ( ) 
~~ fig . subplots_adjust ( left = 0.1 , right = 0.9 , top = 0.9 , bottom = 0.15 ) 
ax = fig . add_subplot ( 1 , 1 , 1 ) 
sorted_overlapping_ports = sorted ( 
key = lambda x : x [ 1 ] ) 
yticks_labels = [ ] 
yticks = [ ] 
xticks_labels = [ ] 
xticks = [ ] 
height = 0.9 
colors = ( [ ( 254. / 255 , 177. / 255. , 178. / 255. ) ] + 
[ ( 255. / 255. , 255. / 255. , 255. / 255. ) ] * ( len ( sorted_overlapping_ports ) - 1 ) ) 
for p , c in sorted_overlapping_ports : 
~~~ ax . barh ( i , c , height , align = 'center' , color = colors . pop ( ) , 
edgecolor = ( 0.5 , 0.5 , 0.5 ) , linestyle = 'dashed' ) 
if i == len ( sorted_overlapping_ports ) - 1 : 
~~~ ax . text ( c / 2.0 , i , '$T_\\mathrm{OL}$' , ha = 'center' , va = 'center' ) 
~~ yticks_labels . append ( p ) 
yticks . append ( i ) 
~~ xticks . append ( sorted_overlapping_ports [ - 1 ] [ 1 ] ) 
xticks_labels . append ( '{:.1f}' . format ( sorted_overlapping_ports [ - 1 ] [ 1 ] ) ) 
y = 0 
colors = [ ( 187. / 255. , 255 / 255. , 188. / 255. ) ] * ( len ( self . results [ 'cycles' ] ) ) + [ ( 119. / 255 , 194. / 255. , 255. / 255. ) ] 
for k , v in [ ( 'nOL' , self . results [ 'T_nOL' ] ) ] + self . results [ 'cycles' ] : 
~~~ ax . barh ( i , v , height , y , align = 'center' , color = colors . pop ( ) ) 
ax . text ( y + v / 2.0 , i , '$T_\\mathrm{' + k + '}$' , ha = 'center' , va = 'center' ) 
xticks . append ( y + v ) 
xticks_labels . append ( '{:.1f}' . format ( y + v ) ) 
y += v 
~~ yticks_labels . append ( 'LD' ) 
ax . tick_params ( axis = 'y' , which = 'both' , left = 'off' , right = 'off' ) 
ax . tick_params ( axis = 'x' , which = 'both' , top = 'off' ) 
ax . set_yticks ( yticks ) 
ax . set_yticklabels ( yticks_labels ) 
ax . set_xticks ( xticks ) 
ax . set_xticklabels ( xticks_labels , rotation = 'vertical' ) 
ax . xaxis . grid ( alpha = 0.7 , linestyle = '--' ) 
fig . savefig ( self . _args . ecm_plot ) 
~~ def strip_and_uncomment ( asm_lines ) : 
asm_stripped = [ ] 
for line in asm_lines : 
~~~ asm_stripped . append ( line . split ( '#' ) [ 0 ] . strip ( ) ) 
~~ return asm_stripped 
~~ def strip_unreferenced_labels ( asm_lines ) : 
~~~ if re . match ( r'^\\S+:' , line ) : 
~~~ label = line [ 0 : line . find ( ':' ) ] 
if not any ( [ re . match ( r'^[^#]*\\s' + re . escape ( label ) + '[\\s,]?.*$' , l ) for l in asm_lines ] ) : 
~~~ line = '' 
~~ ~~ asm_stripped . append ( line ) 
~~ def find_asm_blocks ( asm_lines ) : 
last_labels = OrderedDict ( ) 
packed_ctr = 0 
avx_ctr = 0 
xmm_references = [ ] 
ymm_references = [ ] 
zmm_references = [ ] 
gp_references = [ ] 
mem_references = [ ] 
increments = { } 
for i , line in enumerate ( asm_lines ) : 
~~~ zmm_references += re . findall ( '%zmm[0-9]+' , line ) 
ymm_references += re . findall ( '%ymm[0-9]+' , line ) 
xmm_references += re . findall ( '%xmm[0-9]+' , line ) 
gp_references += re . findall ( '%r[a-z0-9]+' , line ) 
if re . search ( r'\\d*\\(%\\w+(,%\\w+)?(,\\d)?\\)' , line ) : 
~~~ m = re . search ( r'(?P<off>[-]?\\d*)\\(%(?P<basep>\\w+)(,%(?P<idx>\\w+))?(?:,(?P<scale>\\d))?\\)' 
r'(?P<eol>$)?' , 
line ) 
mem_references . append ( ( 
int ( m . group ( 'off' ) ) if m . group ( 'off' ) else 0 , 
m . group ( 'basep' ) , 
m . group ( 'idx' ) , 
int ( m . group ( 'scale' ) ) if m . group ( 'scale' ) else 1 , 
'load' if m . group ( 'eol' ) is None else 'store' ) ) 
~~ if re . match ( r"^[v]?(mul|add|sub|div|fmadd(132|213|231)?)[h]?p[ds]" , line ) : 
~~~ if line . startswith ( 'v' ) : 
~~~ avx_ctr += 1 
~~ packed_ctr += 1 
~~ elif re . match ( r'^\\S+:' , line ) : 
~~~ last_labels [ line [ 0 : line . find ( ':' ) ] ] = i 
~~ elif re . match ( r'^inc[bwlq]?\\s+%[a-z0-9]+' , line ) : 
~~~ reg_start = line . find ( '%' ) + 1 
increments [ line [ reg_start : ] ] = 1 
~~ elif re . match ( r'^add[bwlq]?\\s+\\$[0-9]+,\\s*%[a-z0-9]+' , line ) : 
~~~ const_start = line . find ( '$' ) + 1 
const_end = line [ const_start + 1 : ] . find ( ',' ) + const_start + 1 
reg_start = line . find ( '%' ) + 1 
increments [ line [ reg_start : ] ] = int ( line [ const_start : const_end ] ) 
~~ elif re . match ( r'^dec[bwlq]?' , line ) : 
increments [ line [ reg_start : ] ] = - 1 
~~ elif re . match ( r'^sub[bwlq]?\\s+\\$[0-9]+,' , line ) : 
increments [ line [ reg_start : ] ] = - int ( line [ const_start : const_end ] ) 
~~ elif last_labels and re . match ( r'^j[a-z]+\\s+\\S+\\s*' , line ) : 
~~~ last_label = None 
last_label_line = - 1 
for label_name , label_line in last_labels . items ( ) : 
~~~ if re . match ( r'^j[a-z]+\\s+' + re . escape ( label_name ) + r'\\s*' , line ) : 
~~~ last_label = label_name 
last_label_line = label_line 
~~ ~~ labels = list ( last_labels . keys ( ) ) 
if last_label : 
possible_idx_regs = None 
if mem_references : 
~~~ store_references = [ mref for mref in mem_references 
if mref [ 4 ] == 'store' ] 
refs = store_references or mem_references 
possible_idx_regs = list ( set ( increments . keys ( ) ) . intersection ( 
set ( [ r [ 1 ] for r in refs if r [ 1 ] is not None ] + 
[ r [ 2 ] for r in refs if r [ 2 ] is not None ] ) ) ) 
for mref in refs : 
~~~ for reg in list ( possible_idx_regs ) : 
~~~ if None not in mref [ 1 : 3 ] : 
~~~ if not ( reg == mref [ 1 ] or reg == mref [ 2 ] ) : 
~~~ possible_idx_regs . remove ( reg ) 
~~ ~~ ~~ ~~ idx_reg = None 
if len ( possible_idx_regs ) == 1 : 
~~~ idx_reg = possible_idx_regs [ 0 ] 
~~ elif possible_idx_regs and itemsEqual ( [ increments [ pidxreg ] 
for pidxreg in possible_idx_regs ] ) : 
~~ if idx_reg : 
~~~ mem_scales = [ mref [ 3 ] for mref in refs 
if idx_reg == mref [ 2 ] or idx_reg == mref [ 1 ] ] 
if itemsEqual ( mem_scales ) : 
~~~ pointer_increment = mem_scales [ 0 ] * increments [ idx_reg ] 
~~~ print ( "labels" , pformat ( labels [ labels . index ( last_label ) : ] ) ) 
print ( "lines" , pformat ( asm_lines [ last_label_line : i + 1 ] ) ) 
print ( "increments" , increments ) 
print ( "mem_references" , pformat ( mem_references ) ) 
print ( "idx_reg" , idx_reg ) 
print ( "mem_scales" , mem_scales ) 
~~ ~~ ~~ ~~ blocks . append ( { 'first_line' : last_label_line , 
'last_line' : i , 
'ops' : i - last_label_line , 
'labels' : labels [ labels . index ( last_label ) : ] , 
'packed_instr' : packed_ctr , 
'avx_instr' : avx_ctr , 
'XMM' : ( len ( xmm_references ) , len ( set ( xmm_references ) ) ) , 
'YMM' : ( len ( ymm_references ) , len ( set ( ymm_references ) ) ) , 
'ZMM' : ( len ( zmm_references ) , len ( set ( zmm_references ) ) ) , 
'GP' : ( len ( gp_references ) , len ( set ( gp_references ) ) ) , 
'regs' : ( len ( xmm_references ) + len ( ymm_references ) + 
len ( zmm_references ) + len ( gp_references ) , 
len ( set ( xmm_references ) ) + len ( set ( ymm_references ) ) + 
len ( set ( zmm_references ) ) + 
len ( set ( gp_references ) ) ) , 
'pointer_increment' : pointer_increment , 
'lines' : asm_lines [ last_label_line : i + 1 ] , 
'possible_idx_regs' : possible_idx_regs , 
'mem_references' : mem_references , 
'increments' : increments , } ) 
~~ packed_ctr = 0 
~~ ~~ return list ( enumerate ( blocks ) ) 
~~ def select_best_block ( blocks ) : 
if not blocks : 
~~ best_block = max ( blocks , key = lambda b : b [ 1 ] [ 'packed_instr' ] ) 
if best_block [ 1 ] [ 'packed_instr' ] == 0 : 
~~~ best_block = max ( blocks , 
key = lambda b : ( b [ 1 ] [ 'ops' ] + b [ 1 ] [ 'packed_instr' ] + b [ 1 ] [ 'avx_instr' ] , 
b [ 1 ] [ 'ZMM' ] , b [ 1 ] [ 'YMM' ] , b [ 1 ] [ 'XMM' ] ) ) 
~~ return best_block [ 0 ] 
~~ def userselect_increment ( block ) : 
print ( ) 
increment = None 
while increment is None : 
~~~ increment = int ( increment ) 
~~~ increment = None 
~~ ~~ block [ 'pointer_increment' ] = increment 
return increment 
~~ def userselect_block ( blocks , default = None , debug = False ) : 
"----------------+-----+------+-----++-----------+----------+----------+----------+---------++-------|" ) 
for idx , b in blocks : 
if debug : 
~~~ ln = b [ 'first_line' ] 
for l in b [ 'lines' ] : 
ln += 1 
print ( textwrap . indent ( 
pformat ( { k : v for k , v in b . items ( ) if k not in [ 'lines' ] } ) , 
~~ ~~ block_idx = - 1 
while not ( 0 <= block_idx < len ( blocks ) ) : 
~~~ block_idx = int ( block_idx ) 
~~~ block_idx = - 1 
~~ ~~ return block_idx 
~~ def insert_markers ( asm_lines , start_line , end_line ) : 
asm_lines = ( asm_lines [ : start_line ] + START_MARKER + 
asm_lines [ start_line : end_line + 1 ] + END_MARKER + 
asm_lines [ end_line + 1 : ] ) 
return asm_lines 
~~ def iaca_instrumentation ( input_file , output_file , 
block_selection = 'auto' , 
pointer_increment = 'auto_with_manual_fallback' , 
debug = False ) : 
assembly_orig = input_file . readlines ( ) 
if input_file is output_file : 
~~~ output_file . seek ( 0 ) 
output_file . truncate ( ) 
~~ if debug : 
~~~ block_selection = 'manual' 
~~ assembly = strip_and_uncomment ( copy ( assembly_orig ) ) 
assembly = strip_unreferenced_labels ( assembly ) 
blocks = find_asm_blocks ( assembly ) 
if block_selection == 'auto' : 
~~~ block_idx = select_best_block ( blocks ) 
~~ elif block_selection == 'manual' : 
~~~ block_idx = userselect_block ( blocks , default = select_best_block ( blocks ) , debug = debug ) 
~~ elif isinstance ( block_selection , int ) : 
~~~ block_idx = block_selection 
~~ block = blocks [ block_idx ] [ 1 ] 
if pointer_increment == 'auto' : 
~~~ if block [ 'pointer_increment' ] is None : 
~~ ~~ elif pointer_increment == 'auto_with_manual_fallback' : 
~~~ block [ 'pointer_increment' ] = userselect_increment ( block ) 
~~ ~~ elif pointer_increment == 'manual' : 
~~ elif isinstance ( pointer_increment , int ) : 
~~~ block [ 'pointer_increment' ] = pointer_increment 
"\ ) 
~~ instrumented_asm = insert_markers ( assembly_orig , block [ 'first_line' ] , block [ 'last_line' ] ) 
output_file . writelines ( instrumented_asm ) 
return block 
~~ def iaca_analyse_instrumented_binary ( instrumented_binary_file , micro_architecture ) : 
arch_map = { 
'NHM' : ( 'iaca2.2' , 'v2.2' , [ '-64' ] ) , 
'WSM' : ( 'iaca2.2' , 'v2.2' , [ '-64' ] ) , 
'SNB' : ( 'iaca2.3' , 'v2.3' , [ '-64' ] ) , 
'IVB' : ( 'iaca2.3' , 'v2.3' , [ '-64' ] ) , 
'HSW' : ( 'iaca3.0' , 'v3.0' , [ ] ) , 
'BDW' : ( 'iaca3.0' , 'v3.0' , [ ] ) , 
'SKL' : ( 'iaca3.0' , 'v3.0' , [ ] ) , 
'SKX' : ( 'iaca3.0' , 'v3.0' , [ ] ) , 
if micro_architecture not in arch_map : 
os . environ [ 'PATH' ] += ':' + iaca_path 
iaca_exec , iaca_version , base_args = arch_map [ micro_architecture ] 
if find_executable ( iaca_exec ) is None : 
~~ result = { } 
cmd = [ iaca_exec ] + base_args + [ '-arch' , micro_architecture , instrumented_binary_file ] 
~~~ iaca_output = subprocess . check_output ( cmd ) . decode ( 'utf-8' ) 
result [ 'output' ] = iaca_output 
~~ except OSError as e : 
~~ except subprocess . CalledProcessError as e : 
throughput = float ( match . groups ( ) [ 0 ] ) 
result [ 'throughput' ] = throughput 
ports = [ p . strip ( ) for p in ports [ 0 ] . split ( '|' ) ] [ 2 : ] 
cycles = [ c . strip ( ) for c in cycles [ 0 ] . split ( '|' ) ] [ 2 : ] 
port_cycles = [ ] 
for i in range ( len ( ports ) ) : 
~~~ subports = [ p . strip ( ) for p in ports [ i ] . split ( '-' ) ] 
port_cycles . append ( ( subports [ 0 ] , float ( subcycles [ 0 ] ) ) ) 
port_cycles . append ( ( subports [ 0 ] + subports [ 1 ] , float ( subcycles [ 1 ] ) ) ) 
~~ elif ports [ i ] and cycles [ i ] : 
~~~ port_cycles . append ( ( ports [ i ] , float ( cycles [ i ] ) ) ) 
result [ 'uops' ] = float ( match . groups ( ) [ 0 ] ) 
~~ def main ( ) : 
parser = argparse . ArgumentParser ( 
parser . add_argument ( 'source' , type = argparse . FileType ( ) , nargs = '?' , default = sys . stdin , 
parser . add_argument ( '--outfile' , '-o' , type = argparse . FileType ( 'w' ) , nargs = '?' , 
parser . add_argument ( '--debug' , action = 'store_true' , 
iaca_instrumentation ( input_file = args . source , output_file = args . outfile , 
block_selection = 'manual' , pointer_increment = 1 , debug = args . debug ) 
~~ def simulate ( kernel , model , define_dict , blocking_constant , blocking_length ) : 
kernel . clear_state ( ) 
for k , v in define_dict . items ( ) : 
~~~ kernel . set_constant ( k , v ) 
~~ kernel . set_constant ( blocking_constant , blocking_length ) 
model . analyze ( ) 
return sum ( [ cy for dscr , cy in model . results [ 'cycles' ] ] ) 
~~ def good_prefix ( self , max_error = 0.01 , round_length = 2 , min_prefix = '' , max_prefix = None ) : 
good_prefix = min_prefix 
base_value = self . base_value ( ) 
for k , v in list ( self . PREFIXES . items ( ) ) : 
~~~ if max_prefix is not None and v > self . PREFIXES [ max_prefix ] : 
~~ if abs ( round ( base_value / v , round_length ) * v - base_value ) > base_value * max_error : 
~~ if abs ( round ( base_value / v , round_length ) ) < 0.9 : 
~~ if v < self . PREFIXES [ good_prefix ] : 
~~ good_prefix = k 
~~ return good_prefix 
~~ def space ( start , stop , num , endpoint = True , log = False , base = 10 ) : 
if log : 
~~~ start = math . log ( start , base ) 
stop = math . log ( stop , base ) 
~~ if endpoint : 
~~~ step_length = float ( ( stop - start ) ) / float ( num - 1 ) 
~~~ step_length = float ( ( stop - start ) ) / float ( num ) 
~~ i = 0 
while i < num : 
~~~ if log : 
~~~ yield int ( round ( base ** ( start + i * step_length ) ) ) 
~~~ yield int ( round ( start + i * step_length ) ) 
~~ i += 1 
~~ ~~ def get_last_modified_datetime ( dir_path = os . path . dirname ( __file__ ) ) : 
max_mtime = 0 
for root , dirs , files in os . walk ( dir_path ) : 
~~~ for f in files : 
~~~ p = os . path . join ( root , f ) 
~~~ max_mtime = max ( max_mtime , os . stat ( p ) . st_mtime ) 
~~ except FileNotFoundError : 
~~ ~~ ~~ return datetime . utcfromtimestamp ( max_mtime ) 
~~ def create_parser ( ) : 
parser . add_argument ( '--machine' , '-m' , type = argparse . FileType ( 'r' ) , required = True , 
parser . add_argument ( '--pmodel' , '-p' , choices = models . __all__ , required = True , action = 'append' , 
parser . add_argument ( '-D' , '--define' , nargs = 2 , metavar = ( 'KEY' , 'VALUE' ) , default = [ ] , 
action = AppendStringRange , 
'file.' ) 
parser . add_argument ( '--verbose' , '-v' , action = 'count' , default = 0 , 
parser . add_argument ( 'code_file' , metavar = 'FILE' , type = argparse . FileType ( ) , 
parser . add_argument ( '--asm-block' , metavar = 'BLOCK' , default = 'auto' , 
help = \ 
\ ) 
parser . add_argument ( '--pointer-increment' , metavar = 'INCR' , default = 'auto' , type = int_or_str , 
\ 
parser . add_argument ( '--store' , metavar = 'PICKLE' , type = argparse . FileType ( 'a+b' ) , 
parser . add_argument ( '--unit' , '-u' , choices = [ 'cy/CL' , 'cy/It' , 'It/s' , 'FLOP/s' ] , 
parser . add_argument ( '--cores' , '-c' , metavar = 'CORES' , type = int , default = 1 , 
'cores.' ) 
parser . add_argument ( '--kernel-description' , action = 'store_true' , 
parser . add_argument ( '--clean-intermediates' , action = 'store_true' , 
parser . add_argument ( '--cache-predictor' , '-P' , choices = [ 'LC' , 'SIM' ] , default = 'SIM' , 
parser . add_argument ( '--compiler' , '-C' , type = str , default = None , 
parser . add_argument ( '--compiler-flags' , type = str , default = None , 
for m in models . __all__ : 
getattr ( models , m ) . configure_arggroup ( ag ) 
~~ return parser 
~~ def check_arguments ( args , parser ) : 
if args . asm_block not in [ 'auto' , 'manual' ] : 
~~~ args . asm_block = int ( args . asm_block ) 
~~~ parser . error ( \ ) 
~~ ~~ if not args . unit : 
~~~ if 'Roofline' in args . pmodel or 'RooflineIACA' in args . pmodel : 
~~~ args . unit = 'FLOP/s' 
~~~ args . unit = 'cy/CL' 
~~ ~~ ~~ def run ( parser , args , output_file = sys . stdout ) : 
result_storage = { } 
if args . store : 
~~~ args . store . seek ( 0 ) 
~~~ result_storage = pickle . load ( args . store ) 
~~ except EOFError : 
~~ args . store . close ( ) 
~~ machine = MachineModel ( args . machine . name , args = args ) 
if not args . kernel_description : 
~~~ code = str ( args . code_file . read ( ) ) 
code = clean_code ( code ) 
kernel = KernelCode ( code , filename = args . code_file . name , machine = machine , 
keep_intermediates = not args . clean_intermediates ) 
~~~ description = str ( args . code_file . read ( ) ) 
kernel = KernelDescription ( yaml . load ( description , Loader = yaml . Loader ) , machine = machine ) 
~~ required_consts = [ v [ 1 ] for v in kernel . variables . values ( ) if v [ 1 ] is not None ] 
required_consts += [ [ l [ 'start' ] , l [ 'stop' ] ] for l in kernel . get_loop_stack ( ) ] 
required_consts = [ i for l in required_consts for i in l ] 
required_consts = set ( [ i for l in required_consts for i in l . free_symbols ] ) 
if len ( required_consts ) > 0 : 
~~~ define_dict = { } 
for name , values in args . define : 
~~~ if name not in define_dict : 
~~~ define_dict [ name ] = [ [ name , v ] for v in values ] 
~~ for v in values : 
~~~ if v not in define_dict [ name ] : 
~~~ define_dict [ name ] . append ( [ name , v ] ) 
~~ ~~ ~~ define_product = list ( itertools . product ( * list ( define_dict . values ( ) ) ) ) 
if set ( required_consts ) . difference ( set ( [ symbol_pos_int ( k ) for k in define_dict . keys ( ) ] ) ) : 
required_consts ) ) 
~~~ define_product = [ { } ] 
~~ for define in define_product : 
~~~ kernel . clear_state ( ) 
for k , v in define : 
~~ for model_name in uniquify ( args . pmodel ) : 
if args . verbose > 1 : 
~~~ if not args . kernel_description : 
~~~ kernel . print_kernel_code ( output_file = output_file ) 
~~ kernel . print_variables_info ( output_file = output_file ) 
kernel . print_kernel_info ( output_file = output_file ) 
~~ if args . verbose > 0 : 
~~~ kernel . print_constants_info ( output_file = output_file ) 
~~ model = getattr ( models , model_name ) ( kernel , machine , args , parser ) 
model . report ( output_file = output_file ) 
kernel_name = os . path . split ( args . code_file . name ) [ 1 ] 
if kernel_name not in result_storage : 
~~~ result_storage [ kernel_name ] = { } 
~~ if tuple ( kernel . constants . items ( ) ) not in result_storage [ kernel_name ] : 
~~~ result_storage [ kernel_name ] [ tuple ( kernel . constants . items ( ) ) ] = { } 
~~ result_storage [ kernel_name ] [ tuple ( kernel . constants . items ( ) ) ] [ model_name ] = model . results 
~~ if args . store : 
~~~ temp_name = args . store . name + '.tmp' 
with open ( temp_name , 'wb+' ) as f : 
~~~ pickle . dump ( result_storage , f ) 
~~ shutil . move ( temp_name , args . store . name ) 
~~ ~~ ~~ def main ( ) : 
parser = create_parser ( ) 
check_arguments ( args , parser ) 
run ( parser , args ) 
parser . add_argument ( 'destination' , type = argparse . FileType ( 'r+b' ) , 
parser . add_argument ( 'source' , type = argparse . FileType ( 'rb' ) , nargs = '+' , 
result = pickle . load ( args . destination ) 
for s in args . source : 
~~~ data = pickle . load ( s ) 
update ( result , data ) 
~~ args . destination . seek ( 0 ) 
args . destination . truncate ( ) 
pickle . dump ( result , args . destination ) 
~~ def symbol_pos_int ( * args , ** kwargs ) : 
kwargs . update ( { 'positive' : True , 
'integer' : True } ) 
return sympy . Symbol ( * args , ** kwargs ) 
textblock = textblock . split ( '\\n' ) 
line = prefix + textblock [ 0 ] + '\\n' 
if len ( later_prefix ) == 1 : 
~~ line = line + '\\n' . join ( [ later_prefix + x for x in textblock [ 1 : ] ] ) 
if line [ - 1 ] != '\\n' : 
~~~ return line + '\\n' 
~~~ return line 
~~ ~~ def transform_multidim_to_1d_decl ( decl ) : 
dims = [ ] 
type_ = decl . type 
while type ( type_ ) is c_ast . ArrayDecl : 
~~~ dims . append ( type_ . dim ) 
type_ = type_ . type 
~~ if dims : 
~~~ decl . type . dim = reduce ( lambda l , r : c_ast . BinaryOp ( '*' , l , r ) , dims ) 
decl . type . type = type_ 
~~ return decl . name , dims 
~~ def transform_multidim_to_1d_ref ( aref , dimension_dict ) : 
name = aref 
while type ( name ) is c_ast . ArrayRef : 
~~~ dims . append ( name . subscript ) 
name = name . name 
~~ subscript_list = [ ] 
for i , d in enumerate ( dims ) : 
~~~ subscript_list . append ( d ) 
~~~ subscript_list . append ( c_ast . BinaryOp ( '*' , d , reduce ( 
lambda l , r : c_ast . BinaryOp ( '*' , l , r ) , 
dimension_dict [ name . name ] [ - 1 : - i - 1 : - 1 ] ) ) ) 
~~ ~~ aref . subscript = reduce ( 
lambda l , r : c_ast . BinaryOp ( '+' , l , r ) , subscript_list ) 
aref . name = name 
~~ def transform_array_decl_to_malloc ( decl , with_init = True ) : 
if type ( decl . type ) is not c_ast . ArrayDecl : 
~~ type_ = c_ast . PtrDecl ( [ ] , decl . type . type ) 
if with_init : 
~~~ decl . init = c_ast . FuncCall ( 
c_ast . ID ( 'aligned_malloc' ) , 
c_ast . ExprList ( [ 
c_ast . BinaryOp ( 
'*' , 
c_ast . UnaryOp ( 
'sizeof' , 
c_ast . Typename ( None , [ ] , c_ast . TypeDecl ( 
None , [ ] , decl . type . type . type ) ) ) , 
decl . type . dim ) , 
c_ast . Constant ( 'int' , '32' ) ] ) ) 
~~ decl . type = type_ 
~~ def find_node_type ( ast , node_type ) : 
if type ( ast ) is node_type : 
~~~ return [ ast ] 
~~ elif type ( ast ) is list : 
~~~ return reduce ( operator . add , list ( map ( lambda a : find_node_type ( a , node_type ) , ast ) ) , [ ] ) 
~~ elif ast is None : 
~~~ return reduce ( operator . add , 
[ find_node_type ( o [ 1 ] , node_type ) for o in ast . children ( ) ] , [ ] ) 
~~ ~~ def force_iterable ( f ) : 
def wrapper ( * args , ** kwargs ) : 
~~~ r = f ( * args , ** kwargs ) 
if hasattr ( r , '__iter__' ) : 
~~~ return r 
~~~ return [ r ] 
~~ ~~ return wrapper 
~~ def reduce_path ( path ) : 
relative_path = os . path . relpath ( path ) 
if len ( relative_path ) < len ( path ) : 
~~~ return relative_path 
~~ ~~ def check ( self ) : 
datatypes = [ v [ 0 ] for v in self . variables . values ( ) ] 
~~ def set_constant ( self , name , value ) : 
if isinstance ( name , sympy . Symbol ) : 
~~~ self . constants [ name ] = value 
~~~ self . constants [ symbol_pos_int ( name ) ] = value 
~~ ~~ def set_variable ( self , name , type_ , size ) : 
if self . datatype is None : 
~~~ self . datatype = type_ 
self . variables [ name ] = ( type_ , size ) 
~~ def subs_consts ( self , expr ) : 
if isinstance ( expr , numbers . Number ) : 
~~~ return expr 
~~~ return expr . subs ( self . constants ) 
~~ ~~ def array_sizes ( self , in_bytes = False , subs_consts = False ) : 
var_sizes = { } 
for var_name , var_info in self . variables . items ( ) : 
~~~ var_type , var_size = var_info 
if var_size is None : 
~~ var_sizes [ var_name ] = reduce ( operator . mul , var_size , 1 ) 
if in_bytes : 
~~~ element_size = self . datatypes_size [ var_type ] 
var_sizes [ var_name ] *= element_size 
~~ ~~ if subs_consts : 
~~~ return { k : self . subs_consts ( v ) for k , v in var_sizes . items ( ) } 
~~~ return var_sizes 
~~ ~~ def _calculate_relative_offset ( self , name , access_dimensions ) : 
offset = 0 
base_dims = self . variables [ name ] [ 1 ] 
for dim , offset_info in enumerate ( access_dimensions ) : 
~~~ offset_type , idx_name , dim_offset = offset_info 
if offset_type == 'rel' : 
~~~ offset += self . subs_consts ( 
dim_offset * reduce ( operator . mul , base_dims [ dim + 1 : ] , sympy . Integer ( 1 ) ) ) 
~~ ~~ return offset 
~~ def _remove_duplicate_accesses ( self ) : 
self . destinations = { var_name : set ( acs ) for var_name , acs in self . destinations . items ( ) } 
self . sources = { var_name : set ( acs ) for var_name , acs in self . sources . items ( ) } 
~~ def access_to_sympy ( self , var_name , access ) : 
base_sizes = self . variables [ var_name ] [ 1 ] 
expr = sympy . Number ( 0 ) 
for dimension , a in enumerate ( access ) : 
~~~ base_size = reduce ( operator . mul , base_sizes [ dimension + 1 : ] , sympy . Integer ( 1 ) ) 
expr += base_size * a 
~~ return expr 
~~ def iteration_length ( self , dimension = None ) : 
total_length = 1 
if dimension is not None : 
~~~ loops = [ self . _loop_stack [ dimension ] ] 
~~~ loops = reversed ( self . _loop_stack ) 
~~ for var_name , start , end , incr in loops : 
~~~ length = end - start 
total_length = total_length * length 
~~ return self . subs_consts ( total_length ) 
~~ def get_loop_stack ( self , subs_consts = False ) : 
for l in self . _loop_stack : 
~~~ if subs_consts : 
~~~ yield { 'index' : l [ 0 ] , 
'start' : self . subs_consts ( l [ 1 ] ) , 
'stop' : self . subs_consts ( l [ 2 ] ) , 
'increment' : self . subs_consts ( l [ 3 ] ) } 
~~~ yield { 'index' : l [ 0 ] , 'start' : l [ 1 ] , 'stop' : l [ 2 ] , 'increment' : l [ 3 ] } 
~~ ~~ ~~ def index_order ( self , sources = True , destinations = True ) : 
if sources : 
~~~ arefs = chain ( * self . sources . values ( ) ) 
~~~ arefs = [ ] 
~~ if destinations : 
~~~ arefs = chain ( arefs , * self . destinations . values ( ) ) 
~~ ret = [ ] 
for a in [ aref for aref in arefs if aref is not None ] : 
~~~ ref = [ ] 
for expr in a : 
~~~ ref . append ( expr . free_symbols ) 
~~ ret . append ( ref ) 
~~ def compile_sympy_accesses ( self , sources = True , destinations = True ) : 
for var_name in self . variables : 
~~~ if sources : 
~~~ for r in self . sources . get ( var_name , [ ] ) : 
~~ sympy_accesses [ var_name ] . append ( self . access_to_sympy ( var_name , r ) ) 
~~ ~~ if destinations : 
~~~ for w in self . destinations . get ( var_name , [ ] ) : 
~~ sympy_accesses [ var_name ] . append ( self . access_to_sympy ( var_name , w ) ) 
~~ ~~ ~~ return sympy_accesses 
~~ def compile_relative_distances ( self , sympy_accesses = None ) : 
if sympy_accesses is None : 
~~~ sympy_accesses = self . compile_sympy_accesses ( ) 
~~ sympy_distances = defaultdict ( list ) 
for var_name , accesses in sympy_accesses . items ( ) : 
~~~ for i in range ( 1 , len ( accesses ) ) : 
~~~ sympy_distances [ var_name ] . append ( ( accesses [ i - 1 ] - accesses [ i ] ) . simplify ( ) ) 
~~ ~~ return sympy_distances 
~~ def global_iterator_to_indices ( self , git = None ) : 
base_loop_counters = { } 
global_iterator = symbol_pos_int ( 'global_iterator' ) 
idiv = implemented_function ( sympy . Function ( str ( 'idiv' ) ) , lambda x , y : x // y ) 
last_incr = 1 
for var_name , start , end , incr in reversed ( self . _loop_stack ) : 
~~~ loop_var = symbol_pos_int ( var_name ) 
counter = start + ( idiv ( global_iterator * last_incr , total_length ) * incr ) % length 
last_incr = incr 
base_loop_counters [ loop_var ] = sympy . lambdify ( 
global_iterator , 
self . subs_consts ( counter ) , modules = [ numpy , { 'Mod' : numpy . mod } ] ) 
if git is not None : 
~~~ base_loop_counters [ loop_var ] = sympy . Integer ( self . subs_consts ( counter ) ) 
~~ except ( ValueError , TypeError ) : 
~~~ base_loop_counters [ loop_var ] = base_loop_counters [ loop_var ] ( git ) 
~~ ~~ ~~ return base_loop_counters 
~~ def global_iterator ( self ) : 
global_iterator = sympy . Integer ( 0 ) 
total_length = sympy . Integer ( 1 ) 
global_iterator += ( loop_var - start ) * total_length 
total_length *= length 
~~ return global_iterator 
~~ def indices_to_global_iterator ( self , indices ) : 
global_iterator = self . subs_consts ( self . global_iterator ( ) . subs ( indices ) ) 
return global_iterator 
~~ def max_global_iteration ( self ) : 
return self . indices_to_global_iterator ( { 
symbol_pos_int ( var_name ) : end - 1 for var_name , start , end , incr in self . _loop_stack 
~~ def compile_global_offsets ( self , iteration = 0 , spacing = 0 ) : 
global_load_offsets = [ ] 
global_store_offsets = [ ] 
if isinstance ( iteration , range ) : 
~~~ iteration = numpy . arange ( iteration . start , iteration . stop , iteration . step , dtype = 'O' ) 
~~~ if not isinstance ( iteration , collections . Sequence ) : 
~~~ iteration = [ iteration ] 
~~ iteration = numpy . array ( iteration , dtype = 'O' ) 
~~ base_loop_counters = self . global_iterator_to_indices ( ) 
total_length = self . iteration_length ( ) 
iteration . max ( ) , self . subs_consts ( total_length ) ) 
var_sizes = self . array_sizes ( in_bytes = True , subs_consts = True ) 
base_offsets = { } 
base = 0 
for var_name , var_size in sorted ( var_sizes . items ( ) , key = lambda v : v [ 0 ] ) : 
~~~ base_offsets [ var_name ] = base 
array_total_size = self . subs_consts ( var_size + spacing ) 
array_total_size = ( ( int ( array_total_size ) + 63 ) & ~ 63 ) 
base += array_total_size 
~~ for var_name , var_size in var_sizes . items ( ) : 
~~~ element_size = self . datatypes_size [ self . variables [ var_name ] [ 0 ] ] 
for r in self . sources . get ( var_name , [ ] ) : 
~~~ offset_expr = self . access_to_sympy ( var_name , r ) 
if not any ( [ s in base_loop_counters . keys ( ) for s in offset_expr . free_symbols ] ) : 
~~ offset = force_iterable ( sympy . lambdify ( 
base_loop_counters . keys ( ) , 
self . subs_consts ( 
offset_expr * element_size 
+ base_offsets [ var_name ] ) , numpy ) ) 
global_load_offsets . append ( offset ) 
~~ for w in self . destinations . get ( var_name , [ ] ) : 
~~~ offset_expr = self . access_to_sympy ( var_name , w ) 
global_store_offsets . append ( offset ) 
~~ ~~ counter_per_it = [ v ( iteration ) for v in base_loop_counters . values ( ) ] 
load_offsets = [ ] 
for o in global_load_offsets : 
~~~ load_offsets . append ( o ( * counter_per_it ) ) 
~~ load_offsets = numpy . asarray ( load_offsets ) . T 
store_offsets = [ ] 
for o in global_store_offsets : 
~~~ store_offsets . append ( o ( * counter_per_it ) ) 
~~ store_offsets = numpy . asarray ( store_offsets ) . T 
store_width = store_offsets . shape [ 1 ] if len ( store_offsets . shape ) > 1 else 0 
dtype = [ ( 'load' , load_offsets . dtype , ( load_offsets . shape [ 1 ] , ) ) , 
( 'store' , store_offsets . dtype , ( store_width , ) ) ] 
offsets = numpy . empty ( max ( load_offsets . shape [ 0 ] , store_offsets . shape [ 0 ] ) , dtype = dtype ) 
offsets [ 'load' ] = load_offsets 
offsets [ 'store' ] = store_offsets 
return offsets 
~~ def bytes_per_iteration ( self ) : 
var_name = list ( self . destinations ) [ 0 ] 
var_type = self . variables [ var_name ] [ 0 ] 
return self . datatypes_size [ var_type ] * self . _loop_stack [ - 1 ] [ 3 ] 
~~ def print_kernel_info ( self , output_file = sys . stdout ) : 
'---------+---------------------------------\\n' ) 
'---------+------------...\\n' ) 
for name , offsets in list ( self . sources . items ( ) ) : 
right_side = '\\n' . join ( [ '{!r:}' . format ( o ) for o in offsets ] ) 
for name , offsets in list ( self . destinations . items ( ) ) : 
'----+-------\\n' ) 
for op , count in list ( self . _flops . items ( ) ) : 
~~ def print_variables_info ( self , output_file = sys . stdout ) : 
'---------+-------------------------\\n' ) 
for name , var_info in list ( self . variables . items ( ) ) : 
~~ def print_constants_info ( self , output_file = sys . stdout ) : 
'---------+-----------\\n' ) 
for name , value in list ( self . constants . items ( ) ) : 
~~ def _get_intermediate_file ( self , name , machine_and_compiler_dependent = True , binary = False , 
fp = True ) : 
if self . _filename : 
~~~ base_name = os . path . join ( os . path . dirname ( self . _filename ) , 
'.' + os . path . basename ( self . _filename ) + '_kerncraft' ) 
~~~ base_name = tempfile . mkdtemp ( ) 
~~ if not self . _keep_intermediates : 
~~~ atexit . register ( shutil . rmtree , base_name ) 
~~ if machine_and_compiler_dependent : 
~~~ compiler , compiler_args = self . _machine . get_compiler ( ) 
compiler_args = '_' . join ( compiler_args ) . replace ( '/' , '' ) 
base_name += '/{}/{}/{}/' . format ( 
self . _machine . get_identifier ( ) , compiler , compiler_args ) 
~~ os . makedirs ( base_name , exist_ok = True ) 
file_path = os . path . join ( base_name , name ) 
already_exists = False 
if os . path . exists ( file_path ) : 
~~~ file_modified = datetime . utcfromtimestamp ( os . stat ( file_path ) . st_mtime ) 
if ( file_modified < self . _machine . get_last_modified_datetime ( ) or 
file_modified < kerncraft . get_last_modified_datetime ( ) or 
( self . _filename and 
file_modified < datetime . utcfromtimestamp ( os . stat ( self . _filename ) . st_mtime ) ) ) : 
~~~ os . remove ( file_path ) 
~~~ already_exists = True 
~~ ~~ if fp : 
~~~ if already_exists : 
~~~ mode = 'r+' 
~~~ mode = 'w' 
~~ if binary : 
~~~ mode += 'b' 
~~ f = open ( file_path , mode ) 
return f , already_exists 
~~~ return reduce_path ( file_path ) , already_exists 
~~ ~~ def print_kernel_code ( self , output_file = sys . stdout ) : 
print ( self . kernel_code , file = output_file ) 
~~ def conv_ast_to_sym ( self , math_ast ) : 
if type ( math_ast ) is c_ast . ID : 
~~~ return symbol_pos_int ( math_ast . name ) 
~~ elif type ( math_ast ) is c_ast . Constant : 
~~~ return sympy . Integer ( math_ast . value ) 
~~~ op = { 
'*' : operator . mul , 
'+' : operator . add , 
'-' : operator . sub 
return op [ math_ast . op ] ( 
self . conv_ast_to_sym ( math_ast . left ) , 
self . conv_ast_to_sym ( math_ast . right ) ) 
~~ ~~ def _get_offsets ( self , aref , dim = 0 ) : 
if isinstance ( aref , c_ast . ID ) : 
idxs = [ self . conv_ast_to_sym ( aref . subscript ) ] 
if type ( aref . name ) is c_ast . ArrayRef : 
~~~ idxs += self . _get_offsets ( aref . name , dim = dim + 1 ) 
~~ if dim == 0 : 
~~~ idxs . reverse ( ) 
~~ return tuple ( idxs ) 
~~ def _get_basename ( cls , aref ) : 
if isinstance ( aref . name , c_ast . ArrayRef ) : 
~~~ return cls . _get_basename ( aref . name ) 
~~ elif isinstance ( aref . name , str ) : 
~~~ return aref . name 
~~~ return aref . name . name 
~~ ~~ def get_index_type ( self , loop_nest = None ) : 
if loop_nest is None : 
~~~ loop_nest = self . get_kernel_loop_nest ( ) 
~~ if type ( loop_nest ) is c_ast . For : 
~~~ loop_nest = [ loop_nest ] 
~~ index_types = ( None , None ) 
for s in loop_nest : 
~~~ if type ( s ) is c_ast . For : 
~~~ if type ( s . stmt ) in [ c_ast . For , c_ast . Compound ] : 
~~~ other = self . get_index_type ( loop_nest = s . stmt ) 
~~~ other = None 
~~ index_types = ( s . init . decls [ 0 ] . type . type . names , other ) 
~~ ~~ if index_types [ 0 ] == index_types [ 1 ] or index_types [ 1 ] is None : 
~~~ return index_types [ 0 ] 
~~ ~~ def _build_const_declartions ( self , with_init = True ) : 
decls = [ ] 
index_type = self . get_index_type ( ) 
for k in self . constants : 
~~~ type_decl = c_ast . TypeDecl ( k . name , [ 'const' ] , c_ast . IdentifierType ( index_type ) ) 
init = None 
~~~ init = c_ast . FuncCall ( 
c_ast . ID ( 'atoi' ) , 
c_ast . ExprList ( [ c_ast . ArrayRef ( c_ast . ID ( 'argv' ) , 
c_ast . Constant ( 'int' , str ( i ) ) ) ] ) ) 
decls . append ( c_ast . Decl ( 
k . name , [ 'const' ] , [ ] , [ ] , 
type_decl , init , None ) ) 
~~ return decls 
~~ def get_array_declarations ( self ) : 
return [ d for d in self . kernel_ast . block_items 
if type ( d ) is c_ast . Decl and type ( d . type ) is c_ast . ArrayDecl ] 
~~ def get_kernel_loop_nest ( self ) : 
loop_nest = [ s for s in self . kernel_ast . block_items 
if type ( s ) in [ c_ast . For , c_ast . Pragma , c_ast . FuncCall ] ] 
return loop_nest 
~~ def _build_array_declarations ( self , with_init = True ) : 
array_declarations = deepcopy ( self . get_array_declarations ( ) ) 
array_dict = [ ] 
for d in array_declarations : 
~~~ array_dict . append ( transform_multidim_to_1d_decl ( d ) ) 
transform_array_decl_to_malloc ( d , with_init = with_init ) 
~~ return array_declarations , dict ( array_dict ) 
~~ def _find_inner_most_loop ( self , loop_nest ) : 
r = None 
~~~ return self . _find_inner_most_loop ( s ) or s 
~~~ r = r or self . _find_inner_most_loop ( s ) 
~~ ~~ return r 
~~ def _build_array_initializations ( self , array_dimensions ) : 
kernel = deepcopy ( deepcopy ( self . get_kernel_loop_nest ( ) ) ) 
inner_most = self . _find_inner_most_loop ( kernel ) 
orig_inner_stmt = inner_most . stmt 
inner_most . stmt = c_ast . Compound ( [ ] ) 
rand_float_str = str ( random . uniform ( 1.0 , 0.1 ) ) 
for aref in find_node_type ( orig_inner_stmt , c_ast . ArrayRef ) : 
~~~ transform_multidim_to_1d_ref ( aref , array_dimensions ) 
inner_most . stmt . block_items . append ( c_ast . Assignment ( 
'=' , aref , c_ast . Constant ( 'float' , rand_float_str ) ) ) 
~~ return kernel 
~~ def _build_dummy_calls ( self ) : 
dummy_calls = [ ] 
for d in self . kernel_ast . block_items : 
~~~ if type ( d ) is not c_ast . Decl : continue 
if type ( d . type ) is c_ast . ArrayDecl : 
~~~ dummy_calls . append ( c_ast . FuncCall ( 
c_ast . ID ( 'dummy' ) , 
c_ast . ExprList ( [ c_ast . ID ( d . name ) ] ) ) ) 
c_ast . ExprList ( [ c_ast . UnaryOp ( '&' , c_ast . ID ( d . name ) ) ] ) ) ) 
~~ ~~ dummy_stmt = c_ast . If ( 
cond = c_ast . ID ( 'var_false' ) , 
iftrue = c_ast . Compound ( dummy_calls ) , 
iffalse = None ) 
return dummy_stmt 
~~ def _build_kernel_function_declaration ( self , name = 'kernel' ) : 
array_declarations , array_dimensions = self . _build_array_declarations ( with_init = False ) 
scalar_declarations = self . _build_scalar_declarations ( with_init = False ) 
const_declarations = self . _build_const_declartions ( with_init = False ) 
return c_ast . FuncDecl ( args = c_ast . ParamList ( params = array_declarations + scalar_declarations + 
const_declarations ) , 
type = c_ast . TypeDecl ( declname = name , 
quals = [ ] , 
type = c_ast . IdentifierType ( names = [ 'void' ] ) ) ) 
~~ def _build_scalar_declarations ( self , with_init = True ) : 
scalar_declarations = [ deepcopy ( d ) for d in self . kernel_ast . block_items 
if type ( d ) is c_ast . Decl and type ( d . type ) is c_ast . TypeDecl ] 
for d in scalar_declarations : 
~~~ if d . type . type . names [ 0 ] in [ 'double' , 'float' ] : 
~~~ d . init = c_ast . Constant ( 'float' , str ( random . uniform ( 1.0 , 0.1 ) ) ) 
~~~ d . init = c_ast . Constant ( 'int' , 2 ) 
~~ ~~ ~~ return scalar_declarations 
~~ def get_kernel_code ( self , openmp = False , as_filename = False , name = 'kernel' ) : 
file_name = 'kernel' 
if openmp : 
~~~ file_name += '-omp' 
~~ file_name += '.c' 
fp , already_available = self . _get_intermediate_file ( 
file_name , machine_and_compiler_dependent = False ) 
if already_available : 
~~~ code = fp . read ( ) 
~~~ array_declarations , array_dimensions = self . _build_array_declarations ( ) 
~~~ kernel = deepcopy ( self . get_kernel_loop_nest ( ) ) 
for aref in find_node_type ( kernel , c_ast . ArrayRef ) : 
~~ omp_pragmas = [ p for p in find_node_type ( kernel , c_ast . Pragma ) 
if 'omp' in p . string ] 
if not omp_pragmas : 
~~ ~~ function_ast = c_ast . FuncDef ( decl = c_ast . Decl ( 
name = name , type = self . _build_kernel_function_declaration ( name = name ) , quals = [ ] , 
storage = [ ] , funcspec = [ ] , init = None , bitsize = None ) , 
body = c_ast . Compound ( block_items = kernel ) , 
param_decls = None ) 
code = CGenerator ( ) . visit ( function_ast ) 
code = \ + code 
fp . write ( code ) 
~~ fp . close ( ) 
if as_filename : 
~~~ return fp . name 
~~~ return code 
~~ ~~ def _build_kernel_call ( self , name = 'kernel' ) : 
return c_ast . FuncCall ( name = c_ast . ID ( name = name ) , args = c_ast . ExprList ( exprs = [ 
c_ast . ID ( name = d . name ) for d in ( 
self . _build_array_declarations ( ) [ 0 ] + 
self . _build_scalar_declarations ( ) + 
self . _build_const_declartions ( ) ) ] ) ) 
~~ def get_main_code ( self , as_filename = False , kernel_function_name = 'kernel' ) : 
fp , already_available = self . _get_intermediate_file ( 'main.c' , 
machine_and_compiler_dependent = False ) 
~~~ parser = CParser ( ) 
template_code = self . CODE_TEMPLATE 
template_ast = parser . parse ( clean_code ( template_code , 
macros = True , comments = True , pragmas = False ) ) 
ast = deepcopy ( template_ast ) 
replace_id ( ast , "DECLARE_CONSTS" , self . _build_const_declartions ( with_init = True ) ) 
array_declarations , array_dimensions = self . _build_array_declarations ( ) 
replace_id ( ast , "DECLARE_ARRAYS" , array_declarations ) 
replace_id ( ast , "DECLARE_INIT_SCALARS" , self . _build_scalar_declarations ( ) ) 
replace_id ( ast , "DUMMY_CALLS" , self . _build_dummy_calls ( ) ) 
ast . ext . insert ( 0 , self . _build_kernel_function_declaration ( 
name = kernel_function_name ) ) 
replace_id ( ast , "KERNEL_CALL" , self . _build_kernel_call ( ) ) 
replace_id ( ast , "INIT_ARRAYS" , self . _build_array_initializations ( array_dimensions ) ) 
code = CGenerator ( ) . visit ( ast ) 
code = '\\n' . join ( [ l for l in template_code . split ( '\\n' ) if l . startswith ( "#include" ) ] ) + '\\n\\n' + code 
~~ ~~ def assemble_to_object ( self , in_filename , verbose = False ) : 
file_base_name = os . path . splitext ( os . path . basename ( in_filename ) ) [ 0 ] 
out_filename , already_exists = self . _get_intermediate_file ( file_base_name + '.o' , 
binary = True , 
fp = False ) 
if already_exists : 
~~ compiler , compiler_args = self . _machine . get_compiler ( ) 
compiler_args . append ( '-c' ) 
cmd = [ compiler ] + [ 
in_filename ] + compiler_args + [ '-o' , out_filename ] 
if verbose : 
~~~ subprocess . check_output ( cmd ) 
~~ return out_filename 
~~ def compile_kernel ( self , openmp = False , assembly = False , verbose = False ) : 
compiler , compiler_args = self . _machine . get_compiler ( ) 
in_filename = self . get_kernel_code ( openmp = openmp , as_filename = True ) 
if assembly : 
~~~ compiler_args += [ '-S' ] 
suffix = '.s' 
~~~ suffix = '.o' 
~~ out_filename , already_exists = self . _get_intermediate_file ( 
os . path . splitext ( os . path . basename ( in_filename ) ) [ 0 ] + suffix , binary = not assembly , fp = False ) 
~~~ if verbose : 
~~ compiler_args += [ '-std=c99' ] 
cmd = ( [ compiler ] + 
[ in_filename , 
'-c' , 
'-I' + reduce_path ( os . path . abspath ( os . path . dirname ( 
os . path . realpath ( __file__ ) ) ) + '/headers/' ) , 
'-o' , out_filename ] + 
compiler_args ) 
~~ if compiler == 'icc' and assembly : 
~~~ with open ( out_filename , 'r+' ) as f : 
~~~ assembly = f . read ( ) 
f . seek ( 0 ) 
f . write ( assembly . replace ( 'vkmovb' , 'kmovb' ) ) 
f . truncate ( ) 
~~ ~~ return out_filename 
~~ def iaca_analysis ( self , micro_architecture , asm_block = 'auto' , 
pointer_increment = 'auto_with_manual_fallback' , verbose = False ) : 
asm_filename = self . compile_kernel ( assembly = True , verbose = verbose ) 
asm_marked_filename = os . path . splitext ( asm_filename ) [ 0 ] + '-iaca.s' 
with open ( asm_filename , 'r' ) as in_file , open ( asm_marked_filename , 'w' ) as out_file : 
~~~ self . asm_block = iaca . iaca_instrumentation ( 
in_file , out_file , 
block_selection = asm_block , 
pointer_increment = pointer_increment ) 
~~ obj_name = self . assemble_to_object ( asm_marked_filename , verbose = verbose ) 
return iaca . iaca_analyse_instrumented_binary ( obj_name , micro_architecture ) , self . asm_block 
~~ def build_executable ( self , lflags = None , verbose = False , openmp = False ) : 
kernel_obj_filename = self . compile_kernel ( openmp = openmp , verbose = verbose ) 
out_filename , already_exists = self . _get_intermediate_file ( 
os . path . splitext ( os . path . basename ( kernel_obj_filename ) ) [ 0 ] , binary = True , fp = False ) 
if not already_exists : 
~~~ main_source_filename = self . get_main_code ( as_filename = True ) 
if not ( ( 'LIKWID_INCLUDE' in os . environ or 'LIKWID_INC' in os . environ ) and 
'LIKWID_LIB' in os . environ ) : 
~~~ print ( \ 
\ , 
file = sys . stderr ) 
~~ compiler_args += [ 
'-std=c99' , 
os . environ . get ( 'LIKWID_INCLUDE' , '' ) , 
os . environ . get ( 'LIKWID_INC' , '' ) , 
'-llikwid' ] 
if os . environ . get ( 'LIKWID_LIB' ) == '' : 
~~~ compiler_args = compiler_args [ : - 1 ] 
~~ if lflags is None : 
~~~ lflags = [ ] 
infiles = [ reduce_path ( os . path . abspath ( os . path . dirname ( 
os . path . realpath ( __file__ ) ) ) + '/headers/dummy.c' ) , 
kernel_obj_filename , main_source_filename ] 
cmd = [ compiler ] + infiles + compiler_args + [ '-o' , out_filename ] 
cmd = list ( filter ( bool , cmd ) ) 
~~ def string_to_sympy ( cls , s ) : 
if isinstance ( s , int ) : 
~~~ return sympy . Integer ( s ) 
~~ elif isinstance ( s , list ) : 
~~~ return tuple ( [ cls . string_to_sympy ( e ) for e in s ] ) 
~~ elif s is None : 
~~~ local_dict = { c : symbol_pos_int ( c ) for c in s if c in string . ascii_letters } 
preliminary_expr = parse_expr ( s , local_dict = local_dict ) 
local_dict . update ( 
{ s . name : symbol_pos_int ( s . name ) for s in preliminary_expr . free_symbols } ) 
return parse_expr ( s , local_dict = local_dict ) 
~~ ~~ def get_identifier ( self ) : 
if self . _path : 
~~~ return os . path . basename ( self . _path ) 
~~~ return hashlib . sha256 ( hashlib . sha256 ( repr ( self . _data ) . encode ( ) ) ) . hexdigest ( ) 
~~ ~~ def get_last_modified_datetime ( self ) : 
~~~ statbuf = os . stat ( self . _path ) 
return datetime . utcfromtimestamp ( statbuf . st_mtime ) 
~~~ return datetime . now ( ) 
~~ ~~ def get_cachesim ( self , cores = 1 ) : 
cache_dict = { } 
~~~ cache_dict [ c [ 'level' ] ] [ 'sets' ] //= cores 
~~ ~~ cs , caches , mem = cachesim . CacheSimulator . from_dict ( cache_dict ) 
return cs 
~~ def get_bandwidth ( self , cache_level , read_streams , write_streams , threads_per_core , cores = None ) : 
~~~ target_ratio = read_streams / write_streams 
~~ except ZeroDivisionError : 
~~~ target_ratio = float ( 'inf' ) 
~~ measurement_kernel = 'load' 
measurement_kernel_info = self [ 'benchmarks' ] [ 'kernels' ] [ measurement_kernel ] 
measurement_kernel_ratio = float ( 'inf' ) 
for kernel_name , kernel_info in sorted ( self [ 'benchmarks' ] [ 'kernels' ] . items ( ) ) : 
~~~ kernel_ratio = float ( 'inf' ) 
~~ if abs ( kernel_ratio - target_ratio ) < abs ( measurement_kernel_ratio - target_ratio ) : 
~~~ measurement_kernel = kernel_name 
measurement_kernel_info = kernel_info 
measurement_kernel_ratio = kernel_ratio 
bw_measurements = self [ 'benchmarks' ] [ 'measurements' ] [ bw_level ] [ threads_per_core ] 
if cores is not None : 
~~~ run_index = bw_measurements [ 'cores' ] . index ( cores ) 
bw = bw_measurements [ 'results' ] [ measurement_kernel ] [ run_index ] 
bw = max ( bw_measurements [ 'results' ] [ measurement_kernel ] [ : max_cores ] ) 
~~ if cache_level == 0 : 
~~~ factor = 1.0 
~~ bw = bw * factor 
return bw , measurement_kernel 
~~ def get_compiler ( self , compiler = None , flags = None ) : 
if self . _args : 
~~~ compiler = compiler or self . _args . compiler 
flags = flags or self . _args . compiler_flags 
~~ if compiler is None : 
~~~ for c in self [ 'compiler' ] . keys ( ) : 
~~~ if find_executable ( c ) is not None : 
~~~ compiler = c 
"$PATH." . format ( list ( self [ 'compiler' ] . keys ( ) ) ) ) 
~~ ~~ if flags is None : 
~~~ flags = self [ 'compiler' ] . get ( compiler , '' ) 
~~ def parse_perfctr_event ( perfctr ) : 
split_perfctr = perfctr . split ( ':' ) 
event_tuple = split_perfctr [ : 2 ] 
parameters = { } 
for p in split_perfctr [ 2 : ] : 
~~~ if '=' in p : 
~~~ k , v = p . split ( '=' ) 
if v . startswith ( '0x' ) : 
~~~ parameters [ k ] = int ( v , 16 ) 
~~~ parameters [ k ] = int ( v ) 
~~~ parameters [ p ] = None 
~~ ~~ event_tuple . append ( parameters ) 
return tuple ( event_tuple ) 
~~ def parse_perfmetric ( metric ) : 
perfcounters = re . findall ( r'[A-Z0-9_]+:[A-Z0-9\\[\\]|\\-]+(?::[A-Za-z0-9\\-_=]+)*' , metric ) 
temp_metric = metric 
temp_pc_names = { "SYM{}" . format ( re . sub ( "[\\[\\]\\-|=:]" , "_" , pc ) ) : pc 
for i , pc in enumerate ( perfcounters ) } 
for var_name , pc in temp_pc_names . items ( ) : 
~~~ temp_metric = temp_metric . replace ( pc , var_name ) 
~~ expr = parse_expr ( temp_metric ) 
for s in expr . free_symbols : 
~~~ if s . name in temp_pc_names : 
~~~ s . name = temp_pc_names [ str ( s ) ] 
~~ ~~ events = { s : MachineModel . parse_perfctr_event ( s . name ) for s in expr . free_symbols 
if s . name in perfcounters } 
return expr , events 
~~ def _enforce_no_overlap ( self , start_at = 0 ) : 
i = start_at 
while i + 1 < len ( self . data ) : 
~~~ if self . data [ i ] [ 1 ] >= self . data [ i + 1 ] [ 0 ] : 
~~~ if self . data [ i ] [ 1 ] < self . data [ i + 1 ] [ 1 ] : 
~~~ self . data [ i ] [ 1 ] = self . data [ i + 1 ] [ 1 ] 
~~ del self . data [ i + 1 ] 
~~ ~~ def get_header_path ( ) -> str : 
import os 
return os . path . abspath ( os . path . dirname ( os . path . realpath ( __file__ ) ) ) + '/headers/' 
~~ def _align_iteration_with_cl_boundary ( self , iteration , subtract = True ) : 
inner_loop = list ( self . kernel . get_loop_stack ( subs_consts = True ) ) [ - 1 ] 
inner_increment = inner_loop [ 'increment' ] 
o = self . kernel . compile_global_offsets ( iteration = iteration ) [ 0 ] 
if len ( o [ 1 ] ) : 
~~~ first_offset = min ( o [ 1 ] ) 
~~~ first_offset = min ( o [ 0 ] ) 
~~ diff = first_offset - ( int ( first_offset ) >> self . csim . first_level . cl_bits << self . csim . first_level . cl_bits ) 
if diff == 0 : 
~~~ return iteration 
~~ elif subtract : 
~~~ return iteration - ( diff // element_size ) // inner_increment 
~~~ return iteration + ( elements_per_cacheline - diff // element_size ) // inner_increment 
~~ ~~ def get_loads ( self ) : 
return [ self . stats [ cache_level ] [ 'LOAD_count' ] / self . first_dim_factor 
~~ def get_hits ( self ) : 
return [ self . stats [ cache_level ] [ 'HIT_count' ] / self . first_dim_factor 
~~ def get_misses ( self ) : 
return [ self . stats [ cache_level ] [ 'MISS_count' ] / self . first_dim_factor 
~~ def get_stores ( self ) : 
return [ self . stats [ cache_level ] [ 'STORE_count' ] / self . first_dim_factor 
~~ def get_evicts ( self ) : 
return [ self . stats [ cache_level ] [ 'EVICT_count' ] / self . first_dim_factor 
~~ def get_infos ( self ) : 
first_dim_factor = self . first_dim_factor 
'level' : '{}' . format ( cache_info [ 'level' ] ) , 
'cycles' : None } ) 
~~ return infos 
~~ def measure_bw ( type_ , total_size , threads_per_core , max_threads_per_core , cores_per_socket , 
sockets ) : 
groups = [ ] 
for s in range ( sockets ) : 
~~~ groups += [ 
'-w' , 
'S' + str ( s ) + ':' + str ( total_size ) + 'kB:' + 
str ( threads_per_core * cores_per_socket ) + 
':1:' + str ( int ( max_threads_per_core / threads_per_core ) ) ] 
~~ cmd = [ 'likwid-bench' , '-t' , type_ ] + groups 
output = subprocess . Popen ( cmd , stdout = subprocess . PIPE ) . communicate ( ) [ 0 ] . decode ( 'utf-8' ) 
if not output : 
~~ bw = float ( get_match_or_break ( r'^MByte/s:\\s+([0-9]+(?:\\.[0-9]+)?)\\s*$' , output ) [ 0 ] ) 
return PrefixedUnit ( bw , 'MB/s' ) 
~~ def fix_env_variable ( name , value ) : 
orig = os . environ . get ( name , None ) 
if value is not None : 
~~~ os . environ [ name ] = value 
~~ elif name in os . environ : 
~~~ del os . environ [ name ] 
~~~ yield 
~~ finally : 
~~~ if orig is not None : 
~~~ os . environ [ name ] = orig 
~~ ~~ ~~ def configure_arggroup ( cls , parser ) : 
parser . add_argument ( 
'--no-phenoecm' , action = 'store_true' , 
'--iterations' , type = int , default = 10 , 
'--ignore-warnings' , action = 'store_true' , 
~~ def perfctr ( self , cmd , group = 'MEM' , code_markers = True ) : 
if find_executable ( 'likwid-perfctr' ) is None : 
~~ perf_cmd = [ 'likwid-perfctr' , '-f' , '-O' , '-g' , group ] 
cpu = 'S0:0' 
~~~ cpu += '-' + str ( self . _args . cores - 1 ) 
~~ perf_cmd += [ '-C' , cpu ] 
perf_cmd . append ( '-m' ) 
perf_cmd += cmd 
~~~ with fix_env_variable ( 'OMP_NUM_THREADS' , None ) : 
~~~ output = subprocess . check_output ( perf_cmd ) . decode ( 'utf-8' ) . split ( '\\n' ) 
~~ ~~ except subprocess . CalledProcessError as e : 
~~ results = { } 
for line in output : 
~~~ line = line . split ( ',' ) 
~~~ results [ line [ 0 ] ] = float ( line [ 1 ] ) 
~~ except IndexError : 
~~~ if line [ 2 ] == '-' or line [ 2 ] == 'nan' : 
~~~ counter_value = 0 
~~~ counter_value = int ( line [ 2 ] ) 
~~ if re . fullmatch ( r'[A-Z0-9_]+' , line [ 0 ] ) and re . fullmatch ( r'[A-Z0-9]+' , line [ 1 ] ) : 
~~~ results . setdefault ( line [ 0 ] , { } ) 
results [ line [ 0 ] ] [ line [ 1 ] ] = counter_value 
~~ ~~ except ( IndexError , ValueError ) : 
bench = self . kernel . build_executable ( verbose = self . verbose > 1 , openmp = self . _args . cores > 1 ) 
args = [ str ( s ) for s in list ( self . kernel . constants . values ( ) ) ] 
runtime = 0.0 
time_per_repetition = 2.0 / 10.0 
repetitions = self . iterations // 10 
mem_results = { } 
while runtime < 1.5 : 
~~~ if time_per_repetition != 0.0 : 
~~~ repetitions = 2.0 // time_per_repetition 
~~~ repetitions = int ( repetitions * 10 ) 
~~ mem_results = self . perfctr ( [ bench ] + [ str ( repetitions ) ] + args , group = "MEM" ) 
time_per_repetition = runtime / float ( repetitions ) 
~~ raw_results = [ mem_results ] 
iterations_per_repetition = reduce ( 
operator . mul , 
[ self . kernel . subs_consts ( max_ - min_ ) / self . kernel . subs_consts ( step ) 
for idx , min_ , max_ , step in self . kernel . _loop_stack ] , 
1 ) 
self . kernel . bytes_per_iteration ) 
cys_per_repetition = time_per_repetition * float ( self . machine [ 'clock' ] ) 
if not self . no_phenoecm : 
~~~ T_OL , event_counters = self . machine . parse_perfmetric ( 
T_data , event_dict = self . machine . parse_perfmetric ( 
event_counters . update ( event_dict ) 
cache_metrics = defaultdict ( dict ) 
name = cache_info [ 'level' ] 
~~~ cache_metrics [ name ] [ k ] , event_dict = self . machine . parse_perfmetric ( v ) 
~~ ~~ minimal_runs = build_minimal_runs ( list ( event_counters . values ( ) ) ) 
measured_ctrs = { } 
for run in minimal_runs : 
~~~ ctrs = ',' . join ( [ eventstr ( e ) for e in run ] ) 
r = self . perfctr ( [ bench ] + [ str ( repetitions ) ] + args , group = ctrs ) 
raw_results . append ( r ) 
measured_ctrs . update ( r ) 
~~ event_counter_results = { } 
for sym , ctr in event_counters . items ( ) : 
~~~ event , regs , parameter = ctr [ 0 ] , register_options ( ctr [ 1 ] ) , ctr [ 2 ] 
for r in regs : 
~~~ if r in measured_ctrs [ event ] : 
~~~ event_counter_results [ sym ] = measured_ctrs [ event ] [ r ] 
total_iterations = iterations_per_repetition * repetitions 
total_cachelines = total_iterations / iterations_per_cacheline 
T_OL_result = T_OL . subs ( event_counter_results ) / total_cachelines 
cache_metric_results = defaultdict ( dict ) 
for cache , mtrcs in cache_metrics . items ( ) : 
~~~ for m , e in mtrcs . items ( ) : 
~~~ cache_metric_results [ cache ] [ m ] = e . subs ( event_counter_results ) 
~~ ~~ cache_transfers_per_cl = { cache : { k : PrefixedUnit ( v / total_cachelines , 'CL/CL' ) 
for k , v in d . items ( ) } 
for cache , d in cache_metric_results . items ( ) } 
cache_transfers_per_cl [ 'L1' ] [ 'accesses' ] . unit = 'LOAD/CL' 
mem_bw , mem_bw_kernel = self . machine . get_bandwidth ( 
data_transfers = { 
'T_nOL' : ( cache_metric_results [ 'L1' ] [ 'accesses' ] / total_cachelines * 0.5 ) , 
'T_L1L2' : ( ( cache_metric_results [ 'L1' ] [ 'misses' ] + 
cache_metric_results [ 'L1' ] [ 'evicts' ] ) / 
total_cachelines * cl_size / 
'T_L2L3' : ( ( cache_metric_results [ 'L2' ] [ 'misses' ] + 
cache_metric_results [ 'L2' ] [ 'evicts' ] ) / 
'T_L3MEM' : ( ( cache_metric_results [ 'L3' ] [ 'misses' ] + 
cache_metric_results [ 'L3' ] [ 'evicts' ] ) * 
total_cachelines / mem_bw * 
float ( self . machine [ 'clock' ] ) ) 
ecm_model = { 'T_OL' : T_OL_result } 
ecm_model . update ( data_transfers ) 
~~~ event_counters = { } 
ecm_model = None 
cache_transfers_per_cl = None 
time_per_repetition / iterations_per_repetition ) / 1e6 
iterations_per_repetition / time_per_repetition ) / 1e6 
~~~ with pprint_nosort ( ) : 
~~~ pprint . pprint ( self . results ) 
~~ ~~ if self . verbose > 0 : 
~~ if self . verbose > 0 : 
if self . verbose > 0 : 
~~~ for metric_name in sorted ( metrics ) : 
~~ print ( ) 
for k , v in sorted ( metrics . items ( ) ) : 
** { k : float ( v ) for k , v in self . results [ 'ECM' ] . items ( ) } ) , 
~~ ~~ def parse_description ( ) : 
from os . path import dirname , join , exists 
readme_fpath = join ( dirname ( __file__ ) , 'README.md' ) 
if exists ( readme_fpath ) : 
~~~ textlines = [ ] 
with open ( readme_fpath , 'r' ) as f : 
~~~ capture = False 
for line in f . readlines ( ) : 
~~~ capture = True 
~~ elif line . startswith ( '##' ) : 
~~ elif capture : 
~~~ textlines += [ line ] 
~~ ~~ ~~ text = '' . join ( textlines ) . strip ( ) 
text = text . replace ( '\\n\\n' , '_NLHACK_' ) 
text = text . replace ( '_NLHACK_' , '\\n\\n' ) 
return text 
~~ return '' 
~~ def schedule_retry ( self , config ) : 
raise self . retry ( countdown = config . get ( 'SAILTHRU_RETRY_SECONDS' ) , 
max_retries = config . get ( 'SAILTHRU_RETRY_ATTEMPTS' ) ) 
~~ def _build_purchase_item ( course_id , course_url , cost_in_cents , mode , course_data , sku ) : 
item = { 
'id' : "{}-{}" . format ( course_id , mode ) , 
'url' : course_url , 
'price' : cost_in_cents , 
'qty' : 1 , 
if 'title' in course_data : 
~~~ item [ 'title' ] = course_data [ 'title' ] 
~~ if 'tags' in course_data : 
~~~ item [ 'tags' ] = course_data [ 'tags' ] 
~~ item [ 'vars' ] = dict ( course_data . get ( 'vars' , { } ) , mode = mode , course_run_id = course_id ) 
item [ 'vars' ] [ 'purchase_sku' ] = sku 
return item 
~~ def _record_purchase ( sailthru_client , email , item , purchase_incomplete , message_id , options ) : 
~~~ sailthru_response = sailthru_client . purchase ( email , [ item ] , 
incomplete = purchase_incomplete , message_id = message_id , 
options = options ) 
if not sailthru_response . is_ok ( ) : 
~~~ error = sailthru_response . get_error ( ) 
return not can_retry_sailthru_request ( error ) 
~~ ~~ except SailthruClientError as exc : 
~~ def _get_course_content ( course_id , course_url , sailthru_client , site_code , config ) : 
cache_key = "{}:{}" . format ( site_code , course_url ) 
response = cache . get ( cache_key ) 
if not response : 
~~~ sailthru_response = sailthru_client . api_get ( "content" , { "id" : course_url } ) 
~~~ response = { } 
~~~ response = sailthru_response . json 
cache . set ( cache_key , response , config . get ( 'SAILTHRU_CACHE_TTL_SECONDS' ) ) 
~~ ~~ except SailthruClientError : 
~~ if not response : 
response = _get_course_content_from_ecommerce ( course_id , site_code = site_code ) 
if response : 
~~~ cache . set ( cache_key , response , config . get ( 'SAILTHRU_CACHE_TTL_SECONDS' ) ) 
~~ ~~ ~~ return response 
~~ def _get_course_content_from_ecommerce ( course_id , site_code = None ) : 
api = get_ecommerce_client ( site_code = site_code ) 
~~~ api_response = api . courses ( course_id ) . get ( ) 
~~~ logger . exception ( 
course_id , 
exc_info = True 
return { } 
'title' : api_response . get ( 'name' ) , 
'verification_deadline' : api_response . get ( 'verification_deadline' ) 
~~ def _update_unenrolled_list ( sailthru_client , email , course_url , unenroll ) : 
~~~ sailthru_response = sailthru_client . api_get ( "user" , { "id" : email , "fields" : { "vars" : 1 } } ) 
~~ response_json = sailthru_response . json 
unenroll_list = [ ] 
if response_json and "vars" in response_json and response_json [ "vars" ] and "unenrolled" in response_json [ "vars" ] : 
~~~ unenroll_list = response_json [ "vars" ] [ "unenrolled" ] 
~~ changed = False 
if unenroll : 
~~~ if course_url not in unenroll_list : 
~~~ unenroll_list . append ( course_url ) 
changed = True 
~~ ~~ elif course_url in unenroll_list : 
~~~ unenroll_list . remove ( course_url ) 
~~ if changed : 
~~~ sailthru_response = sailthru_client . api_post ( 
'user' , { 'id' : email , 'key' : 'email' , 'vars' : { 'unenrolled' : unenroll_list } } ) 
~~ ~~ return True 
~~ except SailthruClientError as exc : 
~~ ~~ def update_course_enrollment ( self , email , course_url , purchase_incomplete , mode , unit_cost = None , course_id = None , 
currency = None , message_id = None , site_code = None , sku = None ) : 
config = get_sailthru_configuration ( site_code ) 
~~~ sailthru_client = get_sailthru_client ( site_code ) 
~~ except SailthruError : 
~~ new_enroll = False 
send_template = None 
if not purchase_incomplete : 
~~~ if mode == 'verified' : 
~~~ send_template = config . get ( 'SAILTHRU_UPGRADE_TEMPLATE' ) 
~~ elif mode == 'audit' or mode == 'honor' : 
~~~ new_enroll = True 
send_template = config . get ( 'SAILTHRU_ENROLL_TEMPLATE' ) 
send_template = config . get ( 'SAILTHRU_PURCHASE_TEMPLATE' ) 
~~ ~~ cost_in_cents = int ( unit_cost * 100 ) 
if new_enroll : 
~~~ if not _update_unenrolled_list ( sailthru_client , email , course_url , False ) : 
~~~ schedule_retry ( self , config ) 
~~ ~~ course_data = _get_course_content ( course_id , course_url , sailthru_client , site_code , config ) 
item = _build_purchase_item ( course_id , course_url , cost_in_cents , mode , course_data , sku ) 
options = { } 
if purchase_incomplete and config . get ( 'SAILTHRU_ABANDONED_CART_TEMPLATE' ) : 
~~~ options [ 'reminder_template' ] = config . get ( 'SAILTHRU_ABANDONED_CART_TEMPLATE' ) 
~~ if send_template : 
~~~ options [ 'send_template' ] = send_template 
~~ if not _record_purchase ( sailthru_client , email , item , purchase_incomplete , message_id , options ) : 
~~ ~~ def send_course_refund_email ( self , email , refund_id , amount , course_name , order_number , order_url , site_code = None ) : 
~~ email_vars = { 
'amount' : amount , 
'course_name' : course_name , 
'order_number' : order_number , 
'order_url' : order_url , 
~~~ response = sailthru_client . send ( 
template = config [ 'templates' ] [ 'course_refund' ] , 
email = email , 
_vars = email_vars 
~~ except SailthruClientError : 
refund_id 
~~ if response . is_ok ( ) : 
~~~ error = response . get_error ( ) 
logger . error ( 
refund_id , error . get_error_code ( ) , error . get_message ( ) 
if can_retry_sailthru_request ( error ) : 
~~~ logger . info ( 
schedule_retry ( self , config ) 
~~~ logger . warning ( 
~~ ~~ ~~ def send_offer_assignment_email ( self , user_email , offer_assignment_id , subject , email_body , site_code = None ) : 
response = _send_offer_assignment_notification_email ( config , user_email , subject , email_body , site_code , self ) 
if response and response . is_ok ( ) : 
if _update_assignment_email_status ( offer_assignment_id , send_id , 'success' ) : 
message = email_body ) ) 
token_offer = offer_assignment_id , 
token_email = user_email , 
~~ ~~ ~~ def _send_offer_assignment_notification_email ( config , user_email , subject , email_body , site_code , task ) : 
'subject' : subject , 
'email_body' : email_body , 
template = config [ 'templates' ] [ 'assignment_email' ] , 
email = user_email , 
~~ if not response . is_ok ( ) : 
message = email_body , 
token_error_code = error . get_error_code ( ) , 
token_error_message = error . get_message ( ) 
schedule_retry ( task , config ) 
~~ ~~ return response 
~~ def _update_assignment_email_status ( offer_assignment_id , send_id , status , site_code = None ) : 
api = get_ecommerce_client ( url_postfix = 'assignment-email/' , site_code = site_code ) 
post_data = { 
'offer_assignment_id' : offer_assignment_id , 
'send_id' : send_id , 
'status' : status , 
~~~ api_response = api . status ( ) . post ( post_data ) 
~~ except RequestException : 
token_send_id = send_id 
~~ return True if api_response . get ( 'status' ) == 'updated' else False 
~~ def send_offer_update_email ( self , user_email , subject , email_body , site_code = None ) : 
_send_offer_assignment_notification_email ( config , user_email , subject , email_body , site_code , self ) 
~~ def get_logger_config ( log_dir = '/var/tmp' , 
logging_env = 'no_env' , 
edx_filename = 'edx.log' , 
dev_env = False , 
debug = False , 
local_loglevel = 'INFO' , 
service_variant = 'ecomworker' ) : 
if local_loglevel not in [ 'DEBUG' , 'INFO' , 'WARNING' , 'ERROR' , 'CRITICAL' ] : 
~~~ local_loglevel = 'INFO' 
~~ hostname = platform . node ( ) . split ( '.' ) [ 0 ] 
syslog_format = ( 
'[service_variant={service_variant}]' 
) . format ( 
service_variant = service_variant , 
logging_env = logging_env , hostname = hostname 
~~~ handlers = [ 'console' ] 
~~~ handlers = [ 'local' ] 
~~ logger_config = { 
'version' : 1 , 
'disable_existing_loggers' : False , 
'formatters' : { 
'standard' : { 
'syslog_format' : { 'format' : syslog_format } , 
'raw' : { 'format' : '%(message)s' } , 
'handlers' : { 
'console' : { 
'level' : 'DEBUG' if debug else 'INFO' , 
'class' : 'logging.StreamHandler' , 
'formatter' : 'standard' , 
'stream' : sys . stdout , 
'loggers' : { 
'requests' : { 
'handlers' : handlers , 
'level' : 'WARNING' , 
'propagate' : True 
'' : { 
'level' : 'DEBUG' , 
'propagate' : False 
if dev_env : 
~~~ edx_file_loc = os . path . join ( log_dir , edx_filename ) 
logger_config [ 'handlers' ] . update ( { 
'local' : { 
'class' : 'logging.handlers.RotatingFileHandler' , 
'level' : local_loglevel , 
'filename' : edx_file_loc , 
'maxBytes' : 1024 * 1024 * 2 , 
'backupCount' : 5 , 
~~~ logger_config [ 'handlers' ] . update ( { 
'class' : 'logging.handlers.SysLogHandler' , 
'address' : '/var/run/syslog' if sys . platform == 'darwin' else '/dev/log' , 
'formatter' : 'syslog_format' , 
'facility' : SysLogHandler . LOG_LOCAL0 , 
~~ return logger_config 
~~ def _retry_order ( self , exception , max_fulfillment_retries , order_number ) : 
retries = self . request . retries 
if retries == max_fulfillment_retries : 
~~ countdown = 2 ** retries 
raise self . retry ( exc = exception , countdown = countdown , max_retries = max_fulfillment_retries ) 
~~ def fulfill_order ( self , order_number , site_code = None , email_opt_in = False ) : 
max_fulfillment_retries = get_configuration ( 'MAX_FULFILLMENT_RETRIES' , site_code = site_code ) 
api . orders ( order_number ) . fulfill . put ( email_opt_in = email_opt_in ) 
~~ except exceptions . HttpClientError as exc : 
if status_code == 406 : 
raise Ignore ( ) 
order_number , 
_retry_order ( self , exc , max_fulfillment_retries , order_number ) 
~~ ~~ except ( exceptions . HttpServerError , exceptions . Timeout , SSLError ) as exc : 
~~~ _retry_order ( self , exc , max_fulfillment_retries , order_number ) 
~~ ~~ def get_sailthru_client ( site_code ) : 
if not config . get ( 'SAILTHRU_ENABLE' ) : 
log . debug ( msg ) 
raise SailthruNotEnabled ( msg ) 
~~ key = config . get ( 'SAILTHRU_KEY' ) 
secret = config . get ( 'SAILTHRU_SECRET' ) 
if not ( key and secret ) : 
log . error ( msg ) 
raise ConfigurationError ( msg ) 
~~ return SailthruClient ( key , secret ) 
~~ def get ( self , key ) : 
lock . acquire ( ) 
~~~ if key not in self : 
~~ current_time = time . time ( ) 
if self [ key ] . expire > current_time : 
~~~ return self [ key ] . value 
~~ deletes = [ ] 
for k , val in self . items ( ) : 
~~~ if val . expire <= current_time : 
~~~ deletes . append ( k ) 
~~ ~~ for k in deletes : 
~~~ del self [ k ] 
~~ return None 
~~~ lock . release ( ) 
~~ ~~ def set ( self , key , value , duration ) : 
~~~ self [ key ] = CacheObject ( value , duration ) 
~~ ~~ def get_configuration ( variable , site_code = None ) : 
name = os . environ . get ( CONFIGURATION_MODULE ) 
__import__ ( name ) 
module = sys . modules [ name ] 
setting_value = getattr ( module , variable , None ) 
site_overrides = getattr ( module , 'SITE_OVERRIDES' , None ) 
if site_overrides and site_code is not None : 
~~~ site_specific_overrides = site_overrides . get ( site_code ) 
if site_specific_overrides : 
~~~ override_value = site_specific_overrides . get ( variable ) 
if override_value : 
~~~ setting_value = override_value 
~~ ~~ ~~ if setting_value is None : 
~~ return setting_value 
~~ def get_ecommerce_client ( url_postfix = '' , site_code = None ) : 
ecommerce_api_root = get_configuration ( 'ECOMMERCE_API_ROOT' , site_code = site_code ) 
signing_key = get_configuration ( 'JWT_SECRET_KEY' , site_code = site_code ) 
issuer = get_configuration ( 'JWT_ISSUER' , site_code = site_code ) 
service_username = get_configuration ( 'ECOMMERCE_SERVICE_USERNAME' , site_code = site_code ) 
return EdxRestApiClient ( 
ecommerce_api_root + url_postfix , signing_key = signing_key , issuer = issuer , username = service_username ) 
~~ def get_overrides_filename ( variable ) : 
filename = os . environ . get ( variable ) 
raise EnvironmentError ( msg ) 
~~ return filename 
~~ def get_output_files_layout ( output_category ) : 
if output_category not in ( "inputs" , "table" , "other" ) : 
~~ layouts = _layouts_matrix [ OS_NAME ] [ output_category ] 
return get_value_by_version ( layouts ) 
~~ def get_value_by_version ( d ) : 
cv = CONF . eplus_version [ : 2 ] 
for v , value in sorted ( d . items ( ) , reverse = True ) : 
~~~ if cv >= v : 
~~~ return value 
~~ ~~ ~~ def switch_to_datetime_instants ( df , start_year , eplus_frequency ) : 
if eplus_frequency in ( TIMESTEP , DAILY , HOURLY , MONTHLY ) : 
~~~ if eplus_frequency in ( TIMESTEP , HOURLY , DAILY ) : 
~~~ year_counter = ( 
( df [ [ "month" , "day" ] ] - df [ [ "month" , "day" ] ] . shift ( ) ) == 
pd . Series ( dict ( month = 12 , day = - 31 ) ) 
) . all ( axis = 1 ) . cumsum ( ) 
~~~ year_counter = ( ( df [ "month" ] - df [ "month" ] . shift ( ) ) == - 12 ) . cumsum ( ) 
~~ df [ "year" ] = year_counter + start_year 
columns = { 
TIMESTEP : ( "year" , "month" , "day" , "hour" , "minute" ) , 
HOURLY : ( "year" , "month" , "day" , "hour" ) , 
DAILY : ( "year" , "month" , "day" ) , 
MONTHLY : ( "year" , "month" ) 
} [ eplus_frequency ] 
if eplus_frequency == MONTHLY : 
lambda x : dt . datetime ( * ( tuple ( int ( x [ k ] ) for k in columns ) + ( 1 , ) ) ) , 
axis = 1 
~~~ df . index = df . apply ( lambda x : dt . datetime ( * ( int ( x [ k ] ) for k in columns ) ) , axis = 1 ) 
~~ df . drop ( columns = list ( columns ) , inplace = True ) 
if eplus_frequency == TIMESTEP : 
~~~ ts = df . index [ 1 ] - df . index [ 0 ] 
forced_df = df . asfreq ( ts ) 
~~~ forced_df = df . asfreq ( { 
HOURLY : "H" , 
DAILY : "D" , 
MONTHLY : "MS" 
} [ eplus_frequency ] ) 
~~ if eplus_frequency in ( TIMESTEP , HOURLY , DAILY ) : 
~~~ assert_index_equal ( df . index , forced_df . index ) 
~~~ raise ValueError ( 
f"Couldn\ 
) from None 
~~ ~~ return forced_df 
~~ if eplus_frequency == ANNUAL : 
~~~ if df [ "year" ] . iloc [ 0 ] != start_year : 
f"can\ ) 
~~ df . index = df [ "year" ] . map ( lambda x : dt . datetime ( x , 1 , 1 ) ) 
del df [ "year" ] 
df = df . asfreq ( "YS" ) 
return df 
~~ if eplus_frequency == RUN_PERIOD : 
~~~ return df 
~~ def eplus_version ( self ) : 
if len ( self . eplus_available_versions ) == 0 : 
~~ if self . _eplus_version is not None : 
~~~ return self . _eplus_version 
~~ return sorted ( self . eplus_available_versions . keys ( ) , reverse = True ) [ 0 ] 
~~ def _check_and_sanitize_datetime_instants ( df ) : 
if df is None or len ( df ) == 0 : 
~~ if not isinstance ( df . index , pd . DatetimeIndex ) : 
~~ if df . index . freq != "H" : 
~~~ forced_df = df . asfreq ( "H" ) 
~~ df = forced_df 
~~ if df . index [ 0 ] . minute != 0 : 
~~ return df 
~~ def get_bounds ( self ) : 
start , end = None , None 
if len ( self . _weather_series ) == 0 : 
~~~ return start , end 
~~ for i in ( 0 , - 1 ) : 
~~~ if self . has_tuple_instants : 
~~~ row = self . _weather_series . iloc [ i , : ] 
instant = dt . datetime ( row [ "year" ] , row [ "month" ] , row [ "day" ] , row [ "hour" ] , row [ "minute" ] ) 
~~~ instant = self . _weather_series . index [ i ] . to_pydatetime ( ) 
~~ if i == 0 : 
~~~ start = instant 
~~~ end = instant 
~~ ~~ return start , end 
~~ def from_epw ( cls , buffer_or_path ) : 
from . epw_parse import parse_epw 
_ , buffer = to_buffer ( buffer_or_path ) 
with buffer as f : 
~~~ return parse_epw ( f ) 
~~ ~~ def to_epw ( self , buffer_or_path = None ) : 
df = self . _weather_series . copy ( ) 
df [ "hour" ] += 1 
epw_content = self . _headers_to_epw ( ) + df . to_csv ( header = False , index = False , line_terminator = "\\n" ) 
return multi_mode_write ( 
lambda buffer : buffer . write ( epw_content ) , 
lambda : epw_content , 
buffer_or_path = buffer_or_path 
~~ def parse_idf ( file_like ) : 
tables_data = { } 
head_comment = "" 
record_data = None 
make_new_record = True 
copyright_list = get_multi_line_copyright_message ( ) . split ( "\\n" ) 
for i , raw_line in enumerate ( file_like ) : 
~~~ copyright_line = copyright_list [ i ] 
if raw_line . strip ( ) == copyright_line : 
~~ ~~ except IndexError : 
~~ split_line = raw_line . split ( "!" ) 
if len ( split_line ) == 1 : 
~~~ if len ( split_line [ 0 ] . strip ( ) ) == 0 : 
~~~ content , comment = None , None 
~~~ content , comment = split_line [ 0 ] . strip ( ) , None 
~~~ content , comment = None , "!" . join ( split_line [ 1 : ] ) 
~~~ content , comment = split_line [ 0 ] . strip ( ) , "!" . join ( split_line [ 1 : ] ) 
~~ ~~ if ( content , comment ) == ( None , None ) : 
~~ if not content : 
~~~ head_comment += comment . strip ( ) + "\\n" 
~~ record_end = content [ - 1 ] == ";" 
content_l = [ text . strip ( ) for text in content . split ( "," ) ] 
if make_new_record : 
~~~ table_ref = table_name_to_ref ( content_l [ 0 ] . strip ( ) ) 
if table_ref . lower ( ) in ( 
) : 
~~ if table_ref not in tables_data : 
~~~ tables_data [ table_ref ] = [ ] 
~~ record_data = dict ( ) 
tables_data [ table_ref ] . append ( record_data ) 
content_l = content_l [ 1 : ] 
make_new_record = False 
~~ for value_s in content_l : 
~~~ field_index = len ( record_data ) 
record_data [ field_index ] = value_s 
~~ if record_end : 
~~~ make_new_record = True 
~~ ~~ tables_data [ "_comment" ] = head_comment 
return tables_data 
~~ def run_eplus ( epm_or_idf_path , weather_data_or_epw_path , simulation_dir_path , stdout = None , stderr = None , beat_freq = None ) : 
simulation_dir_path = os . path . abspath ( simulation_dir_path ) 
if not os . path . isdir ( simulation_dir_path ) : 
~~ if not isinstance ( epm_or_idf_path , Epm ) : 
~~~ epm = Epm . from_idf ( epm_or_idf_path ) 
~~~ epm = epm_or_idf_path 
~~ simulation_idf_path = os . path . join ( simulation_dir_path , CONF . default_model_name + ".idf" ) 
epm . to_idf ( simulation_idf_path ) 
simulation_epw_path = os . path . join ( simulation_dir_path , CONF . default_model_name + ".epw" ) 
if isinstance ( weather_data_or_epw_path , WeatherData ) : 
~~~ weather_data_or_epw_path . to_epw ( simulation_epw_path ) 
~~~ _copy_without_read_only ( weather_data_or_epw_path , simulation_epw_path ) 
~~ temp_epw_path = get_simulated_epw_path ( ) 
if temp_epw_path is not None : 
~~~ _copy_without_read_only ( simulation_epw_path , temp_epw_path ) 
~~ eplus_relative_cmd = get_simulation_base_command ( ) 
eplus_cmd = os . path . join ( CONF . eplus_base_dir_path , eplus_relative_cmd ) 
idf_command_style = get_simulation_input_command_style ( "idf" ) 
if idf_command_style == SIMULATION_INPUT_COMMAND_STYLES . simu_dir : 
~~~ idf_file_cmd = os . path . join ( simulation_dir_path , CONF . default_model_name ) 
~~ elif idf_command_style == SIMULATION_INPUT_COMMAND_STYLES . file_path : 
~~~ idf_file_cmd = simulation_idf_path 
~~ epw_command_style = get_simulation_input_command_style ( "epw" ) 
if epw_command_style == SIMULATION_INPUT_COMMAND_STYLES . simu_dir : 
~~~ epw_file_cmd = os . path . join ( simulation_dir_path , CONF . default_model_name ) 
~~ elif epw_command_style == SIMULATION_INPUT_COMMAND_STYLES . file_path : 
~~~ epw_file_cmd = simulation_epw_path 
~~ simulation_command_style = get_simulation_command_style ( ) 
if simulation_command_style == SIMULATION_COMMAND_STYLES . args : 
~~~ cmd_l = [ eplus_cmd , idf_file_cmd , epw_file_cmd ] 
~~ elif simulation_command_style == SIMULATION_COMMAND_STYLES . kwargs : 
~~~ cmd_l = [ eplus_cmd , "-w" , epw_file_cmd , "-r" , idf_file_cmd ] 
~~ run_subprocess ( 
cmd_l , 
cwd = simulation_dir_path , 
stdout = stdout , 
stderr = stderr , 
beat_freq = beat_freq 
if ( temp_epw_path is not None ) and os . path . isfile ( temp_epw_path ) : 
~~~ os . remove ( os . path . join ( temp_epw_path ) ) 
~~ ~~ def simulate ( 
cls , 
epm_or_path , 
weather_data_or_path , 
base_dir_path , 
simulation_name = None , 
stdout = None , 
stderr = None , 
beat_freq = None 
if not os . path . isdir ( base_dir_path ) : 
~~ simulation_dir_path = base_dir_path if simulation_name is None else os . path . join ( base_dir_path , simulation_name ) 
if not os . path . exists ( simulation_dir_path ) : 
~~~ os . mkdir ( simulation_dir_path ) 
~~ stdout = LoggerStreamWriter ( logger_name = __name__ , level = logging . INFO ) if stdout is None else stdout 
stderr = LoggerStreamWriter ( logger_name = __name__ , level = logging . ERROR ) if stderr is None else stderr 
run_eplus ( 
simulation_dir_path , 
return cls ( 
simulation_name = simulation_name 
~~ def _file_refs ( self ) : 
if self . _prepared_file_refs is None : 
~~~ self . _prepared_file_refs = { 
FILE_REFS . idf : FileInfo ( 
constructor = lambda path : self . _epm_cls . from_idf ( path , idd_or_buffer_or_path = self . _idd ) , 
get_path = lambda : get_input_file_path ( self . dir_path , FILE_REFS . idf ) 
) , 
FILE_REFS . epw : FileInfo ( 
constructor = lambda path : self . _weather_data_cls . from_epw ( path ) , 
get_path = lambda : get_input_file_path ( self . dir_path , FILE_REFS . epw ) 
FILE_REFS . eio : FileInfo ( 
constructor = lambda path : self . _eio_cls ( path ) , 
get_path = lambda : get_output_file_path ( self . dir_path , FILE_REFS . eio ) 
FILE_REFS . eso : FileInfo ( 
constructor = lambda path : self . _standard_output_cls ( path ) , 
get_path = lambda : get_output_file_path ( 
self . dir_path , 
FILE_REFS . eso 
FILE_REFS . mtr : FileInfo ( 
get_path = lambda : get_output_file_path ( self . dir_path , FILE_REFS . mtr ) 
FILE_REFS . mtd : FileInfo ( 
constructor = lambda path : self . _mtd_cls ( path ) , 
get_path = lambda : get_output_file_path ( self . dir_path , FILE_REFS . mtd ) 
FILE_REFS . mdd : FileInfo ( 
constructor = lambda path : open ( path ) . read ( ) , 
get_path = lambda : get_output_file_path ( self . dir_path , FILE_REFS . mdd ) 
FILE_REFS . err : FileInfo ( 
constructor = lambda path : self . _err_cls ( path ) , 
get_path = lambda : get_output_file_path ( self . dir_path , FILE_REFS . err ) 
FILE_REFS . summary_table : FileInfo ( 
constructor = lambda path : self . _summary_table_cls ( path ) , 
get_path = lambda : get_output_file_path ( self . dir_path , FILE_REFS . summary_table ) 
~~ return self . _prepared_file_refs 
~~ def exists ( self , file_ref ) : 
if file_ref not in FILE_REFS : 
~~ return os . path . isfile ( self . _path ( file_ref ) ) 
~~ def get_file_path ( self , file_ref ) : 
if not self . exists ( file_ref ) : 
~~ return self . _path ( file_ref ) 
~~ def default_external_files_dir_name ( model_name ) : 
name , ext = os . path . splitext ( model_name ) 
return name + CONF . external_files_suffix 
~~ def _dev_populate_from_json_data ( self , json_data ) : 
comment = json_data . pop ( "_comment" , None ) 
if comment is not None : 
~~~ self . _comment = comment 
~~ external_files_data = json_data . pop ( "_external_files" , dict ( ) ) 
self . _dev_external_files_manager . populate_from_json_data ( external_files_data ) 
added_records = [ ] 
for table_ref , json_data_records in json_data . items ( ) : 
~~~ table = getattr ( self , table_ref ) 
records = table . _dev_add_inert ( json_data_records ) 
added_records . extend ( records ) 
~~ for r in added_records : 
~~~ r . _dev_activate_hooks ( ) 
~~~ r . _dev_activate_links ( ) 
r . _dev_activate_external_files ( ) 
~~ ~~ def get_external_files ( self ) : 
external_files = [ ] 
for table in self . _tables . values ( ) : 
~~~ for r in table : 
~~~ external_files . extend ( [ ef for ef in r . get_external_files ( ) ] ) 
~~ ~~ return external_files 
~~ def set_defaults ( self ) : 
~~~ r . set_defaults ( ) 
~~ ~~ ~~ def from_json_data ( cls , json_data , check_required = True , idd_or_buffer_or_path = None ) : 
epm = cls ( 
idd_or_buffer_or_path = idd_or_buffer_or_path , 
check_required = check_required 
epm . _dev_populate_from_json_data ( json_data ) 
return epm 
~~ def from_idf ( cls , buffer_or_path , check_required = True , idd_or_buffer_or_path = None ) : 
return cls . _create_from_buffer_or_path ( 
parse_idf , 
buffer_or_path , 
~~ def from_json ( cls , buffer_or_path , check_required = True , idd_or_buffer_or_path = None ) : 
json . load , 
~~ def to_json_data ( self ) : 
d = collections . OrderedDict ( ( t . get_ref ( ) , t . to_json_data ( ) ) for t in self . _tables . values ( ) ) 
d [ "_comment" ] = self . _comment 
d . move_to_end ( "_comment" , last = False ) 
d [ "_external_files" ] = self . _dev_external_files_manager 
return d 
~~ def to_json ( self , buffer_or_path = None , indent = 2 ) : 
return json_data_to_json ( 
self . to_json_data ( ) , 
buffer_or_path = buffer_or_path , 
indent = indent 
~~ def to_idf ( self , buffer_or_path = None , dump_external_files = True ) : 
comment = get_multi_line_copyright_message ( ) 
if self . _comment != "" : 
~~ comment += "\\n\\n" 
if isinstance ( buffer_or_path , str ) : 
~~~ dir_path , file_name = os . path . split ( buffer_or_path ) 
model_name , _ = os . path . splitext ( file_name ) 
~~~ model_name , dir_path = None , os . path . curdir 
~~ if dump_external_files : 
~~~ self . dump_external_files ( 
target_dir_path = os . path . join ( dir_path , get_external_files_dir_name ( model_name = model_name ) ) 
~~ formatted_records = [ ] 
~~~ formatted_records . extend ( [ r . to_idf ( model_name = model_name ) for r in sorted ( table ) ] ) 
~~ body = "\\n\\n" . join ( formatted_records ) 
content = comment + body 
lambda f : f . write ( content ) , 
lambda : content , 
buffer_or_path 
~~ def select ( self , filter_by = None ) : 
iterator = self . _records if filter_by is None else filter ( filter_by , self . _records ) 
return Queryset ( self . _table , iterator ) 
~~ def one ( self , filter_by = None ) : 
qs = self if filter_by is None else self . select ( filter_by = filter_by ) 
if len ( qs ) == 0 : 
~~ if len ( qs ) > 1 : 
~~ return qs [ 0 ] 
~~ def get_simulated_epw_path ( ) : 
if OS_NAME == "windows" : 
~~~ return os . path . join ( CONF . eplus_base_dir_path , "WeatherData" , "%s.epw" % CONF . default_model_name ) 
~~ ~~ def prepare_extensible ( self ) : 
for k in self . _tags : 
~~~ if "extensible" in k : 
~~~ cycle_len = int ( k . split ( ":" ) [ 1 ] ) 
~~ cycle_start = None 
cycle_patterns = [ ] 
for i , field_descriptor in enumerate ( self . _field_descriptors ) : 
~~~ if ( cycle_start is not None ) and ( i >= ( cycle_start + cycle_len ) ) : 
~~ if ( cycle_start is None ) and ( "begin-extensible" in field_descriptor . tags ) : 
~~~ cycle_start = i 
~~ if cycle_start is None : 
~~ cycle_patterns . append ( field_descriptor . ref . replace ( "1" , r"(\\d+)" ) ) 
~~ self . _field_descriptors = self . _field_descriptors [ : cycle_start + cycle_len ] 
self . extensible_info = ( cycle_start , cycle_len , tuple ( cycle_patterns ) ) 
for i , fd in enumerate ( self . _field_descriptors [ cycle_start : ] ) : 
~~~ fd . set_extensible_info ( cycle_start , cycle_len , cycle_patterns [ i ] ) 
~~ ~~ def get_field_reduced_index ( self , index ) : 
if self . extensible_info is None : 
~~~ return index 
~~ cycle_start , cycle_len , _ = self . extensible_info 
if index < cycle_start : 
~~ return cycle_start + ( ( index - cycle_start ) % cycle_len ) 
~~ def get_extended_name ( self , index ) : 
field_descriptor = self . get_field_descriptor ( index ) 
~~~ return field_descriptor . name 
cycle_num = ( index - cycle_start ) // cycle_len 
return None if field_descriptor . name is None else field_descriptor . name . replace ( "1" , str ( cycle_num ) ) 
~~ def deserialize ( self , value , index ) : 
if isinstance ( value , ExternalFile ) : 
~~~ value = value . pointer 
~~ if isinstance ( value , Record ) : 
~~~ value = value [ 0 ] 
~~~ raise ValueError ( "can\ ) 
~~ ~~ if isinstance ( value , str ) : 
if value == "" : 
~~ value = unidecode . unidecode ( value ) 
if "retaincase" not in self . tags : 
~~~ value = value . lower ( ) 
~~ if len ( value ) >= 100 : 
~~~ raise FieldValidationError ( 
~~ ~~ if self . is_file_name : 
~~~ value = ExternalFile . deserialize ( value ) 
~~ if self . detailed_type in ( "integer" , "real" ) : 
~~~ if value is None : 
~~ if value in ( "autocalculate" , "autosize" , "useweatherfile" ) : 
~~ if self . detailed_type == "integer" : 
~~~ return int ( value ) 
~~~ return float ( value ) 
~~ ~~ if self . detailed_type in ( "alpha" , "choice" , "node" , "external-list" ) : 
~~ if not isinstance_str ( value ) : 
~~ return value 
~~ if self . detailed_type == "reference" : 
~~~ return NONE_RECORD_HOOK 
~~ references = self . tags . get ( "reference" , [ ] ) 
return RecordHook ( references , index , value ) 
~~ if self . detailed_type == "object-list" : 
~~~ return NONE_LINK 
~~ return Link ( self . tags [ "object-list" ] , value , index ) 
~~ def detailed_type ( self ) : 
if self . _detailed_type is None : 
~~~ if ( "reference" in self . tags ) or ( "reference-class-name" in self . tags ) : 
~~~ self . _detailed_type = "reference" 
~~ elif "type" in self . tags : 
~~ elif "key" in self . tags : 
~~~ self . _detailed_type = "choice" 
~~ elif "object-list" in self . tags : 
~~~ self . _detailed_type = "object-list" 
~~ elif "external-list" in self . tags : 
~~~ self . _detailed_type = "external-list" 
~~ elif self . basic_type == "A" : 
~~~ self . _detailed_type = "alpha" 
~~ elif self . basic_type == "N" : 
~~~ self . _detailed_type = "real" 
~~~ raise ValueError ( "Can\ ) 
~~ ~~ return self . _detailed_type 
~~ def short_refs ( self ) : 
for ef in self . _external_files : 
~~~ if ef . naive_short_ref not in naive_short_refs_d : 
~~~ naive_short_refs_d [ ef . naive_short_ref ] = set ( ) 
~~ naive_short_refs_d [ ef . naive_short_ref ] . add ( ef . ref ) 
~~ short_refs = dict ( ) 
for naive_short_ref , refs in naive_short_refs_d . items ( ) : 
~~~ if len ( refs ) == 1 : 
~~~ short_refs [ refs . pop ( ) ] = naive_short_ref 
~~ base , ext = os . path . splitext ( naive_short_ref ) 
for i , ref in enumerate ( sorted ( refs ) ) : 
~~~ short_refs [ ref ] = f"{base}-{i}.{ext}" 
~~ ~~ return short_refs 
~~ def get_value ( self , column_name_or_i , filter_column_name_or_i , filter_criterion ) : 
column_i = self . _get_column_index ( column_name_or_i ) 
filter_column_i = self . _get_column_index ( filter_column_name_or_i ) 
filter_fct = { 
float : lambda x : float ( x ) == filter_criterion , 
int : lambda x : int ( x ) == filter_criterion , 
str : lambda x : x . lower ( ) == filter_criterion . lower ( ) 
} [ type ( filter_criterion ) ] 
for row_i , row in enumerate ( self . _data ) : 
~~~ if filter_fct ( row [ filter_column_i ] ) : 
~~ return self . _data [ row_i ] [ column_i ] 
~~ def _update_value_inert ( self , index , value ) : 
field_descriptor = self . _table . _dev_descriptor . get_field_descriptor ( index ) 
value = field_descriptor . deserialize ( value , index ) 
if isinstance ( value , Link ) : 
~~~ current_link = self . _data . get ( index ) 
if current_link is not None : 
~~~ current_link . unregister ( ) 
~~ ~~ if isinstance ( value , RecordHook ) : 
~~~ current_record_hook = self . _data . get ( index ) 
if current_record_hook is not None : 
~~~ current_record_hook . unregister ( ) 
~~ ~~ if isinstance ( value , ExternalFile ) : 
~~~ current_external_file = self . _data . get ( index ) 
if current_external_file is not None : 
~~~ current_external_file . _dev_unregister ( ) 
~~ ~~ if value in ( None , NONE_RECORD_HOOK , NONE_LINK , NONE_EXTERNAL_FILE ) : 
~~~ self . _dev_set_none_without_unregistering ( index , check_not_required = False ) 
~~ old_hook = None 
if index == 0 and not self . _table . _dev_auto_pk : 
~~ self . _data [ index ] = value 
if old_hook is not None : 
~~~ self . _table . _dev_record_pk_was_updated ( old_hook . target_value ) 
~~ ~~ def get_serialized_value ( self , ref_or_index , model_name = None ) : 
index = ( 
self . _table . _dev_descriptor . get_field_index ( ref_or_index ) if isinstance ( ref_or_index , str ) 
else ref_or_index 
value = self . _data . get ( index ) 
value = value . serialize ( ) if isinstance ( value , ( Link , RecordHook ) ) else value 
~~~ value = os . path . join ( get_external_files_dir_name ( model_name = model_name ) , value . naive_short_ref ) 
~~ def get_external_files ( self ) : 
return [ v for v in self . _data . values ( ) if isinstance ( v , ExternalFile ) ] 
~~ def update ( self , data = None , ** or_data ) : 
data = or_data if data is None else data 
self . _update_inert ( data ) 
self . _dev_activate_hooks ( ) 
self . _dev_activate_links ( ) 
self . _dev_activate_external_files ( ) 
~~ def copy ( self , new_name = None ) : 
if self . _table . _dev_auto_pk : 
~~~ return self . _table . add ( self . _data ) 
~~ name = str ( uuid . uuid4 ( ) ) if new_name is None else new_name 
new_data = dict ( ( k , name if k == 0 else v ) for ( k , v ) in self . _data . items ( ) ) 
return self . _table . add ( new_data ) 
defaults = { } 
for i in range ( len ( self ) ) : 
~~~ if i in self . _data : 
~~ default = self . get_field_descriptor ( i ) . tags . get ( "default" , [ None ] ) [ 0 ] 
if default is not None : 
~~~ defaults [ i ] = default 
~~ ~~ self . update ( defaults ) 
~~ def add_fields ( self , * args ) : 
if not self . is_extensible ( ) : 
~~~ raise TypeError ( "Can\ ) 
~~ self_len = len ( self ) 
data = dict ( [ ( self_len + i , args [ i ] ) for i in range ( len ( args ) ) ] ) 
self . update ( data ) 
~~ def pop ( self , index = None ) : 
index = self . _prepare_pop_insert_index ( index = index ) 
cycle_start , cycle_len , patterns = self . get_extensible_info ( ) 
fields = self . clear_extensible_fields ( ) 
serialized_value = fields . pop ( index - cycle_start ) 
self . add_fields ( * fields ) 
return serialized_value 
~~ def insert ( self , index , value ) : 
fields . insert ( index , value ) 
~~ def clear_extensible_fields ( self ) : 
~~ cycle_start , cycle_len , patterns = self . get_extensible_info ( ) 
return [ self . get_serialized_value ( i ) for i in range ( cycle_start , len ( self ) ) ] 
~~ def delete ( self ) : 
self . _unregister_links ( ) 
self . _unregister_hooks ( ) 
self . _unregister_external_files ( ) 
self . get_table ( ) . _dev_remove_record_without_unregistering ( self ) 
self . _table = None 
self . _data = None 
~~ def get_field_descriptor ( self , ref_or_index ) : 
if isinstance ( ref_or_index , int ) : 
~~~ index = ref_or_index 
~~~ index = self . _table . _dev_descriptor . get_field_index ( ref_or_index ) 
~~ return self . _table . _dev_descriptor . get_field_descriptor ( index ) 
~~ def to_json_data ( self , model_name = None ) : 
return collections . OrderedDict ( [ ( k , self . get_serialized_value ( k , model_name = model_name ) ) for k in self . _data ] ) 
~~ def to_idf ( self , model_name = None ) : 
json_data = self . to_json_data ( model_name = model_name ) 
s = f"{self._table._dev_descriptor.table_name},\\n" 
fields_nb = max ( self . _data ) + 1 
for i in range ( fields_nb ) : 
raw_value = json_data . get ( i , "" ) 
content = f"{tab}{raw_value}{\ 
spaces_nb = COMMENT_COLUMN_START - len ( content ) 
if spaces_nb < 0 : 
~~~ spaces_nb = TAB_LEN 
~~ name = self . _table . _dev_descriptor . get_extended_name ( i ) 
s += f"{content}{comment}\\n" 
~~ return s 
~~ def check ( ) : 
epw_path = os . path . join ( CONF . eplus_base_dir_path , "WeatherData" , 
"USA_VA_Sterling-Washington.Dulles.Intl.AP.724030_TMY3.epw" ) 
idf_dir_path = os . path . join ( CONF . eplus_base_dir_path , "ExampleFiles" ) 
test_num = 0 
for file_num , file_name in enumerate ( os . listdir ( idf_dir_path ) ) : 
~~~ if file_num < START_FILE_NUM : 
~~ base , ext = os . path . splitext ( file_name ) 
if ext == ".idf" : 
~~~ with tempfile . TemporaryDirectory ( ) as simulation_dir_path : 
~~~ s = simulate ( os . path . join ( idf_dir_path , file_name ) , epw_path , 
simulation_dir_path if DEBUG_SIMUL_DIR_PATH is None else 
DEBUG_SIMUL_DIR_PATH ) 
if s . exists ( "eio" ) : 
test_num += 1 
~~ ~~ ~~ if test_num == MAX_TESTS_NB : 
~~ ~~ ~~ def get_data ( self , environment_title_or_num = - 1 , frequency = None ) : 
if isinstance ( environment_title_or_num , int ) : 
~~~ environment_title = tuple ( self . _raw_environments . keys ( ) ) [ environment_title_or_num ] 
~~~ environment_title = environment_title_or_num 
~~ if environment_title not in self . _dfs : 
~~ environment_dfs = self . _dfs [ environment_title ] 
if frequency is None : 
~~~ for frequency in FREQUENCIES : 
~~~ if environment_dfs [ frequency ] is not None : 
~~ ~~ ~~ if frequency not in FREQUENCIES : 
~~ return self . _dfs [ environment_title ] [ frequency ] 
~~ def get_documented_add ( self , record_descriptors ) : 
def add ( data = None , ** or_data ) : 
return self . batch_add ( [ or_data if data is None else data ] ) [ 0 ] 
~~ add . __doc__ = "\\n" . join ( [ fd . ref . lower ( ) for fd in record_descriptors if fd . ref is not None ] ) 
return add 
~~ def _dev_add_inert ( self , records_data ) : 
for r_data in records_data : 
~~~ record = Record ( 
self , 
data = r_data 
self . _records [ record . get_pk ( ) ] = record 
added_records . append ( record ) 
~~ return added_records 
records = self . _records . values ( ) if filter_by is None else filter ( filter_by , self . _records . values ( ) ) 
return Queryset ( self , records = records ) 
return Queryset ( self , records = self . _records . values ( ) ) . one ( filter_by = filter_by ) 
~~ def batch_add ( self , records_data ) : 
added_records = self . _dev_add_inert ( records_data ) 
for r in added_records : 
~~ return Queryset ( self , records = added_records ) 
~~ def register_record_hook ( self , hook ) : 
for key in hook . keys : 
~~~ if key in self . _record_hooks : 
~~~ field_descriptor = hook . target_record . get_field_descriptor ( hook . target_index ) 
raise FieldValidationError ( 
~~ self . _record_hooks [ key ] = hook 
~~ ~~ def register_link ( self , link ) : 
keys = tuple ( ( ref , link . initial_hook_value ) for ref in link . hook_references ) 
for k in keys : 
~~~ if k in self . _record_hooks : 
~~~ link . set_target ( target_record = self . _record_hooks [ k ] . target_record ) 
~~~ for k in keys : 
~~~ if k in self . _table_hooks : 
~~~ link . set_target ( target_table = self . _table_hooks [ k ] ) 
~~~ field_descriptor = link . source_record . get_field_descriptor ( link . source_index ) 
f"{field_descriptor.get_error_location_message(link.initial_hook_value)}" 
~~ ~~ if link . source_record not in self . _links_by_source : 
~~~ self . _links_by_source [ link . source_record ] = set ( ) 
~~ self . _links_by_source [ link . source_record ] . add ( link ) 
if link . target not in self . _links_by_target : 
~~~ self . _links_by_target [ link . target ] = set ( ) 
~~ self . _links_by_target [ link . target ] . add ( link ) 
~~ def run_subprocess ( command , cwd = None , stdout = None , stderr = None , shell = False , beat_freq = None ) : 
sys . encoding = CONF . encoding 
stdout = sys . stdout if stdout is None else stdout 
stderr = sys . stderr if stderr is None else stderr 
with subprocess . Popen ( 
command , 
stdout = subprocess . PIPE , 
stderr = subprocess . PIPE , 
cwd = cwd , 
shell = shell , 
universal_newlines = True 
) as sub_p : 
~~~ with redirect_stream ( sub_p . stdout , stdout ) , redirect_stream ( sub_p . stderr , stderr ) : 
~~~ sub_p . wait ( timeout = beat_freq ) 
~~ except subprocess . TimeoutExpired : 
if hasattr ( sys . stdout , "flush" ) : 
~~~ sys . stdout . flush ( ) 
~~ ~~ ~~ ~~ return sub_p . returncode 
~~ ~~ def get_string_buffer ( path_or_content , expected_extension ) : 
buffer , path = None , None 
if isinstance ( path_or_content , str ) : 
~~~ if path_or_content [ - len ( expected_extension ) - 1 : ] == ".%s" % expected_extension : 
~~~ if not os . path . isfile ( path_or_content ) : 
~~ buffer , path = open ( path_or_content , encoding = CONF . encoding ) , path_or_content 
~~~ buffer = io . StringIO ( path_or_content , ) 
~~ ~~ elif isinstance ( path_or_content , io . TextIOBase ) : 
~~~ buffer = path_or_content 
~~ elif isinstance ( path_or_content , bytes ) : 
~~~ buffer = io . StringIO ( path_or_content . decode ( encoding = CONF . encoding ) ) 
~~ elif isinstance ( path_or_content , io . BufferedIOBase ) : 
~~~ buffer = io . StringIO ( path_or_content . read ( ) . decode ( encoding = CONF . encoding ) ) 
~~ return buffer , path 
~~ def get_data ( self , simulation_step = None , error_category = None ) : 
if simulation_step is None and error_category is None : 
~~~ return self . _df . dropna ( axis = "rows" , how = "all" ) 
~~ if simulation_step is not None : 
~~~ if simulation_step not in self . _simulation_step_list : 
~~ if error_category is not None : 
~~~ if error_category not in self . CATEGORIES : 
~~ iterables = [ simulation_step , error_category ] 
columns = pd . MultiIndex . from_product ( iterables ) 
series = self . _df [ simulation_step ] [ error_category ] . dropna ( axis = "rows" , how = "all" ) 
df = pd . DataFrame ( index = series . index , columns = columns ) 
df [ simulation_step ] = series 
~~ return self . _df [ simulation_step ] . dropna ( axis = "rows" , how = "all" ) 
~~ df = self . _df . copy ( ) 
df . columns = df . columns . swaplevel ( 0 , 1 ) 
return df [ error_category ] . dropna ( axis = "rows" , how = "all" ) 
~~ ~~ def _create_regex ( self , line , intent_name ) : 
~~~ return re . compile ( self . _create_intent_pattern ( line , intent_name ) , 
re . IGNORECASE ) 
~~ except sre_constants . error as e : 
~~~ LOG . warning ( \ 
~~ ~~ def render_to_fragment ( self , request , ** kwargs ) : 
fragment = Fragment ( TEST_HTML ) 
fragment . add_javascript ( TEST_JS ) 
fragment . add_css ( TEST_CSS ) 
return fragment 
~~ def resources ( self ) : 
seen = set ( ) 
return [ x for x in self . _resources if x not in seen and not seen . add ( x ) ] 
~~ def to_dict ( self ) : 
return { 
'content' : self . content , 
'js_init_fn' : self . js_init_fn , 
'js_init_version' : self . js_init_version , 
'json_init_args' : self . json_init_args 
~~ def from_dict ( cls , pods ) : 
frag = cls ( ) 
frag . content = pods [ 'content' ] 
frag . js_init_fn = pods [ 'js_init_fn' ] 
frag . js_init_version = pods [ 'js_init_version' ] 
frag . json_init_args = pods [ 'json_init_args' ] 
return frag 
~~ def add_content ( self , content ) : 
assert isinstance ( content , six . text_type ) 
self . content += content 
~~ def add_resource ( self , text , mimetype , placement = None ) : 
if not placement : 
~~~ placement = self . _default_placement ( mimetype ) 
~~ res = FragmentResource ( 'text' , text , mimetype , placement ) 
self . _resources . append ( res ) 
~~ def add_resource_url ( self , url , mimetype , placement = None ) : 
~~ self . _resources . append ( FragmentResource ( 'url' , url , mimetype , placement ) ) 
~~ def initialize_js ( self , js_func , json_args = None ) : 
self . js_init_fn = js_func 
self . js_init_version = JS_API_VERSION 
if json_args : 
~~~ self . json_init_args = json_args 
~~ ~~ def resources_to_html ( self , placement ) : 
return '\\n' . join ( 
self . resource_to_html ( resource ) 
for resource in self . resources 
if resource . placement == placement 
~~ def resource_to_html ( resource ) : 
if resource . mimetype == "text/css" : 
~~~ if resource . kind == "text" : 
~~ elif resource . kind == "url" : 
~~ ~~ elif resource . mimetype == "application/javascript" : 
~~~ return u"<script>\\n%s\\n</script>" % resource . data 
~~ ~~ elif resource . mimetype == "text/html" : 
~~~ assert resource . kind == "text" 
return resource . data 
~~ ~~ def get ( self , request , * args , ** kwargs ) : 
fragment = self . render_to_fragment ( request , ** kwargs ) 
response_format = request . GET . get ( 'format' ) or request . POST . get ( 'format' ) or 'html' 
if response_format == 'json' or WEB_FRAGMENT_RESPONSE_TYPE in request . META . get ( 'HTTP_ACCEPT' , 'text/html' ) : 
~~~ return JsonResponse ( fragment . to_dict ( ) ) 
~~~ return self . render_standalone_response ( request , fragment , ** kwargs ) 
if fragment is None : 
~~~ return HttpResponse ( status = 204 ) 
~~ html = self . render_to_standalone_html ( request , fragment , ** kwargs ) 
return HttpResponse ( html ) 
template = get_template ( STANDALONE_TEMPLATE_NAME ) 
context = { 
'head_html' : fragment . head_html ( ) , 
'body_html' : fragment . body_html ( ) , 
'foot_html' : fragment . foot_html ( ) , 
return template . render ( context ) 
~~ def calc ( pvalues , lamb ) : 
m = len ( pvalues ) 
pi0 = ( pvalues > lamb ) . sum ( ) / ( ( 1 - lamb ) * m ) 
pFDR = np . ones ( m ) 
for i in range ( m ) : 
~~~ y = pvalues [ i ] 
Pr = max ( 1 , m - i ) / float ( m ) 
pFDR [ i ] = ( pi0 * y ) / ( Pr * ( 1 - math . pow ( 1 - y , m ) ) ) 
print ( i , pFDR [ i ] , y , Pr , 1.0 - math . pow ( 1 - y , m ) ) 
~~ num_null = pi0 * m 
num_alt = m - num_null 
num_negs = np . array ( range ( m ) ) 
num_pos = m - num_negs 
pp = num_pos / float ( m ) 
qvalues = np . ones ( m ) 
qvalues [ 0 ] = pFDR [ 0 ] 
for i in range ( m - 1 ) : 
~~~ qvalues [ i + 1 ] = min ( qvalues [ i ] , pFDR [ i + 1 ] ) 
~~ sens = ( ( 1.0 - qvalues ) * num_pos ) / num_alt 
sens [ sens > 1.0 ] = 1.0 
df = pd . DataFrame ( dict ( 
pvalue = pvalues , 
qvalue = qvalues , 
FDR = pFDR , 
percentile_positive = pp , 
sens = sens 
df [ "svalue" ] = df . sens [ : : - 1 ] . cummax ( ) [ : : - 1 ] 
return df , num_null , m 
~~ def unwrap_self_for_multiprocessing ( arg ) : 
( inst , method_name , args ) = arg 
return getattr ( inst , method_name ) ( * args ) 
~~ def to_one_dim_array ( values , as_type = None ) : 
if isinstance ( values , ( list , tuple ) ) : 
~~~ values = np . array ( values , dtype = np . float32 ) 
~~ elif isinstance ( values , pd . Series ) : 
~~~ values = values . values 
~~ values = values . flatten ( ) 
if as_type is not None : 
~~~ return values . astype ( as_type ) 
~~ return values 
~~ def lookup_values_from_error_table ( scores , err_df ) : 
ix = find_nearest_matches ( np . float32 ( err_df . cutoff . values ) , np . float32 ( scores ) ) 
return err_df . pvalue . iloc [ ix ] . values , err_df . svalue . iloc [ ix ] . values , err_df . pep . iloc [ ix ] . values , err_df . qvalue . iloc [ ix ] . values 
~~ def posterior_chromatogram_hypotheses_fast ( experiment , prior_chrom_null ) : 
tg_ids = experiment . df . tg_num_id . values 
pp_values = 1 - experiment . df [ "pep" ] . values 
current_tg_id = tg_ids [ 0 ] 
scores = [ ] 
final_result = [ ] 
final_result_h0 = [ ] 
for i in range ( tg_ids . shape [ 0 ] ) : 
~~~ id_ = tg_ids [ i ] 
if id_ != current_tg_id : 
~~~ prior_pg_true = ( 1.0 - prior_chrom_null ) / len ( scores ) 
rr = single_chromatogram_hypothesis_fast ( 
np . array ( scores ) , prior_chrom_null , prior_pg_true ) 
final_result . extend ( rr [ 1 : ] ) 
final_result_h0 . extend ( rr [ 0 ] for i in range ( len ( scores ) ) ) 
current_tg_id = id_ 
~~ scores . append ( 1.0 - pp_values [ i ] ) 
~~ prior_pg_true = ( 1.0 - prior_chrom_null ) / len ( scores ) 
rr = single_chromatogram_hypothesis_fast ( np . array ( scores ) , prior_chrom_null , prior_pg_true ) 
final_result_h0 . extend ( [ rr [ 0 ] ] * len ( scores ) ) 
return final_result , final_result_h0 
~~ def pnorm ( stat , stat0 ) : 
mu , sigma = mean_and_std_dev ( stat0 ) 
stat = to_one_dim_array ( stat , np . float64 ) 
args = ( stat - mu ) / sigma 
return 1 - ( 0.5 * ( 1.0 + scipy . special . erf ( args / np . sqrt ( 2.0 ) ) ) ) 
~~ def pemp ( stat , stat0 ) : 
assert len ( stat0 ) > 0 
assert len ( stat ) > 0 
stat = np . array ( stat ) 
stat0 = np . array ( stat0 ) 
m = len ( stat ) 
m0 = len ( stat0 ) 
statc = np . concatenate ( ( stat , stat0 ) ) 
v = np . array ( [ True ] * m + [ False ] * m0 ) 
v = v [ perm ] 
u = np . where ( v ) [ 0 ] 
p = ( u - np . arange ( m ) ) / float ( m0 ) 
ranks = np . floor ( scipy . stats . rankdata ( - stat ) ) . astype ( int ) - 1 
p = p [ ranks ] 
p [ p <= 1.0 / m0 ] = 1.0 / m0 
return p 
~~ def pi0est ( p_values , lambda_ = np . arange ( 0.05 , 1.0 , 0.05 ) , pi0_method = "smoother" , smooth_df = 3 , smooth_log_pi0 = False ) : 
p = np . array ( p_values ) 
rm_na = np . isfinite ( p ) 
p = p [ rm_na ] 
m = len ( p ) 
ll = 1 
if isinstance ( lambda_ , np . ndarray ) : 
~~~ ll = len ( lambda_ ) 
lambda_ = np . sort ( lambda_ ) 
~~ if ( min ( p ) < 0 or max ( p ) > 1 ) : 
~~ elif ( ll > 1 and ll < 4 ) : 
~~ elif ( np . min ( lambda_ ) < 0 or np . max ( lambda_ ) >= 1 ) : 
~~ if ( ll == 1 ) : 
~~~ pi0 = np . mean ( p >= lambda_ ) / ( 1 - lambda_ ) 
pi0_lambda = pi0 
pi0 = np . minimum ( pi0 , 1 ) 
pi0Smooth = False 
~~~ pi0 = [ ] 
for l in lambda_ : 
~~~ pi0 . append ( np . mean ( p >= l ) / ( 1 - l ) ) 
~~ pi0_lambda = pi0 
if ( pi0_method == "smoother" ) : 
~~~ if smooth_log_pi0 : 
~~~ pi0 = np . log ( pi0 ) 
spi0 = sp . interpolate . UnivariateSpline ( lambda_ , pi0 , k = smooth_df ) 
pi0Smooth = np . exp ( spi0 ( lambda_ ) ) 
~~~ spi0 = sp . interpolate . UnivariateSpline ( lambda_ , pi0 , k = smooth_df ) 
pi0Smooth = spi0 ( lambda_ ) 
~~ pi0 = np . minimum ( pi0Smooth [ ll - 1 ] , 1 ) 
~~ elif ( pi0_method == "bootstrap" ) : 
~~~ minpi0 = np . percentile ( pi0 , 0.1 ) 
W = [ ] 
~~~ W . append ( np . sum ( p >= l ) ) 
~~ mse = ( np . array ( W ) / ( np . power ( m , 2 ) * np . power ( ( 1 - lambda_ ) , 2 ) ) ) * ( 1 - np . array ( W ) / m ) + np . power ( ( pi0 - minpi0 ) , 2 ) 
pi0 = np . minimum ( pi0 [ np . argmin ( mse ) ] , 1 ) 
~~ ~~ if ( pi0 <= 0 ) : 
~~ return { 'pi0' : pi0 , 'pi0_lambda' : pi0_lambda , 'lambda_' : lambda_ , 'pi0_smooth' : pi0Smooth } 
~~ def lfdr ( p_values , pi0 , trunc = True , monotone = True , transf = "probit" , adj = 1.5 , eps = np . power ( 10.0 , - 8 ) ) : 
lfdr_out = p 
if ( min ( p ) < 0 or max ( p ) > 1 ) : 
~~ elif ( pi0 < 0 or pi0 > 1 ) : 
~~ if ( transf == "probit" ) : 
~~~ p = np . maximum ( p , eps ) 
p = np . minimum ( p , 1 - eps ) 
x = scipy . stats . norm . ppf ( p , loc = 0 , scale = 1 ) 
bw = bw_nrd0 ( x ) 
myd = KDEUnivariate ( x ) 
myd . fit ( bw = adj * bw , gridsize = 512 ) 
splinefit = sp . interpolate . splrep ( myd . support , myd . density ) 
y = sp . interpolate . splev ( x , splinefit ) 
lfdr = pi0 * scipy . stats . norm . pdf ( x ) / y 
~~ elif ( transf == "logit" ) : 
~~~ x = np . log ( ( p + eps ) / ( 1 - p + eps ) ) 
dx = np . exp ( x ) / np . power ( ( 1 + np . exp ( x ) ) , 2 ) 
lfdr = ( pi0 * dx ) / y 
~~ if ( trunc ) : 
~~~ lfdr [ lfdr > 1 ] = 1 
~~ if ( monotone ) : 
~~~ lfdr = lfdr [ p . ravel ( ) . argsort ( ) ] 
for i in range ( 1 , len ( x ) ) : 
~~~ if ( lfdr [ i ] < lfdr [ i - 1 ] ) : 
~~~ lfdr [ i ] = lfdr [ i - 1 ] 
~~ ~~ lfdr = lfdr [ scipy . stats . rankdata ( p , "min" ) - 1 ] 
~~ lfdr_out [ rm_na ] = lfdr 
return lfdr_out 
~~ def final_err_table ( df , num_cut_offs = 51 ) : 
cutoffs = df . cutoff . values 
min_ = min ( cutoffs ) 
max_ = max ( cutoffs ) 
margin = ( max_ - min_ ) * 0.05 
sampled_cutoffs = np . linspace ( min_ - margin , max_ + margin , num_cut_offs , dtype = np . float32 ) 
ix = find_nearest_matches ( np . float32 ( df . cutoff . values ) , sampled_cutoffs ) 
sampled_df = df . iloc [ ix ] . copy ( ) 
sampled_df . cutoff = sampled_cutoffs 
sampled_df . reset_index ( inplace = True , drop = True ) 
return sampled_df 
~~ def summary_err_table ( df , qvalues = [ 0 , 0.01 , 0.02 , 0.05 , 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ] ) : 
qvalues = to_one_dim_array ( qvalues ) 
ix = find_nearest_matches ( np . float32 ( df . qvalue . values ) , qvalues ) 
df_sub = df . iloc [ ix ] . copy ( ) 
for i_sub , ( i0 , i1 ) in enumerate ( zip ( ix , ix [ 1 : ] ) ) : 
~~~ if i1 == i0 : 
~~~ df_sub . iloc [ i_sub + 1 , : ] = None 
~~ ~~ df_sub . qvalue = qvalues 
df_sub . reset_index ( inplace = True , drop = True ) 
return df_sub [ [ 'qvalue' , 'pvalue' , 'svalue' , 'pep' , 'fdr' , 'fnr' , 'fpr' , 'tp' , 'tn' , 'fp' , 'fn' , 'cutoff' ] ] 
~~ def error_statistics ( target_scores , decoy_scores , parametric , pfdr , pi0_lambda , pi0_method = "smoother" , pi0_smooth_df = 3 , pi0_smooth_log_pi0 = False , compute_lfdr = False , lfdr_trunc = True , lfdr_monotone = True , lfdr_transf = "probit" , lfdr_adj = 1.5 , lfdr_eps = np . power ( 10.0 , - 8 ) ) : 
target_scores = to_one_dim_array ( target_scores ) 
target_scores = np . sort ( target_scores [ ~ np . isnan ( target_scores ) ] ) 
decoy_scores = to_one_dim_array ( decoy_scores ) 
decoy_scores = np . sort ( decoy_scores [ ~ np . isnan ( decoy_scores ) ] ) 
if parametric : 
~~~ target_pvalues = pnorm ( target_scores , decoy_scores ) 
~~~ target_pvalues = pemp ( target_scores , decoy_scores ) 
~~ pi0 = pi0est ( target_pvalues , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 ) 
target_qvalues = qvalue ( target_pvalues , pi0 [ 'pi0' ] , pfdr ) 
metrics = stat_metrics ( target_pvalues , pi0 [ 'pi0' ] , pfdr ) 
error_stat = pd . DataFrame ( { 'cutoff' : target_scores , 'pvalue' : target_pvalues , 'qvalue' : target_qvalues , 'svalue' : metrics [ 'svalue' ] , 'tp' : metrics [ 'tp' ] , 'fp' : metrics [ 'fp' ] , 'tn' : metrics [ 'tn' ] , 'fn' : metrics [ 'fn' ] , 'fpr' : metrics [ 'fpr' ] , 'fdr' : metrics [ 'fdr' ] , 'fnr' : metrics [ 'fnr' ] } ) 
if compute_lfdr : 
~~~ error_stat [ 'pep' ] = lfdr ( target_pvalues , pi0 [ 'pi0' ] , lfdr_trunc , lfdr_monotone , lfdr_transf , lfdr_adj , lfdr_eps ) 
~~ return error_stat , pi0 
~~ def find_cutoff ( tt_scores , td_scores , cutoff_fdr , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 ) : 
error_stat , pi0 = error_statistics ( tt_scores , td_scores , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , False ) 
if not len ( error_stat ) : 
~~ i0 = ( error_stat . qvalue - cutoff_fdr ) . abs ( ) . idxmin ( ) 
cutoff = error_stat . iloc [ i0 ] [ "cutoff" ] 
return cutoff 
~~ def score ( infile , outfile , classifier , xgb_autotune , apply_weights , xeval_fraction , xeval_num_iter , ss_initial_fdr , ss_iteration_fdr , ss_num_iter , ss_main_score , group_id , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , lfdr_truncate , lfdr_monotone , lfdr_transformation , lfdr_adj , lfdr_eps , level , ipf_max_peakgroup_rank , ipf_max_peakgroup_pep , ipf_max_transition_isotope_overlap , ipf_min_transition_sn , tric_chromprob , threads , test ) : 
if outfile is None : 
~~~ outfile = infile 
~~~ outfile = outfile 
~~ xgb_hyperparams = { 'autotune' : xgb_autotune , 'autotune_num_rounds' : 10 , 'num_boost_round' : 100 , 'early_stopping_rounds' : 10 , 'test_size' : 0.33 } 
xgb_params = { 'eta' : 0.3 , 'gamma' : 0 , 'max_depth' : 6 , 'min_child_weight' : 1 , 'subsample' : 1 , 'colsample_bytree' : 1 , 'colsample_bylevel' : 1 , 'colsample_bynode' : 1 , 'lambda' : 1 , 'alpha' : 0 , 'scale_pos_weight' : 1 , 'silent' : 1 , 'objective' : 'binary:logitraw' , 'nthread' : 1 , 'eval_metric' : 'auc' } 
xgb_params_space = { 'eta' : hp . uniform ( 'eta' , 0.0 , 0.3 ) , 'gamma' : hp . uniform ( 'gamma' , 0.0 , 0.5 ) , 'max_depth' : hp . quniform ( 'max_depth' , 2 , 8 , 1 ) , 'min_child_weight' : hp . quniform ( 'min_child_weight' , 1 , 5 , 1 ) , 'subsample' : 1 , 'colsample_bytree' : 1 , 'colsample_bylevel' : 1 , 'colsample_bynode' : 1 , 'lambda' : hp . uniform ( 'lambda' , 0.0 , 1.0 ) , 'alpha' : hp . uniform ( 'alpha' , 0.0 , 1.0 ) , 'scale_pos_weight' : 1.0 , 'silent' : 1 , 'objective' : 'binary:logitraw' , 'nthread' : 1 , 'eval_metric' : 'auc' } 
if not apply_weights : 
~~~ PyProphetLearner ( infile , outfile , classifier , xgb_hyperparams , xgb_params , xgb_params_space , xeval_fraction , xeval_num_iter , ss_initial_fdr , ss_iteration_fdr , ss_num_iter , ss_main_score , group_id , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , lfdr_truncate , lfdr_monotone , lfdr_transformation , lfdr_adj , lfdr_eps , level , ipf_max_peakgroup_rank , ipf_max_peakgroup_pep , ipf_max_transition_isotope_overlap , ipf_min_transition_sn , tric_chromprob , threads , test ) . run ( ) 
~~~ PyProphetWeightApplier ( infile , outfile , classifier , xgb_hyperparams , xgb_params , xgb_params_space , xeval_fraction , xeval_num_iter , ss_initial_fdr , ss_iteration_fdr , ss_num_iter , ss_main_score , group_id , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , lfdr_truncate , lfdr_monotone , lfdr_transformation , lfdr_adj , lfdr_eps , level , ipf_max_peakgroup_rank , ipf_max_peakgroup_pep , ipf_max_transition_isotope_overlap , ipf_min_transition_sn , tric_chromprob , threads , test , apply_weights ) . run ( ) 
~~ ~~ def ipf ( infile , outfile , ipf_ms1_scoring , ipf_ms2_scoring , ipf_h0 , ipf_grouped_fdr , ipf_max_precursor_pep , ipf_max_peakgroup_pep , ipf_max_precursor_peakgroup_pep , ipf_max_transition_pep ) : 
~~ infer_peptidoforms ( infile , outfile , ipf_ms1_scoring , ipf_ms2_scoring , ipf_h0 , ipf_grouped_fdr , ipf_max_precursor_pep , ipf_max_peakgroup_pep , ipf_max_precursor_peakgroup_pep , ipf_max_transition_pep ) 
~~ def peptide ( infile , outfile , context , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , lfdr_truncate , lfdr_monotone , lfdr_transformation , lfdr_adj , lfdr_eps ) : 
~~ infer_peptides ( infile , outfile , context , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , lfdr_truncate , lfdr_monotone , lfdr_transformation , lfdr_adj , lfdr_eps ) 
~~ def protein ( infile , outfile , context , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , lfdr_truncate , lfdr_monotone , lfdr_transformation , lfdr_adj , lfdr_eps ) : 
~~ infer_proteins ( infile , outfile , context , parametric , pfdr , pi0_lambda , pi0_method , pi0_smooth_df , pi0_smooth_log_pi0 , lfdr_truncate , lfdr_monotone , lfdr_transformation , lfdr_adj , lfdr_eps ) 
~~ def subsample ( infile , outfile , subsample_ratio , test ) : 
~~ subsample_osw ( infile , outfile , subsample_ratio , test ) 
~~ def reduce ( infile , outfile ) : 
~~ reduce_osw ( infile , outfile ) 
~~ def merge ( infiles , outfile , same_run , templatefile ) : 
if len ( infiles ) < 1 : 
~~ merge_osw ( infiles , outfile , templatefile , same_run ) 
~~ def backpropagate ( infile , outfile , apply_scores ) : 
~~ backpropagate_oswr ( infile , outfile , apply_scores ) 
~~ def export ( infile , outfile , format , outcsv , transition_quantification , max_transition_pep , ipf , ipf_max_peptidoform_pep , max_rs_peakgroup_qvalue , peptide , max_global_peptide_qvalue , protein , max_global_protein_qvalue ) : 
if format == "score_plots" : 
~~~ export_score_plots ( infile ) 
~~~ if outfile is None : 
~~~ if outcsv : 
~~~ outfile = infile . split ( ".osw" ) [ 0 ] + ".csv" 
~~~ outfile = infile . split ( ".osw" ) [ 0 ] + ".tsv" 
~~ export_tsv ( infile , outfile , format , outcsv , transition_quantification , max_transition_pep , ipf , ipf_max_peptidoform_pep , max_rs_peakgroup_qvalue , peptide , max_global_peptide_qvalue , protein , max_global_protein_qvalue ) 
~~ ~~ def export_compound ( infile , outfile , format , outcsv , max_rs_peakgroup_qvalue ) : 
~~ export_compound_tsv ( infile , outfile , format , outcsv , max_rs_peakgroup_qvalue ) 
~~ ~~ def filter ( sqmassfiles , infile , max_precursor_pep , max_peakgroup_pep , max_transition_pep ) : 
filter_sqmass ( sqmassfiles , infile , max_precursor_pep , max_peakgroup_pep , max_transition_pep ) 
~~ def search_groups ( self , ** kwargs ) : 
kwargs = dict ( ( k . lower ( ) , kwargs [ k ] . lower ( ) ) for k in kwargs ) 
if 'type' in kwargs and ( 
kwargs [ 'type' ] != 'direct' and kwargs [ 'type' ] != 'effective' ) : 
~~~ del ( kwargs [ 'type' ] ) 
~~ if 'scope' in kwargs and ( 
kwargs [ 'scope' ] != 'one' and kwargs [ 'scope' ] != 'all' ) : 
~~~ del ( kwargs [ 'scope' ] ) 
~~ if "instructor" in kwargs or "student" in kwargs : 
~~~ kwargs [ "stem" ] = "course" 
~~ url = "{}/search?{}" . format ( self . API , urlencode ( kwargs ) ) 
data = self . _get_resource ( url ) 
for datum in data . get ( 'data' , [ ] ) : 
~~~ group = GroupReference ( ) 
group . uwregid = datum . get ( 'regid' ) 
group . display_name = datum . get ( 'displayName' ) 
group . name = datum . get ( 'id' ) 
group . url = datum . get ( 'url' ) 
groups . append ( group ) 
~~ return groups 
~~ def get_group_by_id ( self , group_id ) : 
self . _valid_group_id ( group_id ) 
url = "{}/group/{}" . format ( self . API , group_id ) 
return self . _group_from_json ( data . get ( "data" ) ) 
~~ def create_group ( self , group ) : 
self . _valid_group_id ( group . id ) 
body = { "data" : group . json_data ( ) } 
url = "{}/group/{}" . format ( self . API , group . name ) 
data = self . _put_resource ( url , headers = { } , body = body ) 
~~ def delete_group ( self , group_id ) : 
self . _delete_resource ( url ) 
return True 
~~ def get_members ( self , group_id ) : 
url = "{}/group/{}/member" . format ( self . API , group_id ) 
members = [ ] 
for datum in data . get ( "data" ) : 
~~~ members . append ( self . _group_member_from_json ( datum ) ) 
~~ return members 
~~ def update_members ( self , group_id , members ) : 
body = { "data" : [ m . json_data ( ) for m in members ] } 
headers = { "If-Match" : "*" } 
data = self . _put_resource ( url , headers , body ) 
errors = data . get ( "errors" , [ ] ) 
if len ( errors ) : 
~~~ return errors [ 0 ] . get ( "notFound" , [ ] ) 
~~ return [ ] 
~~ def get_effective_member_count ( self , group_id ) : 
url = "{}/group/{}/effective_member?view=count" . format ( self . API , 
group_id ) 
count = data . get ( "data" ) . get ( "count" ) 
return int ( count ) 
~~ def is_effective_member ( self , group_id , netid ) : 
netid = re . sub ( '@washington.edu' , '' , netid ) 
url = "{}/group/{}/effective_member/{}" . format ( self . API , 
group_id , 
netid ) 
~~~ data = self . _get_resource ( url ) 
~~ except DataFailureException as ex : 
~~~ if ex . status == 404 : 
~~ ~~ ~~ def modify_conf ( ) : 
import redbaron 
import ubelt as ub 
conf_path = 'docs/conf.py' 
source = ub . readfrom ( conf_path ) 
red = redbaron . RedBaron ( source ) 
extra_extensions = [ 
ext_node = red . find ( 'name' , value = 'extensions' ) . parent 
ext_node . value . value . extend ( extra_extensions ) 
theme_node = red . find ( 'name' , value = 'html_theme' ) . parent 
theme_node . value . value = \ 
ub . writeto ( conf_path , red . dumps ( ) ) 
~~ def parse_version ( ) : 
from os . path import dirname , join 
import ast 
modname = setupkw [ 'name' ] 
init_fpath = join ( dirname ( __file__ ) , modname , '__init__.py' ) 
with open ( init_fpath ) as file_ : 
~~~ sourcecode = file_ . read ( ) 
~~ pt = ast . parse ( sourcecode ) 
class VersionVisitor ( ast . NodeVisitor ) : 
~~~ def visit_Assign ( self , node ) : 
~~~ for target in node . targets : 
~~~ if target . id == '__version__' : 
~~~ self . version = node . value . s 
~~ ~~ ~~ ~~ visitor = VersionVisitor ( ) 
visitor . visit ( pt ) 
return visitor . version 
~~ def write ( self , obj , resource_id = None ) : 
self . logger . debug ( 'elasticsearch::write::{}' . format ( resource_id ) ) 
if resource_id is not None : 
~~~ if self . driver . _es . exists ( 
index = self . driver . _index , 
id = resource_id , 
doc_type = '_doc' 
~~ ~~ return self . driver . _es . index ( 
body = obj , 
doc_type = '_doc' , 
refresh = 'wait_for' 
) [ '_id' ] 
~~ def read ( self , resource_id ) : 
self . logger . debug ( 'elasticsearch::read::{}' . format ( resource_id ) ) 
return self . driver . _es . get ( 
) [ '_source' ] 
~~ def update ( self , obj , resource_id ) : 
self . logger . debug ( 'elasticsearch::update::{}' . format ( resource_id ) ) 
return self . driver . _es . index ( 
~~ def delete ( self , resource_id ) : 
self . logger . debug ( 'elasticsearch::delete::{}' . format ( resource_id ) ) 
if self . driver . _es . exists ( 
) == False : 
~~ return self . driver . _es . delete ( 
~~ def list ( self , search_from = None , search_to = None , limit = None ) : 
self . logger . debug ( 'elasticsearch::list' ) 
body = { 
'sort' : [ 
{ "_id" : "asc" } , 
'query' : { 
'match_all' : { } 
if search_from : 
~~~ body [ 'from' ] = search_from 
~~ if search_to : 
~~~ body [ 'size' ] = search_to - search_from 
~~ if limit : 
~~~ body [ 'size' ] = limit 
~~ page = self . driver . _es . search ( 
body = body 
object_list = [ ] 
for x in page [ 'hits' ] [ 'hits' ] : 
~~~ object_list . append ( x [ '_source' ] ) 
~~ return object_list 
~~ def query ( self , search_model : QueryModel ) : 
query_parsed = query_parser ( search_model . query ) 
self . logger . debug ( f'elasticsearch::query::{query_parsed[0]}' ) 
if search_model . sort is not None : 
~~~ self . _mapping_to_sort ( search_model . sort . keys ( ) ) 
sort = self . _sort_object ( search_model . sort ) 
~~~ sort = [ { "_id" : "asc" } ] 
~~ if search_model . query == { } : 
~~~ query = { 'match_all' : { } } 
~~~ query = query_parsed [ 0 ] 
~~ body = { 
'query' : query , 
'sort' : sort , 
'from' : ( search_model . page - 1 ) * search_model . offset , 
'size' : search_model . offset , 
page = self . driver . _es . search ( 
body = body , 
q = query_parsed [ 1 ] 
~~ def text_query ( self , search_model : FullTextModel ) : 
self . logger . debug ( 'elasticsearch::text_query::{}' . format ( search_model . text ) ) 
~~~ sort = [ { "service.metadata.curation.rating" : "asc" } ] 
q = search_model . text 
~~ def forwards ( self , orm ) : 
for title in orm [ 'hero_slider.SliderItemTitle' ] . objects . all ( ) : 
~~~ title . is_published = True 
title . save ( ) 
~~ ~~ def get_slider_items ( context , amount = None ) : 
req = context . get ( 'request' ) 
qs = SliderItem . objects . published ( req ) . order_by ( 'position' ) 
if amount : 
~~~ qs = qs [ : amount ] 
~~ return qs 
~~ def render_hero_slider ( context ) : 
'slider_items' : qs , 
~~ def reader_acquire ( self ) : 
self . _order_mutex . acquire ( ) 
self . _readers_mutex . acquire ( ) 
if self . _readers == 0 : 
~~~ self . _access_mutex . acquire ( ) 
~~ self . _readers += 1 
self . _order_mutex . release ( ) 
self . _readers_mutex . release ( ) 
~~ def reader_release ( self ) : 
self . _readers -= 1 
~~~ self . _access_mutex . release ( ) 
~~ self . _readers_mutex . release ( ) 
~~ def writer_acquire ( self ) : 
self . _access_mutex . acquire ( ) 
~~ def add ( self , task_id , backend , category , backend_args , 
archiving_cfg = None , scheduling_cfg = None ) : 
self . _rwlock . writer_acquire ( ) 
if task_id in self . _tasks : 
~~~ self . _rwlock . writer_release ( ) 
raise AlreadyExistsError ( element = str ( task_id ) ) 
~~ task = Task ( task_id , backend , category , backend_args , 
archiving_cfg = archiving_cfg , 
scheduling_cfg = scheduling_cfg ) 
self . _tasks [ task_id ] = task 
self . _rwlock . writer_release ( ) 
return task 
~~ def remove ( self , task_id ) : 
~~~ self . _rwlock . writer_acquire ( ) 
del self . _tasks [ task_id ] 
~~~ raise NotFoundError ( element = str ( task_id ) ) 
~~ def get ( self , task_id ) : 
~~~ self . _rwlock . reader_acquire ( ) 
task = self . _tasks [ task_id ] 
~~~ self . _rwlock . reader_release ( ) 
~~ return task 
~~ def tasks ( self ) : 
self . _rwlock . reader_acquire ( ) 
tl = [ v for v in self . _tasks . values ( ) ] 
tl . sort ( key = lambda x : x . task_id ) 
self . _rwlock . reader_release ( ) 
return tl 
properties = find_class_properties ( self . __class__ ) 
config = { 
name : self . __getattribute__ ( name ) for name , _ in properties 
return config 
~~ def from_dict ( cls , config ) : 
~~~ obj = cls ( ** config ) 
~~ except TypeError as e : 
~~~ m = cls . KW_ARGS_ERROR_REGEX . match ( str ( e ) ) 
if m : 
~~~ return obj 
~~ ~~ def metadata ( func ) : 
@ functools . wraps ( func ) 
def decorator ( self , * args , ** kwargs ) : 
~~~ for item in func ( self , * args , ** kwargs ) : 
~~~ item [ 'arthur_version' ] = __version__ 
item [ 'job_id' ] = self . job_id 
yield item 
~~ ~~ return decorator 
~~ def execute_perceval_job ( backend , backend_args , qitems , task_id , category , 
archive_args = None , max_retries = MAX_JOB_RETRIES ) : 
rq_job = rq . get_current_job ( ) 
job = PercevalJob ( rq_job . id , task_id , backend , category , 
rq_job . connection , qitems ) 
job . job_id , task_id , backend , category ) 
if not job . has_archiving ( ) and archive_args : 
~~ run_job = True 
resume = False 
failures = 0 
while run_job : 
~~~ job . run ( backend_args , archive_args = archive_args , resume = resume ) 
~~ except AttributeError as e : 
job . job_id , backend , str ( e ) ) 
failures += 1 
if not job . has_resuming ( ) or failures >= max_retries : 
job . job_id , task_id , backend ) 
raise e 
job . job_id , task_id , backend , failures , max_retries ) 
resume = True 
~~~ run_job = False 
~~ ~~ result = job . result 
result . job_id , task_id , result . backend , str ( result . nitems ) , result . category ) 
~~ def initialize_archive_manager ( self , archive_path ) : 
if archive_path == "" : 
~~ if archive_path : 
~~~ self . archive_manager = perceval . archive . ArchiveManager ( archive_path ) 
~~ ~~ def run ( self , backend_args , archive_args = None , resume = False ) : 
args = backend_args . copy ( ) 
if archive_args : 
~~~ self . initialize_archive_manager ( archive_args [ 'archive_path' ] ) 
~~ if not resume : 
~~~ max_date = backend_args . get ( 'from_date' , None ) 
offset = backend_args . get ( 'offset' , None ) 
if max_date : 
~~~ max_date = datetime_to_utc ( max_date ) . timestamp ( ) 
~~ self . _result = JobResult ( self . job_id , self . task_id , self . backend , self . category , 
None , max_date , 0 , offset = offset , 
nresumed = 0 ) 
~~~ if self . result . max_date : 
~~~ args [ 'from_date' ] = unixtime_to_datetime ( self . result . max_date ) 
~~ if self . result . offset : 
~~~ args [ 'offset' ] = self . result . offset 
~~ self . _result . nresumed += 1 
~~ for item in self . _execute ( args , archive_args ) : 
~~~ self . conn . rpush ( self . qitems , pickle . dumps ( item ) ) 
self . _result . nitems += 1 
self . _result . last_uuid = item [ 'uuid' ] 
if not self . result . max_date or self . result . max_date < item [ 'updated_on' ] : 
~~~ self . _result . max_date = item [ 'updated_on' ] 
~~ if 'offset' in item : 
~~~ self . _result . offset = item [ 'offset' ] 
~~ ~~ ~~ def _execute ( self , backend_args , archive_args ) : 
if not archive_args or not archive_args [ 'fetch_from_archive' ] : 
~~~ return perceval . backend . fetch ( self . _bklass , backend_args , self . category , 
manager = self . archive_manager ) 
~~~ return perceval . backend . fetch_from_archive ( self . _bklass , backend_args , 
self . archive_manager , self . category , 
archive_args [ 'archived_after' ] ) 
~~ ~~ def create_index ( idx_url , clean = False ) : 
~~~ r = requests . get ( idx_url ) 
~~ except requests . exceptions . ConnectionError : 
raise ElasticSearchError ( cause = cause ) 
~~ if r . status_code != 200 : 
~~~ r = requests . put ( idx_url ) 
if r . status_code != 200 : 
~~~ logger . info ( "Can\ , idx_url , r . status_code ) 
~~ elif r . status_code == 200 and clean : 
~~~ requests . delete ( idx_url ) 
requests . put ( idx_url ) 
~~ def create_mapping ( idx_url , mapping ) : 
mapping_url = idx_url + '/items/_mapping' 
mapping = json . dumps ( mapping ) 
~~~ r = requests . put ( mapping_url , data = mapping , 
headers = { 'Content-Type' : 'application/json' } ) 
~~~ reason = r . json ( ) [ 'error' ] 
logger . info ( "Can\ , 
mapping_url , reason ) 
~~ ~~ def json_encoder ( * args , ** kwargs ) : 
obj = cherrypy . serving . request . _json_inner_handler ( * args , ** kwargs ) 
for chunk in JSONEncoder ( ) . iterencode ( obj ) : 
~~~ yield chunk . encode ( 'utf-8' ) 
~~ ~~ def write_items ( cls , writer , items_generator ) : 
while True : 
~~~ items = items_generator ( ) 
writer . write ( items ) 
time . sleep ( 1 ) 
~~ ~~ def add ( self ) : 
payload = cherrypy . request . json 
for task_data in payload [ 'tasks' ] : 
~~~ category = task_data [ 'category' ] 
backend_args = task_data [ 'backend_args' ] 
archive_args = task_data . get ( 'archive' , None ) 
sched_args = task_data . get ( 'scheduler' , None ) 
~~ except KeyError as ex : 
raise ex 
~~ from_date = backend_args . get ( 'from_date' , None ) 
if from_date : 
~~~ backend_args [ 'from_date' ] = str_to_datetime ( from_date ) 
~~ super ( ) . add_task ( task_data [ 'task_id' ] , 
task_data [ 'backend' ] , 
category , 
backend_args , 
archive_args = archive_args , 
sched_args = sched_args ) 
~~ def remove ( self ) : 
task_ids = { } 
~~~ task_id = task_data [ 'task_id' ] 
removed = super ( ) . remove_task ( task_id ) 
task_ids [ task_id ] = removed 
~~ result = { 'tasks' : task_ids } 
result = [ task . to_dict ( ) for task in self . _tasks . tasks ] 
result = { 'tasks' : result } 
~~ def add_task ( self , task_id , backend , category , backend_args , 
archive_args = None , sched_args = None ) : 
~~~ archiving_cfg = self . __parse_archive_args ( archive_args ) 
scheduling_cfg = self . __parse_schedule_args ( sched_args ) 
self . __validate_args ( task_id , backend , category , backend_args ) 
~~ except ValueError as e : 
~~~ task = self . _tasks . add ( task_id , backend , category , backend_args , 
~~ except AlreadyExistsError as e : 
~~ self . _scheduler . schedule_task ( task . task_id ) 
~~ def remove_task ( self , task_id ) : 
~~~ self . _scheduler . cancel_task ( task_id ) 
~~ except NotFoundError as e : 
task_id ) 
~~ ~~ def items ( self ) : 
pipe = self . conn . pipeline ( ) 
pipe . lrange ( Q_STORAGE_ITEMS , 0 , - 1 ) 
pipe . ltrim ( Q_STORAGE_ITEMS , 1 , 0 ) 
items = pipe . execute ( ) [ 0 ] 
for item in items : 
~~~ item = pickle . loads ( item ) 
~~ ~~ def __validate_args ( task_id , backend , category , backend_args ) : 
if not task_id or task_id . strip ( ) == "" : 
raise ValueError ( msg ) 
~~ if not backend or backend . strip ( ) == "" : 
~~ if backend_args and not isinstance ( backend_args , dict ) : 
~~ if not category or category . strip ( ) == "" : 
~~ ~~ def __parse_archive_args ( self , archive_args ) : 
if not archive_args : 
~~ archiving_args = copy . deepcopy ( archive_args ) 
if self . archive_path : 
~~~ archiving_args [ 'archive_path' ] = self . archive_path 
~~~ archiving_args [ 'archive_path' ] = os . path . expanduser ( ARCHIVES_DEFAULT_PATH ) 
~~ return ArchivingTaskConfig . from_dict ( archiving_args ) 
~~ def perform_job ( self , job , queue ) : 
result = super ( ) . perform_job ( job , queue ) 
job_status = job . get_status ( ) 
job_result = job . return_value if job_status == 'finished' else None 
'job_id' : job . id , 
'status' : job_status , 
'result' : job_result 
msg = pickle . dumps ( data ) 
self . connection . publish ( self . pubsub_channel , msg ) 
~~ def schedule_job_task ( self , queue_id , task_id , job_args , delay = 0 ) : 
job_id = self . _generate_job_id ( task_id ) 
event = self . _scheduler . enter ( delay , 1 , self . _enqueue_job , 
argument = ( queue_id , job_id , job_args , ) ) 
self . _jobs [ job_id ] = event 
self . _tasks [ task_id ] = job_id 
job_id , task_id , queue_id , delay ) 
return job_id 
~~ def cancel_job_task ( self , task_id ) : 
job_id = self . _tasks . get ( task_id , None ) 
if job_id : 
~~~ self . _cancel_job ( job_id ) 
~~ ~~ finally : 
~~ ~~ def run ( self ) : 
~~~ self . listen ( ) 
logger . critical ( traceback . format_exc ( ) ) 
~~ ~~ def listen ( self ) : 
pubsub = self . conn . pubsub ( ) 
pubsub . subscribe ( self . pubsub_channel ) 
for msg in pubsub . listen ( ) : 
if msg [ 'type' ] != 'message' : 
~~ data = pickle . loads ( msg [ 'data' ] ) 
job_id = data [ 'job_id' ] 
job = rq . job . Job . fetch ( job_id , connection = self . conn ) 
if data [ 'status' ] == 'finished' : 
handler = self . result_handler 
~~ elif data [ 'status' ] == 'failed' : 
handler = self . result_handler_err 
~~ if handler : 
handler ( job ) 
~~ ~~ ~~ def schedule ( self ) : 
if self . async_mode : 
~~~ self . _scheduler . start ( ) 
self . _listener . start ( ) 
~~~ self . _scheduler . schedule ( ) 
~~ ~~ def schedule_task ( self , task_id ) : 
task = self . registry . get ( task_id ) 
job_args = self . _build_job_arguments ( task ) 
archiving_cfg = task . archiving_cfg 
fetch_from_archive = False if not archiving_cfg else archiving_cfg . fetch_from_archive 
queue = Q_ARCHIVE_JOBS if fetch_from_archive else Q_CREATION_JOBS 
job_id = self . _scheduler . schedule_job_task ( queue , 
task . task_id , job_args , 
delay = 0 ) 
~~ def cancel_task ( self , task_id ) : 
self . registry . remove ( task_id ) 
self . _scheduler . cancel_job_task ( task_id ) 
~~ def _handle_successful_job ( self , job ) : 
result = job . result 
task_id = job . kwargs [ 'task_id' ] 
~~~ task = self . registry . get ( task_id ) 
~~ except NotFoundError : 
task_id , job . id ) 
~~ if task . archiving_cfg and task . archiving_cfg . fetch_from_archive : 
~~ if result . nitems > 0 : 
~~~ task . backend_args [ 'next_from_date' ] = unixtime_to_datetime ( result . max_date ) 
if result . offset : 
~~~ task . backend_args [ 'next_offset' ] = result . offset 
~~ ~~ job_args = self . _build_job_arguments ( task ) 
delay = task . scheduling_cfg . delay if task . scheduling_cfg else WAIT_FOR_QUEUING 
job_id = self . _scheduler . schedule_job_task ( Q_UPDATING_JOBS , 
task_id , job_args , 
delay = delay ) 
job_id , task_id , job . id ) 
~~ def _handle_failed_job ( self , job ) : 
job . id , task_id ) 
~~ def _build_job_arguments ( task ) : 
job_args = { } 
job_args [ 'qitems' ] = Q_STORAGE_ITEMS 
job_args [ 'task_id' ] = task . task_id 
job_args [ 'backend' ] = task . backend 
backend_args = copy . deepcopy ( task . backend_args ) 
if 'next_from_date' in backend_args : 
~~~ backend_args [ 'from_date' ] = backend_args . pop ( 'next_from_date' ) 
~~ if 'next_offset' in backend_args : 
~~~ backend_args [ 'offset' ] = backend_args . pop ( 'next_offset' ) 
~~ job_args [ 'backend_args' ] = backend_args 
job_args [ 'category' ] = task . category 
job_args [ 'archive_args' ] = archiving_cfg . to_dict ( ) if archiving_cfg else None 
sched_cfg = task . scheduling_cfg 
job_args [ 'max_retries' ] = sched_cfg . max_retries if sched_cfg else MAX_JOB_RETRIES 
return job_args 
~~ def create_field ( field_info ) : 
field_type = field_info . get ( 'type' ) 
if field_type not in FIELDS_NAME_MAP : 
~~ field_class = FIELDS_NAME_MAP . get ( field_type ) 
params = dict ( field_info ) 
params . pop ( 'type' ) 
return field_class . from_dict ( params ) 
~~ def create_validator ( data_struct_dict , name = None ) : 
if name is None : 
~~~ name = 'FromDictValidator' 
~~ attrs = { } 
for field_name , field_info in six . iteritems ( data_struct_dict ) : 
~~~ field_type = field_info [ 'type' ] 
if field_type == DictField . FIELD_TYPE_NAME and isinstance ( field_info . get ( 'validator' ) , dict ) : 
~~~ field_info [ 'validator' ] = create_validator ( field_info [ 'validator' ] ) 
~~ attrs [ field_name ] = create_field ( field_info ) 
~~ name = force_str ( name ) 
return type ( name , ( Validator , ) , attrs ) 
~~ def cartesian_product ( parameter_dict , combined_parameters = ( ) ) : 
if not combined_parameters : 
~~~ combined_parameters = list ( parameter_dict ) 
~~~ combined_parameters = list ( combined_parameters ) 
~~ for idx , item in enumerate ( combined_parameters ) : 
~~~ if isinstance ( item , str ) : 
~~~ combined_parameters [ idx ] = ( item , ) 
~~ ~~ iterator_list = [ ] 
for item_tuple in combined_parameters : 
~~~ inner_iterator_list = [ parameter_dict [ key ] for key in item_tuple ] 
zipped_iterator = zip ( * inner_iterator_list ) 
iterator_list . append ( zipped_iterator ) 
~~ result_dict = { } 
for key in parameter_dict : 
~~~ result_dict [ key ] = [ ] 
~~ cartesian_iterator = itools . product ( * iterator_list ) 
for cartesian_tuple in cartesian_iterator : 
~~~ for idx , item_tuple in enumerate ( combined_parameters ) : 
~~~ for inneridx , key in enumerate ( item_tuple ) : 
~~~ result_dict [ key ] . append ( cartesian_tuple [ idx ] [ inneridx ] ) 
~~ ~~ ~~ return result_dict 
~~ def find_unique_points ( explored_parameters ) : 
ranges = [ param . f_get_range ( copy = False ) for param in explored_parameters ] 
zipped_tuples = list ( zip ( * ranges ) ) 
~~~ unique_elements = OrderedDict ( ) 
for idx , val_tuple in enumerate ( zipped_tuples ) : 
~~~ if val_tuple not in unique_elements : 
~~~ unique_elements [ val_tuple ] = [ ] 
~~ unique_elements [ val_tuple ] . append ( idx ) 
~~ return list ( unique_elements . items ( ) ) 
~~~ logger = logging . getLogger ( 'pypet.find_unique' ) 
unique_elements = [ ] 
~~~ matches = False 
for added_tuple , pos_list in unique_elements : 
~~~ matches = True 
for idx2 , val in enumerate ( added_tuple ) : 
~~~ if not explored_parameters [ idx2 ] . _equal_values ( val_tuple [ idx2 ] , val ) : 
~~ ~~ if matches : 
~~~ pos_list . append ( idx ) 
~~ ~~ if not matches : 
~~~ unique_elements . append ( ( val_tuple , [ idx ] ) ) 
~~ ~~ return unique_elements 
~~ ~~ def _change_logging_kwargs ( kwargs ) : 
log_levels = kwargs . pop ( 'log_level' , None ) 
log_folder = kwargs . pop ( 'log_folder' , 'logs' ) 
logger_names = kwargs . pop ( 'logger_names' , '' ) 
if log_levels is None : 
~~~ log_levels = kwargs . pop ( 'log_levels' , logging . INFO ) 
~~ log_multiproc = kwargs . pop ( 'log_multiproc' , True ) 
if not isinstance ( logger_names , ( tuple , list ) ) : 
~~~ logger_names = [ logger_names ] 
~~ if not isinstance ( log_levels , ( tuple , list ) ) : 
~~~ log_levels = [ log_levels ] 
~~ if len ( log_levels ) == 1 : 
~~~ log_levels = [ log_levels [ 0 ] for _ in logger_names ] 
~~ dictionary = copy . deepcopy ( LOGGING_DICT ) 
prefixes = [ '' ] 
if not log_multiproc : 
~~~ for key in list ( dictionary . keys ( ) ) : 
~~~ if key . startswith ( 'multiproc_' ) : 
~~~ del dictionary [ key ] 
~~~ prefixes . append ( 'multiproc_' ) 
~~ for prefix in prefixes : 
~~~ for handler_dict in dictionary [ prefix + 'handlers' ] . values ( ) : 
~~~ if 'filename' in handler_dict : 
~~~ filename = os . path . join ( log_folder , handler_dict [ 'filename' ] ) 
filename = os . path . normpath ( filename ) 
handler_dict [ 'filename' ] = filename 
~~ ~~ dictionary [ prefix + 'loggers' ] = { } 
logger_dict = dictionary [ prefix + 'loggers' ] 
for idx , logger_name in enumerate ( logger_names ) : 
~~~ logger_dict [ logger_name ] = { 
'level' : log_levels [ idx ] , 
'handlers' : list ( dictionary [ prefix + 'handlers' ] . keys ( ) ) 
~~ ~~ kwargs [ 'log_config' ] = dictionary 
~~ def simple_logging_config ( func ) : 
def new_func ( self , * args , ** kwargs ) : 
~~~ if use_simple_logging ( kwargs ) : 
~~~ if 'log_config' in kwargs : 
~~ _change_logging_kwargs ( kwargs ) 
~~ return func ( self , * args , ** kwargs ) 
~~ return new_func 
~~ def try_make_dirs ( filename ) : 
~~~ dirname = os . path . dirname ( os . path . normpath ( filename ) ) 
racedirs ( dirname ) 
~~ except Exception as exc : 
~~ ~~ def get_strings ( args ) : 
string_list = [ ] 
for elem in ast . walk ( ast . parse ( args ) ) : 
~~~ if isinstance ( elem , ast . Str ) : 
~~~ string_list . append ( elem . s ) 
~~ ~~ return string_list 
~~ def rename_log_file ( filename , trajectory = None , 
env_name = None , 
traj_name = None , 
set_name = None , 
run_name = None , 
process_name = None , 
host_name = None ) : 
if pypetconstants . LOG_ENV in filename : 
~~~ if env_name is None : 
~~~ env_name = trajectory . v_environment_name 
~~ filename = filename . replace ( pypetconstants . LOG_ENV , env_name ) 
~~ if pypetconstants . LOG_TRAJ in filename : 
~~~ if traj_name is None : 
~~~ traj_name = trajectory . v_name 
~~ filename = filename . replace ( pypetconstants . LOG_TRAJ , traj_name ) 
~~ if pypetconstants . LOG_RUN in filename : 
~~~ if run_name is None : 
~~~ run_name = trajectory . f_wildcard ( '$' ) 
~~ filename = filename . replace ( pypetconstants . LOG_RUN , run_name ) 
~~ if pypetconstants . LOG_SET in filename : 
~~~ if set_name is None : 
~~~ set_name = trajectory . f_wildcard ( '$set' ) 
~~ filename = filename . replace ( pypetconstants . LOG_SET , set_name ) 
~~ if pypetconstants . LOG_PROC in filename : 
~~~ if process_name is None : 
~~~ process_name = multip . current_process ( ) . name + '-' + str ( os . getpid ( ) ) 
~~ filename = filename . replace ( pypetconstants . LOG_PROC , process_name ) 
~~ if pypetconstants . LOG_HOST in filename : 
~~~ if host_name is None : 
~~~ host_name = socket . getfqdn ( ) . replace ( '.' , '-' ) 
~~ filename = filename . replace ( pypetconstants . LOG_HOST , host_name ) 
~~ def _set_logger ( self , name = None ) : 
~~~ cls = self . __class__ 
name = '%s.%s' % ( cls . __module__ , cls . __name__ ) 
~~ self . _logger = logging . getLogger ( name ) 
~~ def extract_replacements ( self , trajectory ) : 
self . env_name = trajectory . v_environment_name 
self . traj_name = trajectory . v_name 
self . set_name = trajectory . f_wildcard ( '$set' ) 
self . run_name = trajectory . f_wildcard ( '$' ) 
~~ def show_progress ( self , n , total_runs ) : 
if self . report_progress : 
~~~ percentage , logger_name , log_level = self . report_progress 
if logger_name == 'print' : 
~~~ logger = 'print' 
~~~ logger = logging . getLogger ( logger_name ) 
~~ if n == - 1 : 
~~~ digits = int ( math . log10 ( total_runs + 0.1 ) ) + 1 
~~ fmt_string = self . _format_string % ( n + 1 , total_runs ) + '%s' 
reprint = log_level == 0 
progressbar ( n , total_runs , percentage_step = percentage , 
logger = logger , log_level = log_level , 
fmt_string = fmt_string , reprint = reprint ) 
~~ ~~ def _check_and_replace_parser_args ( parser , section , option , rename_func , make_dirs = True ) : 
args = parser . get ( section , option , raw = True ) 
strings = get_strings ( args ) 
replace = False 
for string in strings : 
~~~ isfilename = any ( x in string for x in FILENAME_INDICATORS ) 
if isfilename : 
~~~ newstring = rename_func ( string ) 
if make_dirs : 
~~~ try_make_dirs ( newstring ) 
~~ raw_string = string . replace ( '\\\\' , '\\\\\\\\' ) 
raw_newstring = newstring . replace ( '\\\\' , '\\\\\\\\' ) 
args = args . replace ( raw_string , raw_newstring ) 
replace = True 
~~ ~~ if replace : 
~~~ parser . set ( section , option , args ) 
~~ ~~ def _parser_to_string_io ( parser ) : 
memory_file = StringIO ( ) 
parser . write ( memory_file ) 
memory_file . flush ( ) 
memory_file . seek ( 0 ) 
return memory_file 
~~ def _find_multiproc_options ( parser ) : 
sections = parser . sections ( ) 
if not any ( section . startswith ( 'multiproc_' ) for section in sections ) : 
~~ mp_parser = NoInterpolationParser ( ) 
for section in sections : 
~~~ if section . startswith ( 'multiproc_' ) : 
~~~ new_section = section . replace ( 'multiproc_' , '' ) 
mp_parser . add_section ( new_section ) 
options = parser . options ( section ) 
for option in options : 
~~~ val = parser . get ( section , option , raw = True ) 
mp_parser . set ( new_section , option , val ) 
~~ ~~ ~~ return mp_parser 
~~ def _find_multiproc_dict ( dictionary ) : 
if not any ( key . startswith ( 'multiproc_' ) for key in dictionary . keys ( ) ) : 
~~ mp_dictionary = { } 
for key in dictionary . keys ( ) : 
~~~ new_key = key . replace ( 'multiproc_' , '' ) 
mp_dictionary [ new_key ] = dictionary [ key ] 
~~ ~~ mp_dictionary [ 'version' ] = dictionary [ 'version' ] 
if 'disable_existing_loggers' in dictionary : 
~~~ mp_dictionary [ 'disable_existing_loggers' ] = dictionary [ 'disable_existing_loggers' ] 
~~ return mp_dictionary 
~~ def check_log_config ( self ) : 
~~~ if self . report_progress is True : 
~~~ self . report_progress = ( 5 , 'pypet' , logging . INFO ) 
~~ elif isinstance ( self . report_progress , ( int , float ) ) : 
~~~ self . report_progress = ( self . report_progress , 'pypet' , logging . INFO ) 
~~ elif isinstance ( self . report_progress , str ) : 
~~~ self . report_progress = ( 5 , self . report_progress , logging . INFO ) 
~~ elif len ( self . report_progress ) == 2 : 
~~~ self . report_progress = ( self . report_progress [ 0 ] , self . report_progress [ 1 ] , 
logging . INFO ) 
~~ ~~ if self . log_config : 
~~~ if self . log_config == pypetconstants . DEFAULT_LOGGING : 
~~~ pypet_path = os . path . abspath ( os . path . dirname ( __file__ ) ) 
init_path = os . path . join ( pypet_path , 'logging' ) 
self . log_config = os . path . join ( init_path , 'default.ini' ) 
~~ if isinstance ( self . log_config , str ) : 
~~~ if not os . path . isfile ( self . log_config ) : 
'`%s`.' % self . log_config ) 
~~ parser = NoInterpolationParser ( ) 
parser . read ( self . log_config ) 
~~ elif isinstance ( self . log_config , cp . RawConfigParser ) : 
~~~ parser = self . log_config 
~~~ parser = None 
~~ if parser is not None : 
~~~ self . _sp_config = self . _parser_to_string_io ( parser ) 
self . _mp_config = self . _find_multiproc_options ( parser ) 
if self . _mp_config is not None : 
~~~ self . _mp_config = self . _parser_to_string_io ( self . _mp_config ) 
~~ ~~ elif isinstance ( self . log_config , dict ) : 
~~~ self . _sp_config = self . log_config 
self . _mp_config = self . _find_multiproc_dict ( self . _sp_config ) 
~~ ~~ if self . log_stdout : 
~~~ if self . log_stdout is True : 
~~~ self . log_stdout = ( 'STDOUT' , logging . INFO ) 
~~ if isinstance ( self . log_stdout , str ) : 
~~~ self . log_stdout = ( self . log_stdout , logging . INFO ) 
~~ if isinstance ( self . log_stdout , int ) : 
~~~ self . log_stdout = ( 'STDOUT' , self . log_stdout ) 
~~ ~~ ~~ def _handle_config_parsing ( self , log_config ) : 
parser = NoInterpolationParser ( ) 
parser . readfp ( log_config ) 
rename_func = lambda string : rename_log_file ( string , 
env_name = self . env_name , 
traj_name = self . traj_name , 
set_name = self . set_name , 
run_name = self . run_name ) 
~~~ options = parser . options ( section ) 
~~~ if option == 'args' : 
~~~ self . _check_and_replace_parser_args ( parser , section , option , 
rename_func = rename_func ) 
~~ ~~ ~~ return parser 
~~ def _handle_dict_config ( self , log_config ) : 
new_dict = dict ( ) 
for key in log_config . keys ( ) : 
~~~ if key == 'filename' : 
~~~ filename = log_config [ key ] 
filename = rename_log_file ( filename , 
new_dict [ key ] = filename 
try_make_dirs ( filename ) 
~~ elif isinstance ( log_config [ key ] , dict ) : 
~~~ inner_dict = self . _handle_dict_config ( log_config [ key ] ) 
new_dict [ key ] = inner_dict 
~~~ new_dict [ key ] = log_config [ key ] 
~~ ~~ return new_dict 
~~ def make_logging_handlers_and_tools ( self , multiproc = False ) : 
log_stdout = self . log_stdout 
if sys . stdout is self . _stdout_to_logger : 
~~~ log_stdout = False 
~~ if self . log_config : 
~~~ if multiproc : 
~~~ proc_log_config = self . _mp_config 
~~~ proc_log_config = self . _sp_config 
~~ if proc_log_config : 
~~~ if isinstance ( proc_log_config , dict ) : 
~~~ new_dict = self . _handle_dict_config ( proc_log_config ) 
dictConfig ( new_dict ) 
~~~ parser = self . _handle_config_parsing ( proc_log_config ) 
memory_file = self . _parser_to_string_io ( parser ) 
fileConfig ( memory_file , disable_existing_loggers = False ) 
~~ ~~ ~~ if log_stdout : 
~~~ std_name , std_level = self . log_stdout 
stdout = StdoutToLogger ( std_name , log_level = std_level ) 
stdout . start ( ) 
self . _tools . append ( stdout ) 
~~ ~~ def finalize ( self , remove_all_handlers = True ) : 
for tool in self . _tools : 
~~~ tool . finalize ( ) 
~~ self . _tools = [ ] 
self . _stdout_to_logger = None 
for config in ( self . _sp_config , self . _mp_config ) : 
~~~ if hasattr ( config , 'close' ) : 
~~~ config . close ( ) 
~~ ~~ self . _sp_config = None 
self . _mp_config = None 
if remove_all_handlers : 
~~~ self . tabula_rasa ( ) 
~~ ~~ def start ( self ) : 
if sys . stdout is not self : 
~~~ self . _original_steam = sys . stdout 
sys . stdout = self 
self . _redirection = True 
~~ if self . _redirection : 
~~ ~~ def write ( self , buf ) : 
if not self . _recursion : 
~~~ self . _recursion = True 
~~~ for line in buf . rstrip ( ) . splitlines ( ) : 
~~~ self . _logger . log ( self . _log_level , line . rstrip ( ) ) 
~~~ self . _recursion = False 
~~ ~~ def finalize ( self ) : 
if self . _original_steam is not None and self . _redirection : 
~~~ sys . stdout = self . _original_steam 
self . _redirection = False 
self . _original_steam = None 
~~ ~~ def results_equal ( a , b ) : 
if a . v_is_parameter and b . v_is_parameter : 
~~ if a . v_is_parameter or b . v_is_parameter : 
~~ if a . v_full_name != b . v_full_name : 
~~ if hasattr ( a , '_data' ) and not hasattr ( b , '_data' ) : 
~~ if hasattr ( a , '_data' ) : 
~~~ akeyset = set ( a . _data . keys ( ) ) 
bkeyset = set ( b . _data . keys ( ) ) 
if akeyset != bkeyset : 
~~ for key in a . _data : 
~~~ val = a . _data [ key ] 
bval = b . _data [ key ] 
if not nested_equal ( val , bval ) : 
~~ ~~ ~~ return True 
~~ def parameters_equal ( a , b ) : 
if ( not b . v_is_parameter and 
not a . v_is_parameter ) : 
~~ if ( not b . v_is_parameter or 
~~ if a . f_is_empty ( ) and b . f_is_empty ( ) : 
~~ if a . f_is_empty ( ) != b . f_is_empty ( ) : 
~~ if not a . _values_of_same_type ( a . f_get ( ) , b . f_get ( ) ) : 
~~ if not a . _equal_values ( a . f_get ( ) , b . f_get ( ) ) : 
~~ if a . f_has_range ( ) != b . f_has_range ( ) : 
~~ if a . f_has_range ( ) : 
~~~ if a . f_get_range_length ( ) != b . f_get_range_length ( ) : 
~~ for myitem , bitem in zip ( a . f_get_range ( copy = False ) , b . f_get_range ( copy = False ) ) : 
~~~ if not a . _values_of_same_type ( myitem , bitem ) : 
~~ if not a . _equal_values ( myitem , bitem ) : 
~~ def get_all_attributes ( instance ) : 
~~~ result_dict = instance . __dict__ . copy ( ) 
~~ except AttributeError : 
~~~ result_dict = { } 
~~ if hasattr ( instance , '__all_slots__' ) : 
~~~ all_slots = instance . __all_slots__ 
~~~ all_slots = slots . get_all_slots ( instance . __class__ ) 
~~ for slot in all_slots : 
~~~ result_dict [ slot ] = getattr ( instance , slot ) 
~~ result_dict . pop ( '__dict__' , None ) 
result_dict . pop ( '__weakref__' , None ) 
return result_dict 
~~ def nested_equal ( a , b ) : 
if a is b : 
~~ if a is None or b is None : 
~~ a_sparse = spsp . isspmatrix ( a ) 
b_sparse = spsp . isspmatrix ( b ) 
if a_sparse != b_sparse : 
~~ if a_sparse : 
~~~ if a . nnz == 0 : 
~~~ return b . nnz == 0 
~~~ return not np . any ( ( a != b ) . data ) 
~~ ~~ a_series = isinstance ( a , pd . Series ) 
b_series = isinstance ( b , pd . Series ) 
if a_series != b_series : 
~~ if a_series : 
~~~ eq = ( a == b ) . all ( ) 
return eq 
~~ except ( TypeError , ValueError ) : 
~~~ if not len ( a ) == len ( b ) : 
~~ for idx , itema in enumerate ( a ) : 
~~~ itemb = b [ idx ] 
if not nested_equal ( itema , itemb ) : 
~~ ~~ a_frame = isinstance ( a , pd . DataFrame ) 
b_frame = isinstance ( b , pd . DataFrame ) 
if a_frame != b_frame : 
~~ if a_frame : 
~~~ if a . empty and b . empty : 
~~ new_frame = a == b 
new_frame = new_frame | ( pd . isnull ( a ) & pd . isnull ( b ) ) 
if isinstance ( new_frame , pd . DataFrame ) : 
~~~ return np . all ( new_frame . as_matrix ( ) ) 
~~ ~~ except ( ValueError , TypeError ) : 
~~~ for name in a : 
~~~ cola = a [ name ] 
if not name in b : 
~~ colb = b [ name ] 
if not len ( cola ) == len ( colb ) : 
~~ for idx , itema in enumerate ( cola ) : 
~~~ itemb = colb [ idx ] 
~~ ~~ a_array = isinstance ( a , np . ndarray ) 
b_array = isinstance ( b , np . ndarray ) 
if a_array != b_array : 
~~ if a_array : 
~~~ if a . shape != b . shape : 
~~ return np . all ( a == b ) 
~~ a_list = isinstance ( a , ( Sequence , list , tuple ) ) 
b_list = isinstance ( b , ( Sequence , list , tuple ) ) 
if a_list != b_list : 
~~ if a_list : 
~~~ return all ( nested_equal ( x , y ) for x , y in zip ( a , b ) ) 
~~ a_mapping = isinstance ( a , ( Mapping , dict ) ) 
b_mapping = isinstance ( b , ( Mapping , dict ) ) 
if a_mapping != b_mapping : 
~~ if a_mapping : 
~~~ keys_a = a . keys ( ) 
if set ( keys_a ) != set ( b . keys ( ) ) : 
~~ return all ( nested_equal ( a [ k ] , b [ k ] ) for k in keys_a ) 
~~ equality = NotImplemented 
~~~ equality = a . __eq__ ( b ) 
~~ except ( AttributeError , NotImplementedError , TypeError , ValueError ) : 
~~ if equality is NotImplemented : 
~~~ equality = b . __eq__ ( a ) 
~~ ~~ if equality is NotImplemented : 
~~~ cmp = a . __cmp__ ( b ) 
if cmp is not NotImplemented : 
~~~ equality = cmp == 0 
~~ ~~ except ( AttributeError , NotImplementedError , TypeError , ValueError ) : 
~~~ cmp = b . __cmp__ ( a ) 
~~ ~~ if equality is not NotImplemented : 
~~~ return bool ( equality ) 
~~ ~~ attributes_a = get_all_attributes ( a ) 
attributes_b = get_all_attributes ( b ) 
if len ( attributes_a ) != len ( attributes_b ) : 
~~ if len ( attributes_a ) > 0 : 
~~~ keys_a = list ( attributes_a . keys ( ) ) 
if set ( keys_a ) != set ( attributes_b . keys ( ) ) : 
~~ return all ( nested_equal ( attributes_a [ k ] , attributes_b [ k ] ) for k in keys_a ) 
~~ def manual_run ( turn_into_run = True , store_meta_data = True , clean_up = True ) : 
def wrapper ( func ) : 
~~~ @ functools . wraps ( func ) 
def new_func ( traj , * args , ** kwargs ) : 
~~~ do_wrap = not traj . _run_by_environment 
if do_wrap : 
~~~ traj . f_start_run ( turn_into_run = turn_into_run ) 
~~ result = func ( traj , * args , ** kwargs ) 
~~~ traj . f_finalize_run ( store_meta_data = store_meta_data , 
clean_up = clean_up ) 
~~ return wrapper 
~~ def deprecated ( msg = '' ) : 
def new_func ( * args , ** kwargs ) : 
warnings . warn ( 
warning_string , 
category = DeprecationWarning , 
return func ( * args , ** kwargs ) 
~~ def copydoc ( fromfunc , sep = "\\n" ) : 
def _decorator ( func ) : 
~~~ sourcedoc = fromfunc . __doc__ 
split_doc = sourcedoc . split ( '\\n' ) 
split_doc_no_abstract = [ line for line in split_doc if not 'ABSTRACT' in line ] 
if len ( split_doc ) != len ( split_doc_no_abstract ) : 
~~~ sourcedoc = '\\n' . join ( split_doc_no_abstract [ : - 1 ] ) 
~~ if func . __doc__ is None : 
~~~ func . __doc__ = sourcedoc 
~~~ func . __doc__ = sep . join ( [ sourcedoc , func . __doc__ ] ) 
~~ return func 
~~ return _decorator 
~~ def kwargs_mutual_exclusive ( param1_name , param2_name , map2to1 = None ) : 
~~~ if param2_name in kwargs : 
~~~ if param1_name in kwargs : 
~~ param2 = kwargs . pop ( param2_name ) 
if map2to1 is not None : 
~~~ param1 = map2to1 ( param2 ) 
~~~ param1 = param2 
~~ kwargs [ param1_name ] = param1 
~~ return func ( * args , ** kwargs ) 
~~ def kwargs_api_change ( old_name , new_name = None ) : 
~~~ if old_name in kwargs : 
~~~ if new_name is None : 
~~ warnings . warn ( warning_string , category = DeprecationWarning ) 
value = kwargs . pop ( old_name ) 
if new_name is not None : 
~~~ kwargs [ new_name ] = value 
~~ ~~ return func ( * args , ** kwargs ) 
~~ def not_in_run ( func ) : 
doc = func . __doc__ 
if doc is not None : 
~~~ func . __doc__ = '\\n' . join ( [ doc , na_string ] ) 
~~ func . _not_in_run = True 
~~~ if self . _is_run : 
func . __name__ ) 
~~ def with_open_store ( func ) : 
~~ func . _with_open_store = True 
~~~ if not self . traj . v_storage_service . is_open : 
~~ def retry ( n , errors , wait = 0.0 , logger_name = None ) : 
~~~ retries = 0 
~~~ result = func ( * args , ** kwargs ) 
if retries and logger_name : 
~~ except errors : 
~~~ if retries >= n : 
~~~ if logger_name : 
str ( args ) , 
str ( kwargs ) ) ) 
~~ raise 
~~ elif logger_name : 
~~ retries += 1 
if wait : 
~~~ time . sleep ( wait ) 
~~ ~~ ~~ ~~ return new_func 
~~ def _prfx_getattr_ ( obj , item ) : 
if item . startswith ( 'f_' ) or item . startswith ( 'v_' ) : 
~~~ return getattr ( obj , item [ 2 : ] ) 
~~ def _prfx_setattr_ ( obj , item , value ) : 
if item . startswith ( 'v_' ) : 
~~~ return setattr ( obj , item [ 2 : ] , value ) 
~~~ return super ( obj . __class__ , obj ) . __setattr__ ( item , value ) 
~~ ~~ def prefix_naming ( cls ) : 
if hasattr ( cls , '__getattr__' ) : 
~~ cls . __getattr__ = _prfx_getattr_ 
cls . __setattr__ = _prfx_setattr_ 
return cls 
~~ def add_params ( traj ) : 
traj . v_standard_parameter = Brian2Parameter 
traj . v_fast_access = True 
traj . f_add_parameter ( 'Net.C' , 281 * pF ) 
traj . f_add_parameter ( 'Net.gL' , 30 * nS ) 
traj . f_add_parameter ( 'Net.EL' , - 70.6 * mV ) 
traj . f_add_parameter ( 'Net.VT' , - 50.4 * mV ) 
traj . f_add_parameter ( 'Net.DeltaT' , 2 * mV ) 
traj . f_add_parameter ( 'Net.tauw' , 40 * ms ) 
traj . f_add_parameter ( 'Net.a' , 4 * nS ) 
traj . f_add_parameter ( 'Net.b' , 0.08 * nA ) 
traj . f_add_parameter ( 'Net.I' , .8 * nA ) 
traj . f_add_parameter ( 'Net.N' , 50 ) 
traj . f_add_parameter ( 'Net.eqs' , eqs ) 
traj . f_add_parameter ( 'reset' , 'vm=Vr;w+=b' ) 
~~ def run_net ( traj ) : 
eqs = traj . eqs 
namespace = traj . Net . f_to_dict ( short_names = True , fast_access = True ) 
neuron = NeuronGroup ( traj . N , model = eqs , threshold = traj . Vcut , reset = traj . reset , 
namespace = namespace ) 
neuron . vm = traj . EL 
neuron . w = traj . a * ( neuron . vm - traj . EL ) 
net = Network ( neuron ) 
MSpike = SpikeMonitor ( neuron ) 
net . add ( MSpike ) 
MStateV = StateMonitor ( neuron , variables = [ 'vm' ] , record = [ 1 , 2 , 3 ] ) 
net . add ( MStateV ) 
net . run ( 500 * ms , report = 'text' ) 
traj . v_standard_result = Brian2MonitorResult 
traj . f_add_result ( 'SpikeMonitor' , MSpike ) 
traj . f_add_result ( 'StateMonitorV' , MStateV ) 
~~ def euler_scheme ( traj , diff_func ) : 
steps = traj . steps 
initial_conditions = traj . initial_conditions 
dimension = len ( initial_conditions ) 
result_array = np . zeros ( ( steps , dimension ) ) 
func_params_dict = traj . func_params . f_to_dict ( short_names = True , fast_access = True ) 
result_array [ 0 ] = initial_conditions 
for idx in range ( 1 , steps ) : 
~~~ result_array [ idx ] = diff_func ( result_array [ idx - 1 ] , ** func_params_dict ) * traj . dt + result_array [ idx - 1 ] 
~~ def add_parameters ( traj ) : 
traj . f_add_parameter ( ArrayParameter , 'initial_conditions' , np . array ( [ 0.0 , 0.0 , 0.0 ] ) , 
traj . f_add_parameter ( 'func_params.sigma' , 10.0 ) 
traj . f_add_parameter ( 'func_params.beta' , 8.0 / 3.0 ) 
traj . f_add_parameter ( 'func_params.rho' , 28.0 ) 
~~ def diff_lorenz ( value_array , sigma , beta , rho ) : 
diff_array = np . zeros ( 3 ) 
diff_array [ 0 ] = sigma * ( value_array [ 1 ] - value_array [ 0 ] ) 
diff_array [ 1 ] = value_array [ 0 ] * ( rho - value_array [ 2 ] ) - value_array [ 1 ] 
diff_array [ 2 ] = value_array [ 0 ] * value_array [ 1 ] - beta * value_array [ 2 ] 
return diff_array 
~~ def _create_storage ( storage_service , trajectory = None , ** kwargs ) : 
kwargs_copy = kwargs . copy ( ) 
kwargs_copy [ 'trajectory' ] = trajectory 
matching_kwargs = get_matching_kwargs ( storage_service , kwargs_copy ) 
storage_service = storage_service ( ** matching_kwargs ) 
unused_kwargs = set ( kwargs . keys ( ) ) - set ( matching_kwargs . keys ( ) ) 
return storage_service , unused_kwargs 
~~ def storage_factory ( storage_service , trajectory = None , ** kwargs ) : 
if 'filename' in kwargs and storage_service is None : 
~~~ filename = kwargs [ 'filename' ] 
_ , ext = os . path . splitext ( filename ) 
if ext in ( '.hdf' , '.h4' , '.hdf4' , '.he2' , '.h5' , '.hdf5' , '.he5' ) : 
~~~ storage_service = HDF5StorageService 
( ext , filename ) ) 
~~ ~~ elif isinstance ( storage_service , str ) : 
~~~ class_name = storage_service . split ( '.' ) [ - 1 ] 
storage_service = create_class ( class_name , [ storage_service , HDF5StorageService ] ) 
~~ if inspect . isclass ( storage_service ) : 
~~~ return _create_storage ( storage_service , trajectory , ** kwargs ) 
~~~ return storage_service , set ( kwargs . keys ( ) ) 
~~ ~~ def multiply ( traj ) : 
if traj . diff_name == 'diff_lorenz' : 
~~~ traj . f_add_parameter ( 'func_params.sigma' , 10.0 ) 
~~ elif traj . diff_name == 'diff_roessler' : 
~~~ traj . f_add_parameter ( 'func_params.a' , 0.1 ) 
traj . f_add_parameter ( 'func_params.c' , 14.0 ) 
~~ ~~ def diff_roessler ( value_array , a , c ) : 
b = a 
diff_array [ 0 ] = - value_array [ 1 ] - value_array [ 2 ] 
diff_array [ 1 ] = value_array [ 0 ] + a * value_array [ 1 ] 
diff_array [ 2 ] = b + value_array [ 2 ] * ( value_array [ 0 ] - c ) 
~~ def compact_hdf5_file ( filename , name = None , index = None , keep_backup = True ) : 
if name is None and index is None : 
~~~ index = - 1 
~~ tmp_traj = load_trajectory ( name , index , as_new = False , load_all = pypetconstants . LOAD_NOTHING , 
force = True , filename = filename ) 
service = tmp_traj . v_storage_service 
complevel = service . complevel 
complib = service . complib 
shuffle = service . shuffle 
fletcher32 = service . fletcher32 
name_wo_ext , ext = os . path . splitext ( filename ) 
tmp_filename = name_wo_ext + '_tmp' + ext 
abs_filename = os . path . abspath ( filename ) 
abs_tmp_filename = os . path . abspath ( tmp_filename ) 
command = [ 'ptrepack' , '-v' , 
'--complib' , complib , 
'--complevel' , str ( complevel ) , 
'--shuffle' , str ( int ( shuffle ) ) , 
'--fletcher32' , str ( int ( fletcher32 ) ) , 
abs_filename , abs_tmp_filename ] 
retcode = subprocess . call ( command ) 
if retcode != 0 : 
( filename , str ( retcode ) ) ) 
if keep_backup : 
~~~ backup_file_name = name_wo_ext + '_backup' + ext 
os . rename ( filename , backup_file_name ) 
~~~ os . remove ( filename ) 
~~ os . rename ( tmp_filename , filename ) 
~~ return retcode 
~~ def _explored_parameters_in_group ( traj , group_node ) : 
explored = False 
for param in traj . f_get_explored_parameters ( ) : 
~~~ if param in group_node : 
~~~ explored = True 
~~ ~~ return explored 
assert ( isinstance ( traj , Trajectory ) ) 
scale = traj . simulation . scale 
traj . f_add_parameter ( 'model.eqs' , model_eqs , 
traj . f_add_parameter ( 'model.synaptic.eqs' , conn_eqs , 
traj . f_add_parameter ( 'model.reset_func' , 'V=0.0' , 
~~ def _build_model_eqs ( traj ) : 
model_eqs = traj . model . eqs 
post_eqs = { } 
for name_post in [ 'i' , 'e' ] : 
~~~ variables_dict = { } 
new_model_eqs = model_eqs . replace ( 'POST' , name_post ) 
for name_pre in [ 'i' , 'e' ] : 
~~~ conn_eqs = traj . model . synaptic . eqs 
new_conn_eqs = conn_eqs . replace ( 'PRE' , name_pre ) 
new_model_eqs += new_conn_eqs 
tau1 = traj . model . synaptic [ 'tau1' ] 
tau2 = traj . model . synaptic [ 'tau2_' + name_pre ] 
normalization = ( tau1 - tau2 ) / tau2 
invtau1 = 1.0 / tau1 
invtau2 = 1.0 / tau2 
variables_dict [ 'invtau1_' + name_pre ] = invtau1 
variables_dict [ 'invtau2_' + name_pre ] = invtau2 
variables_dict [ 'normalization_' + name_pre ] = normalization 
variables_dict [ 'tau1_' + name_pre ] = tau1 
variables_dict [ 'tau2_' + name_pre ] = tau2 
~~ variables_dict [ 'tau_' + name_post ] = traj . model [ 'tau_' + name_post ] 
post_eqs [ name_post ] = Equations ( new_model_eqs , ** variables_dict ) 
~~ return post_eqs 
~~ def pre_build ( self , traj , brian_list , network_dict ) : 
self . _pre_build = not _explored_parameters_in_group ( traj , traj . parameters . model ) 
if self . _pre_build : 
~~~ self . _build_model ( traj , brian_list , network_dict ) 
~~ ~~ def build ( self , traj , brian_list , network_dict ) : 
if not hasattr ( self , '_pre_build' ) or not self . _pre_build : 
~~ ~~ def _build_model ( self , traj , brian_list , network_dict ) : 
model = traj . parameters . model 
eqs_dict = self . _build_model_eqs ( traj ) 
eqs_i = eqs_dict [ 'i' ] 
neurons_i = NeuronGroup ( N = model . N_i , 
model = eqs_i , 
threshold = model . V_th , 
reset = model . reset_func , 
refractory = model . refractory , 
method = 'Euler' ) 
eqs_e = eqs_dict [ 'e' ] 
neurons_e = NeuronGroup ( N = model . N_e , 
model = eqs_e , 
neurons_e . mu = rand ( model . N_e ) * ( model . mu_e_max - model . mu_e_min ) + model . mu_e_min 
neurons_i . mu = rand ( model . N_i ) * ( model . mu_i_max - model . mu_i_min ) + model . mu_i_min 
neurons_e . V = rand ( model . N_e ) 
neurons_i . V = rand ( model . N_i ) 
brian_list . append ( neurons_i ) 
brian_list . append ( neurons_e ) 
network_dict [ 'neurons_e' ] = neurons_e 
network_dict [ 'neurons_i' ] = neurons_i 
traj . f_add_parameter ( 'connections.strength_factor' , 2.5 , 
traj . f_add_parameter ( 'connections.p_ii' , 0.25 , 
traj . f_add_parameter ( 'connections.p_ei' , 0.25 , 
traj . f_add_parameter ( 'connections.p_ie' , 0.25 , 
traj . f_add_parameter ( 'connections.p_ee' , 0.1 , 
traj . f_add_parameter ( 'connections.J_ii' , 0.027 / np . sqrt ( scale ) , 
traj . f_add_parameter ( 'connections.J_ei' , 0.032 / np . sqrt ( scale ) , 
traj . f_add_parameter ( 'connections.J_ie' , 0.009 / np . sqrt ( scale ) , 
traj . f_add_parameter ( 'connections.J_ee' , 0.012 / np . sqrt ( scale ) , 
self . _pre_build = not _explored_parameters_in_group ( traj , traj . parameters . connections ) 
self . _pre_build = ( self . _pre_build and 'neurons_i' in network_dict and 
'neurons_e' in network_dict ) 
~~~ self . _build_connections ( traj , brian_list , network_dict ) 
~~ ~~ def _build_connections ( self , traj , brian_list , network_dict ) : 
connections = traj . connections 
neurons_i = network_dict [ 'neurons_i' ] 
neurons_e = network_dict [ 'neurons_e' ] 
conns_list = [ self . conn_ii , self . conn_ei , self . conn_ie ] 
if connections . R_ee > 1.0 : 
~~~ cluster_list = [ ] 
cluster_conns_list = [ ] 
model = traj . model 
clusters = int ( model . N_e / connections . clustersize_e ) 
p_out = ( connections . p_ee * model . N_e ) / ( connections . R_ee * connections . clustersize_e + model . N_e - connections . clustersize_e ) 
p_in = p_out * connections . R_ee 
traj . f_add_derived_parameter ( 'connections.p_ee_in' , p_in , 
traj . f_add_derived_parameter ( 'connections.p_ee_out' , p_out , 
low_index = 0 
high_index = connections . clustersize_e 
for irun in range ( clusters ) : 
~~~ cluster = neurons_e [ low_index : high_index ] 
conn = Synapses ( cluster , cluster , 
cluster_conns_list . append ( conn ) 
if low_index > 0 : 
~~~ rest_low = neurons_e [ 0 : low_index ] 
low_conn = Synapses ( cluster , rest_low , 
cluster_conns_list . append ( low_conn ) 
~~ if high_index < model . N_e : 
~~~ rest_high = neurons_e [ high_index : model . N_e ] 
high_conn = Synapses ( cluster , rest_high , 
cluster_conns_list . append ( high_conn ) 
~~ low_index = high_index 
high_index += connections . clustersize_e 
~~ self . cluster_conns = cluster_conns_list 
conns_list += cluster_conns_list 
self . conn_ee = Synapses ( neurons_e , neurons_e , 
conns_list . append ( self . conn_ee ) 
~~ brian_list . extend ( conns_list ) 
network_dict [ 'connections' ] = conns_list 
~~ def add_parameters ( self , traj ) : 
par = traj . f_add_parameter ( Brian2Parameter , 'simulation.durations.initial_run' , 500 * ms , 
par . v_annotations . order = 0 
par = traj . f_add_parameter ( Brian2Parameter , 'simulation.durations.measurement_run' , 1500 * ms , 
par . v_annotations . order = 1 
~~ def _compute_fano_factor ( spike_res , neuron_id , time_window , start_time , end_time ) : 
assert ( end_time >= start_time + time_window ) 
bins = ( end_time - start_time ) / time_window 
bins = int ( np . floor ( bins ) ) 
binned_spikes = np . zeros ( bins ) 
spike_array_neuron = spike_res . t [ spike_res . i == neuron_id ] 
for bin in range ( bins ) : 
~~~ lower_time = start_time + time_window * bin 
upper_time = start_time + time_window * ( bin + 1 ) 
spike_array_interval = spike_array_neuron [ spike_array_neuron >= lower_time ] 
spike_array_interval = spike_array_interval [ spike_array_interval < upper_time ] 
spikes = len ( spike_array_interval ) 
binned_spikes [ bin ] = spikes 
~~ var = np . var ( binned_spikes ) 
avg = np . mean ( binned_spikes ) 
if avg > 0 : 
~~~ return var / float ( avg ) 
~~~ return 0 
~~ ~~ def _compute_mean_fano_factor ( neuron_ids , spike_res , time_window , start_time , end_time ) : 
ffs = np . zeros ( len ( neuron_ids ) ) 
for idx , neuron_id in enumerate ( neuron_ids ) : 
~~~ ff = CNFanoFactorComputer . _compute_fano_factor ( 
spike_res , neuron_id , time_window , start_time , end_time ) 
ffs [ idx ] = ff 
~~ mean_ff = np . mean ( ffs ) 
return mean_ff 
~~ def analyse ( self , traj , network , current_subrun , subrun_list , network_dict ) : 
if len ( subrun_list ) == 0 : 
~~~ spikes_e = traj . results . monitors . spikes_e 
time_window = traj . parameters . analysis . statistics . time_window 
start_time = traj . parameters . simulation . durations . initial_run 
end_time = start_time + traj . parameters . simulation . durations . measurement_run 
neuron_ids = traj . parameters . analysis . statistics . neuron_ids 
mean_ff = self . _compute_mean_fano_factor ( 
neuron_ids , spikes_e , time_window , start_time , end_time ) 
~~ ~~ def add_to_network ( self , traj , network , current_subrun , subrun_list , network_dict ) : 
if current_subrun . v_annotations . order == 1 : 
~~~ self . _add_monitors ( traj , network , network_dict ) 
~~ ~~ def _add_monitors ( self , traj , network , network_dict ) : 
monitor_list = [ ] 
self . spike_monitor = SpikeMonitor ( neurons_e ) 
monitor_list . append ( self . spike_monitor ) 
self . V_monitor = StateMonitor ( neurons_e , 'V' , 
record = list ( traj . neuron_records ) ) 
monitor_list . append ( self . V_monitor ) 
self . I_syn_e_monitor = StateMonitor ( neurons_e , 'I_syn_e' , 
monitor_list . append ( self . I_syn_e_monitor ) 
self . I_syn_i_monitor = StateMonitor ( neurons_e , 'I_syn_i' , 
monitor_list . append ( self . I_syn_i_monitor ) 
network . add ( * monitor_list ) 
network_dict [ 'monitors' ] = monitor_list 
~~ def _make_folder ( self , traj ) : 
print_folder = os . path . join ( traj . analysis . plot_folder , 
traj . v_name , traj . v_crun ) 
print_folder = os . path . abspath ( print_folder ) 
if not os . path . isdir ( print_folder ) : 
~~~ os . makedirs ( print_folder ) 
~~ return print_folder 
~~ def _plot_result ( self , traj , result_name ) : 
result = traj . f_get ( result_name ) 
varname = result . record_variables [ 0 ] 
values = result [ varname ] 
times = result . t 
record = result . record 
for idx , celia_neuron in enumerate ( record ) : 
~~~ plt . subplot ( len ( record ) , 1 , idx + 1 ) 
plt . plot ( times , values [ idx , : ] ) 
if idx == 0 : 
~~~ plt . title ( '%s' % varname ) 
~~ if idx == 1 : 
~~~ plt . ylabel ( '%s' % ( varname ) ) 
~~ if idx == len ( record ) - 1 : 
~~~ plt . xlabel ( 't' ) 
~~ ~~ ~~ def _print_graphs ( self , traj ) : 
print_folder = self . _make_folder ( traj ) 
plt . scatter ( self . spike_monitor . t , self . spike_monitor . i , s = 1 ) 
plt . xlabel ( 't' ) 
filename = os . path . join ( print_folder , 'spike.png' ) 
plt . savefig ( filename ) 
plt . close ( ) 
fig = plt . figure ( ) 
self . _plot_result ( traj , 'monitors.V' ) 
filename = os . path . join ( print_folder , 'V.png' ) 
fig . savefig ( filename ) 
self . _plot_result ( traj , 'monitors.I_syn_e' ) 
filename = os . path . join ( print_folder , 'I_syn_e.png' ) 
self . _plot_result ( traj , 'monitors.I_syn_i' ) 
filename = os . path . join ( print_folder , 'I_syn_i.png' ) 
if not traj . analysis . show_plots : 
~~~ plt . close ( 'all' ) 
~~ ~~ def analyse ( self , traj , network , current_subrun , subrun_list , network_dict ) : 
~~~ traj . f_add_result ( Brian2MonitorResult , 'monitors.spikes_e' , self . spike_monitor , 
traj . f_add_result ( Brian2MonitorResult , 'monitors.V' , self . V_monitor , 
traj . f_add_result ( Brian2MonitorResult , 'monitors.I_syn_e' , self . I_syn_e_monitor , 
traj . f_add_result ( Brian2MonitorResult , 'monitors.I_syn_i' , self . I_syn_i_monitor , 
print ( 'Plotting' ) 
if traj . parameters . analysis . make_plots : 
~~~ self . _print_graphs ( traj ) 
~~ ~~ ~~ def get_batch ( ) : 
optlist , args = getopt . getopt ( sys . argv [ 1 : ] , '' , longopts = 'batch=' ) 
batch = 0 
for o , a in optlist : 
~~~ if o == '--batch' : 
~~~ batch = int ( a ) 
~~ ~~ return batch 
~~ def explore_batch ( traj , batch ) : 
explore_dict = { } 
explore_dict [ 'sigma' ] = np . arange ( 10.0 * batch , 10.0 * ( batch + 1 ) , 1.0 ) . tolist ( ) 
traj . f_explore ( explore_dict ) 
~~ def vars ( self ) : 
if self . _vars is None : 
~~~ self . _vars = NNTreeNodeVars ( self ) 
~~ return self . _vars 
~~ def func ( self ) : 
if self . _func is None : 
~~~ self . _func = NNTreeNodeFunc ( self ) 
~~ return self . _func 
~~ def _rename ( self , full_name ) : 
self . _full_name = full_name 
if full_name : 
~~~ self . _name = full_name . rsplit ( '.' , 1 ) [ - 1 ] 
~~ ~~ def _set_details ( self , depth , branch , run_branch ) : 
self . _depth = depth 
self . _branch = branch 
self . _run_branch = run_branch 
~~ def _map_type_to_dict ( self , type_name ) : 
root = self . _root_instance 
if type_name == RESULT : 
~~~ return root . _results 
~~ elif type_name == PARAMETER : 
~~~ return root . _parameters 
~~ elif type_name == DERIVED_PARAMETER : 
~~~ return root . _derived_parameters 
~~ elif type_name == CONFIG : 
~~~ return root . _config 
~~ elif type_name == LEAF : 
~~~ return root . _other_leaves 
~~ ~~ def _fetch_from_string ( self , store_load , name , args , kwargs ) : 
if not isinstance ( name , str ) : 
~~ node = self . _root_instance . f_get ( name ) 
return self . _fetch_from_node ( store_load , node , args , kwargs ) 
~~ def _fetch_from_node ( self , store_load , node , args , kwargs ) : 
msg = self . _node_to_msg ( store_load , node ) 
return msg , node , args , kwargs 
~~ def _fetch_from_tuple ( self , store_load , store_tuple , args , kwargs ) : 
node = store_tuple [ 1 ] 
msg = store_tuple [ 0 ] 
if len ( store_tuple ) > 2 : 
~~~ args = store_tuple [ 2 ] 
~~ if len ( store_tuple ) > 3 : 
~~~ kwargs = store_tuple [ 3 ] 
~~ if len ( store_tuple ) > 4 : 
~~ _ = self . _fetch_from_node ( store_load , node , args , kwargs ) 
~~ def _node_to_msg ( store_load , node ) : 
if node . v_is_leaf : 
~~~ if store_load == STORE : 
~~~ return pypetconstants . LEAF 
~~ elif store_load == LOAD : 
~~ elif store_load == REMOVE : 
~~~ return pypetconstants . DELETE 
~~~ return pypetconstants . GROUP 
~~ ~~ ~~ def _fetch_items ( self , store_load , iterable , args , kwargs ) : 
only_empties = kwargs . pop ( 'only_empties' , False ) 
non_empties = kwargs . pop ( 'non_empties' , False ) 
item_list = [ ] 
for iter_item in iterable : 
~~~ item_tuple = self . _fetch_from_string ( store_load , iter_item , args , kwargs ) 
~~~ item_tuple = self . _fetch_from_node ( store_load , iter_item , args , kwargs ) 
~~~ item_tuple = self . _fetch_from_tuple ( store_load , iter_item , args , kwargs ) 
~~ ~~ item = item_tuple [ 1 ] 
msg = item_tuple [ 0 ] 
if item . v_is_leaf : 
~~~ if only_empties and not item . f_is_empty ( ) : 
~~ if non_empties and item . f_is_empty ( ) : 
~~ ~~ item_list . append ( item_tuple ) 
~~ return item_list 
~~ def _remove_subtree ( self , start_node , name , predicate = None ) : 
def _delete_from_children ( node , child_name ) : 
~~~ del node . _children [ child_name ] 
if child_name in node . _groups : 
~~~ del node . _groups [ child_name ] 
~~ elif child_name in node . _leaves : 
~~~ del node . _leaves [ child_name ] 
~~ ~~ def _remove_subtree_inner ( node , predicate ) : 
~~~ if not predicate ( node ) : 
~~ elif node . v_is_group : 
~~~ for name_ in itools . chain ( list ( node . _leaves . keys ( ) ) , 
list ( node . _groups . keys ( ) ) ) : 
~~~ child_ = node . _children [ name_ ] 
child_deleted = _remove_subtree_inner ( child_ , predicate ) 
if child_deleted : 
~~~ _delete_from_children ( node , name_ ) 
del child_ 
~~ ~~ for link_ in list ( node . _links . keys ( ) ) : 
~~~ node . f_remove_link ( link_ ) 
~~ if len ( node . _children ) == 0 : 
~~~ self . _delete_node ( node ) 
~~ ~~ if name in start_node . _links : 
~~~ start_node . f_remove_link ( name ) 
~~~ child = start_node . _children [ name ] 
if predicate is None : 
~~~ predicate = lambda x : True 
~~ if _remove_subtree_inner ( child , predicate ) : 
~~~ _delete_from_children ( start_node , name ) 
del child 
~~ ~~ ~~ def _delete_node ( self , node ) : 
full_name = node . v_full_name 
if full_name == '' : 
~~ if node . v_is_leaf : 
~~~ if full_name in root . _parameters : 
~~~ del root . _parameters [ full_name ] 
~~ elif full_name in root . _config : 
~~~ del root . _config [ full_name ] 
~~ elif full_name in root . _derived_parameters : 
~~~ del root . _derived_parameters [ full_name ] 
~~ elif full_name in root . _results : 
~~~ del root . _results [ full_name ] 
~~ elif full_name in root . _other_leaves : 
~~~ del root . _other_leaves [ full_name ] 
~~ if full_name in root . _explored_parameters : 
~~~ if root . _stored : 
~~~ root . _explored_parameters [ full_name ] = None 
~~~ del root . _explored_parameters [ full_name ] 
~~ if len ( root . _explored_parameters ) == 0 : 
~~~ root . f_shrink ( ) 
~~ ~~ del self . _flat_leaf_storage_dict [ full_name ] 
~~~ del root . _all_groups [ full_name ] 
if full_name in root . _run_parent_groups : 
~~~ del root . _run_parent_groups [ full_name ] 
~~ ~~ if full_name in root . _linked_by : 
~~~ linking = root . _linked_by [ full_name ] 
for linking_name in list ( linking . keys ( ) ) : 
~~~ linking_group , link_set = linking [ linking_name ] 
for link in list ( link_set ) : 
~~~ linking_group . f_remove_link ( link ) 
~~ ~~ ~~ if ( node . v_location , node . v_name ) in self . _root_instance . _new_nodes : 
~~~ del self . _root_instance . _new_nodes [ ( node . v_location , node . v_name ) ] 
~~ self . _remove_from_nodes_and_leaves ( node ) 
node . _vars = None 
node . _func = None 
~~ def _remove_node_or_leaf ( self , instance , recursive = False ) : 
full_name = instance . v_full_name 
split_name = deque ( full_name . split ( '.' ) ) 
self . _remove_along_branch ( self . _root_instance , split_name , recursive ) 
~~ def _remove_along_branch ( self , actual_node , split_name , recursive = False ) : 
if len ( split_name ) == 0 : 
~~~ if actual_node . v_is_group and actual_node . f_has_children ( ) : 
~~~ if recursive : 
~~~ for child in list ( actual_node . _children . keys ( ) ) : 
~~~ actual_node . f_remove_child ( child , recursive = True ) 
~~ ~~ self . _delete_node ( actual_node ) 
~~ name = split_name . popleft ( ) 
if name in actual_node . _links : 
~~~ if len ( split_name ) > 0 : 
~~ actual_node . f_remove_link ( name ) 
~~~ child = actual_node . _children [ name ] 
if self . _remove_along_branch ( child , split_name , recursive = recursive ) : 
~~~ del actual_node . _children [ name ] 
if name in actual_node . _groups : 
~~~ del actual_node . _groups [ name ] 
~~ elif name in actual_node . _leaves : 
~~~ del actual_node . _leaves [ name ] 
~~ del child 
~~ ~~ ~~ def _translate_shortcut ( self , name ) : 
if isinstance ( name , int ) : 
~~~ return True , self . _root_instance . f_wildcard ( '$' , name ) 
~~ if name . startswith ( 'run_' ) or name . startswith ( 'r_' ) : 
~~~ split_name = name . split ( '_' ) 
if len ( split_name ) == 2 : 
~~~ index = split_name [ 1 ] 
if index . isdigit ( ) : 
~~~ return True , self . _root_instance . f_wildcard ( '$' , int ( index ) ) 
~~ elif index == 'A' : 
~~~ return True , self . _root_instance . f_wildcard ( '$' , - 1 ) 
~~ ~~ ~~ if name . startswith ( 'runtoset_' ) or name . startswith ( 'rts_' ) : 
~~~ return True , self . _root_instance . f_wildcard ( '$set' , int ( index ) ) 
~~~ return True , self . _root_instance . f_wildcard ( '$set' , - 1 ) 
~~ ~~ ~~ if name in SHORTCUT_SET : 
~~~ if name == 'par' : 
~~~ return True , 'parameters' 
~~ elif name == 'dpar' : 
~~~ return True , 'derived_parameters' 
~~ elif name == 'res' : 
~~~ return True , 'results' 
~~ elif name == 'conf' : 
~~~ return True , 'config' 
~~ ~~ return False , name 
~~ def _add_prefix ( self , split_names , start_node , group_type_name ) : 
prepend = [ ] 
if start_node . v_depth < 3 and not group_type_name == GROUP : 
~~~ if start_node . v_depth == 0 : 
~~~ if group_type_name == DERIVED_PARAMETER_GROUP : 
~~~ if split_names [ 0 ] == 'derived_parameters' : 
~~~ return split_names 
~~~ prepend += [ 'derived_parameters' ] 
~~ ~~ elif group_type_name == RESULT_GROUP : 
~~~ if split_names [ 0 ] == 'results' : 
~~~ prepend += [ 'results' ] 
~~ ~~ elif group_type_name == CONFIG_GROUP : 
~~~ if split_names [ 0 ] == 'config' : 
~~~ prepend += [ 'config' ] 
~~ ~~ elif group_type_name == PARAMETER_GROUP : 
~~~ if split_names [ 0 ] == 'parameters' : 
~~~ return split_names [ 0 ] 
~~~ prepend += [ 'parameters' ] 
~~ ~~ if root . _is_run and root . _auto_run_prepend : 
~~~ dummy = root . f_wildcard ( '$' , - 1 ) 
crun = root . f_wildcard ( '$' ) 
if any ( name in root . _run_information for name in split_names ) : 
~~ elif any ( name == dummy for name in split_names ) : 
~~ elif ( group_type_name == RESULT_GROUP or 
group_type_name == DERIVED_PARAMETER_GROUP ) : 
~~~ prepend += [ 'runs' , crun ] 
~~ elif start_node . v_depth == 1 : 
~~~ if len ( split_names ) == 1 and split_names [ 0 ] == 'runs' : 
~~ ~~ elif start_node . v_depth == 2 and start_node . v_name == 'runs' : 
~~~ prepend += [ crun ] 
~~ ~~ ~~ ~~ if prepend : 
~~~ split_names = prepend + split_names 
~~ return split_names 
~~ def _determine_types ( start_node , first_name , add_leaf , add_link ) : 
if start_node . v_is_root : 
~~~ where = first_name 
~~~ where = start_node . _branch 
~~ if where in SUBTREE_MAPPING : 
~~~ type_tuple = SUBTREE_MAPPING [ where ] 
~~~ type_tuple = ( GROUP , LEAF ) 
~~ if add_link : 
~~~ return type_tuple [ 0 ] , LINK 
~~ if add_leaf : 
~~~ return type_tuple 
~~~ return type_tuple [ 0 ] , type_tuple [ 0 ] 
~~ ~~ def _add_generic ( self , start_node , type_name , group_type_name , args , kwargs , 
add_prefix = True , check_naming = True ) : 
args = list ( args ) 
create_new = True 
name = '' 
instance = None 
constructor = None 
add_link = type_name == LINK 
if add_link : 
~~~ name = args [ 0 ] 
instance = args [ 1 ] 
create_new = False 
~~ elif len ( args ) == 1 and len ( kwargs ) == 0 : 
~~~ item = args [ 0 ] 
~~~ name = item . v_full_name 
instance = item 
~~ ~~ if create_new : 
~~~ if len ( args ) > 0 and inspect . isclass ( args [ 0 ] ) : 
~~~ constructor = args . pop ( 0 ) 
~~ if len ( args ) > 0 and isinstance ( args [ 0 ] , str ) : 
~~~ name = args . pop ( 0 ) 
~~ elif 'name' in kwargs : 
~~~ name = kwargs . pop ( 'name' ) 
~~ elif 'full_name' in kwargs : 
~~~ name = kwargs . pop ( 'full_name' ) 
~~ ~~ split_names = name . split ( '.' ) 
if check_naming : 
~~~ for idx , name in enumerate ( split_names ) : 
~~~ translated_shortcut , name = self . _translate_shortcut ( name ) 
replaced , name = self . _replace_wildcards ( name ) 
if translated_shortcut or replaced : 
~~~ split_names [ idx ] = name 
~~ ~~ faulty_names = self . _check_names ( split_names , start_node ) 
if faulty_names : 
~~~ full_name = '.' . join ( split_names ) 
raise ValueError ( 
~~~ if instance is None : 
~~ if instance . v_is_root : 
~~ if start_node . v_is_root and name in SUBTREE_MAPPING : 
~~ if not self . _root_instance . f_contains ( instance , with_links = False , shortcuts = False ) : 
~~ ~~ ~~ if add_prefix : 
~~~ split_names = self . _add_prefix ( split_names , start_node , group_type_name ) 
~~ if group_type_name == GROUP : 
~~~ add_leaf = type_name != group_type_name and not add_link 
group_type_name , type_name = self . _determine_types ( start_node , split_names [ 0 ] , 
add_leaf , add_link ) 
~~ if self . _root_instance . _is_run and type_name in SENSITIVE_TYPES : 
~~ return self . _add_to_tree ( start_node , split_names , type_name , group_type_name , instance , 
constructor , args , kwargs ) 
~~ def _replace_wildcards ( self , name , run_idx = None ) : 
if self . _root_instance . f_is_wildcard ( name ) : 
~~~ return True , self . _root_instance . f_wildcard ( name , run_idx ) 
~~~ return False , name 
~~ ~~ def _add_to_tree ( self , start_node , split_names , type_name , group_type_name , 
instance , constructor , args , kwargs ) : 
~~~ act_node = start_node 
last_idx = len ( split_names ) - 1 
link_added = False 
for idx , name in enumerate ( split_names ) : 
~~~ if name not in act_node . _children : 
~~~ if idx == last_idx : 
~~~ if add_link : 
~~~ new_node = self . _create_link ( act_node , name , instance ) 
link_added = True 
~~ elif group_type_name != type_name : 
~~~ new_node = self . _create_any_param_or_result ( act_node , 
name , 
type_name , 
instance , 
constructor , 
args , kwargs ) 
self . _flat_leaf_storage_dict [ new_node . v_full_name ] = new_node 
~~~ new_node = self . _create_any_group ( act_node , name , 
group_type_name , 
group_type_name ) 
~~ if name in self . _root_instance . _run_information : 
~~~ self . _root_instance . _run_parent_groups [ act_node . v_full_name ] = act_node 
~~ if self . _root_instance . _is_run : 
~~~ if link_added : 
~~~ self . _root_instance . _new_links [ ( act_node . v_full_name , name ) ] = ( act_node , new_node ) 
~~~ self . _root_instance . _new_nodes [ ( act_node . v_full_name , name ) ] = ( act_node , new_node ) 
~~~ if name in act_node . _links : 
( name , act_node . v_full_name ) ) 
~~ if idx == last_idx : 
~~~ if self . _root_instance . _no_clobber : 
'data.' % ( name , act_node . v_full_name ) ) 
~~ ~~ ~~ act_node = act_node . _children [ name ] 
~~ return act_node 
( name , start_node . v_full_name ) ) 
~~ ~~ def _create_link ( self , act_node , name , instance ) : 
act_node . _links [ name ] = instance 
act_node . _children [ name ] = instance 
if full_name not in self . _root_instance . _linked_by : 
~~~ self . _root_instance . _linked_by [ full_name ] = { } 
~~ linking = self . _root_instance . _linked_by [ full_name ] 
if act_node . v_full_name not in linking : 
~~~ linking [ act_node . v_full_name ] = ( act_node , set ( ) ) 
~~ linking [ act_node . v_full_name ] [ 1 ] . add ( name ) 
if name not in self . _links_count : 
~~~ self . _links_count [ name ] = 0 
~~ self . _links_count [ name ] = self . _links_count [ name ] + 1 
instance . v_full_name ) ) 
return instance 
~~ def _check_names ( self , split_names , parent_node = None ) : 
faulty_names = '' 
if parent_node is not None and parent_node . v_is_root and split_names [ 0 ] == 'overview' : 
~~ for split_name in split_names : 
~~~ if len ( split_name ) == 0 : 
faulty_names , split_name ) 
~~ elif split_name . startswith ( '_' ) : 
~~ elif re . match ( CHECK_REGEXP , split_name ) is None : 
~~ elif '$' in split_name : 
~~~ if split_name not in self . _root_instance . _wildcard_keys : 
~~ ~~ elif split_name in self . _not_admissible_names : 
category = SyntaxWarning ) 
~~ elif split_name in self . _python_keywords : 
~~ ~~ name = split_names [ - 1 ] 
if len ( name ) >= pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH : 
pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH ) 
~~ return faulty_names 
~~ def _create_any_group ( self , parent_node , name , type_name , instance = None , constructor = None , 
args = None , kwargs = None ) : 
~~~ args = [ ] 
~~ if kwargs is None : 
~~~ kwargs = { } 
~~ full_name = self . _make_full_name ( parent_node . v_full_name , name ) 
if instance is None : 
~~~ if constructor is None : 
~~~ if type_name == RESULT_GROUP : 
~~~ constructor = ResultGroup 
~~ elif type_name == PARAMETER_GROUP : 
~~~ constructor = ParameterGroup 
~~ elif type_name == CONFIG_GROUP : 
~~~ constructor = ConfigGroup 
~~ elif type_name == DERIVED_PARAMETER_GROUP : 
~~~ constructor = DerivedParameterGroup 
~~ elif type_name == GROUP : 
~~~ constructor = NNGroupNode 
~~ ~~ instance = self . _root_instance . _construct_instance ( constructor , full_name , 
* args , ** kwargs ) 
~~~ instance . _rename ( full_name ) 
if type_name == RESULT_GROUP : 
~~~ if type ( instance ) in ( NNGroupNode , 
ParameterGroup , 
ConfigGroup , 
DerivedParameterGroup ) : 
str ( type ( instance ) ) ) 
~~ ~~ elif type_name == PARAMETER_GROUP : 
ResultGroup , 
~~ ~~ elif type_name == CONFIG_GROUP : 
~~ ~~ elif type_name == DERIVED_PARAMETER_GROUP : 
ResultGroup ) : 
'parameters' % str ( type ( instance ) ) ) 
~~ ~~ elif type_name == GROUP : 
~~~ if type ( instance ) in ( ResultGroup , 
~~ ~~ self . _set_details_tree_node ( parent_node , name , instance ) 
instance . _nn_interface = self 
self . _root_instance . _all_groups [ instance . v_full_name ] = instance 
self . _add_to_nodes_and_leaves ( instance ) 
parent_node . _children [ name ] = instance 
parent_node . _groups [ name ] = instance 
~~ def _create_any_param_or_result ( self , parent_node , name , type_name , instance , constructor , 
args , kwargs ) : 
full_name = self . _make_full_name ( parent_node . v_full_name , name ) 
~~~ if type_name == RESULT : 
~~~ constructor = root . _standard_result 
~~ elif type_name in [ PARAMETER , CONFIG , DERIVED_PARAMETER ] : 
~~~ constructor = root . _standard_parameter 
~~~ constructor = root . _standard_leaf 
~~ ~~ instance = root . _construct_instance ( constructor , full_name , * args , ** kwargs ) 
~~ self . _set_details_tree_node ( parent_node , name , instance ) 
where_dict = self . _map_type_to_dict ( type_name ) 
full_name = instance . _full_name 
if full_name in where_dict : 
~~ if type_name != RESULT and full_name in root . _changed_default_parameters : 
~~~ self . _logger . info ( 
full_name ) 
change_args , change_kwargs = root . _changed_default_parameters . pop ( full_name ) 
instance . f_set ( * change_args , ** change_kwargs ) 
~~ where_dict [ full_name ] = instance 
parent_node . _leaves [ name ] = instance 
if full_name in self . _root_instance . _explored_parameters : 
self . _root_instance . _explored_parameters [ full_name ] = instance 
~~ def _set_details_tree_node ( self , parent_node , name , instance ) : 
depth = parent_node . _depth + 1 
if parent_node . v_is_root : 
~~~ branch = parent_node . _branch 
~~~ run_branch = name 
~~~ run_branch = parent_node . _run_branch 
~~ instance . _set_details ( depth , branch , run_branch ) 
~~ def _iter_nodes ( self , node , recursive = False , max_depth = float ( 'inf' ) , 
with_links = True , in_search = False , predicate = None ) : 
def _run_predicate ( x , run_name_set ) : 
~~~ branch = x . v_run_branch 
return branch == 'trajectory' or branch in run_name_set 
~~ if max_depth is None : 
~~~ max_depth = float ( 'inf' ) 
~~ if predicate is None : 
~~ elif isinstance ( predicate , ( tuple , list ) ) : 
~~~ run_list = predicate 
run_name_set = set ( ) 
for item in run_list : 
~~~ if item == - 1 : 
~~~ run_name_set . add ( self . _root_instance . f_wildcard ( '$' , - 1 ) ) 
~~ elif isinstance ( item , int ) : 
~~~ run_name_set . add ( self . _root_instance . f_idx_to_run ( item ) ) 
~~~ run_name_set . add ( item ) 
~~ ~~ predicate = lambda x : _run_predicate ( x , run_name_set ) 
~~ if recursive : 
~~~ return NaturalNamingInterface . _recursive_traversal_bfs ( node , 
self . _root_instance . _linked_by , 
max_depth , with_links , 
in_search , predicate ) 
~~~ iterator = ( x for x in self . _make_child_iterator ( node , with_links ) if 
predicate ( x [ 2 ] ) ) 
if in_search : 
~~~ return ( x [ 2 ] for x in iterator ) 
~~ ~~ ~~ def _to_dict ( self , node , fast_access = True , short_names = False , nested = False , 
copy = True , with_links = True ) : 
if ( fast_access or short_names or nested ) and not copy : 
~~ if nested and short_names : 
~~ if node . v_is_root : 
~~~ temp_dict = self . _flat_leaf_storage_dict 
if not fast_access and not short_names : 
~~~ if copy : 
~~~ return temp_dict . copy ( ) 
~~~ return temp_dict 
~~~ iterator = temp_dict . values ( ) 
~~~ iterator = node . f_iter_leaves ( with_links = with_links ) 
for val in iterator : 
~~~ if short_names : 
~~~ new_key = val . v_name 
~~~ new_key = val . v_full_name 
~~ if new_key in result_dict : 
~~ new_val = self . _apply_fast_access ( val , fast_access ) 
result_dict [ new_key ] = new_val 
~~ if nested : 
~~~ if node . v_is_root : 
~~~ nest_dict = result_dict 
~~~ strip = len ( node . v_full_name ) + 1 
nest_dict = { key [ strip : ] : val for key , val in result_dict . items ( ) } 
~~ result_dict = nest_dictionary ( nest_dict , '.' ) 
~~ return result_dict 
~~ def _make_child_iterator ( node , with_links , current_depth = 0 ) : 
cdp1 = current_depth + 1 
if with_links : 
~~~ iterator = ( ( cdp1 , x [ 0 ] , x [ 1 ] ) for x in node . _children . items ( ) ) 
~~~ leaves = ( ( cdp1 , x [ 0 ] , x [ 1 ] ) for x in node . _leaves . items ( ) ) 
groups = ( ( cdp1 , y [ 0 ] , y [ 1 ] ) for y in node . _groups . items ( ) ) 
iterator = itools . chain ( groups , leaves ) 
~~ return iterator 
~~ def _recursive_traversal_bfs ( node , linked_by = None , 
max_depth = float ( 'inf' ) , 
~~ iterator_queue = IteratorChain ( [ ( 0 , node . v_name , node ) ] ) 
start = True 
visited_linked_nodes = set ( [ ] ) 
~~~ depth , name , item = next ( iterator_queue ) 
full_name = item . _full_name 
if start or predicate ( item ) : 
~~~ if full_name in visited_linked_nodes : 
~~~ if in_search : 
~~~ yield depth , name , item 
~~ ~~ elif depth <= max_depth : 
~~~ if start : 
~~~ start = False 
~~~ yield item 
~~ ~~ if full_name in linked_by : 
~~~ visited_linked_nodes . add ( full_name ) 
~~ if not item . _is_leaf and depth < max_depth : 
~~~ child_iterator = NaturalNamingInterface . _make_child_iterator ( item , 
with_links , 
current_depth = depth ) 
iterator_queue . add ( child_iterator ) 
~~ ~~ ~~ ~~ except StopIteration : 
~~ ~~ ~~ def _very_fast_search ( self , node , key , max_depth , with_links , crun ) : 
if key in self . _links_count : 
~~ parent_full_name = node . v_full_name 
starting_depth = node . v_depth 
candidate_dict = self . _get_candidate_dict ( key , crun ) 
~~~ upper_bound = 1 
~~~ upper_bound = FAST_UPPER_BOUND 
~~ if len ( candidate_dict ) > upper_bound : 
~~ result_node = None 
for goal_name in candidate_dict : 
~~~ if goal_name . startswith ( parent_full_name ) : 
~~~ candidate = candidate_dict [ goal_name ] 
if candidate . v_depth - starting_depth <= max_depth : 
~~~ if result_node is not None : 
% ( key , goal_name , result_node . v_full_name ) ) 
~~ result_node = candidate 
~~ ~~ ~~ if result_node is not None : 
~~~ return result_node , result_node . v_depth 
~~ ~~ def _search ( self , node , key , max_depth = float ( 'inf' ) , with_links = True , crun = None ) : 
if key in node . _children and ( with_links or key not in node . _links ) : 
~~~ return node . _children [ key ] , 1 
~~~ result = self . _very_fast_search ( node , key , max_depth , with_links , crun ) 
~~~ return result 
~~ ~~ except pex . TooManyGroupsError : 
~~ except pex . NotUniqueNodeError : 
~~ nodes_iterator = self . _iter_nodes ( node , recursive = True , 
max_depth = max_depth , in_search = True , 
with_links = with_links ) 
result_node = None 
result_depth = float ( 'inf' ) 
for depth , name , child in nodes_iterator : 
~~~ if depth > result_depth : 
~~ if key == name : 
% ( key , child . v_depth , result_node . v_full_name , 
child . v_full_name ) ) 
~~ result_node = child 
result_depth = depth 
~~ ~~ return result_node , result_depth 
~~ def _backwards_search ( self , start_node , split_name , max_depth = float ( 'inf' ) , shortcuts = True ) : 
colon_name = '.' . join ( split_name ) 
key = split_name [ - 1 ] 
candidate_dict = self . _get_candidate_dict ( key , None , use_upper_bound = False ) 
parent_full_name = start_node . v_full_name 
split_length = len ( split_name ) 
for candidate_name in candidate_dict : 
~~~ candidate = candidate_dict [ candidate_name ] 
if key != candidate . v_name or candidate . v_full_name in full_name_set : 
~~ if candidate_name . startswith ( parent_full_name ) : 
~~~ if parent_full_name != '' : 
~~~ reduced_candidate_name = candidate_name [ len ( parent_full_name ) + 1 : ] 
~~~ reduced_candidate_name = candidate_name 
~~ candidate_split_name = reduced_candidate_name . split ( '.' ) 
if len ( candidate_split_name ) > max_depth : 
~~ if len ( split_name ) == 1 or reduced_candidate_name . endswith ( colon_name ) : 
~~~ result_list . append ( candidate ) 
full_name_set . add ( candidate . v_full_name ) 
~~ elif shortcuts : 
~~~ candidate_set = set ( candidate_split_name ) 
climbing = True 
for name in split_name : 
~~~ if name not in candidate_set : 
~~~ climbing = False 
~~ ~~ if climbing : 
~~~ count = 0 
candidate_length = len ( candidate_split_name ) 
for idx in range ( candidate_length ) : 
~~~ if idx + split_length - count > candidate_length : 
~~ if split_name [ count ] == candidate_split_name [ idx ] : 
~~~ count += 1 
if count == len ( split_name ) : 
~~ ~~ ~~ ~~ ~~ ~~ ~~ return result_list 
~~ def _get_all ( self , node , name , max_depth , shortcuts ) : 
if max_depth is None : 
~~ if isinstance ( name , list ) : 
~~~ split_name = name 
~~ elif isinstance ( name , tuple ) : 
~~~ split_name = list ( name ) 
~~ elif isinstance ( name , int ) : 
~~~ split_name = [ name ] 
~~~ split_name = name . split ( '.' ) 
~~ for idx , key in enumerate ( split_name ) : 
~~~ _ , key = self . _translate_shortcut ( key ) 
_ , key = self . _replace_wildcards ( key ) 
split_name [ idx ] = key 
~~ return self . _backwards_search ( node , split_name , max_depth , shortcuts ) 
~~ def _get ( self , node , name , fast_access , 
shortcuts , max_depth , auto_load , with_links ) : 
if auto_load and not with_links : 
~~~ if len ( split_name ) == 1 and split_name [ 0 ] == '' : 
~~~ return node 
~~ key = split_name [ 0 ] 
_ , key = self . _translate_shortcut ( key ) 
if key in SUBTREE_MAPPING and key not in node . _children : 
~~~ node . f_add_group ( key ) 
~~ ~~ if max_depth is None : 
~~ if len ( split_name ) > max_depth and shortcuts : 
( str ( name ) , max_depth ) ) 
~~ try_auto_load_directly1 = False 
try_auto_load_directly2 = False 
wildcard_positions = [ ] 
for idx , key in enumerate ( split_name ) : 
~~~ translated_shortcut , key = self . _translate_shortcut ( key ) 
if translated_shortcut : 
~~~ split_name [ idx ] = key 
~~ if key [ 0 ] == '_' : 
~~ is_wildcard = self . _root_instance . f_is_wildcard ( key ) 
if ( not is_wildcard and key not in self . _nodes_and_leaves and 
key not in self . _links_count ) : 
~~~ try_auto_load_directly1 = True 
try_auto_load_directly2 = True 
~~ if is_wildcard : 
~~~ wildcard_positions . append ( ( idx , key ) ) 
if root . f_wildcard ( key ) not in self . _nodes_and_leaves : 
~~ if root . f_wildcard ( key , - 1 ) not in self . _nodes_and_leaves : 
~~~ try_auto_load_directly2 = True 
~~ ~~ ~~ run_idx = root . v_idx 
if try_auto_load_directly1 and try_auto_load_directly2 and not auto_load : 
~~~ for wildcard_pos , wildcard in wildcard_positions : 
~~~ split_name [ wildcard_pos ] = root . f_wildcard ( wildcard , run_idx ) 
str ( '.' . join ( split_name ) ) ) 
~~ if run_idx > - 1 : 
~~~ with self . _disable_logging : 
~~ result = self . _perform_get ( node , split_name , fast_access , 
shortcuts , max_depth , auto_load , with_links , 
try_auto_load_directly1 ) 
~~ except ( pex . DataNotInStorageError , AttributeError ) as exc : 
~~~ wildcard_exception = exc 
~~ ~~ ~~ if wildcard_positions : 
~~~ split_name [ wildcard_pos ] = root . f_wildcard ( wildcard , - 1 ) 
~~~ return self . _perform_get ( node , split_name , fast_access , 
try_auto_load_directly2 ) 
~~ except ( pex . DataNotInStorageError , AttributeError ) : 
~~~ if wildcard_exception is not None : 
~~~ raise wildcard_exception 
~~ ~~ ~~ def _perform_get ( self , node , split_name , fast_access , 
try_auto_load_directly ) : 
result = None 
name = '.' . join ( split_name ) 
if len ( split_name ) > max_depth : 
( name , node . v_full_name ) ) 
~~ if shortcuts and not try_auto_load_directly : 
~~~ first = split_name [ 0 ] 
if len ( split_name ) == 1 and first in node . _children and ( with_links or 
first not in node . _links ) : 
~~~ result = node . _children [ first ] 
~~~ result = self . _check_flat_dicts ( node , split_name ) 
if result is None : 
~~~ result = node 
crun = None 
for key in split_name : 
~~~ if key in self . _root_instance . _run_information : 
~~~ crun = key 
~~ result , depth = self . _search ( result , key , max_depth , with_links , crun ) 
max_depth -= depth 
~~ ~~ ~~ ~~ ~~ elif not try_auto_load_directly : 
~~~ if name in result . _children and ( with_links or not name in result . _links ) : 
~~~ result = result . _children [ name ] 
~~~ raise AttributeError ( 
~~ ~~ ~~ if result is None and auto_load : 
~~~ result = node . f_load_child ( '.' . join ( split_name ) , 
load_data = pypetconstants . LOAD_DATA ) 
if ( self . _root_instance . v_idx != - 1 and 
result . v_is_leaf and 
result . v_is_parameter and 
result . v_explored ) : 
~~~ result . _set_parameter_access ( self . _root_instance . v_idx ) 
~~ ~~ except : 
~~ ~~ if result is None : 
~~ if result . v_is_leaf : 
~~~ if auto_load and result . f_is_empty ( ) : 
~~~ self . _root_instance . f_load_item ( result ) 
~~ ~~ return self . _apply_fast_access ( result , fast_access ) 
~~ ~~ def kids ( self ) : 
if self . _kids is None : 
~~~ self . _kids = NNTreeNodeKids ( self ) 
~~ return self . _kids 
~~ def _add_group_from_storage ( self , args , kwargs ) : 
return self . _nn_interface . _add_generic ( self , 
type_name = GROUP , 
group_type_name = GROUP , 
args = args , 
kwargs = kwargs , 
add_prefix = False , 
check_naming = False ) 
~~ def _add_leaf_from_storage ( self , args , kwargs ) : 
type_name = LEAF , 
args = args , kwargs = kwargs , 
~~ def f_dir_data ( self ) : 
if ( self . _nn_interface is not None and 
self . _nn_interface . _root_instance is not None 
and self . v_root . v_auto_load ) : 
~~~ if self . v_is_root : 
~~~ self . f_load ( recursive = True , max_depth = 1 , 
load_data = pypetconstants . LOAD_SKELETON , 
with_meta_data = False , 
with_run_information = False ) 
~~~ self . f_load ( recursive = True , max_depth = 1 , load_data = pypetconstants . LOAD_SKELETON ) 
~~ ~~ except Exception as exc : 
~~ ~~ return list ( self . _children . keys ( ) ) 
~~ def _debug ( self ) : 
class Bunch ( object ) : 
pass 
~~ debug_tree = Bunch ( ) 
if not self . v_annotations . f_is_empty ( ) : 
~~~ debug_tree . v_annotations = self . v_annotations 
~~ if not self . v_comment == '' : 
~~~ debug_tree . v_comment = self . v_comment 
~~ for leaf_name in self . _leaves : 
~~~ leaf = self . _leaves [ leaf_name ] 
setattr ( debug_tree , leaf_name , leaf ) 
~~ for link_name in self . _links : 
~~~ linked_node = self . _links [ link_name ] 
~~ for group_name in self . _groups : 
~~~ group = self . _groups [ group_name ] 
setattr ( debug_tree , group_name , group . _debug ( ) ) 
~~ return debug_tree 
~~ def f_get_parent ( self ) : 
if self . v_is_root : 
~~ elif self . v_location == '' : 
~~~ return self . v_root 
~~~ return self . v_root . f_get ( self . v_location , fast_access = False , shortcuts = False ) 
~~ ~~ def f_add_group ( self , * args , ** kwargs ) : 
return self . _nn_interface . _add_generic ( self , type_name = GROUP , 
args = args , kwargs = kwargs , add_prefix = False ) 
~~ def f_add_link ( self , name_or_item , full_name_or_item = None ) : 
if isinstance ( name_or_item , str ) : 
~~~ name = name_or_item 
if isinstance ( full_name_or_item , str ) : 
~~~ instance = self . v_root . f_get ( full_name_or_item ) 
~~~ instance = full_name_or_item 
~~~ instance = name_or_item 
name = instance . v_name 
~~ return self . _nn_interface . _add_generic ( self , type_name = LINK , 
group_type_name = GROUP , args = ( name , instance ) , 
kwargs = { } , 
add_prefix = False ) 
~~ def f_remove_link ( self , name ) : 
if name not in self . _links : 
~~ self . _nn_interface . _remove_link ( self , name ) 
~~ def f_add_leaf ( self , * args , ** kwargs ) : 
return self . _nn_interface . _add_generic ( self , type_name = LEAF , 
~~ def f_remove ( self , recursive = True , predicate = None ) : 
parent = self . f_get_parent ( ) 
parent . f_remove_child ( self . v_name , recursive = recursive , predicate = predicate ) 
~~ def f_remove_child ( self , name , recursive = False , predicate = None ) : 
if name not in self . _children : 
( self . v_full_name , name ) ) 
~~~ child = self . _children [ name ] 
if ( name not in self . _links and 
not child . v_is_leaf and 
child . f_has_children ( ) and 
not recursive ) : 
~~~ self . _nn_interface . _remove_subtree ( self , name , predicate ) 
~~ ~~ ~~ def f_contains ( self , item , with_links = True , shortcuts = False , max_depth = None ) : 
~~~ search_string = item . v_full_name 
parent_full_name = self . v_full_name 
if not search_string . startswith ( parent_full_name ) : 
~~ if parent_full_name != '' : 
~~~ search_string = search_string [ len ( parent_full_name ) + 1 : ] 
~~~ search_string = search_string 
~~~ search_string = item 
item = None 
~~ if search_string == '' : 
~~~ result = self . f_get ( search_string , 
shortcuts = shortcuts , max_depth = max_depth , with_links = with_links ) 
~~ if item is not None : 
~~~ return id ( item ) == id ( result ) 
~~ ~~ def f_iter_nodes ( self , recursive = True , with_links = True , max_depth = None , predicate = None ) : 
return self . _nn_interface . _iter_nodes ( self , recursive = recursive , with_links = with_links , 
max_depth = max_depth , 
predicate = predicate ) 
~~ def f_iter_leaves ( self , with_links = True ) : 
for node in self . f_iter_nodes ( with_links = with_links ) : 
~~~ if node . v_is_leaf : 
~~~ yield node 
~~ ~~ ~~ def f_get_all ( self , name , max_depth = None , shortcuts = True ) : 
return self . _nn_interface . _get_all ( self , name , max_depth = max_depth , shortcuts = shortcuts ) 
~~ def f_get_default ( self , name , default = None , fast_access = True , with_links = True , 
shortcuts = True , max_depth = None , auto_load = False ) : 
~~~ return self . f_get ( name , fast_access = fast_access , 
shortcuts = shortcuts , 
auto_load = auto_load , 
~~ except ( AttributeError , pex . DataNotInStorageError ) : 
~~~ return default 
~~ ~~ def f_get ( self , name , fast_access = False , with_links = True , 
return self . _nn_interface . _get ( self , name , fast_access = fast_access , 
~~ def f_get_children ( self , copy = True ) : 
if copy : 
~~~ return self . _children . copy ( ) 
~~~ return self . _children 
~~ ~~ def f_get_groups ( self , copy = True ) : 
~~~ return self . _groups . copy ( ) 
~~~ return self . _groups 
~~ ~~ def f_get_leaves ( self , copy = True ) : 
~~~ return self . _leaves . copy ( ) 
~~~ return self . _leaves 
~~ ~~ def f_get_links ( self , copy = True ) : 
~~~ return self . _links . copy ( ) 
~~~ return self . _links 
~~ ~~ def f_store_child ( self , name , recursive = False , store_data = pypetconstants . STORE_DATA , 
max_depth = None ) : 
if not self . f_contains ( name , shortcuts = False ) : 
~~ traj = self . _nn_interface . _root_instance 
storage_service = traj . v_storage_service 
storage_service . store ( pypetconstants . TREE , self , name , 
trajectory_name = traj . v_name , 
recursive = recursive , 
store_data = store_data , 
max_depth = max_depth ) 
~~ def f_store ( self , recursive = True , store_data = pypetconstants . STORE_DATA , 
traj = self . _nn_interface . _root_instance 
storage_service . store ( pypetconstants . GROUP , self , 
~~ def f_load_child ( self , name , recursive = False , load_data = pypetconstants . LOAD_DATA , 
storage_service . load ( pypetconstants . TREE , self , name , 
load_data = load_data , 
return self . f_get ( name , shortcuts = False ) 
~~ def f_load ( self , recursive = True , load_data = pypetconstants . LOAD_DATA , 
storage_service . load ( pypetconstants . GROUP , self , 
return self 
~~ def f_add_parameter_group ( self , * args , ** kwargs ) : 
return self . _nn_interface . _add_generic ( self , type_name = PARAMETER_GROUP , 
group_type_name = PARAMETER_GROUP , 
args = args , kwargs = kwargs ) 
~~ def f_add_parameter ( self , * args , ** kwargs ) : 
return self . _nn_interface . _add_generic ( self , type_name = PARAMETER , 
~~ def f_add_result_group ( self , * args , ** kwargs ) : 
return self . _nn_interface . _add_generic ( self , type_name = RESULT_GROUP , 
group_type_name = RESULT_GROUP , 
~~ def f_add_result ( self , * args , ** kwargs ) : 
return self . _nn_interface . _add_generic ( self , type_name = RESULT , 
~~ def f_add_derived_parameter_group ( self , * args , ** kwargs ) : 
return self . _nn_interface . _add_generic ( self , type_name = DERIVED_PARAMETER_GROUP , 
group_type_name = DERIVED_PARAMETER_GROUP , 
~~ def f_add_derived_parameter ( self , * args , ** kwargs ) : 
return self . _nn_interface . _add_generic ( self , type_name = DERIVED_PARAMETER , 
~~ def f_add_config_group ( self , * args , ** kwargs ) : 
return self . _nn_interface . _add_generic ( self , type_name = CONFIG_GROUP , 
group_type_name = CONFIG_GROUP , 
~~ def f_add_config ( self , * args , ** kwargs ) : 
return self . _nn_interface . _add_generic ( self , type_name = CONFIG , 
~~ def eval_one_max ( traj , individual ) : 
traj . f_add_result ( '$set.$.individual' , list ( individual ) ) 
fitness = sum ( individual ) 
traj . f_add_result ( '$set.$.fitness' , fitness ) 
traj . f_store ( ) 
return ( fitness , ) 
~~ def unit_from_expression ( expr ) : 
if expr == '1' : 
~~~ return get_unit_fast ( 1 ) 
~~ elif isinstance ( expr , str ) : 
~~~ mod = ast . parse ( expr , mode = 'eval' ) 
expr = mod . body 
return unit_from_expression ( expr ) 
~~ elif expr . __class__ is ast . Name : 
~~~ return ALLUNITS [ expr . id ] 
~~ elif expr . __class__ is ast . Num : 
~~~ return expr . n 
~~ elif expr . __class__ is ast . UnaryOp : 
~~~ op = expr . op . __class__ . __name__ 
operand = unit_from_expression ( expr . operand ) 
if op == 'USub' : 
~~~ return - operand 
~~ ~~ elif expr . __class__ is ast . BinOp : 
left = unit_from_expression ( expr . left ) 
right = unit_from_expression ( expr . right ) 
if op == 'Add' : 
~~~ u = left + right 
~~ elif op == 'Sub' : 
~~~ u = left - right 
~~ elif op == 'Mult' : 
~~~ u = left * right 
~~ elif op == 'Div' : 
~~~ u = left / right 
~~ elif op == 'Pow' : 
~~~ n = unit_from_expression ( expr . right ) 
u = left ** n 
~~ elif op == 'Mod' : 
~~~ u = left % right 
~~ return u 
~~ ~~ def f_supports ( self , data ) : 
if isinstance ( data , Quantity ) : 
~~ elif super ( Brian2Parameter , self ) . f_supports ( data ) : 
~~ def _supports ( self , data ) : 
~~ elif super ( Brian2Result , self ) . _supports ( data ) : 
~~ def f_set_single ( self , name , item ) : 
if type ( item ) in [ SpikeMonitor , StateMonitor , PopulationRateMonitor ] : 
~~~ if self . v_stored : 
~~ self . _extract_monitor_data ( item ) 
~~~ super ( Brian2MonitorResult , self ) . f_set_single ( name , item ) 
~~ ~~ def add_commit_variables ( traj , commit ) : 
git_time_value = time . strftime ( '%Y_%m_%d_%Hh%Mm%Ss' , time . localtime ( commit . committed_date ) ) 
git_short_name = str ( commit . hexsha [ 0 : 7 ] ) 
git_commit_name = 'commit_%s_' % git_short_name 
git_commit_name = 'git.' + git_commit_name + git_time_value 
if not traj . f_contains ( 'config.' + git_commit_name , shortcuts = False ) : 
~~~ git_commit_name += '.' 
traj . f_add_config ( git_commit_name + 'hexsha' , commit . hexsha , 
traj . f_add_config ( git_commit_name + 'name_rev' , commit . name_rev , 
traj . f_add_config ( git_commit_name + 'committed_date' , 
traj . f_add_config ( git_commit_name + 'message' , str ( commit . message ) , 
~~ ~~ def make_git_commit ( environment , git_repository , user_message , git_fail ) : 
repo = git . Repo ( git_repository ) 
index = repo . index 
traj = environment . v_trajectory 
if traj . v_comment : 
~~~ commentstr = '' 
~~ if user_message : 
diff = index . diff ( None ) 
if diff : 
~~~ if git_fail : 
~~ repo . git . add ( '-u' ) 
commit = index . commit ( message ) 
new_commit = True 
~~~ commit = repo . commit ( None ) 
new_commit = False 
~~ add_commit_variables ( traj , commit ) 
return new_commit , commit . hexsha 
~~ def flatten_dictionary ( nested_dict , separator ) : 
flat_dict = { } 
for key , val in nested_dict . items ( ) : 
~~~ if isinstance ( val , dict ) : 
~~~ new_flat_dict = flatten_dictionary ( val , separator ) 
for flat_key , inval in new_flat_dict . items ( ) : 
~~~ new_key = key + separator + flat_key 
flat_dict [ new_key ] = inval 
~~~ flat_dict [ key ] = val 
~~ ~~ return flat_dict 
~~ def nest_dictionary ( flat_dict , separator ) : 
nested_dict = { } 
for key , val in flat_dict . items ( ) : 
~~~ split_key = key . split ( separator ) 
act_dict = nested_dict 
final_key = split_key . pop ( ) 
for new_key in split_key : 
~~~ if not new_key in act_dict : 
~~~ act_dict [ new_key ] = { } 
~~ act_dict = act_dict [ new_key ] 
~~ act_dict [ final_key ] = val 
~~ return nested_dict 
~~ def progressbar ( index , total , percentage_step = 10 , logger = 'print' , log_level = logging . INFO , 
reprint = True , time = True , length = 20 , fmt_string = None , reset = False ) : 
return _progressbar ( index = index , total = total , percentage_step = percentage_step , 
logger = logger , log_level = log_level , reprint = reprint , 
time = time , length = length , fmt_string = fmt_string , reset = reset ) 
~~ def _get_argspec ( func ) : 
if inspect . isclass ( func ) : 
~~~ func = func . __init__ 
~~ if not inspect . isfunction ( func ) : 
~~~ return [ ] , False 
~~ parameters = inspect . signature ( func ) . parameters 
args = [ ] 
uses_starstar = False 
for par in parameters . values ( ) : 
~~~ if ( par . kind == inspect . Parameter . POSITIONAL_OR_KEYWORD or 
par . kind == inspect . Parameter . KEYWORD_ONLY ) : 
~~~ args . append ( par . name ) 
~~ elif par . kind == inspect . Parameter . VAR_KEYWORD : 
~~~ uses_starstar = True 
~~ ~~ return args , uses_starstar 
~~ def get_matching_kwargs ( func , kwargs ) : 
args , uses_startstar = _get_argspec ( func ) 
if uses_startstar : 
~~~ return kwargs . copy ( ) 
~~~ matching_kwargs = dict ( ( k , kwargs [ k ] ) for k in args if k in kwargs ) 
return matching_kwargs 
~~ ~~ def result_sort ( result_list , start_index = 0 ) : 
if len ( result_list ) < 2 : 
~~~ return result_list 
~~ to_sort = result_list [ start_index : ] 
minmax = [ x [ 0 ] for x in to_sort ] 
minimum = min ( minmax ) 
maximum = max ( minmax ) 
sorted_list = [ None for _ in range ( minimum , maximum + 1 ) ] 
for elem in to_sort : 
~~~ key = elem [ 0 ] - minimum 
sorted_list [ key ] = elem 
~~ idx_count = start_index 
for elem in sorted_list : 
~~~ if elem is not None : 
~~~ result_list [ idx_count ] = elem 
idx_count += 1 
~~ ~~ return result_list 
~~ def format_time ( timestamp ) : 
format_string = '%Y_%m_%d_%Hh%Mm%Ss' 
formatted_time = datetime . datetime . fromtimestamp ( timestamp ) . strftime ( format_string ) 
return formatted_time 
~~ def port_to_tcp ( port = None ) : 
domain_name = socket . getfqdn ( ) 
~~~ addr_list = socket . getaddrinfo ( domain_name , None ) 
~~ except Exception : 
~~~ addr_list = socket . getaddrinfo ( '127.0.0.1' , None ) 
~~ family , socktype , proto , canonname , sockaddr = addr_list [ 0 ] 
host = convert_ipv6 ( sockaddr [ 0 ] ) 
address = 'tcp://' + host 
if port is None : 
~~~ port = ( ) 
~~ if not isinstance ( port , int ) : 
~~~ context = zmq . Context ( ) 
~~~ socket_ = context . socket ( zmq . REP ) 
socket_ . ipv6 = is_ipv6 ( address ) 
port = socket_ . bind_to_random_port ( address , * port ) 
pypet_root_logger = logging . getLogger ( 'pypet' ) 
~~ socket_ . close ( ) 
context . term ( ) 
~~ return address + ':' + str ( port ) 
~~ def racedirs ( path ) : 
if os . path . isfile ( path ) : 
~~~ if os . path . isdir ( path ) : 
~~ os . makedirs ( path ) 
~~ except EnvironmentError as exc : 
~~~ if exc . errno != 17 : 
~~ ~~ ~~ ~~ def _reset ( self , index , total , percentage_step , length ) : 
self . _start_time = datetime . datetime . now ( ) 
self . _start_index = index 
self . _current_index = index 
self . _percentage_step = percentage_step 
self . _total = float ( total ) 
self . _total_minus_one = total - 1 
self . _length = length 
self . _norm_factor = total * percentage_step / 100.0 
self . _current_interval = int ( ( index + 1.0 ) / self . _norm_factor ) 
~~ def _get_remaining ( self , index ) : 
~~~ current_time = datetime . datetime . now ( ) 
time_delta = current_time - self . _start_time 
~~~ total_seconds = time_delta . total_seconds ( ) 
~~~ total_seconds = ( ( time_delta . microseconds + 
( time_delta . seconds + 
time_delta . days * 24 * 3600 ) * 10 ** 6 ) / 10.0 ** 6 ) 
~~ remaining_seconds = int ( ( self . _total - self . _start_index - 1.0 ) * 
total_seconds / float ( index - self . _start_index ) - 
total_seconds ) 
remaining_delta = datetime . timedelta ( seconds = remaining_seconds ) 
~~~ remaining_str = '' 
~~ return remaining_str 
~~ def f_to_dict ( self , copy = True ) : 
~~~ return self . _dict . copy ( ) 
~~~ return self . _dict 
~~ ~~ def f_get ( self , * args ) : 
if len ( args ) == 0 : 
~~~ if len ( self . _dict ) == 1 : 
~~~ return self . _dict [ list ( self . _dict . keys ( ) ) [ 0 ] ] 
~~ elif len ( self . _dict ) > 1 : 
( str ( list ( self . _dict . keys ( ) ) ) ) ) 
~~ ~~ result_list = [ ] 
for name in args : 
~~~ name = self . _translate_key ( name ) 
~~~ result_list . append ( self . _dict [ name ] ) 
~~ ~~ if len ( args ) == 1 : 
~~~ return result_list [ 0 ] 
~~~ return tuple ( result_list ) 
~~ ~~ def f_set ( self , * args , ** kwargs ) : 
for idx , arg in enumerate ( args ) : 
~~~ valstr = self . _translate_key ( idx ) 
self . f_set_single ( valstr , arg ) 
~~ for key , arg in kwargs . items ( ) : 
~~~ self . f_set_single ( key , arg ) 
~~ ~~ def f_remove ( self , key ) : 
key = self . _translate_key ( key ) 
~~~ del self . _dict [ key ] 
~~ ~~ def f_ann_to_str ( self ) : 
resstr = '' 
for key in sorted ( self . _dict . keys ( ) ) : 
~~ return resstr [ : - 2 ] 
~~ def make_ordinary_result ( result , key , trajectory = None , reload = True ) : 
shared_data = result . f_get ( key ) 
if trajectory is not None : 
~~~ shared_data . traj = trajectory 
~~ shared_data . _request_data ( 'make_ordinary' ) 
result . f_remove ( key ) 
if reload : 
~~~ trajectory . f_load_item ( result , load_data = pypetconstants . OVERWRITE_DATA ) 
~~ def make_shared_result ( result , key , trajectory , new_class = None ) : 
data = result . f_get ( key ) 
if new_class is None : 
~~~ if isinstance ( data , ObjectTable ) : 
~~~ new_class = SharedTable 
~~ elif isinstance ( data , pd . DataFrame ) : 
~~~ new_class = SharedPandasFrame 
~~ elif isinstance ( data , ( tuple , list ) ) : 
~~~ new_class = SharedArray 
~~ elif isinstance ( data , ( np . ndarray , np . matrix ) ) : 
~~~ new_class = SharedCArray 
~~ ~~ shared_data = new_class ( result . f_translate_key ( key ) , result , trajectory = trajectory ) 
result [ key ] = shared_data 
shared_data . _request_data ( 'make_shared' ) 
~~ def create_shared_data ( self , ** kwargs ) : 
if 'flag' not in kwargs : 
~~~ kwargs [ 'flag' ] = self . FLAG 
~~ if 'data' in kwargs : 
~~~ kwargs [ 'obj' ] = kwargs . pop ( 'data' ) 
~~ if 'trajectory' in kwargs : 
~~~ self . traj = kwargs . pop ( 'trajectory' ) 
~~ if 'traj' in kwargs : 
~~~ self . traj = kwargs . pop ( 'traj' ) 
~~ if 'name' in kwargs : 
~~~ self . name = kwargs . pop [ 'name' ] 
~~ if 'parent' in kwargs : 
~~~ self . parent = kwargs . pop ( 'parent' ) 
if self . name is not None : 
~~~ self . parent [ self . name ] = self 
~~ ~~ return self . _request_data ( 'create_shared_data' , kwargs = kwargs ) 
~~ def _request_data ( self , request , args = None , kwargs = None ) : 
return self . _storage_service . store ( pypetconstants . ACCESS_DATA , 
self . parent . v_full_name , 
self . name , 
request , args , kwargs , 
trajectory_name = self . traj . v_name ) 
~~ def get_data_node ( self ) : 
if not self . _storage_service . is_open : 
category = RuntimeWarning ) 
~~ return self . _request_data ( '__thenode__' ) 
~~ def _supports ( self , item ) : 
result = super ( SharedResult , self ) . _supports ( item ) 
result = result or type ( item ) in SharedResult . SUPPORTED_DATA 
~~ def create_shared_data ( self , name = None , ** kwargs ) : 
~~~ item = self . f_get ( ) 
~~~ item = self . f_get ( name ) 
~~ return item . create_shared_data ( ** kwargs ) 
~~ def manipulate_multiproc_safe ( traj ) : 
traj . last_process_name = mp . current_process ( ) . name 
traj . results . f_store ( store_data = 3 ) 
~~ def multiply ( traj , result_list ) : 
z = traj . x * traj . y 
result_list [ traj . v_idx ] = z 
~~ def _lock ( self , name , client_id , request_id ) : 
if name in self . _locks : 
~~~ other_client_id , other_request_id = self . _locks [ name ] 
if other_client_id == client_id : 
~~~ response = ( self . LOCK_ERROR + self . DELIMITER + 
self . _logger . warning ( response ) 
return response 
~~~ return self . WAIT 
~~~ self . _locks [ name ] = ( client_id , request_id ) 
return self . GO 
~~ ~~ def _unlock ( self , name , client_id , request_id ) : 
if other_client_id != client_id : 
~~~ response = ( self . RELEASE_ERROR + self . DELIMITER + 
other_client_id , 
other_request_id , 
client_id , 
request_id ) ) 
self . _logger . error ( response ) 
~~~ del self . _locks [ name ] 
return self . RELEASED 
~~~ self . _start ( ) 
running = True 
while running : 
~~~ msg = '' 
client_id = '' 
request_id = '' 
request = self . _socket . recv_string ( ) 
split_msg = request . split ( self . DELIMITER ) 
if len ( split_msg ) == 4 : 
~~~ msg , name , client_id , request_id = split_msg 
~~ if msg == self . LOCK : 
~~~ response = self . _lock ( name , client_id , request_id ) 
~~ elif msg == self . UNLOCK : 
~~~ response = self . _unlock ( name , client_id , request_id ) 
~~ elif msg == self . PING : 
~~~ response = self . PONG 
~~ elif msg == self . DONE : 
~~~ response = self . CLOSED 
running = False 
~~~ response = ( self . MSG_ERROR + self . DELIMITER + 
~~ respond = self . _pre_respond_hook ( response ) 
if respond : 
response , client_id , request_id ) 
self . _socket . send_string ( response ) 
~~ ~~ self . _close ( ) 
~~ ~~ def _lock ( self , name , client_id , request_id ) : 
~~~ other_client_id , other_request_id , lock_time = self . _locks [ name ] 
~~~ current_time = time . time ( ) 
if current_time - lock_time < self . _timeout : 
other_request_id ) ) 
self . _logger . info ( response ) 
self . _locks [ name ] = ( client_id , request_id , time . time ( ) ) 
self . _timeout_locks [ ( name , other_client_id ) ] = ( request_id , lock_time ) 
~~~ self . _locks [ name ] = ( client_id , request_id , time . time ( ) ) 
~~ ~~ elif ( name , client_id ) in self . _timeout_locks : 
~~~ other_request_id , lock_time = self . _timeout_locks [ ( name , client_id ) ] 
timeout = time . time ( ) - lock_time - self . _timeout 
response = ( self . RELEASE_ERROR + self . DELIMITER + 
~~ ~~ def send_done ( self ) : 
self . start ( test_connection = False ) 
self . _req_rep ( ZMQServer . DONE ) 
~~ def finalize ( self ) : 
if self . _context is not None : 
~~~ if self . _socket is not None : 
~~~ self . _close_socket ( confused = False ) 
~~ self . _context . term ( ) 
self . _context = None 
self . _poll = None 
~~ ~~ def start ( self , test_connection = True ) : 
if self . _context is None : 
self . _context = zmq . Context ( ) 
self . _poll = zmq . Poller ( ) 
self . _start_socket ( ) 
if test_connection : 
~~~ self . test_ping ( ) 
~~ ~~ ~~ def _req_rep_retry ( self , request ) : 
retries_left = self . RETRIES 
while retries_left : 
self . _send_request ( request ) 
socks = dict ( self . _poll . poll ( self . TIMEOUT ) ) 
if socks . get ( self . _socket ) == zmq . POLLIN : 
~~~ response = self . _receive_response ( ) 
return response , self . RETRIES - retries_left 
retries_left ) 
self . _close_socket ( confused = True ) 
retries_left -= 1 
if retries_left == 0 : 
~~ time . sleep ( self . SLEEP ) 
~~ ~~ ~~ def acquire ( self ) : 
~~~ str_response , retries = self . _req_rep_retry ( LockerServer . LOCK ) 
response = str_response . split ( LockerServer . DELIMITER ) 
if response [ 0 ] == LockerServer . GO : 
~~ elif response [ 0 ] == LockerServer . LOCK_ERROR and retries > 0 : 
~~ elif response [ 0 ] == LockerServer . WAIT : 
~~~ time . sleep ( self . SLEEP ) 
~~ ~~ ~~ def release ( self ) : 
str_response , retries = self . _req_rep_retry ( LockerServer . UNLOCK ) 
if response [ 0 ] == LockerServer . RELEASED : 
~~ elif response [ 0 ] == LockerServer . RELEASE_ERROR and retries > 0 : 
count = 0 
self . _start ( ) 
~~~ result = self . _socket . recv_pyobj ( ) 
if isinstance ( result , tuple ) : 
~~~ request , data = result 
~~~ request = result 
~~ if request == self . SPACE : 
~~~ if self . queue . qsize ( ) + count < self . queue_maxsize : 
~~~ self . _socket . send_string ( self . SPACE_AVAILABLE ) 
count += 1 
~~~ self . _socket . send_string ( self . SPACE_NOT_AVAILABLE ) 
~~ ~~ elif request == self . PING : 
~~~ self . _socket . send_string ( self . PONG ) 
~~ elif request == self . DATA : 
~~~ self . _socket . send_string ( self . STORING ) 
self . queue . put ( data ) 
count -= 1 
~~ elif request == self . DONE : 
~~~ self . _socket . send_string ( ZMQServer . CLOSED ) 
self . queue . put ( ( 'DONE' , [ ] , { } ) ) 
self . _close ( ) 
~~ ~~ ~~ def put ( self , data , block = True ) : 
~~~ response = self . _req_rep ( QueuingServerMessageListener . SPACE ) 
if response == QueuingServerMessageListener . SPACE_AVAILABLE : 
~~~ self . _req_rep ( ( QueuingServerMessageListener . DATA , data ) ) 
~~~ time . sleep ( 0.01 ) 
~~ ~~ ~~ def _detect_fork ( self ) : 
if self . _pid is None : 
~~~ self . _pid = os . getpid ( ) 
~~ if self . _context is not None : 
~~~ current_pid = os . getpid ( ) 
if current_pid != self . _pid : 
self . _pid = current_pid 
~~ ~~ ~~ def start ( self , test_connection = True ) : 
self . _detect_fork ( ) 
super ( ForkAwareLockerClient , self ) . start ( test_connection ) 
~~ def _put_on_queue ( self , to_put ) : 
old = self . pickle_queue 
self . pickle_queue = False 
~~~ self . queue . put ( to_put , block = True ) 
~~~ self . pickle_queue = old 
~~ ~~ def _put_on_pipe ( self , to_put ) : 
self . acquire_lock ( ) 
self . _send_chunks ( to_put ) 
self . release_lock ( ) 
~~ def _handle_data ( self , msg , args , kwargs ) : 
stop = False 
~~~ if msg == 'DONE' : 
~~~ stop = True 
~~ elif msg == 'STORE' : 
~~~ if 'msg' in kwargs : 
~~~ store_msg = kwargs . pop ( 'msg' ) 
~~~ store_msg = args [ 0 ] 
args = args [ 1 : ] 
~~ if 'stuff_to_store' in kwargs : 
~~~ stuff_to_store = kwargs . pop ( 'stuff_to_store' ) 
~~~ stuff_to_store = args [ 0 ] 
~~ trajectory_name = kwargs [ 'trajectory_name' ] 
if self . _trajectory_name != trajectory_name : 
~~~ if self . _storage_service . is_open : 
~~~ self . _close_file ( ) 
~~ self . _trajectory_name = trajectory_name 
self . _open_file ( ) 
~~ self . _storage_service . store ( store_msg , stuff_to_store , * args , ** kwargs ) 
self . _storage_service . store ( pypetconstants . FLUSH , None ) 
self . _check_and_collect_garbage ( ) 
'`%s`.' % msg ) 
~~ ~~ except Exception : 
time . sleep ( 0.01 ) 
~~ return stop 
~~ def run ( self ) : 
~~~ msg , args , kwargs = self . _receive_data ( ) 
stop = self . _handle_data ( msg , args , kwargs ) 
if stop : 
~~ ~~ ~~ finally : 
~~ self . _trajectory_name = '' 
~~ ~~ def _receive_data ( self ) : 
result = self . queue . get ( block = True ) 
if hasattr ( self . queue , 'task_done' ) : 
~~~ self . queue . task_done ( ) 
~~ def _receive_data ( self ) : 
~~~ while len ( self . _buffer ) < self . max_size and self . conn . poll ( ) : 
~~~ data = self . _read_chunks ( ) 
~~~ self . _buffer . append ( data ) 
~~ ~~ if len ( self . _buffer ) > 0 : 
~~~ return self . _buffer . popleft ( ) 
~~ ~~ ~~ def store ( self , * args , ** kwargs ) : 
~~~ self . acquire_lock ( ) 
return self . _storage_service . store ( * args , ** kwargs ) 
~~~ if self . lock is not None : 
~~~ self . release_lock ( ) 
~~ except RuntimeError : 
~~ ~~ ~~ ~~ def store ( self , msg , stuff_to_store , * args , ** kwargs ) : 
trajectory_name = kwargs [ 'trajectory_name' ] 
if trajectory_name not in self . references : 
~~~ self . references [ trajectory_name ] = [ ] 
~~ self . references [ trajectory_name ] . append ( ( msg , cp . copy ( stuff_to_store ) , args , kwargs ) ) 
~~ def store_references ( self , references ) : 
for trajectory_name in references : 
~~~ self . _storage_service . store ( pypetconstants . LIST , references [ trajectory_name ] , trajectory_name = trajectory_name ) 
~~ self . _check_and_collect_garbage ( ) 
~~ def parse_config ( init_func ) : 
@ functools . wraps ( init_func ) 
def new_func ( env , * args , ** kwargs ) : 
~~~ config_interpreter = ConfigInterpreter ( kwargs ) 
new_kwargs = config_interpreter . interpret ( ) 
init_func ( env , * args , ** new_kwargs ) 
config_interpreter . add_parameters ( env . traj ) 
~~ def _collect_section ( self , section ) : 
kwargs = { } 
~~~ if self . parser . has_section ( section ) : 
~~~ options = self . parser . options ( section ) 
~~~ str_val = self . parser . get ( section , option ) 
val = ast . literal_eval ( str_val ) 
kwargs [ option ] = val 
~~ ~~ return kwargs 
~~ ~~ def _collect_config ( self ) : 
sections = ( 'storage_service' , 'trajectory' , 'environment' ) 
~~~ kwargs . update ( self . _collect_section ( section ) ) 
~~ return kwargs 
~~ def interpret ( self ) : 
if self . config_file : 
~~~ new_kwargs = self . _collect_config ( ) 
for key in new_kwargs : 
~~~ if key not in self . kwargs : 
~~~ self . kwargs [ key ] = new_kwargs [ key ] 
~~ ~~ if not use_simple_logging ( self . kwargs ) and 'log_config' not in self . kwargs : 
~~~ self . kwargs [ 'log_config' ] = self . config_file 
~~ ~~ return self . kwargs 
~~~ parameters = self . _collect_section ( 'parameters' ) 
for name in parameters : 
~~~ value = parameters [ name ] 
if not isinstance ( value , tuple ) : 
~~~ value = ( value , ) 
~~ traj . f_add_parameter ( name , * value ) 
~~ config = self . _collect_section ( 'config' ) 
for name in config : 
~~~ value = config [ name ] 
~~ traj . f_add_config ( name , * value ) 
~~ ~~ ~~ def convert_rule ( rule_number ) : 
binary_rule = [ ( rule_number // pow ( 2 , i ) ) % 2 for i in range ( 8 ) ] 
return np . array ( binary_rule ) 
~~ def make_initial_state ( name , ncells , seed = 42 ) : 
if name == 'single' : 
~~~ just_one_cell = np . zeros ( ncells ) 
just_one_cell [ int ( ncells / 2 ) ] = 1.0 
return just_one_cell 
~~ elif name == 'random' : 
~~~ np . random . seed ( seed ) 
random_init = np . random . randint ( 2 , size = ncells ) 
return random_init 
~~ ~~ def plot_pattern ( pattern , rule_number , filename ) : 
plt . imshow ( pattern ) 
#plt.show() 
~~ def cellular_automaton_1D ( initial_state , rule_number , steps ) : 
ncells = len ( initial_state ) 
pattern = np . zeros ( ( steps , ncells ) ) 
pattern [ 0 , : ] = initial_state 
binary_rule = convert_rule ( rule_number ) 
neighbourhood_factors = np . array ( [ 1 , 2 , 4 ] ) 
all_cells = range ( ncells ) 
for step in range ( steps - 1 ) : 
~~~ current_row = pattern [ step , : ] 
next_row = pattern [ step + 1 , : ] 
for irun in all_cells : 
~~~ neighbour_indices = range ( irun - 1 , irun + 2 ) 
neighbourhood = np . take ( current_row , neighbour_indices , mode = 'wrap' ) 
decimal_neighborhood = int ( np . sum ( neighbourhood * neighbourhood_factors ) ) 
next_state = binary_rule [ decimal_neighborhood ] 
next_row [ irun ] = next_state 
~~ ~~ return pattern 
folder = os . path . join ( os . getcwd ( ) , 'experiments' , 'ca_patterns_original' ) 
if not os . path . isdir ( folder ) : 
~~~ os . makedirs ( folder ) 
~~ filename = os . path . join ( folder , 'all_patterns.p' ) 
for idx , rule_number in enumerate ( rules_to_test ) : 
~~~ for initial_name in initial_states : 
~~~ initial_state = make_initial_state ( initial_name , ncells , seed = seed ) 
pattern = cellular_automaton_1D ( initial_state , rule_number , steps ) 
all_patterns . append ( ( rule_number , initial_name , pattern ) ) 
~~ progressbar ( idx , len ( rules_to_test ) , reprint = True ) 
~~ with open ( filename , 'wb' ) as file : 
~~~ pickle . dump ( all_patterns , file = file ) 
for idx , pattern_tuple in enumerate ( all_patterns ) : 
~~~ rule_number , initial_name , pattern = pattern_tuple 
filename = os . path . join ( folder , 'rule_%s_%s.png' % ( str ( rule_number ) , initial_name ) ) 
plot_pattern ( pattern , rule_number , filename ) 
progressbar ( idx , len ( all_patterns ) , reprint = True ) 
~~ ~~ def get_all_slots ( cls ) : 
slots_iterator = ( getattr ( c , '__slots__' , ( ) ) for c in cls . __mro__ ) 
slots_converted = ( ( slots , ) if isinstance ( slots , str ) else slots 
for slots in slots_iterator ) 
all_slots = set ( ) 
all_slots . update ( * slots_converted ) 
return all_slots 
~~ def signal_update ( self ) : 
if not self . active : 
~~ self . _updates += 1 
current_time = time . time ( ) 
dt = current_time - self . _last_time 
if dt > self . _display_time : 
~~~ dfullt = current_time - self . _start_time 
seconds = int ( dfullt ) % 60 
minutes = int ( dfullt ) / 60 
if minutes == 0 : 
~~~ formatted_time = '%ds' % seconds 
~~~ formatted_time = '%dm%02ds' % ( minutes , seconds ) 
~~ nodespersecond = self . _updates / dfullt 
self . _logger . info ( message ) 
self . _last_time = current_time 
~~ ~~ def _overview_group ( self ) : 
if self . _overview_group_ is None : 
~~~ self . _overview_group_ = self . _all_create_or_get_groups ( 'overview' ) [ 0 ] 
~~ return self . _overview_group_ 
~~ def _all_get_filters ( self , kwargs = None ) : 
if kwargs is None : 
~~ complib = kwargs . pop ( 'complib' , None ) 
complevel = kwargs . pop ( 'complevel' , None ) 
shuffle = kwargs . pop ( 'shuffle' , None ) 
fletcher32 = kwargs . pop ( 'fletcher32' , None ) 
if complib is not None : 
~~~ self . _filters = None 
~~~ complib = self . _complib 
~~ if complevel is not None : 
~~~ complevel = self . _complevel 
~~ if shuffle is not None : 
~~~ shuffle = self . _shuffle 
~~ if fletcher32 is not None : 
~~~ fletcher32 = self . _fletcher32 
~~ if self . _filters is None : 
~~~ self . _filters = pt . Filters ( complib = complib , complevel = complevel , 
shuffle = shuffle , fletcher32 = fletcher32 ) 
self . _hdf5file . filters = self . _filters 
self . _hdf5store . _filters = self . _filters 
self . _hdf5store . _complevel = complevel 
self . _hdf5store . _complib = complib 
self . _hdf5store . _fletcher32 = fletcher32 
~~ return self . _filters 
~~ def _srvc_set_config ( self , trajectory ) : 
def _set_config ( name , value , comment ) : 
~~~ if not trajectory . f_contains ( 'config.' + name , shortcuts = False ) : 
~~~ trajectory . f_add_config ( Parameter , name , value , comment = comment ) 
~~ ~~ for attr_name in HDF5StorageService . NAME_TABLE_MAPPING : 
~~~ table_name = HDF5StorageService . NAME_TABLE_MAPPING [ attr_name ] 
value = getattr ( self , attr_name ) 
_set_config ( 'hdf5.overview.' + table_name , 
value , 
~~ _set_config ( 'hdf5.purge_duplicate_comments' , 
self . _purge_duplicate_comments , 
_set_config ( 'hdf5.results_per_run' , self . _results_per_run , 
_set_config ( 'hdf5.derived_parameters_per_run' , 
self . _derived_parameters_per_run , 
_set_config ( 'hdf5.complevel' , self . _complevel , 
_set_config ( 'hdf5.complib' , self . _complib , 
_set_config ( 'hdf5.encoding' , self . _encoding , 
_set_config ( 'hdf5.fletcher32' , self . _fletcher32 , 
_set_config ( 'hdf5.shuffle' , self . _shuffle , 
_set_config ( 'hdf5.pandas_format' , self . _pandas_format , 
if trajectory . f_contains ( 'config.hdf5' , shortcuts = False ) : 
~~~ if trajectory . config . hdf5 . v_comment == '' : 
~~ ~~ trajectory . v_storage_service = self 
~~ def load ( self , msg , stuff_to_load , * args , ** kwargs ) : 
opened = True 
~~~ opened = self . _srvc_opening_routine ( 'r' , kwargs = kwargs ) 
if msg == pypetconstants . TRAJECTORY : 
~~~ self . _trj_load_trajectory ( stuff_to_load , * args , ** kwargs ) 
~~ elif msg == pypetconstants . LEAF : 
~~~ self . _prm_load_parameter_or_result ( stuff_to_load , * args , ** kwargs ) 
~~ elif msg == pypetconstants . GROUP : 
~~~ self . _grp_load_group ( stuff_to_load , * args , ** kwargs ) 
~~ elif msg == pypetconstants . TREE : 
~~~ self . _tree_load_sub_branch ( stuff_to_load , * args , ** kwargs ) 
~~ elif msg == pypetconstants . LIST : 
~~~ self . _srvc_load_several_items ( stuff_to_load , * args , ** kwargs ) 
~~ ~~ except pt . NoSuchNodeError as exc : 
raise pex . DataNotInStorageError ( repr ( exc ) ) 
~~~ self . _srvc_closing_routine ( opened ) 
~~ ~~ def store ( self , msg , stuff_to_store , * args , ** kwargs ) : 
~~~ opened = self . _srvc_opening_routine ( 'a' , msg , kwargs ) 
if msg == pypetconstants . MERGE : 
~~~ self . _trj_merge_trajectories ( * args , ** kwargs ) 
~~ elif msg == pypetconstants . BACKUP : 
~~~ self . _trj_backup_trajectory ( stuff_to_store , * args , ** kwargs ) 
~~ elif msg == pypetconstants . PREPARE_MERGE : 
~~~ self . _trj_prepare_merge ( stuff_to_store , * args , ** kwargs ) 
~~ elif msg == pypetconstants . TRAJECTORY : 
~~~ self . _trj_store_trajectory ( stuff_to_store , * args , ** kwargs ) 
~~ elif msg == pypetconstants . SINGLE_RUN : 
~~~ self . _srn_store_single_run ( stuff_to_store , * args , ** kwargs ) 
~~ elif msg in pypetconstants . LEAF : 
~~~ self . _prm_store_parameter_or_result ( stuff_to_store , * args , ** kwargs ) 
~~ elif msg == pypetconstants . DELETE : 
~~~ self . _all_delete_parameter_or_result_or_group ( stuff_to_store , * args , ** kwargs ) 
~~~ self . _grp_store_group ( stuff_to_store , * args , ** kwargs ) 
~~~ self . _tree_store_sub_branch ( stuff_to_store , * args , ** kwargs ) 
~~ elif msg == pypetconstants . DELETE_LINK : 
~~~ self . _lnk_delete_link ( stuff_to_store , * args , ** kwargs ) 
~~~ self . _srvc_store_several_items ( stuff_to_store , * args , ** kwargs ) 
~~ elif msg == pypetconstants . ACCESS_DATA : 
~~~ return self . _hdf5_interact_with_data ( stuff_to_store , * args , ** kwargs ) 
~~ elif msg == pypetconstants . OPEN_FILE : 
self . _keep_open = True 
~~ elif msg == pypetconstants . CLOSE_FILE : 
self . _keep_open = False 
~~ elif msg == pypetconstants . FLUSH : 
~~~ self . _hdf5file . flush ( ) 
~~ ~~ def _srvc_load_several_items ( self , iterable , * args , ** kwargs ) : 
for input_tuple in iterable : 
~~~ msg = input_tuple [ 0 ] 
item = input_tuple [ 1 ] 
if len ( input_tuple ) > 2 : 
~~~ args = input_tuple [ 2 ] 
~~ if len ( input_tuple ) > 3 : 
~~~ kwargs = input_tuple [ 3 ] 
~~ if len ( input_tuple ) > 4 : 
~~ self . load ( msg , item , * args , ** kwargs ) 
~~ ~~ def _srvc_check_hdf_properties ( self , traj ) : 
for attr_name in HDF5StorageService . ATTR_LIST : 
~~~ config = traj . f_get ( 'config.hdf5.' + attr_name ) . f_get ( ) 
setattr ( self , attr_name , config ) 
( attr_name , str ( getattr ( self , attr_name ) ) ) ) 
~~ ~~ for attr_name , table_name in HDF5StorageService . NAME_TABLE_MAPPING . items ( ) : 
~~~ if table_name in ( 'parameters' , 'config' ) : 
~~~ table_name += '_overview' 
~~ config = traj . f_get ( 'config.hdf5.overview.' + table_name ) . f_get ( ) 
( table_name , str ( getattr ( self , attr_name ) ) ) ) 
~~ ~~ for attr_name , name in HDF5StorageService . PR_ATTR_NAME_MAPPING . items ( ) : 
~~~ config = traj . f_get ( 'config.hdf5.' + name ) . f_get ( ) 
( name , str ( getattr ( self , attr_name ) ) ) ) 
~~ ~~ if ( ( not self . _overview_results_summary or 
not self . _overview_derived_parameters_summary ) and 
self . _purge_duplicate_comments ) : 
~~ self . _filters = None 
~~ def _srvc_store_several_items ( self , iterable , * args , ** kwargs ) : 
~~ self . store ( msg , item , * args , ** kwargs ) 
~~ ~~ def _srvc_opening_routine ( self , mode , msg = None , kwargs = ( ) ) : 
self . _mode = mode 
self . _srvc_extract_file_information ( kwargs ) 
if not self . is_open : 
~~~ if 'a' in mode : 
~~~ ( path , filename ) = os . path . split ( self . _filename ) 
racedirs ( os . path . abspath ( path ) ) 
self . _hdf5store = HDFStore ( self . _filename , mode = self . _mode , complib = self . _complib , 
complevel = self . _complevel , fletcher32 = self . _fletcher32 ) 
self . _hdf5file = self . _hdf5store . _handle 
self . _hdf5file . title = self . _file_title 
if self . _trajectory_name is not None : 
~~~ if not '/' + self . _trajectory_name in self . _hdf5file : 
~~~ if not msg == pypetconstants . TRAJECTORY : 
~~~ self . _trajectory_group = self . _hdf5file . get_node ( '/' + 
self . _trajectory_name ) 
( self . _filename , self . _trajectory_name ) ) 
~~ elif mode == 'r' : 
~~~ if self . _trajectory_name is not None and self . _trajectory_index is not None : 
~~ if not os . path . isfile ( self . _filename ) : 
~~ self . _hdf5store = HDFStore ( self . _filename , mode = self . _mode , complib = self . _complib , 
if self . _trajectory_index is not None : 
~~~ nodelist = self . _hdf5file . list_nodes ( where = '/' ) 
if ( self . _trajectory_index >= len ( nodelist ) or 
self . _trajectory_index < - len ( nodelist ) ) : 
% ( self . _trajectory_index , len ( nodelist ) , self . _filename ) ) 
~~ self . _trajectory_group = nodelist [ self . _trajectory_index ] 
self . _trajectory_name = self . _trajectory_group . _v_name 
~~ elif self . _trajectory_name is not None : 
% ( self . _filename , self . _trajectory_name ) ) 
~~ self . _trajectory_group = self . _hdf5file . get_node ( '/' + self . _trajectory_name ) 
~~ self . _node_processing_timer = NodeProcessingTimer ( display_time = self . _display_time , 
logger_name = self . _logger . name ) 
self . _overview_group_ = None 
~~ ~~ def _srvc_closing_routine ( self , closing ) : 
if ( not self . _keep_open and 
closing and 
self . is_open ) : 
~~~ f_fd = self . _hdf5file . fileno ( ) 
self . _hdf5file . flush ( ) 
~~~ os . fsync ( f_fd ) 
~~~ self . _hdf5store . flush ( fsync = True ) 
~~~ f_fd = self . _hdf5store . _handle . fileno ( ) 
self . _hdf5store . flush ( ) 
os . fsync ( f_fd ) 
~~ ~~ except OSError as exc : 
self . _logger . debug ( errmsg ) 
~~ self . _hdf5store . close ( ) 
if self . _hdf5file . isopen : 
~~ self . _hdf5file = None 
self . _hdf5store = None 
self . _trajectory_group = None 
self . _trajectory_name = None 
self . _trajectory_index = None 
~~ ~~ def _srvc_extract_file_information ( self , kwargs ) : 
if 'filename' in kwargs : 
~~~ self . _filename = kwargs . pop ( 'filename' ) 
~~ if 'file_title' in kwargs : 
~~~ self . _file_title = kwargs . pop ( 'file_title' ) 
~~ if 'trajectory_name' in kwargs : 
~~~ self . _trajectory_name = kwargs . pop ( 'trajectory_name' ) 
~~ if 'trajectory_index' in kwargs : 
~~~ self . _trajectory_index = kwargs . pop ( 'trajectory_index' ) 
~~ ~~ def _trj_backup_trajectory ( self , traj , backup_filename = None ) : 
mypath , _ = os . path . split ( self . _filename ) 
if backup_filename is None : 
~~~ backup_filename = os . path . join ( '%s' % mypath , 'backup_%s.hdf5' % traj . v_name ) 
~~ backup_hdf5file = pt . open_file ( filename = backup_filename , 
mode = 'a' , title = backup_filename ) 
if '/' + self . _trajectory_name in backup_hdf5file : 
~~ backup_root = backup_hdf5file . root 
self . _trajectory_group . _f_copy ( newparent = backup_root , recursive = True ) 
backup_hdf5file . flush ( ) 
backup_hdf5file . close ( ) 
~~ def _trj_read_out_row ( colnames , row ) : 
result_dict = { } 
for colname in colnames : 
~~~ result_dict [ colname ] = row [ colname ] 
~~ def _trj_merge_trajectories ( self , other_trajectory_name , rename_dict , move_nodes = False , 
delete_trajectory = False , other_filename = None ) : 
if other_filename is None or other_filename == self . filename : 
~~~ other_filename = self . filename 
other_file = self . _hdf5file 
other_is_different = False 
~~~ other_file = pt . open_file ( filename = other_filename , mode = 'r+' ) 
other_is_different = True 
~~~ if not '/' + other_trajectory_name in other_file : 
other_trajectory_name , 
other_filename ) ) 
~~ for old_name in rename_dict : 
~~~ new_name = rename_dict [ old_name ] 
split_name = old_name . split ( '.' ) 
old_location = '/' + other_trajectory_name + '/' + '/' . join ( split_name ) 
split_name = new_name . split ( '.' ) 
new_parent_location = '/' + self . _trajectory_name + '/' + '/' . join ( split_name [ : - 1 ] ) 
new_short_name = split_name [ - 1 ] 
old_node = other_file . get_node ( old_location ) 
if move_nodes : 
~~~ self . _hdf5file . move_node ( where = old_node , newparent = new_parent_location , 
newname = new_short_name , createparents = True ) 
~~~ if other_is_different : 
~~~ new_parent_dot_location = '.' . join ( split_name [ : - 1 ] ) 
new_parent_or_loc , _ = self . _all_create_or_get_groups ( 
new_parent_dot_location ) 
create_parents = False 
~~~ new_parent_or_loc = new_parent_location 
create_parents = True 
~~ self . _hdf5file . copy_node ( where = old_node , newparent = new_parent_or_loc , 
newname = new_short_name , createparents = create_parents , 
recursive = True ) 
~~ ~~ if delete_trajectory : 
~~~ other_file . remove_node ( where = '/' , name = other_trajectory_name , recursive = True ) 
~~~ other_file . flush ( ) 
other_file . close ( ) 
~~ ~~ ~~ def _trj_prepare_merge ( self , traj , changed_parameters , old_length ) : 
if not traj . _stored : 
~~~ traj . f_store ( ) 
~~ infotable = getattr ( self . _overview_group , 'info' ) 
insert_dict = self . _all_extract_insert_dict ( traj , infotable . colnames ) 
self . _all_add_or_modify_row ( traj . v_name , insert_dict , infotable , index = 0 , 
flags = ( HDF5StorageService . MODIFY_ROW , ) ) 
for param_name in changed_parameters : 
~~~ param = traj . f_get ( param_name ) 
~~~ self . _all_delete_parameter_or_result_or_group ( param ) 
~~ except pt . NoSuchNodeError : 
~~ ~~ run_table = getattr ( self . _overview_group , 'runs' ) 
actual_rows = run_table . nrows 
self . _trj_fill_run_table ( traj , actual_rows , len ( traj ) ) 
for idx in range ( old_length , len ( traj ) ) : 
~~~ run_name = traj . f_idx_to_run ( idx ) 
run_info = traj . f_get_run_information ( run_name ) 
run_info [ 'name' ] = run_name 
traj . _set_explored_parameters_to_idx ( idx ) 
run_summary = self . _srn_summarize_explored_parameters ( list ( 
traj . _explored_parameters . values ( ) ) ) 
run_info [ 'parameter_summary' ] = run_summary 
self . _all_add_or_modify_row ( run_name , run_info , run_table , index = idx , 
~~ traj . f_restore_default ( ) 
~~ def _trj_load_trajectory ( self , traj , as_new , load_parameters , load_derived_parameters , 
load_results , load_other_data , recursive , max_depth , 
with_run_information , with_meta_data , force ) : 
if ( as_new and ( load_derived_parameters != pypetconstants . LOAD_NOTHING or 
load_results != pypetconstants . LOAD_NOTHING or 
load_other_data != pypetconstants . LOAD_NOTHING ) ) : 
~~ if as_new and load_parameters != pypetconstants . LOAD_DATA : 
~~ loadconstants = ( pypetconstants . LOAD_NOTHING , pypetconstants . LOAD_SKELETON , 
pypetconstants . LOAD_DATA , pypetconstants . OVERWRITE_DATA ) 
if not ( load_parameters in loadconstants and load_derived_parameters in loadconstants and 
load_results in loadconstants and load_other_data in loadconstants ) : 
~~ traj . _stored = not as_new 
load_data = max ( load_parameters , load_derived_parameters , load_results , load_other_data ) 
if with_meta_data : 
~~~ self . _trj_load_meta_data ( traj , load_data , as_new , with_run_information , force ) 
~~ if ( load_parameters != pypetconstants . LOAD_NOTHING or 
load_derived_parameters != pypetconstants . LOAD_NOTHING or 
load_other_data != pypetconstants . LOAD_NOTHING ) : 
~~ maximum_display_other = 10 
for children in [ self . _trajectory_group . _v_groups , self . _trajectory_group . _v_links ] : 
~~~ for hdf5_group_name in children : 
~~~ hdf5_group = children [ hdf5_group_name ] 
child_name = hdf5_group . _v_name 
load_subbranch = True 
if child_name == 'config' : 
~~~ if as_new : 
~~~ loading = pypetconstants . LOAD_NOTHING 
~~~ loading = load_parameters 
~~ ~~ elif child_name == 'parameters' : 
~~ elif child_name == 'results' : 
~~~ loading = load_results 
~~ elif child_name == 'derived_parameters' : 
~~~ loading = load_derived_parameters 
~~ elif child_name == 'overview' : 
~~~ loading = load_other_data 
load_subbranch = False 
~~ if loading == pypetconstants . LOAD_NOTHING : 
~~ if load_subbranch : 
( child_name , str ( loading ) ) ) 
~~~ if counter < maximum_display_other : 
~~ elif counter == maximum_display_other : 
~~ self . _tree_load_sub_branch ( traj , child_name , load_data = loading , with_links = True , 
_trajectory = traj , _as_new = as_new , 
_hdf5_group = self . _trajectory_group ) 
~~ ~~ ~~ def _trj_load_meta_data ( self , traj , load_data , as_new , with_run_information , force ) : 
metatable = self . _overview_group . info 
metarow = metatable [ 0 ] 
~~~ version = metarow [ 'version' ] . decode ( 'utf-8' ) 
~~ except ( IndexError , ValueError ) as ke : 
~~~ python = metarow [ 'python' ] . decode ( 'utf-8' ) 
~~ self . _trj_check_version ( version , python , force ) 
self . _grp_load_group ( traj , load_data = load_data , 
with_links = False , recursive = False , _traj = traj , 
_as_new = as_new , _hdf5_group = self . _trajectory_group ) 
if as_new : 
~~~ length = int ( metarow [ 'length' ] ) 
for irun in range ( length ) : 
~~~ traj . _add_run_info ( irun ) 
~~~ traj . _comment = metarow [ 'comment' ] . decode ( 'utf-8' ) 
traj . _timestamp = float ( metarow [ 'timestamp' ] ) 
traj . _trajectory_timestamp = traj . _timestamp 
traj . _time = metarow [ 'time' ] . decode ( 'utf-8' ) 
traj . _trajectory_time = traj . _time 
traj . _name = metarow [ 'name' ] . decode ( 'utf-8' ) 
traj . _trajectory_name = traj . _name 
traj . _version = version 
traj . _python = python 
single_run_table = self . _overview_group . runs 
if with_run_information : 
~~~ for row in single_run_table . iterrows ( ) : 
~~~ name = row [ 'name' ] . decode ( 'utf-8' ) 
idx = int ( row [ 'idx' ] ) 
timestamp = float ( row [ 'timestamp' ] ) 
time_ = row [ 'time' ] . decode ( 'utf-8' ) 
completed = int ( row [ 'completed' ] ) 
summary = row [ 'parameter_summary' ] . decode ( 'utf-8' ) 
hexsha = row [ 'short_environment_hexsha' ] . decode ( 'utf-8' ) 
~~~ runtime = row [ 'runtime' ] . decode ( 'utf-8' ) 
finish_timestamp = float ( row [ 'finish_timestamp' ] ) 
~~~ runtime = '' 
finish_timestamp = 0.0 
~~ info_dict = { 'idx' : idx , 
'timestamp' : timestamp , 
'finish_timestamp' : finish_timestamp , 
'runtime' : runtime , 
'time' : time_ , 
'completed' : completed , 
'name' : name , 
'parameter_summary' : summary , 
'short_environment_hexsha' : hexsha } 
traj . _add_run_info ( ** info_dict ) 
~~~ traj . _length = single_run_table . nrows 
~~ ~~ self . _trj_load_exploration ( traj ) 
self . _srvc_load_hdf5_settings ( ) 
~~ def _tree_load_sub_branch ( self , traj_node , branch_name , 
load_data = pypetconstants . LOAD_DATA , 
with_links = True , recursive = False , 
max_depth = None , _trajectory = None , 
_as_new = False , _hdf5_group = None ) : 
if load_data == pypetconstants . LOAD_NOTHING : 
~~ if _trajectory is None : 
~~~ _trajectory = traj_node . v_root 
~~ if _hdf5_group is None : 
~~~ hdf5_group_name = traj_node . v_full_name . replace ( '.' , '/' ) 
if hdf5_group_name == '' : 
~~~ _hdf5_group = self . _trajectory_group 
~~~ _hdf5_group = self . _hdf5file . get_node ( where = self . _trajectory_group , 
name = hdf5_group_name ) 
% ( traj_node . v_full_name , hdf5_group_name ) ) 
~~ ~~ ~~ split_names = branch_name . split ( '.' ) 
final_group_name = split_names . pop ( ) 
current_depth = 1 
for name in split_names : 
~~~ if current_depth > max_depth : 
~~ _hdf5_group = getattr ( _hdf5_group , name ) 
self . _tree_load_nodes_dfs ( traj_node , load_data = load_data , with_links = with_links , 
recursive = False , max_depth = max_depth , current_depth = current_depth , 
trajectory = _trajectory , as_new = _as_new , 
hdf5_group = _hdf5_group ) 
current_depth += 1 
traj_node = traj_node . _children [ name ] 
~~ if current_depth <= max_depth : 
~~~ _hdf5_group = getattr ( _hdf5_group , final_group_name ) 
recursive = recursive , max_depth = max_depth , 
current_depth = current_depth , trajectory = _trajectory , 
as_new = _as_new , hdf5_group = _hdf5_group ) 
~~ ~~ def _trj_check_version ( self , version , python , force ) : 
curr_python = pypetconstants . python_version_string 
if ( version != VERSION or curr_python != python ) and not force : 
( VERSION , curr_python , version , python ) ) 
~~ elif version != VERSION or curr_python != python : 
~~ ~~ def _trj_fill_run_table ( self , traj , start , stop ) : 
def _make_row ( info_dict ) : 
~~~ row = ( info_dict [ 'idx' ] , 
info_dict [ 'name' ] , 
info_dict [ 'time' ] , 
info_dict [ 'timestamp' ] , 
info_dict [ 'finish_timestamp' ] , 
info_dict [ 'runtime' ] , 
info_dict [ 'parameter_summary' ] , 
info_dict [ 'short_environment_hexsha' ] , 
info_dict [ 'completed' ] ) 
return row 
~~ runtable = getattr ( self . _overview_group , 'runs' ) 
rows = [ ] 
updated_run_information = traj . _updated_run_information 
for idx in range ( start , stop ) : 
~~~ info_dict = traj . _run_information [ traj . _single_run_ids [ idx ] ] 
rows . append ( _make_row ( info_dict ) ) 
updated_run_information . discard ( idx ) 
~~ if rows : 
~~~ runtable . append ( rows ) 
runtable . flush ( ) 
~~ rows = [ ] 
indices = [ ] 
for idx in updated_run_information : 
~~~ info_dict = traj . f_get_run_information ( idx , copy = False ) 
indices . append ( idx ) 
~~~ runtable . modify_coordinates ( indices , rows ) 
~~ traj . _updated_run_information = set ( ) 
~~ def _trj_store_meta_data ( self , traj ) : 
descriptiondict = { 'name' : pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_LOCATION_LENGTH , 
pos = 0 ) , 
'time' : pt . StringCol ( len ( traj . v_time ) , pos = 1 ) , 
'timestamp' : pt . FloatCol ( pos = 3 ) , 
'comment' : pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_COMMENT_LENGTH , 
pos = 4 ) , 
'length' : pt . IntCol ( pos = 2 ) , 
'version' : pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH , 
pos = 5 ) , 
'python' : pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH , 
pos = 5 ) } 
infotable = self . _all_get_or_create_table ( where = self . _overview_group , tablename = 'info' , 
description = descriptiondict , 
expectedrows = len ( traj ) ) 
flags = ( HDF5StorageService . ADD_ROW , 
HDF5StorageService . MODIFY_ROW ) ) 
rundescription_dict = { 'name' : pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH , 
pos = 1 ) , 
'time' : pt . StringCol ( len ( traj . v_time ) , pos = 2 ) , 
'idx' : pt . IntCol ( pos = 0 ) , 
'completed' : pt . IntCol ( pos = 8 ) , 
'parameter_summary' : pt . StringCol ( 
pypetconstants . HDF5_STRCOL_MAX_COMMENT_LENGTH , 
pos = 6 ) , 
'short_environment_hexsha' : pt . StringCol ( 7 , pos = 7 ) , 
'finish_timestamp' : pt . FloatCol ( pos = 4 ) , 
'runtime' : pt . StringCol ( 
pypetconstants . HDF5_STRCOL_MAX_RUNTIME_LENGTH , 
runtable = self . _all_get_or_create_table ( where = self . _overview_group , 
tablename = 'runs' , 
description = rundescription_dict ) 
hdf5_description_dict = { 'complib' : pt . StringCol ( 7 , pos = 0 ) , 
'complevel' : pt . IntCol ( pos = 1 ) , 
'shuffle' : pt . BoolCol ( pos = 2 ) , 
'fletcher32' : pt . BoolCol ( pos = 3 ) , 
'pandas_format' : pt . StringCol ( 7 , pos = 4 ) , 
'encoding' : pt . StringCol ( 11 , pos = 5 ) } 
pos = 7 
for name , table_name in HDF5StorageService . NAME_TABLE_MAPPING . items ( ) : 
~~~ hdf5_description_dict [ table_name ] = pt . BoolCol ( pos = pos ) 
pos += 1 
~~ hdf5_description_dict . update ( { 'purge_duplicate_comments' : pt . BoolCol ( pos = pos + 2 ) , 
'results_per_run' : pt . IntCol ( pos = pos + 3 ) , 
'derived_parameters_per_run' : pt . IntCol ( pos = pos + 4 ) } ) 
hdf5table = self . _all_get_or_create_table ( where = self . _overview_group , 
tablename = 'hdf5_settings' , 
description = hdf5_description_dict ) 
insert_dict = { } 
for attr_name in self . ATTR_LIST : 
~~~ insert_dict [ attr_name ] = getattr ( self , attr_name ) 
~~ for attr_name , table_name in self . NAME_TABLE_MAPPING . items ( ) : 
~~~ insert_dict [ table_name ] = getattr ( self , attr_name ) 
~~ for attr_name , name in self . PR_ATTR_NAME_MAPPING . items ( ) : 
~~~ insert_dict [ name ] = getattr ( self , attr_name ) 
~~ self . _all_add_or_modify_row ( traj . v_name , insert_dict , hdf5table , index = 0 , 
actual_rows = runtable . nrows 
self . _trj_fill_run_table ( traj , actual_rows , len ( traj . _run_information ) ) 
self . _grp_store_group ( traj , store_data = pypetconstants . STORE_DATA , 
with_links = False , 
recursive = False , 
self . _trj_store_explorations ( traj ) 
tostore_tables = [ ] 
~~~ if getattr ( self , name ) : 
~~~ tostore_tables . append ( table_name ) 
~~ ~~ self . _srvc_make_overview_tables ( tostore_tables , traj ) 
~~ def _trj_load_exploration ( self , traj ) : 
if hasattr ( self . _overview_group , 'explorations' ) : 
~~~ explorations_table = self . _overview_group . _f_get_child ( 'explorations' ) 
for row in explorations_table . iterrows ( ) : 
~~~ param_name = row [ 'explorations' ] . decode ( 'utf-8' ) 
if param_name not in traj . _explored_parameters : 
~~~ traj . _explored_parameters [ param_name ] = None 
~~~ for what in ( 'parameters' , 'derived_parameters' ) : 
~~~ if hasattr ( self . _trajectory_group , what ) : 
~~~ parameters = self . _trajectory_group . _f_get_child ( what ) 
for group in parameters . _f_walk_groups ( ) : 
~~~ if self . _all_get_from_attrs ( group , HDF5StorageService . LENGTH ) : 
~~~ group_location = group . _v_pathname 
full_name = '.' . join ( group_location . split ( '/' ) [ 2 : ] ) 
traj . _explored_parameters [ full_name ] = None 
~~ ~~ ~~ ~~ ~~ ~~ def _trj_store_explorations ( self , traj ) : 
nexplored = len ( traj . _explored_parameters ) 
if nexplored > 0 : 
~~~ if hasattr ( self . _overview_group , 'explorations' ) : 
if len ( explorations_table ) != nexplored : 
~~~ self . _hdf5file . remove_node ( where = self . _overview_group , 
name = 'explorations' ) 
~~ ~~ ~~ if not hasattr ( self . _overview_group , 'explorations' ) : 
~~~ explored_list = list ( traj . _explored_parameters . keys ( ) ) 
if explored_list : 
~~~ string_col = self . _all_get_table_col ( 'explorations' , 
explored_list , 
'overview.explorations' ) 
~~~ string_col = pt . StringCol ( 1 ) 
~~ description = { 'explorations' : string_col } 
explorations_table = self . _hdf5file . create_table ( where = self . _overview_group , 
name = 'explorations' , 
description = description ) 
rows = [ ( x . encode ( 'utf-8' ) , ) for x in explored_list ] 
if rows : 
~~~ explorations_table . append ( rows ) 
explorations_table . flush ( ) 
~~ ~~ ~~ def _srvc_make_overview_tables ( self , tables_to_make , traj = None ) : 
for table_name in tables_to_make : 
~~~ paramdescriptiondict = { } 
expectedrows = 0 
paramdescriptiondict [ 'location' ] = pt . StringCol ( 
pypetconstants . HDF5_STRCOL_MAX_LOCATION_LENGTH , 
pos = 0 ) 
paramdescriptiondict [ 'name' ] = pt . StringCol ( pypetconstants . HDF5_STRCOL_MAX_NAME_LENGTH , 
pos = 1 ) 
paramdescriptiondict [ 'comment' ] = pt . StringCol ( 
pypetconstants . HDF5_STRCOL_MAX_COMMENT_LENGTH ) 
paramdescriptiondict [ 'value' ] = pt . StringCol ( 
pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH , pos = 2 ) 
if table_name == 'config_overview' : 
~~~ if traj is not None : 
~~~ expectedrows = len ( traj . _config ) 
~~ ~~ if table_name == 'parameters_overview' : 
~~~ expectedrows = len ( traj . _parameters ) 
~~ ~~ if table_name == 'explored_parameters_overview' : 
~~~ paramdescriptiondict [ 'range' ] = pt . StringCol ( 
pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH ) 
paramdescriptiondict [ 'length' ] = pt . IntCol ( ) 
if traj is not None : 
~~~ expectedrows = len ( traj . _explored_parameters ) 
~~ ~~ if table_name . endswith ( 'summary' ) : 
~~~ paramdescriptiondict [ 'hexdigest' ] = pt . StringCol ( 64 , pos = 10 ) 
~~ if table_name == 'derived_parameters_overview' : 
~~~ expectedrows = self . _derived_parameters_per_run 
~~~ expectedrows *= len ( traj ) 
expectedrows += len ( traj . _derived_parameters ) 
~~ ~~ if table_name == 'results_overview' : 
~~~ expectedrows = self . _results_per_run 
expectedrows += len ( traj . _results ) 
~~ ~~ if expectedrows > 0 : 
~~~ paramtable = self . _all_get_or_create_table ( where = self . _overview_group , 
tablename = table_name , 
description = paramdescriptiondict , 
expectedrows = expectedrows ) 
description = paramdescriptiondict ) 
~~ paramtable . flush ( ) 
~~ ~~ def _trj_store_trajectory ( self , traj , only_init = False , store_data = pypetconstants . STORE_DATA , 
if not only_init : 
store_data = pypetconstants . STORE_NOTHING 
~~ if not traj . _stored and self . _trajectory_group is not None : 
( traj . v_name , self . _filename ) ) 
~~ self . _srvc_check_hdf_properties ( traj ) 
if self . _trajectory_group is None : 
~~~ self . _trajectory_group = self . _hdf5file . create_group ( where = '/' , 
name = self . _trajectory_name , 
title = self . _trajectory_name , 
filters = self . _all_get_filters ( ) ) 
~~ self . _trj_store_meta_data ( traj ) 
if store_data in ( pypetconstants . STORE_DATA_SKIPPING , 
pypetconstants . STORE_DATA , 
pypetconstants . OVERWRITE_DATA ) : 
~~~ counter = 0 
maximum_display_other = 10 
name_set = set ( [ 'parameters' , 'config' , 'derived_parameters' , 'results' ] ) 
for child_name in traj . _children : 
~~~ if child_name in name_set : 
~~ self . _tree_store_sub_branch ( traj , child_name , store_data = store_data , 
with_links = True , 
recursive = True , max_depth = max_depth , 
hdf5_group = self . _trajectory_group ) 
~~ traj . _stored = True 
~~ def _tree_store_sub_branch ( self , traj_node , branch_name , 
store_data = pypetconstants . STORE_DATA , 
max_depth = None , 
hdf5_group = None ) : 
if store_data == pypetconstants . STORE_NOTHING : 
~~ if hdf5_group is None : 
~~~ location = traj_node . v_full_name 
hdf5_location = location . replace ( '.' , '/' ) 
~~~ if location == '' : 
~~~ hdf5_group = self . _trajectory_group 
~~~ hdf5_group = self . _hdf5file . get_node ( where = self . _trajectory_group , 
name = hdf5_location ) 
~~ ~~ except pt . NoSuchNodeError : 
( traj_node . v_name , hdf5_location ) ) 
if traj_node . v_is_leaf : 
'disk.' % ( traj_node . v_name , hdf5_location ) ) 
self . _tree_store_sub_branch ( traj_node . _nn_interface . _root_instance , 
traj_node . v_full_name + '.' + branch_name , 
store_data = store_data , with_links = with_links , 
max_depth = max_depth + traj_node . v_depth , 
~~ ~~ ~~ current_depth = 1 
split_names = branch_name . split ( '.' ) 
leaf_name = split_names . pop ( ) 
~~ self . _tree_store_nodes_dfs ( traj_node , name , store_data = store_data , with_links = with_links , 
recursive = False , max_depth = max_depth , 
current_depth = current_depth , parent_hdf5_group = hdf5_group ) 
hdf5_group = getattr ( hdf5_group , name ) 
~~~ self . _tree_store_nodes_dfs ( traj_node , leaf_name , store_data = store_data , 
with_links = with_links , recursive = recursive , 
max_depth = max_depth , current_depth = current_depth , 
parent_hdf5_group = hdf5_group ) 
~~ ~~ def _tree_create_leaf ( self , name , trajectory , hdf5_group ) : 
class_name = self . _all_get_from_attrs ( hdf5_group , HDF5StorageService . CLASS_NAME ) 
class_constructor = trajectory . _create_class ( class_name ) 
instance = trajectory . _construct_instance ( class_constructor , name ) 
~~ def _tree_load_nodes_dfs ( self , parent_traj_node , load_data , with_links , recursive , 
max_depth , current_depth , trajectory , as_new , hdf5_group ) : 
~~ loading_list = [ ( parent_traj_node , current_depth , hdf5_group ) ] 
while loading_list : 
~~~ parent_traj_node , current_depth , hdf5_group = loading_list . pop ( ) 
if isinstance ( hdf5_group , pt . link . SoftLink ) : 
~~~ if with_links : 
~~~ self . _tree_load_link ( parent_traj_node , load_data = load_data , traj = trajectory , 
as_new = as_new , hdf5_soft_link = hdf5_group ) 
~~ name = hdf5_group . _v_name 
is_leaf = self . _all_get_from_attrs ( hdf5_group , HDF5StorageService . LEAF ) 
in_trajectory = name in parent_traj_node . _children 
if is_leaf : 
~~~ if in_trajectory : 
~~~ instance = parent_traj_node . _children [ name ] 
~~~ instance = self . _tree_create_leaf ( name , trajectory , hdf5_group ) 
parent_traj_node . _add_leaf_from_storage ( args = ( instance , ) , kwargs = { } ) 
~~ self . _prm_load_parameter_or_result ( instance , load_data = load_data , 
_hdf5_group = hdf5_group ) 
~~~ instance . _stored = False 
~~~ traj_group = parent_traj_node . _children [ name ] 
if load_data == pypetconstants . OVERWRITE_DATA : 
~~~ traj_group . v_annotations . f_empty ( ) 
traj_group . v_comment = '' 
~~~ if HDF5StorageService . CLASS_NAME in hdf5_group . _v_attrs : 
~~~ class_name = self . _all_get_from_attrs ( hdf5_group , 
HDF5StorageService . CLASS_NAME ) 
args = ( instance , ) 
~~~ args = ( name , ) 
~~ traj_group = parent_traj_node . _add_group_from_storage ( args = args , kwargs = { } ) 
~~ self . _grp_load_group ( traj_group , load_data = load_data , with_links = with_links , 
_traj = trajectory , _as_new = as_new , 
if recursive and current_depth < max_depth : 
~~~ new_depth = current_depth + 1 
for children in ( hdf5_group . _v_groups , hdf5_group . _v_links ) : 
~~~ for new_hdf5_group_name in children : 
~~~ new_hdf5_group = children [ new_hdf5_group_name ] 
loading_list . append ( ( traj_group , new_depth , new_hdf5_group ) ) 
~~ ~~ ~~ ~~ ~~ ~~ def _tree_load_link ( self , new_traj_node , load_data , traj , as_new , hdf5_soft_link ) : 
~~~ linked_group = hdf5_soft_link ( ) 
link_name = hdf5_soft_link . _v_name 
if ( not link_name in new_traj_node . _links or 
load_data == pypetconstants . OVERWRITE_DATA ) : 
~~~ link_location = linked_group . _v_pathname 
full_name = '.' . join ( link_location . split ( '/' ) [ 2 : ] ) 
if not full_name in traj : 
~~~ self . _tree_load_sub_branch ( traj , full_name , 
with_links = False , recursive = False , _trajectory = traj , 
~~ if ( load_data == pypetconstants . OVERWRITE_DATA and 
link_name in new_traj_node . _links ) : 
~~~ new_traj_node . f_remove_link ( link_name ) 
~~ if not link_name in new_traj_node . _links : 
~~~ new_traj_node . _nn_interface . _add_generic ( new_traj_node , 
type_name = nn . LINK , 
group_type_name = nn . GROUP , 
args = ( link_name , 
traj . f_get ( full_name ) ) , 
~~ ~~ ~~ except pt . NoSuchNodeError : 
( hdf5_soft_link . _v_name , new_traj_node . v_full_name ) ) 
~~ ~~ def _tree_store_nodes_dfs ( self , parent_traj_node , name , store_data , with_links , recursive , 
max_depth , current_depth , 
parent_hdf5_group ) : 
~~ store_list = [ ( parent_traj_node , name , current_depth , parent_hdf5_group ) ] 
while store_list : 
~~~ parent_traj_node , name , current_depth , parent_hdf5_group = store_list . pop ( ) 
if name in parent_traj_node . _links : 
~~~ self . _tree_store_link ( parent_traj_node , name , parent_hdf5_group ) 
~~ traj_node = parent_traj_node . _children [ name ] 
if not hasattr ( parent_hdf5_group , name ) : 
~~~ newly_created = True 
new_hdf5_group = self . _hdf5file . create_group ( where = parent_hdf5_group , 
name = name , filters = self . _all_get_filters ( ) ) 
~~~ newly_created = False 
new_hdf5_group = getattr ( parent_hdf5_group , name ) 
~~ if traj_node . v_is_leaf : 
~~~ self . _prm_store_parameter_or_result ( traj_node , store_data = store_data , 
_hdf5_group = new_hdf5_group , 
_newly_created = newly_created ) 
~~~ self . _grp_store_group ( traj_node , store_data = store_data , with_links = with_links , 
~~~ for child in traj_node . _children . keys ( ) : 
~~~ store_list . append ( ( traj_node , child , current_depth + 1 , new_hdf5_group ) ) 
~~ ~~ ~~ ~~ ~~ def _tree_store_link ( self , node_in_traj , link , hdf5_group ) : 
if hasattr ( hdf5_group , link ) : 
~~ linked_traj_node = node_in_traj . _links [ link ] 
linking_name = linked_traj_node . v_full_name . replace ( '.' , '/' ) 
linking_name = '/' + self . _trajectory_name + '/' + linking_name 
~~~ to_link_hdf5_group = self . _hdf5file . get_node ( where = linking_name ) 
node_in_traj . v_full_name , 
linked_traj_node . v_full_name ) ) 
root = node_in_traj . _nn_interface . _root_instance 
self . _tree_store_sub_branch ( root , linked_traj_node . v_full_name , 
store_data = pypetconstants . STORE_DATA_SKIPPING , 
with_links = False , recursive = False , 
to_link_hdf5_group = self . _hdf5file . get_node ( where = linking_name ) 
~~ self . _hdf5file . create_soft_link ( where = hdf5_group , 
name = link , 
target = to_link_hdf5_group ) 
~~ def _srn_store_single_run ( self , traj , 
recursive = True , 
if store_data != pypetconstants . STORE_NOTHING : 
~~ for name_pair in traj . _new_nodes : 
~~~ _ , name = name_pair 
parent_group , child_node = traj . _new_nodes [ name_pair ] 
if not child_node . _stored : 
~~~ self . _tree_store_sub_branch ( parent_group , name , 
max_depth = max_depth - child_node . v_depth , 
hdf5_group = None ) 
~~ ~~ for name_pair in traj . _new_links : 
~~~ _ , link = name_pair 
parent_group , _ = traj . _new_links [ name_pair ] 
self . _tree_store_sub_branch ( parent_group , link , 
max_depth = max_depth - parent_group . v_depth - 1 , 
~~ ~~ ~~ def _srn_summarize_explored_parameters ( self , paramlist ) : 
runsummary = '' 
paramlist = sorted ( paramlist , key = lambda name : name . v_name + name . v_location ) 
for idx , expparam in enumerate ( paramlist ) : 
~~~ if idx > 0 : 
~~ valstr = expparam . f_val_to_str ( ) 
if len ( valstr ) >= pypetconstants . HDF5_STRCOL_MAX_COMMENT_LENGTH : 
~~~ valstr = valstr [ 0 : pypetconstants . HDF5_STRCOL_MAX_COMMENT_LENGTH - 3 ] 
valstr += '...' 
~~ if expparam . v_name in runsummary : 
~~~ param_name = expparam . v_full_name 
~~~ param_name = expparam . v_name 
~~ return runsummary 
~~ def _all_store_param_or_result_table_entry ( self , instance , table , flags , 
additional_info = None ) : 
location = instance . v_location 
fullname = instance . v_full_name 
if ( flags == ( HDF5StorageService . ADD_ROW , ) and table . nrows < 2 
and 'location' in table . colnames ) : 
~~~ flags = ( HDF5StorageService . ADD_ROW , HDF5StorageService . MODIFY_ROW ) 
~~ if flags == ( HDF5StorageService . ADD_ROW , ) : 
~~~ condvars = None 
condition = None 
~~~ condvars = { 'namecol' : table . cols . name , 'locationcol' : table . cols . location , 
'name' : name , 'location' : location } 
~~ if HDF5StorageService . REMOVE_ROW in flags : 
~~~ insert_dict = { } 
~~~ colnames = set ( table . colnames ) 
insert_dict = self . _all_extract_insert_dict ( instance , colnames , additional_info ) 
~~ self . _all_add_or_modify_row ( fullname , insert_dict , table , condition = condition , 
condvars = condvars , flags = flags ) 
~~ def _all_get_or_create_table ( self , where , tablename , description , expectedrows = None ) : 
where_node = self . _hdf5file . get_node ( where ) 
if not tablename in where_node : 
~~~ if not expectedrows is None : 
~~~ table = self . _hdf5file . create_table ( where = where_node , name = tablename , 
description = description , title = tablename , 
expectedrows = expectedrows , 
~~~ table = where_node . _f_get_child ( tablename ) 
~~ return table 
~~ def _all_get_node_by_name ( self , name ) : 
path_name = name . replace ( '.' , '/' ) 
where = '/%s/%s' % ( self . _trajectory_name , path_name ) 
return self . _hdf5file . get_node ( where = where ) 
~~ def _all_set_attributes_to_recall_natives ( data , ptitem , prefix ) : 
if type ( data ) is tuple : 
~~~ HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . COLL_TYPE , 
HDF5StorageService . COLL_TUPLE ) 
~~ elif type ( data ) is list : 
HDF5StorageService . COLL_LIST ) 
~~ elif type ( data ) is np . ndarray : 
HDF5StorageService . COLL_NDARRAY ) 
~~ elif type ( data ) is np . matrix : 
HDF5StorageService . COLL_MATRIX ) 
~~ elif type ( data ) in pypetconstants . PARAMETER_SUPPORTED_DATA : 
HDF5StorageService . COLL_SCALAR ) 
strtype = type ( data ) . __name__ 
if not strtype in pypetconstants . PARAMETERTYPEDICT : 
( str ( data ) , repr ( type ( data ) ) ) ) 
~~ HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE , strtype ) 
~~ elif type ( data ) is dict : 
~~~ if len ( data ) > 0 : 
HDF5StorageService . COLL_DICT ) 
HDF5StorageService . COLL_EMPTY_DICT ) 
~~ if type ( data ) in ( list , tuple ) : 
~~~ strtype = type ( data [ 0 ] ) . __name__ 
'`%s`.' % ( str ( data ) , strtype ) ) 
~~ HDF5StorageService . _all_set_attr ( ptitem , prefix + 
HDF5StorageService . SCALAR_TYPE , strtype ) 
~~ ~~ elif ( type ( data ) in ( np . ndarray , np . matrix ) and 
np . issubdtype ( data . dtype , str ) ) : 
~~~ HDF5StorageService . _all_set_attr ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE , 
str . __name__ ) 
~~ ~~ def _all_recall_native_type ( self , data , ptitem , prefix ) : 
typestr = self . _all_get_from_attrs ( ptitem , prefix + HDF5StorageService . SCALAR_TYPE ) 
colltype = self . _all_get_from_attrs ( ptitem , prefix + HDF5StorageService . COLL_TYPE ) 
type_changed = False 
if colltype == HDF5StorageService . COLL_SCALAR : 
~~~ if isinstance ( data , np . ndarray ) : 
~~~ data = np . array ( [ data ] ) [ 0 ] 
type_changed = True 
~~ if not typestr is None : 
~~~ if typestr != type ( data ) . __name__ : 
~~~ if typestr == str . __name__ : 
~~~ data = data . decode ( self . _encoding ) 
~~~ data = pypetconstants . PARAMETERTYPEDICT [ typestr ] ( data ) 
~~~ data = pypetconstants . COMPATPARAMETERTYPEDICT [ typestr ] ( data ) 
~~ ~~ type_changed = True 
~~ ~~ ~~ elif ( colltype == HDF5StorageService . COLL_TUPLE or 
colltype == HDF5StorageService . COLL_LIST ) : 
~~~ if type ( data ) is not list and type is not tuple : 
~~~ type_changed = True 
data = list ( data ) 
~~ if len ( data ) > 0 : 
~~~ first_item = data [ 0 ] 
if not typestr == type ( first_item ) . __name__ : 
~~~ if not isinstance ( data , list ) : 
~~~ data = list ( data ) 
~~ for idx , item in enumerate ( data ) : 
~~~ data [ idx ] = data [ idx ] . decode ( self . _encoding ) 
~~~ data [ idx ] = pypetconstants . PARAMETERTYPEDICT [ typestr ] ( item ) 
~~~ data [ idx ] = pypetconstants . COMPATPARAMETERTYPEDICT [ typestr ] ( item ) 
~~ ~~ ~~ if colltype == HDF5StorageService . COLL_TUPLE : 
~~~ if type ( data ) is not tuple : 
~~~ data = tuple ( data ) 
~~ ~~ ~~ elif colltype == HDF5StorageService . COLL_EMPTY_DICT : 
~~~ data = { } 
~~ elif isinstance ( data , np . ndarray ) : 
~~~ data = np . core . defchararray . decode ( data , self . _encoding ) 
~~ if colltype == HDF5StorageService . COLL_MATRIX : 
~~~ data = np . matrix ( data ) 
~~ ~~ return data , type_changed 
~~ def _all_add_or_modify_row ( self , item_name , insert_dict , table , index = None , condition = None , 
condvars = None , 
flags = ( ADD_ROW , MODIFY_ROW , ) ) : 
if len ( flags ) == 0 : 
~~ if index is not None and condition is not None : 
~~ elif condition is not None : 
~~~ row_iterator = table . where ( condition , condvars = condvars ) 
~~ elif index is not None : 
~~~ row_iterator = table . iterrows ( index , index + 1 ) 
~~~ row_iterator = None 
~~~ row = next ( row_iterator ) 
~~~ row = None 
~~ except StopIteration : 
~~ if ( ( HDF5StorageService . MODIFY_ROW in flags or HDF5StorageService . ADD_ROW in flags ) and 
HDF5StorageService . REMOVE_ROW in flags ) : 
~~ if row is None and HDF5StorageService . ADD_ROW in flags : 
~~~ row = table . row 
self . _all_insert_into_row ( row , insert_dict ) 
row . append ( ) 
~~ elif row is not None and HDF5StorageService . MODIFY_ROW in flags : 
~~~ self . _all_insert_into_row ( row , insert_dict ) 
row . update ( ) 
~~ elif HDF5StorageService . REMOVE_ROW in flags : 
~~~ if row is not None : 
~~~ row_number = row . nrow 
~~~ table . remove_rows ( start = row_number , stop = row_number + 1 ) 
~~ self . _all_kill_iterator ( row_iterator ) 
table . flush ( ) 
if HDF5StorageService . REMOVE_ROW not in flags and row is None : 
~~ ~~ def _all_insert_into_row ( self , row , insert_dict ) : 
for key , val in insert_dict . items ( ) : 
~~~ row [ key ] = val 
~~ except KeyError as ke : 
~~ ~~ ~~ def _all_extract_insert_dict ( self , item , colnames , additional_info = None ) : 
if 'length' in colnames : 
~~~ insert_dict [ 'length' ] = len ( item ) 
~~ if 'comment' in colnames : 
~~~ comment = self . _all_cut_string ( item . v_comment . encode ( 'utf-8' ) , 
self . _logger ) 
insert_dict [ 'comment' ] = comment 
~~ if 'location' in colnames : 
~~~ insert_dict [ 'location' ] = item . v_location . encode ( 'utf-8' ) 
~~ if 'name' in colnames : 
~~~ name = item . _name if ( not item . v_is_root or not item . v_is_run ) else item . _crun 
insert_dict [ 'name' ] = name . encode ( 'utf-8' ) 
~~ if 'class_name' in colnames : 
~~~ insert_dict [ 'class_name' ] = item . f_get_class_name ( ) . encode ( 'utf-8' ) 
~~ if 'value' in colnames : 
~~~ insert_dict [ 'value' ] = self . _all_cut_string ( 
item . f_val_to_str ( ) . encode ( 'utf-8' ) , 
pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH , 
~~ if 'hexdigest' in colnames : 
~~~ insert_dict [ 'hexdigest' ] = additional_info [ 'hexdigest' ] 
~~ if 'idx' in colnames : 
~~~ insert_dict [ 'idx' ] = item . v_idx 
~~ if 'time' in colnames : 
~~~ time_ = item . _time 
insert_dict [ 'time' ] = time_ . encode ( 'utf-8' ) 
~~ if 'timestamp' in colnames : 
~~~ timestamp = item . _timestamp 
insert_dict [ 'timestamp' ] = timestamp 
~~ if 'range' in colnames : 
~~~ third_length = pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH // 3 + 10 
item_range = itools . islice ( item . f_get_range ( copy = False ) , 0 , third_length ) 
insert_dict [ 'range' ] = self . _all_cut_string ( 
range_string . encode ( 'utf-8' ) , 
pypetconstants . HDF5_STRCOL_MAX_RANGE_LENGTH , 
~~ if 'array' in colnames : 
insert_dict [ 'array' ] = self . _all_cut_string ( 
~~ if 'version' in colnames : 
~~~ insert_dict [ 'version' ] = item . v_version . encode ( 'utf-8' ) 
~~ if 'python' in colnames : 
~~~ insert_dict [ 'python' ] = item . v_python . encode ( 'utf-8' ) 
~~ if 'finish_timestamp' in colnames : 
~~~ insert_dict [ 'finish_timestamp' ] = item . _finish_timestamp_run 
~~ return insert_dict 
~~ def _all_cut_string ( string , max_length , logger ) : 
if len ( string ) > max_length : 
( string , max_length ) ) 
string = string [ 0 : max_length - 3 ] + '...' . encode ( 'utf-8' ) 
~~ return string 
~~ def _all_create_or_get_group ( self , name , parent_hdf5_group = None ) : 
if not name in parent_hdf5_group : 
~~~ new_hdf5_group = self . _hdf5file . create_group ( where = parent_hdf5_group , 
name = name , 
title = name , 
return new_hdf5_group , True 
~~~ new_hdf5_group = parent_hdf5_group . _f_get_child ( name ) 
return new_hdf5_group , False 
~~ ~~ def _all_create_or_get_groups ( self , key , start_hdf5_group = None ) : 
if start_hdf5_group is None : 
~~~ newhdf5_group = self . _trajectory_group 
~~~ newhdf5_group = start_hdf5_group 
~~ created = False 
if key == '' : 
~~~ return newhdf5_group , created 
~~ split_key = key . split ( '.' ) 
for name in split_key : 
~~~ newhdf5_group , created = self . _all_create_or_get_group ( name , newhdf5_group ) 
~~ return newhdf5_group , created 
~~ def _ann_store_annotations ( self , item_with_annotations , node , overwrite = False ) : 
if overwrite is True or overwrite == 'v_annotations' : 
~~~ annotated = self . _all_get_from_attrs ( node , HDF5StorageService . ANNOTATED ) 
if annotated : 
~~~ current_attrs = node . _v_attrs 
for attr_name in current_attrs . _v_attrnames : 
~~~ if attr_name . startswith ( HDF5StorageService . ANNOTATION_PREFIX ) : 
~~~ delattr ( current_attrs , attr_name ) 
~~ ~~ delattr ( current_attrs , HDF5StorageService . ANNOTATED ) 
~~ ~~ if not item_with_annotations . v_annotations . f_is_empty ( ) : 
~~~ anno_dict = item_with_annotations . v_annotations . _dict 
current_attrs = node . _v_attrs 
changed = False 
for field_name in anno_dict : 
~~~ val = anno_dict [ field_name ] 
field_name_with_prefix = HDF5StorageService . ANNOTATION_PREFIX + field_name 
if field_name_with_prefix not in current_attrs : 
~~~ setattr ( current_attrs , field_name_with_prefix , val ) 
~~ ~~ if changed : 
~~~ setattr ( current_attrs , HDF5StorageService . ANNOTATED , True ) 
~~ ~~ ~~ def _ann_load_annotations ( self , item_with_annotations , node ) : 
annotated = self . _all_get_from_attrs ( node , HDF5StorageService . ANNOTATED ) 
~~~ annotations = item_with_annotations . v_annotations 
if not annotations . f_is_empty ( ) : 
~~ current_attrs = node . _v_attrs 
~~~ key = attr_name 
key = key . replace ( HDF5StorageService . ANNOTATION_PREFIX , '' ) 
data = getattr ( current_attrs , attr_name ) 
setattr ( annotations , key , data ) 
~~ ~~ ~~ ~~ def _grp_store_group ( self , traj_group , store_data = pypetconstants . STORE_DATA , 
with_links = True , recursive = False , max_depth = None , 
_hdf5_group = None , _newly_created = False ) : 
~~ elif store_data == pypetconstants . STORE_DATA_SKIPPING and traj_group . _stored : 
traj_group . v_full_name ) 
~~ elif not recursive : 
~~~ if _hdf5_group is None : 
~~~ _hdf5_group , _newly_created = self . _all_create_or_get_groups ( traj_group . v_full_name ) 
~~ overwrite = store_data == pypetconstants . OVERWRITE_DATA 
if ( traj_group . v_comment != '' and 
( HDF5StorageService . COMMENT not in _hdf5_group . _v_attrs or overwrite ) ) : 
~~~ setattr ( _hdf5_group . _v_attrs , HDF5StorageService . COMMENT , traj_group . v_comment ) 
~~ if ( ( _newly_created or overwrite ) and 
type ( traj_group ) not in ( nn . NNGroupNode , nn . ConfigGroup , nn . ParameterGroup , 
nn . DerivedParameterGroup , nn . ResultGroup ) ) : 
~~~ setattr ( _hdf5_group . _v_attrs , HDF5StorageService . CLASS_NAME , 
traj_group . f_get_class_name ( ) ) 
~~ self . _ann_store_annotations ( traj_group , _hdf5_group , overwrite = overwrite ) 
traj_group . _stored = True 
self . _node_processing_timer . signal_update ( ) 
~~~ parent_traj_group = traj_group . f_get_parent ( ) 
parent_hdf5_group = self . _all_create_or_get_groups ( parent_traj_group . v_full_name ) [ 0 ] 
self . _tree_store_nodes_dfs ( parent_traj_group , traj_group . v_name , store_data = store_data , 
max_depth = max_depth , current_depth = 0 , 
parent_hdf5_group = parent_hdf5_group ) 
~~ ~~ def _grp_load_group ( self , traj_group , load_data = pypetconstants . LOAD_DATA , with_links = True , 
recursive = False , max_depth = None , 
_traj = None , _as_new = False , _hdf5_group = None ) : 
if _hdf5_group is None : 
~~~ _hdf5_group = self . _all_get_node_by_name ( traj_group . v_full_name ) 
_traj = traj_group . v_root 
~~~ parent_traj_node = traj_group . f_get_parent ( ) 
self . _tree_load_nodes_dfs ( parent_traj_node , load_data = load_data , with_links = with_links , 
current_depth = 0 , 
trajectory = _traj , as_new = _as_new , 
~~~ if load_data == pypetconstants . LOAD_NOTHING : 
~~ elif load_data == pypetconstants . OVERWRITE_DATA : 
~~ self . _all_load_skeleton ( traj_group , _hdf5_group ) 
traj_group . _stored = not _as_new 
~~ ~~ def _all_load_skeleton ( self , traj_node , hdf5_group ) : 
if traj_node . v_annotations . f_is_empty ( ) : 
~~~ self . _ann_load_annotations ( traj_node , hdf5_group ) 
~~ if traj_node . v_comment == '' : 
~~~ comment = self . _all_get_from_attrs ( hdf5_group , HDF5StorageService . COMMENT ) 
if comment is None : 
~~~ comment = '' 
~~ traj_node . v_comment = comment 
~~ ~~ def _prm_extract_missing_flags ( data_dict , flags_dict ) : 
for key , data in data_dict . items ( ) : 
~~~ if not key in flags_dict : 
~~~ dtype = type ( data ) 
if ( dtype is np . ndarray or dtype is dict ) and len ( data ) == 0 : 
~~~ flags_dict [ key ] = HDF5StorageService . ARRAY 
~~~ flags_dict [ key ] = HDF5StorageService . TYPE_FLAG_MAPPING [ dtype ] 
~~ ~~ ~~ ~~ ~~ def _prm_meta_add_summary ( self , instance ) : 
if instance . v_comment == '' : 
~~ where = instance . v_branch 
definitely_store_comment = True 
bytes_comment = instance . v_comment . encode ( 'utf-8' ) 
hexdigest = hashlib . sha1 ( bytes_comment ) . hexdigest ( ) 
hexdigest = hexdigest . encode ( 'utf-8' ) 
table_name = where + '_summary' 
if table_name in self . _overview_group : 
~~~ table = getattr ( self . _overview_group , table_name ) 
~~~ return definitely_store_comment 
~~~ condvars = { 'hexdigestcol' : table . cols . hexdigest , 
'hexdigest' : hexdigest } 
row_iterator = table . where ( condition , condvars = condvars ) 
row = None 
~~ if row is None : 
~~~ self . _all_store_param_or_result_table_entry ( instance , table , 
flags = ( 
HDF5StorageService . ADD_ROW , ) , 
additional_info = { 
'hexdigest' : hexdigest } ) 
~~~ definitely_store_comment = False 
self . _all_kill_iterator ( row_iterator ) 
~~~ definitely_store_comment = True 
~~ return definitely_store_comment 
~~ def _prm_add_meta_info ( self , instance , group , overwrite = False ) : 
if overwrite : 
~~~ flags = ( ) 
~~~ flags = ( HDF5StorageService . ADD_ROW , ) 
~~ definitely_store_comment = True 
~~~ definitely_store_comment = self . _prm_meta_add_summary ( instance ) 
~~~ table_name = instance . v_branch + '_overview' 
table = getattr ( self . _overview_group , table_name ) 
if len ( table ) < pypetconstants . HDF5_MAX_OVERVIEW_TABLE_LENGTH : 
flags = flags ) 
~~ if ( ( not self . _purge_duplicate_comments or definitely_store_comment ) and 
instance . v_comment != '' ) : 
~~~ setattr ( group . _v_attrs , HDF5StorageService . COMMENT , instance . v_comment ) 
~~ setattr ( group . _v_attrs , HDF5StorageService . CLASS_NAME , instance . f_get_class_name ( ) ) 
setattr ( group . _v_attrs , HDF5StorageService . LEAF , True ) 
if instance . v_is_parameter and instance . v_explored : 
~~~ tablename = 'explored_parameters_overview' 
table = getattr ( self . _overview_group , tablename ) 
~~ ~~ ~~ def _prm_store_from_dict ( self , fullname , store_dict , hdf5_group , store_flags , kwargs ) : 
for key , data_to_store in store_dict . items ( ) : 
~~~ original_hdf5_group = None 
flag = store_flags [ key ] 
if '.' in key : 
~~~ original_hdf5_group = hdf5_group 
split_key = key . split ( '.' ) 
key = split_key . pop ( ) 
for inner_key in split_key : 
~~~ hdf5_group , newly_created = self . _all_create_or_get_group ( inner_key , 
hdf5_group ) 
if newly_created : 
~~~ setattr ( hdf5_group . _v_attrs , HDF5StorageService . STORAGE_TYPE , 
HDF5StorageService . NESTED_GROUP ) 
~~~ store_type = self . _all_get_from_attrs ( hdf5_group , HDF5StorageService . STORAGE_TYPE ) 
if store_type != HDF5StorageService . NESTED_GROUP : 
~~ ~~ ~~ ~~ if key in hdf5_group : 
~~~ self . _logger . debug ( 
( key , fullname ) ) 
~~ if flag == HDF5StorageService . TABLE : 
~~~ self . _prm_write_into_pytable ( key , data_to_store , hdf5_group , fullname , 
** kwargs ) 
~~ elif flag == HDF5StorageService . DICT : 
~~~ self . _prm_write_dict_as_table ( key , data_to_store , hdf5_group , fullname , 
~~ elif flag == HDF5StorageService . ARRAY : 
~~~ self . _prm_write_into_array ( key , data_to_store , hdf5_group , fullname , 
~~ elif flag in ( HDF5StorageService . CARRAY , 
HDF5StorageService . EARRAY , 
HDF5StorageService . VLARRAY ) : 
~~~ self . _prm_write_into_other_array ( key , data_to_store , 
hdf5_group , fullname , 
flag = flag , ** kwargs ) 
~~ elif flag in ( HDF5StorageService . SERIES , 
HDF5StorageService . FRAME , 
~~~ self . _prm_write_pandas_data ( key , data_to_store , hdf5_group , fullname , 
flag , ** kwargs ) 
~~ elif flag == HDF5StorageService . SHARED_DATA : 
~~ if original_hdf5_group is not None : 
~~~ hdf5_group = original_hdf5_group 
~~ ~~ ~~ def _prm_store_parameter_or_result ( self , 
store_flags = None , 
overwrite = None , 
_hdf5_group = None , 
_newly_created = False , 
** kwargs ) : 
~~ elif store_data == pypetconstants . STORE_DATA_SKIPPING and instance . _stored : 
instance . v_full_name ) 
~~ elif store_data == pypetconstants . OVERWRITE_DATA : 
~~~ if not overwrite : 
~~~ overwrite = True 
~~ ~~ fullname = instance . v_full_name 
~~~ _hdf5_group , _newly_created = self . _all_create_or_get_groups ( fullname ) 
~~ store_dict = { } 
if store_flags is None : 
~~~ store_flags = { } 
~~~ if not instance . f_is_empty ( ) : 
~~~ store_dict = instance . _store ( ) 
~~~ instance_flags = { } 
~~ instance_flags . update ( store_flags ) 
store_flags = instance_flags 
self . _prm_extract_missing_flags ( store_dict , store_flags ) 
~~~ if isinstance ( overwrite , str ) : 
~~~ overwrite = [ overwrite ] 
~~ if overwrite is True : 
~~~ to_delete = [ key for key in store_dict . keys ( ) if key in _hdf5_group ] 
self . _all_delete_parameter_or_result_or_group ( instance , 
delete_only = to_delete , 
_hdf5_group = _hdf5_group ) 
~~ elif isinstance ( overwrite , ( list , tuple ) ) : 
~~~ overwrite_set = set ( overwrite ) 
key_set = set ( store_dict . keys ( ) ) 
stuff_not_to_be_overwritten = overwrite_set - key_set 
if overwrite != 'v_annotations' and len ( stuff_not_to_be_overwritten ) > 0 : 
str ( stuff_not_to_be_overwritten ) ) 
~~ stuff_to_overwrite = overwrite_set & key_set 
if len ( stuff_to_overwrite ) > 0 : 
~~~ self . _all_delete_parameter_or_result_or_group ( instance , 
delete_only = list ( 
stuff_to_overwrite ) ) 
'overwriting.' % str ( overwrite ) ) 
~~ ~~ self . _prm_store_from_dict ( fullname , store_dict , _hdf5_group , store_flags , kwargs ) 
self . _ann_store_annotations ( instance , _hdf5_group , overwrite = overwrite ) 
if _newly_created or overwrite is True : 
~~~ self . _prm_add_meta_info ( instance , _hdf5_group , 
overwrite = not _newly_created ) 
~~ instance . _stored = True 
~~~ self . _logger . error ( 
for key in store_dict . keys ( ) : 
~~~ if key in _hdf5_group : 
~~~ hdf5_child = _hdf5_group . _f_get_child ( key ) 
hdf5_child . _f_remove ( recursive = True ) 
~~ ~~ if _hdf5_group . _v_nchildren == 0 : 
~~~ _hdf5_group . _f_remove ( recursive = True ) 
~~ ~~ def _prm_select_shared_pandas_data ( self , pd_node , full_name , ** kwargs ) : 
~~~ pathname = pd_node . _v_pathname 
pandas_store = self . _hdf5store 
return pandas_store . select ( pathname , ** kwargs ) 
~~ ~~ def _prm_write_shared_array ( self , key , data , hdf5_group , full_name , flag , ** kwargs ) : 
if flag == HDF5StorageService . ARRAY : 
~~~ self . _prm_write_into_array ( key , data , hdf5_group , full_name , ** kwargs ) 
~~~ self . _prm_write_into_other_array ( key , data , hdf5_group , full_name , 
( flag , key , full_name ) ) 
~~ self . _hdf5file . flush ( ) 
~~ def _prm_write_shared_table ( self , key , hdf5_group , fullname , ** kwargs ) : 
first_row = None 
description = None 
if 'first_row' in kwargs : 
~~~ first_row = kwargs . pop ( 'first_row' ) 
if not 'description' in kwargs : 
~~~ description = { } 
for colname in first_row : 
~~~ data = first_row [ colname ] 
column = self . _all_get_table_col ( key , [ data ] , fullname ) 
description [ colname ] = column 
~~ ~~ ~~ if 'description' in kwargs : 
~~~ description = kwargs . pop ( 'description' ) 
~~ if 'filters' in kwargs : 
~~~ filters = kwargs . pop ( 'filters' ) 
~~~ filters = self . _all_get_filters ( kwargs ) 
~~ table = self . _hdf5file . create_table ( where = hdf5_group , name = key , 
filters = filters , 
if first_row is not None : 
for key in description : 
~~~ row [ key ] = first_row [ key ] 
~~ row . append ( ) 
~~ ~~ def _prm_write_dict_as_table ( self , key , data_to_store , group , fullname , ** kwargs ) : 
if key in group : 
~~ if key in group : 
~~ temp_dict = { } 
for innerkey in data_to_store : 
~~~ val = data_to_store [ innerkey ] 
temp_dict [ innerkey ] = [ val ] 
~~ objtable = ObjectTable ( data = temp_dict ) 
self . _prm_write_into_pytable ( key , objtable , group , fullname , ** kwargs ) 
new_table = group . _f_get_child ( key ) 
self . _all_set_attributes_to_recall_natives ( temp_dict , new_table , 
HDF5StorageService . DATA_PREFIX ) 
setattr ( new_table . _v_attrs , HDF5StorageService . STORAGE_TYPE , 
HDF5StorageService . DICT ) 
~~ def _prm_write_pandas_data ( self , key , data , group , fullname , flag , ** kwargs ) : 
~~~ if 'filters' not in kwargs : 
kwargs [ 'filters' ] = filters 
~~ if 'format' not in kwargs : 
~~~ kwargs [ 'format' ] = self . pandas_format 
~~ if 'encoding' not in kwargs : 
~~~ kwargs [ 'encoding' ] = self . encoding 
~~ overwrite = kwargs . pop ( 'overwrite' , False ) 
if key in group and not ( overwrite or kwargs . get ( 'append' , False ) ) : 
~~ if data is not None and ( kwargs [ 'format' ] == 'f' or kwargs [ 'format' ] == 'fixed' ) : 
~~~ kwargs [ 'expectedrows' ] = data . shape [ 0 ] 
~~ name = group . _v_pathname + '/' + key 
self . _hdf5store . put ( name , data , ** kwargs ) 
frame_group = group . _f_get_child ( key ) 
setattr ( frame_group . _v_attrs , HDF5StorageService . STORAGE_TYPE , flag ) 
~~ ~~ def _prm_write_into_other_array ( self , key , data , group , fullname , 
flag , ** kwargs ) : 
~~~ if flag == HDF5StorageService . CARRAY : 
~~~ factory = self . _hdf5file . create_carray 
~~ elif flag == HDF5StorageService . EARRAY : 
~~~ factory = self . _hdf5file . create_earray 
~~ elif flag == HDF5StorageService . VLARRAY : 
~~~ factory = self . _hdf5file . create_vlarray 
~~~ other_array = factory ( where = group , name = key , obj = data , 
filters = filters , ** kwargs ) 
~~ except ( ValueError , TypeError ) as exc : 
~~~ conv_data = data [ : ] 
conv_data = np . core . defchararray . encode ( conv_data , self . encoding ) 
other_array = factory ( where = group , name = key , 
obj = conv_data , 
~~~ raise exc 
~~ ~~ if data is not None : 
~~~ self . _all_set_attributes_to_recall_natives ( data , other_array , 
~~ setattr ( other_array . _v_attrs , HDF5StorageService . STORAGE_TYPE , flag ) 
~~ ~~ def _prm_write_into_array ( self , key , data , group , fullname , ** kwargs ) : 
~~~ if key in group : 
~~~ array = self . _hdf5file . create_array ( where = group , 
name = key , obj = data , ** kwargs ) 
~~ except ( TypeError , ValueError ) as exc : 
~~~ if type ( data ) is dict and len ( data ) == 0 : 
~~~ conv_data = ( ) 
~~ elif isinstance ( data , str ) : 
~~~ conv_data = data . encode ( self . _encoding ) 
~~ elif isinstance ( data , int ) : 
~~~ conv_data = np . int64 ( data ) 
~~~ conv_data = [ ] 
for string in data : 
~~~ conv_data . append ( string . encode ( self . _encoding ) ) 
~~ ~~ array = self . _hdf5file . create_array ( where = group , 
name = key , obj = conv_data , ** kwargs ) 
~~~ self . _all_set_attributes_to_recall_natives ( data , array , 
~~ setattr ( array . _v_attrs , HDF5StorageService . STORAGE_TYPE , 
HDF5StorageService . ARRAY ) 
~~ ~~ def _lnk_delete_link ( self , link_name ) : 
translated_name = '/' + self . _trajectory_name + '/' + link_name . replace ( '.' , '/' ) 
link = self . _hdf5file . get_node ( where = translated_name ) 
link . _f_remove ( ) 
~~ def _all_delete_parameter_or_result_or_group ( self , instance , 
delete_only = None , 
remove_from_item = False , 
_hdf5_group = None ) : 
split_name = instance . v_location . split ( '.' ) 
~~~ where = '/' + self . _trajectory_name + '/' + '/' . join ( split_name ) 
node_name = instance . v_name 
_hdf5_group = self . _hdf5file . get_node ( where = where , name = node_name ) 
~~ if delete_only is None : 
~~~ if instance . v_is_group and not recursive and len ( _hdf5_group . _v_children ) != 0 : 
~~ _hdf5_group . _f_remove ( recursive = True ) 
~~~ if not instance . v_is_leaf : 
~~ if isinstance ( delete_only , str ) : 
~~~ delete_only = [ delete_only ] 
~~ for delete_item in delete_only : 
~~~ if ( remove_from_item and 
hasattr ( instance , '__contains__' ) and 
hasattr ( instance , '__delattr__' ) and 
delete_item in instance ) : 
~~~ delattr ( instance , delete_item ) 
~~~ _hdf5_sub_group = self . _hdf5file . get_node ( where = _hdf5_group , 
name = delete_item ) 
_hdf5_sub_group . _f_remove ( recursive = True ) 
( delete_item , instance . v_full_name ) ) 
~~ ~~ ~~ ~~ def _prm_write_into_pytable ( self , tablename , data , hdf5_group , fullname , ** kwargs ) : 
datasize = data . shape [ 0 ] 
~~~ description_dict , data_type_dict = self . _prm_make_description ( data , fullname ) 
description_dicts = [ { } ] 
if len ( description_dict ) > ptpa . MAX_COLUMNS : 
~~~ new_table_group = self . _hdf5file . create_group ( where = hdf5_group , 
name = tablename , 
filters = self . _all_get_filters ( kwargs . copy ( ) ) ) 
for innerkey in description_dict : 
~~~ val = description_dict [ innerkey ] 
if count == ptpa . MAX_COLUMNS : 
~~~ description_dicts . append ( { } ) 
~~ description_dicts [ - 1 ] [ innerkey ] = val 
~~ setattr ( new_table_group . _v_attrs , HDF5StorageService . STORAGE_TYPE , 
HDF5StorageService . TABLE ) 
setattr ( new_table_group . _v_attrs , HDF5StorageService . SPLIT_TABLE , 1 ) 
hdf5_group = new_table_group 
~~~ description_dicts = [ description_dict ] 
~~ for idx , descr_dict in enumerate ( description_dicts ) : 
~~~ if idx == 0 : 
~~~ tblname = tablename 
~~~ tblname = tablename + '_%d' % idx 
~~ table = self . _hdf5file . create_table ( where = hdf5_group , name = tblname , 
description = descr_dict , 
title = tblname , 
expectedrows = datasize , 
row = table . row 
for n in range ( datasize ) : 
~~~ for key in descr_dict : 
~~~ row [ key ] = data [ key ] [ n ] 
~~ if idx == 0 and len ( description_dict ) <= ptpa . MAX_COLUMNS : 
~~~ for field_name in data_type_dict : 
~~~ type_description = data_type_dict [ field_name ] 
self . _all_set_attr ( table , field_name , type_description ) 
~~ setattr ( table . _v_attrs , HDF5StorageService . STORAGE_TYPE , 
~~ table . flush ( ) 
~~ if len ( description_dict ) > ptpa . MAX_COLUMNS : 
~~~ tblname = tablename + '__' + HDF5StorageService . STORAGE_TYPE 
field_names , data_types = list ( zip ( * data_type_dict . items ( ) ) ) 
data_type_table_dict = { 'field_name' : field_names , 'data_type' : data_types } 
descr_dict , _ = self . _prm_make_description ( data_type_table_dict , fullname ) 
table = self . _hdf5file . create_table ( where = hdf5_group , name = tblname , 
expectedrows = len ( field_names ) , 
filters = self . _all_get_filters ( kwargs ) ) 
for n in range ( len ( field_names ) ) : 
~~~ for key in data_type_table_dict : 
~~~ row [ key ] = data_type_table_dict [ key ] [ n ] 
~~ setattr ( table . _v_attrs , HDF5StorageService . DATATYPE_TABLE , 1 ) 
~~ ~~ def _prm_make_description ( self , data , fullname ) : 
def _convert_lists_and_tuples ( series_of_data ) : 
if isinstance ( series_of_data [ 0 ] , 
~~~ for idx , item in enumerate ( series_of_data ) : 
~~~ series_of_data [ idx ] = np . array ( item ) 
for key in data : 
~~~ val = data [ key ] 
self . _all_set_attributes_to_recall_natives ( val [ 0 ] , PTItemMock ( original_data_type_dict ) , 
HDF5StorageService . FORMATTED_COLUMN_PREFIX % 
key ) 
_convert_lists_and_tuples ( val ) 
col = self . _all_get_table_col ( key , val , fullname ) 
descriptiondict [ key ] = col 
~~ return descriptiondict , original_data_type_dict 
~~ def _all_get_table_col ( self , key , column , fullname ) : 
val = column [ 0 ] 
~~~ if type ( val ) is int : 
~~~ return pt . IntCol ( ) 
~~ if isinstance ( val , ( str , bytes ) ) : 
~~~ itemsize = int ( self . _prm_get_longest_stringsize ( column ) ) 
return pt . StringCol ( itemsize ) 
~~ if isinstance ( val , np . ndarray ) : 
~~~ if ( np . issubdtype ( val . dtype , str ) or 
np . issubdtype ( val . dtype , bytes ) ) : 
return pt . StringCol ( itemsize , shape = val . shape ) 
~~~ return pt . Col . from_dtype ( np . dtype ( ( val . dtype , val . shape ) ) ) 
~~~ return pt . Col . from_dtype ( np . dtype ( type ( val ) ) ) 
~~ ~~ def _prm_get_longest_stringsize ( string_list ) : 
maxlength = 1 
for stringar in string_list : 
~~~ if isinstance ( stringar , np . ndarray ) : 
~~~ if stringar . ndim > 0 : 
~~~ for string in stringar . ravel ( ) : 
~~~ maxlength = max ( len ( string ) , maxlength ) 
~~~ maxlength = max ( len ( stringar . tolist ( ) ) , maxlength ) 
~~~ maxlength = max ( len ( stringar ) , maxlength ) 
~~ ~~ return int ( maxlength * 1.5 ) 
~~ def _prm_load_into_dict ( self , full_name , load_dict , hdf5_group , instance , 
load_only , load_except , load_flags , _prefix = '' ) : 
for node in hdf5_group : 
~~~ load_type = self . _all_get_from_attrs ( node , HDF5StorageService . STORAGE_TYPE ) 
if _prefix : 
~~~ load_name = '%s.%s' % ( _prefix , node . _v_name ) 
~~~ load_name = node . _v_name 
~~ if load_type == HDF5StorageService . NESTED_GROUP : 
~~~ self . _prm_load_into_dict ( full_name = full_name , 
load_dict = load_dict , 
hdf5_group = node , 
instance = instance , 
load_only = load_only , 
load_except = load_except , 
load_flags = load_flags , 
_prefix = load_name ) 
~~ if load_only is not None : 
~~~ if load_name not in load_only : 
~~~ load_only . remove ( load_name ) 
~~ ~~ elif load_except is not None : 
~~~ if load_name in load_except : 
~~~ load_except . remove ( load_name ) 
~~ ~~ if load_name in load_flags : 
~~~ load_type = load_flags [ load_name ] 
~~ if load_type == HDF5StorageService . DICT : 
~~~ to_load = self . _prm_read_dictionary ( node , full_name ) 
~~ elif load_type == HDF5StorageService . TABLE : 
~~~ to_load = self . _prm_read_table ( node , full_name ) 
~~ elif load_type in ( HDF5StorageService . ARRAY , HDF5StorageService . CARRAY , 
HDF5StorageService . EARRAY , HDF5StorageService . VLARRAY ) : 
~~~ to_load = self . _prm_read_array ( node , full_name ) 
~~ elif load_type in ( HDF5StorageService . FRAME , 
HDF5StorageService . SERIES , 
~~~ to_load = self . _prm_read_pandas ( node , full_name ) 
~~ elif load_type . startswith ( HDF5StorageService . SHARED_DATA ) : 
~~~ to_load = self . _prm_read_shared_data ( node , instance ) 
( full_name , str ( node ) , str ( load_type ) ) ) 
~~ if to_load is None : 
~~ load_dict [ load_name ] = to_load 
~~ ~~ def _prm_load_parameter_or_result ( self , instance , 
load_only = None , 
load_except = None , 
load_flags = None , 
_hdf5_group = None , ) : 
~~~ _hdf5_group = self . _all_get_node_by_name ( instance . v_full_name ) 
~~ if load_data == pypetconstants . OVERWRITE_DATA : 
~~~ if instance . v_is_parameter and instance . v_locked : 
~~ instance . f_empty ( ) 
instance . v_annotations . f_empty ( ) 
instance . v_comment = '' 
~~ self . _all_load_skeleton ( instance , _hdf5_group ) 
instance . _stored = True 
if isinstance ( load_only , str ) : 
~~~ load_only = [ load_only ] 
~~ if isinstance ( load_except , str ) : 
~~~ load_except = [ load_except ] 
~~ if load_data == pypetconstants . LOAD_SKELETON : 
~~~ self . _node_processing_timer . signal_update ( ) 
~~ elif load_only is not None : 
~~~ if load_except is not None : 
~~ elif instance . v_is_parameter and instance . v_locked : 
str ( load_only ) ) 
load_only = set ( load_only ) 
~~ elif load_except is not None : 
str ( load_except ) ) 
load_except = set ( load_except ) 
~~ elif not instance . f_is_empty ( ) : 
~~ full_name = instance . v_full_name 
if load_flags is None : 
~~~ load_flags = { } 
~~ instance_flags . update ( load_flags ) 
load_flags = instance_flags 
self . _prm_load_into_dict ( full_name = full_name , 
hdf5_group = _hdf5_group , 
load_flags = load_flags ) 
if load_only is not None : 
~~~ if len ( load_only ) > 0 : 
( str ( load_only ) , full_name ) ) 
~~~ if len ( load_except ) > 0 : 
~~ ~~ if load_dict : 
~~~ instance . _load ( load_dict ) 
if instance . v_is_parameter : 
~~~ instance . f_lock ( ) 
~~ ~~ self . _node_processing_timer . signal_update ( ) 
~~ def _prm_read_dictionary ( self , leaf , full_name ) : 
~~~ temp_table = self . _prm_read_table ( leaf , full_name ) 
temp_dict = temp_table . to_dict ( 'list' ) 
innder_dict = { } 
for innerkey , vallist in temp_dict . items ( ) : 
~~~ innder_dict [ innerkey ] = vallist [ 0 ] 
~~ return innder_dict 
~~ ~~ def _prm_read_shared_data ( self , shared_node , instance ) : 
~~~ data_type = self . _all_get_from_attrs ( shared_node , 
HDF5StorageService . SHARED_DATA_TYPE ) 
constructor = shared . FLAG_CLASS_MAPPING [ data_type ] 
name = shared_node . _v_name 
result = constructor ( name = name , parent = instance ) 
~~ ~~ def _prm_read_pandas ( self , pd_node , full_name ) : 
~~~ name = pd_node . _v_name 
pathname = pd_node . _v_pathname 
pandas_data = pandas_store . get ( pathname ) 
return pandas_data 
~~ ~~ def _prm_read_table ( self , table_or_group , full_name ) : 
~~~ result_table = None 
if self . _all_get_from_attrs ( table_or_group , HDF5StorageService . SPLIT_TABLE ) : 
~~~ table_name = table_or_group . _v_name 
data_type_table_name = table_name + '__' + HDF5StorageService . STORAGE_TYPE 
data_type_table = table_or_group . _v_children [ data_type_table_name ] 
data_type_dict = { } 
for row in data_type_table : 
~~~ fieldname = row [ 'field_name' ] . decode ( 'utf-8' ) 
data_type_dict [ fieldname ] = row [ 'data_type' ] . decode ( 'utf-8' ) 
~~ for sub_table in table_or_group : 
~~~ sub_table_name = sub_table . _v_name 
if sub_table_name == data_type_table_name : 
~~ for colname in sub_table . colnames : 
~~~ col = sub_table . col ( colname ) 
data_list = list ( col ) 
prefix = HDF5StorageService . FORMATTED_COLUMN_PREFIX % colname 
for idx , data in enumerate ( data_list ) : 
~~~ data , type_changed = self . _all_recall_native_type ( data , 
PTItemMock ( 
data_type_dict ) , 
prefix ) 
if type_changed : 
~~~ data_list [ idx ] = data 
~~ ~~ if result_table is None : 
~~~ result_table = ObjectTable ( data = { colname : data_list } ) 
~~~ result_table [ colname ] = data_list 
~~~ for colname in table_or_group . colnames : 
~~~ col = table_or_group . col ( colname ) 
~~~ data , type_changed = self . _all_recall_native_type ( data , table_or_group , 
~~ ~~ ~~ return result_table 
~~ ~~ def _prm_read_array ( self , array , full_name ) : 
~~~ result = self . _svrc_read_array ( array ) 
result , dummy = self . _all_recall_native_type ( result , array , 
~~ ~~ def load_trajectory ( name = None , 
index = None , 
as_new = False , 
load_parameters = pypetconstants . LOAD_DATA , 
load_derived_parameters = pypetconstants . LOAD_SKELETON , 
load_results = pypetconstants . LOAD_SKELETON , 
load_other_data = pypetconstants . LOAD_SKELETON , 
load_data = None , 
force = False , 
dynamic_imports = None , 
new_name = 'my_trajectory' , 
add_time = True , 
wildcard_functions = None , 
with_run_information = True , 
storage_service = storage . HDF5StorageService , 
~~ elif name is not None and index is not None : 
~~ traj = Trajectory ( name = new_name , add_time = add_time , dynamic_imports = dynamic_imports , 
wildcard_functions = wildcard_functions ) 
traj . f_load ( name = name , index = index , as_new = as_new , load_parameters = load_parameters , 
load_derived_parameters = load_derived_parameters , load_results = load_results , 
load_other_data = load_other_data , recursive = recursive , load_data = load_data , 
max_depth = max_depth , force = force , with_run_information = with_run_information , 
storage_service = storage_service , ** kwargs ) 
return traj 
~~ def make_set_name ( idx ) : 
GROUPSIZE = 1000 
set_idx = idx // GROUPSIZE 
if set_idx >= 0 : 
~~~ return pypetconstants . FORMATTED_SET_NAME % set_idx 
~~~ return pypetconstants . SET_NAME_DUMMY 
~~ ~~ def f_add_wildcard_functions ( self , func_dict ) : 
~~~ """#TODO""" 
for wildcards , function in func_dict . items ( ) : 
~~~ if not isinstance ( wildcards , tuple ) : 
~~~ wildcards = ( wildcards , ) 
~~ for wildcard in wildcards : 
~~~ if wildcard in self . _wildcard_keys : 
~~ self . _wildcard_keys [ wildcard ] = wildcards 
~~ self . _wildcard_functions [ wildcards ] = function 
~~ ~~ def f_wildcard ( self , wildcard = '$' , run_idx = None ) : 
if run_idx is None : 
~~~ run_idx = self . v_idx 
~~ wildcards = self . _wildcard_keys [ wildcard ] 
~~~ return self . _wildcard_cache [ ( wildcards , run_idx ) ] 
~~~ translation = self . _wildcard_functions [ wildcards ] ( run_idx ) 
self . _wildcard_cache [ ( wildcards , run_idx ) ] = translation 
return translation 
~~ ~~ def v_full_copy ( self , val ) : 
self . _full_copy = bool ( val ) 
for param in self . _explored_parameters . values ( ) : 
~~~ if param is not None : 
~~~ param . v_full_copy = bool ( val ) 
~~ ~~ ~~ def f_set_properties ( self , ** kwargs ) : 
for name in kwargs : 
~~~ val = kwargs [ name ] 
if not name . startswith ( 'v_' ) : 
~~~ name = 'v_' + name 
~~ if not name in self . _nn_interface . _not_admissible_names : 
~~~ setattr ( self , name , val ) 
~~ ~~ ~~ def f_add_to_dynamic_imports ( self , dynamic_imports ) : 
if not isinstance ( dynamic_imports , ( list , tuple ) ) : 
~~~ dynamic_imports = [ dynamic_imports ] 
~~ for item in dynamic_imports : 
~~~ if not ( isinstance ( item , str ) or inspect . isclass ( item ) ) : 
str ( item ) ) 
~~ ~~ self . _dynamic_imports . extend ( dynamic_imports ) 
~~ def f_set_crun ( self , name_or_idx ) : 
if ( name_or_idx is None or name_or_idx == self . f_wildcard ( '$' , - 1 ) or 
name_or_idx == - 1 ) : 
~~~ self . f_restore_default ( ) 
~~~ if isinstance ( name_or_idx , str ) : 
~~~ self . _idx = self . f_idx_to_run ( name_or_idx ) 
self . _crun = name_or_idx 
~~~ self . _crun = self . f_idx_to_run ( name_or_idx ) 
self . _idx = name_or_idx 
~~ self . _set_explored_parameters_to_idx ( self . v_idx ) 
~~ ~~ def f_iter_runs ( self , start = 0 , stop = None , step = 1 , yields = 'name' ) : 
if stop is None : 
~~~ stop = len ( self ) 
~~ elif stop > len ( self ) : 
~~ yields = yields . lower ( ) 
if yields == 'name' : 
~~~ yield_func = lambda x : self . f_idx_to_run ( x ) 
~~ elif yields == 'idx' : 
~~~ yield_func = lambda x : x 
~~ elif yields == 'self' : 
~~~ yield_func = lambda x : self 
~~ elif yields == 'copy' : 
~~~ yield_func = lambda x : self . __copy__ ( ) 
~~ for idx in range ( start , stop , step ) : 
~~~ self . f_set_crun ( idx ) 
yield yield_func ( idx ) 
~~ self . f_set_crun ( None ) 
~~ def f_shrink ( self , force = False ) : 
if self . _stored and not force : 
~~ for param in self . _explored_parameters . values ( ) : 
~~~ param . f_unlock ( ) 
~~~ param . _shrink ( ) 
( param . v_full_name , repr ( exc ) ) ) 
~~ ~~ self . _explored_parameters = { } 
self . _run_information = { } 
self . _single_run_ids = { } 
self . _add_run_info ( 0 ) 
self . _test_run_addition ( 1 ) 
~~ def _preset ( self , name , args , kwargs ) : 
if self . f_contains ( name , shortcuts = False ) : 
~~~ self . _changed_default_parameters [ name ] = ( args , kwargs ) 
~~ ~~ def f_preset_config ( self , config_name , * args , ** kwargs ) : 
if not config_name . startswith ( 'config.' ) : 
~~~ config_name = 'config.' + config_name 
~~ self . _preset ( config_name , args , kwargs ) 
~~ def f_preset_parameter ( self , param_name , * args , ** kwargs ) : 
if not param_name . startswith ( 'parameters.' ) : 
~~~ param_name = 'parameters.' + param_name 
~~ self . _preset ( param_name , args , kwargs ) 
~~ def _prepare_experiment ( self ) : 
if len ( self . _changed_default_parameters ) : 
~~~ raise pex . PresettingError ( 
str ( self . _changed_default_parameters ) ) 
~~ self . f_lock_parameters ( ) 
self . f_lock_derived_parameters ( ) 
~~ def f_get_from_runs ( self , name , include_default_run = True , use_indices = False , 
fast_access = False , with_links = True , 
result_dict = OrderedDict ( ) 
old_crun = self . v_crun 
~~~ if len ( self . _run_parent_groups ) > 0 : 
~~~ for run_name in self . f_iter_runs ( ) : 
~~~ value = None 
already_found = False 
for run_parent_group in self . _run_parent_groups . values ( ) : 
~~~ if run_name not in run_parent_group . _children : 
~~~ value = run_parent_group . f_get ( run_name + '.' + name , 
fast_access = False , 
with_links = with_links , 
auto_load = auto_load ) 
if already_found : 
~~~ already_found = True 
~~ ~~ except ( AttributeError , pex . DataNotInStorageError ) : 
~~ ~~ if value is None and include_default_run : 
~~~ for run_parent_group in self . _run_parent_groups . values ( ) : 
~~~ value = run_parent_group . f_get ( self . f_wildcard ( '$' , - 1 ) + 
'.' + name , 
~~ ~~ ~~ if value is not None : 
~~~ if value . v_is_leaf : 
~~~ value = self . _nn_interface . _apply_fast_access ( value , fast_access ) 
~~ if use_indices : 
~~~ key = self . f_idx_to_run ( run_name ) 
~~~ key = run_name 
~~ result_dict [ key ] = value 
~~~ self . v_crun = old_crun 
~~ ~~ def _is_completed ( self , name_or_id = None ) : 
if name_or_id is None : 
~~~ return all ( 
( runinfo [ 'completed' ] for runinfo in self . _run_information . values ( ) ) ) 
~~~ return self . f_get_run_information ( name_or_id , copy = False ) [ 'completed' ] 
~~ ~~ def f_expand ( self , build_dict , fail_safe = True ) : 
if len ( self . _explored_parameters ) == 0 : 
return self . f_explore ( build_dict ) 
~~ enlarge_set = set ( [ self . f_get ( key ) . v_full_name 
for key in build_dict . keys ( ) ] ) 
if not set ( self . _explored_parameters . keys ( ) ) == enlarge_set : 
( str ( set ( self . _explored_parameters . keys ( ) ) ) , 
str ( set ( build_dict . keys ( ) ) ) ) ) 
~~ if any ( x is None for x in self . _explored_parameters . values ( ) ) : 
~~ old_ranges = None 
if fail_safe : 
~~~ old_ranges = { } 
for param_name in self . _explored_parameters : 
~~~ old_ranges [ param_name ] = self . _explored_parameters [ param_name ] . f_get_range ( ) 
~~~ old_ranges = cp . deepcopy ( old_ranges ) 
old_ranges = None 
length = None 
for key , builditerable in build_dict . items ( ) : 
~~~ act_param = self . f_get ( key ) 
act_param . f_unlock ( ) 
act_param . _expand ( builditerable ) 
name = act_param . v_full_name 
self . _explored_parameters [ name ] = act_param 
if count == 0 : 
~~~ length = act_param . f_get_range_length ( ) 
~~ elif not length == act_param . f_get_range_length ( ) : 
~~ count += 1 
~~ original_length = len ( self ) 
for irun in range ( original_length , length ) : 
~~~ self . _add_run_info ( irun ) 
~~ self . _test_run_addition ( length ) 
self . _remove_exploration ( ) 
~~~ if old_ranges is not None : 
~~~ for param_name in old_ranges : 
~~~ param_range = old_ranges [ param_name ] 
param = self . _explored_parameters [ param_name ] 
param . f_unlock ( ) 
~~ param . _explore ( param_range ) 
param . _explored = True 
~~ ~~ raise 
~~ ~~ def _remove_exploration ( self ) : 
~~~ if param . _stored : 
~~~ self . f_delete_item ( param ) 
~~ ~~ ~~ ~~ def f_copy ( self , copy_leaves = True , 
with_links = True ) : 
new_traj = Trajectory ( _copy_traj = True ) 
new_traj . _length = self . _length 
new_traj . _name = self . _name 
new_traj . _timestamp = self . _timestamp 
new_traj . _time = self . _time 
new_traj . _single_run_ids = self . _single_run_ids 
new_traj . _run_information = self . _run_information 
new_traj . _updated_run_information = self . _updated_run_information 
new_traj . _fast_access = self . _fast_access 
new_traj . _shortcuts = self . _shortcuts 
new_traj . _iter_recursive = self . _iter_recursive 
new_traj . _max_depth = self . _max_depth 
new_traj . _auto_load = self . _auto_load 
new_traj . _with_links = self . _with_links 
new_traj . _environment_hexsha = self . _environment_hexsha 
new_traj . _environment_name = self . _environment_name 
new_traj . _idx = self . _idx 
new_traj . _crun = self . _crun 
new_traj . _standard_parameter = self . _standard_parameter 
new_traj . _standard_result = self . _standard_result 
new_traj . _standard_leaf = self . _standard_leaf 
new_traj . _auto_run_prepend = self . _auto_run_prepend 
new_traj . _no_clobber = self . _no_clobber 
new_traj . _full_copy = self . _full_copy 
new_traj . _dynamic_imports = self . _dynamic_imports 
new_traj . _wildcard_functions = self . _wildcard_functions 
new_traj . _wildcard_keys = self . _wildcard_keys 
new_traj . _reversed_wildcards = self . _reversed_wildcards 
new_traj . _wildcard_cache = self . _wildcard_cache 
new_traj . _comment = self . _comment 
new_traj . _stored = self . _stored 
new_traj . _storage_service = self . _storage_service 
new_traj . _is_run = self . _is_run 
new_traj . _copy_from ( self , copy_leaves = copy_leaves , 
for my_dict , new_dict in ( ( self . _new_nodes , new_traj . _new_nodes ) , 
( self . _new_links , new_traj . _new_links ) ) : 
~~~ for key in my_dict : 
~~~ value = my_dict [ key ] 
parent , child = value 
if parent is self : 
~~~ new_parent = new_traj 
~~~ new_parent = new_traj . f_get ( parent . v_full_name , 
shortcuts = False , 
auto_load = False ) 
~~ new_child = new_parent . _children [ key [ 1 ] ] 
new_dict [ key ] = ( new_parent , new_child ) 
~~ ~~ return new_traj 
~~ def _copy_from ( self , node , 
copy_leaves = True , 
overwrite = False , 
def _copy_skeleton ( node_in , node_out ) : 
new_annotations = node_out . v_annotations 
node_in . _annotations = new_annotations 
node_in . v_comment = node_out . v_comment 
~~ def _add_leaf ( leaf ) : 
leaf_full_name = leaf . v_full_name 
~~~ found_leaf = self . f_get ( leaf_full_name , 
~~~ found_leaf . __setstate__ ( leaf . __getstate__ ( ) ) 
~~ return found_leaf 
~~ if copy_leaves is True or ( copy_leaves == 'explored' and 
leaf . v_is_parameter and leaf . v_explored ) : 
~~~ new_leaf = self . f_add_leaf ( cp . copy ( leaf ) ) 
~~~ new_leaf = self . f_add_leaf ( leaf ) 
~~ if new_leaf . v_is_parameter and new_leaf . v_explored : 
~~~ self . _explored_parameters [ new_leaf . v_full_name ] = new_leaf 
~~ return new_leaf 
~~ def _add_group ( group ) : 
group_full_name = group . v_full_name 
~~~ found_group = self . f_get ( group_full_name , 
~~~ _copy_skeleton ( found_group , group ) 
~~ return found_group 
~~ new_group = self . f_add_group ( group_full_name ) 
_copy_skeleton ( new_group , group ) 
return new_group 
~~ is_run = self . _is_run 
~~~ return _add_leaf ( node ) 
~~~ other_root = node . v_root 
if other_root is self : 
~~ result = _add_group ( node ) 
nodes_iterator = node . f_iter_nodes ( recursive = True , with_links = with_links ) 
has_links = [ ] 
if node . _links : 
~~~ has_links . append ( node ) 
~~ for child in nodes_iterator : 
~~~ if child . v_is_leaf : 
~~~ _add_leaf ( child ) 
~~~ _add_group ( child ) 
if child . _links : 
~~~ has_links . append ( child ) 
~~ ~~ ~~ if with_links : 
~~~ for current in has_links : 
~~~ mine = self . f_get ( current . v_full_name , with_links = False , 
shortcuts = False , auto_load = False ) 
my_link_set = set ( mine . _links . keys ( ) ) 
other_link_set = set ( current . _links . keys ( ) ) 
new_links = other_link_set - my_link_set 
for link in new_links : 
~~~ where_full_name = current . _links [ link ] . v_full_name 
mine . f_add_link ( link , where_full_name ) 
~~ ~~ ~~ return result 
~~~ self . _is_run = is_run 
~~ ~~ def f_explore ( self , build_dict ) : 
for run_idx in range ( len ( self ) ) : 
~~~ if self . f_is_completed ( run_idx ) : 
~~ ~~ added_explored_parameters = [ ] 
~~~ length = len ( self ) 
if not act_param . v_is_leaf or not act_param . v_is_parameter : 
~~ act_param . f_unlock ( ) 
act_param . _explore ( builditerable ) 
added_explored_parameters . append ( act_param ) 
full_name = act_param . v_full_name 
self . _explored_parameters [ full_name ] = act_param 
act_param . _explored = True 
if len ( self . _explored_parameters ) == 1 : 
~~ ~~ for irun in range ( length ) : 
~~~ for param in added_explored_parameters : 
param . _shrink ( ) 
param . _explored = False 
full_name = param . v_full_name 
del self . _explored_parameters [ full_name ] 
~~ if len ( self . _explored_parameters ) == 0 : 
~~~ self . f_shrink ( force = True ) 
~~ ~~ def _update_run_information ( self , run_information_dict ) : 
idx = run_information_dict [ 'idx' ] 
name = run_information_dict [ 'name' ] 
self . _run_information [ name ] = run_information_dict 
self . _updated_run_information . add ( idx ) 
~~ def _add_run_info ( self , idx , name = '' , timestamp = 42.0 , finish_timestamp = 1.337 , 
short_environment_hexsha = 'N/A' ) : 
if idx in self . _single_run_ids : 
~~~ old_name = self . _single_run_ids [ idx ] 
del self . _single_run_ids [ old_name ] 
del self . _single_run_ids [ idx ] 
del self . _run_information [ old_name ] 
~~ if name == '' : 
~~~ name = self . f_wildcard ( '$' , idx ) 
~~ self . _single_run_ids [ name ] = idx 
self . _single_run_ids [ idx ] = name 
info_dict = { 'idx' : idx , 
'time' : time , 
'parameter_summary' : parameter_summary , 
'short_environment_hexsha' : short_environment_hexsha } 
self . _run_information [ name ] = info_dict 
self . _length = len ( self . _run_information ) 
~~ def f_lock_parameters ( self ) : 
for par in self . _parameters . values ( ) : 
~~~ if not par . f_is_empty ( ) : 
~~~ par . f_lock ( ) 
~~ ~~ ~~ def f_lock_derived_parameters ( self ) : 
for par in self . _derived_parameters . values ( ) : 
~~ ~~ ~~ def _finalize ( self , store_meta_data = True ) : 
self . _is_run = False 
self . f_set_crun ( None ) 
if store_meta_data : 
~~~ self . f_store ( only_init = True ) 
~~ ~~ def f_load_skeleton ( self ) : 
self . f_load ( self . v_name , as_new = False , load_parameters = pypetconstants . LOAD_SKELETON , 
~~ def f_load ( self , name = None , index = None , as_new = False , load_parameters = pypetconstants . LOAD_DATA , 
with_meta_data = True , 
storage_service = None , ** kwargs ) : 
~~~ name = self . v_name 
~~ if as_new : 
~~~ load_parameters = pypetconstants . LOAD_DATA 
load_derived_parameters = pypetconstants . LOAD_NOTHING 
load_results = pypetconstants . LOAD_NOTHING 
load_other_data = pypetconstants . LOAD_NOTHING 
~~ unused_kwargs = set ( kwargs . keys ( ) ) 
if self . v_storage_service is None or storage_service is not None or len ( kwargs ) > 0 : 
~~~ self . _storage_service , unused_kwargs = storage_factory ( storage_service = storage_service , 
trajectory = self , ** kwargs ) 
~~ if len ( unused_kwargs ) > 0 : 
str ( unused_kwargs ) ) 
~~ if dynamic_imports is not None : 
~~~ self . f_add_to_dynamic_imports ( dynamic_imports ) 
~~ if load_data is not None : 
~~~ load_parameters = load_data 
load_derived_parameters = load_data 
load_results = load_data 
load_other_data = load_data 
~~ self . _storage_service . load ( pypetconstants . TRAJECTORY , self , trajectory_name = name , 
trajectory_index = index , 
as_new = as_new , load_parameters = load_parameters , 
load_derived_parameters = load_derived_parameters , 
load_results = load_results , 
load_other_data = load_other_data , 
with_run_information = with_run_information , 
with_meta_data = with_meta_data , 
force = force ) 
~~~ for param in self . _parameters . values ( ) : 
~~ ~~ ~~ def _check_if_both_have_same_parameters ( self , other_trajectory , 
ignore_data , consecutive_merge ) : 
if not isinstance ( other_trajectory , Trajectory ) : 
~~ if self . _stored and not consecutive_merge : 
~~~ self . f_load_skeleton ( ) 
~~ if other_trajectory . _stored : 
~~~ other_trajectory . f_load_skeleton ( ) 
~~ other_wildcard_set = set ( x [ 1 ] for x in other_trajectory . _wildcard_functions . keys ( ) ) 
wildcard_set = set ( x [ 1 ] for x in self . _wildcard_functions . keys ( ) ) 
diff = wildcard_set . symmetric_difference ( other_wildcard_set ) 
( str ( wildcard_set ) , str ( other_wildcard_set ) ) ) 
~~ if self . _stored : 
~~~ with self . _nn_interface . _disable_logging : 
~~~ self . f_load_items ( self . _parameters . keys ( ) , only_empties = True ) 
~~ ~~ if other_trajectory . _stored : 
~~~ other_trajectory . f_load_items ( other_trajectory . _parameters . keys ( ) , 
only_empties = True ) 
~~ ~~ self . f_restore_default ( ) 
other_trajectory . f_restore_default ( ) 
allmyparams = self . _parameters . copy ( ) 
allotherparams = other_trajectory . _parameters . copy ( ) 
if 'derived_parameters' in self : 
~~~ my_traj_dpars = self . _derived_parameters 
if self . _stored : 
~~~ self . f_load_items ( my_traj_dpars . keys ( ) , only_empties = True ) 
~~ ~~ allmyparams . update ( my_traj_dpars ) 
other_traj_dpars = other_trajectory . _derived_parameters 
if other_trajectory . _stored : 
~~~ other_trajectory . f_load_items ( other_traj_dpars . keys ( ) , only_empties = True ) 
~~ ~~ allotherparams . update ( other_traj_dpars ) 
~~ my_keyset = set ( allmyparams . keys ( ) ) 
other_keyset = set ( allotherparams . keys ( ) ) 
diff = my_keyset . symmetric_difference ( other_keyset ) - ignore_data 
run_dummys = ( self . f_wildcard ( '$' , - 1 ) , other_trajectory . f_wildcard ( '$' , - 1 ) ) 
~~~ run_difference_can_be_resolved = True 
for full_name in diff : 
~~~ split_name = full_name . split ( '.' ) 
if not any ( x in self . _run_information or 
x in other_trajectory . _run_information or 
x in run_dummys 
for x in split_name ) : 
~~~ run_difference_can_be_resolved = False 
~~ elif full_name in allotherparams : 
~~~ del allotherparams [ full_name ] 
~~ ~~ if not run_difference_can_be_resolved : 
~~ ~~ for key , other_param in allotherparams . items ( ) : 
~~~ if key in ignore_data : 
~~ my_param = self . f_get ( key ) 
if any ( x in self . _run_information or 
x in other_trajectory . _run_information 
for x in split_key ) : 
~~~ if not my_param . _values_of_same_type ( my_param . f_get ( ) , other_param . f_get ( ) ) : 
( key , str ( type ( my_param . f_get ( ) ) ) , 
str ( type ( other_param . f_get ( ) ) ) ) ) 
~~ ~~ ~~ ~~ def f_backup ( self , ** kwargs ) : 
self . _storage_service . store ( pypetconstants . BACKUP , self , trajectory_name = self . v_name , 
~~ def _make_reversed_wildcards ( self , old_length = - 1 ) : 
if len ( self . _reversed_wildcards ) > 0 : 
~~~ start = old_length 
~~~ start = - 1 
~~ for wildcards , func in self . _wildcard_functions . items ( ) : 
~~~ for irun in range ( start , len ( self ) ) : 
~~~ translated_name = func ( irun ) 
if not translated_name in self . _reversed_wildcards : 
~~~ self . _reversed_wildcards [ translated_name ] = ( [ ] , wildcards ) 
~~ self . _reversed_wildcards [ translated_name ] [ 0 ] . append ( irun ) 
~~ ~~ ~~ def f_merge_many ( self , other_trajectories , 
ignore_data = ( ) , 
move_data = False , 
delete_other_trajectory = False , 
keep_info = True , 
keep_other_trajectory_info = True , 
merge_config = True , 
backup = True ) : 
other_length = len ( other_trajectories ) 
self . f_load_skeleton ( ) 
if backup : 
~~~ self . f_backup ( ) 
~~ for idx , other in enumerate ( other_trajectories ) : 
~~~ self . f_merge ( other , ignore_data = ignore_data , 
move_data = move_data , 
delete_other_trajectory = delete_other_trajectory , 
keep_info = keep_info , 
keep_other_trajectory_info = keep_other_trajectory_info , 
merge_config = merge_config , 
backup = False , 
consecutive_merge = True ) 
self . _reversed_wildcards = { } 
self . f_store ( ) 
~~ def f_merge ( self , other_trajectory , trial_parameter = None , remove_duplicates = False , 
backup = True , 
consecutive_merge = False , 
slow_merge = False ) : 
if consecutive_merge and trial_parameter is not None : 
~~ if consecutive_merge and backup : 
~~ timestamp = time . time ( ) 
original_ignore_data = set ( ignore_data ) 
ignore_data = original_ignore_data . copy ( ) 
old_len = len ( self ) 
self . _check_if_both_have_same_parameters ( other_trajectory , ignore_data , consecutive_merge ) 
self . _make_reversed_wildcards ( old_length = old_len ) 
other_trajectory . _make_reversed_wildcards ( ) 
~~~ other_trajectory . f_backup ( ) 
self . f_backup ( ) 
used_runs , changed_parameters = self . _merge_parameters ( 
other_trajectory , 
remove_duplicates , 
trial_parameter , 
ignore_data ) 
ignore_data . update ( set ( changed_parameters ) ) 
if len ( used_runs ) == 0 : 
~~ rename_dict = { } 
allowed_translations = set ( [ translation for translation , pair in 
other_trajectory . _reversed_wildcards . items ( ) if 
any ( x in used_runs for x in pair [ 0 ] ) ] ) 
self . _merge_single_runs ( other_trajectory , used_runs ) 
self . _merge_derived_parameters ( other_trajectory = other_trajectory , 
used_runs = used_runs , 
rename_dict = rename_dict , 
allowed_translations = allowed_translations , 
ignore_data = ignore_data ) 
self . _merge_results ( other_trajectory = other_trajectory , 
self . _storage_service . store ( pypetconstants . PREPARE_MERGE , self , 
trajectory_name = self . v_name , 
changed_parameters = changed_parameters , 
old_length = old_len ) 
if not slow_merge : 
~~~ other_filename = other_trajectory . v_storage_service . filename 
other_filename = None 
other_trajectory_name = other_trajectory . v_name , 
rename_dict = rename_dict , move_nodes = move_data , 
delete_trajectory = delete_other_trajectory , 
other_filename = other_filename ) 
~~ except pex . NoSuchServiceError as exc : 
slow_merge = True 
~~ except ValueError as exc : 
~~ ~~ if slow_merge : 
~~~ self . _merge_slowly ( other_trajectory , rename_dict ) 
~~ if merge_config : 
~~~ self . _merge_config ( other_trajectory ) 
~~ self . _merge_links ( other_trajectory = other_trajectory , 
ignore_data = original_ignore_data ) 
formatted_time = datetime . datetime . fromtimestamp ( timestamp ) . strftime ( '%Y_%m_%d_%Hh%Mm%Ss' ) 
hexsha = hashlib . sha1 ( ( self . v_name + 
str ( self . v_timestamp ) + 
other_trajectory . v_name + 
str ( other_trajectory . v_timestamp ) + 
VERSION ) . encode ( 'utf-8' ) ) . hexdigest ( ) 
short_hexsha = hexsha [ 0 : 7 ] 
if keep_info : 
~~~ merge_name = 'merge_%s_%s' % ( short_hexsha , formatted_time ) 
config_name = 'merge.%s.merged_runs' % merge_name 
self . f_add_config ( config_name , len ( used_runs ) , 
config_name = 'merge.%s.timestamp' % merge_name 
self . f_add_config ( config_name , timestamp , 
config_name = 'merge.%s.hexsha' % merge_name 
self . f_add_config ( config_name , hexsha , 
config_name = 'merge.%s.remove_duplicates' % merge_name 
self . f_add_config ( config_name , remove_duplicates , 
if original_ignore_data : 
~~~ config_name = 'merge.%s.ignore_data' % merge_name 
self . f_add_config ( config_name , tuple ( original_ignore_data ) , 
~~ config_name = 'merge.%s.length_before_merge' % merge_name 
self . f_add_config ( config_name , len ( self ) , 
if self . v_version != VERSION : 
~~~ config_name = 'merge.%s.version' % merge_name 
self . f_add_config ( config_name , self . v_version , 
~~ if trial_parameter is not None : 
~~~ config_name = 'merge.%s.trial_parameter' % merge_name 
self . f_add_config ( config_name , len ( other_trajectory ) , 
~~ if keep_other_trajectory_info : 
~~~ if other_trajectory . v_version != self . v_version : 
~~~ config_name = 'merge.%s.other_trajectory.version' % merge_name 
self . f_add_config ( config_name , other_trajectory . v_version , 
~~ config_name = 'merge.%s.other_trajectory.name' % merge_name 
self . f_add_config ( config_name , other_trajectory . v_name , 
config_name = 'merge.%s.other_trajectory.timestamp' % merge_name 
self . f_add_config ( config_name , other_trajectory . v_timestamp , 
config_name = 'merge.%s.other_trajectory.length' % merge_name 
if other_trajectory . v_comment : 
~~~ config_name = 'merge.%s.other_trajectory.comment' % merge_name 
self . f_add_config ( config_name , other_trajectory . v_comment , 
~~ ~~ ~~ if not consecutive_merge : 
self . f_store ( store_data = pypetconstants . STORE_DATA ) 
~~ other_trajectory . _reversed_wildcards = { } 
~~ def _merge_single_runs ( self , other_trajectory , used_runs ) : 
run_indices = range ( len ( other_trajectory ) ) 
run_name_dict = OrderedDict ( ) 
to_store_groups_with_annotations = [ ] 
for idx in run_indices : 
~~~ if idx in used_runs : 
~~~ other_info_dict = other_trajectory . f_get_run_information ( idx ) 
time_ = other_info_dict [ 'time' ] 
timestamp = other_info_dict [ 'timestamp' ] 
completed = other_info_dict [ 'completed' ] 
short_environment_hexsha = other_info_dict [ 'short_environment_hexsha' ] 
finish_timestamp = other_info_dict [ 'finish_timestamp' ] 
runtime = other_info_dict [ 'runtime' ] 
new_idx = used_runs [ idx ] 
new_runname = self . f_wildcard ( '$' , new_idx ) 
run_name_dict [ idx ] = new_runname 
info_dict = dict ( 
idx = new_idx , 
time = time_ , 
timestamp = timestamp , 
completed = completed , 
short_environment_hexsha = short_environment_hexsha , 
finish_timestamp = finish_timestamp , 
runtime = runtime ) 
self . _add_run_info ( ** info_dict ) 
~~ ~~ ~~ def _rename_full_name ( self , full_name , other_trajectory , used_runs = None , new_run_idx = None ) : 
split_name = full_name . split ( '.' ) 
for idx , name in enumerate ( split_name ) : 
~~~ if name in other_trajectory . _reversed_wildcards : 
~~~ run_indices , wildcards = other_trajectory . _reversed_wildcards [ name ] 
if new_run_idx is None : 
~~~ run_idx = None 
for run_jdx in run_indices : 
~~~ if run_jdx in used_runs : 
~~~ run_idx = used_runs [ run_jdx ] 
~~ elif run_jdx == - 1 : 
~~~ run_idx = - 1 
~~ ~~ if run_idx is None : 
~~~ run_idx = new_run_idx 
~~ new_name = self . f_wildcard ( wildcards [ 0 ] , run_idx ) 
split_name [ idx ] = new_name 
~~ ~~ full_name = '.' . join ( split_name ) 
return full_name 
~~ def _merge_derived_parameters ( self , 
used_runs , 
rename_dict , 
allowed_translations , 
ignore_data ) : 
other_derived_parameters = other_trajectory . _derived_parameters . copy ( ) 
new_first_run_idx = min ( used_runs . values ( ) ) 
run_name_dummy = other_trajectory . f_wildcard ( '$' , - 1 ) 
for param_name in other_derived_parameters : 
~~~ if param_name in ignore_data : 
~~ split_name = param_name . split ( '.' ) 
if not any ( x in run_name_dummy for x in split_name ) : 
~~ ignore_data . add ( param_name ) 
param = other_derived_parameters [ param_name ] 
new_param_name = self . _rename_full_name ( param_name , other_trajectory , 
used_runs = used_runs ) 
if new_param_name in self : 
~~~ my_param = self . f_get ( new_param_name , fast_access = False ) 
if ( my_param . _equal_values ( my_param . f_get ( ) , param . f_get ( ) ) and 
not ( my_param . f_has_range ( ) or param . f_has_range ( ) ) ) : 
~~ ~~ first_new_param_name = self . _rename_full_name ( param_name , 
new_run_idx = new_first_run_idx ) 
rename_dict [ param_name ] = first_new_param_name 
comment = param . v_comment 
param_type = param . f_get_class_name ( ) 
param_type = self . _create_class ( param_type ) 
first_param = self . f_add_leaf ( param_type , 
first_new_param_name , 
comment = comment ) 
for run_idx in used_runs . values ( ) : 
~~~ if run_idx == new_first_run_idx : 
~~ next_name = self . _rename_full_name ( param_name , other_trajectory , 
new_run_idx = run_idx ) 
split_name = next_name . split ( '.' ) 
link_name = split_name . pop ( ) 
location_name = '.' . join ( split_name ) 
if not self . f_contains ( location_name , shortcuts = False ) : 
~~~ the_group = self . f_add_group ( location_name ) 
~~~ the_group = self . f_get ( location_name ) 
~~ the_group . f_add_link ( link_name , first_param ) 
~~ ~~ for param_name in other_derived_parameters : 
ignore_data . add ( param_name ) 
if any ( x in other_trajectory . _reversed_wildcards and x not in allowed_translations 
~~ new_name = self . _rename_full_name ( param_name , other_trajectory , 
if self . f_contains ( new_name ) : 
~~~ my_param = self . f_get ( new_name , fast_access = False ) 
~~ ~~ rename_dict [ param_name ] = new_name 
~~ ~~ def _merge_links ( self , other_trajectory , used_runs , allowed_translations , ignore_data ) : 
linked_items = other_trajectory . _linked_by 
run_name_dummys = set ( [ f ( - 1 ) for f in other_trajectory . _wildcard_functions . values ( ) ] ) 
if len ( linked_items ) > 0 : 
for old_linked_name in other_trajectory . _linked_by : 
~~~ if old_linked_name in ignore_data : 
~~ split_name = old_linked_name . split ( '.' ) 
if any ( x in run_name_dummys for x in split_name ) : 
( old_linked_name , str ( run_name_dummys ) ) ) 
~~ old_link_dict = other_trajectory . _linked_by [ old_linked_name ] 
split_name = old_linked_name . split ( '.' ) 
if all ( x in allowed_translations for x in split_name ) : 
~~~ new_linked_full_name = self . _rename_full_name ( old_linked_name , 
~~~ new_linked_full_name = old_linked_name 
~~ for linking_node , link_set in old_link_dict . values ( ) : 
~~~ linking_full_name = linking_node . v_full_name 
split_name = linking_full_name . split ( '.' ) 
( linking_full_name , str ( run_name_dummys ) ) ) 
~~ split_name = linking_full_name . split ( '.' ) 
if any ( x in allowed_translations for x in split_name ) : 
~~~ new_linking_full_name = self . _rename_full_name ( linking_full_name , 
~~~ new_linking_full_name = linking_full_name 
~~ for link in link_set : 
~~~ if ( linking_full_name + '.' + link ) in ignore_data : 
~~ if link in run_name_dummys : 
( link , 
linking_full_name , 
str ( run_name_dummys ) ) ) 
~~~ new_linked_item = self . f_get ( new_linked_full_name , 
shortcuts = False ) 
if self . f_contains ( new_linking_full_name ) : 
~~~ new_linking_item = self . f_get ( new_linking_full_name , 
~~~ new_linking_item = self . f_add_group ( new_linking_full_name ) 
~~ if link in allowed_translations : 
~~~ run_indices , wildcards = other_trajectory . _reversed_wildcards [ link ] 
link = self . f_wildcard ( wildcards [ 0 ] , used_runs [ run_indices [ 0 ] ] ) 
~~ if not link in new_linking_item . _links : 
~~~ new_linking_item . f_add_link ( link , new_linked_item ) 
( link , new_linked_item . v_full_name ) ) 
~~ ~~ except ( AttributeError , ValueError ) as exc : 
( link , linking_full_name , old_linked_name , 
repr ( exc ) ) ) 
~~ ~~ ~~ ~~ ~~ ~~ def _merge_config ( self , other_trajectory ) : 
if 'config.git' in other_trajectory : 
git_node = other_trajectory . f_get ( 'config.git' ) 
param_list = [ ] 
for param in git_node . f_iter_leaves ( with_links = False ) : 
~~~ if not self . f_contains ( param . v_full_name , shortcuts = False ) : 
~~~ param_list . append ( self . f_add_config ( param ) ) 
~~ ~~ if param_list : 
~~~ self . f_store_items ( param_list ) 
~~ if 'config.environment' in other_trajectory : 
env_node = other_trajectory . f_get ( 'config.environment' ) 
for param in env_node . f_iter_leaves ( with_links = False ) : 
~~ if 'config.merge' in other_trajectory : 
merge_node = other_trajectory . f_get ( 'config.merge' ) 
for param in merge_node . f_iter_leaves ( with_links = False ) : 
~~ ~~ def _merge_slowly ( self , other_trajectory , rename_dict ) : 
for other_key in rename_dict : 
~~~ new_key = rename_dict [ other_key ] 
other_instance = other_trajectory . f_get ( other_key ) 
if other_instance . f_is_empty ( ) : 
~~~ other_trajectory . f_load_item ( other_instance ) 
~~ ~~ if not self . f_contains ( new_key ) : 
~~~ class_name = other_instance . f_get_class_name ( ) 
class_ = self . _create_class ( class_name ) 
my_instance = self . f_add_leaf ( class_ , new_key ) 
~~~ my_instance = self . f_get ( new_key , shortcuts = False ) 
~~ if not my_instance . f_is_empty ( ) : 
~~ load_dict = other_instance . _store ( ) 
my_instance . _load ( load_dict ) 
my_instance . f_set_annotations ( ** other_instance . v_annotations . f_to_dict ( copy = False ) ) 
my_instance . v_comment = other_instance . v_comment 
self . f_store_item ( my_instance ) 
if other_instance . v_is_parameter : 
~~~ other_instance . f_unlock ( ) 
my_instance . f_unlock ( ) 
~~ other_instance . f_empty ( ) 
my_instance . f_empty ( ) 
~~ ~~ def _merge_results ( self , other_trajectory , rename_dict , used_runs , allowed_translations , 
other_results = other_trajectory . _results . copy ( ) 
for result_name in other_results : 
~~~ if result_name in ignore_data : 
~~ split_name = result_name . split ( '.' ) 
ignore_data . add ( result_name ) 
~~ new_name = self . _rename_full_name ( result_name , other_trajectory , 
~~ rename_dict [ result_name ] = new_name 
~~ ~~ def _merge_parameters ( self , other_trajectory , remove_duplicates = False , 
trial_parameter_name = None , 
ignore_data = ( ) ) : 
if trial_parameter_name : 
~~~ if remove_duplicates : 
remove_duplicates = False 
~~ ~~ params_to_change = { } 
~~~ my_trial_parameter = self . f_get ( trial_parameter_name ) 
other_trial_parameter = other_trajectory . f_get ( trial_parameter_name ) 
if not isinstance ( my_trial_parameter , BaseParameter ) : 
~~ if my_trial_parameter . f_has_range ( ) : 
~~~ my_trial_list = my_trial_parameter . f_get_range ( copy = False ) 
~~~ my_trial_list = [ my_trial_parameter . f_get ( ) ] 
~~ if other_trial_parameter . f_has_range ( ) : 
~~~ other_trial_list = other_trial_parameter . f_get_range ( copy = False ) 
~~~ other_trial_list = [ other_trial_parameter . f_get ( ) ] 
~~ mytrialset = set ( my_trial_list ) 
if mytrialset != set ( range ( mymaxtrial_T1 + 1 ) ) : 
~~ othertrialset = set ( other_trial_list ) 
if othertrialset != set ( range ( othermaxtrial_T2 + 1 ) ) : 
( othermaxtrial_T2 , str ( othertrialset ) ) ) 
~~ trial_parameter_name = my_trial_parameter . v_full_name 
if not trial_parameter_name in self . _explored_parameters : 
~~~ self . _explored_parameters [ trial_parameter_name ] = my_trial_parameter 
~~ params_to_change [ trial_parameter_name ] = ( my_trial_parameter , other_trial_parameter ) 
~~ params_to_merge = other_trajectory . _parameters . copy ( ) 
params_to_merge . update ( other_trajectory . _derived_parameters ) 
for ignore in ignore_data : 
~~~ if ignore in params_to_merge : 
~~~ del params_to_merge [ ignore ] 
~~ ~~ run_name_dummys = set ( [ f ( - 1 ) for f in other_trajectory . _wildcard_functions . values ( ) ] ) 
for key in params_to_merge : 
~~~ other_param = params_to_merge [ key ] 
if any ( x in other_trajectory . _reversed_wildcards for x in split_key ) : 
if not my_param . _values_of_same_type ( my_param . f_get ( ) , other_param . f_get ( ) ) : 
~~ if my_param . v_full_name == trial_parameter_name : 
~~ if ( my_param . f_has_range ( ) or 
other_param . f_has_range ( ) or 
not my_param . _equal_values ( my_param . f_get ( ) , other_param . f_get ( ) ) ) : 
~~~ params_to_change [ key ] = ( my_param , other_param ) 
if not my_param . f_has_range ( ) and not other_param . f_has_range ( ) : 
~~~ remove_duplicates = False 
~~ ~~ ~~ used_runs = { } 
for idx in range ( len ( other_trajectory ) ) : 
~~~ used_runs [ idx ] = idx 
~~ if remove_duplicates : 
~~~ for irun in range ( len ( other_trajectory ) ) : 
~~~ for jrun in range ( len ( self ) ) : 
~~~ change = True 
for my_param , other_param in params_to_change . values ( ) : 
~~~ if other_param . f_has_range ( ) : 
~~~ other_param . _set_parameter_access ( irun ) 
~~ if my_param . f_has_range ( ) : 
~~~ my_param . _set_parameter_access ( jrun ) 
~~ val1 = my_param . f_get ( ) 
val2 = other_param . f_get ( ) 
if not my_param . _equal_values ( val1 , val2 ) : 
~~~ change = False 
~~ ~~ if change : 
~~~ del used_runs [ irun ] 
~~ ~~ ~~ for my_param , other_param in params_to_change . values ( ) : 
~~~ other_param . _restore_default ( ) 
my_param . _restore_default ( ) 
~~ ~~ adding_length = len ( used_runs ) 
starting_length = len ( self ) 
if adding_length == 0 : 
~~~ return used_runs , [ ] 
~~ count = 0 
for key in sorted ( used_runs . keys ( ) ) : 
~~~ used_runs [ key ] = starting_length + count 
~~ for my_param , other_param in params_to_change . values ( ) : 
~~~ fullname = my_param . v_full_name 
if fullname == trial_parameter_name : 
~~~ other_range = [ x + mymaxtrial_T1 + 1 for x in other_trial_list ] 
~~~ other_range = ( x for jdx , x in enumerate ( other_param . f_get_range ( copy = False ) ) 
if jdx in used_runs ) 
~~~ other_range = ( other_param . f_get ( ) for _ in range ( adding_length ) ) 
~~ ~~ if not my_param . f_has_range ( ) : 
~~~ my_param . f_unlock ( ) 
my_param . _explore ( ( my_param . f_get ( ) for _ in range ( len ( self ) ) ) ) 
~~ my_param . f_unlock ( ) 
my_param . _expand ( other_range ) 
if not fullname in self . _explored_parameters : 
~~~ self . _explored_parameters [ fullname ] = my_param 
~~ ~~ return used_runs , list ( params_to_change . keys ( ) ) 
~~ def f_migrate ( self , new_name = None , in_store = False , 
new_storage_service = None , ** kwargs ) : 
~~~ self . _name = new_name 
if new_storage_service is not None or len ( kwargs ) > 0 : 
~~~ self . _storage_service , unused_kwargs = storage_factory ( 
storage_service = new_storage_service , 
~~ self . _stored = in_store 
~~ def f_store ( self , only_init = False , store_data = pypetconstants . STORE_DATA , 
if self . _is_run : 
~~~ if self . _new_nodes or self . _new_links : 
~~~ self . _storage_service . store ( pypetconstants . SINGLE_RUN , self , 
recursive = not only_init , 
~~~ self . _storage_service . store ( pypetconstants . TRAJECTORY , self , 
only_init = only_init , 
self . _stored = True 
~~ ~~ def f_is_empty ( self ) : 
return ( len ( self . _parameters ) == 0 and 
len ( self . _derived_parameters ) == 0 and 
len ( self . _results ) == 0 and 
len ( self . _other_leaves ) == 0 ) 
~~ def f_restore_default ( self ) : 
self . _idx = - 1 
self . _crun = None 
~~~ param . _restore_default ( ) 
~~ ~~ ~~ def _set_explored_parameters_to_idx ( self , idx ) : 
~~~ param . _set_parameter_access ( idx ) 
~~ ~~ ~~ def _make_single_run ( self ) : 
self . _new_nodes = OrderedDict ( ) 
self . _new_links = OrderedDict ( ) 
self . _is_run = True 
~~ def f_get_run_names ( self , sort = True ) : 
if sort : 
~~~ return [ self . f_idx_to_run ( idx ) for idx in range ( len ( self ) ) ] 
~~~ return list ( self . _run_information . keys ( ) ) 
~~ ~~ def f_get_run_information ( self , name_or_idx = None , copy = True ) : 
if name_or_idx is None : 
~~~ return cp . deepcopy ( self . _run_information ) 
~~~ return self . _run_information 
~~~ return self . _run_information [ name_or_idx ] . copy ( ) 
~~~ return self . _run_information [ name_or_idx ] 
~~~ name_or_idx = self . f_idx_to_run ( name_or_idx ) 
~~ ~~ ~~ def f_find_idx ( self , name_list , predicate ) : 
if self . _is_run and not self . v_full_copy : 
~~ if isinstance ( name_list , str ) : 
~~~ name_list = [ name_list ] 
~~ iter_list = [ ] 
for name in name_list : 
~~~ param = self . f_get ( name ) 
if not param . v_is_parameter : 
( name , str ( type ( param ) ) ) ) 
~~ if param . f_has_range ( ) : 
~~~ iter_list . append ( iter ( param . f_get_range ( copy = False ) ) ) 
~~~ iter_list . append ( itools . repeat ( param . f_get ( ) , len ( self ) ) ) 
~~ ~~ logic_iter = map ( predicate , * iter_list ) 
for idx , item in enumerate ( logic_iter ) : 
~~~ if item : 
~~~ yield idx 
~~ ~~ ~~ def f_start_run ( self , run_name_or_idx = None , turn_into_run = True ) : 
if self . _run_started : 
~~~ return self 
~~ if run_name_or_idx is None : 
~~~ if self . v_idx == - 1 : 
~~~ self . f_set_crun ( run_name_or_idx ) 
~~ self . _run_started = True 
if turn_into_run : 
~~~ self . _make_single_run ( ) 
~~ self . _set_start ( ) 
~~ def f_finalize_run ( self , store_meta_data = True , clean_up = True ) : 
if not self . _run_started : 
~~ self . _set_finish ( ) 
if clean_up and self . _is_run : 
~~~ self . _finalize_run ( ) 
~~ self . _is_run = False 
self . _run_started = False 
self . _updated_run_information . add ( self . v_idx ) 
~~ return self 
~~ def _set_start ( self ) : 
init_time = time . time ( ) 
formatted_time = datetime . datetime . fromtimestamp ( init_time ) . strftime ( '%Y_%m_%d_%Hh%Mm%Ss' ) 
run_info_dict = self . _run_information [ self . v_crun ] 
run_info_dict [ 'timestamp' ] = init_time 
run_info_dict [ 'time' ] = formatted_time 
if self . _environment_hexsha is not None : 
~~~ run_info_dict [ 'short_environment_hexsha' ] = self . _environment_hexsha [ 0 : 7 ] 
~~ ~~ def _summarize_explored_parameters ( self ) : 
for idx , expparam in enumerate ( self . _explored_parameters . values ( ) ) : 
~~ def _set_finish ( self ) : 
timestamp_run = run_info_dict [ 'timestamp' ] 
run_summary = self . _summarize_explored_parameters ( ) 
finish_timestamp_run = time . time ( ) 
findatetime = datetime . datetime . fromtimestamp ( finish_timestamp_run ) 
startdatetime = datetime . datetime . fromtimestamp ( timestamp_run ) 
runtime_run = str ( findatetime - startdatetime ) 
run_info_dict [ 'parameter_summary' ] = run_summary 
run_info_dict [ 'completed' ] = 1 
run_info_dict [ 'finish_timestamp' ] = finish_timestamp_run 
run_info_dict [ 'runtime' ] = runtime_run 
~~ def _construct_instance ( self , constructor , full_name , * args , ** kwargs ) : 
if getattr ( constructor , 'KNOWS_TRAJECTORY' , False ) : 
~~~ return constructor ( full_name , self , * args , ** kwargs ) 
~~~ return constructor ( full_name , * args , ** kwargs ) 
~~ ~~ def _return_item_dictionary ( param_dict , fast_access , copy ) : 
if not copy and fast_access : 
~~ if not fast_access : 
~~~ return param_dict . copy ( ) 
~~~ return param_dict 
~~~ resdict = { } 
for key in param_dict : 
~~~ param = param_dict [ key ] 
val = param . f_get ( ) 
resdict [ key ] = val 
~~ return resdict 
~~ ~~ def _finalize_run ( self ) : 
self . _run_information [ self . v_crun ] [ 'completed' ] = 1 
while len ( self . _new_links ) : 
~~~ name_pair , child_parent_pair = self . _new_links . popitem ( last = False ) 
parent_node , _ = child_parent_pair 
_ , link = name_pair 
parent_node . f_remove_child ( link ) 
~~ while len ( self . _new_nodes ) : 
~~~ _ , child_parent_pair = self . _new_nodes . popitem ( last = False ) 
parent , child = child_parent_pair 
child_name = child . v_name 
parent . f_remove_child ( child_name , recursive = True ) 
~~ ~~ def f_to_dict ( self , fast_access = False , short_names = False , nested = False , 
return self . _nn_interface . _to_dict ( self , fast_access = fast_access , 
short_names = short_names , 
nested = nested , 
copy = copy , with_links = with_links ) 
~~ def f_get_config ( self , fast_access = False , copy = True ) : 
return self . _return_item_dictionary ( self . _config , fast_access , copy ) 
~~ def f_get_parameters ( self , fast_access = False , copy = True ) : 
return self . _return_item_dictionary ( self . _parameters , fast_access , copy ) 
~~ def f_get_explored_parameters ( self , fast_access = False , copy = True ) : 
return self . _return_item_dictionary ( self . _explored_parameters , fast_access , copy ) 
~~ def f_get_derived_parameters ( self , fast_access = False , copy = True ) : 
return self . _return_item_dictionary ( self . _derived_parameters , fast_access , copy ) 
~~ def f_get_results ( self , fast_access = False , copy = True ) : 
return self . _return_item_dictionary ( self . _results , fast_access , copy ) 
~~ def f_store_item ( self , item , * args , ** kwargs ) : 
self . f_store_items ( [ item ] , * args , ** kwargs ) 
~~ def f_store_items ( self , iterator , * args , ** kwargs ) : 
if not self . _stored : 
~~ fetched_items = self . _nn_interface . _fetch_items ( STORE , iterator , args , kwargs ) 
if fetched_items : 
~~~ self . _storage_service . store ( pypetconstants . LIST , fetched_items , 
trajectory_name = self . v_name ) 
~~ ~~ def f_load_item ( self , item , * args , ** kwargs ) : 
self . f_load_items ( [ item ] , * args , ** kwargs ) 
~~ def f_load_items ( self , iterator , * args , ** kwargs ) : 
~~~ raise TypeError ( 
~~ fetched_items = self . _nn_interface . _fetch_items ( LOAD , iterator , args , kwargs ) 
~~~ self . _storage_service . load ( pypetconstants . LIST , fetched_items , 
~~ ~~ def f_remove_item ( self , item , recursive = False ) : 
self . f_remove_items ( [ item ] , recursive = recursive ) 
~~ def f_remove_items ( self , iterator , recursive = False ) : 
fetched_items = self . _nn_interface . _fetch_items ( REMOVE , iterator , ( ) , { } ) 
~~~ for _ , item , dummy1 , dummy2 in fetched_items : 
~~~ self . _nn_interface . _remove_node_or_leaf ( item , recursive = recursive ) 
~~ ~~ def f_delete_links ( self , iterator_of_links , remove_from_trajectory = False ) : 
to_delete_links = [ ] 
group_link_pairs = [ ] 
for elem in iterator_of_links : 
~~~ if isinstance ( elem , str ) : 
~~~ split_names = elem . split ( '.' ) 
parent_name = '.' . join ( split_names [ : - 1 ] ) 
link = split_names [ - 1 ] 
parent_node = self . f_get ( parent_name ) if parent_name != '' else self 
link_name = parent_node . v_full_name + '.' + link if parent_name != '' else link 
to_delete_links . append ( ( pypetconstants . DELETE_LINK , link_name ) ) 
group_link_pairs . append ( ( parent_node , link ) ) 
~~~ link_name = elem [ 0 ] . v_full_name + '.' + elem [ 1 ] 
group_link_pairs . append ( elem ) 
~~~ self . _storage_service . store ( pypetconstants . LIST , to_delete_links , 
~~ if remove_from_trajectory : 
~~~ for group , link in group_link_pairs : 
~~~ group . f_remove_link ( link ) 
~~ ~~ ~~ def f_remove ( self , recursive = True , predicate = None ) : 
if not recursive : 
~~ for child in list ( self . _children . keys ( ) ) : 
~~~ self . f_remove_child ( child , recursive = True , predicate = predicate ) 
~~ ~~ def f_delete_item ( self , item , * args , ** kwargs ) : 
self . f_delete_items ( [ item ] , * args , ** kwargs ) 
~~ def f_delete_items ( self , iterator , * args , ** kwargs ) : 
remove_from_trajectory = kwargs . pop ( 'remove_from_trajectory' , False ) 
recursive = kwargs . get ( 'recursive' , False ) 
fetched_items = self . _nn_interface . _fetch_items ( REMOVE , iterator , args , kwargs ) 
~~ for _ , item , dummy1 , dummy2 in fetched_items : 
~~~ if remove_from_trajectory : 
~~~ item . _stored = False 
~~ ~~ def _pool_single_run ( kwargs ) : 
wrap_mode = kwargs [ 'wrap_mode' ] 
traj = kwargs [ 'traj' ] 
traj . v_storage_service = _pool_single_run . storage_service 
if wrap_mode == pypetconstants . WRAP_MODE_LOCAL : 
~~~ traj . v_storage_service . free_references ( ) 
~~ return _sigint_handling_single_run ( kwargs ) 
~~ def _frozen_pool_single_run ( kwargs ) : 
idx = kwargs . pop ( 'idx' ) 
frozen_kwargs = _frozen_pool_single_run . kwargs 
traj = frozen_kwargs [ 'traj' ] 
traj . f_set_crun ( idx ) 
return _sigint_handling_single_run ( frozen_kwargs ) 
~~ def _configure_pool ( kwargs ) : 
_pool_single_run . storage_service = kwargs [ 'storage_service' ] 
_configure_niceness ( kwargs ) 
_configure_logging ( kwargs , extract = False ) 
~~ def _configure_frozen_pool ( kwargs ) : 
_frozen_pool_single_run . kwargs = kwargs 
traj . v_full_copy = kwargs [ 'full_copy' ] 
~~ def _process_single_run ( kwargs ) : 
_configure_logging ( kwargs ) 
result_queue = kwargs [ 'result_queue' ] 
result = _sigint_handling_single_run ( kwargs ) 
result_queue . put ( result ) 
result_queue . close ( ) 
~~ def _configure_frozen_scoop ( kwargs ) : 
def _delete_old_scoop_rev_data ( old_scoop_rev ) : 
~~~ if old_scoop_rev is not None : 
~~~ elements = shared . elements 
for key in elements : 
~~~ var_dict = elements [ key ] 
if old_scoop_rev in var_dict : 
~~~ del var_dict [ old_scoop_rev ] 
~~ ~~ ~~ scoop_rev = kwargs . pop ( 'scoop_rev' ) 
~~~ old_scoop_rev = _frozen_scoop_single_run . kwargs [ 'scoop_rev' ] 
configured = old_scoop_rev == scoop_rev 
~~ except ( AttributeError , KeyError ) : 
~~~ old_scoop_rev = None 
configured = False 
~~ if not configured : 
~~~ _frozen_scoop_single_run . kwargs = shared . getConst ( scoop_rev , timeout = 424.2 ) 
frozen_kwargs = _frozen_scoop_single_run . kwargs 
frozen_kwargs [ 'scoop_rev' ] = scoop_rev 
frozen_kwargs [ 'traj' ] . v_full_copy = frozen_kwargs [ 'full_copy' ] 
if not scoop . IS_ORIGIN : 
~~~ _configure_niceness ( frozen_kwargs ) 
_configure_logging ( frozen_kwargs , extract = False ) 
~~ _delete_old_scoop_rev_data ( old_scoop_rev ) 
~~ ~~ def _scoop_single_run ( kwargs ) : 
~~~ is_origin = scoop . IS_ORIGIN 
~~~ is_origin = True 
~~ if not is_origin : 
~~~ _configure_niceness ( kwargs ) 
~~ return _single_run ( kwargs ) 
~~ ~~ def _configure_logging ( kwargs , extract = True ) : 
~~~ logging_manager = kwargs [ 'logging_manager' ] 
if extract : 
~~~ logging_manager . extract_replacements ( kwargs [ 'traj' ] ) 
~~ logging_manager . make_logging_handlers_and_tools ( multiproc = True ) 
traceback . print_exc ( ) 
~~ ~~ def _configure_niceness ( kwargs ) : 
niceness = kwargs [ 'niceness' ] 
if niceness is not None : 
~~~ current = os . nice ( 0 ) 
if niceness - current > 0 : 
~~~ os . nice ( niceness - current ) 
~~ ~~ except AttributeError : 
~~~ psutil . Process ( ) . nice ( niceness ) 
~~ ~~ ~~ def _sigint_handling_single_run ( kwargs ) : 
~~~ graceful_exit = kwargs [ 'graceful_exit' ] 
if graceful_exit : 
~~~ sigint_handling . start ( ) 
if sigint_handling . hit : 
~~~ result = ( sigint_handling . SIGINT , None ) 
~~~ result = _single_run ( kwargs ) 
~~~ result = ( sigint_handling . SIGINT , result ) 
~~ ~~ return result 
~~~ pypet_root_logger = logging . getLogger ( 'pypet' ) 
~~ ~~ def _single_run ( kwargs ) : 
runfunc = kwargs [ 'runfunc' ] 
runargs = kwargs [ 'runargs' ] 
kwrunparams = kwargs [ 'runkwargs' ] 
clean_up_after_run = kwargs [ 'clean_up_runs' ] 
automatic_storing = kwargs [ 'automatic_storing' ] 
idx = traj . v_idx 
total_runs = len ( traj ) 
'\\n=========================================\\n' % ( idx , total_runs ) ) 
traj . f_start_run ( turn_into_run = True ) 
result = runfunc ( traj , * runargs , ** kwrunparams ) 
if automatic_storing : 
~~ if wrap_mode == pypetconstants . WRAP_MODE_LOCAL : 
~~~ result = ( ( traj . v_idx , result ) , 
traj . f_get_run_information ( traj . v_idx , copy = False ) , 
traj . v_storage_service . references ) 
traj . v_storage_service . free_references ( ) 
traj . f_get_run_information ( traj . v_idx , copy = False ) ) 
~~ traj . f_finalize_run ( store_meta_data = False , 
clean_up = clean_up_after_run ) 
~~ def _wrap_handling ( kwargs ) : 
handler = kwargs [ 'handler' ] 
graceful_exit = kwargs [ 'graceful_exit' ] 
~~ handler . run ( ) 
~~ def load_class ( full_class_string ) : 
class_data = full_class_string . split ( "." ) 
module_path = "." . join ( class_data [ : - 1 ] ) 
class_str = class_data [ - 1 ] 
module = importlib . import_module ( module_path ) 
return getattr ( module , class_str ) 
~~ def create_class ( class_name , dynamic_imports ) : 
~~~ new_class = globals ( ) [ class_name ] 
if not inspect . isclass ( new_class ) : 
~~ return new_class 
~~ except ( KeyError , TypeError ) : 
~~~ for dynamic_class in dynamic_imports : 
~~~ if inspect . isclass ( dynamic_class ) : 
~~~ if class_name == dynamic_class . __name__ : 
~~~ return dynamic_class 
~~~ class_name_to_test = dynamic_class . split ( '.' ) [ - 1 ] 
if class_name == class_name_to_test : 
~~~ new_class = load_class ( dynamic_class ) 
return new_class 
~~ ~~ def f_get_range_length ( self ) : 
if not self . f_has_range ( ) : 
~~ elif hasattr ( self , '__len__' ) : 
~~~ return len ( self ) 
~~ ~~ def f_val_to_str ( self ) : 
old_locked = self . _locked 
~~~ return repr ( self . f_get ( ) ) 
~~~ self . _locked = old_locked 
~~ ~~ def _equal_values ( self , val1 , val2 ) : 
if self . f_supports ( val1 ) != self . f_supports ( val2 ) : 
~~ if not self . f_supports ( val1 ) and not self . f_supports ( val2 ) : 
~~ if not self . _values_of_same_type ( val1 , val2 ) : 
~~ return comparisons . nested_equal ( val1 , val2 ) 
~~ def _values_of_same_type ( self , val1 , val2 ) : 
str ( type ( val1 ) ) , str ( type ( val2 ) ) ) 
~~ return type ( val1 ) is type ( val2 ) 
~~ def f_supports ( self , data ) : 
dtype = type ( data ) 
if dtype is tuple or dtype is list : 
~~~ if len ( data ) == 0 : 
~~ old_type = None 
for item in data : 
~~~ if not type ( item ) in pypetconstants . PARAMETER_SUPPORTED_DATA : 
~~ if not old_type is None and old_type != type ( item ) : 
~~ old_type = type ( item ) 
~~ elif dtype is np . ndarray or dtype is np . matrix : 
~~~ if data . size == 0 : 
~~ dtype = data . dtype 
if np . issubdtype ( dtype , np . str ) : 
~~~ dtype = np . str 
~~ ~~ return dtype in pypetconstants . PARAMETER_SUPPORTED_DATA 
~~ if not type ( val1 ) is type ( val2 ) : 
~~ if type ( val1 ) is np . array : 
~~~ if not val1 . dtype is val2 . dtype : 
~~ if not np . shape ( val1 ) == np . shape ( val2 ) : 
~~ ~~ if type ( val1 ) is tuple : 
~~~ return ( type ( val1 [ 0 ] ) is type ( val2 [ 0 ] ) ) and ( len ( val1 ) == len ( val2 ) ) 
~~ def f_get_range ( self , copy = True ) : 
self . v_full_name ) 
~~ elif copy : 
~~~ return self . _explored_range [ : ] 
~~~ return self . _explored_range 
~~ ~~ def _explore ( self , explore_iterable ) : 
if self . v_locked : 
~~ if self . f_has_range ( ) : 
~~ if self . _data is None : 
~~ data_list = self . _data_sanity_checks ( explore_iterable ) 
self . _explored_range = data_list 
self . _explored = True 
self . f_lock ( ) 
~~ def _expand ( self , explore_iterable ) : 
~~ if not self . f_has_range ( ) : 
self . _explored_range . extend ( data_list ) 
~~ def _data_sanity_checks ( self , explore_iterable ) : 
data_list = [ ] 
for val in explore_iterable : 
~~~ if not self . f_supports ( val ) : 
~~ if not self . _values_of_same_type ( val , self . _default ) : 
( self . v_full_name , str ( type ( val ) ) , str ( type ( self . _default ) ) ) ) 
~~ data_list . append ( val ) 
~~ if len ( data_list ) == 0 : 
~~ return data_list 
~~ def _store ( self ) : 
if self . _data is not None : 
~~~ store_dict = { 'data' : ObjectTable ( data = { 'data' : [ self . _data ] } ) } 
~~~ store_dict [ 'explored_data' ] = ObjectTable ( data = { 'data' : self . _explored_range } ) 
~~ self . _locked = True 
return store_dict 
~~ def _load ( self , load_dict ) : 
~~ if 'data' in load_dict : 
~~~ self . _data = load_dict [ 'data' ] [ 'data' ] [ 0 ] 
self . _default = self . _data 
~~ if 'explored_data' in load_dict : 
~~~ self . _explored_range = [ x for x in load_dict [ 'explored_data' ] [ 'data' ] . tolist ( ) ] 
if type ( self . _data ) not in ( np . ndarray , tuple , np . matrix , list ) : 
~~~ return super ( ArrayParameter , self ) . _store ( ) 
~~~ store_dict = { 'data' + ArrayParameter . IDENTIFIER : self . _data } 
if self . f_has_range ( ) : 
~~~ smart_dict = { } 
store_dict [ 'explored_data' + ArrayParameter . IDENTIFIER ] = ObjectTable ( columns = [ 'idx' ] , index = list ( range ( len ( self ) ) ) ) 
for idx , elem in enumerate ( self . _explored_range ) : 
~~~ if isinstance ( elem , np . ndarray ) : 
~~~ hash_elem = HashArray ( elem ) 
~~ elif isinstance ( elem , list ) : 
~~~ hash_elem = tuple ( elem ) 
~~~ hash_elem = elem 
~~ if hash_elem in smart_dict : 
~~~ name_idx = smart_dict [ hash_elem ] 
add = False 
~~~ name_idx = count 
add = True 
~~ name = self . _build_name ( name_idx ) 
store_dict [ 'explored_data' + ArrayParameter . IDENTIFIER ] [ 'idx' ] [ idx ] = name_idx 
if add : 
~~~ store_dict [ name ] = elem 
smart_dict [ hash_elem ] = name_idx 
~~ ~~ ~~ self . _locked = True 
~~ ~~ def _load ( self , load_dict ) : 
~~~ self . _data = load_dict [ 'data' + ArrayParameter . IDENTIFIER ] 
if 'explored_data' + ArrayParameter . IDENTIFIER in load_dict : 
~~~ explore_table = load_dict [ 'explored_data' + ArrayParameter . IDENTIFIER ] 
idx = explore_table [ 'idx' ] 
explore_list = [ ] 
for name_idx in idx : 
~~~ arrayname = self . _build_name ( name_idx ) 
explore_list . append ( load_dict [ arrayname ] ) 
~~ self . _explored_range = [ x for x in explore_list ] 
~~~ super ( ArrayParameter , self ) . _load ( load_dict ) 
~~ self . _default = self . _data 
self . _locked = True 
if ( type ( val1 ) in ( np . ndarray , tuple , np . matrix ) ) and ( type ( val2 ) is type ( val1 ) ) : 
~~~ return super ( ArrayParameter , self ) . _values_of_same_type ( val1 , val2 ) 
if dtype is tuple or dtype is list and len ( data ) == 0 : 
~~ elif dtype is np . ndarray and data . size == 0 and data . ndim == 1 : 
~~~ return super ( ArrayParameter , self ) . f_supports ( data ) 
~~ ~~ def _values_of_same_type ( self , val1 , val2 ) : 
if self . _is_supported_matrix ( val1 ) and self . _is_supported_matrix ( val2 ) : 
~~~ return super ( SparseParameter , self ) . _values_of_same_type ( val1 , val2 ) 
if self . _is_supported_matrix ( val1 ) : 
~~~ if self . _is_supported_matrix ( val2 ) : 
~~~ _ , _ , hash_tuple_1 = self . _serialize_matrix ( val1 ) 
_ , _ , hash_tuple_2 = self . _serialize_matrix ( val2 ) 
return hash ( hash_tuple_1 ) == hash ( hash_tuple_2 ) 
~~~ return super ( SparseParameter , self ) . _equal_values ( val1 , val2 ) 
~~ ~~ def _is_supported_matrix ( data ) : 
return ( spsp . isspmatrix_csc ( data ) or 
spsp . isspmatrix_csr ( data ) or 
spsp . isspmatrix_bsr ( data ) or 
spsp . isspmatrix_dia ( data ) ) 
if self . _is_supported_matrix ( data ) : 
~~~ return super ( SparseParameter , self ) . f_supports ( data ) 
~~ ~~ def _serialize_matrix ( matrix ) : 
if ( spsp . isspmatrix_csc ( matrix ) or 
spsp . isspmatrix_csr ( matrix ) or 
spsp . isspmatrix_bsr ( matrix ) ) : 
~~~ if matrix . size > 0 : 
~~~ return_list = [ matrix . data , matrix . indices , matrix . indptr , matrix . shape ] 
~~~ return_list = [ '__empty__' , ( ) , ( ) , matrix . shape ] 
~~ return_names = SparseParameter . OTHER_NAME_LIST 
if spsp . isspmatrix_csc ( matrix ) : 
~~~ return_list = [ 'csc' ] + return_list 
~~ elif spsp . isspmatrix_csr ( matrix ) : 
~~~ return_list = [ 'csr' ] + return_list 
~~ elif spsp . isspmatrix_bsr ( matrix ) : 
~~~ return_list = [ 'bsr' ] + return_list 
~~ ~~ elif spsp . isspmatrix_dia ( matrix ) : 
~~~ return_list = [ 'dia' , matrix . data , matrix . offsets , matrix . shape ] 
~~~ return_list = [ 'dia' , '__empty__' , ( ) , matrix . shape ] 
~~ return_names = SparseParameter . DIA_NAME_LIST 
~~ hash_list = [ ] 
for item in return_list : 
~~~ if type ( item ) is np . ndarray : 
~~~ hash_list . append ( HashArray ( item ) ) 
~~~ hash_list . append ( item ) 
~~ ~~ return return_list , return_names , tuple ( hash_list ) 
if not self . _is_supported_matrix ( self . _data ) : 
~~~ return super ( SparseParameter , self ) . _store ( ) 
~~~ store_dict = { } 
data_list , name_list , hash_tuple = self . _serialize_matrix ( self . _data ) 
rename_list = [ 'data%s%s' % ( SparseParameter . IDENTIFIER , name ) 
for name in name_list ] 
is_dia = int ( len ( rename_list ) == 4 ) 
store_dict [ 'data%sis_dia' % SparseParameter . IDENTIFIER ] = is_dia 
for idx , name in enumerate ( rename_list ) : 
~~~ store_dict [ name ] = data_list [ idx ] 
store_dict [ 'explored_data' + SparseParameter . IDENTIFIER ] = ObjectTable ( columns = [ 'idx' , 'is_dia' ] , 
index = list ( range ( len ( self ) ) ) ) 
~~~ data_list , name_list , hash_tuple = self . _serialize_matrix ( elem ) 
if hash_tuple in smart_dict : 
~~~ name_idx = smart_dict [ hash_tuple ] 
~~ is_dia = int ( len ( name_list ) == 4 ) 
rename_list = self . _build_names ( name_idx , is_dia ) 
store_dict [ 'explored_data' + SparseParameter . IDENTIFIER ] [ 'idx' ] [ idx ] = name_idx 
store_dict [ 'explored_data' + SparseParameter . IDENTIFIER ] [ 'is_dia' ] [ 
idx ] = is_dia 
~~~ for irun , name in enumerate ( rename_list ) : 
~~~ store_dict [ name ] = data_list [ irun ] 
~~ smart_dict [ hash_tuple ] = name_idx 
~~ ~~ def _build_names ( self , name_idx , is_dia ) : 
name_list = self . _get_name_list ( is_dia ) 
return tuple ( [ 'explored%s.set_%05d.xspm_%s_%08d' % ( SparseParameter . IDENTIFIER , 
name_idx // 200 , name , name_idx ) 
for name in name_list ] ) 
~~ def _reconstruct_matrix ( data_list ) : 
matrix_format = data_list [ 0 ] 
data = data_list [ 1 ] 
is_empty = isinstance ( data , str ) and data == '__empty__' 
if matrix_format == 'csc' : 
~~~ if is_empty : 
~~~ return spsp . csc_matrix ( data_list [ 4 ] ) 
~~~ return spsp . csc_matrix ( tuple ( data_list [ 1 : 4 ] ) , shape = data_list [ 4 ] ) 
~~ ~~ elif matrix_format == 'csr' : 
~~~ return spsp . csr_matrix ( data_list [ 4 ] ) 
~~~ return spsp . csr_matrix ( tuple ( data_list [ 1 : 4 ] ) , shape = data_list [ 4 ] ) 
~~ ~~ elif matrix_format == 'bsr' : 
~~~ return spsp . bsr_matrix ( data_list [ 4 ] ) 
~~~ return spsp . bsr_matrix ( tuple ( data_list [ 1 : 4 ] ) , shape = data_list [ 4 ] ) 
~~ ~~ elif matrix_format == 'dia' : 
~~~ return spsp . dia_matrix ( data_list [ 3 ] ) 
~~~ return spsp . dia_matrix ( tuple ( data_list [ 1 : 3 ] ) , shape = data_list [ 3 ] ) 
~~~ is_dia = load_dict [ 'data%sis_dia' % SparseParameter . IDENTIFIER ] 
data_list = [ load_dict [ name ] for name in rename_list ] 
self . _data = self . _reconstruct_matrix ( data_list ) 
if 'explored_data' + SparseParameter . IDENTIFIER in load_dict : 
~~~ explore_table = load_dict [ 'explored_data' + SparseParameter . IDENTIFIER ] 
idx_col = explore_table [ 'idx' ] 
dia_col = explore_table [ 'is_dia' ] 
for irun , name_id in enumerate ( idx_col ) : 
~~~ is_dia = dia_col [ irun ] 
~~~ name_list = self . _build_names ( name_id , is_dia ) 
data_list = [ load_dict [ name ] for name in name_list ] 
~~~ name_list = self . _build_names_old ( name_id , is_dia ) 
~~ matrix = self . _reconstruct_matrix ( data_list ) 
explore_list . append ( matrix ) 
~~ self . _explored_range = explore_list 
~~~ super ( SparseParameter , self ) . _load ( load_dict ) 
store_dict = { } 
~~~ dump = pickle . dumps ( self . _data , protocol = self . v_protocol ) 
store_dict [ 'data' ] = dump 
store_dict [ PickleParameter . PROTOCOL ] = self . v_protocol 
~~~ store_dict [ 'explored_data' ] = ObjectTable ( columns = [ 'idx' ] , index = list ( range ( len ( self ) ) ) ) 
smart_dict = { } 
for idx , val in enumerate ( self . _explored_range ) : 
~~~ obj_id = id ( val ) 
if obj_id in smart_dict : 
~~~ name_id = smart_dict [ obj_id ] 
~~~ name_id = count 
~~ name = self . _build_name ( name_id ) 
store_dict [ 'explored_data' ] [ 'idx' ] [ idx ] = name_id 
~~~ store_dict [ name ] = pickle . dumps ( val , protocol = self . v_protocol ) 
smart_dict [ obj_id ] = name_id 
~~~ dump = load_dict [ 'data' ] 
self . _data = pickle . loads ( dump ) 
~~~ self . v_protocol = load_dict [ PickleParameter . PROTOCOL ] 
~~~ self . v_protocol = PickleParameter . _get_protocol ( dump ) 
~~~ explore_table = load_dict [ 'explored_data' ] 
name_col = explore_table [ 'idx' ] 
for name_id in name_col : 
~~~ arrayname = self . _build_name ( name_id ) 
loaded = pickle . loads ( load_dict [ arrayname ] ) 
explore_list . append ( loaded ) 
~~ def f_translate_key ( self , key ) : 
if isinstance ( key , int ) : 
~~~ if key == 0 : 
~~~ key = self . v_name 
~~~ key = self . v_name + '_%d' % key 
~~ ~~ return key 
~~ def f_val_to_str ( self ) : 
resstrlist = [ ] 
strlen = 0 
for key in self . _data : 
~~~ val = self . _data [ key ] 
resstrlist . append ( resstr ) 
strlen += len ( resstr ) 
if strlen > pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH : 
~~ ~~ return_string = "" . join ( resstrlist ) 
if len ( return_string ) > pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH : 
~~~ return_string = return_string [ 0 : pypetconstants . HDF5_STRCOL_MAX_VALUE_LENGTH - 3 ] + '...' 
~~ return return_string 
~~~ return self . _data . copy ( ) 
~~~ return self . _data 
if args and self . v_name is None : 
~~ for idx , arg in enumerate ( args ) : 
~~~ valstr = self . f_translate_key ( idx ) 
~~~ if len ( self . _data ) == 1 : 
~~~ return list ( self . _data . values ( ) ) [ 0 ] 
~~ elif len ( self . _data ) > 1 : 
( self . v_full_name , str ( list ( self . _data . keys ( ) ) ) ) ) 
~~~ name = self . f_translate_key ( name ) 
if not name in self . _data : 
~~~ if name == 'data' and len ( self . _data ) == 1 : 
~~~ return self . _data [ list ( self . _data . keys ( ) ) [ 0 ] ] 
( name , self . v_full_name ) ) 
~~ ~~ result_list . append ( self . _data [ name ] ) 
~~ if len ( args ) == 1 : 
~~ ~~ def f_set_single ( self , name , item ) : 
if self . v_stored : 
~~ if self . _supports ( item ) : 
~~~ if name in self . _data : 
~~ self . _data [ name ] = item 
( name , str ( type ( item ) ) ) ) 
~~ ~~ def f_remove ( self , * args ) : 
for arg in args : 
~~~ arg = self . f_translate_key ( arg ) 
if arg in self . _data : 
~~~ del self . _data [ arg ] 
~~ ~~ ~~ def _supports ( self , item ) : 
if SparseParameter . _is_supported_matrix ( item ) : 
~~~ return super ( SparseResult , self ) . _supports ( item ) 
~~ ~~ def _store ( self ) : 
if SparseParameter . _is_supported_matrix ( val ) : 
~~~ data_list , name_list , hash_tuple = SparseParameter . _serialize_matrix ( val ) 
rename_list = [ '%s%s%s' % ( key , SparseParameter . IDENTIFIER , name ) 
store_dict [ key + SparseResult . IDENTIFIER + 'is_dia' ] = is_dia 
~~~ store_dict [ key ] = val 
~~ ~~ return store_dict 
for key in list ( load_dict . keys ( ) ) : 
~~~ if key in load_dict : 
~~~ if SparseResult . IDENTIFIER in key : 
~~~ new_key = key . split ( SparseResult . IDENTIFIER ) [ 0 ] 
is_dia = load_dict . pop ( new_key + SparseResult . IDENTIFIER + 'is_dia' ) 
name_list = SparseParameter . _get_name_list ( is_dia ) 
rename_list = [ '%s%s%s' % ( new_key , SparseResult . IDENTIFIER , name ) 
data_list = [ load_dict . pop ( name ) for name in rename_list ] 
matrix = SparseParameter . _reconstruct_matrix ( data_list ) 
self . _data [ new_key ] = matrix 
~~~ self . _data [ key ] = load_dict [ key ] 
~~ ~~ ~~ ~~ def f_set_single ( self , name , item ) : 
~~ if name == PickleResult . PROTOCOL : 
for key , val in self . _data . items ( ) : 
~~~ store_dict [ key ] = pickle . dumps ( val , protocol = self . v_protocol ) 
~~ store_dict [ PickleResult . PROTOCOL ] = self . v_protocol 
~~~ self . v_protocol = load_dict . pop ( PickleParameter . PROTOCOL ) 
~~~ dump = next ( load_dict . values ( ) ) 
self . v_protocol = PickleParameter . _get_protocol ( dump ) 
~~ for key in load_dict : 
~~~ val = load_dict [ key ] 
self . _data [ key ] = pickle . loads ( val ) 
~~ ~~ def main ( ) : 
folder = os . getcwd ( ) 
merge_all_in_folder ( folder , 
dynamic_imports = FunctionParameter , 
backup = False ) 
print ( 'Done' ) 
~~ def upload_file ( filename , session ) : 
outfilesource = os . path . join ( os . getcwd ( ) , filename ) 
outfiletarget = 'sftp://' + ADDRESS + WORKING_DIR 
out = saga . filesystem . File ( outfilesource , session = session , flags = OVERWRITE ) 
out . copy ( outfiletarget ) 
~~ def download_file ( filename , session ) : 
infilesource = os . path . join ( 'sftp://' + ADDRESS + WORKING_DIR , 
filename ) 
infiletarget = os . path . join ( os . getcwd ( ) , filename ) 
incoming = saga . filesystem . File ( infilesource , session = session , flags = OVERWRITE ) 
incoming . copy ( infiletarget ) 
~~ def create_session ( ) : 
ctx = saga . Context ( "UserPass" ) 
ctx . user_id = USER 
ctx . user_pass = PASSWORD 
session = saga . Session ( ) 
session . add_context ( ctx ) 
return session 
~~ def merge_trajectories ( session ) : 
jd = saga . job . Description ( ) 
jd . executable = 'python' 
jd . arguments = [ 'merge_trajs.py' ] 
jd . output = "mysagajob_merge.stdout" 
jd . error = "mysagajob_merge.stderr" 
jd . working_directory = WORKING_DIR 
js = saga . job . Service ( 'ssh://' + ADDRESS , session = session ) 
myjob = js . create_job ( jd ) 
myjob . run ( ) 
myjob . wait ( ) 
~~ def start_jobs ( session ) : 
batches = range ( 3 ) 
jobs = [ ] 
for batch in batches : 
jd . output = "mysagajob.stdout" + str ( batch ) 
jd . error = "mysagajob.stderr" + str ( batch ) 
jobs . append ( myjob ) 
~~ for myjob in jobs : 
filename = os . path . join ( 'hdf5' , 'example_21.hdf5' ) 
env = Environment ( trajectory = 'Example_21_SCOOP' , 
filename = filename , 
file_title = 'Example_21_SCOOP' , 
log_stdout = True , 
multiproc = True , 
overwrite_file = True ) 
traj = env . trajectory 
traj . f_explore ( cartesian_product ( { 'x' : [ float ( x ) for x in range ( 20 ) ] , 
'y' : [ float ( y ) for y in range ( 20 ) ] } ) ) 
env . run ( multiply ) 
assert traj . f_is_completed ( ) 
env . disable_logging ( ) 
~~ def run_neuron ( traj ) : 
V_init = traj . par . neuron . V_init 
I = traj . par . neuron . I 
tau_V = traj . par . neuron . tau_V 
tau_ref = traj . par . neuron . tau_ref 
dt = traj . par . simulation . dt 
duration = traj . par . simulation . duration 
steps = int ( duration / float ( dt ) ) 
V_array = np . zeros ( steps ) 
V_array [ 0 ] = V_init 
for step in range ( 1 , steps ) : 
~~~ if V_array [ step - 1 ] >= 1 : 
~~~ V_array [ step ] = 0 
spiketimes . append ( ( step - 1 ) * dt ) 
~~ elif spiketimes and step * dt - spiketimes [ - 1 ] <= tau_ref : 
~~~ dV = - 1 / tau_V * V_array [ step - 1 ] + I 
V_array [ step ] = V_array [ step - 1 ] + dV * dt 
traj . f_add_result ( 'neuron.$' , V = V_array , nspikes = len ( spiketimes ) , 
return len ( spiketimes ) / float ( traj . par . simulation . duration ) * 1000 
~~ def neuron_postproc ( traj , result_list ) : 
I_range = traj . par . neuron . f_get ( 'I' ) . f_get_range ( ) 
ref_range = traj . par . neuron . f_get ( 'tau_ref' ) . f_get_range ( ) 
I_index = sorted ( set ( I_range ) ) 
ref_index = sorted ( set ( ref_range ) ) 
rates_frame = pd . DataFrame ( columns = ref_index , index = I_index ) 
for result_tuple in result_list : 
~~~ run_idx = result_tuple [ 0 ] 
firing_rates = result_tuple [ 1 ] 
I_val = I_range [ run_idx ] 
ref_val = ref_range [ run_idx ] 
~~ traj . f_add_result ( 'summary.firing_rates' , rates_frame = rates_frame , 
traj . f_add_parameter ( 'neuron.V_init' , 0.0 , 
traj . f_add_parameter ( 'neuron.I' , 0.0 , 
traj . f_add_parameter ( 'neuron.tau_V' , 10.0 , 
traj . f_add_parameter ( 'neuron.tau_ref' , 5.0 , 
traj . f_add_parameter ( 'simulation.duration' , 1000.0 , 
'milliseconds.' ) 
traj . f_add_parameter ( 'simulation.dt' , 0.1 , 
~~ def add_exploration ( traj ) : 
explore_dict = { 'neuron.I' : np . arange ( 0 , 1.01 , 0.01 ) . tolist ( ) , 
'neuron.tau_ref' : [ 5.0 , 7.5 , 10.0 ] } 
explore_dict = cartesian_product ( explore_dict , ( 'neuron.tau_ref' , 'neuron.I' ) ) 
~~ def execute_network_pre_run ( self , traj , network , network_dict , component_list , analyser_list ) : 
self . _execute_network_run ( traj , network , network_dict , component_list , analyser_list , 
pre_run = True ) 
~~ def execute_network_run ( self , traj , network , network_dict , component_list , analyser_list ) : 
pre_run = False ) 
~~ def _extract_subruns ( self , traj , pre_run = False ) : 
if pre_run : 
~~~ durations_list = traj . f_get_all ( self . _pre_durations_group_name ) 
~~~ durations_list = traj . f_get_all ( self . _durations_group_name ) 
~~ subruns = { } 
orders = [ ] 
for durations in durations_list : 
~~~ for duration_param in durations . f_iter_leaves ( with_links = False ) : 
~~~ if 'order' in duration_param . v_annotations : 
~~~ order = duration_param . v_annotations . order 
duration_param . v_full_name ) 
~~ if order in subruns : 
~~~ subruns [ order ] = duration_param 
orders . append ( order ) 
~~ ~~ ~~ return [ subruns [ order ] for order in sorted ( orders ) ] 
~~ def _execute_network_run ( self , traj , network , network_dict , component_list , 
analyser_list , pre_run = False ) : 
subrun_list = self . _extract_subruns ( traj , pre_run = pre_run ) 
subrun_number = 0 
while len ( subrun_list ) > 0 : 
~~~ current_subrun = subrun_list . pop ( 0 ) 
for component in component_list : 
~~~ component . add_to_network ( traj , network , current_subrun , subrun_list , 
network_dict ) 
~~ for analyser in analyser_list : 
~~~ analyser . add_to_network ( traj , network , current_subrun , subrun_list , 
~~ self . add_to_network ( traj , network , current_subrun , subrun_list , 
( current_subrun . v_name , subrun_number , str ( current_subrun . f_get ( ) ) ) ) 
network . run ( duration = current_subrun . f_get ( ) , report = self . _report , 
report_period = self . _report_period ) 
for analyser in analyser_list : 
~~~ analyser . analyse ( traj , network , current_subrun , subrun_list , 
~~ self . remove_from_network ( traj , network , current_subrun , subrun_list , 
~~~ analyser . remove_from_network ( traj , network , current_subrun , subrun_list , 
~~ for component in component_list : 
~~~ component . remove_from_network ( traj , network , current_subrun , subrun_list , 
~~ subrun_number += 1 
~~ ~~ def add_parameters ( self , traj ) : 
for component in self . components : 
~~~ component . add_parameters ( traj ) 
~~ if self . analysers : 
for analyser in self . analysers : 
~~~ analyser . add_parameters ( traj ) 
self . network_runner . add_parameters ( traj ) 
~~ def pre_build ( self , traj ) : 
~~~ component . pre_build ( traj , self . _brian_list , self . _network_dict ) 
~~~ analyser . pre_build ( traj , self . _brian_list , self . _network_dict ) 
self . network_runner . pre_build ( traj , self . _brian_list , self . _network_dict ) 
self . _pre_built = True 
~~ def build ( self , traj ) : 
~~~ component . build ( traj , self . _brian_list , self . _network_dict ) 
~~~ analyser . build ( traj , self . _brian_list , self . _network_dict ) 
self . network_runner . build ( traj , self . _brian_list , self . _network_dict ) 
~~ def pre_run_network ( self , traj ) : 
self . pre_build ( traj ) 
self . _logger . info ( '\\n------------------------\\n' 
'------------------------' ) 
self . _network = self . _network_constructor ( * self . _brian_list ) 
self . network_runner . execute_network_pre_run ( traj , self . _network , self . _network_dict , 
self . components , self . analysers ) 
self . _logger . info ( '\\n-----------------------------\\n' 
'-----------------------------' ) 
self . _pre_run = True 
if hasattr ( self . _network , 'store' ) : 
~~~ self . _network . store ( 'pre_run' ) 
~~ ~~ def run_network ( self , traj ) : 
if self . _pre_built : 
~~~ if self . _pre_run and hasattr ( self . _network , 'restore' ) : 
~~~ self . _network . restore ( 'pre_run' ) 
self . _network . store ( 'pre_run' ) 
~~ self . _run_network ( traj ) 
~~~ self . _run_network ( traj ) 
~~ ~~ def _run_network ( self , traj ) : 
self . build ( traj ) 
self . _pretty_print_explored_parameters ( traj ) 
if not self . _pre_run : 
~~~ self . _network = self . _network_constructor ( * self . _brian_list ) 
~~ self . network_runner . execute_network_run ( traj , self . _network , self . _network_dict , 
~~ def make_filename ( traj ) : 
explored_parameters = traj . f_get_explored_parameters ( ) 
filename = '' 
for param in explored_parameters . values ( ) : 
~~~ short_name = param . v_name 
filename += '%s_%s__' % ( short_name , str ( val ) ) 
~~ return filename [ : - 2 ] + '.png' 
~~ def wrap_automaton ( traj ) : 
initial_state = make_initial_state ( traj . initial_name , traj . ncells , traj . seed ) 
pattern = cellular_automaton_1D ( initial_state , traj . rule_number , traj . steps ) 
logger = logging . getLogger ( ) 
folder = os . path . join ( os . getcwd ( ) , 'experiments' , 'ca_patterns_pypet' ) 
~~ filename = os . path . join ( folder , 'all_patterns.hdf5' ) 
env = Environment ( trajectory = 'cellular_automata' , 
ncores = 4 , 
wrap_mode = 'QUEUE' , 
traj = env . traj 
exp_dict = { 'rule_number' : [ 10 , 30 , 90 , 110 , 184 ] , 
'initial_name' : [ 'single' , 'random' ] , } 
exp_dict = cartesian_product ( exp_dict ) 
traj . f_explore ( exp_dict ) 
env . run ( wrap_automaton ) 
traj . f_load ( load_data = 2 ) 
for idx , run_name in enumerate ( traj . f_iter_runs ( ) ) : 
~~~ filename = os . path . join ( folder , make_filename ( traj ) ) 
plot_pattern ( traj . crun . pattern , traj . rule_number , filename ) 
progressbar ( idx , len ( traj ) , logger = logger ) 
~~ env . disable_logging ( ) 
~~ def next ( self ) : 
~~~ return next ( self . _current ) 
~~~ self . _current = iter ( self . _chain . popleft ( ) ) 
~~ ~~ ~~ ~~ def merge_all_in_folder ( folder , ext = '.hdf5' , 
storage_service = None , 
delete_other_files = False , 
in_dir = os . listdir ( folder ) 
all_files = [ ] 
for file in in_dir : 
~~~ full_file = os . path . join ( folder , file ) 
if os . path . isfile ( full_file ) : 
~~~ _ , extension = os . path . splitext ( full_file ) 
if extension == ext : 
~~~ all_files . append ( full_file ) 
~~ ~~ ~~ all_files = sorted ( all_files ) 
trajs = [ ] 
for full_file in all_files : 
~~~ traj = load_trajectory ( index = - 1 , 
storage_service = storage_service , 
filename = full_file , 
load_data = 0 , 
force = force , 
dynamic_imports = dynamic_imports ) 
trajs . append ( traj ) 
~~ first_traj = trajs . pop ( 0 ) 
first_traj . f_merge_many ( trajs , 
ignore_data = ignore_data , 
backup = backup ) 
if delete_other_files : 
~~~ for file in all_files [ 1 : ] : 
~~~ os . remove ( file ) 
~~ ~~ return first_traj 
~~ def _handle_sigint ( self , signum , frame ) : 
if self . hit : 
raise KeyboardInterrupt ( prompt ) 
~~~ self . hit = True 
sys . stderr . write ( prompt ) 
~~ ~~ def write_fcs ( filename , chn_names , data , 
endianness = "big" , 
compat_chn_names = True , 
compat_copy = True , 
compat_negative = True , 
compat_percent = True , 
compat_max_int16 = 10000 ) : 
filename = pathlib . Path ( filename ) 
if not isinstance ( data , np . ndarray ) : 
~~~ data = np . array ( data , dtype = float ) 
~~ nanrows = np . isnan ( data ) . any ( axis = 1 ) 
if np . sum ( nanrows ) : 
warnings . warn ( msg ) 
data = data [ ~ nanrows ] 
~~ if endianness not in [ "little" , "big" ] : 
assert len ( chn_names ) == data . shape [ 1 ] , msg 
rpl = [ [ "" , "u" ] , 
[ "" , "2" ] , 
if compat_chn_names : 
[ "?" , "" ] , 
[ "_" , "" ] , 
~~ for ii in range ( len ( chn_names ) ) : 
~~~ for ( a , b ) in rpl : 
~~~ chn_names [ ii ] = chn_names [ ii ] . replace ( a , b ) 
~~ ~~ pcnt_cands = [ ] 
for ch in range ( data . shape [ 1 ] ) : 
~~~ if data [ : , ch ] . min ( ) >= 0 and data [ : , ch ] . max ( ) <= 1 : 
~~~ pcnt_cands . append ( ch ) 
~~ ~~ if compat_percent and pcnt_cands : 
~~~ if compat_copy : 
~~~ data = data . copy ( ) 
~~ for ch in pcnt_cands : 
~~~ data [ : , ch ] *= 100 
~~ ~~ if compat_negative : 
~~~ toflip = [ ] 
~~~ if np . mean ( data [ : , ch ] ) < 0 : 
~~~ toflip . append ( ch ) 
~~ ~~ if len ( toflip ) : 
~~ for ch in toflip : 
~~~ data [ : , ch ] *= - 1 
~~ ~~ ~~ data1 = data . flatten ( ) . tolist ( ) 
DATA = struct . pack ( '>%sf' % len ( data1 ) , * data1 ) 
header_size = 256 
if endianness == "little" : 
~~~ byteord = '1,2,3,4' 
~~~ byteord = '4,3,2,1' 
~~ TEXT = '/$BEGINANALYSIS/0/$ENDANALYSIS/0' 
TEXT += '/$BEGINSTEXT/0/$ENDSTEXT/0' 
TEXT += '/$BEGINDATA/{data_start_byte}/$ENDDATA/{data_end_byte}' 
TEXT += '/$BYTEORD/{0}/$DATATYPE/F' . format ( byteord ) 
TEXT += '/$MODE/L/$NEXTDATA/0/$TOT/{0}' . format ( data . shape [ 0 ] ) 
TEXT += '/$PAR/{0}' . format ( data . shape [ 1 ] ) 
for jj in range ( data . shape [ 1 ] ) : 
~~~ if ( compat_max_int16 and 
np . max ( data [ : , jj ] ) > compat_max_int16 and 
np . max ( data [ : , jj ] ) < 2 ** 15 ) : 
~~~ pnrange = int ( 2 ** 15 ) 
~~ elif jj in pcnt_cands : 
~~~ pnrange = 100 
~~~ pnrange = 1 
~~~ pnrange = int ( abs ( np . max ( data [ : , jj ] ) ) ) 
~~ fmt_str = '/$P{0}B/32/$P{0}E/0,0/$P{0}N/{1}/$P{0}R/{2}/$P{0}D/Linear' 
TEXT += fmt_str . format ( jj + 1 , chn_names [ jj ] , pnrange ) 
~~ TEXT += '/' 
data_start_byte = header_size + len ( TEXT ) + text_padding 
data_end_byte = data_start_byte + len ( DATA ) - 1 
TEXT = TEXT . format ( data_start_byte = data_start_byte , 
data_end_byte = data_end_byte ) 
lentxt = len ( TEXT ) 
ver = 'FCS3.0' 
if data_end_byte <= 99999999 : 
+ textfirst 
+ textlast 
+ datafirst 
+ datalast 
+ anafirst 
+ analast ) 
with filename . open ( "wb" ) as fd : 
~~~ fd . write ( HEADER . encode ( "ascii" , "replace" ) ) 
fd . write ( TEXT . encode ( "ascii" , "replace" ) ) 
fd . write ( DATA ) 
fd . write ( b'00000000' ) 
~~ ~~ def read_tdms ( tdms_file ) : 
tdms_file = nptdms . TdmsFile ( tdms_file ) 
ch_names = [ ] 
ch_data = [ ] 
for o in tdms_file . objects . values ( ) : 
~~~ if o . data is not None and len ( o . data ) : 
~~~ chn = o . path . split ( '/' ) [ - 1 ] . strip ( "\ ) 
if "unit_string" in o . properties : 
~~~ unit = o . properties [ "unit_string" ] 
~~~ ch_names . append ( chn ) 
~~ ch_data . append ( o . data ) 
~~ ~~ return ch_names , ch_data 
~~ def add_deformation ( chn_names , data ) : 
if "deformation" not in chn_names : 
~~~ for ii , ch in enumerate ( chn_names ) : 
~~~ if ch == "circularity" : 
~~~ chn_names . append ( "deformation" ) 
data . append ( 1 - data [ ii ] ) 
~~ ~~ ~~ return chn_names , data 
~~ def tdms2fcs ( tdms_file ) : 
fcs_file = tdms_file [ : - 4 ] + "fcs" 
chn_names , data = read_tdms ( tdms_file ) 
chn_names , data = add_deformation ( chn_names , data ) 
fcswrite . write_fcs ( filename = fcs_file , 
chn_names = chn_names , 
data = np . array ( data ) . transpose ( ) ) 
~~ def way ( self , w ) : 
if w . id not in self . way_ids : 
~~ way_points = [ ] 
for n in w . nodes : 
~~~ way_points . append ( Point ( n . location . lon , n . location . lat ) ) 
~~ except o . InvalidLocationError : 
~~ ~~ self . ways [ w . id ] = Way ( w . id , way_points ) 
~~ def missing_node_ids ( self ) : 
present_node_ids = self . nodes . keys ( ) 
for nid in self . node_ids : 
~~~ if nid not in present_node_ids : 
~~~ yield nid 
~~ ~~ ~~ def node ( self , n ) : 
if n . id not in self . node_ids : 
~~~ self . nodes [ n . id ] = Node ( n . id , 
n . location . lon , 
n . location . lat , 
{ t . k : t . v for t in n . tags } ) 
~~ ~~ def build_route ( relation ) : 
if relation . tags . get ( 'type' ) != 'route' : 
~~ short_name = create_route_short_name ( relation ) 
color = relation . tags . get ( 'color' ) 
return Route ( relation . id , 
short_name , 
create_route_long_name ( relation , short_name ) , 
map_osm_route_type_to_gtfs ( relation . tags . get ( 'route' ) ) , 
'https://www.openstreetmap.org/relation/{}' . format ( relation . id ) , 
color . strip ( '#' ) if color else '' , 
get_agency_id ( relation ) ) 
~~ def create_route_long_name ( relation , short_name ) : 
if relation . tags . get ( 'from' ) and relation . tags . get ( 'to' ) : 
~~~ return "{0}-to-{1}" . format ( relation . tags . get ( 'from' ) , 
relation . tags . get ( 'to' ) ) 
if short_name and name . startswith ( short_name ) : 
~~~ return name [ len ( short_name ) : ] 
~~ return name 
~~ def get_agency_id ( relation ) : 
op = relation . tags . get ( 'operator' ) 
if op : 
~~~ return int ( hashlib . sha256 ( op . encode ( 'utf-8' ) ) . hexdigest ( ) , 16 ) % 10 ** 8 
~~ return - 1 
~~ def process ( self ) : 
self . rh = RelationHandler ( ) 
self . rh . apply_file ( self . filename ) 
node_ids , stop_node_ids , way_ids , reverse_map = self . __collect_ids ( ) 
self . nh = NodeHandler ( node_ids ) 
self . nh . apply_file ( self . filename , locations = True ) 
for idx , missing_node_id in enumerate ( self . nh . missing_node_ids ) : 
logging . warning ( 
reverse_map [ missing_node_id ] , missing_node_id ) 
~~ if count : 
~~~ logging . warning ( 
count ) 
~~ self . wh = WayHandler ( way_ids ) 
self . wh . apply_file ( self . filename , locations = True ) 
~~ def relation ( self , rel ) : 
rel_type = rel . tags . get ( 'type' ) 
if any ( [ rel . deleted , 
not rel . visible , 
not self . is_new_version ( rel ) , 
rel_type not in [ 'route' , 'public_transport' ] ] ) : 
~~ route_tag = rel . tags . get ( 'route' ) 
if rel_type == 'route' and route_tag not in self . transit_route_types : 
~~ public_transport = rel . tags . get ( 'public_transport' ) 
if rel_type == 'public_transport' and public_transport != 'stop_area' : 
~~ self . relations [ rel . id ] = Relation ( rel . id , { 
'type' : rel_type , 
'public_transport' : public_transport , 
'route' : route_tag , 
'operator' : rel . tags . get ( 'operator' ) , 
'color' : rel . tags . get ( 'color' ) , 
'ref' : rel . tags . get ( 'ref' ) , 
'from' : rel . tags . get ( 'from' ) , 
'to' : rel . tags . get ( 'to' ) , 
'name' : rel . tags . get ( 'name' ) , 
'alt_name' : rel . tags . get ( 'alt_name' ) , 
'url' : rel . tags . get ( 'url' ) , 
'contact_website' : rel . tags . get ( 'contact:website' ) } , 
[ ( member . type , member . ref , member . role ) for member in rel . members ] ) 
self . versions [ rel . id ] = rel . version 
~~ def create_dummy_data ( routes , stops ) : 
stops_per_route = defaultdict ( lambda : [ ] ) 
stops_map = { } 
for s in stops : 
~~~ if not s . route_id : 
~~ stops_per_route [ s . route_id ] . append ( s ) 
stops_map [ s . stop_id ] = s 
~~ calendar = _create_dummy_calendar ( ) 
trips = _create_dummy_trips ( 
routes , 
stops_per_route , 
calendar ) 
stop_times = _create_dummy_stoptimes ( trips , stops_per_route ) 
frequencies = _create_dummy_frequencies ( trips ) 
return DummyData ( calendar , stop_times , trips , frequencies ) 
~~ def patch_agencies ( agencies ) : 
for agency_id , agency_url , agency_name , agency_timezone in agencies : 
~~~ if not agency_url : 
~~~ agency_url = 'http://hiposfer.com' 
~~ if not agency_timezone : 
~~~ agency_timezone = 'Europe/Berlin' 
~~ yield Agency ( agency_id , agency_url , agency_name , agency_timezone ) 
~~ ~~ def _create_dummy_trip_stoptimes ( trip_id , stops , first_service_time ) : 
waiting = datetime . timedelta ( seconds = 30 ) 
arrival = first_service_time 
last_departure = first_service_time 
last_departure_hour = ( arrival + waiting ) . hour 
last_stop = None 
departure_hour = None 
arrival_hour = None 
for stop_sequence , stop in enumerate ( stops ) : 
~~~ arrival = last_departure + get_time_from_last_stop ( last_stop , stop ) 
departure = arrival + waiting 
if arrival . hour < last_departure_hour : 
~~~ diff = last_departure_hour 
arrival_hour = arrival . hour + diff 
departure_hour = departure . hour + diff 
last_departure_hour = departure . hour + diff 
~~~ arrival_hour = arrival . hour 
departure_hour = departure . hour 
last_departure_hour = departure . hour 
~~ if departure . hour < arrival . hour : 
~~ yield { 'trip_id' : trip_id , 
'arrival_time' : '{:02}:{}' . format ( 
arrival_hour , 
arrival . strftime ( '%M:%S' ) ) , 
'departure_time' : '{:02}:{}' . format ( 
departure_hour , 
departure . strftime ( '%M:%S' ) ) , 
'stop_id' : stop . stop_id , 
'stop_sequence' : stop_sequence } 
last_stop = stop 
last_departure = departure 
~~ ~~ def write_zipped ( self , filepath ) : 
with zipfile . ZipFile ( filepath , mode = 'w' , compression = zipfile . ZIP_DEFLATED ) as zfile : 
~~~ for name , buffer in self . _buffers . items ( ) : 
~~~ encoded_values = io . BytesIO ( buffer . getvalue ( ) . encode ( 'utf-8' ) ) 
zfile . writestr ( '{}.txt' . format ( name ) , 
encoded_values . getbuffer ( ) ) 
~~ for name , path in self . _files . items ( ) : 
~~~ zfile . write ( path , arcname = name ) 
~~ ~~ ~~ def write_unzipped ( self , destination ) : 
for name , buffer in self . _buffers . items ( ) : 
~~~ with open ( os . path . join ( destination , 
'{}.txt' . format ( name ) ) , 
'w' , encoding = 'utf-8' ) as file : 
~~~ file . write ( buffer . getvalue ( ) ) 
~~ ~~ for name , path in self . _files . items ( ) : 
~~~ shutil . copy ( path , os . path . join ( destination , name ) ) 
~~ ~~ def build_agency ( relation , nodes ) : 
agency_url = relation . tags . get ( 'url' ) or relation . tags . get ( 'contact_website' ) 
if not op : 
~~ agency_id = int ( hashlib . sha256 ( op . encode ( 'utf8' ) ) . hexdigest ( ) , 16 ) % 10 ** 8 
return Agency ( agency_id , agency_url , op , '' ) 
~~ def extract_stops ( relation , nodes , visited_stop_ids , stop_to_station_map ) : 
for member_type , member_id , member_role in relation . member_info : 
~~~ if member_id not in visited_stop_ids and member_id in nodes and member_role in ( 'stop' , 'halt' ) : 
~~~ location_type = '' 
visited_stop_ids . add ( member_id ) 
yield Stop ( 
member_id , 
nodes [ member_id ] . tags . get ( 'name' ) or 
nodes [ member_id ] . lon if member_id in nodes else '' , 
nodes [ member_id ] . lat if member_id in nodes else '' , 
relation . id , 
_map_wheelchair ( nodes [ member_id ] . tags . get ( 'wheelchair' ) ) , 
location_type , 
stop_to_station_map . get ( member_id , '' ) ) 
~~ ~~ ~~ def build_shape ( relation , nodes , ways ) : 
sequence_index = 0 
~~~ if member_id in nodes : 
~~~ yield Shape ( 
nodes [ member_id ] . lat , 
nodes [ member_id ] . lon , 
sequence_index ) 
sequence_index += 1 
~~ elif member_id in ways : 
~~ ~~ ~~ def allow_network_access ( self , value : bool ) : 
if self . _is_running : 
~~ self . _allow_network_access = value 
~~ def run_command ( self , 
args : List [ str ] , 
max_num_processes : int = None , 
max_stack_size : int = None , 
max_virtual_memory : int = None , 
as_root : bool = False , 
stdin : FileIO = None , 
timeout : int = None , 
check : bool = False , 
truncate_stdout : int = None , 
truncate_stderr : int = None ) -> 'CompletedCommand' : 
cmd = [ 'docker' , 'exec' , '-i' , self . name , 'cmd_runner.py' ] 
if stdin is None : 
~~~ cmd . append ( '--stdin_devnull' ) 
~~ if max_num_processes is not None : 
~~~ cmd += [ '--max_num_processes' , str ( max_num_processes ) ] 
~~ if max_stack_size is not None : 
~~~ cmd += [ '--max_stack_size' , str ( max_stack_size ) ] 
~~ if max_virtual_memory is not None : 
~~~ cmd += [ '--max_virtual_memory' , str ( max_virtual_memory ) ] 
~~ if timeout is not None : 
~~~ cmd += [ '--timeout' , str ( timeout ) ] 
~~ if truncate_stdout is not None : 
~~~ cmd += [ '--truncate_stdout' , str ( truncate_stdout ) ] 
~~ if truncate_stderr is not None : 
~~~ cmd += [ '--truncate_stderr' , str ( truncate_stderr ) ] 
~~ if not as_root : 
~~~ cmd += [ '--linux_user_id' , str ( self . _linux_uid ) ] 
~~ cmd += args 
if self . debug : 
~~ with tempfile . TemporaryFile ( ) as f : 
~~~ subprocess . run ( cmd , stdin = stdin , stdout = f , stderr = subprocess . PIPE , check = True ) 
json_len = int ( f . readline ( ) . decode ( ) . rstrip ( ) ) 
results_json = json . loads ( f . read ( json_len ) . decode ( ) ) 
stdout_len = int ( f . readline ( ) . decode ( ) . rstrip ( ) ) 
stdout = tempfile . NamedTemporaryFile ( ) 
stdout . write ( f . read ( stdout_len ) ) 
stdout . seek ( 0 ) 
stderr_len = int ( f . readline ( ) . decode ( ) . rstrip ( ) ) 
stderr = tempfile . NamedTemporaryFile ( ) 
stderr . write ( f . read ( stderr_len ) ) 
stderr . seek ( 0 ) 
result = CompletedCommand ( return_code = results_json [ 'return_code' ] , 
timed_out = results_json [ 'timed_out' ] , 
stdout_truncated = results_json [ 'stdout_truncated' ] , 
stderr_truncated = results_json [ 'stderr_truncated' ] ) 
if ( result . return_code != 0 or results_json [ 'timed_out' ] ) and check : 
~~~ raise subprocess . CalledProcessError ( 
result . return_code , cmd , 
output = result . stdout , stderr = result . stderr ) 
~~~ f . seek ( 0 ) 
print ( f . read ( ) ) 
print ( e . stderr ) 
~~ ~~ ~~ def add_files ( self , * filenames : str , owner : str = SANDBOX_USERNAME , read_only : bool = False ) : 
if owner != SANDBOX_USERNAME and owner != 'root' : 
~~~ raise ValueError ( \ . format ( owner ) ) 
~~ with tempfile . TemporaryFile ( ) as f , tarfile . TarFile ( fileobj = f , mode = 'w' ) as tar_file : 
~~~ for filename in filenames : 
~~~ tar_file . add ( filename , arcname = os . path . basename ( filename ) ) 
~~ f . seek ( 0 ) 
subprocess . check_call ( 
[ 'docker' , 'cp' , '-' , 
self . name + ':' + SANDBOX_WORKING_DIR_NAME ] , 
stdin = f ) 
file_basenames = [ os . path . basename ( filename ) for filename in filenames ] 
if owner == SANDBOX_USERNAME : 
~~~ self . _chown_files ( file_basenames ) 
~~ if read_only : 
~~~ chmod_cmd = [ 'chmod' , '444' ] + file_basenames 
self . run_command ( chmod_cmd , as_root = True ) 
~~ ~~ ~~ def add_and_rename_file ( self , filename : str , new_filename : str ) -> None : 
dest = os . path . join ( 
self . name + ':' + SANDBOX_WORKING_DIR_NAME , 
new_filename ) 
subprocess . check_call ( [ 'docker' , 'cp' , filename , dest ] ) 
self . _chown_files ( [ new_filename ] ) 
~~ def get_enrollments_for_course ( self , course_id , params = { } ) : 
url = COURSES_API . format ( course_id ) + "/enrollments" 
enrollments = [ ] 
for datum in self . _get_paged_resource ( url , params = params ) : 
~~~ enrollments . append ( CanvasEnrollment ( data = datum ) ) 
~~ return enrollments 
~~ def get_enrollments_for_course_by_sis_id ( self , sis_course_id , params = { } ) : 
return self . get_enrollments_for_course ( 
self . _sis_id ( sis_course_id , sis_field = "course" ) , params ) 
~~ def get_enrollments_for_section ( self , section_id , params = { } ) : 
url = SECTIONS_API . format ( section_id ) + "/enrollments" 
~~ def get_enrollments_for_section_by_sis_id ( self , sis_section_id , params = { } ) : 
return self . get_enrollments_for_section ( 
self . _sis_id ( sis_section_id , sis_field = "section" ) , params ) 
~~ def get_enrollments_for_regid ( self , regid , params = { } , 
include_courses = True ) : 
sis_user_id = self . _sis_id ( regid , sis_field = "user" ) 
url = USERS_API . format ( sis_user_id ) + "/enrollments" 
courses = Courses ( ) if include_courses else None 
~~~ enrollment = CanvasEnrollment ( data = datum ) 
if include_courses : 
~~~ course_id = datum [ "course_id" ] 
course = courses . get_course ( course_id ) 
if course . sis_course_id is not None : 
~~~ enrollment . course = course 
enrollment . course_url = course . course_url 
enrollment . course_name = course . name 
enrollment . sis_course_id = course . sis_course_id 
~~~ enrollment . course_url = re . sub ( 
r'/users/\\d+$' , '' , enrollment . html_url ) 
~~ enrollments . append ( enrollment ) 
~~ def enroll_user ( self , course_id , user_id , enrollment_type , params = None ) : 
if not params : 
~~~ params = { } 
~~ params [ "user_id" ] = user_id 
params [ "type" ] = enrollment_type 
data = self . _post_resource ( url , { "enrollment" : params } ) 
return CanvasEnrollment ( data = data ) 
~~ def capacity_vesics_1975 ( sl , fd , h_l = 0 , h_b = 0 , vertical_load = 1 , slope = 0 , base_tilt = 0 , verbose = 0 , gwl = 1e6 , ** kwargs ) : 
if not kwargs . get ( "disable_requires" , False ) : 
~~~ models . check_required ( sl , [ "phi_r" , "cohesion" , "unit_dry_weight" ] ) 
models . check_required ( fd , [ "length" , "width" , "depth" ] ) 
~~ area_foundation = fd . length * fd . width 
c_a = 0.6 - 1.0 * sl . cohesion 
horizontal_load = np . sqrt ( h_l ** 2 + h_b ** 2 ) 
fd . nq_factor = ( ( np . tan ( np . pi / 4 + sl . phi_r / 2 ) ) ** 2 * np . exp ( np . pi * np . tan ( sl . phi_r ) ) ) 
if sl . phi_r == 0 : 
~~~ fd . nc_factor = 5.14 
~~~ fd . nc_factor = ( fd . nq_factor - 1 ) / np . tan ( sl . phi_r ) 
~~ fd . ng_factor = 2.0 * ( fd . nq_factor + 1 ) * np . tan ( sl . phi_r ) 
s_c = 1.0 + fd . nq_factor / fd . nc_factor * fd . width / fd . length 
s_q = 1 + fd . width / fd . length * np . tan ( sl . phi_r ) 
if fd . depth / fd . width > 1 : 
~~~ k = np . arctan ( fd . depth / fd . width ) 
~~~ k = fd . depth / fd . width 
~~ d_c = 1 + 0.4 * k 
d_q = 1 + 2 * np . tan ( sl . phi_r ) * ( 1 - np . sin ( sl . phi_r ) ) ** 2 * k 
d_g = 1.0 
m__b = ( 2.0 + fd . width / fd . length ) / ( 1 + fd . width / fd . length ) 
m_l = ( 2.0 + fd . length / fd . width ) / ( 1 + fd . length / fd . width ) 
m = np . sqrt ( m__b ** 2 + m_l ** 2 ) 
~~~ i_q = 1.0 
i_g = 1.0 
~~~ i_q = ( 1.0 - horizontal_load / ( vertical_load + area_foundation * 
c_a / np . tan ( sl . phi_r ) ) ) ** m 
i_g = ( 1.0 - horizontal_load / ( vertical_load + area_foundation * 
c_a / np . tan ( sl . phi_r ) ) ) ** ( m + 1 ) 
~~ i_c = i_q - ( 1 - i_q ) / ( fd . nq_factor - 1 ) 
check_i_c = 1 - m * horizontal_load / ( area_foundation * c_a * fd . nc_factor ) 
if abs ( check_i_c - i_c ) / i_c > 0.001 : 
~~~ raise DesignError 
~~ if sl . phi_r == 0 : 
~~~ g_c = i_q 
~~~ g_c = i_q - ( 1 - i_q ) / ( 5.14 * np . tan ( sl . phi_r ) ) 
~~ g_q = ( 1.0 - np . tan ( slope ) ) ** 2 
g_g = g_q 
~~~ b_c = g_c 
~~~ b_c = 1 - 2 * base_tilt / ( 5.14 * np . tan ( sl . phi_r ) ) 
~~ b_q = ( 1.0 - base_tilt * np . tan ( sl . phi_r ) ) ** 2 
b_g = b_q 
if gwl == 0 : 
~~~ q_d = sl . unit_eff_weight * fd . depth 
unit_weight = sl . unit_bouy_weight 
~~ elif gwl > 0 and gwl < fd . depth : 
~~~ q_d = ( sl . unit_dry_weight * gwl ) + ( sl . unit_bouy_weight * ( fd . depth - gwl ) ) 
~~ elif gwl >= fd . depth and gwl <= fd . depth + fd . width : 
~~~ sl . average_unit_bouy_weight = sl . unit_bouy_weight + ( 
( ( gwl - fd . depth ) / fd . width ) * ( sl . unit_dry_weight - sl . unit_bouy_weight ) ) 
q_d = sl . unit_dry_weight * fd . depth 
unit_weight = sl . average_unit_bouy_weight 
~~ elif gwl > fd . depth + fd . width : 
~~~ q_d = sl . unit_dry_weight * fd . depth 
unit_weight = sl . unit_dry_weight 
~~ if verbose : 
~~ fd . q_ult = ( sl . cohesion * fd . nc_factor * s_c * d_c * i_c * g_c * b_c + 
q_d * fd . nq_factor * s_q * d_q * i_q * g_q * b_q + 
0.5 * fd . width * unit_weight * 
fd . ng_factor * s_g * d_g * i_g * g_g * b_g ) 
~~ return fd . q_ult 
~~ def capacity_terzaghi_1943 ( sl , fd , round_footing = False , verbose = 0 , ** kwargs ) : 
~~ a02 = ( ( np . exp ( np . pi * ( 0.75 - sl . phi / 360 ) * np . tan ( sl . phi_r ) ) ) ** 2 ) 
a0_check = ( np . exp ( ( 270 - sl . phi ) / 180 * np . pi * np . tan ( sl . phi_r ) ) ) 
if ( a02 - a0_check ) / a02 > 0.001 : 
~~ fd . nq_factor = ( a02 / ( 2 * ( np . cos ( ( 45 + sl . phi / 2 ) * np . pi / 180 ) ) ** 2 ) ) 
fd . ng_factor = ( 2 * ( fd . nq_factor + 1 ) * np . tan ( sl . phi_r ) / ( 1 + 0.4 * np . sin ( 4 * sl . phi_r ) ) ) 
~~~ fd . nc_factor = 5.7 
~~ if round_footing : 
~~~ s_c = 1.3 
s_g = 0.6 
~~ elif fd . length / fd . width < 5 : 
s_g = 0.8 
~~~ s_c = 1.0 
s_g = 1.0 
~~ s_q = 1.0 
fd . q_ult = ( sl . cohesion * fd . nc_factor * s_c + q_d * fd . nq_factor * s_q + 0.5 * fd . width * 
sl . unit_dry_weight * fd . ng_factor * s_g ) 
~~ def capacity_hansen_1970 ( sl , fd , h_l = 0 , h_b = 0 , vertical_load = 1 , slope = 0 , base_tilt = 0 , verbose = 0 , ** kwargs ) : 
~~ fd . ng_factor = 1.5 * ( fd . nq_factor - 1 ) * np . tan ( sl . phi_r ) 
~~~ s_c = 0.2 * fd . width / fd . length 
~~~ s_c = 1.0 + fd . nq_factor / fd . nc_factor * fd . width / fd . length 
~~ s_q = 1.0 + fd . width / fd . length * np . sin ( sl . phi_r ) 
s_g = 1.0 - 0.4 * fd . width / fd . length 
if sl . phi == 0 : 
~~~ d_c = 0.4 * k 
~~ d_q = 1 + 2 * np . tan ( sl . phi_r ) * ( 1 - np . sin ( sl . phi_r ) ) ** 2 * k 
i_c = 0.5 - 0.5 * np . sqrt ( 1 - horizontal_load / area_foundation * c_a ) 
~~~ i_q = ( ( 1.0 - 0.5 * horizontal_load / 
( vertical_load + area_foundation * c_a / np . tan ( sl . phi_r ) ) ) ** 5 ) 
i_c = i_q - ( 1 - i_q ) / ( fd . nq_factor - 1 ) 
i_g = ( ( 1 - ( 0.7 * horizontal_load ) / 
~~~ g_c = ( slope / np . pi * 180 ) / 147 
~~~ g_c = 1.0 - ( slope / np . pi * 180 ) / 147 
~~ g_q = 1 - 0.5 * np . tan ( slope ) ** 5 
~~~ b_c = ( base_tilt / np . pi * 180 ) / 147 
~~~ b_c = 1.0 - ( base_tilt / np . pi * 180 ) / 147 
~~ b_q = ( np . exp ( - 0.0349 * ( base_tilt / np . pi * 180 ) * np . tan ( sl . phi_r ) ) ) 
b_g = ( np . exp ( - 0.0471 * ( base_tilt / np . pi * 180 ) * np . tan ( sl . phi_r ) ) ) 
~~ q_d = sl . unit_dry_weight * fd . depth 
~~~ fd . q_ult = ( sl . cohesion * fd . nc_factor * 
( 1 + s_c + d_c - i_c - g_c - b_c ) + q_d ) 
s_c * d_c * i_c * g_c * b_c + 
0.5 * fd . width * sl . unit_dry_weight * 
~~ ~~ def capacity_meyerhof_1963 ( sl , fd , gwl = 1e6 , h_l = 0 , h_b = 0 , vertical_load = 1 , verbose = 0 , ** kwargs ) : 
~~ horizontal_load = np . sqrt ( h_l ** 2 + h_b ** 2 ) 
fd . nq_factor = ( ( np . tan ( np . pi / 4 + sl . phi_r / 2 ) ) ** 2 * 
np . exp ( np . pi * np . tan ( sl . phi_r ) ) ) 
~~ fd . ng_factor = ( fd . nq_factor - 1 ) * np . tan ( 1.4 * sl . phi_r ) 
~~ kp = ( np . tan ( np . pi / 4 + sl . phi_r / 2 ) ) ** 2 
s_c = 1 + 0.2 * kp * fd . width / fd . length 
if sl . phi > 10 : 
~~~ s_q = 1.0 + 0.1 * kp * fd . width / fd . length 
~~~ s_q = 1.0 
~~ s_g = s_q 
d_c = 1 + 0.2 * np . sqrt ( kp ) * fd . depth / fd . width 
~~~ d_q = 1 + 0.1 * np . sqrt ( kp ) * fd . depth / fd . width 
~~~ d_q = 1.0 
~~ d_g = d_q 
theta_load = np . arctan ( horizontal_load / vertical_load ) 
i_c = ( 1 - theta_load / ( np . pi * 0.5 ) ) ** 2 
i_q = i_c 
if sl . phi > 0 : 
~~~ i_g = ( 1 - theta_load / sl . phi_r ) ** 2 
~~~ i_g = 0 
~~ if gwl == 0 : 
~~~ q_d = sl . unit_bouy_weight * fd . depth 
~~ fd . q_ult = ( sl . cohesion * fd . nc_factor * s_c * d_c * i_c + 
q_d * fd . nq_factor * s_q * d_q * i_q + 
fd . ng_factor * s_g * d_g * i_g ) 
return fd . q_ult 
~~ def capacity_nzs_vm4_2011 ( sl , fd , h_l = 0 , h_b = 0 , vertical_load = 1 , slope = 0 , verbose = 0 , ** kwargs ) : 
h_eff_b = kwargs . get ( "h_eff_b" , 0 ) 
h_eff_l = kwargs . get ( "h_eff_l" , 0 ) 
loc_v_l = kwargs . get ( "loc_v_l" , fd . length / 2 ) 
loc_v_b = kwargs . get ( "loc_v_b" , fd . width / 2 ) 
ecc_b = h_b * h_eff_b / vertical_load 
ecc_l = h_l * h_eff_l / vertical_load 
width_eff = min ( fd . width , 2 * 
( loc_v_b + ecc_b ) , 2 * ( fd . width - loc_v_b - ecc_b ) ) 
length_eff = min ( fd . length , 2 * 
( loc_v_l + ecc_l ) , 2 * ( fd . length - loc_v_l - ecc_l ) ) 
area_foundation = length_eff * width_eff 
if width_eff / 2 < fd . width / 6 : 
~~ fd . nq_factor = ( ( np . tan ( np . pi / 4 + sl . phi_r / 2 ) ) ** 2 * np . exp ( np . pi * np . tan ( sl . phi_r ) ) ) 
~~ fd . ng_factor = 2.0 * ( fd . nq_factor - 1 ) * np . tan ( sl . phi_r ) 
s_c = 1.0 + fd . nq_factor / fd . nc_factor * width_eff / length_eff 
s_q = 1 + width_eff / length_eff * np . tan ( sl . phi_r ) 
if fd . depth / width_eff > 1 : 
~~~ k = np . arctan ( fd . depth / width_eff ) 
~~~ k = fd . depth / width_eff 
~~~ d_c = 1 + 0.4 * k 
d_q = 1.0 
~~~ d_q = ( 1 + 2 * np . tan ( sl . phi_r ) * 
( 1 - np . sin ( sl . phi_r ) ) ** 2 * k ) 
d_c = d_q - ( 1 - d_q ) / ( fd . nq_factor * np . tan ( sl . phi_r ) ) 
~~ d_g = 1.0 
~~~ i_c = 0.5 * ( 1 + np . sqrt ( 1 - horizontal_load / ( area_foundation * sl . cohesion ) ) ) 
i_q = 1.0 
~~~ if h_b == 0 : 
~~~ i_q = 1 - horizontal_load / ( vertical_load + area_foundation * sl . cohesion / 
np . tan ( sl . phi_r ) ) 
i_g = i_q 
~~ elif h_b > 0 and h_l == 0 : 
~~~ i_q = ( ( 1 - 0.7 * horizontal_load / ( vertical_load + area_foundation * sl . cohesion / 
np . tan ( sl . phi_r ) ) ) ** 3 ) 
i_g = ( ( 1 - horizontal_load / ( vertical_load + area_foundation * sl . cohesion / 
~~ i_c = ( i_q * fd . nq_factor - 1 ) / ( fd . nq_factor - 1 ) 
~~ g_c = 1 - slope * ( 1.0 - fd . depth / ( 2 * width_eff ) ) / 150 
g_q = ( 1 - np . tan ( slope * ( 1 - fd . depth / ( 2 * width_eff ) ) ) ) ** 2 
~~ fd . q_ult = ( sl . cohesion * fd . nc_factor * s_c * d_c * i_c * g_c + 
q_d * fd . nq_factor * s_q * d_q * i_q * g_q + 
0.5 * width_eff * sl . unit_dry_weight * 
fd . ng_factor * s_g * d_g * i_g * g_g ) 
~~ def capacity_salgado_2008 ( sl , fd , h_l = 0 , h_b = 0 , vertical_load = 1 , verbose = 0 , ** kwargs ) : 
~~ h_eff_b = kwargs . get ( "h_eff_b" , 0 ) 
width_eff = min ( fd . width , 2 * ( loc_v_b + ecc_b ) , 2 * ( fd . width - loc_v_b - ecc_b ) ) 
length_eff = min ( fd . length , 2 * ( loc_v_l + ecc_l ) , 2 * ( fd . length - loc_v_l - ecc_l ) ) 
~~ fd . nq_factor = np . exp ( np . pi * np . tan ( sl . phi_r ) ) * ( 1 + np . sin ( sl . phi_r ) ) / ( 1 - np . sin ( sl . phi_r ) ) 
fd . ng_factor = 1.5 * ( fd . nq_factor - 1 ) * np . tan ( sl . phi_r ) 
~~ s_q = 1 + ( width_eff / length_eff ) * np . tan ( sl . phi_r ) 
s_g = max ( 1 - 0.4 * width_eff / length_eff , 0.6 ) 
s_c = 1.0 
d_q = 1 + 2 * np . tan ( sl . phi_r ) * ( 1 - np . sin ( sl . phi_r ) ) ** 2 * fd . depth / width_eff 
d_c = 1.0 
~~ fd . q_ult = ( sl . cohesion * fd . nc_factor * s_c * d_c + 
q_d * fd . nq_factor * s_q * d_q + 
fd . ng_factor * s_g * d_g ) 
~~ def size_footing_for_capacity ( sl , vertical_load , fos = 1.0 , length_to_width = 1.0 , verbose = 0 , ** kwargs ) : 
method = kwargs . get ( "method" , 'vesics' ) 
depth_to_width = kwargs . get ( "depth_to_width" , 0 ) 
depth = kwargs . get ( "depth" , 0 ) 
use_depth_to_width = 0 
if not depth : 
~~~ use_depth_to_width = 1 
~~ fd = models . FoundationRaft ( ) 
for i in range ( 50 ) : 
~~~ fd . length = length_to_width * fd . width 
if use_depth_to_width : 
~~~ fd . depth = depth_to_width * fd . width 
~~ capacity_method_selector ( sl , fd , method ) 
q = fd . q_ult 
bearing_capacity = q * fd . length * fd . width 
fs_actual = bearing_capacity / vertical_load 
if fs_actual < fos : 
~~~ fd . width += 0.5 
~~ ~~ width_array = [ ] 
fs_array = [ ] 
for j in range ( 11 ) : 
~~~ width_array . append ( fd . width ) 
fd . length = length_to_width * fd . width 
capacity = q * fd . length * fd . width 
fs_array . append ( capacity / vertical_load ) 
fd . width = fd . width - 0.5 / 10 
~~ for fs in range ( len ( fs_array ) ) : 
~~~ if fs_array [ fs ] < fos : 
~~~ fd . width = width_array [ fs - 1 ] 
~~ if fs == len ( fs_array ) - 1 : 
~~ ~~ return fd 
~~ def capacity_method_selector ( sl , fd , method , ** kwargs ) : 
if method == 'vesics' : 
~~~ capacity_vesics_1975 ( sl , fd , ** kwargs ) 
~~ elif method == 'nzs' : 
~~~ capacity_nzs_vm4_2011 ( sl , fd , ** kwargs ) 
~~ elif method == 'terzaghi' : 
~~~ capacity_terzaghi_1943 ( sl , fd , ** kwargs ) 
~~ elif method == 'hansen' : 
~~~ capacity_hansen_1970 ( sl , fd , ** kwargs ) 
~~ elif method == 'meyerhoff' : 
~~~ capacity_meyerhof_1963 ( sl , fd , ** kwargs ) 
~~ elif method == 'salgado' : 
~~~ capacity_salgado_2008 ( sl , fd , ** kwargs ) 
~~ ~~ def deprecated_capacity_meyerhof_and_hanna_1978 ( sl_0 , sl_1 , h0 , fd , verbose = 0 ) : 
sl_0 . nq_factor_0 = ( 
( np . tan ( np . pi / 4 + np . deg2rad ( sl_0 . phi / 2 ) ) ) ** 2 * np . exp ( np . pi * np . tan ( np . deg2rad ( sl_0 . phi ) ) ) ) 
if sl_0 . phi == 0 : 
~~~ sl_0 . nc_factor_0 = 5.14 
~~~ sl_0 . nc_factor_0 = ( sl_0 . nq_factor_0 - 1 ) / np . tan ( np . deg2rad ( sl_0 . phi ) ) 
~~ sl_0 . ng_factor_0 = ( sl_0 . nq_factor_0 - 1 ) * np . tan ( 1.4 * np . deg2rad ( sl_0 . phi ) ) 
sl_1 . nq_factor_1 = ( 
( np . tan ( np . pi / 4 + np . deg2rad ( sl_1 . phi / 2 ) ) ) ** 2 * np . exp ( np . pi * np . tan ( np . deg2rad ( sl_1 . phi ) ) ) ) 
if sl_1 . phi == 0 : 
~~~ sl_1 . nc_factor_1 = 5.14 
~~~ sl_1 . nc_factor_1 = ( sl_1 . nq_factor_1 - 1 ) / np . tan ( np . deg2rad ( sl_1 . phi ) ) 
~~ sl_1 . ng_factor_1 = ( sl_1 . nq_factor_1 - 1 ) * np . tan ( 1.4 * np . deg2rad ( sl_1 . phi ) ) 
~~ sl_0 . kp_0 = ( np . tan ( np . pi / 4 + np . deg2rad ( sl_0 . phi / 2 ) ) ) ** 2 
sl_1 . kp_1 = ( np . tan ( np . pi / 4 + np . deg2rad ( sl_1 . phi / 2 ) ) ) ** 2 
if sl_0 . phi >= 10 : 
~~~ sl_0 . s_c_0 = 1 + 0.2 * sl_0 . kp_0 * ( fd . width / fd . length ) 
sl_0 . s_q_0 = 1.0 + 0.1 * sl_0 . kp_0 * ( fd . width / fd . length ) 
~~~ sl_0 . s_c_0 = 1 + 0.2 * ( fd . width / fd . length ) 
sl_0 . s_q_0 = 1.0 
~~ sl_0 . s_g_0 = sl_0 . s_q_0 
if sl_1 . phi >= 10 : 
~~~ sl_1 . s_c_1 = 1 + 0.2 * sl_1 . kp_1 * ( fd . width / fd . length ) 
sl_1 . s_q_1 = 1.0 + 0.1 * sl_1 . kp_1 * ( fd . width / fd . length ) 
~~~ sl_1 . s_c_1 = 1 + 0.2 * ( fd . width / fd . length ) 
sl_1 . s_q_1 = 1.0 
~~ sl_1 . s_g_1 = sl_1 . s_q_1 
sl_0 . q_0 = ( sl_0 . cohesion * sl_0 . nc_factor_0 ) + ( 0.5 * sl_0 . unit_dry_weight * fd . width * sl_0 . ng_factor_0 ) 
sl_1 . q_1 = ( sl_1 . cohesion * sl_1 . nc_factor_1 ) + ( 0.5 * sl_1 . unit_dry_weight * fd . width * sl_1 . ng_factor_1 ) 
q1_q0 = sl_1 . q_1 / sl_0 . q_0 
x_0 = np . array ( [ 0 , 20.08 , 22.42 , 25.08 , 27.58 , 30.08 , 32.58 , 34.92 , 37.83 , 40.00 , 42.67 , 45.00 , 47.00 , 49.75 ] ) 
y_0 = np . array ( [ 0.93 , 0.93 , 0.93 , 0.93 , 1.01 , 1.17 , 1.32 , 1.56 , 1.87 , 2.26 , 2.72 , 3.35 , 3.81 , 4.82 ] ) 
x_2 = np . array ( [ 0 , 20.08 , 22.50 , 25.08 , 27.58 , 30.08 , 32.50 , 35.00 , 37.67 , 40.17 , 42.67 , 45.00 , 47.50 , 50.00 ] ) 
y_2 = np . array ( [ 1.55 , 1.55 , 1.71 , 1.86 , 2.10 , 2.33 , 2.72 , 3.11 , 3.81 , 4.43 , 5.28 , 6.14 , 7.46 , 9.24 ] ) 
x_4 = np . array ( [ 0 , 20.00 , 22.51 , 25.10 , 27.69 , 30.11 , 32.45 , 35.04 , 37.88 , 40.14 , 42.65 , 45.07 , 47.33 , 50.08 ] ) 
y_4 = np . array ( [ 2.49 , 2.49 , 2.64 , 2.87 , 3.34 , 3.81 , 4.43 , 5.20 , 6.29 , 7.38 , 9.01 , 11.11 , 14.29 , 19.34 ] ) 
x_10 = np . array ( [ 0 , 20.00 , 22.50 , 25.08 , 28.00 , 30.00 , 32.50 , 34.92 , 37.50 , 40.17 , 42.42 , 45.00 , 47.17 , 50.08 ] ) 
y_10 = np . array ( [ 3.27 , 3.27 , 3.74 , 4.44 , 5.37 , 6.07 , 7.16 , 8.33 , 10.04 , 12.30 , 15.95 , 21.17 , 27.47 , 40.00 ] ) 
x_int = sl_0 . phi 
if sl_0 . phi < 1 : 
~~~ fd . ks = 0 
~~~ if q1_q0 == 0 : 
~~~ fd . ks = np . interp ( x_int , x_0 , y_0 ) 
~~ elif q1_q0 == 0.2 : 
~~~ fd . ks = np . interp ( x_int , x_2 , y_2 ) 
~~ elif q1_q0 == 0.4 : 
~~~ fd . ks = np . interp ( x_int , x_4 , y_4 ) 
~~ elif q1_q0 == 1.0 : 
~~~ fd . ks = np . interp ( x_int , x_10 , y_10 ) 
~~ elif 0 < q1_q0 < 0.2 : 
~~~ ks_1 = np . interp ( x_int , x_0 , y_0 ) 
ks_2 = np . interp ( x_int , x_2 , y_2 ) 
fd . ks = ( ( ( ks_2 - ks_1 ) * q1_q0 ) / 0.2 ) + ks_1 
~~ elif 0.2 < q1_q0 < 0.4 : 
~~~ ks_1 = np . interp ( x_int , x_2 , y_2 ) 
ks_2 = np . interp ( x_int , x_4 , y_4 ) 
fd . ks = ( ( ( ks_2 - ks_1 ) * ( q1_q0 - 0.2 ) ) / 0.2 ) + ks_1 
~~ elif 0.4 < q1_q0 < 1.0 : 
~~~ ks_1 = np . interp ( x_int , x_4 , y_4 ) 
ks_2 = np . interp ( x_int , x_10 , y_10 ) 
fd . ks = ( ( ( ks_2 - ks_1 ) * ( q1_q0 - 0.4 ) ) / 0.6 ) + ks_1 
~~ ~~ if sl_0 . cohesion == 0 : 
~~~ c1_c0 = 0 
~~~ c1_c0 = sl_1 . cohesion / sl_0 . cohesion 
~~ x = np . array ( [ 0.000 , 0.082 , 0.206 , 0.298 , 0.404 , 0.509 , 0.598 , 0.685 , 0.772 ] ) 
y = np . array ( [ 0.627 , 0.700 , 0.794 , 0.855 , 0.912 , 0.948 , 0.968 , 0.983 , 0.997 ] ) 
ca_c0 = np . interp ( c1_c0 , x , y ) 
fd . ca = ca_c0 * sl_0 . cohesion 
r = 1 + ( fd . width / fd . length ) 
q_b1 = ( sl_1 . cohesion * sl_1 . nc_factor_1 * sl_1 . s_c_1 ) 
q_b2 = ( sl_0 . unit_dry_weight * h0 * sl_1 . nq_factor_1 * sl_1 . s_q_1 ) 
q_b3 = ( sl_1 . unit_dry_weight * fd . width * sl_1 . ng_factor_1 * sl_1 . s_g_1 / 2 ) 
fd . q_b = q_b1 + q_b2 + q_b3 
fd . q_ult4 = ( r * ( 2 * fd . ca * ( h0 - fd . depth ) / fd . width ) * a ) 
fd . q_ult5 = r * ( sl_0 . unit_dry_weight * ( ( h0 - fd . depth ) ** 2 ) ) * ( 1 + ( 2 * fd . depth / ( h0 - fd . depth ) ) ) * ( 
fd . ks * np . tan ( np . deg2rad ( sl_0 . phi ) ) / fd . width ) * s 
fd . q_ult6 = ( sl_0 . unit_dry_weight * ( h0 - fd . depth ) ) 
fd . q_ult = fd . q_b + fd . q_ult4 + fd . q_ult5 - fd . q_ult6 
q_t1 = ( sl_0 . cohesion * sl_0 . nc_factor_0 * sl_0 . s_c_0 ) 
q_t2 = ( sl_0 . unit_dry_weight * fd . depth * sl_0 . nq_factor_0 * sl_0 . s_q_0 ) 
q_t3 = ( sl_0 . unit_dry_weight * fd . width * sl_0 . ng_factor_0 * sl_0 . s_g_0 / 2 ) 
fd . q_t = q_t1 + q_t2 + q_t3 
if fd . q_ult > fd . q_t : 
~~~ fd . q_ult = fd . q_t 
~~ def capacity_meyerhof_and_hanna_1978 ( sl_0 , sl_1 , h0 , fd , gwl = 1e6 , verbose = 0 ) : 
sp = sm . SoilProfile ( ) 
sp . add_layer ( 0 , sl_0 ) 
sp . add_layer ( h0 , sl_1 ) 
sp . gwl = gwl 
return capacity_sp_meyerhof_and_hanna_1978 ( sp , fd ) 
~~ def capacity_sp_meyerhof_and_hanna_1978 ( sp , fd , verbose = 0 ) : 
assert isinstance ( sp , sm . SoilProfile ) 
sl_0 = sp . layer ( 1 ) 
sl_1 = sp . layer ( 2 ) 
h0 = sp . layer_depth ( 2 ) 
gwl = sp . gwl 
s = 1 
~~~ q_at_interface = sl_0 . unit_bouy_weight * h0 
unit_eff_weight_0_at_fd_depth = sl_0 . unit_bouy_weight 
unit_eff_weight_0_at_interface = sl_0 . unit_bouy_weight 
unit_eff_weight_1_below_foundation = sl_1 . unit_bouy_weight 
~~~ q_at_interface = ( sl_0 . unit_dry_weight * gwl ) + ( sl_0 . unit_bouy_weight * ( h0 - gwl ) ) 
q_d = ( sl_0 . unit_dry_weight * gwl ) + ( sl_0 . unit_bouy_weight * ( fd . depth - gwl ) ) 
unit_eff_weight_0_at_fd_depth = q_d / fd . depth 
~~ elif fd . depth < gwl <= fd . width + fd . depth : 
~~~ average_unit_bouy_weight = sl_0 . unit_bouy_weight + ( 
( ( gwl - fd . depth ) / fd . width ) * ( sl_0 . unit_dry_weight - sl_0 . unit_bouy_weight ) ) 
q_at_interface = ( sl_0 . unit_dry_weight * gwl ) + ( sl_0 . unit_bouy_weight * ( h0 - gwl ) ) 
unit_eff_weight_0_at_fd_depth = sl_0 . unit_dry_weight 
unit_eff_weight_0_at_interface = average_unit_bouy_weight 
~~~ average_unit_bouy_weight = sl_1 . unit_bouy_weight + ( 
( ( gwl - h0 ) / fd . width ) * ( sl_1 . unit_dry_weight - sl_1 . unit_bouy_weight ) ) 
q_at_interface = sl_0 . unit_dry_weight * h0 
unit_eff_weight_0_at_interface = sl_0 . unit_dry_weight 
unit_eff_weight_1_below_foundation = average_unit_bouy_weight 
~~~ q_at_interface = sl_0 . unit_dry_weight * h0 
unit_eff_weight_1_below_foundation = sl_1 . unit_dry_weight 
~~ q_ult6 = q_at_interface - unit_eff_weight_0_at_fd_depth * fd . depth 
q_0 = ( sl_0 . cohesion * sl_0 . nc_factor_0 ) + ( 0.5 * unit_eff_weight_0_at_interface * fd . width * sl_0 . ng_factor_0 ) 
q_b2 = ( q_at_interface * sl_1 . nq_factor_1 * sl_1 . s_q_1 ) 
q_1 = ( sl_1 . cohesion * sl_1 . nc_factor_1 ) + ( 0.5 * unit_eff_weight_1_below_foundation * fd . width * sl_1 . ng_factor_1 ) 
q_b3 = ( unit_eff_weight_1_below_foundation * fd . width * sl_1 . ng_factor_1 * sl_1 . s_g_1 / 2 ) 
q_ult5 = r * ( unit_eff_weight_0_at_interface * ( ( h0 - fd . depth ) ** 2 ) ) * ( 1 + ( 2 * fd . depth / ( h0 - fd . depth ) ) ) * ( 
np . tan ( np . deg2rad ( sl_0 . phi ) ) / fd . width ) * s 
q_t2 = ( unit_eff_weight_0_at_fd_depth * fd . depth * sl_0 . nq_factor_0 * sl_0 . s_q_0 ) 
q_t3 = ( unit_eff_weight_0_at_interface * fd . width * sl_0 . ng_factor_0 * sl_0 . s_g_0 / 2 ) 
q_b = q_b1 + q_b2 + q_b3 
q1_q0 = q_1 / q_0 
x = np . array ( [ 0.000 , 0.082 , 0.206 , 0.298 , 0.404 , 0.509 , 0.598 , 0.685 , 0.772 ] ) 
ca_c0 = np . interp ( q1_q0 , x , y ) 
ca = ca_c0 * sl_0 . cohesion 
~~~ raise DesignError ( 
~~ ~~ q_ult4 = ( r * ( 2 * ca * ( h0 - fd . depth ) / fd . width ) * a ) 
q_ult5_ks = q_ult5 * fd . ks 
q_ult = q_b + q_ult4 + q_ult5_ks - q_ult6 
q_t = q_t1 + q_t2 + q_t3 
if q_ult > q_t : 
~~~ if h0 > fd . width / 2 : 
~~~ fd . q_ult = q_t 
~~~ vert_eff_stress_interface = sp . vertical_effective_stress ( h0 ) 
vert_eff_stress_lowest = sp . vertical_effective_stress ( fd . width + fd . depth ) 
average_eff_stress = ( vert_eff_stress_interface + vert_eff_stress_lowest ) / 2 
c_2_eff = sl_1 . cohesion + average_eff_stress * np . tan ( np . radians ( sl_1 . phi ) ) 
if sl_0 . cohesion > c_2_eff : 
~~~ h_over_b = ( h0 - fd . depth ) / fd . width 
c1_over_c2 = sl_0 . cohesion / c_2_eff 
c_1_over_c_2 = [ 0.1 , 0.2 , 0.25 , 0.333 , 0.5 , 0.667 , 1. ] 
m_1 = [ 1.584 , 1.444 , 1.389 , 1.311 , 1.193 , 1.109 , 1. ] 
m_125 = [ 1.446 , 1.342 , 1.302 , 1.241 , 1.152 , 1.088 , 1. ] 
m_167 = [ 1.302 , 1.235 , 1.208 , 1.167 , 1.107 , 1.064 , 1. ] 
m_25 = [ 1.154 , 1.121 , 1.107 , 1.088 , 1.056 , 1.033 , 1. ] 
m_5 = [ 1 , 1 , 1 , 1 , 1 , 1 , 1 ] 
if h_over_b == 0.1 : 
~~~ m = np . interp ( c1_over_c2 , c_1_over_c_2 , m_1 ) 
~~ elif h_over_b == 0.125 : 
~~~ m = np . interp ( c1_over_c2 , c_1_over_c_2 , m_125 ) 
~~ elif h_over_b == 0.167 : 
~~~ m = np . interp ( c1_over_c2 , c_1_over_c_2 , m_167 ) 
~~ elif h_over_b == 0.250 : 
~~~ m = np . interp ( c1_over_c2 , c_1_over_c_2 , m_25 ) 
~~ elif h_over_b >= 0.5 : 
~~~ m = np . interp ( c1_over_c2 , c_1_over_c_2 , m_5 ) 
~~ elif 0.1 < h_over_b < 0.125 : 
~~~ m_a = np . interp ( c1_over_c2 , c_1_over_c_2 , m_1 ) 
m_b = np . interp ( c1_over_c2 , c_1_over_c_2 , m_125 ) 
m = np . interp ( h_over_b , [ 0.1 , 0.125 ] , [ m_a , m_b ] ) 
~~ elif 0.125 < h_over_b < 0.167 : 
~~~ m_a = np . interp ( c1_over_c2 , c_1_over_c_2 , m_125 ) 
m_b = np . interp ( c1_over_c2 , c_1_over_c_2 , m_167 ) 
m = np . interp ( h_over_b , [ 0.125 , 0.167 ] , [ m_a , m_b ] ) 
~~ elif 0.167 < h_over_b < 0.25 : 
~~~ m_a = np . interp ( c1_over_c2 , c_1_over_c_2 , m_167 ) 
m_b = np . interp ( c1_over_c2 , c_1_over_c_2 , m_25 ) 
m = np . interp ( h_over_b , [ 0.167 , 0.250 ] , [ m_a , m_b ] ) 
~~ elif 0.25 < h_over_b < 0.5 : 
~~~ m_a = np . interp ( c1_over_c2 , c_1_over_c_2 , m_25 ) 
m_b = np . interp ( c1_over_c2 , c_1_over_c_2 , m_5 ) 
m = np . interp ( h_over_b , [ 0.250 , 0.500 ] , [ m_a , m_b ] ) 
~~ fd . q_ult = ( sl_0 . cohesion * m * sl_0 . nc_factor_0 ) + ( unit_eff_weight_0_at_fd_depth * fd . depth ) 
~~~ fd . q_ult = q_ult 
~~ def get_roles_in_account ( self , account_id , params = { } ) : 
url = ACCOUNTS_API . format ( account_id ) + "/roles" 
roles = [ ] 
for datum in self . _get_resource ( url , params = params ) : 
~~~ roles . append ( CanvasRole ( data = datum ) ) 
~~ return roles 
~~ def get_roles_by_account_sis_id ( self , account_sis_id , params = { } ) : 
return self . get_roles_in_account ( self . _sis_id ( account_sis_id , 
sis_field = "account" ) , 
params ) 
~~ def get_effective_course_roles_in_account ( self , account_id ) : 
course_roles = [ ] 
params = { "show_inherited" : "1" } 
for role in self . get_roles_in_account ( account_id , params ) : 
~~~ if role . base_role_type != "AccountMembership" : 
~~~ course_roles . append ( role ) 
~~ ~~ return course_roles 
~~ def get_role ( self , account_id , role_id ) : 
url = ACCOUNTS_API . format ( account_id ) + "/roles/{}" . format ( role_id ) 
return CanvasRole ( data = self . _get_resource ( url ) ) 
~~ def get_role_by_account_sis_id ( self , account_sis_id , role_id ) : 
return self . get_role ( self . _sis_id ( account_sis_id , sis_field = "account" ) , 
role_id ) 
~~ def get_course ( self , course_id , params = { } ) : 
include = params . get ( "include" , [ ] ) 
if "term" not in include : 
~~~ include . append ( "term" ) 
~~ params [ "include" ] = include 
url = COURSES_API . format ( course_id ) 
return CanvasCourse ( data = self . _get_resource ( url , params = params ) ) 
~~ def get_course_by_sis_id ( self , sis_course_id , params = { } ) : 
return self . get_course ( self . _sis_id ( sis_course_id , sis_field = "course" ) , 
~~ def get_courses_in_account ( self , account_id , params = { } ) : 
if "published" in params : 
~~~ params [ "published" ] = "true" if params [ "published" ] else "" 
~~ url = ACCOUNTS_API . format ( account_id ) + "/courses" 
courses = [ ] 
for data in self . _get_paged_resource ( url , params = params ) : 
~~~ courses . append ( CanvasCourse ( data = data ) ) 
~~ return courses 
~~ def get_courses_in_account_by_sis_id ( self , sis_account_id , params = { } ) : 
return self . get_courses_in_account ( 
self . _sis_id ( sis_account_id , sis_field = "account" ) , params ) 
~~ def get_published_courses_in_account ( self , account_id , params = { } ) : 
params [ "published" ] = True 
return self . get_courses_in_account ( account_id , params ) 
~~ def get_published_courses_in_account_by_sis_id ( self , sis_account_id , 
params = { } ) : 
return self . get_published_courses_in_account ( 
~~ def get_courses_for_regid ( self , regid , params = { } ) : 
self . _as_user = regid 
data = self . _get_resource ( "/api/v1/courses" , params = params ) 
self . _as_user = None 
for datum in data : 
~~~ if "sis_course_id" in datum : 
~~~ courses . append ( CanvasCourse ( data = datum ) ) 
~~~ courses . append ( self . get_course ( datum [ "id" ] , params ) ) 
~~ ~~ return courses 
~~ def create_course ( self , account_id , course_name ) : 
url = ACCOUNTS_API . format ( account_id ) + "/courses" 
body = { "course" : { "name" : course_name } } 
return CanvasCourse ( data = self . _post_resource ( url , body ) ) 
~~ def update_sis_id ( self , course_id , sis_course_id ) : 
body = { "course" : { "sis_course_id" : sis_course_id } } 
return CanvasCourse ( data = self . _put_resource ( url , body ) ) 
~~ def get_activity_by_account ( self , account_id , term_id ) : 
url = ( "/api/v1/accounts/sis_account_id:%s/analytics/" 
"terms/sis_term_id:%s/activity.json" ) % ( account_id , term_id ) 
return self . _get_resource ( url ) 
~~ def get_grades_by_account ( self , account_id , term_id ) : 
"terms/sis_term_id:%s/grades.json" ) % ( account_id , term_id ) 
~~ def get_statistics_by_account ( self , account_id , term_id ) : 
"terms/sis_term_id:%s/statistics.json" ) % ( account_id , term_id ) 
~~ def get_activity_by_sis_course_id ( self , sis_course_id ) : 
url = "/api/v1/courses/%s/analytics/activity.json" % ( 
self . _sis_id ( sis_course_id , sis_field = "course" ) ) 
~~ def get_assignments_by_sis_course_id ( self , sis_course_id ) : 
url = "/api/v1/courses/%s/analytics/assignments.json" % ( 
~~ def get_student_summaries_by_sis_course_id ( self , sis_course_id ) : 
url = "/api/v1/courses/%s/analytics/student_summaries.json" % ( 
~~ def get_student_activity_for_sis_course_id_and_sis_user_id ( 
self , sis_user_id , sis_course_id ) : 
url = ( "/api/v1/courses/%s/analytics/users/" 
"sis_user_id:%s/activity.json" ) % ( 
self . _sis_id ( sis_course_id , sis_field = "course" ) , sis_user_id ) 
~~ def get_student_assignments_for_sis_course_id_and_sis_user_id ( 
url = ( "/api/v1/courses/%s/analytics/" 
"users/sis_user_id:%s/assignments.json" ) % ( 
~~ def get_student_assignments_for_sis_course_id_and_canvas_user_id ( 
self , sis_course_id , user_id ) : 
url = "/api/v1/courses/%s/analytics/users/%s/assignments.json" % ( 
self . _sis_id ( sis_course_id , sis_field = "course" ) , user_id ) 
~~ def get_student_messaging_for_sis_course_id_and_sis_user_id ( 
"users/sis_user_id:%s/communication.json" ) % ( 
~~ def get_submissions_by_course_and_assignment ( 
self , course_id , assignment_id , params = { } ) : 
url += "/assignments/{}/submissions" . format ( assignment_id ) 
~~~ submissions . append ( Submission ( data = data ) ) 
~~ def get_submissions_multiple_assignments_by_sis_id ( 
self , is_section , sis_id , students = None , assignments = None , 
** params ) : 
if is_section : 
~~~ return self . get_submissions_multiple_assignments ( 
is_section , self . _sis_id ( sis_id , 'section' ) , students , 
assignments , ** params ) 
is_section , self . _sis_id ( sis_id , 'course' ) , students , 
~~ ~~ def get_submissions_multiple_assignments ( 
self , is_section , course_id , students = None , assignments = None , 
api = SECTIONS_API if is_section else COURSES_API 
if students is not None : 
~~~ params [ 'student_ids' ] = students 
~~ if assignments is not None : 
~~~ params [ 'assignment_ids' ] = assignments 
~~ url = api . format ( course_id ) + "/students/submissions" 
data = self . _get_paged_resource ( url , params = params ) 
for submission in data : 
~~~ submissions . append ( Submission ( data = submission ) ) 
~~ def rotational_stiffness ( sl , fd , axis = "length" , a0 = 0.0 , ** kwargs ) : 
~~~ gf . models . check_required ( sl , [ "g_mod" , "poissons_ratio" ] ) 
gf . models . check_required ( fd , [ "length" , "width" , "depth" ] ) 
~~ if fd . depth > 0.0 : 
~~ l = fd . length * 0.5 
b = fd . width * 0.5 
v = sl . poissons_ratio 
if axis == "length" : 
~~~ i_bx = fd . i_ll 
k_rx = 1 - 0.2 * a0 
k_f_0 = ( sl . g_mod / ( 1 - v ) * i_bx ** 0.75 * ( l / b ) ** 0.25 * ( 2.4 + 0.5 * ( b / l ) ) ) * k_rx 
~~~ i_by = fd . i_ww 
k_ry = 1 - 0.3 * a0 
k_f_0 = ( sl . g_mod / ( 1 - v ) * i_by ** 0.75 * ( 3 * ( l / b ) ** 0.15 ) ) * k_ry 
~~ return k_f_0 
~~ def get_external_tools_in_account ( self , account_id , params = { } ) : 
url = ACCOUNTS_API . format ( account_id ) + "/external_tools" 
external_tools = [ ] 
~~~ external_tools . append ( data ) 
~~ return external_tools 
~~ def get_external_tools_in_course ( self , course_id , params = { } ) : 
url = COURSES_API . format ( course_id ) + "/external_tools" 
~~ def _create_external_tool ( self , context , context_id , json_data ) : 
url = context . format ( context_id ) + "/external_tools" 
return self . _post_resource ( url , body = json_data ) 
~~ def _update_external_tool ( self , context , context_id , external_tool_id , 
json_data ) : 
url = context . format ( context_id ) + "/external_tools/{}" . format ( 
external_tool_id ) 
return self . _put_resource ( url , body = json_data ) 
~~ def _delete_external_tool ( self , context , context_id , external_tool_id ) : 
response = self . _delete_resource ( url ) 
~~ def _get_sessionless_launch_url ( self , context , context_id , tool_id ) : 
url = context . format ( context_id ) + "/external_tools/sessionless_launch" 
params = { "id" : tool_id } 
return self . _get_resource ( url , params ) 
~~ def get_sessionless_launch_url_from_account_sis_id ( 
self , tool_id , account_sis_id ) : 
return self . get_sessionless_launch_url_from_account ( 
tool_id , self . _sis_id ( account_sis_id , "account" ) ) 
~~ def get_sessionless_launch_url_from_course_sis_id ( 
self , tool_id , course_sis_id ) : 
return self . get_sessionless_launch_url_from_course ( 
tool_id , self . _sis_id ( course_sis_id , "course" ) ) 
~~ def create_foundation ( length , width , depth = 0.0 , height = 0.0 ) : 
a_foundation = FoundationRaft ( ) 
a_foundation . length = length 
a_foundation . width = width 
a_foundation . depth = depth 
a_foundation . height = height 
return a_foundation 
~~ def create_soil ( phi = 0.0 , cohesion = 0.0 , unit_dry_weight = 0.0 , pw = 9800 ) : 
soil = Soil ( pw = pw ) 
soil . phi = phi 
soil . cohesion = cohesion 
soil . unit_dry_weight = unit_dry_weight 
return soil 
~~ def check_required ( obj , required_parameters ) : 
for parameter in required_parameters : 
~~~ if not hasattr ( obj , parameter ) or getattr ( obj , parameter ) is None : 
~~ ~~ ~~ def get_user ( self , user_id ) : 
url = USERS_API . format ( user_id ) + "/profile" 
return CanvasUser ( data = self . _get_resource ( url ) ) 
~~ def get_users_for_course ( self , course_id , params = { } ) : 
url = COURSES_API . format ( course_id ) + "/users" 
users = [ ] 
~~~ users . append ( CanvasUser ( data = datum ) ) 
~~ return users 
~~ def get_users_for_sis_course_id ( self , sis_course_id , params = { } ) : 
return self . get_users_for_course ( 
~~ def create_user ( self , user , account_id = None ) : 
if account_id is None : 
~~~ account_id = self . _canvas_account_id 
~~~ raise MissingAccountID ( ) 
~~ ~~ url = ACCOUNTS_API . format ( account_id ) + "/users" 
data = self . _post_resource ( url , user . post_data ( ) ) 
return CanvasUser ( data = data ) 
~~ def get_user_logins ( self , user_id , params = { } ) : 
url = USERS_API . format ( user_id ) + "/logins" 
logins = [ ] 
for login_data in data : 
~~~ logins . append ( Login ( data = login_data ) ) 
~~ return logins 
~~ def update_user_login ( self , login , account_id = None ) : 
~~~ raise MissingAccountID 
~~ ~~ login_id = login . login_id 
url = ACCOUNTS_API . format ( account_id ) + "/logins/{}" . format ( login_id ) 
data = self . _put_resource ( url , login . put_data ( ) ) 
return Login ( data = data ) 
~~ def _next_page ( self , response ) : 
for link in response . getheader ( "link" , "" ) . split ( "," ) : 
~~~ ( url , rel ) = link . split ( ";" ) 
if "next" in rel : 
~~~ return url . lstrip ( "<" ) . rstrip ( ">" ) 
~~ ~~ ~~ def _get_resource_url ( self , url , auto_page , data_key ) : 
headers = { 'Accept' : 'application/json' , 
'Connection' : 'keep-alive' } 
response = DAO . getURL ( url , headers ) 
if response . status != 200 : 
~~~ raise DataFailureException ( url , response . status , response . data ) 
~~ data = json . loads ( response . data ) 
self . next_page_url = self . _next_page ( response ) 
if auto_page and self . next_page_url : 
~~~ if isinstance ( data , list ) : 
~~~ data . extend ( self . _get_resource_url ( self . next_page_url , True , 
data_key ) ) 
~~ elif isinstance ( data , dict ) and data_key is not None : 
~~~ data [ data_key ] . extend ( self . _get_resource_url ( 
self . next_page_url , True , data_key ) [ data_key ] ) 
~~ def _get_paged_resource ( self , url , params = None , data_key = None ) : 
~~ self . _set_as_user ( params ) 
auto_page = not ( 'page' in params or 'per_page' in params ) 
if 'per_page' not in params and self . _per_page != DEFAULT_PAGINATION : 
~~~ params [ "per_page" ] = self . _per_page 
~~ full_url = url + self . _params ( params ) 
return self . _get_resource_url ( full_url , auto_page , data_key ) 
~~ def _get_resource ( self , url , params = None , data_key = None ) : 
full_url = url + self . _params ( params ) 
return self . _get_resource_url ( full_url , True , data_key ) 
~~ def _put_resource ( self , url , body ) : 
params = { } 
self . _set_as_user ( params ) 
headers = { 'Content-Type' : 'application/json' , 
'Accept' : 'application/json' , 
url = url + self . _params ( params ) 
response = DAO . putURL ( url , headers , json . dumps ( body ) ) 
if not ( response . status == 200 or response . status == 201 or 
response . status == 204 ) : 
~~ return json . loads ( response . data ) 
~~ def _post_resource ( self , url , body ) : 
response = DAO . postURL ( url , headers , json . dumps ( body ) ) 
if not ( response . status == 200 or response . status == 204 ) : 
~~ def _delete_resource ( self , url ) : 
response = DAO . deleteURL ( url , headers ) 
~~ return response 
~~ def get_admins ( self , account_id , params = { } ) : 
url = ADMINS_API . format ( account_id ) 
admins = [ ] 
~~~ admins . append ( CanvasAdmin ( data = data ) ) 
~~ return admins 
~~ def create_admin ( self , account_id , user_id , role ) : 
body = { "user_id" : unquote ( str ( user_id ) ) , 
"role" : role , 
"send_confirmation" : False } 
return CanvasAdmin ( data = self . _post_resource ( url , body ) ) 
~~ def create_admin_by_sis_id ( self , sis_account_id , user_id , role ) : 
return self . create_admin ( self . _sis_id ( sis_account_id ) , user_id , role ) 
~~ def delete_admin ( self , account_id , user_id , role ) : 
url = ADMINS_API . format ( account_id ) + "/{}?role={}" . format ( 
user_id , quote ( role ) ) 
~~ def delete_admin_by_sis_id ( self , sis_account_id , user_id , role ) : 
return self . delete_admin ( self . _sis_id ( sis_account_id ) , user_id , role ) 
~~ def get_grading_standards_for_course ( self , course_id ) : 
url = COURSES_API . format ( course_id ) + "/grading_standards" 
standards = [ ] 
for data in self . _get_resource ( url ) : 
~~~ standards . append ( GradingStandard ( data = data ) ) 
~~ return standards 
~~ def create_grading_standard_for_course ( self , course_id , name , 
grading_scheme , creator ) : 
"title" : name , 
"grading_scheme_entry" : grading_scheme , 
"as_user_id" : creator 
return GradingStandard ( data = self . _post_resource ( url , body ) ) 
~~ def get_section ( self , section_id , params = { } ) : 
url = SECTIONS_API . format ( section_id ) 
return CanvasSection ( data = self . _get_resource ( url , params = params ) ) 
~~ def get_section_by_sis_id ( self , sis_section_id , params = { } ) : 
return self . get_section ( 
~~ def get_sections_in_course ( self , course_id , params = { } ) : 
url = COURSES_API . format ( course_id ) + "/sections" 
sections = [ ] 
~~~ sections . append ( CanvasSection ( data = data ) ) 
~~ return sections 
~~ def get_sections_in_course_by_sis_id ( self , sis_course_id , params = { } ) : 
return self . get_sections_in_course ( 
~~ def get_sections_with_students_in_course ( self , course_id , params = { } ) : 
if "students" not in include : 
~~~ include . append ( "students" ) 
return self . get_sections_in_course ( course_id , params ) 
~~ def get_sections_with_students_in_course_by_sis_id ( self , sis_course_id , 
return self . get_sections_with_students_in_course ( 
~~ def create_section ( self , course_id , name , sis_section_id ) : 
body = { "course_section" : { "name" : name , 
"sis_section_id" : sis_section_id } } 
return CanvasSection ( data = self . _post_resource ( url , body ) ) 
~~ def update_section ( self , section_id , name , sis_section_id ) : 
body = { "course_section" : { } } 
if name : 
~~~ body [ "course_section" ] [ "name" ] = name 
~~ if sis_section_id : 
~~~ body [ "course_section" ] [ "sis_section_id" ] = sis_section_id 
~~ return CanvasSection ( data = self . _put_resource ( url , body ) ) 
~~ def get_quizzes ( self , course_id ) : 
url = QUIZZES_API . format ( course_id ) 
quizzes = [ ] 
~~~ quizzes . append ( Quiz ( data = datum ) ) 
~~ return quizzes 
~~ def get_account ( self , account_id ) : 
url = ACCOUNTS_API . format ( account_id ) 
return CanvasAccount ( data = self . _get_resource ( url ) ) 
~~ def get_sub_accounts ( self , account_id , params = { } ) : 
url = ACCOUNTS_API . format ( account_id ) + "/sub_accounts" 
accounts = [ ] 
~~~ accounts . append ( CanvasAccount ( data = datum ) ) 
~~ return accounts 
~~ def update_account ( self , account ) : 
url = ACCOUNTS_API . format ( account . account_id ) 
body = { "account" : { "name" : account . name } } 
return CanvasAccount ( data = self . _put_resource ( url , body ) ) 
~~ def update_sis_id ( self , account_id , sis_account_id ) : 
if account_id == self . _canvas_account_id : 
~~ url = ACCOUNTS_API . format ( account_id ) 
body = { "account" : { "sis_account_id" : sis_account_id } } 
~~ def get_auth_settings ( self , account_id ) : 
url = ACCOUNTS_API . format ( account_id ) + "/sso_settings" 
return CanvasSSOSettings ( data = self . _get_resource ( url ) ) 
~~ def update_auth_settings ( self , account_id , auth_settings ) : 
data = self . _put_resource ( url , auth_settings . json_data ( ) ) 
return CanvasSSOSettings ( data = data ) 
~~ def settlement_schmertmann ( sp , fd , load , youngs_modulus_soil , ** kwargs ) : 
length = float ( fd . length ) 
breadth = float ( fd . width ) 
depth = float ( fd . depth ) 
load = float ( load ) 
sp . gwl = kwargs . get ( "gwl" , sp . gwl ) 
sp . unit_sat_weight = kwargs . get ( "unit_sat_weight" , sp . unit_sat_weight ) 
verbose = kwargs . get ( "verbose" , 0 ) 
years = kwargs . get ( "years" , 0 ) 
q = load / ( length * breadth ) 
sigma_v0_eff = ( sp . unit_dry_weight * min ( depth , sp . gwl ) + 
( sp . unit_sat_weight - 9.8 ) * max ( [ 0 , depth - sp . gwl ] ) ) 
delta_q = q - sigma_v0_eff 
c_1 = max ( 1 - 0.5 * ( sigma_v0_eff / delta_q ) , 0.5 ) 
if years == 0 : 
~~~ c_2 = 1.0 
~~~ c_2 = 1.0 + 0.2 * np . log10 ( years / 0.1 ) 
~~ long = max ( length , breadth ) 
short = min ( length , breadth ) 
c_3 = max ( 1.03 - 0.03 * ( long / short ) , 0.73 ) 
if long / short > 10 : 
~~~ zp = short + depth 
z_top = 0.2 
z_bottom = 4 * short + depth 
~~~ z_top = 0.1 
zp = 0.5 * short + depth 
z_bottom = 2 * short + depth 
~~ sigma_vp_eff = ( sp . unit_dry_weight * min ( zp , sp . gwl ) + 
( sp . unit_sat_weight - 9.8 ) * max ( [ 0 , zp - sp . gwl ] ) ) 
i_zp = 0.5 + 0.1 * ( delta_q / sigma_vp_eff ) ** 0.5 
i_z_top = ( i_zp + z_top ) / 2 
i_z_bottom = i_zp / 2 
settlement = ( c_1 * c_2 * c_3 * delta_q * 
( i_z_top * ( zp - depth ) + i_z_bottom * ( z_bottom - zp ) ) / youngs_modulus_soil ) 
~~~ log ( "delta_q:" , delta_q ) 
log ( "c_1:" , c_1 ) 
log ( "c_2:" , c_2 ) 
log ( "c_3:" , c_3 ) 
log ( "zp:" , zp ) 
log ( "sigma_vp_eff:" , sigma_vp_eff ) 
log ( "i_zp:" , i_zp ) 
log ( "i_z_top:" , i_z_top ) 
log ( "i_z_bottom:" , i_z_bottom ) 
log ( "settlement:" , settlement ) 
~~ return settlement 
~~ def get_all_terms ( self ) : 
if not self . _canvas_account_id : 
~~ params = { "workflow_state" : 'all' , 'per_page' : 500 } 
url = ACCOUNTS_API . format ( self . _canvas_account_id ) + "/terms" 
data_key = 'enrollment_terms' 
terms = [ ] 
response = self . _get_paged_resource ( url , params , data_key ) 
for data in response [ data_key ] : 
~~~ terms . append ( CanvasTerm ( data = data ) ) 
~~ return terms 
~~ def get_term_by_sis_id ( self , sis_term_id ) : 
for term in self . get_all_terms ( ) : 
~~~ if term . sis_term_id == sis_term_id : 
~~~ return term 
~~ ~~ ~~ def update_term_overrides ( self , sis_term_id , overrides = { } ) : 
~~ url = ACCOUNTS_API . format ( 
self . _canvas_account_id ) + "/terms/{}" . format ( 
self . _sis_id ( sis_term_id , sis_field = 'term' ) ) 
body = { 'enrollment_term' : { 'overrides' : overrides } } 
return CanvasTerm ( data = self . _put_resource ( url , body ) ) 
~~ def log ( out_str , o2 = "" , o3 = "" , o4 = "" ) : 
print ( out_str , o2 , o3 , o4 ) 
~~ def import_str ( self , csv , params = { } ) : 
~~ params [ "import_type" ] = SISImportModel . CSV_IMPORT_TYPE 
url = SIS_IMPORTS_API . format ( 
self . _canvas_account_id ) + ".json{}" . format ( self . _params ( params ) ) 
headers = { "Content-Type" : "text/csv" } 
return SISImportModel ( data = self . _post_resource ( url , headers , csv ) ) 
~~ def import_dir ( self , dir_path , params = { } ) : 
~~ body = self . _build_archive ( dir_path ) 
params [ "import_type" ] = SISImportModel . CSV_IMPORT_TYPE 
headers = { "Content-Type" : "application/zip" } 
return SISImportModel ( data = self . _post_resource ( url , headers , body ) ) 
~~ def get_import_status ( self , sis_import ) : 
~~ url = SIS_IMPORTS_API . format ( 
self . _canvas_account_id ) + "/{}.json" . format ( sis_import . import_id ) 
return SISImportModel ( data = self . _get_resource ( url ) ) 
~~ def _build_archive ( self , dir_path ) : 
zip_path = os . path . join ( dir_path , "import.zip" ) 
archive = zipfile . ZipFile ( zip_path , "w" ) 
for filename in CSV_FILES : 
~~~ filepath = os . path . join ( dir_path , filename ) 
if os . path . exists ( filepath ) : 
~~~ archive . write ( filepath , filename , zipfile . ZIP_DEFLATED ) 
~~ ~~ archive . close ( ) 
with open ( zip_path , "rb" ) as f : 
~~~ body = f . read ( ) 
~~ return body 
~~ def get_assignments ( self , course_id ) : 
url = ASSIGNMENTS_API . format ( course_id ) 
assignments = [ ] 
~~~ assignments . append ( Assignment ( data = datum ) ) 
~~ return assignments 
~~ def update_assignment ( self , assignment ) : 
url = ASSIGNMENTS_API . format ( assignment . course_id ) + "/{}" . format ( 
assignment . assignment_id ) 
data = self . _put_resource ( url , assignment . json_data ( ) ) 
return Assignment ( data = data ) 
~~ def get_available_reports ( self , account_id ) : 
url = ACCOUNTS_API . format ( account_id ) + "/reports" 
report_types = [ ] 
for datum in self . _get_resource ( url ) : 
~~~ report_types . append ( ReportType ( data = datum , account_id = account_id ) ) 
~~ return report_types 
~~ def get_reports_by_type ( self , account_id , report_type ) : 
url = ACCOUNTS_API . format ( account_id ) + "/reports/{}" . format ( 
report_type ) 
reports = [ ] 
~~~ datum [ "account_id" ] = account_id 
reports . append ( Report ( data = datum ) ) 
~~ return reports 
~~ def create_report ( self , report_type , account_id , term_id = None , params = { } ) : 
if term_id is not None : 
~~~ params [ "enrollment_term_id" ] = term_id 
~~ url = ACCOUNTS_API . format ( account_id ) + "/reports/{}" . format ( 
body = { "parameters" : params } 
data = self . _post_resource ( url , body ) 
data [ "account_id" ] = account_id 
return Report ( data = data ) 
~~ def create_course_provisioning_report ( self , account_id , term_id = None , 
params [ "courses" ] = True 
return self . create_report ( ReportType . PROVISIONING , account_id , term_id , 
~~ def create_course_sis_export_report ( self , account_id , term_id = None , 
return self . create_report ( ReportType . SIS_EXPORT , account_id , term_id , 
~~ def create_unused_courses_report ( self , account_id , term_id = None ) : 
return self . create_report ( ReportType . UNUSED_COURSES , account_id , 
term_id ) 
~~ def get_report_data ( self , report ) : 
if report . report_id is None or report . status is None : 
~~~ raise ReportFailureException ( report ) 
~~ interval = getattr ( settings , 'CANVAS_REPORT_POLLING_INTERVAL' , 5 ) 
while report . status != "complete" : 
~~~ if report . status == "error" : 
~~ sleep ( interval ) 
report = self . get_report_status ( report ) 
~~ if report . attachment is None or report . attachment . url is None : 
~~ data = self . _get_report_file ( report . attachment . url ) 
return data . split ( "\\n" ) 
~~ def get_report_status ( self , report ) : 
if ( report . account_id is None or report . type is None or 
report . report_id is None ) : 
~~ url = ACCOUNTS_API . format ( report . account_id ) + "/reports/{}/{}" . format ( 
report . type , report . report_id ) 
data [ "account_id" ] = report . account_id 
~~ def delete_report ( self , report ) : 
url = ACCOUNTS_API . format ( report . account_id ) + "/reports/{}/{}" . format ( 
~~ def crop_image ( img , start_y , start_x , h , w ) : 
return img [ start_y : start_y + h , start_x : start_x + w , : ] . copy ( ) 
~~ def move_detections ( label , dy , dx ) : 
for k in label . keys ( ) : 
~~~ if k . startswith ( "detection" ) : 
~~~ detections = label [ k ] 
for detection in detections : 
~~~ detection . move_image ( - dx , - dy ) 
~~ ~~ ~~ ~~ def hflip_detections ( label , w ) : 
~~~ detection . cx = w - detection . cx 
if k == "detections_2.5d" : 
~~~ detection . theta = math . pi - detection . theta 
~~ ~~ ~~ ~~ ~~ def augment_detections ( hyper_params , feature , label ) : 
if hyper_params . problem . get ( "augmentation" , None ) is None : 
~~~ return feature , label 
~~ img_h , img_w , img_c = feature [ "image" ] . shape 
augmented_feature = { } 
augmented_label = { } 
augmented_feature [ "image" ] = feature [ "image" ] . copy ( ) 
if "depth" in feature : 
~~~ augmented_feature [ "depth" ] = feature [ "depth" ] . copy ( ) 
~~ if "calibration" in feature : 
~~~ augmented_feature [ "calibration" ] = feature [ "calibration" ] 
~~ augmented_feature [ "hflipped" ] = np . array ( [ 0 ] , dtype = np . uint8 ) 
augmented_feature [ "crop_offset" ] = np . array ( [ 0 , 0 ] , dtype = np . int8 ) 
~~~ augmented_label [ k ] = [ detection . copy ( ) for detection in label [ k ] ] 
~~ if hyper_params . problem . augmentation . get ( "enable_horizontal_flip" , False ) : 
~~~ if random . random ( ) < 0.5 : 
~~~ img_h , img_w , img_c = augmented_feature [ "image" ] . shape 
augmented_feature [ "image" ] = np . fliplr ( augmented_feature [ "image" ] ) 
~~~ augmented_feature [ "depth" ] = np . fliplr ( augmented_feature [ "depth" ] ) 
~~ augmented_feature [ "hflipped" ] [ 0 ] = 1 
hflip_detections ( augmented_label , img_w ) 
~~ ~~ if hyper_params . problem . augmentation . get ( "enable_micro_translation" , False ) : 
dx = int ( random . random ( ) * 3 ) 
dy = int ( random . random ( ) * 3 ) 
augmented_feature [ "image" ] = crop_image ( augmented_feature [ "image" ] , dy , dx , img_h - dy , img_w - dx ) 
~~~ augmented_feature [ "depth" ] = crop_image ( augmented_feature [ "depth" ] , dy , dx , img_h - dy , img_w - dx ) 
~~ augmented_feature [ "crop_offset" ] [ 0 ] += dy 
augmented_feature [ "crop_offset" ] [ 1 ] += dx 
move_detections ( augmented_label , - dy , - dx ) 
~~ if hyper_params . problem . augmentation . get ( "random_crop" , None ) is not None : 
target_w = hyper_params . problem . augmentation . random_crop . shape . width 
target_h = hyper_params . problem . augmentation . random_crop . shape . height 
delta_x = max ( int ( math . ceil ( ( target_w + 1 - img_w ) / 2 ) ) , 0 ) 
delta_y = max ( int ( math . ceil ( ( target_h + 1 - img_h ) / 2 ) ) , 0 ) 
move_detections ( augmented_label , delta_y , delta_x ) 
augmented_feature [ "image" ] = cv2 . copyMakeBorder ( augmented_feature [ "image" ] , 
delta_y , delta_y , delta_x , delta_x , 
cv2 . BORDER_CONSTANT ) 
img_h , img_w , img_c = augmented_feature [ "image" ] . shape 
start_x = 0 
start_y = 0 
if len ( augmented_label [ "detections_2d" ] ) != 0 : 
~~~ idx = random . randint ( 0 , len ( augmented_label [ "detections_2d" ] ) - 1 ) 
detection = augmented_label [ "detections_2d" ] [ idx ] 
start_x = int ( detection . cx - random . random ( ) * ( target_w - 20 ) / 2.0 - 10 ) 
start_y = int ( detection . cy - random . random ( ) * ( target_h - 20 ) / 2.0 - 10 ) 
~~~ start_x = int ( img_w * random . random ( ) ) 
start_y = int ( img_h * random . random ( ) ) 
~~ if start_x < 0 : 
~~~ start_x = 0 
~~ if start_y < 0 : 
~~~ start_y = 0 
~~ if start_x >= img_w - target_w : 
~~~ start_x = img_w - target_w - 1 
~~ if start_y >= img_h - target_h : 
~~~ start_y = img_h - target_h - 1 
~~ augmented_feature [ "image" ] = crop_image ( augmented_feature [ "image" ] , start_y , start_x , target_h , target_w ) 
~~~ augmented_feature [ "depth" ] = crop_image ( augmented_feature [ "depth" ] , start_y , start_x , target_h , target_w ) 
~~ augmented_feature [ "crop_offset" ] [ 0 ] += start_y 
augmented_feature [ "crop_offset" ] [ 1 ] += start_x 
move_detections ( augmented_label , - start_y , - start_x ) 
~~ if hyper_params . problem . augmentation . get ( "enable_texture_augmentation" , False ) : 
~~~ augmented_feature [ "image" ] = full_texture_augmentation ( augmented_feature [ "image" ] ) 
~~ ~~ return augmented_feature , augmented_label 
~~ def create_metrics ( 
self , metric_configs : Iterable [ MetricConfig ] ) -> Dict [ str , Metric ] : 
return self . registry . create_metrics ( metric_configs ) 
~~ def _setup_logging ( self , log_level : str ) : 
level = getattr ( logging , log_level ) 
names = ( 
'aiohttp.access' , 'aiohttp.internal' , 'aiohttp.server' , 
'aiohttp.web' , self . name ) 
for name in names : 
~~~ setup_logger ( name = name , stream = sys . stderr , level = level ) 
~~ ~~ def _configure_registry ( self , include_process_stats : bool = False ) : 
if include_process_stats : 
~~~ self . registry . register_additional_collector ( 
ProcessCollector ( registry = None ) ) 
~~ ~~ def _get_exporter ( self , args : argparse . Namespace ) -> PrometheusExporter : 
exporter = PrometheusExporter ( 
self . name , self . description , args . host , args . port , self . registry ) 
exporter . app . on_startup . append ( self . on_application_startup ) 
exporter . app . on_shutdown . append ( self . on_application_shutdown ) 
return exporter 
~~ def create_metrics ( self , 
configs : Iterable [ MetricConfig ] ) -> Dict [ str , Metric ] : 
metrics : Dict [ str , Metric ] = { 
config . name : self . _register_metric ( config ) 
for config in configs 
self . _metrics . update ( metrics ) 
return metrics 
~~ def get_metric ( 
self , name : str , 
labels : Union [ Dict [ str , str ] , None ] = None ) -> Metric : 
metric = self . _metrics [ name ] 
if labels : 
~~~ return metric . labels ( ** labels ) 
~~ return metric 
run_app ( 
self . app , 
host = self . host , 
port = self . port , 
print = lambda * args , ** kargs : None , 
access_log_format = \ ) 
~~ def _make_application ( self ) -> Application : 
app = Application ( ) 
app [ 'exporter' ] = self 
app . router . add_get ( '/' , self . _handle_home ) 
app . router . add_get ( '/metrics' , self . _handle_metrics ) 
app . on_startup . append ( self . _log_startup_message ) 
return app 
~~ async def _handle_home ( self , request : Request ) -> Response : 
if self . description : 
~~~ title = self . name 
~~ text = dedent ( 
f\ ) 
return Response ( content_type = 'text/html' , text = text ) 
~~ async def _handle_metrics ( self , request : Request ) -> Response : 
if self . _update_handler : 
~~~ await self . _update_handler ( self . registry . get_metrics ( ) ) 
~~ response = Response ( body = self . registry . generate_metrics ( ) ) 
response . content_type = CONTENT_TYPE_LATEST 
~~ def wa ( client , event , channel , nick , rest ) : 
res = client . query ( rest ) 
return next ( res . results ) . text 
~~ def fix_HTTPMessage ( ) : 
~~ http_client . HTTPMessage . get_content_type = http_client . HTTPMessage . gettype 
http_client . HTTPMessage . get_param = http_client . HTTPMessage . getparam 
~~ def query ( self , input , params = ( ) , ** kwargs ) : 
data = dict ( 
input = input , 
appid = self . app_id , 
data = itertools . chain ( params , data . items ( ) , kwargs . items ( ) ) 
query = urllib . parse . urlencode ( tuple ( data ) ) 
url = 'https://api.wolframalpha.com/v2/query?' + query 
resp = urllib . request . urlopen ( url ) 
assert resp . headers . get_content_type ( ) == 'text/xml' 
assert resp . headers . get_param ( 'charset' ) == 'utf-8' 
return Result ( resp ) 
return itertools . chain ( self . pods , self . assumptions , self . warnings ) 
~~ def results ( self ) : 
return ( 
pod 
for pod in self . pods 
if pod . primary 
or pod . title == 'Result' 
~~ def encode ( request , data ) : 
~~~ return request 
~~ request . add_header ( 'Content-Type' , 'application/json' ) 
request . data = json . dumps ( data ) 
return request 
~~ def call_api ( 
method , 
url , 
headers = None , 
params = None , 
data = None , 
files = None , 
timeout = None , 
method = method . upper ( ) 
headers = deepcopy ( headers ) or { } 
headers [ 'Accept' ] = self . accept_type 
params = deepcopy ( params ) or { } 
data = data or { } 
files = files or { } 
if self . username and self . api_key : 
~~~ params . update ( self . get_credentials ( ) ) 
~~ url = urljoin ( self . base_url , url ) 
r = requests . request ( 
headers = headers , 
params = params , 
files = files , 
data = data , 
timeout = timeout , 
return r , r . status_code 
~~ def get ( self , url , params = None , ** kwargs ) : 
return self . call_api ( 
"GET" , 
~~ def delete ( self , url , params = None , ** kwargs ) : 
"DELETE" , 
~~ def put ( self , url , params = None , data = None , files = None , ** kwargs ) : 
"PUT" , 
~~ def post ( self , url , params = None , data = None , files = None , ** kwargs ) : 
"POST" , 
~~ def _process_query ( self , query , prepared = False ) : 
if prepared is True : 
~~~ files = { 'query' : str ( query ) } 
res , status = self . post ( 
self . disambiguate_service , 
headers = { 'Accept' : 'application/json' } , 
if status == 200 : 
~~~ return self . decode ( res ) , status 
return None , status 
~~ ~~ text = query [ 'text' ] 
sentence_coordinates = [ 
"offsetStart" : 0 , 
"offsetEnd" : len ( text ) 
sentences_groups = [ ] 
if len ( text ) > self . max_text_length : 
~~~ res , status_code = self . segment ( text ) 
if status_code == 200 : 
~~~ sentence_coordinates = res [ 'sentences' ] 
total_nb_sentences = len ( sentence_coordinates ) 
~~ logger . debug ( 
'sentences.' . format ( 
total_nb_sentences , self . sentences_per_group 
sentences_groups = self . _group_sentences ( 
total_nb_sentences , 
self . sentences_per_group 
~~~ query [ 'sentence' ] = "true" 
~~ if total_nb_sentences > 1 : 
~~~ query [ 'sentences' ] = sentence_coordinates 
~~ if len ( sentences_groups ) > 0 : 
~~~ for group in sentences_groups : 
~~~ query [ 'processSentence' ] = group 
res , status_code = self . _process_query ( query , prepared = True ) 
~~~ if 'entities' in res : 
~~~ query [ 'entities' ] = res [ u'entities' ] 
~~ query [ 'language' ] = res [ u'language' ] 
~~~ logger . error ( 
return None , status_code 
~~~ res , status_code = self . _process_query ( query , prepared = True ) 
~~~ query [ 'language' ] = res [ u'language' ] 
if 'entities' in res : 
~~ ~~ return query , status_code 
~~ def _group_sentences ( total_nb_sentences , group_length ) : 
current_sentence_group = [ ] 
for i in range ( 0 , total_nb_sentences ) : 
~~~ if i % group_length == 0 : 
~~~ if len ( current_sentence_group ) > 0 : 
~~~ sentences_groups . append ( current_sentence_group ) 
~~ current_sentence_group = [ i ] 
~~~ current_sentence_group . append ( i ) 
~~ ~~ if len ( current_sentence_group ) > 0 : 
~~ return sentences_groups 
~~ def disambiguate_pdf ( self , file , language = None , entities = None ) : 
"customisation" : "generic" 
if language : 
~~~ body [ 'language' ] = { "lang" : language } 
~~ if entities : 
~~~ body [ 'entities' ] = entities 
~~ files = { 
'query' : str ( body ) , 
'file' : ( 
file , 
open ( file , 'rb' ) , 
'application/pdf' , 
{ 'Expires' : '0' } 
if status != 200 : 
~~ return self . decode ( res ) , status 
~~ def disambiguate_terms ( self , terms , language = "en" , entities = None ) : 
"termVector" : terms , 
"entities" : [ ] , 
"onlyNER" : "false" , 
body [ 'language' ] = { "lang" : language } 
if entities : 
~~ files = { 'query' : str ( body ) } 
~~ ~~ def disambiguate_text ( self , text , language = None , entities = None ) : 
"text" : text , 
~~ result , status_code = self . _process_query ( body ) 
if status_code != 200 : 
~~ return result , status_code 
~~ def disambiguate_query ( self , query , language = None , entities = None ) : 
"shortText" : query , 
~~ ~~ def segment ( self , text ) : 
files = { 'text' : text } 
res , status_code = self . post ( self . segmentation_service , files = files ) 
~~ return self . decode ( res ) , status_code 
~~ def get_language ( self , text ) : 
res , status_code = self . post ( self . language_service , files = files ) 
~~ def get_concept ( self , conceptId , lang = 'en' ) : 
url = urljoin ( self . concept_service + '/' , conceptId ) 
res , status_code = self . get ( url , params = { 'lang' : lang } ) 
~~ def makemigrations ( migrations_root ) : 
from flask_migrate import ( Migrate , init as migrate_init , 
migrate as migrate_exec ) 
migrations_root = migrations_root or os . path . join ( 
os . environ . get ( 'FANTASY_MIGRATION_PATH' , 
os . getcwd ( ) ) , 
'migrations' ) 
migrations_root = os . path . expanduser ( migrations_root ) 
mig = Migrate ( app , app . db , directory = migrations_root ) 
if not os . path . exists ( migrations_root ) : 
~~~ migrate_init ( migrations_root ) 
~~ models_file = os . path . join ( migrations_root , 'models.txt' ) 
if not os . path . exists ( models_file ) : 
~~~ with open ( models_file , 'w' ) as fw : 
~~ pass 
~~ with open ( models_file , 'r' ) as fp : 
~~~ modules = fp . readlines ( ) 
~~ modules = filter ( lambda x : x . strip ( "\\n" ) , modules ) 
modules = map ( lambda x : x . strip ( "\\n" ) . split ( "#" ) [ 0 ] . strip ( ) , modules ) 
modules = list ( filter ( lambda x : x , modules ) ) 
if not modules : 
~~~ click . echo ( 
fg = 'yellow' ) ) 
sys . exit ( 0 ) 
~~ for m in modules : 
~~~ importlib . import_module ( m + '.models' ) 
~~ migrate_exec ( migrations_root ) 
mig . init_app ( app , app . db ) 
~~ def migrate ( migrations_root ) : 
from flask_migrate import Migrate , upgrade as migrate_upgrade 
from flask_sqlalchemy import SQLAlchemy 
from sqlalchemy . engine . url import make_url 
from sqlalchemy_utils import database_exists , create_database 
db = SQLAlchemy ( ) 
dsn = make_url ( app . config [ 'SQLALCHEMY_DATABASE_URI' ] ) 
if not database_exists ( dsn ) : 
~~~ create_database ( dsn ) 
~~ migrations_root = migrations_root or os . path . join ( 
if os . path . exists ( migrations_root ) : 
~~~ mig = Migrate ( app , db , directory = migrations_root ) 
mig . init_app ( app , db ) 
migrate_upgrade ( migrations_root ) 
sys . exit ( - 1 ) 
~~ def requirements ( work_dir , hive_root , with_requirements , 
with_dockerfile , active_module , active_module_file ) : 
~~~ """""" 
import sys 
sys . path . insert ( 0 , hive_root ) 
hive_root = os . path . abspath ( os . path . expanduser ( hive_root ) ) 
work_dir = work_dir or os . path . join ( 
os . environ . get ( 'FANTASY_APP_PATH' , 
os . getcwd ( ) ) ) 
work_dir = os . path . expanduser ( work_dir ) 
requirements_root = os . path . join ( work_dir , 'requirements' ) 
migrate_root = os . path . join ( work_dir , 'migrations' ) 
active_module_paths = [ ] 
active_module_list = [ ] 
if active_module_file : 
~~~ with open ( active_module_file , 'r' ) as fp : 
~~~ for l in fp : 
~~~ pkg = l . split ( '#' ) [ 0 ] . strip ( ) 
if pkg : 
~~~ active_module_list . append ( l . strip ( "\\n" ) ) 
~~ ~~ pass 
~~ ~~ active_module_list += active_module 
for m in active_module_list : 
~~~ mod = importlib . import_module ( m ) 
active_module_paths . append ( os . path . dirname ( mod . __file__ ) ) 
~~~ click . echo ( \ % m , color = "yellow" ) 
~~ def build_requirements ( ) : 
if not os . path . exists ( requirements_root ) : 
~~~ os . makedirs ( requirements_root ) 
shutil . copy ( 
os . path . join ( hive_root , 'requirements.txt' ) , 
os . path . join ( requirements_root , 'hive.txt' ) 
fg = "yellow" ) ) 
requirements_files = [ ] 
for m in active_module_paths : 
~~~ t = os . path . join ( m , 'requirements.txt' ) 
if os . path . exists ( t ) : 
~~~ requirements_files . append ( t ) 
~~ module_packages = set ( ) 
with fileinput . input ( requirements_files ) as fp : 
~~~ for line in fp : 
~~~ pkg = line . split ( '#' ) [ 0 ] . strip ( ) 
~~~ module_packages . add ( pkg ) 
~~ with click . open_file ( 
os . path . join ( requirements_root , 'hive-modules.txt' ) , 
'w' ) as fp : 
~~~ for p in module_packages : 
~~~ fp . write ( "%s\\n" % p ) 
~~ def build_dockerfile ( ) : 
~~~ """Dockerfile""" 
modules_in_hive = map ( 
lambda x : x . replace ( hive_root , '' ) . lstrip ( '/' ) , 
filter ( lambda x : x . startswith ( hive_root ) , 
active_module_paths ) ) 
docker_file = os . path . join ( 
os . path . dirname ( requirements_root ) , 
'Dockerfile' 
if os . path . exists ( docker_file ) : 
with open ( docker_file , 'r' ) as fp : 
~~~ buffer = fp . read ( ) 
~~ import re 
replaced = re . sub ( \ , 
\ % modules_path , 
buffer ) 
with open ( docker_file , 'w' ) as fp : 
~~~ fp . write ( replaced ) 
~~ def build_migrations ( ) : 
~~~ models_pairs = filter ( 
lambda pair : os . path . exists ( pair [ 0 ] ) , 
map ( lambda x : ( os . path . join ( x [ 0 ] , 'models.py' ) , x [ 1 ] ) , 
[ ( v , active_module_list [ i ] ) for i , v in 
enumerate ( active_module_paths ) ] ) ) 
~~~ _ , models = zip ( * models_pairs ) 
',' . join ( active_module_list ) , 
with open ( os . path . join ( migrate_root , 'models.txt' ) , 'w' ) as fp : 
~~~ for p in models : 
~~ def build_tasks ( ) : 
~~~ tasks_pairs = filter ( 
map ( lambda x : ( os . path . join ( x [ 0 ] , 'tasks.py' ) , x [ 1 ] ) , 
~~~ _ , tasks = zip ( * tasks_pairs ) 
with open ( os . path . join ( migrate_root , 'tasks.txt' ) , 'w' ) as fp : 
~~~ for p in tasks : 
~~ if with_requirements : 
~~~ build_requirements ( ) 
~~ if with_dockerfile : 
~~~ build_dockerfile ( ) 
~~ if os . path . exists ( migrate_root ) : 
~~~ build_migrations ( ) 
~~~ build_tasks ( ) 
~~ def queue ( celery_arguments ) : 
~~~ """[]""" 
if not app . celery : 
~~~ return click . echo ( 
~~ celery = app . celery 
celery . autodiscover_tasks ( ) 
argv = celery_arguments . split ( ) 
argv . insert ( 0 , 'worker' ) 
argv . insert ( 0 , 'Queue' ) 
celery . worker_main ( argv ) 
~~ def smart_database ( app ) : 
~~~ """""" 
~~ def smart_migrate ( app , migrations_root ) : 
~~~ """migrationprimary_nodemigrate""" 
db = app . db 
if os . path . exists ( migrations_root ) and os . environ [ 'FANTASY_PRIMARY_NODE' ] != 'no' : 
~~~ from flask_migrate import ( Migrate , 
upgrade as migrate_upgrade ) 
migrate = Migrate ( app , db , directory = migrations_root ) 
migrate . init_app ( app , db ) 
~~ def smart_account ( app ) : 
~~~ """""" 
if os . environ [ 'FANTASY_ACTIVE_ACCOUNT' ] == 'no' : 
~~ from flask_security import SQLAlchemyUserDatastore , Security 
account_module_name , account_class_name = os . environ [ 
'FANTASY_ACCOUNT_MODEL' ] . rsplit ( '.' , 1 ) 
account_module = importlib . import_module ( account_module_name ) 
account_class = getattr ( account_module , account_class_name ) 
role_module_name , role_class_name = os . environ [ 
'FANTASY_ROLE_MODEL' ] . rsplit ( '.' , 1 ) 
role_module = importlib . import_module ( role_module_name ) 
role_class = getattr ( role_module , role_class_name ) 
r = True if os . environ [ 
'FANTASY_ACCOUNT_SECURITY_MODE' ] != 'no' else False 
Security ( app , 
SQLAlchemyUserDatastore ( 
app . db , account_class , role_class ) , 
register_blueprint = r ) 
~~ def load_tasks ( app , entry_file = None ) : 
~~~ """celery""" 
from celery import Task 
tasks_txt = os . path . join ( os . path . dirname ( entry_file ) , 'migrations' , 
'tasks.txt' ) 
if not os . path . exists ( tasks_txt ) : 
~~~ import sys 
~~ class ContextTask ( Task ) : 
~~~ abstract = True 
def __call__ ( self , * args , ** kwargs ) : 
~~~ with app . app_context ( ) : 
~~~ return super ( ) . __call__ ( * args , ** kwargs ) 
~~ ~~ ~~ app . celery . config_from_object ( app . config , namespace = 'CELERY' ) 
app . celery . Task = ContextTask 
with app . app_context ( ) : 
~~~ with open ( tasks_txt , 'r' ) as f : 
~~~ for line in f : 
~~~ mod = line . strip ( '\\n' ) 
if mod : 
~~~ importlib . import_module ( mod + '.tasks' ) 
~~ def create_app ( app_name , config = { } , db = None , celery = None ) : 
track_mode = os . environ [ 'FANTASY_TRACK_MODE' ] == 'yes' 
if track_mode : 
~~ active_db = os . environ [ 'FANTASY_ACTIVE_DB' ] == 'yes' 
~~ from webargs . flaskparser import parser 
from . import error_handler , hacker , cli 
hacker . hack_webargs ( ) 
migrations_root = os . path . join ( 
~~ mod = importlib . import_module ( app_name ) 
app = FantasyFlask ( __name__ , root_path = os . path . dirname ( mod . __file__ ) ) 
~~ if config : 
~~ config_module = os . environ . get ( 'FANTASY_SETTINGS_MODULE' , None ) 
~~ if config_module : 
~~~ app . config . from_object ( config_module ) 
~~ if track_mode : 
~~ if celery : 
~~~ app . celery = celery 
~~ with app . app_context ( ) : 
~~~ if track_mode : 
~~ if db is None : 
~~~ global _db 
app . db = _db 
~~~ app . db = db 
~~ if os . environ [ 'FANTASY_ACTIVE_CACHE' ] != 'no' : 
~~~ from flask_caching import Cache 
app . cache = Cache ( app , config = app . config ) 
~~ if os . environ . get ( 'FANTASY_ACTIVE_SENTRY' ) != 'no' : 
~~~ from raven . contrib . flask import Sentry 
Sentry ( app ) 
~~ if hasattr ( mod , 'run_app' ) : 
~~~ run_app = getattr ( mod , 'run_app' ) 
~~~ run_app ( app ) 
~~~ if hasattr ( app , 'sentry' ) : 
~~~ app . sentry . handle_exception ( e ) 
~~ import sys 
import traceback 
~~ if active_db and app . db : 
~~ smart_database ( app ) 
smart_migrate ( app , migrations_root ) 
smart_account ( app ) 
app . db . init_app ( app ) 
@ app . teardown_request 
def session_clear ( exception = None ) : 
~~~ if exception and app . db . session . is_active : 
~~~ app . db . session . rollback ( ) 
app . db . session . remove ( ) 
~~ @ parser . error_handler 
def h_webargs ( error ) : 
~~~ return error_handler . webargs_error ( error ) 
~~ @ app . errorhandler ( 422 ) 
def h_422 ( error ) : 
~~~ return error_handler . http422 ( error ) 
~~ @ app . errorhandler ( 500 ) 
def h_500 ( error ) : 
~~~ return error_handler . http500 ( error ) 
~~ if hasattr ( mod , 'error_handler' ) : 
~~~ error_handle = getattr ( mod , 'error_handle' ) 
error_handle ( app ) 
~~ if hasattr ( mod , 'run_admin' ) : 
~~~ import flask_admin 
admin = flask_admin . Admin ( name = os . environ . get ( 'FANTASY_ADMIN_NAME' , 
'Admin' ) , 
template_mode = os . environ . get ( 
'FANTASY_ADMIN_TEMPLATE_MODE' , 
'bootstrap3' ) ) 
run_admin = getattr ( mod , 'run_admin' ) 
run_admin ( admin ) 
admin . init_app ( app ) 
~~ app . cli . add_command ( cli . ff ) 
~~ if hasattr ( mod , 'run_cli' ) : 
~~~ run_cli = getattr ( mod , 'run_cli' ) 
run_cli ( app ) 
~~ return app 
~~ def get_config ( app , prefix = 'hive_' ) : 
items = app . config . items ( ) 
prefix = prefix . upper ( ) 
def strip_prefix ( tup ) : 
~~~ return ( tup [ 0 ] . replace ( prefix , '' ) , tup [ 1 ] ) 
~~ return dict ( [ strip_prefix ( i ) for i in items if i [ 0 ] . startswith ( prefix ) ] ) 
~~ def config_value ( key , app = None , default = None , prefix = 'hive_' ) : 
app = app or current_app 
return get_config ( app , prefix = prefix ) . get ( key . upper ( ) , default ) 
~~ def random_str ( length = 16 , only_digits = False ) : 
choices = string . digits 
if not only_digits : 
~~~ choices += string . ascii_uppercase 
~~ return '' . join ( random . SystemRandom ( ) . choice ( choices ) 
for _ in range ( length ) ) 
~~ def vector ( members : Iterable [ T ] , meta : Optional [ IPersistentMap ] = None ) -> Vector [ T ] : 
return Vector ( pvector ( members ) , meta = meta ) 
~~ def v ( * members : T , meta : Optional [ IPersistentMap ] = None ) -> Vector [ T ] : 
~~ def eval_file ( filename : str , ctx : compiler . CompilerContext , module : types . ModuleType ) : 
last = None 
for form in reader . read_file ( filename , resolver = runtime . resolve_alias ) : 
~~~ last = compiler . compile_and_exec_form ( form , ctx , module ) 
~~ return last 
~~ def eval_stream ( stream , ctx : compiler . CompilerContext , module : types . ModuleType ) : 
for form in reader . read ( stream , resolver = runtime . resolve_alias ) : 
~~ def eval_str ( s : str , ctx : compiler . CompilerContext , module : types . ModuleType , eof : Any ) : 
last = eof 
for form in reader . read_str ( s , resolver = runtime . resolve_alias , eof = eof ) : 
~~ def bootstrap_repl ( which_ns : str ) -> types . ModuleType : 
repl_ns = runtime . Namespace . get_or_create ( sym . symbol ( "basilisp.repl" ) ) 
ns = runtime . Namespace . get_or_create ( sym . symbol ( which_ns ) ) 
repl_module = importlib . import_module ( "basilisp.repl" ) 
ns . add_alias ( sym . symbol ( "basilisp.repl" ) , repl_ns ) 
ns . refer_all ( repl_ns ) 
return repl_module 
file_or_code , 
code , 
in_ns , 
use_var_indirection , 
warn_on_shadowed_name , 
warn_on_shadowed_var , 
warn_on_var_indirection , 
basilisp . init ( ) 
ctx = compiler . CompilerContext ( 
filename = CLI_INPUT_FILE_PATH 
if code 
else ( 
STDIN_INPUT_FILE_PATH if file_or_code == STDIN_FILE_NAME else file_or_code 
opts = { 
compiler . WARN_ON_SHADOWED_NAME : warn_on_shadowed_name , 
compiler . WARN_ON_SHADOWED_VAR : warn_on_shadowed_var , 
compiler . USE_VAR_INDIRECTION : use_var_indirection , 
compiler . WARN_ON_VAR_INDIRECTION : warn_on_var_indirection , 
eof = object ( ) 
with runtime . ns_bindings ( in_ns ) as ns : 
~~~ if code : 
~~~ print ( runtime . lrepr ( eval_str ( file_or_code , ctx , ns . module , eof ) ) ) 
~~ elif file_or_code == STDIN_FILE_NAME : 
~~~ print ( 
runtime . lrepr ( 
eval_stream ( click . get_text_stream ( "stdin" ) , ctx , ns . module ) 
~~~ print ( runtime . lrepr ( eval_file ( file_or_code , ctx , ns . module ) ) ) 
~~ ~~ ~~ def multifn ( dispatch : DispatchFunction , default = None ) -> MultiFunction [ T ] : 
name = sym . symbol ( dispatch . __qualname__ , ns = dispatch . __module__ ) 
return MultiFunction ( name , dispatch , default ) 
~~ def __add_method ( m : lmap . Map , key : T , method : Method ) -> lmap . Map : 
return m . assoc ( key , method ) 
~~ def add_method ( self , key : T , method : Method ) -> None : 
self . _methods . swap ( MultiFunction . __add_method , key , method ) 
~~ def get_method ( self , key : T ) -> Optional [ Method ] : 
method_cache = self . methods 
return Maybe ( method_cache . entry ( key , None ) ) . or_else ( 
~~ def __remove_method ( m : lmap . Map , key : T ) -> lmap . Map : 
return m . dissoc ( key ) 
~~ def remove_method ( self , key : T ) -> Optional [ Method ] : 
method = self . methods . entry ( key , None ) 
if method : 
~~~ self . _methods . swap ( MultiFunction . __remove_method , key ) 
~~ return method 
~~ def _is_async ( o : IMeta ) -> bool : 
Maybe ( o . meta ) 
. map ( lambda m : m . entry ( SYM_ASYNC_META_KEY , None ) ) 
. or_else_get ( False ) 
~~ def _is_macro ( v : Var ) -> bool : 
Maybe ( v . meta ) 
~~ def _loc ( form : Union [ LispForm , ISeq ] ) -> Optional [ Tuple [ int , int ] ] : 
~~~ assert isinstance ( line , int ) and isinstance ( col , int ) 
return line , col 
~~ ~~ def _with_loc ( f : ParseFunction ) : 
@ wraps ( f ) 
def _parse_form ( ctx : ParserContext , form : Union [ LispForm , ISeq ] ) -> Node : 
~~~ form_loc = _loc ( form ) 
if form_loc is None : 
~~~ return f ( ctx , form ) 
~~~ return f ( ctx , form ) . fix_missing_locations ( form_loc ) 
~~ ~~ return _parse_form 
~~ def _clean_meta ( meta : Optional [ lmap . Map ] ) -> Optional [ lmap . Map ] : 
if meta is None : 
~~~ new_meta = meta . dissoc ( reader . READER_LINE_KW , reader . READER_COL_KW ) 
return None if len ( new_meta ) == 0 else new_meta 
~~ ~~ def _with_meta ( gen_node ) : 
@ wraps ( gen_node ) 
def with_meta ( 
ctx : ParserContext , 
form : Union [ llist . List , lmap . Map , ISeq , lset . Set , vec . Vector ] , 
) -> Node : 
descriptor = gen_node ( ctx , form ) 
if isinstance ( form , IMeta ) : 
~~~ assert isinstance ( form . meta , ( lmap . Map , type ( None ) ) ) 
form_meta = _clean_meta ( form . meta ) 
if form_meta is not None : 
~~~ meta_ast = _parse_ast ( ctx , form_meta ) 
assert isinstance ( meta_ast , MapNode ) or ( 
isinstance ( meta_ast , Const ) and meta_ast . type == ConstType . MAP 
return WithMeta ( 
form = form , meta = meta_ast , expr = descriptor , env = ctx . get_node_env ( ) 
~~ ~~ return descriptor 
~~ return with_meta 
ctx : ParserContext , form : ISeq 
) -> Tuple [ List [ DefTypeBase ] , List [ Method ] ] : 
current_interface_sym : Optional [ sym . Symbol ] = None 
current_interface : Optional [ DefTypeBase ] = None 
interfaces = [ ] 
methods : List [ Method ] = [ ] 
interface_methods : MutableMapping [ sym . Symbol , List [ Method ] ] = { } 
for elem in form : 
~~~ if isinstance ( elem , sym . Symbol ) : 
~~~ if current_interface is not None : 
~~~ if current_interface_sym in interface_methods : 
~~~ raise ParserException ( 
form = elem , 
~~ assert ( 
current_interface_sym is not None 
interface_methods [ current_interface_sym ] = methods 
~~ current_interface_sym = elem 
current_interface = _parse_ast ( ctx , elem ) 
methods = [ ] 
if not isinstance ( current_interface , ( MaybeClass , MaybeHostForm , VarRef ) ) : 
~~ interfaces . append ( current_interface ) 
~~ elif isinstance ( elem , ISeq ) : 
~~~ if current_interface is None : 
~~ methods . append ( __deftype_method ( ctx , elem , current_interface ) ) 
~~ ~~ if current_interface is not None : 
~~~ if len ( methods ) > 0 : 
form = current_interface_sym , 
~~ ~~ return interfaces , list ( chain . from_iterable ( interface_methods . values ( ) ) ) 
~~ def _assert_no_recur ( node : Node ) -> None : 
if node . op == NodeOp . RECUR : 
~~ elif node . op in { NodeOp . FN , NodeOp . LOOP } : 
~~~ node . visit ( _assert_no_recur ) 
if node . op == NodeOp . DO : 
~~~ assert isinstance ( node , Do ) 
for child in node . statements : 
~~~ _assert_no_recur ( child ) 
~~ _assert_recur_is_tail ( node . ret ) 
~~ elif node . op in { NodeOp . FN , NodeOp . FN_METHOD , NodeOp . METHOD } : 
~~~ assert isinstance ( node , ( Fn , FnMethod , Method ) ) 
node . visit ( _assert_recur_is_tail ) 
~~ elif node . op == NodeOp . IF : 
~~~ assert isinstance ( node , If ) 
_assert_no_recur ( node . test ) 
_assert_recur_is_tail ( node . then ) 
_assert_recur_is_tail ( node . else_ ) 
~~ elif node . op in { NodeOp . LET , NodeOp . LETFN } : 
~~~ assert isinstance ( node , ( Let , LetFn ) ) 
for binding in node . bindings : 
~~~ assert binding . init is not None 
_assert_no_recur ( binding . init ) 
~~ _assert_recur_is_tail ( node . body ) 
~~ elif node . op == NodeOp . LOOP : 
~~~ assert isinstance ( node , Loop ) 
~~ ~~ elif node . op == NodeOp . RECUR : 
~~ elif node . op == NodeOp . TRY : 
~~~ assert isinstance ( node , Try ) 
_assert_recur_is_tail ( node . body ) 
for catch in node . catches : 
~~~ _assert_recur_is_tail ( catch ) 
~~ if node . finally_ : 
~~~ _assert_no_recur ( node . finally_ ) 
ctx : ParserContext , form : sym . Symbol 
) -> Union [ MaybeClass , MaybeHostForm , VarRef ] : 
assert form . ns is not None 
if form . ns == ctx . current_ns . name : 
~~~ v = ctx . current_ns . find ( sym . symbol ( form . name ) ) 
if v is not None : 
~~~ return VarRef ( form = form , var = v , env = ctx . get_node_env ( ) ) 
~~ ~~ elif form . ns == _BUILTINS_NS : 
~~~ class_ = munge ( form . name , allow_builtins = True ) 
target = getattr ( builtins , class_ , None ) 
if target is None : 
~~ return MaybeClass ( 
form = form , class_ = class_ , target = target , env = ctx . get_node_env ( ) 
~~ if "." in form . name : 
~~ ns_sym = sym . symbol ( form . ns ) 
if ns_sym in ctx . current_ns . imports or ns_sym in ctx . current_ns . import_aliases : 
~~~ v = Var . find ( form ) 
~~ if ns_sym in ctx . current_ns . import_aliases : 
~~~ ns = ctx . current_ns . import_aliases [ ns_sym ] 
assert ns is not None 
ns_name = ns . name 
~~~ ns_name = ns_sym . name 
~~ safe_module_name = munge ( ns_name ) 
assert ( 
safe_module_name in sys . modules 
ns_module = sys . modules [ safe_module_name ] 
safe_name = munge ( form . name ) 
if safe_name in vars ( ns_module ) : 
~~~ return MaybeHostForm ( 
form = form , 
class_ = munge ( ns_sym . name ) , 
field = safe_name , 
target = vars ( ns_module ) [ safe_name ] , 
env = ctx . get_node_env ( ) , 
~~ safe_name = munge ( form . name , allow_builtins = True ) 
if safe_name not in vars ( ns_module ) : 
~~~ raise ParserException ( "can\ , form = form ) 
~~ return MaybeHostForm ( 
~~ elif ns_sym in ctx . current_ns . aliases : 
~~~ aliased_ns : runtime . Namespace = ctx . current_ns . aliases [ ns_sym ] 
v = Var . find ( sym . symbol ( form . name , ns = aliased_ns . name ) ) 
if v is None : 
~~ return VarRef ( form = form , var = v , env = ctx . get_node_env ( ) ) 
~~ ~~ def __resolve_bare_symbol ( 
) -> Union [ MaybeClass , VarRef ] : 
assert form . ns is None 
v = ctx . current_ns . find ( form ) 
~~ munged = munge ( form . name , allow_builtins = True ) 
if munged in vars ( builtins ) : 
~~~ return MaybeClass ( 
class_ = munged , 
target = vars ( builtins ) [ munged ] , 
~~ assert munged not in vars ( ctx . current_ns . module ) 
raise ParserException ( 
~~ def _resolve_sym ( 
if form . ns is None and form . name . endswith ( "." ) : 
~~~ ns , name = form . name [ : - 1 ] . rsplit ( "." , maxsplit = 1 ) 
form = sym . symbol ( name , ns = ns ) 
~~~ form = sym . symbol ( form . name [ : - 1 ] ) 
~~ ~~ if form . ns is not None : 
~~~ return __resolve_namespaced_symbol ( ctx , form ) 
~~~ return __resolve_bare_symbol ( ctx , form ) 
~~ ~~ def parse_ast ( ctx : ParserContext , form : ReaderForm ) -> Node : 
return _parse_ast ( ctx , form ) . assoc ( top_level = True ) 
~~ def warn_on_shadowed_var ( self ) -> bool : 
return self . warn_on_shadowed_name or self . _opts . entry ( 
WARN_ON_SHADOWED_VAR , False 
s : sym . Symbol , 
binding : Binding , 
warn_on_shadowed_name : bool = True , 
warn_on_shadowed_var : bool = True , 
warn_if_unused : bool = True , 
st = self . symbol_table 
if warn_on_shadowed_name and self . warn_on_shadowed_name : 
~~~ if st . find_symbol ( s ) is not None : 
~~ ~~ if ( 
warn_on_shadowed_name or warn_on_shadowed_var 
) and self . warn_on_shadowed_var : 
~~~ if self . current_ns . find ( s ) is not None : 
~~ ~~ if s . meta is not None and s . meta . entry ( SYM_NO_WARN_WHEN_UNUSED_META_KEY , None ) : 
~~~ warn_if_unused = False 
~~ st . new_symbol ( s , binding , warn_if_unused = warn_if_unused ) 
~~ def map_lrepr ( 
entries : Callable [ [ ] , Iterable [ Tuple [ Any , Any ] ] ] , 
start : str , 
end : str , 
meta = None , 
** kwargs , 
) -> str : 
print_level = kwargs [ "print_level" ] 
if isinstance ( print_level , int ) and print_level < 1 : 
~~~ return SURPASSED_PRINT_LEVEL 
~~ kwargs = _process_kwargs ( ** kwargs ) 
def entry_reprs ( ) : 
~~~ for k , v in entries ( ) : 
~~ ~~ trailer = [ ] 
print_dup = kwargs [ "print_dup" ] 
print_length = kwargs [ "print_length" ] 
if not print_dup and isinstance ( print_length , int ) : 
~~~ items = seq ( entry_reprs ( ) ) . take ( print_length + 1 ) . to_list ( ) 
if len ( items ) > print_length : 
~~~ items . pop ( ) 
trailer . append ( SURPASSED_PRINT_LENGTH ) 
~~~ items = list ( entry_reprs ( ) ) 
~~ seq_lrepr = PRINT_SEPARATOR . join ( items + trailer ) 
print_meta = kwargs [ "print_meta" ] 
if print_meta and meta : 
~~ return f"{start}{seq_lrepr}{end}" 
~~ def seq_lrepr ( 
iterable : Iterable [ Any ] , start : str , end : str , meta = None , ** kwargs 
trailer = [ ] 
~~~ items = seq ( iterable ) . take ( print_length + 1 ) . to_list ( ) 
~~~ items = iterable 
~~ items = list ( map ( lambda o : lrepr ( o , ** kwargs ) , items ) ) 
seq_lrepr = PRINT_SEPARATOR . join ( items + trailer ) 
o : Any , 
human_readable : bool = False , 
print_dup : bool = PRINT_DUP , 
print_length : PrintCountSetting = PRINT_LENGTH , 
print_level : PrintCountSetting = PRINT_LEVEL , 
print_meta : bool = PRINT_META , 
print_readably : bool = PRINT_READABLY , 
if isinstance ( o , LispObject ) : 
~~~ return o . _lrepr ( 
human_readable = human_readable , 
print_dup = print_dup , 
print_length = print_length , 
print_level = print_level , 
print_meta = print_meta , 
print_readably = print_readably , 
~~~ return _lrepr_fallback ( 
o , 
kwargs = { 
"human_readable" : human_readable , 
"print_dup" : print_dup , 
"print_length" : print_length , 
"print_level" : print_level , 
"print_meta" : print_meta , 
"print_readably" : print_readably , 
if isinstance ( o , bool ) : 
~~~ return _lrepr_bool ( o ) 
~~ elif o is None : 
~~~ return _lrepr_nil ( o ) 
~~ elif isinstance ( o , str ) : 
~~~ return _lrepr_str ( 
o , human_readable = human_readable , print_readably = print_readably 
~~ elif isinstance ( o , dict ) : 
~~~ return _lrepr_py_dict ( o , ** kwargs ) 
~~ elif isinstance ( o , list ) : 
~~~ return _lrepr_py_list ( o , ** kwargs ) 
~~ elif isinstance ( o , set ) : 
~~~ return _lrepr_py_set ( o , ** kwargs ) 
~~ elif isinstance ( o , tuple ) : 
~~~ return _lrepr_py_tuple ( o , ** kwargs ) 
~~ elif isinstance ( o , complex ) : 
~~~ return _lrepr_complex ( o ) 
~~ elif isinstance ( o , datetime . datetime ) : 
~~~ return _lrepr_datetime ( o ) 
~~ elif isinstance ( o , Decimal ) : 
~~~ return _lrepr_decimal ( o , print_dup = print_dup ) 
~~ elif isinstance ( o , Fraction ) : 
~~~ return _lrepr_fraction ( o ) 
~~ elif isinstance ( o , Pattern ) : 
~~~ return _lrepr_pattern ( o ) 
~~ elif isinstance ( o , uuid . UUID ) : 
~~~ return _lrepr_uuid ( o ) 
~~~ return repr ( o ) 
~~ ~~ def visit ( self , f : Callable [ ... , None ] , * args , ** kwargs ) : 
for child_kw in self . children : 
~~~ child_attr = munge ( child_kw . name ) 
if child_attr . endswith ( "s" ) : 
~~~ iter_child : Iterable [ Node ] = getattr ( self , child_attr ) 
for item in iter_child : 
~~~ f ( item , * args , ** kwargs ) 
~~~ child : Node = getattr ( self , child_attr ) 
f ( child , * args , ** kwargs ) 
~~ ~~ ~~ def fix_missing_locations ( 
self , start_loc : Optional [ Tuple [ int , int ] ] = None 
) -> "Node" : 
if self . env . line is None or self . env . col is None : 
~~~ loc = start_loc 
~~~ loc = ( self . env . line , self . env . col ) 
~~ assert loc is not None and all ( 
[ e is not None for e in loc ] 
new_attrs : MutableMapping [ str , Union [ NodeEnv , Node , Iterable [ Node ] ] ] = { 
"env" : attr . evolve ( self . env , line = loc [ 0 ] , col = loc [ 1 ] ) 
new_children = [ ] 
~~~ new_children . append ( item . fix_missing_locations ( start_loc ) ) 
~~ new_attrs [ child_attr ] = vec . vector ( new_children ) 
new_attrs [ child_attr ] = child . fix_missing_locations ( start_loc ) 
~~ ~~ return self . assoc ( ** new_attrs ) 
if os . getenv ( "BASILISP_EMIT_GENERATED_PYTHON" , "true" ) != "true" : 
~~ if runtime . print_generated_python ( ) : 
~~~ print ( to_py_str ( module ) ) 
~~~ runtime . add_generated_python ( to_py_str ( module ) ) 
form : ReaderForm , 
ctx : CompilerContext , 
module : types . ModuleType , 
wrapped_fn_name : str = _DEFAULT_FN , 
collect_bytecode : Optional [ BytecodeCollector ] = None , 
) -> Any : 
if form is None : 
~~~ _bootstrap_module ( ctx . generator_context , ctx . py_ast_optimizer , module ) 
~~ final_wrapped_name = genname ( wrapped_fn_name ) 
lisp_ast = parse_ast ( ctx . parser_context , form ) 
py_ast = gen_py_ast ( ctx . generator_context , lisp_ast ) 
form_ast = list ( 
map ( 
_statementize , 
itertools . chain ( 
py_ast . dependencies , 
[ _expressionize ( GeneratedPyAST ( node = py_ast . node ) , final_wrapped_name ) ] , 
ast_module = ast . Module ( body = form_ast ) 
ast_module = ctx . py_ast_optimizer . visit ( ast_module ) 
ast . fix_missing_locations ( ast_module ) 
_emit_ast_string ( ast_module ) 
bytecode = compile ( ast_module , ctx . filename , "exec" ) 
if collect_bytecode : 
~~~ collect_bytecode ( bytecode ) 
~~ exec ( bytecode , module . __dict__ ) 
return getattr ( module , final_wrapped_name ) ( ) 
~~ def _incremental_compile_module ( 
optimizer : PythonASTOptimizer , 
py_ast : GeneratedPyAST , 
mod : types . ModuleType , 
source_filename : str , 
) -> None : 
module_body = list ( 
map ( _statementize , itertools . chain ( py_ast . dependencies , [ py_ast . node ] ) ) 
module = ast . Module ( body = list ( module_body ) ) 
module = optimizer . visit ( module ) 
ast . fix_missing_locations ( module ) 
_emit_ast_string ( module ) 
bytecode = compile ( module , source_filename , "exec" ) 
~~ exec ( bytecode , mod . __dict__ ) 
~~ def _bootstrap_module ( 
gctx : GeneratorContext , 
_incremental_compile_module ( 
optimizer , 
py_module_preamble ( gctx ) , 
mod , 
source_filename = gctx . filename , 
collect_bytecode = collect_bytecode , 
mod . __basilisp_bootstrapped__ = True 
~~ def compile_module ( 
forms : Iterable [ ReaderForm ] , 
_bootstrap_module ( ctx . generator_context , ctx . py_ast_optimizer , module ) 
for form in forms : 
~~~ nodes = gen_py_ast ( ctx . generator_context , parse_ast ( ctx . parser_context , form ) ) 
ctx . py_ast_optimizer , 
nodes , 
module , 
source_filename = ctx . filename , 
~~ ~~ def compile_bytecode ( 
code : List [ types . CodeType ] , 
_bootstrap_module ( gctx , optimizer , module ) 
for bytecode in code : 
~~~ exec ( bytecode , module . __dict__ ) 
~~ ~~ def sequence ( s : Iterable ) -> ISeq [ Any ] : 
~~~ i = iter ( s ) 
return _Sequence ( i , next ( i ) ) 
~~~ return EMPTY 
~~ ~~ def munge ( s : str , allow_builtins : bool = False ) -> str : 
new_str = [ ] 
for c in s : 
~~~ new_str . append ( _MUNGE_REPLACEMENTS . get ( c , c ) ) 
~~ new_s = "" . join ( new_str ) 
if keyword . iskeyword ( new_s ) : 
~~~ return f"{new_s}_" 
~~ if not allow_builtins and new_s in builtins . __dict__ : 
~~ return new_s 
~~ def demunge ( s : str ) -> str : 
def demunge_replacer ( match : Match ) -> str : 
~~~ full_match = match . group ( 0 ) 
replacement = _DEMUNGE_REPLACEMENTS . get ( full_match , None ) 
if replacement : 
~~~ return replacement 
~~ return full_match 
~~ return re . sub ( _DEMUNGE_PATTERN , demunge_replacer , s ) . replace ( "_" , "-" ) 
~~ def fraction ( numerator : int , denominator : int ) -> Fraction : 
return Fraction ( numerator = numerator , denominator = denominator ) 
~~ def get_handler ( level : str , fmt : str ) -> logging . Handler : 
handler : logging . Handler = logging . NullHandler ( ) 
if os . getenv ( "BASILISP_USE_DEV_LOGGER" ) == "true" : 
~~~ handler = logging . StreamHandler ( ) 
~~ handler . setFormatter ( logging . Formatter ( fmt ) ) 
handler . setLevel ( level ) 
return handler 
return Map ( pmap ( initial = kvs ) , meta = meta ) 
~~ def timed ( f : Optional [ Callable [ [ int ] , None ] ] = None ) : 
start = time . perf_counter ( ) 
yield 
end = time . perf_counter ( ) 
if f : 
~~~ ns = int ( ( end - start ) * 1_000_000_000 ) 
f ( ns ) 
~~ ~~ def partition ( coll , n : int ) : 
assert n > 0 
start = 0 
stop = n 
while stop <= len ( coll ) : 
~~~ yield tuple ( e for e in coll [ start : stop ] ) 
start += n 
stop += n 
~~ if start < len ( coll ) < stop : 
~~~ stop = len ( coll ) 
yield tuple ( e for e in coll [ start : stop ] ) 
~~ ~~ def _with_loc ( f : W ) -> W : 
@ functools . wraps ( f ) 
def with_lineno_and_col ( ctx ) : 
~~~ meta = lmap . map ( 
{ READER_LINE_KW : ctx . reader . line , READER_COL_KW : ctx . reader . col } 
v = f ( ctx ) 
~~~ return v 
~~ ~~ return cast ( W , with_lineno_and_col ) 
~~ def _read_namespaced ( 
ctx : ReaderContext , allowed_suffix : Optional [ str ] = None 
) -> Tuple [ Optional [ str ] , str ] : 
ns : List [ str ] = [ ] 
name : List [ str ] = [ ] 
reader = ctx . reader 
has_ns = False 
~~~ token = reader . peek ( ) 
if token == "/" : 
~~~ reader . next_token ( ) 
if has_ns : 
~~ elif len ( name ) == 0 : 
~~~ name . append ( "/" ) 
~~~ if "/" in name : 
~~ has_ns = True 
ns = name 
name = [ ] 
~~ ~~ elif ns_name_chars . match ( token ) : 
name . append ( token ) 
~~ elif allowed_suffix is not None and token == allowed_suffix : 
~~ ~~ ns_str = None if not has_ns else "" . join ( ns ) 
name_str = "" . join ( name ) 
if ns_str is None : 
~~~ if "/" in name_str and name_str != "/" : 
~~~ raise SyntaxError ( "\ ) 
~~ ~~ assert ns_str is None or len ( ns_str ) > 0 
return ns_str , name_str 
~~ def _read_coll ( 
ctx : ReaderContext , 
f : Callable [ [ Collection [ Any ] ] , Union [ llist . List , lset . Set , vector . Vector ] ] , 
end_token : str , 
coll_name : str , 
coll : List = [ ] 
if token == "" : 
~~ if whitespace_chars . match ( token ) : 
~~~ reader . advance ( ) 
~~ if token == end_token : 
return f ( coll ) 
~~ elem = _read_next ( ctx ) 
if elem is COMMENT : 
~~ coll . append ( elem ) 
~~ ~~ def _read_list ( ctx : ReaderContext ) -> llist . List : 
start = ctx . reader . advance ( ) 
assert start == "(" 
return _read_coll ( ctx , llist . list , ")" , "list" ) 
~~ def _read_vector ( ctx : ReaderContext ) -> vector . Vector : 
assert start == "[" 
return _read_coll ( ctx , vector . vector , "]" , "vector" ) 
~~ def _read_set ( ctx : ReaderContext ) -> lset . Set : 
assert start == "{" 
def set_if_valid ( s : Collection ) -> lset . Set : 
~~~ if len ( s ) != len ( set ( s ) ) : 
~~ return lset . set ( s ) 
~~ return _read_coll ( ctx , set_if_valid , "}" , "set" ) 
~~ def _read_map ( ctx : ReaderContext ) -> lmap . Map : 
start = reader . advance ( ) 
d : MutableMapping [ Any , Any ] = { } 
~~~ if reader . peek ( ) == "}" : 
~~ k = _read_next ( ctx ) 
if k is COMMENT : 
~~ v = _read_next ( ctx ) 
if v is COMMENT : 
~~ if k in d : 
~~ d [ k ] = v 
~~ return lmap . map ( d ) 
ctx : ReaderContext 
) -> MaybeNumber : 
chars : List [ str ] = [ ] 
is_complex = False 
is_decimal = False 
is_float = False 
is_integer = False 
is_ratio = False 
if token == "-" : 
~~~ following_token = reader . next_token ( ) 
if not begin_num_chars . match ( following_token ) : 
~~~ reader . pushback ( ) 
~~~ for _ in chars : 
~~~ raise SyntaxError ( 
~~ return _read_sym ( ctx ) 
~~ chars . append ( token ) 
~~ elif token == "." : 
~~~ if is_float : 
~~ is_float = True 
~~ elif token == "J" : 
~~~ if is_complex : 
~~ is_complex = True 
~~ elif token == "M" : 
~~~ if is_decimal : 
~~ is_decimal = True 
~~ elif token == "N" : 
~~~ if is_integer : 
~~ is_integer = True 
~~ elif token == "/" : 
~~~ if is_ratio : 
~~ is_ratio = True 
~~ elif not num_chars . match ( token ) : 
~~ reader . next_token ( ) 
chars . append ( token ) 
s = "" . join ( chars ) 
if ( 
sum ( 
is_complex and is_decimal , 
is_complex and is_integer , 
is_complex and is_ratio , 
is_decimal or is_float , 
is_integer , 
is_ratio , 
> 1 
~~ if is_complex : 
~~~ imaginary = float ( s [ : - 1 ] ) if is_float else int ( s [ : - 1 ] ) 
return complex ( 0 , imaginary ) 
~~ elif is_decimal : 
~~~ return decimal . Decimal ( s [ : - 1 ] ) 
~~ except decimal . InvalidOperation : 
~~ ~~ elif is_float : 
~~~ return float ( s ) 
~~ elif is_ratio : 
num , denominator = s . split ( "/" ) 
return Fraction ( numerator = int ( num ) , denominator = int ( denominator ) ) 
~~ elif is_integer : 
~~~ return int ( s [ : - 1 ] ) 
~~ return int ( s ) 
~~ def _read_str ( ctx : ReaderContext , allow_arbitrary_escapes : bool = False ) -> str : 
s : List [ str ] = [ ] 
~~~ token = reader . next_token ( ) 
~~ if token == "\\\\" : 
escape_char = _STR_ESCAPE_CHARS . get ( token , None ) 
if escape_char : 
~~~ s . append ( escape_char ) 
~~ if allow_arbitrary_escapes : 
~~~ s . append ( "\\\\" ) 
~~ ~~ if token == \ : 
return "" . join ( s ) 
~~ s . append ( token ) 
~~ ~~ def _read_sym ( ctx : ReaderContext ) -> MaybeSymbol : 
ns , name = _read_namespaced ( ctx , allowed_suffix = "#" ) 
if not ctx . is_syntax_quoted and name . endswith ( "#" ) : 
~~ if ns is not None : 
~~~ if any ( map ( lambda s : len ( s ) == 0 , ns . split ( "." ) ) ) : 
~~ ~~ if name . startswith ( "." ) and ns is not None : 
~~ if ns is None : 
~~~ if name == "nil" : 
~~ elif name == "true" : 
~~ elif name == "false" : 
~~ ~~ if ctx . is_syntax_quoted and not name . endswith ( "#" ) : 
~~~ return ctx . resolve ( symbol . symbol ( name , ns ) ) 
~~ return symbol . symbol ( name , ns = ns ) 
~~ def _read_kw ( ctx : ReaderContext ) -> keyword . Keyword : 
assert start == ":" 
ns , name = _read_namespaced ( ctx ) 
if "." in name : 
~~ return keyword . keyword ( name , ns = ns ) 
~~ def _read_meta ( ctx : ReaderContext ) -> IMeta : 
assert start == "^" 
meta = _read_next_consuming_comment ( ctx ) 
meta_map : Optional [ lmap . Map [ LispForm , LispForm ] ] = None 
if isinstance ( meta , symbol . Symbol ) : 
~~~ meta_map = lmap . map ( { keyword . keyword ( "tag" ) : meta } ) 
~~ elif isinstance ( meta , keyword . Keyword ) : 
~~~ meta_map = lmap . map ( { meta : True } ) 
~~ elif isinstance ( meta , lmap . Map ) : 
~~~ meta_map = meta 
~~ obj_with_meta = _read_next_consuming_comment ( ctx ) 
~~ ~~ def _read_function ( ctx : ReaderContext ) -> llist . List : 
if ctx . is_in_anon_fn : 
~~ with ctx . in_anon_fn ( ) : 
~~~ form = _read_list ( ctx ) 
~~ arg_set = set ( ) 
def arg_suffix ( arg_num ) : 
~~~ if arg_num is None : 
~~~ return "1" 
~~ elif arg_num == "&" : 
~~~ return "rest" 
~~~ return arg_num 
~~ ~~ def sym_replacement ( arg_num ) : 
~~~ suffix = arg_suffix ( arg_num ) 
return symbol . symbol ( f"arg-{suffix}" ) 
~~ def identify_and_replace ( f ) : 
~~~ if isinstance ( f , symbol . Symbol ) : 
~~~ if f . ns is None : 
~~~ match = fn_macro_args . match ( f . name ) 
if match is not None : 
~~~ arg_num = match . group ( 2 ) 
suffix = arg_suffix ( arg_num ) 
arg_set . add ( suffix ) 
return sym_replacement ( arg_num ) 
~~ ~~ ~~ return f 
~~ body = walk . postwalk ( identify_and_replace , form ) if len ( form ) > 0 else None 
arg_list : List [ symbol . Symbol ] = [ ] 
numbered_args = sorted ( map ( int , filter ( lambda k : k != "rest" , arg_set ) ) ) 
if len ( numbered_args ) > 0 : 
~~~ max_arg = max ( numbered_args ) 
arg_list = [ sym_replacement ( str ( i ) ) for i in range ( 1 , max_arg + 1 ) ] 
if "rest" in arg_set : 
~~~ arg_list . append ( _AMPERSAND ) 
arg_list . append ( sym_replacement ( "rest" ) ) 
~~ ~~ return llist . l ( _FN , vector . vector ( arg_list ) , body ) 
~~ def _read_quoted ( ctx : ReaderContext ) -> llist . List : 
assert start == "\ 
next_form = _read_next_consuming_comment ( ctx ) 
return llist . l ( _QUOTE , next_form ) 
~~ def _expand_syntax_quote ( 
ctx : ReaderContext , form : IterableLispForm 
) -> Iterable [ LispForm ] : 
expanded = [ ] 
~~~ if _is_unquote ( elem ) : 
~~~ expanded . append ( llist . l ( _LIST , elem [ 1 ] ) ) 
~~ elif _is_unquote_splicing ( elem ) : 
~~~ expanded . append ( elem [ 1 ] ) 
~~~ expanded . append ( llist . l ( _LIST , _process_syntax_quoted_form ( ctx , elem ) ) ) 
~~ ~~ return expanded 
~~ def _process_syntax_quoted_form ( ctx : ReaderContext , form : ReaderForm ) -> ReaderForm : 
lconcat = lambda v : llist . list ( v ) . cons ( _CONCAT ) 
if _is_unquote ( form ) : 
~~ elif _is_unquote_splicing ( form ) : 
~~ elif isinstance ( form , llist . List ) : 
~~~ return llist . l ( _SEQ , lconcat ( _expand_syntax_quote ( ctx , form ) ) ) 
~~ elif isinstance ( form , vector . Vector ) : 
~~~ return llist . l ( _APPLY , _VECTOR , lconcat ( _expand_syntax_quote ( ctx , form ) ) ) 
~~ elif isinstance ( form , lset . Set ) : 
~~~ return llist . l ( _APPLY , _HASH_SET , lconcat ( _expand_syntax_quote ( ctx , form ) ) ) 
~~ elif isinstance ( form , lmap . Map ) : 
~~~ flat_kvs = seq ( form . items ( ) ) . flatten ( ) . to_list ( ) 
return llist . l ( _APPLY , _HASH_MAP , lconcat ( _expand_syntax_quote ( ctx , flat_kvs ) ) ) 
~~ elif isinstance ( form , symbol . Symbol ) : 
~~~ if form . ns is None and form . name . endswith ( "#" ) : 
~~~ return llist . l ( _QUOTE , ctx . gensym_env [ form . name ] ) 
~~~ genned = symbol . symbol ( langutil . genname ( form . name [ : - 1 ] ) ) . with_meta ( 
form . meta 
ctx . gensym_env [ form . name ] = genned 
return llist . l ( _QUOTE , genned ) 
~~ ~~ return llist . l ( _QUOTE , form ) 
~~~ return form 
~~ ~~ def _read_syntax_quoted ( ctx : ReaderContext ) -> ReaderForm : 
assert start == "`" 
with ctx . syntax_quoted ( ) : 
~~~ return _process_syntax_quoted_form ( ctx , _read_next_consuming_comment ( ctx ) ) 
~~ ~~ def _read_unquote ( ctx : ReaderContext ) -> LispForm : 
assert start == "~" 
with ctx . unquoted ( ) : 
~~~ next_char = ctx . reader . peek ( ) 
if next_char == "@" : 
~~~ ctx . reader . advance ( ) 
return llist . l ( _UNQUOTE_SPLICING , next_form ) 
~~~ next_form = _read_next_consuming_comment ( ctx ) 
return llist . l ( _UNQUOTE , next_form ) 
~~ ~~ ~~ def _read_deref ( ctx : ReaderContext ) -> LispForm : 
assert start == "@" 
return llist . l ( _DEREF , next_form ) 
~~ def _read_character ( ctx : ReaderContext ) -> str : 
assert start == "\\\\" 
token = reader . peek ( ) 
~~~ if token == "" or whitespace_chars . match ( token ) : 
~~ if not alphanumeric_chars . match ( token ) : 
token = reader . next_token ( ) 
~~ char = "" . join ( s ) 
special = _SPECIAL_CHARS . get ( char , None ) 
if special is not None : 
~~~ return special 
~~ match = unicode_char . match ( char ) 
~~~ return chr ( int ( f"0x{match.group(1)}" , 16 ) ) 
~~ except ( ValueError , OverflowError ) : 
~~ ~~ if len ( char ) > 1 : 
~~ return char 
~~ def _read_regex ( ctx : ReaderContext ) -> Pattern : 
s = _read_str ( ctx , allow_arbitrary_escapes = True ) 
~~~ return langutil . regex_from_str ( s ) 
~~ except re . error : 
~~ ~~ def _read_reader_macro ( ctx : ReaderContext ) -> LispReaderForm : 
assert start == "#" 
token = ctx . reader . peek ( ) 
if token == "{" : 
~~~ return _read_set ( ctx ) 
~~ elif token == "(" : 
~~~ return _read_function ( ctx ) 
~~ elif token == "\ : 
s = _read_sym ( ctx ) 
return llist . l ( _VAR , s ) 
~~ elif token == \ : 
~~~ return _read_regex ( ctx ) 
~~ elif token == "_" : 
return COMMENT 
~~ elif ns_name_chars . match ( token ) : 
~~~ s = _read_sym ( ctx ) 
assert isinstance ( s , symbol . Symbol ) 
v = _read_next_consuming_comment ( ctx ) 
if s in ctx . data_readers : 
~~~ f = ctx . data_readers [ s ] 
return f ( v ) 
~~ def _read_comment ( ctx : ReaderContext ) -> LispReaderForm : 
assert start == ";" 
if newline_chars . match ( token ) : 
return _read_next ( ctx ) 
~~ if token == "" : 
~~~ return ctx . eof 
~~ reader . advance ( ) 
~~ ~~ def _read_next_consuming_comment ( ctx : ReaderContext ) -> ReaderForm : 
~~~ v = _read_next ( ctx ) 
if v is ctx . eof : 
~~ if v is COMMENT or isinstance ( v , Comment ) : 
~~ return v 
if token == "(" : 
~~~ return _read_list ( ctx ) 
~~ elif token == "[" : 
~~~ return _read_vector ( ctx ) 
~~ elif token == "{" : 
~~~ return _read_map ( ctx ) 
~~ elif begin_num_chars . match ( token ) : 
~~~ return _read_num ( ctx ) 
~~ elif whitespace_chars . match ( token ) : 
~~ elif token == ":" : 
~~~ return _read_kw ( ctx ) 
~~~ return _read_str ( ctx ) 
~~~ return _read_quoted ( ctx ) 
~~ elif token == "\\\\" : 
~~~ return _read_character ( ctx ) 
~~~ return _read_sym ( ctx ) 
~~ elif token == "#" : 
~~~ return _read_reader_macro ( ctx ) 
~~ elif token == "^" : 
~~ elif token == ";" : 
~~~ return _read_comment ( ctx ) 
~~ elif token == "`" : 
~~~ return _read_syntax_quoted ( ctx ) 
~~ elif token == "~" : 
~~~ return _read_unquote ( ctx ) 
~~ elif token == "@" : 
~~~ return _read_deref ( ctx ) 
~~ elif token == "" : 
~~ ~~ def read ( 
stream , 
resolver : Resolver = None , 
data_readers : DataReaders = None , 
eof : Any = EOF , 
is_eof_error : bool = False , 
) -> Iterable [ ReaderForm ] : 
reader = StreamReader ( stream ) 
ctx = ReaderContext ( reader , resolver = resolver , data_readers = data_readers , eof = eof ) 
~~~ expr = _read_next ( ctx ) 
if expr is ctx . eof : 
~~~ if is_eof_error : 
~~~ raise EOFError 
~~ if expr is COMMENT or isinstance ( expr , Comment ) : 
~~ yield expr 
~~ ~~ def read_str ( 
s : str , 
eof : Any = None , 
with io . StringIO ( s ) as buf : 
~~~ yield from read ( 
buf , 
resolver = resolver , 
data_readers = data_readers , 
eof = eof , 
is_eof_error = is_eof_error , 
~~ ~~ def read_file ( 
filename : str , 
with open ( filename ) as f : 
f , 
~~ ~~ def _update_loc ( self , c ) : 
if newline_chars . match ( c ) : 
~~~ self . _col . append ( 0 ) 
self . _line . append ( self . _line [ - 1 ] + 1 ) 
~~~ self . _col . append ( self . _col [ - 1 ] + 1 ) 
self . _line . append ( self . _line [ - 1 ] ) 
~~ ~~ def pushback ( self ) -> None : 
if abs ( self . _idx - 1 ) > self . _pushback_depth : 
~~ self . _idx -= 1 
~~ def next_token ( self ) -> str : 
if self . _idx < StreamReader . DEFAULT_INDEX : 
~~~ self . _idx += 1 
~~~ c = self . _stream . read ( 1 ) 
self . _update_loc ( c ) 
self . _buffer . append ( c ) 
~~ return self . peek ( ) 
~~ def _basilisp_bytecode ( 
mtime : int , source_size : int , code : List [ types . CodeType ] 
) -> bytes : 
data = bytearray ( MAGIC_NUMBER ) 
data . extend ( _w_long ( mtime ) ) 
data . extend ( _w_long ( source_size ) ) 
~~ def _get_basilisp_bytecode ( 
fullname : str , mtime : int , source_size : int , cache_data : bytes 
) -> List [ types . CodeType ] : 
exc_details = { "name" : fullname } 
magic = cache_data [ : 4 ] 
raw_timestamp = cache_data [ 4 : 8 ] 
raw_size = cache_data [ 8 : 12 ] 
if magic != MAGIC_NUMBER : 
~~~ message = ( 
logger . debug ( message ) 
~~ elif len ( raw_timestamp ) != 4 : 
raise EOFError ( message ) 
~~ elif _r_long ( raw_timestamp ) != mtime : 
~~ elif len ( raw_size ) != 4 : 
~~ elif _r_long ( raw_size ) != source_size : 
~~ return marshal . loads ( cache_data [ 12 : ] ) 
~~ def _cache_from_source ( path : str ) -> str : 
cache_path , cache_file = os . path . split ( importlib . util . cache_from_source ( path ) ) 
filename , _ = os . path . splitext ( cache_file ) 
return os . path . join ( cache_path , filename + ".lpyc" ) 
~~ def hook_imports ( ) : 
if any ( [ isinstance ( o , BasilispImporter ) for o in sys . meta_path ] ) : 
~~ sys . meta_path . insert ( 
~~ def find_spec ( 
fullname : str , 
target : types . ModuleType = None , 
) -> Optional [ importlib . machinery . ModuleSpec ] : 
package_components = fullname . split ( "." ) 
if path is None : 
~~~ path = sys . path 
module_name = package_components 
~~~ module_name = [ package_components [ - 1 ] ] 
~~ for entry in path : 
~~~ filenames = [ 
for filename in filenames : 
~~~ if os . path . exists ( filename ) : 
~~~ state = { 
"fullname" : fullname , 
"filename" : filename , 
"path" : entry , 
"target" : target , 
"cache_filename" : _cache_from_source ( filename ) , 
logger . debug ( 
return importlib . machinery . ModuleSpec ( 
fullname , self , origin = filename , loader_state = state 
~~ ~~ ~~ return None 
~~ def _exec_cached_module ( 
loader_state : Mapping [ str , str ] , 
path_stats : Mapping [ str , int ] , 
filename = loader_state [ "filename" ] 
cache_filename = loader_state [ "cache_filename" ] 
with timed ( 
lambda duration : logger . debug ( 
cache_data = self . get_data ( cache_filename ) 
cached_code = _get_basilisp_bytecode ( 
fullname , path_stats [ "mtime" ] , path_stats [ "size" ] , cache_data 
compiler . compile_bytecode ( 
cached_code , 
compiler . GeneratorContext ( filename = filename ) , 
compiler . PythonASTOptimizer ( ) , 
~~ ~~ def _exec_module ( 
~~~ all_bytecode = [ ] 
def add_bytecode ( bytecode : types . CodeType ) : 
~~~ all_bytecode . append ( bytecode ) 
forms = reader . read_file ( filename , resolver = runtime . resolve_alias ) 
forms , 
compiler . CompilerContext ( filename = filename ) , 
collect_bytecode = add_bytecode , 
~~ cache_file_bytes = _basilisp_bytecode ( 
path_stats [ "mtime" ] , path_stats [ "size" ] , all_bytecode 
self . _cache_bytecode ( filename , cache_filename , cache_file_bytes ) 
~~ def exec_module ( self , module ) : 
fullname = module . __name__ 
cached = self . _cache [ fullname ] 
cached [ "module" ] = module 
spec = cached [ "spec" ] 
filename = spec . loader_state [ "filename" ] 
path_stats = self . path_stats ( filename ) 
ns_name = demunge ( fullname ) 
ns : runtime . Namespace = runtime . set_current_ns ( ns_name ) . value 
ns . module = module 
if os . getenv ( _NO_CACHE_ENVVAR , None ) == "true" : 
~~~ self . _exec_module ( fullname , spec . loader_state , path_stats , module ) 
~~~ self . _exec_cached_module ( 
fullname , spec . loader_state , path_stats , module 
~~ except ( EOFError , ImportError , IOError , OSError ) as e : 
self . _exec_module ( fullname , spec . loader_state , path_stats , module ) 
~~ ~~ runtime . Namespace . add_default_import ( ns_name ) 
~~ def symbol ( name : str , ns : Optional [ str ] = None , meta = None ) -> Symbol : 
return Symbol ( name , ns = ns , meta = meta ) 
~~ def complete ( 
) -> Iterable [ str ] : 
assert text . startswith ( ":" ) 
interns = kw_cache . deref ( ) 
text = text [ 1 : ] 
if "/" in text : 
~~~ prefix , suffix = text . split ( "/" , maxsplit = 1 ) 
results = filter ( 
lambda kw : ( kw . ns is not None and kw . ns == prefix ) 
and kw . name . startswith ( suffix ) , 
interns . itervalues ( ) , 
~~~ results = filter ( 
lambda kw : kw . name . startswith ( text ) 
or ( kw . ns is not None and kw . ns . startswith ( text ) ) , 
~~ return map ( str , results ) 
~~ def __get_or_create ( 
) -> PMap : 
if h in kw_cache : 
~~~ return kw_cache 
~~ kw = Keyword ( name , ns = ns ) 
return kw_cache . set ( h , kw ) 
~~ def keyword ( 
name : str , 
ns : Optional [ str ] = None , 
) -> Keyword : 
h = hash ( ( name , ns ) ) 
return kw_cache . swap ( __get_or_create , h , name , ns ) [ h ] 
~~ def _chain_py_ast ( * genned : GeneratedPyAST , ) -> Tuple [ PyASTStream , PyASTStream ] : 
deps = chain . from_iterable ( map ( lambda n : n . dependencies , genned ) ) 
nodes = map ( lambda n : n . node , genned ) 
return deps , nodes 
~~ def _load_attr ( name : str , ctx : ast . AST = ast . Load ( ) ) -> ast . Attribute : 
attrs = name . split ( "." ) 
def attr_node ( node , idx ) : 
~~~ if idx >= len ( attrs ) : 
~~~ node . ctx = ctx 
return node 
~~ return attr_node ( 
ast . Attribute ( value = node , attr = attrs [ idx ] , ctx = ast . Load ( ) ) , idx + 1 
~~ return attr_node ( ast . Name ( id = attrs [ 0 ] , ctx = ast . Load ( ) ) , 1 ) 
~~ def _simple_ast_generator ( gen_ast ) : 
@ wraps ( gen_ast ) 
def wrapped_ast_generator ( ctx : GeneratorContext , form : LispForm ) -> GeneratedPyAST : 
~~~ return GeneratedPyAST ( node = gen_ast ( ctx , form ) ) 
~~ return wrapped_ast_generator 
~~ def _collection_ast ( 
ctx : GeneratorContext , form : Iterable [ Node ] 
) -> Tuple [ PyASTStream , PyASTStream ] : 
return _chain_py_ast ( * map ( partial ( gen_py_ast , ctx ) , form ) ) 
~~ def _clean_meta ( form : IMeta ) -> LispForm : 
meta = form . meta . dissoc ( reader . READER_LINE_KW , reader . READER_COL_KW ) 
if len ( meta ) == 0 : 
~~ return cast ( lmap . Map , meta ) 
~~ def _ast_with_loc ( 
py_ast : GeneratedPyAST , env : NodeEnv , include_dependencies : bool = False 
) -> GeneratedPyAST : 
if env . line is not None : 
~~~ py_ast . node . lineno = env . line 
if include_dependencies : 
~~~ for dep in py_ast . dependencies : 
~~~ dep . lineno = env . line 
~~ ~~ ~~ if env . col is not None : 
~~~ py_ast . node . col_offset = env . col 
~~~ dep . col_offset = env . col 
~~ ~~ ~~ return py_ast 
~~ def _with_ast_loc ( f ) : 
def with_lineno_and_col ( 
ctx : GeneratorContext , node : Node , * args , ** kwargs 
~~~ py_ast = f ( ctx , node , * args , ** kwargs ) 
return _ast_with_loc ( py_ast , node . env ) 
~~ return with_lineno_and_col 
~~ def _with_ast_loc_deps ( f ) : 
return _ast_with_loc ( py_ast , node . env , include_dependencies = True ) 
~~ def _is_dynamic ( v : Var ) -> bool : 
~~ def _is_redefable ( v : Var ) -> bool : 
~~ def statementize ( e : ast . AST ) -> ast . AST : 
if isinstance ( 
e , 
( 
ast . Assign , 
ast . AnnAssign , 
ast . AugAssign , 
ast . Expr , 
ast . Raise , 
ast . Assert , 
ast . Pass , 
ast . Import , 
ast . ImportFrom , 
ast . If , 
ast . For , 
ast . While , 
ast . Continue , 
ast . Break , 
ast . Try , 
ast . ExceptHandler , 
ast . With , 
ast . FunctionDef , 
ast . Return , 
ast . Yield , 
ast . YieldFrom , 
ast . Global , 
ast . ClassDef , 
ast . AsyncFunctionDef , 
ast . AsyncFor , 
ast . AsyncWith , 
~~~ return e 
~~ return ast . Expr ( value = e ) 
~~ def expressionize ( 
body : GeneratedPyAST , 
fn_name : str , 
args : Optional [ Iterable [ ast . arg ] ] = None , 
vargs : Optional [ ast . arg ] = None , 
) -> ast . FunctionDef : 
args = Maybe ( args ) . or_else_get ( [ ] ) 
body_nodes : List [ ast . AST ] = list ( map ( statementize , body . dependencies ) ) 
body_nodes . append ( ast . Return ( value = body . node ) ) 
return ast . FunctionDef ( 
name = fn_name , 
args = ast . arguments ( 
kwarg = None , 
vararg = vargs , 
kwonlyargs = [ ] , 
defaults = [ ] , 
kw_defaults = [ ] , 
body = body_nodes , 
decorator_list = [ ] , 
returns = None , 
~~ def __should_warn_on_redef ( 
ctx : GeneratorContext , defsym : sym . Symbol , safe_name : str , def_meta : lmap . Map 
) -> bool : 
no_warn_on_redef = def_meta . entry ( SYM_NO_WARN_ON_REDEF_META_KEY , False ) 
if no_warn_on_redef : 
~~ elif safe_name in ctx . current_ns . module . __dict__ : 
~~ elif defsym in ctx . current_ns . interns : 
~~~ var = ctx . current_ns . find ( defsym ) 
if var . meta is not None and var . meta . entry ( SYM_REDEF_META_KEY ) : 
~~ elif var . is_bound : 
ctx : GeneratorContext , node : Def 
assert node . op == NodeOp . DEF 
defsym = node . name 
is_defn = False 
if node . init is not None : 
~~~ if node . init . op == NodeOp . FN : 
~~~ assert isinstance ( node . init , Fn ) 
def_ast = _fn_to_py_ast ( ctx , node . init , def_name = defsym . name ) 
is_defn = True 
~~ elif ( 
node . init . op == NodeOp . WITH_META 
and isinstance ( node . init , WithMeta ) 
and node . init . expr . op == NodeOp . FN 
~~~ assert isinstance ( node . init , WithMeta ) 
def_ast = _with_meta_to_py_ast ( ctx , node . init , def_name = defsym . name ) 
~~~ def_ast = gen_py_ast ( ctx , node . init ) 
~~~ def_ast = GeneratedPyAST ( node = ast . NameConstant ( None ) ) 
~~ ns_name = ast . Call ( func = _NEW_SYM_FN_NAME , args = [ _NS_VAR_NAME ] , keywords = [ ] ) 
def_name = ast . Call ( func = _NEW_SYM_FN_NAME , args = [ ast . Str ( defsym . name ) ] , keywords = [ ] ) 
safe_name = munge ( defsym . name ) 
def_meta = node . meta . form 
is_dynamic = def_meta . entry ( SYM_DYNAMIC_META_KEY , False ) 
dynamic_kwarg = ( 
[ ast . keyword ( arg = "dynamic" , value = ast . NameConstant ( is_dynamic ) ) ] 
if is_dynamic 
else [ ] 
if __should_warn_on_redef ( ctx , defsym , safe_name , def_meta ) : 
~~ meta_ast = gen_py_ast ( ctx , node . meta ) 
if is_defn : 
~~~ def_dependencies = list ( 
chain ( 
[ ] if node . top_level else [ ast . Global ( names = [ safe_name ] ) ] , 
def_ast . dependencies , 
[ ] if meta_ast is None else meta_ast . dependencies , 
ast . Assign ( 
targets = [ ast . Name ( id = safe_name , ctx = ast . Store ( ) ) ] , 
value = def_ast . node , 
~~ return GeneratedPyAST ( 
node = ast . Call ( 
func = _INTERN_VAR_FN_NAME , 
args = [ ns_name , def_name , ast . Name ( id = safe_name , ctx = ast . Load ( ) ) ] , 
keywords = list ( 
dynamic_kwarg , 
[ ] 
if meta_ast is None 
else [ ast . keyword ( arg = "meta" , value = meta_ast . node ) ] , 
dependencies = def_dependencies , 
ctx : GeneratorContext , node : DefType 
assert node . op == NodeOp . DEFTYPE 
type_name = munge ( node . name ) 
ctx . symbol_table . new_symbol ( sym . symbol ( node . name ) , type_name , LocalType . DEFTYPE ) 
bases = [ ] 
for base in node . interfaces : 
~~~ base_node = gen_py_ast ( ctx , base ) 
count ( base_node . dependencies ) == 0 
bases . append ( base_node . node ) 
~~ decorator = ast . Call ( 
func = _ATTR_CLASS_DECORATOR_NAME , 
args = [ ] , 
keywords = [ 
ast . keyword ( arg = "cmp" , value = ast . NameConstant ( False ) ) , 
ast . keyword ( arg = "frozen" , value = ast . NameConstant ( node . is_frozen ) ) , 
ast . keyword ( arg = "slots" , value = ast . NameConstant ( True ) ) , 
with ctx . new_symbol_table ( node . name ) : 
~~~ type_nodes = [ ] 
for field in node . fields : 
~~~ safe_field = munge ( field . name ) 
type_nodes . append ( 
targets = [ ast . Name ( id = safe_field , ctx = ast . Store ( ) ) ] , 
value = ast . Call ( func = _ATTRIB_FIELD_FN_NAME , args = [ ] , keywords = [ ] ) , 
ctx . symbol_table . new_symbol ( sym . symbol ( field . name ) , safe_field , field . local ) 
~~ type_deps : List [ ast . AST ] = [ ] 
for method in node . methods : 
~~~ type_ast = __deftype_method_to_py_ast ( ctx , method ) 
type_nodes . append ( type_ast . node ) 
type_deps . extend ( type_ast . dependencies ) 
node = ast . Name ( id = type_name , ctx = ast . Load ( ) ) , 
dependencies = list ( 
type_deps , 
ast . ClassDef ( 
name = type_name , 
bases = bases , 
keywords = [ ] , 
body = type_nodes , 
decorator_list = [ decorator ] , 
~~ ~~ def _do_to_py_ast ( ctx : GeneratorContext , node : Do ) -> GeneratedPyAST : 
assert node . op == NodeOp . DO 
assert not node . is_body 
body_ast = GeneratedPyAST . reduce ( 
* map ( partial ( gen_py_ast , ctx ) , chain ( node . statements , [ node . ret ] ) ) 
fn_body_ast : List [ ast . AST ] = [ ] 
do_result_name = genname ( _DO_PREFIX ) 
fn_body_ast . extend ( map ( statementize , body_ast . dependencies ) ) 
fn_body_ast . append ( 
targets = [ ast . Name ( id = do_result_name , ctx = ast . Store ( ) ) ] , value = body_ast . node 
return GeneratedPyAST ( 
node = ast . Name ( id = do_result_name , ctx = ast . Load ( ) ) , dependencies = fn_body_ast 
~~ def _synthetic_do_to_py_ast ( ctx : GeneratorContext , node : Do ) -> GeneratedPyAST : 
assert node . is_body 
return GeneratedPyAST . reduce ( 
~~ def __fn_name ( s : Optional [ str ] ) -> str : 
return genname ( "__" + munge ( Maybe ( s ) . or_else_get ( _FN_PREFIX ) ) ) 
~~ def __fn_args_to_py_ast ( 
ctx : GeneratorContext , params : Iterable [ Binding ] , body : Do 
) -> Tuple [ List [ ast . arg ] , Optional [ ast . arg ] , List [ ast . AST ] ] : 
fn_args , varg = [ ] , None 
for binding in params : 
arg_name = genname ( munge ( binding . name ) ) 
if not binding . is_variadic : 
~~~ fn_args . append ( ast . arg ( arg = arg_name , annotation = None ) ) 
ctx . symbol_table . new_symbol ( 
sym . symbol ( binding . name ) , arg_name , LocalType . ARG 
~~~ varg = ast . arg ( arg = arg_name , annotation = None ) 
safe_local = genname ( munge ( binding . name ) ) 
targets = [ ast . Name ( id = safe_local , ctx = ast . Store ( ) ) ] , 
value = ast . Call ( 
func = _COLLECT_ARGS_FN_NAME , 
args = [ ast . Name ( id = arg_name , ctx = ast . Load ( ) ) ] , 
sym . symbol ( binding . name ) , safe_local , LocalType . ARG 
~~ ~~ body_ast = _synthetic_do_to_py_ast ( ctx , body ) 
fn_body_ast . append ( ast . Return ( value = body_ast . node ) ) 
return fn_args , varg , fn_body_ast 
~~ def __single_arity_fn_to_py_ast ( 
ctx : GeneratorContext , 
node : Fn , 
method : FnMethod , 
def_name : Optional [ str ] = None , 
meta_node : Optional [ MetaNode ] = None , 
assert node . op == NodeOp . FN 
assert method . op == NodeOp . FN_METHOD 
lisp_fn_name = node . local . name if node . local is not None else None 
py_fn_name = __fn_name ( lisp_fn_name ) if def_name is None else munge ( def_name ) 
py_fn_node = ast . AsyncFunctionDef if node . is_async else ast . FunctionDef 
with ctx . new_symbol_table ( py_fn_name ) , ctx . new_recur_point ( 
method . loop_id , RecurType . FN , is_variadic = node . is_variadic 
~~~ if lisp_fn_name is not None : 
~~~ ctx . symbol_table . new_symbol ( 
sym . symbol ( lisp_fn_name ) , py_fn_name , LocalType . FN 
~~ fn_args , varg , fn_body_ast = __fn_args_to_py_ast ( 
ctx , method . params , method . body 
meta_deps , meta_decorators = __fn_meta ( ctx , meta_node ) 
node = ast . Name ( id = py_fn_name , ctx = ast . Load ( ) ) , 
meta_deps , 
py_fn_node ( 
name = py_fn_name , 
args = fn_args , 
vararg = varg , 
body = fn_body_ast , 
decorator_list = list ( 
meta_decorators , 
[ _BASILISP_FN_FN_NAME ] , 
[ _TRAMPOLINE_FN_NAME ] 
if ctx . recur_point . has_recur 
else [ ] , 
arity_map : Mapping [ int , str ] , 
default_name : Optional [ str ] = None , 
max_fixed_arity : Optional [ int ] = None , 
is_async : bool = False , 
dispatch_map_name = f"{name}_dispatch_map" 
dispatch_keys , dispatch_vals = [ ] , [ ] 
for k , v in arity_map . items ( ) : 
~~~ dispatch_keys . append ( ast . Num ( k ) ) 
dispatch_vals . append ( ast . Name ( id = v , ctx = ast . Load ( ) ) ) 
~~ handle_return = __handle_async_return if is_async else __handle_return 
nargs_name = genname ( "nargs" ) 
method_name = genname ( "method" ) 
body = [ 
targets = [ ast . Name ( id = nargs_name , ctx = ast . Store ( ) ) ] , 
func = ast . Name ( id = "len" , ctx = ast . Load ( ) ) , 
args = [ ast . Name ( id = _MULTI_ARITY_ARG_NAME , ctx = ast . Load ( ) ) ] , 
targets = [ ast . Name ( id = method_name , ctx = ast . Store ( ) ) ] , 
func = ast . Attribute ( 
value = ast . Name ( id = dispatch_map_name , ctx = ast . Load ( ) ) , 
attr = "get" , 
ctx = ast . Load ( ) , 
args = [ ast . Name ( id = nargs_name , ctx = ast . Load ( ) ) ] , 
ast . If ( 
test = ast . Compare ( 
left = ast . NameConstant ( None ) , 
ops = [ ast . IsNot ( ) ] , 
comparators = [ ast . Name ( id = method_name , ctx = ast . Load ( ) ) ] , 
handle_return ( 
ast . Call ( 
func = ast . Name ( id = method_name , ctx = ast . Load ( ) ) , 
args = [ 
ast . Starred ( 
value = ast . Name ( 
id = _MULTI_ARITY_ARG_NAME , ctx = ast . Load ( ) 
orelse = [ ] 
if default_name is None 
else [ 
left = ast . Name ( id = nargs_name , ctx = ast . Load ( ) ) , 
ops = [ ast . GtE ( ) ] , 
comparators = [ ast . Num ( max_fixed_arity ) ] , 
func = ast . Name ( id = default_name , ctx = ast . Load ( ) ) , 
orelse = [ ] , 
ast . Raise ( 
exc = ast . Call ( 
func = _load_attr ( "basilisp.lang.runtime.RuntimeException" ) , 
ast . Name ( id = nargs_name , ctx = ast . Load ( ) ) , 
cause = None , 
py_fn_node = ast . AsyncFunctionDef if is_async else ast . FunctionDef 
node = ast . Name ( id = name , ctx = ast . Load ( ) ) , 
dependencies = chain ( 
targets = [ ast . Name ( id = dispatch_map_name , ctx = ast . Store ( ) ) ] , 
value = ast . Dict ( keys = dispatch_keys , values = dispatch_vals ) , 
vararg = ast . arg ( arg = _MULTI_ARITY_ARG_NAME , annotation = None ) , 
decorator_list = list ( chain ( meta_decorators , [ _BASILISP_FN_FN_NAME ] ) ) , 
methods : Collection [ FnMethod ] , 
assert all ( [ method . op == NodeOp . FN_METHOD for method in methods ] ) 
arity_to_name = { } 
rest_arity_name : Optional [ str ] = None 
fn_defs = [ ] 
for method in methods : 
~~~ arity_name = f"{py_fn_name}__arity{\ 
if method . is_variadic : 
~~~ rest_arity_name = arity_name 
~~~ arity_to_name [ method . fixed_arity ] = arity_name 
~~ with ctx . new_symbol_table ( arity_name ) , ctx . new_recur_point ( 
fn_defs . append ( 
name = arity_name , 
decorator_list = [ _TRAMPOLINE_FN_NAME ] 
~~ ~~ dispatch_fn_ast = __multi_arity_dispatch_fn ( 
ctx , 
py_fn_name , 
arity_to_name , 
default_name = rest_arity_name , 
max_fixed_arity = node . max_fixed_arity , 
meta_node = meta_node , 
is_async = node . is_async , 
node = dispatch_fn_ast . node , 
dependencies = list ( chain ( fn_defs , dispatch_fn_ast . dependencies ) ) , 
~~ def _fn_to_py_ast ( 
if len ( node . methods ) == 1 : 
~~~ return __single_arity_fn_to_py_ast ( 
ctx , node , next ( iter ( node . methods ) ) , def_name = def_name , meta_node = meta_node 
~~~ return __multi_arity_fn_to_py_ast ( 
ctx , node , node . methods , def_name = def_name , meta_node = meta_node 
~~ ~~ def __if_body_to_py_ast ( 
ctx : GeneratorContext , node : Node , result_name : str 
if node . op == NodeOp . RECUR and ctx . recur_point . type == RecurType . LOOP : 
~~~ assert isinstance ( node , Recur ) 
return _recur_to_py_ast ( ctx , node ) 
~~ elif node . op == NodeOp . DO : 
if_body = _synthetic_do_to_py_ast ( ctx , node . assoc ( is_body = True ) ) 
node = ast . Assign ( 
targets = [ ast . Name ( id = result_name , ctx = ast . Store ( ) ) ] , value = if_body . node 
dependencies = list ( map ( statementize , if_body . dependencies ) ) , 
~~~ py_ast = gen_py_ast ( ctx , node ) 
targets = [ ast . Name ( id = result_name , ctx = ast . Store ( ) ) ] , value = py_ast . node 
dependencies = py_ast . dependencies , 
~~ ~~ def _if_to_py_ast ( ctx : GeneratorContext , node : If ) -> GeneratedPyAST : 
assert node . op == NodeOp . IF 
test_ast = gen_py_ast ( ctx , node . test ) 
result_name = genname ( _IF_RESULT_PREFIX ) 
then_ast = __if_body_to_py_ast ( ctx , node . then , result_name ) 
else_ast = __if_body_to_py_ast ( ctx , node . else_ , result_name ) 
test_name = genname ( _IF_TEST_PREFIX ) 
test_assign = ast . Assign ( 
targets = [ ast . Name ( id = test_name , ctx = ast . Store ( ) ) ] , value = test_ast . node 
ifstmt = ast . If ( 
test = ast . BoolOp ( 
op = ast . Or ( ) , 
values = [ 
ast . Compare ( 
ops = [ ast . Is ( ) ] , 
comparators = [ ast . Name ( id = test_name , ctx = ast . Load ( ) ) ] , 
left = ast . NameConstant ( False ) , 
values = [ ] , 
body = list ( map ( statementize , chain ( else_ast . dependencies , [ else_ast . node ] ) ) ) , 
orelse = list ( map ( statementize , chain ( then_ast . dependencies , [ then_ast . node ] ) ) ) , 
node = ast . Name ( id = result_name , ctx = ast . Load ( ) ) , 
dependencies = list ( chain ( test_ast . dependencies , [ test_assign , ifstmt ] ) ) , 
~~ def _import_to_py_ast ( ctx : GeneratorContext , node : Import ) -> GeneratedPyAST : 
assert node . op == NodeOp . IMPORT 
deps : List [ ast . AST ] = [ ] 
for alias in node . aliases : 
~~~ safe_name = munge ( alias . name ) 
~~~ module = importlib . import_module ( safe_name ) 
if alias . alias is not None : 
~~~ ctx . add_import ( sym . symbol ( alias . name ) , module , sym . symbol ( alias . alias ) ) 
~~~ ctx . add_import ( sym . symbol ( alias . name ) , module ) 
~~ ~~ except ModuleNotFoundError as e : 
~~~ raise ImportError ( 
) from e 
~~ py_import_alias = ( 
munge ( alias . alias ) 
if alias . alias is not None 
else safe_name . split ( "." , maxsplit = 1 ) [ 0 ] 
deps . append ( 
targets = [ ast . Name ( id = py_import_alias , ctx = ast . Store ( ) ) ] , 
func = _load_attr ( "builtins.__import__" ) , 
args = [ ast . Str ( safe_name ) ] , 
last = ast . Name ( id = py_import_alias , ctx = ast . Load ( ) ) 
func = _load_attr ( f"{_NS_VAR_VALUE}.add_import" ) , 
func = _NEW_SYM_FN_NAME , args = [ ast . Str ( safe_name ) ] , keywords = [ ] 
last , 
return GeneratedPyAST ( node = last , dependencies = deps ) 
~~ def _invoke_to_py_ast ( ctx : GeneratorContext , node : Invoke ) -> GeneratedPyAST : 
assert node . op == NodeOp . INVOKE 
fn_ast = gen_py_ast ( ctx , node . fn ) 
args_deps , args_nodes = _collection_ast ( ctx , node . args ) 
node = ast . Call ( func = fn_ast . node , args = list ( args_nodes ) , keywords = [ ] ) , 
dependencies = list ( chain ( fn_ast . dependencies , args_deps ) ) , 
~~ def _let_to_py_ast ( ctx : GeneratorContext , node : Let ) -> GeneratedPyAST : 
assert node . op == NodeOp . LET 
with ctx . new_symbol_table ( "let" ) : 
~~~ let_body_ast : List [ ast . AST ] = [ ] 
~~~ init_node = binding . init 
assert init_node is not None 
init_ast = gen_py_ast ( ctx , init_node ) 
binding_name = genname ( munge ( binding . name ) ) 
let_body_ast . extend ( init_ast . dependencies ) 
let_body_ast . append ( 
targets = [ ast . Name ( id = binding_name , ctx = ast . Store ( ) ) ] , 
value = init_ast . node , 
sym . symbol ( binding . name ) , binding_name , LocalType . LET 
~~ let_result_name = genname ( "let_result" ) 
body_ast = _synthetic_do_to_py_ast ( ctx , node . body ) 
let_body_ast . extend ( map ( statementize , body_ast . dependencies ) ) 
targets = [ ast . Name ( id = let_result_name , ctx = ast . Store ( ) ) ] , 
value = body_ast . node , 
node = ast . Name ( id = let_result_name , ctx = ast . Load ( ) ) , dependencies = let_body_ast 
~~ ~~ def _loop_to_py_ast ( ctx : GeneratorContext , node : Loop ) -> GeneratedPyAST : 
assert node . op == NodeOp . LOOP 
with ctx . new_symbol_table ( "loop" ) : 
~~~ binding_names = [ ] 
init_bindings : List [ ast . AST ] = [ ] 
init_bindings . extend ( init_ast . dependencies ) 
binding_names . append ( binding_name ) 
init_bindings . append ( 
sym . symbol ( binding . name ) , binding_name , LocalType . LOOP 
~~ loop_result_name = genname ( "loop" ) 
with ctx . new_recur_point ( 
node . loop_id , RecurType . LOOP , binding_names = binding_names 
~~~ loop_body_ast : List [ ast . AST ] = [ ] 
loop_body_ast . extend ( body_ast . dependencies ) 
loop_body_ast . append ( 
targets = [ ast . Name ( id = loop_result_name , ctx = ast . Store ( ) ) ] , 
loop_body_ast . append ( ast . Break ( ) ) 
node = _load_attr ( loop_result_name ) , 
targets = [ 
ast . Name ( id = loop_result_name , ctx = ast . Store ( ) ) 
value = ast . NameConstant ( None ) , 
init_bindings , 
ast . While ( 
test = ast . NameConstant ( True ) , 
body = loop_body_ast , 
~~ ~~ ~~ def _quote_to_py_ast ( ctx : GeneratorContext , node : Quote ) -> GeneratedPyAST : 
assert node . op == NodeOp . QUOTE 
return _const_node_to_py_ast ( ctx , node . expr ) 
~~ def __fn_recur_to_py_ast ( ctx : GeneratorContext , node : Recur ) -> GeneratedPyAST : 
assert node . op == NodeOp . RECUR 
assert ctx . recur_point . is_variadic is not None 
recur_nodes : List [ ast . AST ] = [ ] 
recur_deps : List [ ast . AST ] = [ ] 
for expr in node . exprs : 
~~~ expr_ast = gen_py_ast ( ctx , expr ) 
recur_nodes . append ( expr_ast . node ) 
recur_deps . extend ( expr_ast . dependencies ) 
func = _TRAMPOLINE_ARGS_FN_NAME , 
args = list ( 
chain ( [ ast . NameConstant ( ctx . recur_point . is_variadic ) ] , recur_nodes ) 
dependencies = recur_deps , 
~~ def __deftype_method_recur_to_py_ast ( 
ctx : GeneratorContext , node : Recur 
~~ this_entry = ctx . symbol_table . find_symbol ( ctx . current_this ) 
ast . NameConstant ( ctx . recur_point . is_variadic ) , 
ast . Name ( id = this_entry . munged , ctx = ast . Load ( ) ) , 
recur_nodes , 
~~ def __loop_recur_to_py_ast ( ctx : GeneratorContext , node : Recur ) -> GeneratedPyAST : 
recur_targets : List [ ast . Name ] = [ ] 
recur_exprs : List [ ast . AST ] = [ ] 
for name , expr in zip ( ctx . recur_point . binding_names , node . exprs ) : 
recur_targets . append ( ast . Name ( id = name , ctx = ast . Store ( ) ) ) 
recur_exprs . append ( expr_ast . node ) 
~~ if len ( recur_targets ) == 1 : 
~~~ assert len ( recur_exprs ) == 1 
recur_deps . append ( ast . Assign ( targets = recur_targets , value = recur_exprs [ 0 ] ) ) 
~~~ recur_deps . append ( 
targets = [ ast . Tuple ( elts = recur_targets , ctx = ast . Store ( ) ) ] , 
value = ast . Tuple ( elts = recur_exprs , ctx = ast . Load ( ) ) , 
~~ recur_deps . append ( ast . Continue ( ) ) 
return GeneratedPyAST ( node = ast . NameConstant ( None ) , dependencies = recur_deps ) 
~~ def _recur_to_py_ast ( ctx : GeneratorContext , node : Recur ) -> GeneratedPyAST : 
handle_recur = _RECUR_TYPE_HANDLER . get ( ctx . recur_point . type ) 
handle_recur is not None 
ctx . recur_point . has_recur = True 
return handle_recur ( ctx , node ) 
~~ def _set_bang_to_py_ast ( ctx : GeneratorContext , node : SetBang ) -> GeneratedPyAST : 
assert node . op == NodeOp . SET_BANG 
val_temp_name = genname ( "set_bang_val" ) 
val_ast = gen_py_ast ( ctx , node . val ) 
target = node . target 
assert isinstance ( 
target , ( HostField , Local , VarRef ) 
if isinstance ( target , HostField ) : 
~~~ target_ast = _interop_prop_to_py_ast ( ctx , target , is_assigning = True ) 
~~ elif isinstance ( target , VarRef ) : 
~~~ target_ast = _var_sym_to_py_ast ( ctx , target , is_assigning = True ) 
~~ elif isinstance ( target , Local ) : 
~~~ target_ast = _local_sym_to_py_ast ( ctx , target , is_assigning = True ) 
~~~ raise GeneratorException ( 
node = ast . Name ( id = val_temp_name , ctx = ast . Load ( ) ) , 
val_ast . dependencies , 
targets = [ ast . Name ( id = val_temp_name , ctx = ast . Store ( ) ) ] , 
value = val_ast . node , 
target_ast . dependencies , 
[ ast . Assign ( targets = [ target_ast . node ] , value = val_ast . node ) ] , 
~~ def _throw_to_py_ast ( ctx : GeneratorContext , node : Throw ) -> GeneratedPyAST : 
assert node . op == NodeOp . THROW 
throw_fn = genname ( _THROW_PREFIX ) 
exc_ast = gen_py_ast ( ctx , node . exception ) 
raise_body = ast . Raise ( exc = exc_ast . node , cause = None ) 
node = ast . Call ( func = ast . Name ( id = throw_fn , ctx = ast . Load ( ) ) , args = [ ] , keywords = [ ] ) , 
dependencies = [ 
ast . FunctionDef ( 
name = throw_fn , 
vararg = None , 
body = list ( chain ( exc_ast . dependencies , [ raise_body ] ) ) , 
~~ def _try_to_py_ast ( ctx : GeneratorContext , node : Try ) -> GeneratedPyAST : 
assert node . op == NodeOp . TRY 
try_expr_name = genname ( "try_expr" ) 
catch_handlers = list ( 
map ( partial ( __catch_to_py_ast , ctx , try_expr_name = try_expr_name ) , node . catches ) 
finallys : List [ ast . AST ] = [ ] 
if node . finally_ is not None : 
~~~ finally_ast = _synthetic_do_to_py_ast ( ctx , node . finally_ ) 
finallys . extend ( map ( statementize , finally_ast . dependencies ) ) 
finallys . append ( statementize ( finally_ast . node ) ) 
node = ast . Name ( id = try_expr_name , ctx = ast . Load ( ) ) , 
ast . Try ( 
body = list ( 
body_ast . dependencies , 
targets = [ ast . Name ( id = try_expr_name , ctx = ast . Store ( ) ) ] , 
handlers = catch_handlers , 
finalbody = finallys , 
~~ def _local_sym_to_py_ast ( 
ctx : GeneratorContext , node : Local , is_assigning : bool = False 
assert node . op == NodeOp . LOCAL 
sym_entry = ctx . symbol_table . find_symbol ( sym . symbol ( node . name ) ) 
assert sym_entry is not None 
if node . local == LocalType . FIELD : 
~~~ this_entry = ctx . symbol_table . find_symbol ( ctx . current_this ) 
node = _load_attr ( 
f"{this_entry.munged}.{sym_entry.munged}" , 
ctx = ast . Store ( ) if is_assigning else ast . Load ( ) , 
~~~ return GeneratedPyAST ( 
node = ast . Name ( 
id = sym_entry . munged , ctx = ast . Store ( ) if is_assigning else ast . Load ( ) 
~~ ~~ def __var_find_to_py_ast ( 
var_name : str , ns_name : str , py_var_ctx : ast . AST 
node = ast . Attribute ( 
func = _FIND_VAR_FN_NAME , 
func = _NEW_SYM_FN_NAME , 
args = [ ast . Str ( var_name ) ] , 
keywords = [ ast . keyword ( arg = "ns" , value = ast . Str ( ns_name ) ) ] , 
attr = "value" , 
ctx = py_var_ctx , 
~~ def _var_sym_to_py_ast ( 
ctx : GeneratorContext , node : VarRef , is_assigning : bool = False 
assert node . op == NodeOp . VAR 
var = node . var 
ns = var . ns 
ns_module = ns . module 
safe_ns = munge ( ns_name ) 
var_name = var . name . name 
py_var_ctx = ast . Store ( ) if is_assigning else ast . Load ( ) 
if node . return_var : 
~~ if ctx . use_var_indirection or _is_dynamic ( var ) or _is_redefable ( var ) : 
~~~ return __var_find_to_py_ast ( var_name , ns_name , py_var_ctx ) 
~~ safe_name = munge ( var_name ) 
if safe_name not in ns_module . __dict__ : 
~~~ safe_name = munge ( var_name , allow_builtins = True ) 
~~ if safe_name in ns_module . __dict__ : 
~~~ if ns is ctx . current_ns : 
~~~ return GeneratedPyAST ( node = ast . Name ( id = safe_name , ctx = py_var_ctx ) ) 
~~ return GeneratedPyAST ( node = _load_attr ( f"{safe_ns}.{safe_name}" , ctx = py_var_ctx ) ) 
~~ if ctx . warn_on_var_indirection : 
~~ return __var_find_to_py_ast ( var_name , ns_name , py_var_ctx ) 
~~ def _interop_call_to_py_ast ( ctx : GeneratorContext , node : HostCall ) -> GeneratedPyAST : 
assert node . op == NodeOp . HOST_CALL 
target_ast = gen_py_ast ( ctx , node . target ) 
value = target_ast . node , 
attr = munge ( node . method , allow_builtins = True ) , 
args = list ( args_nodes ) , 
dependencies = list ( chain ( target_ast . dependencies , args_deps ) ) , 
~~ def _interop_prop_to_py_ast ( 
ctx : GeneratorContext , node : HostField , is_assigning : bool = False 
assert node . op == NodeOp . HOST_FIELD 
attr = munge ( node . field ) , 
dependencies = target_ast . dependencies , 
~~ def _maybe_class_to_py_ast ( _ : GeneratorContext , node : MaybeClass ) -> GeneratedPyAST : 
assert node . op == NodeOp . MAYBE_CLASS 
id = Maybe ( _MODULE_ALIASES . get ( node . class_ ) ) . or_else_get ( node . class_ ) , 
~~ def _maybe_host_form_to_py_ast ( 
_ : GeneratorContext , node : MaybeHostForm 
assert node . op == NodeOp . MAYBE_HOST_FORM 
f"{Maybe(_MODULE_ALIASES.get(node.class_)).or_else_get(node.class_)}.{node.field}" 
~~ def _with_meta_to_py_ast ( 
ctx : GeneratorContext , node : WithMeta , ** kwargs 
assert node . op == NodeOp . WITH_META 
handle_expr = _WITH_META_EXPR_HANDLER . get ( node . expr . op ) 
handle_expr is not None 
return handle_expr ( ctx , node . expr , meta_node = node . meta , ** kwargs ) 
~~ def _const_val_to_py_ast ( ctx : GeneratorContext , form : LispForm ) -> GeneratedPyAST : 
handle_value = _CONST_VALUE_HANDLERS . get ( type ( form ) ) 
if handle_value is None and isinstance ( form , ISeq ) : 
return handle_value ( ctx , form ) 
~~ def _collection_literal_to_py_ast ( 
ctx : GeneratorContext , form : Iterable [ LispForm ] 
) -> Iterable [ GeneratedPyAST ] : 
yield from map ( partial ( _const_val_to_py_ast , ctx ) , form ) 
~~ def _const_node_to_py_ast ( ctx : GeneratorContext , lisp_ast : Const ) -> GeneratedPyAST : 
assert lisp_ast . op == NodeOp . CONST 
node_type = lisp_ast . type 
handle_const_node = _CONSTANT_HANDLER . get ( node_type ) 
node_val = lisp_ast . val 
return handle_const_node ( ctx , node_val ) 
~~ def gen_py_ast ( ctx : GeneratorContext , lisp_ast : Node ) -> GeneratedPyAST : 
op : NodeOp = lisp_ast . op 
handle_node = _NODE_HANDLERS . get ( op ) 
handle_node is not None 
return handle_node ( ctx , lisp_ast ) 
~~ def _module_imports ( ctx : GeneratorContext ) -> Iterable [ ast . Import ] : 
yield ast . Import ( names = [ ast . alias ( name = "basilisp" , asname = None ) ] ) 
for imp in ctx . imports : 
~~~ name = imp . key . name 
alias = _MODULE_ALIASES . get ( name , None ) 
yield ast . Import ( names = [ ast . alias ( name = name , asname = alias ) ] ) 
~~ ~~ def _from_module_import ( ) -> ast . ImportFrom : 
return ast . ImportFrom ( 
module = "basilisp.lang.runtime" , 
names = [ ast . alias ( name = "Var" , asname = _VAR_ALIAS ) ] , 
level = 0 , 
~~ def _ns_var ( 
py_ns_var : str = _NS_VAR , lisp_ns_var : str = LISP_NS_VAR , lisp_ns_ns : str = CORE_NS 
) -> ast . Assign : 
return ast . Assign ( 
targets = [ ast . Name ( id = py_ns_var , ctx = ast . Store ( ) ) ] , 
args = [ ast . Str ( lisp_ns_var ) ] , 
keywords = [ ast . keyword ( arg = "ns" , value = ast . Str ( lisp_ns_ns ) ) ] , 
~~ def py_module_preamble ( ctx : GeneratorContext , ) -> GeneratedPyAST : 
preamble : List [ ast . AST ] = [ ] 
preamble . extend ( _module_imports ( ctx ) ) 
preamble . append ( _from_module_import ( ) ) 
preamble . append ( _ns_var ( ) ) 
return GeneratedPyAST ( node = ast . NameConstant ( None ) , dependencies = preamble ) 
~~ def warn_on_var_indirection ( self ) -> bool : 
return not self . use_var_indirection and self . _opts . entry ( 
WARN_ON_VAR_INDIRECTION , True 
return Set ( pset ( members ) , meta = meta ) 
~~ def s ( * members : T , meta = None ) -> Set [ T ] : 
~~ def _filter_dead_code ( nodes : Iterable [ ast . AST ] ) -> List [ ast . AST ] : 
new_nodes : List [ ast . AST ] = [ ] 
for node in nodes : 
~~~ if isinstance ( node , ( ast . Break , ast . Continue , ast . Return ) ) : 
~~~ new_nodes . append ( node ) 
~~ new_nodes . append ( node ) 
~~ return new_nodes 
~~ def visit_ExceptHandler ( self , node : ast . ExceptHandler ) -> Optional [ ast . AST ] : 
new_node = self . generic_visit ( node ) 
assert isinstance ( new_node , ast . ExceptHandler ) 
return ast . copy_location ( 
ast . ExceptHandler ( 
type = new_node . type , 
name = new_node . name , 
body = _filter_dead_code ( new_node . body ) , 
new_node , 
~~ def visit_Expr ( self , node : ast . Expr ) -> Optional [ ast . Expr ] : 
node . value , 
ast . Name , 
ast . NameConstant , 
ast . Num , 
ast . Str , 
~~ return node 
~~ def visit_FunctionDef ( self , node : ast . FunctionDef ) -> Optional [ ast . AST ] : 
assert isinstance ( new_node , ast . FunctionDef ) 
args = new_node . args , 
decorator_list = new_node . decorator_list , 
returns = new_node . returns , 
~~ def visit_If ( self , node : ast . If ) -> Optional [ ast . AST ] : 
assert isinstance ( new_node , ast . If ) 
test = new_node . test , 
orelse = _filter_dead_code ( new_node . orelse ) , 
~~ def visit_While ( self , node : ast . While ) -> Optional [ ast . AST ] : 
assert isinstance ( new_node , ast . While ) 
~~ def visit_Try ( self , node : ast . Try ) -> Optional [ ast . AST ] : 
assert isinstance ( new_node , ast . Try ) 
handlers = new_node . handlers , 
finalbody = _filter_dead_code ( new_node . finalbody ) , 
~~ def _new_module ( name : str , doc = None ) -> types . ModuleType : 
mod = types . ModuleType ( name , doc = doc ) 
mod . __loader__ = None 
mod . __package__ = None 
mod . __spec__ = None 
return mod 
~~ def first ( o ) : 
~~ if isinstance ( o , ISeq ) : 
~~~ return o . first 
~~ s = to_seq ( o ) 
if s is None : 
~~ return s . first 
~~ def rest ( o ) -> Optional [ ISeq ] : 
~~~ s = o . rest 
~~~ return lseq . EMPTY 
~~ n = to_seq ( o ) 
if n is None : 
~~ return n . rest 
~~ def nthrest ( coll , i : int ) : 
~~~ if coll is None : 
~~~ return coll 
~~ i -= 1 
coll = rest ( coll ) 
~~ ~~ def nthnext ( coll , i : int ) -> Optional [ ISeq ] : 
~~~ return to_seq ( coll ) 
coll = next_ ( coll ) 
~~ ~~ def cons ( o , seq ) -> ISeq : 
if seq is None : 
~~~ return llist . l ( o ) 
~~ if isinstance ( seq , ISeq ) : 
~~~ return seq . cons ( o ) 
~~ return Maybe ( to_seq ( seq ) ) . map ( lambda s : s . cons ( o ) ) . or_else ( lambda : llist . l ( o ) ) 
~~ def to_seq ( o ) -> Optional [ ISeq ] : 
~~~ return _seq_or_nil ( o ) 
~~ if isinstance ( o , ISeqable ) : 
~~~ return _seq_or_nil ( o . seq ( ) ) 
~~ return _seq_or_nil ( lseq . sequence ( o ) ) 
~~ def concat ( * seqs ) -> ISeq : 
allseqs = lseq . sequence ( itertools . chain ( * filter ( None , map ( to_seq , seqs ) ) ) ) 
if allseqs is None : 
~~ return allseqs 
~~ def apply ( f , args ) : 
final = list ( args [ : - 1 ] ) 
~~~ last = args [ - 1 ] 
~~ s = to_seq ( last ) 
if s is not None : 
~~~ final . extend ( s ) 
~~ return f ( * final ) 
~~ def apply_kw ( f , args ) : 
~~ kwargs = to_py ( last , lambda kw : munge ( kw . name , allow_builtins = True ) ) 
return f ( * final , ** kwargs ) 
~~ def nth ( coll , i , notfound = __nth_sentinel ) : 
if coll is None : 
~~~ return coll [ i ] 
~~ except IndexError as ex : 
~~~ if notfound is not __nth_sentinel : 
~~~ return notfound 
~~ raise ex 
~~ except TypeError as ex : 
~~~ for j , e in enumerate ( coll ) : 
~~~ if i == j : 
~~ ~~ if notfound is not __nth_sentinel : 
~~ def assoc ( m , * kvs ) : 
if m is None : 
~~~ return lmap . Map . empty ( ) . assoc ( * kvs ) 
~~ if isinstance ( m , IAssociative ) : 
~~~ return m . assoc ( * kvs ) 
~~ raise TypeError ( 
~~ def update ( m , k , f , * args ) : 
~~~ return lmap . Map . empty ( ) . assoc ( k , f ( None , * args ) ) 
~~~ old_v = m . entry ( k ) 
new_v = f ( old_v , * args ) 
return m . assoc ( k , new_v ) 
~~ def conj ( coll , * xs ) : 
~~~ l = llist . List . empty ( ) 
return l . cons ( * xs ) 
~~ if isinstance ( coll , IPersistentCollection ) : 
~~~ return coll . cons ( * xs ) 
~~ def partial ( f , * args ) : 
def partial_f ( * inner_args ) : 
~~~ return f ( * itertools . chain ( args , inner_args ) ) 
~~ return partial_f 
~~ def deref ( o , timeout_s = None , timeout_val = None ) : 
if isinstance ( o , IDeref ) : 
~~~ return o . deref ( ) 
~~ elif isinstance ( o , IBlockingDeref ) : 
~~~ return o . deref ( timeout_s , timeout_val ) 
~~ def equals ( v1 , v2 ) -> bool : 
if isinstance ( v1 , ( bool , type ( None ) ) ) or isinstance ( v2 , ( bool , type ( None ) ) ) : 
~~~ return v1 is v2 
~~ return v1 == v2 
~~ def divide ( x : LispNumber , y : LispNumber ) -> LispNumber : 
if isinstance ( x , int ) and isinstance ( y , int ) : 
~~~ return Fraction ( x , y ) 
~~ return x / y 
~~ def sort ( coll , f = None ) -> Optional [ ISeq ] : 
return to_seq ( sorted ( coll , key = Maybe ( f ) . map ( functools . cmp_to_key ) . value ) ) 
~~ def contains ( coll , k ) : 
if isinstance ( coll , IAssociative ) : 
~~~ return coll . contains ( k ) 
~~ return k in coll 
~~ def get ( m , k , default = None ) : 
if isinstance ( m , IAssociative ) : 
~~~ return m . entry ( k , default = default ) 
~~~ return m [ k ] 
~~ except ( KeyError , IndexError , TypeError ) as e : 
return default 
~~ ~~ def to_lisp ( o , keywordize_keys : bool = True ) : 
if not isinstance ( o , ( dict , frozenset , list , set , tuple ) ) : 
~~~ return o 
~~~ return _to_lisp_backup ( o , keywordize_keys = keywordize_keys ) 
~~ ~~ def to_py ( o , keyword_fn : Callable [ [ kw . Keyword ] , Any ] = _kw_name ) : 
if isinstance ( o , ISeq ) : 
~~~ return _to_py_list ( o , keyword_fn = keyword_fn ) 
~~ elif not isinstance ( 
o , ( IPersistentList , IPersistentMap , IPersistentSet , IPersistentVector ) 
~~~ return _to_py_backup ( o , keyword_fn = keyword_fn ) 
~~ ~~ def lrepr ( o , human_readable : bool = False ) -> str : 
core_ns = Namespace . get ( sym . symbol ( CORE_NS ) ) 
assert core_ns is not None 
return lobj . lrepr ( 
sym . symbol ( _PRINT_LENGTH_VAR_NAME ) 
) . value , 
sym . symbol ( _PRINT_LEVEL_VAR_NAME ) 
sym . symbol ( _PRINT_READABLY_VAR_NAME ) 
~~ def repl_complete ( text : str , state : int ) -> Optional [ str ] : 
if __NOT_COMPLETEABLE . match ( text ) : 
~~ elif text . startswith ( ":" ) : 
~~~ completions = kw . complete ( text ) 
~~~ ns = get_current_ns ( ) 
completions = ns . complete ( text ) 
~~ return list ( completions ) [ state ] if completions is not None else None 
~~ def _collect_args ( args ) -> ISeq : 
if isinstance ( args , tuple ) : 
~~~ return llist . list ( args ) 
~~ def _trampoline ( f ) : 
def trampoline ( * args , ** kwargs ) : 
~~~ ret = f ( * args , ** kwargs ) 
if isinstance ( ret , _TrampolineArgs ) : 
~~~ args = ret . args 
kwargs = ret . kwargs 
~~ ~~ return trampoline 
~~ def _with_attrs ( ** kwargs ) : 
~~~ for k , v in kwargs . items ( ) : 
~~~ setattr ( f , k , v ) 
~~ return f 
~~ def _fn_with_meta ( f , meta : Optional [ lmap . Map ] ) : 
if not isinstance ( meta , lmap . Map ) : 
~~ if inspect . iscoroutinefunction ( f ) : 
~~~ @ functools . wraps ( f ) 
async def wrapped_f ( * args , ** kwargs ) : 
~~~ return await f ( * args , ** kwargs ) 
def wrapped_f ( * args , ** kwargs ) : 
~~~ return f ( * args , ** kwargs ) 
f . meta . update ( meta ) 
if hasattr ( f , "meta" ) and isinstance ( f . meta , lmap . Map ) 
else meta 
return wrapped_f 
~~ def _basilisp_fn ( f ) : 
assert not hasattr ( f , "meta" ) 
f . _basilisp_fn = True 
f . meta = None 
f . with_meta = partial ( _fn_with_meta , f ) 
return f 
~~ def init_ns_var ( which_ns : str = CORE_NS , ns_var_name : str = NS_VAR_NAME ) -> Var : 
core_sym = sym . Symbol ( which_ns ) 
core_ns = Namespace . get_or_create ( core_sym ) 
ns_var = Var . intern ( core_sym , sym . Symbol ( ns_var_name ) , core_ns , dynamic = True ) 
return ns_var 
~~ def set_current_ns ( 
ns_name : str , 
module : types . ModuleType = None , 
ns_var_name : str = NS_VAR_NAME , 
ns_var_ns : str = NS_VAR_NS , 
) -> Var : 
symbol = sym . Symbol ( ns_name ) 
ns = Namespace . get_or_create ( symbol , module = module ) 
ns_var_sym = sym . Symbol ( ns_var_name , ns = ns_var_ns ) 
ns_var = Maybe ( Var . find ( ns_var_sym ) ) . or_else_raise ( 
lambda : RuntimeException ( 
ns_var . push_bindings ( ns ) 
~~ def ns_bindings ( 
yield ns_var . value 
~~~ ns_var . pop_bindings ( ) 
~~ ~~ def remove_ns_bindings ( ns_var_name : str = NS_VAR_NAME , ns_var_ns : str = NS_VAR_NS ) : 
~~ ~~ def get_current_ns ( 
ns_var_name : str = NS_VAR_NAME , ns_var_ns : str = NS_VAR_NS 
) -> Namespace : 
ns_sym = sym . Symbol ( ns_var_name , ns = ns_var_ns ) 
ns : Namespace = Maybe ( Var . find ( ns_sym ) ) . map ( lambda v : v . value ) . or_else_raise ( 
return ns 
~~ def resolve_alias ( s : sym . Symbol , ns : Optional [ Namespace ] = None ) -> sym . Symbol : 
if s in _SPECIAL_FORMS : 
~~~ return s 
~~ ns = Maybe ( ns ) . or_else ( get_current_ns ) 
if s . ns is not None : 
~~~ aliased_ns = ns . get_alias ( sym . symbol ( s . ns ) ) 
if aliased_ns is not None : 
~~~ return sym . symbol ( s . name , aliased_ns . name ) 
~~~ which_var = ns . find ( sym . symbol ( s . name ) ) 
if which_var is not None : 
~~~ return sym . symbol ( which_var . name . name , which_var . ns . name ) 
~~~ return sym . symbol ( s . name , ns = ns . name ) 
~~ ~~ ~~ def resolve_var ( s : sym . Symbol , ns : Optional [ Namespace ] = None ) -> Optional [ Var ] : 
return Var . find ( resolve_alias ( s , ns ) ) 
~~ def add_generated_python ( 
generated_python : str , 
var_name : str = _GENERATED_PYTHON_VAR_NAME , 
which_ns : Optional [ str ] = None , 
if which_ns is None : 
~~~ which_ns = get_current_ns ( ) . name 
~~ ns_sym = sym . Symbol ( var_name , ns = which_ns ) 
v = Maybe ( Var . find ( ns_sym ) ) . or_else ( 
lambda : Var . intern ( 
sym . symbol ( var_name ) , 
"" , 
dynamic = True , 
meta = lmap . map ( { _PRIVATE_META_KEY : True } ) , 
v . value = v . value + generated_python 
~~ def print_generated_python ( 
var_name : str = _PRINT_GENERATED_PY_VAR_NAME , core_ns_name : str = CORE_NS 
ns_sym = sym . Symbol ( var_name , ns = core_ns_name ) 
Maybe ( Var . find ( ns_sym ) ) 
. map ( lambda v : v . value ) 
~~ def bootstrap ( ns_var_name : str = NS_VAR_NAME , core_ns_name : str = CORE_NS ) -> None : 
core_ns_sym = sym . symbol ( core_ns_name ) 
ns_var_sym = sym . symbol ( ns_var_name , ns = core_ns_name ) 
__NS = Maybe ( Var . find ( ns_var_sym ) ) . or_else_raise ( 
def in_ns ( s : sym . Symbol ) : 
~~~ ns = Namespace . get_or_create ( s ) 
__NS . value = ns 
~~ Var . intern_unbound ( core_ns_sym , sym . symbol ( "unquote" ) ) 
Var . intern_unbound ( core_ns_sym , sym . symbol ( "unquote-splicing" ) ) 
Var . intern ( 
core_ns_sym , sym . symbol ( "in-ns" ) , in_ns , meta = lmap . map ( { _REDEF_META_KEY : True } ) 
core_ns_sym , 
sym . symbol ( _PRINT_GENERATED_PY_VAR_NAME ) , 
False , 
sym . symbol ( _GENERATED_PYTHON_VAR_NAME ) , 
core_ns_sym , sym . symbol ( _PRINT_DUP_VAR_NAME ) , lobj . PRINT_DUP , dynamic = True 
core_ns_sym , sym . symbol ( _PRINT_LENGTH_VAR_NAME ) , lobj . PRINT_LENGTH , dynamic = True 
core_ns_sym , sym . symbol ( _PRINT_LEVEL_VAR_NAME ) , lobj . PRINT_LEVEL , dynamic = True 
core_ns_sym , sym . symbol ( _PRINT_META_VAR_NAME ) , lobj . PRINT_META , dynamic = True 
sym . symbol ( _PRINT_READABLY_VAR_NAME ) , 
lobj . PRINT_READABLY , 
~~ def intern ( 
ns : sym . Symbol , name : sym . Symbol , val , dynamic : bool = False , meta = None 
) -> "Var" : 
var_ns = Namespace . get_or_create ( ns ) 
var = var_ns . intern ( name , Var ( var_ns , name , dynamic = dynamic , meta = meta ) ) 
var . root = val 
return var 
~~ def intern_unbound ( 
ns : sym . Symbol , name : sym . Symbol , dynamic : bool = False , meta = None 
return var_ns . intern ( name , Var ( var_ns , name , dynamic = dynamic , meta = meta ) ) 
~~ def find_in_ns ( ns_sym : sym . Symbol , name_sym : sym . Symbol ) -> "Optional[Var]" : 
ns = Namespace . get ( ns_sym ) 
if ns : 
~~~ return ns . find ( name_sym ) 
~~ def find ( ns_qualified_sym : sym . Symbol ) -> "Optional[Var]" : 
ns = Maybe ( ns_qualified_sym . ns ) . or_else_raise ( 
lambda : ValueError ( 
ns_sym = sym . symbol ( ns ) 
name_sym = sym . symbol ( ns_qualified_sym . name ) 
return Var . find_in_ns ( ns_sym , name_sym ) 
~~ def find_safe ( ns_qualified_sym : sym . Symbol ) -> "Var" : 
v = Var . find ( ns_qualified_sym ) 
~~~ raise RuntimeException ( 
~~ def add_default_import ( cls , module : str ) : 
if module in cls . GATED_IMPORTS : 
~~~ cls . DEFAULT_IMPORTS . swap ( lambda s : s . cons ( sym . symbol ( module ) ) ) 
~~ ~~ def add_alias ( self , alias : sym . Symbol , namespace : "Namespace" ) -> None : 
self . _aliases . swap ( lambda m : m . assoc ( alias , namespace ) ) 
~~ def intern ( self , sym : sym . Symbol , var : Var , force : bool = False ) -> Var : 
m : lmap . Map = self . _interns . swap ( Namespace . _intern , sym , var , force = force ) 
return m . entry ( sym ) 
~~ def _intern ( 
m : lmap . Map , sym : sym . Symbol , new_var : Var , force : bool = False 
) -> lmap . Map : 
var = m . entry ( sym , None ) 
if var is None or force : 
~~~ return m . assoc ( sym , new_var ) 
~~ return m 
~~ def find ( self , sym : sym . Symbol ) -> Optional [ Var ] : 
v = self . interns . entry ( sym , None ) 
~~~ return self . refers . entry ( sym , None ) 
~~ def add_import ( 
self , sym : sym . Symbol , module : types . ModuleType , * aliases : sym . Symbol 
self . _imports . swap ( lambda m : m . assoc ( sym , module ) ) 
if aliases : 
~~~ self . _import_aliases . swap ( 
lambda m : m . assoc ( 
* itertools . chain . from_iterable ( [ ( alias , sym ) for alias in aliases ] ) 
~~ ~~ def get_import ( self , sym : sym . Symbol ) -> Optional [ types . ModuleType ] : 
mod = self . imports . entry ( sym , None ) 
if mod is None : 
~~~ alias = self . import_aliases . get ( sym , None ) 
if alias is None : 
~~ return self . imports . entry ( alias , None ) 
~~ return mod 
~~ def add_refer ( self , sym : sym . Symbol , var : Var ) -> None : 
if not var . is_private : 
~~~ self . _refers . swap ( lambda s : s . assoc ( sym , var ) ) 
~~ ~~ def get_refer ( self , sym : sym . Symbol ) -> Optional [ Var ] : 
return self . refers . entry ( sym , None ) 
~~ def __refer_all ( cls , refers : lmap . Map , other_ns_interns : lmap . Map ) -> lmap . Map : 
final_refers = refers 
for entry in other_ns_interns : 
~~~ s : sym . Symbol = entry . key 
var : Var = entry . value 
~~~ final_refers = final_refers . assoc ( s , var ) 
~~ ~~ return final_refers 
~~ def refer_all ( self , other_ns : "Namespace" ) : 
self . _refers . swap ( Namespace . __refer_all , other_ns . interns ) 
ns_cache : NamespaceMap , 
name : sym . Symbol , 
core_ns_name = CORE_NS , 
ns = ns_cache . entry ( name , None ) 
if ns is not None : 
~~~ return ns_cache 
~~ new_ns = Namespace ( name , module = module ) 
if name . name != core_ns_name : 
~~~ core_ns = ns_cache . entry ( sym . symbol ( core_ns_name ) , None ) 
new_ns . refer_all ( core_ns ) 
~~ return ns_cache . assoc ( name , new_ns ) 
~~ def get_or_create ( 
cls , name : sym . Symbol , module : types . ModuleType = None 
) -> "Namespace" : 
return cls . _NAMESPACES . swap ( Namespace . __get_or_create , name , module = module ) [ 
name 
~~ def get ( cls , name : sym . Symbol ) -> "Optional[Namespace]" : 
return cls . _NAMESPACES . deref ( ) . entry ( name , None ) 
~~ def remove ( cls , name : sym . Symbol ) -> Optional [ "Namespace" ] : 
~~~ oldval : lmap . Map = cls . _NAMESPACES . deref ( ) 
ns : Optional [ Namespace ] = oldval . entry ( name , None ) 
newval = oldval 
~~~ newval = oldval . dissoc ( name ) 
~~ if cls . _NAMESPACES . compare_and_set ( oldval , newval ) : 
~~~ return ns 
~~ ~~ ~~ def __completion_matcher ( text : str ) -> CompletionMatcher : 
def is_match ( entry : Tuple [ sym . Symbol , Any ] ) -> bool : 
~~~ return entry [ 0 ] . name . startswith ( text ) 
~~ return is_match 
~~ def __complete_alias ( 
self , prefix : str , name_in_ns : Optional [ str ] = None 
candidates = filter ( 
Namespace . __completion_matcher ( prefix ) , [ ( s , n ) for s , n in self . aliases ] 
if name_in_ns is not None : 
~~~ for _ , candidate_ns in candidates : 
~~~ for match in candidate_ns . __complete_interns ( 
name_in_ns , include_private_vars = False 
~~~ yield f"{prefix}/{match}" 
~~~ for alias , _ in candidates : 
~~~ yield f"{alias}/" 
~~ ~~ ~~ def __complete_imports_and_aliases ( 
self , prefix : str , name_in_module : Optional [ str ] = None 
imports = self . imports 
aliases = lmap . map ( 
alias : imports . entry ( import_name ) 
for alias , import_name in self . import_aliases 
Namespace . __completion_matcher ( prefix ) , itertools . chain ( aliases , imports ) 
if name_in_module is not None : 
~~~ for _ , module in candidates : 
~~~ for name in module . __dict__ : 
~~~ if name . startswith ( name_in_module ) : 
~~~ yield f"{prefix}/{name}" 
~~~ for candidate_name , _ in candidates : 
~~~ yield f"{candidate_name}/" 
~~ ~~ ~~ def __complete_interns ( 
self , value : str , include_private_vars : bool = True 
if include_private_vars : 
~~~ is_match = Namespace . __completion_matcher ( value ) 
~~~ _is_match = Namespace . __completion_matcher ( value ) 
def is_match ( entry : Tuple [ sym . Symbol , Var ] ) -> bool : 
~~~ return _is_match ( entry ) and not entry [ 1 ] . is_private 
~~ ~~ return map ( 
lambda entry : f"{entry[0].name}" , 
filter ( is_match , [ ( s , v ) for s , v in self . interns ] ) , 
~~ def __complete_refers ( self , value : str ) -> Iterable [ str ] : 
return map ( 
Namespace . __completion_matcher ( value ) , [ ( s , v ) for s , v in self . refers ] 
~~ def complete ( self , text : str ) -> Iterable [ str ] : 
assert not text . startswith ( ":" ) 
results = itertools . chain ( 
self . __complete_alias ( prefix , name_in_ns = suffix ) , 
self . __complete_imports_and_aliases ( prefix , name_in_module = suffix ) , 
~~~ results = itertools . chain ( 
self . __complete_alias ( text ) , 
self . __complete_imports_and_aliases ( text ) , 
self . __complete_interns ( text ) , 
self . __complete_refers ( text ) , 
~~ return results 
~~ def args ( self ) -> Tuple : 
if not self . _has_varargs : 
~~~ return self . _args 
~~~ final = self . _args [ - 1 ] 
if isinstance ( final , ISeq ) : 
~~~ inits = self . _args [ : - 1 ] 
return tuple ( itertools . chain ( inits , final ) ) 
~~ return self . _args 
~~~ return ( ) 
plist ( iterable = members ) , meta = meta 
~~ def l ( * members , meta = None ) -> List : 
~~ def change_style ( style , representer ) : 
def new_representer ( dumper , data ) : 
~~~ scalar = representer ( dumper , data ) 
scalar . style = style 
return scalar 
~~ return new_representer 
~~ def get_public_key ( platform , service , purpose , key_use , version , public_key , keys_folder ) : 
public_key_data = get_file_contents ( keys_folder , public_key ) 
pub_key = load_pem_public_key ( public_key_data . encode ( ) , backend = backend ) 
pub_bytes = pub_key . public_bytes ( Encoding . PEM , PublicFormat . SubjectPublicKeyInfo ) 
kid = _generate_kid_from_key ( pub_bytes . decode ( ) ) 
key = _create_key ( platform = platform , service = service , key_use = key_use , 
key_type = "public" , purpose = purpose , version = version , 
public_key = public_key_data ) 
return kid , key 
~~ def get_private_key ( platform , service , purpose , key_use , version , private_key , keys_folder ) : 
private_key_data = get_file_contents ( keys_folder , private_key ) 
private_key = load_pem_private_key ( private_key_data . encode ( ) , None , backend = backend ) 
pub_key = private_key . public_key ( ) 
key_type = "private" , purpose = purpose , version = version , 
public_key = pub_bytes . decode ( ) , private_key = private_key_data ) 
~~ def decrypt_with_key ( encrypted_token , key ) : 
~~~ jwe_token = jwe . JWE ( algs = [ 'RSA-OAEP' , 'A256GCM' ] ) 
jwe_token . deserialize ( encrypted_token ) 
jwe_token . decrypt ( key ) 
return jwe_token . payload . decode ( ) 
~~ except ( ValueError , InvalidJWEData ) as e : 
~~~ raise InvalidTokenException ( str ( e ) ) from e 
~~ ~~ def decrypt ( token , key_store , key_purpose , leeway = 120 ) : 
tokens = token . split ( '.' ) 
if len ( tokens ) != 5 : 
~~ decrypted_token = JWEHelper . decrypt ( token , key_store , key_purpose ) 
payload = JWTHelper . decode ( decrypted_token , key_store , key_purpose , leeway ) 
return payload 
~~ def encrypt ( json , key_store , key_purpose ) : 
jwt_key = key_store . get_key_for_purpose_and_type ( key_purpose , "private" ) 
payload = JWTHelper . encode ( json , jwt_key . kid , key_store , key_purpose ) 
jwe_key = key_store . get_key_for_purpose_and_type ( key_purpose , "public" ) 
return JWEHelper . encrypt ( payload , jwe_key . kid , key_store , key_purpose ) 
~~ def get_key_for_purpose_and_type ( self , purpose , key_type ) : 
key = [ key for key in self . keys . values ( ) if key . purpose == purpose and key . key_type == key_type ] 
~~~ return key [ 0 ] 
~~ ~~ def get_default_args ( func ) : 
args , _ , _ , defaults , * rest = inspect . getfullargspec ( func ) 
return dict ( zip ( reversed ( args ) , reversed ( defaults ) ) ) 
~~ def map_arguments_to_objects ( kwargs , objects , object_key , object_tuple_key , argument_key , result_value , default_result ) : 
map_ = map_objects_to_result ( objects , object_key , object_tuple_key , result_value , default_result ) 
element_count = get_request_count ( kwargs ) 
return [ map_ [ get_argument_key ( kwargs , argument_key , index ) ] for index in range ( 0 , element_count ) ] 
~~ def delete ( self , * args ) : 
cache = get_cache ( ) 
key = self . get_cache_key ( * args ) 
if key in cache : 
~~~ del cache [ key ] 
~~ ~~ def multiget_cached ( object_key , argument_key = None , default_result = None , 
result_fields = None , join_table_name = None , coerce_args_to_strings = False ) : 
def create_wrapper ( inner_f ) : 
~~~ return MultigetCacheWrapper ( 
inner_f , object_key , argument_key , default_result , result_fields , 
join_table_name , coerce_args_to_strings = coerce_args_to_strings 
~~ return create_wrapper 
~~ def get_dot_target_name ( version = None , module = None ) : 
version = version or get_current_version_name ( ) 
module = module or get_current_module_name ( ) 
return '-dot-' . join ( ( version , module ) ) 
~~ def get_dot_target_name_safe ( version = None , module = None ) : 
version = version or get_current_version_name_safe ( ) 
module = module or get_current_module_name_safe ( ) 
if version and module : 
~~~ return '-dot-' . join ( ( version , module ) ) 
~~ def _get_os_environ_dict ( keys ) : 
return { k : os . environ . get ( k , _UNDEFINED ) for k in keys } 
~~ def name ( obj ) -> str : 
if not isroutine ( obj ) and not hasattr ( obj , '__name__' ) and hasattr ( obj , '__class__' ) : 
~~~ obj = obj . __class__ 
~~ module = getmodule ( obj ) 
return module . __name__ + ':' + obj . __qualname__ 
~~ def to_python ( self ) : 
self . selector , 
COMPARISON_MAP . get ( self . comparison , self . comparison ) , 
self . argument 
~~ async def connect ( self ) : 
self . reader , self . writer = await asyncio . open_connection ( 
self . host , self . port , loop = self . loop ) 
self . welcome_msg = await self . reader . read ( self . buffer_size ) 
~~ async def send ( self , commands ) : 
msg = self . _prepare_send ( commands ) 
self . writer . write ( msg ) 
await self . writer . drain ( ) 
~~ async def receive ( self ) : 
~~~ incomming = await self . reader . read ( self . buffer_size ) 
~~ except OSError : 
~~ return _parse_receive ( incomming ) 
~~ async def wait_for ( self , cmd , value = None , timeout = 60 ) : 
~~~ async with async_timeout ( timeout * 60 ) : 
~~~ msgs = await self . receive ( ) 
msg = check_messages ( msgs , cmd , value = value ) 
if msg : 
~~~ return msg 
~~ ~~ ~~ ~~ except asyncio . TimeoutError : 
~~~ return OrderedDict ( ) 
~~ ~~ def close ( self ) : 
if self . writer . can_write_eof ( ) : 
~~~ self . writer . write_eof ( ) 
~~ self . writer . close ( ) 
~~ def lazyload ( reference : str , * args , ** kw ) : 
assert check_argument_types ( ) 
def lazily_load_reference ( self ) : 
~~~ ref = reference 
if ref . startswith ( '.' ) : 
~~~ ref = traverse ( self , ref [ 1 : ] ) 
~~ return load ( ref , * args , ** kw ) 
~~ return lazy ( lazily_load_reference ) 
~~ def iter_parse ( fiql_str ) : 
while len ( fiql_str ) : 
~~~ constraint_match = CONSTRAINT_COMP . split ( fiql_str , 1 ) 
if len ( constraint_match ) < 2 : 
~~~ yield ( constraint_match [ 0 ] , None , None , None ) 
~~ yield ( 
constraint_match [ 0 ] , 
unquote_plus ( constraint_match [ 1 ] ) , 
constraint_match [ 4 ] , 
unquote_plus ( constraint_match [ 6 ] ) if constraint_match [ 6 ] else None 
fiql_str = constraint_match [ 8 ] 
~~ ~~ def parse_str_to_expression ( fiql_str ) : 
nesting_lvl = 0 
last_element = None 
expression = Expression ( ) 
for ( preamble , selector , comparison , argument ) in iter_parse ( fiql_str ) : 
~~~ if preamble : 
~~~ for char in preamble : 
~~~ if char == '(' : 
~~~ if isinstance ( last_element , BaseExpression ) : 
~~~ raise FiqlFormatException ( 
last_element . __class__ , Expression ) ) 
~~ expression = expression . create_nested_expression ( ) 
nesting_lvl += 1 
~~ elif char == ')' : 
~~~ expression = expression . get_parent ( ) 
last_element = expression 
nesting_lvl -= 1 
~~~ if not expression . has_constraint ( ) : 
Operator , Constraint ) ) 
~~ if isinstance ( last_element , Operator ) : 
Operator , Operator ) ) 
~~ last_element = Operator ( char ) 
expression = expression . add_operator ( last_element ) 
~~ ~~ ~~ if selector : 
last_element . __class__ , Constraint ) ) 
~~ last_element = Constraint ( selector , comparison , argument ) 
expression . add_element ( last_element ) 
~~ ~~ if nesting_lvl != 0 : 
~~ if not expression . has_constraint ( ) : 
~~ return expression 
~~ def encode_model ( obj ) : 
obj_dict = obj . to_dict ( ) 
for key , val in obj_dict . iteritems ( ) : 
~~~ if isinstance ( val , types . StringType ) : 
~~~ unicode ( val ) 
~~ except UnicodeDecodeError : 
~~~ obj_dict [ key ] = base64 . b64encode ( val ) 
~~ ~~ ~~ return obj_dict 
~~ def dump ( ndb_model , fp , ** kwargs ) : 
for chunk in NdbEncoder ( ** kwargs ) . iterencode ( ndb_model ) : 
~~~ fp . write ( chunk ) 
~~ ~~ def object_hook_handler ( self , val ) : 
return { k : self . decode_date ( v ) for k , v in val . iteritems ( ) } 
~~ def decode_date ( self , val ) : 
if isinstance ( val , basestring ) and val . count ( '-' ) == 2 and len ( val ) > 9 : 
~~~ dt = dateutil . parser . parse ( val ) 
if val . endswith ( ( '+00:00' , '-00:00' , 'Z' ) ) : 
~~~ dt = dt . replace ( tzinfo = None ) 
~~ return dt 
~~ ~~ return val 
~~ def decode ( self , val ) : 
new_val = self . decode_date ( val ) 
if val != new_val : 
~~~ return new_val 
~~ return json . JSONDecoder . decode ( self , val ) 
~~ def default ( self , obj ) : 
obj_type = type ( obj ) 
if obj_type not in self . _ndb_type_encoding : 
~~~ if hasattr ( obj , '__metaclass__' ) : 
~~~ obj_type = obj . __metaclass__ 
~~~ for ndb_type in NDB_TYPES : 
~~~ if isinstance ( obj , ndb_type ) : 
~~~ obj_type = ndb_type 
~~ ~~ ~~ ~~ fn = self . _ndb_type_encoding . get ( obj_type ) 
if fn : 
~~~ return fn ( obj ) 
~~ return json . JSONEncoder . default ( self , obj ) 
~~ def traverse ( obj , target : str , default = nodefault , executable : bool = False , separator : str = '.' , protect : bool = True ) : 
value = obj 
remainder = target 
if not target : 
~~ while separator : 
~~~ name , separator , remainder = remainder . partition ( separator ) 
numeric = name . lstrip ( '-' ) . isdigit ( ) 
~~~ if numeric or ( protect and name . startswith ( '_' ) ) : 
~~~ raise AttributeError ( ) 
~~ value = getattr ( value , name ) 
if executable and callable ( value ) : 
~~~ value = value ( ) 
~~~ value = value [ int ( name ) if numeric else name ] 
~~~ if default is nodefault : 
~~ return default 
~~ ~~ ~~ return value 
~~ def load ( target : str , namespace : str = None , default = nodefault , executable : bool = False , separators : Sequence [ str ] = ( '.' , ':' ) , 
protect : bool = True ) : 
if namespace and ':' not in target : 
~~~ allowable = dict ( ( i . name , i ) for i in iter_entry_points ( namespace ) ) 
if target not in allowable : 
~~ return allowable [ target ] . load ( ) 
~~ parts , _ , target = target . partition ( separators [ 1 ] ) 
~~~ obj = __import__ ( parts ) 
~~~ if default is not nodefault : 
~~ return traverse ( 
obj , 
separators [ 0 ] . join ( parts . split ( separators [ 0 ] ) [ 1 : ] + target . split ( separators [ 0 ] ) ) , 
default = default , 
executable = executable , 
protect = protect 
) if target else obj 
~~ def run ( ) : 
cam = CAM ( ) 
print ( cam . welcome_msg ) 
print ( cam . send ( b'/cmd:deletelist' ) ) 
sleep ( 0.1 ) 
print ( cam . receive ( ) ) 
print ( cam . wait_for ( cmd = 'cmd' , timeout = 0.1 ) ) 
cam . close ( ) 
~~ def validate_version ( ) : 
import leicacam 
version_string = leicacam . __version__ 
versions = version_string . split ( '.' , 3 ) 
~~~ for ver in versions : 
~~~ int ( ver ) 
~~ return version_string 
~~ def generate ( ) : 
old_dir = os . getcwd ( ) 
proj_dir = os . path . join ( os . path . dirname ( __file__ ) , os . pardir ) 
os . chdir ( proj_dir ) 
version = validate_version ( ) 
if not version : 
~~~ os . chdir ( old_dir ) 
options = [ 
'--user' , 'arve0' , '--project' , 'leicacam' , '-v' , '--with-unreleased' , 
'--future-release' , version ] 
generator = ChangelogGenerator ( options ) 
generator . run ( ) 
os . chdir ( old_dir ) 
~~ def strongly_connected_components ( graph : Graph ) -> List : 
result = [ ] 
stack = [ ] 
low = { } 
def visit ( node : str ) : 
~~~ if node in low : return 
num = len ( low ) 
low [ node ] = num 
stack_pos = len ( stack ) 
stack . append ( node ) 
for successor in graph [ node ] : 
~~~ visit ( successor ) 
low [ node ] = min ( low [ node ] , low [ successor ] ) 
~~ if num == low [ node ] : 
~~~ component = tuple ( stack [ stack_pos : ] ) 
del stack [ stack_pos : ] 
result . append ( component ) 
for item in component : 
~~~ low [ item ] = len ( graph ) 
~~ ~~ ~~ for node in graph : 
~~~ visit ( node ) 
~~ def robust_topological_sort ( graph : Graph ) -> list : 
components = strongly_connected_components ( graph ) 
node_component = { } 
for component in components : 
~~~ for node in component : 
~~~ node_component [ node ] = component 
~~ ~~ component_graph = { } 
~~~ component_graph [ component ] = [ ] 
~~ for node in graph : 
~~~ node_c = node_component [ node ] 
~~~ successor_c = node_component [ successor ] 
if node_c != successor_c : 
~~~ component_graph [ node_c ] . append ( successor_c ) 
~~ ~~ ~~ return topological_sort ( component_graph ) 
~~ def set_parent ( self , parent ) : 
if not isinstance ( parent , Expression ) : 
Expression , type ( parent ) ) ) 
~~ self . parent = parent 
~~ def get_parent ( self ) : 
if not isinstance ( self . parent , Expression ) : 
Expression , type ( self . parent ) ) ) 
~~ return self . parent 
~~ def add_operator ( self , operator ) : 
if not isinstance ( operator , Operator ) : 
operator . __class__ ) ) 
~~ if not self . _working_fragment . operator : 
~~~ self . _working_fragment . operator = operator 
~~ elif operator > self . _working_fragment . operator : 
~~~ last_constraint = self . _working_fragment . elements . pop ( ) 
self . _working_fragment = self . _working_fragment . create_nested_expression ( ) 
self . _working_fragment . add_element ( last_constraint ) 
self . _working_fragment . add_operator ( operator ) 
~~ elif operator < self . _working_fragment . operator : 
~~~ if self . _working_fragment . parent : 
~~~ return self . _working_fragment . parent . add_operator ( operator ) 
~~~ return Expression ( ) . add_element ( self . _working_fragment ) . add_operator ( operator ) 
~~ ~~ return self 
~~ def add_element ( self , element ) : 
if isinstance ( element , BaseExpression ) : 
~~~ element . set_parent ( self . _working_fragment ) 
self . _working_fragment . elements . append ( element ) 
~~~ return self . add_operator ( element ) 
~~ ~~ def op_and ( self , * elements ) : 
expression = self . add_operator ( Operator ( ';' ) ) 
for element in elements : 
~~~ expression . add_element ( element ) 
~~ def op_or ( self , * elements ) : 
expression = self . add_operator ( Operator ( ',' ) ) 
if len ( self . elements ) == 0 : 
~~ if len ( self . elements ) == 1 : 
~~~ return self . elements [ 0 ] . to_python ( ) 
~~ operator = self . operator or Operator ( ';' ) 
return [ operator . to_python ( ) ] + [ elem . to_python ( ) for elem in self . elements ] 
~~ async def run ( loop ) : 
cam = AsyncCAM ( loop = loop ) 
await cam . connect ( ) 
await cam . send ( b'/cmd:deletelist' ) 
print ( await cam . receive ( ) ) 
print ( await cam . wait_for ( cmd = 'cmd' , timeout = 0.1 ) ) 
print ( await cam . wait_for ( cmd = 'cmd' , timeout = 0 ) ) 
print ( await cam . wait_for ( cmd = 'test' , timeout = 0.1 ) ) 
~~ def logger ( function ) : 
@ functools . wraps ( function ) 
out = sep . join ( [ repr ( x ) for x in args ] ) 
out = out + end 
_LOGGER . debug ( out ) 
return function ( * args , ** kwargs ) 
~~ def _parse_receive ( incomming ) : 
incomming = incomming . rstrip ( b'\\x00' ) 
msgs = incomming . splitlines ( ) 
return [ bytes_as_dict ( msg ) for msg in msgs ] 
~~ def tuples_as_bytes ( cmds ) : 
tmp = [ ] 
for key , val in cmds . items ( ) : 
~~~ key = str ( key ) 
val = str ( val ) 
tmp . append ( '/' + key + ':' + val ) 
~~ def tuples_as_dict ( _list ) : 
_dict = OrderedDict ( ) 
for key , val in _list : 
_dict [ key ] = val 
~~ return _dict 
~~ def bytes_as_dict ( msg ) : 
cmds = OrderedDict ( ) 
for cmd in cmd_strings : 
~~~ unpacked = cmd . split ( ':' ) 
if len ( unpacked ) > 2 : 
~~~ key = unpacked [ 0 ] 
val = ':' . join ( unpacked [ 1 : ] ) 
~~ elif len ( unpacked ) < 2 : 
~~~ key , val = unpacked 
~~ cmds [ key ] = val 
~~ return cmds 
~~ def check_messages ( msgs , cmd , value = None ) : 
for msg in msgs : 
~~~ if value and msg . get ( cmd ) == value : 
~~ if not value and msg . get ( cmd ) : 
~~ ~~ return None 
~~ def _prepare_send ( self , commands ) : 
if isinstance ( commands , bytes ) : 
~~~ msg = self . prefix_bytes + commands 
~~~ msg = tuples_as_bytes ( self . prefix + commands ) 
return msg 
~~ def connect ( self ) : 
self . socket = socket . socket ( ) 
self . socket . connect ( ( self . host , self . port ) ) 
self . welcome_msg = self . socket . recv ( 
self . buffer_size ) 
~~ def flush ( self ) : 
~~~ msg = self . socket . recv ( self . buffer_size ) 
~~ ~~ except socket . error : 
~~ ~~ def send ( self , commands ) : 
return self . socket . send ( msg ) 
~~ def receive ( self ) : 
~~~ incomming = self . socket . recv ( self . buffer_size ) 
~~ except socket . error : 
~~ def wait_for ( self , cmd , value = None , timeout = 60 ) : 
wait = time ( ) + timeout * 60 
~~~ if time ( ) > wait : 
~~ msgs = self . receive ( ) 
~~ sleep ( self . delay ) 
~~ ~~ def enable ( self , slide = 0 , wellx = 1 , welly = 1 , fieldx = 1 , fieldy = 1 ) : 
cmd = [ 
( 'cmd' , 'enable' ) , 
( 'slide' , str ( slide ) ) , 
( 'wellx' , str ( wellx ) ) , 
( 'welly' , str ( welly ) ) , 
( 'fieldx' , str ( fieldx ) ) , 
( 'fieldy' , str ( fieldy ) ) , 
( 'value' , 'true' ) 
self . send ( cmd ) 
return self . wait_for ( * cmd [ 0 ] ) 
~~ def save_template ( self , filename = "{ScanningTemplate}leicacam.xml" ) : 
( 'sys' , '0' ) , 
( 'cmd' , 'save' ) , 
( 'fil' , str ( filename ) ) 
~~ def load_template ( self , filename = "{ScanningTemplate}leicacam.xml" ) : 
basename = os . path . basename ( filename ) 
if basename [ - 4 : ] == '.xml' : 
~~~ basename = basename [ : - 4 ] 
~~ if basename [ : 18 ] != '{ScanningTemplate}' : 
~~~ basename = '{ScanningTemplate}' + basename 
~~ cmd = [ 
( 'cmd' , 'load' ) , 
( 'fil' , str ( basename ) ) 
return self . wait_for ( * cmd [ 1 ] ) 
~~ def get_information ( self , about = 'stage' ) : 
( 'cmd' , 'getinfo' ) , 
( 'dev' , str ( about ) ) 
~~ def locate_package_json ( ) : 
directory = settings . SYSTEMJS_PACKAGE_JSON_DIR 
if not directory : 
~~~ raise ImproperlyConfigured ( 
~~ path = os . path . join ( directory , 'package.json' ) 
if not os . path . isfile ( path ) : 
~~~ raise ImproperlyConfigured ( "\ % path ) 
~~ return path 
~~ def parse_package_json ( ) : 
with open ( locate_package_json ( ) ) as pjson : 
~~~ data = json . loads ( pjson . read ( ) ) 
~~ def find_systemjs_location ( ) : 
location = os . path . abspath ( os . path . dirname ( locate_package_json ( ) ) ) 
conf = parse_package_json ( ) 
if 'jspm' in conf : 
~~~ conf = conf [ 'jspm' ] 
~~~ conf = conf [ 'directories' ] 
~~ jspm_packages = conf [ 'packages' ] if 'packages' in conf else 'jspm_packages' 
base = conf [ 'baseURL' ] if 'baseURL' in conf else '.' 
return os . path . join ( location , base , jspm_packages , 'system.js' ) 
~~ def load_systemjs_manifest ( self ) : 
_manifest_name = self . manifest_name 
self . manifest_name = self . systemjs_manifest_name 
bundle_files = self . load_manifest ( ) 
self . manifest_name = _manifest_name 
for file , hashed_file in bundle_files . copy ( ) . items ( ) : 
~~~ if not self . exists ( file ) or not self . exists ( hashed_file ) : 
~~~ del bundle_files [ file ] 
~~ ~~ return bundle_files 
~~ def log ( self , msg , level = 2 ) : 
if self . verbosity >= level : 
~~~ self . stdout . write ( msg ) 
~~ ~~ def cached ( method ) -> property : 
name = "_" + method . __name__ 
@ property 
def wrapper ( self ) : 
~~~ return getattr ( self , name ) 
~~~ val = method ( self ) 
setattr ( self , name , val ) 
return val 
~~ def chunkiter ( iterable , chunksize ) : 
iterator = iter ( iterable ) 
for chunk in iter ( lambda : list ( itertools . islice ( iterator , chunksize ) ) , [ ] ) : 
~~~ yield chunk 
~~ ~~ def chunkprocess ( func ) : 
def wrapper ( iterable , chunksize , * args , ** kwargs ) : 
~~~ for chunk in chunkiter ( iterable , chunksize ) : 
~~~ yield func ( chunk , * args , ** kwargs ) 
~~ def flatten ( iterable , map2iter = None ) : 
if map2iter and isinstance ( iterable ) : 
~~~ iterable = map2iter ( iterable ) 
~~ for item in iterable : 
~~~ if isinstance ( item , str ) or not isinstance ( item , abc . Iterable ) : 
~~~ yield from flatten ( item , map2iter ) 
~~ ~~ ~~ def deepupdate ( 
mapping : abc . MutableMapping , other : abc . Mapping , listextend = False 
def inner ( other , previouskeys ) : 
for key , value in other . items ( ) : 
~~~ if isinstance ( value , abc . Mapping ) : 
~~~ inner ( value , ( * previouskeys , key ) ) 
~~~ node = mapping 
for previouskey in previouskeys : 
~~~ node = node . setdefault ( previouskey , { } ) 
~~ target = node . get ( key ) 
listextend 
and isinstance ( target , abc . MutableSequence ) 
and isinstance ( value , abc . Sequence ) 
~~~ target . extend ( value ) 
~~~ node [ key ] = value 
~~ ~~ ~~ ~~ inner ( other , ( ) ) 
~~ def quietinterrupt ( msg = None ) : 
def handler ( ) : 
~~~ if msg : 
~~~ print ( msg , file = sys . stderr ) 
~~ sys . exit ( 1 ) 
~~ signal . signal ( signal . SIGINT , handler ) 
~~ def printtsv ( table , sep = "\\t" , file = sys . stdout ) : 
for record in table : 
~~~ print ( * record , sep = sep , file = file ) 
~~ ~~ def mkdummy ( name , ** attrs ) : 
return type ( 
name , ( ) , dict ( __repr__ = ( lambda self : "<%s>" % name ) , ** attrs ) 
) ( ) 
~~ def pipe ( value , * functions , funcs = None ) : 
if funcs : 
~~~ functions = funcs 
~~ for function in functions : 
~~~ value = function ( value ) 
~~ def pipeline ( * functions , funcs = None ) : 
~~ head , * tail = functions 
return lambda * args , ** kwargs : pipe ( head ( * args , ** kwargs ) , funcs = tail ) 
~~ def human_readable ( self , decimal = False ) : 
divisor = 1000 if decimal else 1024 
number = int ( self ) 
unit = "" 
for unit in self . units : 
~~~ if number < divisor : 
~~ number /= divisor 
~~ return number , unit . upper ( ) 
~~ def from_str ( cls , human_readable_str , decimal = False , bits = False ) : 
num = [ ] 
c = "" 
for c in human_readable_str : 
~~~ if c not in cls . digits : 
~~ num . append ( c ) 
~~ num = "" . join ( num ) 
~~~ num = int ( num ) 
~~~ num = float ( num ) 
~~ if bits : 
~~~ num /= 8 
~~ return cls ( round ( num * divisor ** cls . key [ c . lower ( ) ] ) ) 
~~ def find ( self , path , all = False ) : 
bits = path . split ( '/' ) 
dirs_to_serve = [ 'jspm_packages' , settings . SYSTEMJS_OUTPUT_DIR ] 
if not bits or bits [ 0 ] not in dirs_to_serve : 
~~ return super ( SystemFinder , self ) . find ( path , all = all ) 
~~ def find_apps ( self , templates = None ) : 
all_apps = OrderedDict ( ) 
if not templates : 
~~~ all_files = self . discover_templates ( ) 
for tpl_name , fp in all_files : 
~~~ with io . open ( fp , 'r' , encoding = settings . FILE_CHARSET ) as template_file : 
~~~ src_data = template_file . read ( ) 
~~ for t in Lexer ( src_data ) . tokenize ( ) : 
~~~ if t . token_type == TOKEN_BLOCK : 
~~~ imatch = SYSTEMJS_TAG_RE . match ( t . contents ) 
if imatch : 
~~~ all_apps . setdefault ( tpl_name , [ ] ) 
all_apps [ tpl_name ] . append ( imatch . group ( 'app' ) ) 
~~ ~~ ~~ ~~ ~~ else : 
~~~ for tpl_name in templates : 
~~~ template = loader . get_template ( tpl_name ) 
~~ except TemplateDoesNotExist : 
~~ import_nodes = template . template . nodelist . get_nodes_by_type ( SystemImportNode ) 
for node in import_nodes : 
~~~ app = node . path . resolve ( RESOLVE_CONTEXT ) 
if not app : 
~~~ self . stdout . write ( self . style . WARNING ( 
tpl = tpl_name , ctx = RESOLVE_CONTEXT ) 
~~ all_apps . setdefault ( tpl_name , [ ] ) 
all_apps [ tpl_name ] . append ( app ) 
~~ ~~ ~~ return all_apps 
~~ def render ( self , context ) : 
module_path = self . path . resolve ( context ) 
if not settings . SYSTEMJS_ENABLED : 
~~~ if settings . SYSTEMJS_DEFAULT_JS_EXTENSIONS : 
~~~ name , ext = posixpath . splitext ( module_path ) 
if not ext : 
~~~ module_path = '{}.js' . format ( module_path ) 
~~ ~~ if settings . SYSTEMJS_SERVER_URL : 
~~ return tpl . format ( app = module_path , url = settings . SYSTEMJS_SERVER_URL ) 
~~ rel_path = System . get_bundle_path ( module_path ) 
url = staticfiles_storage . url ( rel_path ) 
tag_attrs = { 'type' : 'text/javascript' } 
for key , value in self . tag_attrs . items ( ) : 
~~~ if not isinstance ( value , bool ) : 
~~~ value = value . resolve ( context ) 
~~ tag_attrs [ key ] = value 
url = url , attrs = flatatt ( tag_attrs ) 
~~ def find_sourcemap_comment ( filepath , block_size = 100 ) : 
block_number = - 1 
sourcemap = None 
~~~ of = io . open ( filepath , 'br+' ) 
of . seek ( 0 , os . SEEK_END ) 
block_end_byte = of . tell ( ) 
while block_end_byte > 0 and MAX_TRACKBACK > 0 : 
~~~ if ( block_end_byte - block_size > 0 ) : 
~~~ of . seek ( block_number * block_size , os . SEEK_END ) 
blocks . append ( of . read ( block_size ) ) 
~~~ of . seek ( 0 , os . SEEK_SET ) 
blocks = [ of . read ( block_end_byte ) ] 
~~ content = b'' . join ( reversed ( blocks ) ) 
lines_found = content . count ( b'\\n' ) 
MAX_TRACKBACK -= lines_found 
block_end_byte -= block_size 
block_number -= 1 
if SOURCEMAPPING_URL_COMMENT in content : 
~~~ offset = 0 
lines = content . split ( b'\\n' ) 
~~~ if line . startswith ( SOURCEMAPPING_URL_COMMENT ) : 
~~~ offset = len ( line ) 
sourcemap = line 
~~ ~~ while i + 1 < len ( lines ) : 
offset += len ( lines [ i + 1 ] ) 
~~ if sourcemap : 
of . seek ( - offset , os . SEEK_END ) 
of . truncate ( ) 
~~ return force_text ( sourcemap ) 
~~~ of . close ( ) 
~~ return sourcemap 
~~ def get_paths ( self ) : 
outfile = self . get_outfile ( ) 
rel_path = os . path . relpath ( outfile , settings . STATIC_ROOT ) 
return outfile , rel_path 
~~ def needs_ext ( self ) : 
if settings . SYSTEMJS_DEFAULT_JS_EXTENSIONS : 
~~~ name , ext = posixpath . splitext ( self . app ) 
~~ def bundle ( self ) : 
outfile , rel_path = self . get_paths ( ) 
options = self . opts 
if self . system . _has_jspm_log ( ) : 
options . setdefault ( 'log' , 'err' ) 
~~ if options . get ( 'minify' ) : 
~~ if options . get ( 'skip_source_maps' ) : 
~~~ cmd = self . command . format ( app = self . app , outfile = outfile , ** options ) 
proc = subprocess . Popen ( 
cmd , shell = True , cwd = self . system . cwd , stdout = self . stdout , 
stdin = self . stdin , stderr = self . stderr ) 
if err and self . system . _has_jspm_log ( ) : 
logger . warn ( fmt , self . app , err ) 
raise BundleError ( fmt % ( self . app , err ) ) 
~~ if result . strip ( ) : 
~~~ logger . info ( result ) 
~~ ~~ except ( IOError , OSError ) as e : 
~~~ if isinstance ( e , BundleError ) : 
self . __class__ . __name__ , cmd , e ) ) 
~~~ if not options . get ( 'sfx' ) : 
~~~ sourcemap = find_sourcemap_comment ( outfile ) 
with open ( outfile , 'a' ) as of : 
~~~ of . write ( "\\nSystem.import(\ . format ( 
app = self . app , 
ext = '.js' if self . needs_ext ( ) else '' , 
sourcemap = sourcemap if sourcemap else '' , 
~~ ~~ ~~ return rel_path 
~~ def trace ( self , app ) : 
if app not in self . _trace_cache : 
~~~ process = subprocess . Popen ( 
stdout = subprocess . PIPE , stderr = subprocess . PIPE , 
env = self . env , universal_newlines = True , cwd = self . _package_json_dir 
out , err = process . communicate ( ) 
~~~ raise TraceError ( err ) 
~~ self . _trace_cache [ app ] = json . loads ( out ) 
~~ return self . _trace_cache [ app ] 
~~ def hashes_match ( self , dep_tree ) : 
hashes = self . get_hashes ( ) 
for module , info in dep_tree . items ( ) : 
~~~ md5 = self . get_hash ( info [ 'path' ] ) 
if md5 != hashes [ info [ 'path' ] ] : 
~~ def process_item ( self , item , spider ) : 
self . items . append ( item ) 
if len ( self . items ) >= self . max_chunk_size : 
~~~ self . _upload_chunk ( spider ) 
~~ return item 
~~ def open_spider ( self , spider ) : 
self . ts = datetime . utcnow ( ) . replace ( microsecond = 0 ) . isoformat ( ) . replace ( ':' , '-' ) 
~~ def _upload_chunk ( self , spider ) : 
if not self . items : 
~~ f = self . _make_fileobj ( ) 
object_key = self . object_key_template . format ( ** self . _get_uri_params ( spider ) ) 
~~~ self . s3 . upload_fileobj ( f , self . bucket_name , object_key ) 
~~ except ClientError : 
~~~ self . stats . inc_value ( 'pipeline/s3/fail' ) 
~~~ self . stats . inc_value ( 'pipeline/s3/success' ) 
~~~ self . chunk_number += len ( self . items ) 
self . items = [ ] 
~~ ~~ def _make_fileobj ( self ) : 
bio = BytesIO ( ) 
f = gzip . GzipFile ( mode = 'wb' , fileobj = bio ) if self . use_gzip else bio 
exporter = JsonLinesItemExporter ( f ) 
exporter . start_exporting ( ) 
for item in self . items : 
~~~ exporter . export_item ( item ) 
~~ exporter . finish_exporting ( ) 
if f is not bio : 
~~ bio . seek ( 0 ) 
return bio 
~~ def get_account_state ( self , address , ** kwargs ) : 
return self . _call ( JSONRPCMethods . GET_ACCOUNT_STATE . value , params = [ address , ] , ** kwargs ) 
~~ def get_asset_state ( self , asset_id , ** kwargs ) : 
return self . _call ( JSONRPCMethods . GET_ASSET_STATE . value , params = [ asset_id , ] , ** kwargs ) 
~~ def get_block ( self , block_hash , verbose = True , ** kwargs ) : 
return self . _call ( 
JSONRPCMethods . GET_BLOCK . value , params = [ block_hash , int ( verbose ) , ] , ** kwargs ) 
~~ def get_block_hash ( self , block_index , ** kwargs ) : 
return self . _call ( JSONRPCMethods . GET_BLOCK_HASH . value , [ block_index , ] , ** kwargs ) 
~~ def get_block_sys_fee ( self , block_index , ** kwargs ) : 
return self . _call ( JSONRPCMethods . GET_BLOCK_SYS_FEE . value , [ block_index , ] , ** kwargs ) 
~~ def get_contract_state ( self , script_hash , ** kwargs ) : 
return self . _call ( JSONRPCMethods . GET_CONTRACT_STATE . value , [ script_hash , ] , ** kwargs ) 
~~ def get_raw_transaction ( self , tx_hash , verbose = True , ** kwargs ) : 
JSONRPCMethods . GET_RAW_TRANSACTION . value , params = [ tx_hash , int ( verbose ) , ] , ** kwargs ) 
~~ def get_storage ( self , script_hash , key , ** kwargs ) : 
hexkey = binascii . hexlify ( key . encode ( 'utf-8' ) ) . decode ( 'utf-8' ) 
hexresult = self . _call ( 
JSONRPCMethods . GET_STORAGE . value , params = [ script_hash , hexkey , ] , ** kwargs ) 
~~~ assert hexresult 
result = bytearray ( binascii . unhexlify ( hexresult . encode ( 'utf-8' ) ) ) 
~~~ result = hexresult 
~~ def get_tx_out ( self , tx_hash , index , ** kwargs ) : 
return self . _call ( JSONRPCMethods . GET_TX_OUT . value , params = [ tx_hash , index , ] , ** kwargs ) 
~~ def invoke ( self , script_hash , params , ** kwargs ) : 
contract_params = encode_invocation_params ( params ) 
raw_result = self . _call ( 
JSONRPCMethods . INVOKE . value , [ script_hash , contract_params , ] , ** kwargs ) 
return decode_invocation_result ( raw_result ) 
~~ def invoke_function ( self , script_hash , operation , params , ** kwargs ) : 
JSONRPCMethods . INVOKE_FUNCTION . value , [ script_hash , operation , contract_params , ] , 
~~ def invoke_script ( self , script , ** kwargs ) : 
raw_result = self . _call ( JSONRPCMethods . INVOKE_SCRIPT . value , [ script , ] , ** kwargs ) 
~~ def send_raw_transaction ( self , hextx , ** kwargs ) : 
return self . _call ( JSONRPCMethods . SEND_RAW_TRANSACTION . value , [ hextx , ] , ** kwargs ) 
~~ def validate_address ( self , addr , ** kwargs ) : 
return self . _call ( JSONRPCMethods . VALIDATE_ADDRESS . value , [ addr , ] , ** kwargs ) 
~~ def _call ( self , method , params = None , request_id = None ) : 
params = params or [ ] 
rid = request_id or self . _id_counter 
if request_id is None : 
~~~ self . _id_counter += 1 
~~ payload = { 'jsonrpc' : '2.0' , 'method' : method , 'params' : params , 'id' : rid } 
headers = { 'Content-Type' : 'application/json' } 
scheme = 'https' if self . tls else 'http' 
url = '{}://{}:{}' . format ( scheme , self . host , self . port ) 
~~~ response = self . session . post ( url , headers = headers , data = json . dumps ( payload ) ) 
response . raise_for_status ( ) 
~~ except HTTPError : 
~~~ raise TransportError ( 
response . status_code ) , 
response = response ) 
~~~ response_data = response . json ( ) 
~~~ raise ProtocolError ( 
~~ if response_data . get ( 'error' ) : 
~~~ code = response_data [ 'error' ] . get ( 'code' , '' ) 
message = response_data [ 'error' ] . get ( 'message' , '' ) 
raise ProtocolError ( 
~~ elif 'result' not in response_data : 
data = response_data ) 
~~ return response_data [ 'result' ] 
~~ def is_hash256 ( s ) : 
if not s or not isinstance ( s , str ) : 
~~ return re . match ( '^[0-9A-F]{64}$' , s . strip ( ) , re . IGNORECASE ) 
~~ def is_hash160 ( s ) : 
~~ if not len ( s ) == 40 : 
~~ for c in s : 
~~~ if ( c < '0' or c > '9' ) and ( c < 'A' or c > 'F' ) and ( c < 'a' or c > 'f' ) : 
~~ def encode_invocation_params ( params ) : 
final_params = [ ] 
for p in params : 
~~~ if isinstance ( p , bool ) : 
~~~ final_params . append ( { 'type' : ContractParameterTypes . BOOLEAN . value , 'value' : p } ) 
~~ elif isinstance ( p , int ) : 
~~~ final_params . append ( { 'type' : ContractParameterTypes . INTEGER . value , 'value' : p } ) 
~~ elif is_hash256 ( p ) : 
~~~ final_params . append ( { 'type' : ContractParameterTypes . HASH256 . value , 'value' : p } ) 
~~ elif is_hash160 ( p ) : 
~~~ final_params . append ( { 'type' : ContractParameterTypes . HASH160 . value , 'value' : p } ) 
~~ elif isinstance ( p , bytearray ) : 
~~~ final_params . append ( { 'type' : ContractParameterTypes . BYTE_ARRAY . value , 'value' : p } ) 
~~ elif isinstance ( p , str ) : 
~~~ final_params . append ( { 'type' : ContractParameterTypes . STRING . value , 'value' : p } ) 
~~ elif isinstance ( p , list ) : 
~~~ innerp = encode_invocation_params ( p ) 
final_params . append ( { 'type' : ContractParameterTypes . ARRAY . value , 'value' : innerp } ) 
~~ ~~ return final_params 
~~ def decode_invocation_result ( result ) : 
if 'stack' not in result : 
~~ result = copy . deepcopy ( result ) 
result [ 'stack' ] = _decode_invocation_result_stack ( result [ 'stack' ] ) 
~~ def to_df ( self , ** kwargs ) : 
return pd . read_sql ( sql = self . statement , con = self . session . bind , ** kwargs ) 
~~ def main ( ctx , edit , create ) : 
~~~ click_log . basic_config ( 's3conf' ) 
if edit : 
~~~ if ctx . invoked_subcommand is None : 
config . ConfigFileResolver ( config . LOCAL_CONFIG_FILE ) . edit ( create = create ) 
~~ ~~ if ctx . invoked_subcommand is None : 
~~~ click . echo ( main . get_help ( ctx ) ) 
~~ ~~ except exceptions . FileDoesNotExist as e : 
~~~ raise UsageError ( \ . format ( str ( e ) ) ) 
~~ ~~ def env ( section , map_files , phusion , phusion_path , quiet , edit , create ) : 
settings = config . Settings ( section = section ) 
storage = STORAGES [ 's3' ] ( settings = settings ) 
conf = s3conf . S3Conf ( storage = storage , settings = settings ) 
~~~ conf . edit ( create = create ) 
~~~ env_vars = conf . get_envfile ( ) . as_dict ( ) 
if env_vars . get ( 'S3CONF_MAP' ) and map_files : 
~~~ conf . download_mapping ( env_vars . get ( 'S3CONF_MAP' ) ) 
~~ if not quiet : 
~~~ for var_name , var_value in sorted ( env_vars . items ( ) , key = lambda x : x [ 0 ] ) : 
~~~ click . echo ( '{}={}' . format ( var_name , var_value ) ) 
~~ ~~ if phusion : 
~~~ s3conf . phusion_dump ( env_vars , phusion_path ) 
~~ ~~ ~~ except exceptions . EnvfilePathNotDefinedError : 
~~~ raise exceptions . EnvfilePathNotDefinedUsageError ( ) 
~~ except exceptions . FileDoesNotExist as e : 
~~ ~~ def exec_command ( ctx , section , command , map_files ) : 
existing_sections = config . ConfigFileResolver ( config . LOCAL_CONFIG_FILE ) . sections ( ) 
if section not in existing_sections : 
section = None 
~~ if not command : 
click . echo ( exec_command . get_help ( ctx ) ) 
~~ settings = config . Settings ( section = section ) 
env_vars = conf . get_envfile ( ) . as_dict ( ) 
~~ current_env = os . environ . copy ( ) 
current_env . update ( env_vars ) 
logger . debug ( \ , command ) 
subprocess . run ( shlex . split ( command ) , env = current_env , check = True ) 
~~ except exceptions . EnvfilePathNotDefinedError : 
~~ ~~ def download ( remote_path , local_path ) : 
storage = STORAGES [ 's3' ] ( ) 
conf = s3conf . S3Conf ( storage = storage ) 
conf . download ( remote_path , local_path ) 
~~ def upload ( remote_path , local_path ) : 
conf . upload ( local_path , remote_path ) 
~~ def downsync ( section , map_files ) : 
~~~ settings = config . Settings ( section = section ) 
local_root = os . path . join ( config . LOCAL_CONFIG_FOLDER , section ) 
conf . downsync ( local_root , map_files = map_files ) 
~~ ~~ def diff ( section ) : 
click . echo ( '' . join ( conf . diff ( local_root ) ) ) 
~~ ~~ def set_variable ( section , value , create ) : 
~~~ value = section 
conf = s3conf . S3Conf ( settings = settings ) 
env_vars = conf . get_envfile ( ) 
env_vars . set ( value , create = create ) 
~~ ~~ def unset_variable ( section , value ) : 
env_vars . unset ( value ) 
~~ ~~ def init ( section , remote_file ) : 
if not remote_file . startswith ( 's3://' ) : 
config_file_path = os . path . join ( os . getcwd ( ) , '.s3conf' , 'config' ) 
config_file = config . ConfigFileResolver ( config_file_path , section = section ) 
config_file . set ( 'S3CONF' , remote_file ) 
gitignore_file_path = os . path . join ( os . getcwd ( ) , '.s3conf' , '.gitignore' ) 
config_file . save ( ) 
open ( gitignore_file_path , 'w' ) . write ( '*\\n!config\\n' ) 
~~ def update ( self , t_obj ) : 
if isinstance ( t_obj , Iterable ) : 
~~~ self . _session . add_all ( t_obj ) 
~~~ self . _session . add ( t_obj ) 
~~ ~~ def insert ( self , table , insert_obj , ignore = True ) : 
if isinstance ( insert_obj , pd . DataFrame ) : 
~~~ if insert_obj . empty : 
~~ insert_obj = insert_obj . to_dict ( orient = 'records' ) 
~~ elif not isinstance ( insert_obj , list ) : 
~~ ignore_str = 'IGNORE' if ignore else '' 
return self . _session . execute ( 
table . __table__ . insert ( ) . prefix_with ( ignore_str ) , insert_obj ) 
~~ def parse_env_var ( value ) : 
k , _ , v = value . partition ( '=' ) 
k , v = k . strip ( ) , v . strip ( ) . encode ( 'unicode-escape' ) . decode ( 'ascii' ) 
if v and v [ 0 ] == v [ - 1 ] in [ \ , "\ ] : 
~~~ v = __escape_decoder ( v [ 1 : - 1 ] ) [ 0 ] 
~~ return k , v 
~~ def basic ( username , password ) : 
none ( ) 
_config . username = username 
_config . password = password 
~~ def api_key ( api_key ) : 
_config . api_key_prefix [ "Authorization" ] = "api-key" 
_config . api_key [ "Authorization" ] = "key=" + b64encode ( api_key . encode ( ) ) . decode ( ) 
~~ def _get_json_content_from_folder ( folder ) : 
for dirpath , dirnames , filenames in os . walk ( folder ) : 
~~~ if filename . lower ( ) . endswith ( ".json" ) : 
~~~ filepath = os . path . join ( dirpath , filename ) 
with open ( filepath , "rb" ) as file : 
~~~ yield json . loads ( file . read ( ) . decode ( "UTF-8" ) ) 
~~ ~~ ~~ ~~ ~~ def get_schemas ( ) : 
schemas = { } 
for name in os . listdir ( JSON_PATH ) : 
~~~ if name not in NO_SCHEMA : 
~~~ schemas [ name ] = Schema ( name ) 
~~ ~~ return schemas 
~~ def get_schema ( self ) : 
path = os . path . join ( self . _get_schema_folder ( ) , self . _name + ".json" ) 
with open ( path , "rb" ) as file : 
~~~ schema = json . loads ( file . read ( ) . decode ( "UTF-8" ) ) 
~~ return schema 
~~ def get_resolver ( self ) : 
store = { } 
for schema in get_schemas ( ) . values ( ) : 
~~~ store [ schema . get_uri ( ) ] = schema . get_schema ( ) 
~~ schema = self . get_schema ( ) 
return jsonschema . RefResolver . from_schema ( schema , store = store ) 
~~ def validate ( self , object ) : 
resolver = self . get_resolver ( ) 
jsonschema . validate ( object , self . get_schema ( ) , resolver = resolver ) 
~~ def get_valid_examples ( self ) : 
path = os . path . join ( self . _get_schema_folder ( ) , "examples" , "valid" ) 
return list ( _get_json_content_from_folder ( path ) ) 
~~ def get_invalid_examples ( self ) : 
path = os . path . join ( self . _get_schema_folder ( ) , "examples" , "invalid" ) 
~~ def request ( self , url , method = 'get' , data = None , files = None , 
raw = False , raw_all = False , headers = dict ( ) , raise_for = dict ( ) , session = None ) : 
except ImportError as exc : 
~~~ exc . args = ( \ 
raise exc 
~~ if not self . _requests_setup_done : 
~~~ patched_session = self . _requests_setup ( 
requests , ** ( self . request_adapter_settings or dict ( ) ) ) 
if patched_session is not None : self . _requests_session = patched_session 
~~ if session is None : 
~~~ session = getattr ( self , '_requests_session' , None ) 
if not session : session = self . _requests_session = requests . session ( ) 
~~ elif not session : session = requests 
method = method . lower ( ) 
kwz = ( self . _requests_base_keywords or dict ( ) ) . copy ( ) 
kwz . update ( self . request_extra_keywords or dict ( ) ) 
kwz , func = dict ( ) , ft . partial ( session . request , method . upper ( ) , ** kwz ) 
kwz_headers = ( self . request_base_headers or dict ( ) ) . copy ( ) 
kwz_headers . update ( headers ) 
~~~ if method in [ 'post' , 'put' ] : 
~~~ if all ( hasattr ( data , k ) for k in [ 'seek' , 'read' ] ) : 
~~~ data . seek ( 0 ) 
kwz [ 'data' ] = iter ( ft . partial ( data . read , 200 * 2 ** 10 ) , b'' ) 
~~ else : kwz [ 'data' ] = data 
~~~ kwz [ 'data' ] = json . dumps ( data ) 
kwz_headers . setdefault ( 'Content-Type' , 'application/json' ) 
~~ ~~ if files is not None : 
~~~ for k , file_tuple in files . iteritems ( ) : 
~~~ if len ( file_tuple ) == 2 : files [ k ] = tuple ( file_tuple ) + ( 'application/octet-stream' , ) 
file_tuple [ 1 ] . seek ( 0 ) 
~~ kwz [ 'files' ] = files 
~~ if kwz_headers : kwz [ 'headers' ] = kwz_headers 
code = res = None 
~~~ res = func ( url , ** kwz ) 
code = res . status_code 
if code == requests . codes . no_content : return 
if code != requests . codes . ok : res . raise_for_status ( ) 
~~ except requests . RequestException as err : 
~~~ message = res . text 
try : message = json . loads ( message ) 
else : 
~~~ msg_err , msg_data = message . pop ( 'error' , None ) , message 
if msg_err : 
~~ ~~ ~~ raise raise_for . get ( code , ProtocolError ) ( code , message ) 
~~ if raw : res = res . content 
elif raw_all : res = code , dict ( res . headers . items ( ) ) , res . content 
else : res = json . loads ( res . text ) 
~~ def auth_user_get_url ( self , scope = None ) : 
return '{}?{}' . format ( self . auth_url_user , urllib . urlencode ( dict ( 
response_type = 'code' , redirect_uri = self . auth_redirect_uri ) ) ) 
~~ def auth_user_process_url ( self , url ) : 
url = urlparse . urlparse ( url ) 
url_qs = dict ( it . chain . from_iterable ( 
urlparse . parse_qsl ( v ) for v in [ url . query , url . fragment ] ) ) 
if url_qs . get ( 'error' ) : 
~~~ raise APIAuthError ( 
~~ self . auth_code = url_qs [ 'code' ] 
return self . auth_code 
~~ def auth_get_token ( self , check_scope = True ) : 
res = self . auth_access_data_raw = self . _auth_token_request ( ) 
return self . _auth_token_process ( res , check_scope = check_scope ) 
~~ def get_user_id ( self ) : 
if self . _user_id is None : 
~~~ self . _user_id = self . get_user_data ( ) [ 'id' ] 
~~ return self . _user_id 
~~ def listdir ( self , folder_id = 'me/skydrive' , limit = None , offset = None ) : 
return self ( self . _api_url_join ( folder_id , 'files' ) , dict ( limit = limit , offset = offset ) ) 
~~ def get ( self , obj_id , byte_range = None ) : 
kwz = dict ( ) 
if byte_range : kwz [ 'headers' ] = dict ( Range = 'bytes={}' . format ( byte_range ) ) 
return self ( self . _api_url_join ( obj_id , 'content' ) , dict ( download = 'true' ) , raw = True , ** kwz ) 
~~ def put ( self , path_or_tuple , folder_id = 'me/skydrive' , 
overwrite = None , downsize = None , bits_api_fallback = True ) : 
api_overwrite = self . _translate_api_flag ( overwrite , 'overwrite' , [ 'ChooseNewName' ] ) 
api_downsize = self . _translate_api_flag ( downsize , 'downsize' ) 
name , src = self . _process_upload_source ( path_or_tuple ) 
if not isinstance ( bits_api_fallback , ( int , float , long ) ) : 
~~~ bits_api_fallback = bool ( bits_api_fallback ) 
~~ if bits_api_fallback is not False : 
~~~ if bits_api_fallback is True : bits_api_fallback = self . api_put_max_bytes 
src . seek ( 0 , os . SEEK_END ) 
if src . tell ( ) >= bits_api_fallback : 
~~~ log . info ( 
* ( ( float ( v ) / 2 ** 20 ) for v in [ src . tell ( ) , bits_api_fallback ] ) ) 
~~ if overwrite is not None and api_overwrite != 'true' : 
~~~ raise NoAPISupportError ( \ 
\ . format ( overwrite ) ) 
~~ if downsize is not None : 
~~~ log . info ( \ 
return self . info ( file_id ) 
~~ ~~ return self ( self . _api_url_join ( folder_id , 'files' , name ) , 
dict ( overwrite = api_overwrite , downsize_photo_uploads = api_downsize ) , 
data = src , method = 'put' , auth_header = True ) 
~~ def put_bits ( self , path_or_tuple , 
folder_id = None , folder_path = None , frag_bytes = None , 
raw_id = False , chunk_callback = None ) : 
if folder_id is not None and folder_path is not None : 
~~~ raise ValueError ( \ ) 
~~ if folder_id is None and folder_path is None : folder_id = 'me/skydrive' 
if folder_id and re . search ( r'^me(/.*)$' , folder_id ) : folder_id = self . info ( folder_id ) [ 'id' ] 
if not frag_bytes : frag_bytes = self . api_bits_default_frag_bytes 
user_id = self . get_user_id ( ) 
~~~ match = re . search ( r'^(?i)folder.[a-f0-9]+.' 
'(?P<user_id>[a-f0-9]+(?P<folder_n>!\\d+)?)$' , folder_id ) 
if match and not match . group ( 'folder_n' ) : 
~~~ folder_id , folder_path = None , '' 
~~~ if not match : 
~~ folder_id = match . group ( 'user_id' ) 
~~ ~~ if folder_id : 
~~~ url = self . api_bits_url_by_id . format ( folder_id = folder_id , user_id = user_id , filename = name ) 
~~~ url = self . api_bits_url_by_path . format ( 
folder_id = folder_id , user_id = user_id , file_path = ujoin ( folder_path , name ) . lstrip ( '/' ) ) 
~~ code , headers , body = self ( 
url , method = 'post' , auth_header = True , raw_all = True , 
headers = { 
'X-Http-Method-Override' : 'BITS_POST' , 
'BITS-Packet-Type' : 'Create-Session' , 
'BITS-Supported-Protocols' : self . api_bits_protocol_id } ) 
h = lambda k , hs = dict ( ( k . lower ( ) , v ) for k , v in headers . viewitems ( ) ) : hs . get ( k , '' ) 
checks = [ code == 201 , 
h ( 'bits-packet-type' ) . lower ( ) == 'ack' , 
h ( 'bits-protocol' ) . lower ( ) == self . api_bits_protocol_id . lower ( ) , 
h ( 'bits-session-id' ) ] 
if not all ( checks ) : 
~~ bits_sid = h ( 'bits-session-id' ) 
c , src_len = 0 , src . tell ( ) 
cn = src_len / frag_bytes 
if frag_bytes * cn != src_len : cn += 1 
src . seek ( 0 ) 
for n in xrange ( 1 , cn + 1 ) : 
frag = BITSFragment ( src , frag_bytes ) 
c1 = c + frag_bytes 
self ( 
url , method = 'post' , raw = True , data = frag , 
'BITS-Packet-Type' : 'Fragment' , 
'BITS-Session-Id' : bits_sid , 
c = c1 
if chunk_callback : 
~~~ chunk_callback ( 
bytes_transferred = c , bytes_total = src_len , 
chunks_transferred = n , chunks_total = cn , 
bits_session_id = bits_sid ) 
~~ ~~ if self . api_bits_auth_refresh_before_commit_hack : 
~~~ self . auth_get_token ( ) 
'BITS-Packet-Type' : 'Close-Session' , 
'BITS-Session-Id' : bits_sid } ) 
checks = [ code in [ 200 , 201 ] , h ( 'bits-packet-type' ) . lower ( ) == 'ack' ] 
~~ file_id = h ( 'x-resource-id' ) 
if not raw_id : file_id = 'file.{}.{}' . format ( user_id , file_id ) 
return file_id 
~~ def mkdir ( self , name = None , folder_id = 'me/skydrive' , metadata = dict ( ) ) : 
metadata = metadata . copy ( ) 
if name : metadata [ 'name' ] = name 
return self ( folder_id , data = metadata , method = 'post' , auth_header = True ) 
~~ def info_update ( self , obj_id , data ) : 
return self ( obj_id , method = 'put' , data = data , auth_header = True ) 
~~ def link ( self , obj_id , link_type = 'shared_read_link' ) : 
assert link_type in [ 'embed' , 'shared_read_link' , 'shared_edit_link' ] 
return self ( self . _api_url_join ( obj_id , link_type ) , method = 'get' ) 
~~ def copy ( self , obj_id , folder_id , move = False ) : 
return self ( obj_id , 
method = 'copy' if not move else 'move' , 
data = dict ( destination = folder_id ) , auth_header = True ) 
~~ def move ( self , obj_id , folder_id ) : 
return self . copy ( obj_id , folder_id , move = True ) 
~~ def comment_add ( self , obj_id , message ) : 
return self ( self . _api_url_join ( obj_id , 'comments' ) , 
method = 'post' , data = dict ( message = message ) , auth_header = True ) 
~~ def resolve_path ( self , path , root_id = 'me/skydrive' , objects = False , listdir_limit = 500 ) : 
if path : 
~~~ if isinstance ( path , types . StringTypes ) : 
~~~ if not path . startswith ( 'me/skydrive' ) : 
~~~ path = filter ( None , it . chain . from_iterable ( p . split ( '\\\\' ) for p in path . split ( '/' ) ) ) 
~~ else : root_id , path = path , None 
~~ if path : 
~~~ for i , name in enumerate ( path ) : 
~~~ offset = None 
~~~ obj_list = self . listdir ( root_id , offset = offset , limit = listdir_limit ) 
try : root_id = dict ( it . imap ( op . itemgetter ( 'name' , 'id' ) , obj_list ) ) [ name ] 
except KeyError : 
offset = ( offset or 0 ) + listdir_limit 
~~ else : break 
~~ ~~ ~~ except ( KeyError , ProtocolError ) as err : 
~~~ if isinstance ( err , ProtocolError ) and err . code != 404 : raise 
raise DoesNotExists ( root_id , path [ i : ] ) 
~~ ~~ ~~ return root_id if not objects else self . info ( root_id ) 
~~ def listdir ( self , folder_id = 'me/skydrive' , type_filter = None , limit = None , offset = None ) : 
lst = super ( OneDriveAPI , self ) . listdir ( folder_id = folder_id , limit = limit , offset = offset ) [ 'data' ] 
if type_filter : 
~~~ if isinstance ( type_filter , types . StringTypes ) : type_filter = { type_filter } 
lst = list ( obj for obj in lst if obj [ 'type' ] in type_filter ) 
~~ return lst 
if folder_id . startswith ( 'me/skydrive' ) : 
folder_id = self . info ( folder_id ) [ 'id' ] 
~~ return super ( OneDriveAPI , self ) . copy ( obj_id , folder_id , move = move ) 
~~ def from_conf ( cls , path = None , ** overrides ) : 
from onedrive import portalocker 
import yaml 
~~~ path = cls . conf_path_default 
~~ path = os . path . expanduser ( path ) 
with open ( path , 'rb' ) as src : 
~~~ portalocker . lock ( src , portalocker . LOCK_SH ) 
yaml_str = src . read ( ) 
portalocker . unlock ( src ) 
~~ conf = yaml . safe_load ( yaml_str ) 
conf . setdefault ( 'conf_save' , path ) 
conf_cls = dict ( ) 
for ns , keys in cls . conf_update_keys . viewitems ( ) : 
~~~ v = conf . get ( ns , dict ( ) ) . get ( k ) 
~~~ if not cls . conf_raise_structure_errors : raise 
raise KeyError ( ( 
~~ if v is not None : conf_cls [ '{}_{}' . format ( ns , k ) ] = conf [ ns ] [ k ] 
~~ ~~ conf_cls . update ( overrides ) 
if isinstance ( conf . get ( 'client' , dict ( ) ) . get ( 'id' ) , ( int , long ) ) : 
cid = conf [ 'client' ] [ 'id' ] 
if not re . search ( r'\\b(0*)?{:d}\\b' . format ( cid ) , yaml_str ) and re . search ( r'\\b(0*)?{:o}\\b' . format ( cid ) , yaml_str ) : 
~~~ cid = int ( '{:0}' . format ( cid ) ) 
~~ conf [ 'client' ] [ 'id' ] = '{:016d}' . format ( cid ) 
~~ self = cls ( ** conf_cls ) 
self . conf_save = conf [ 'conf_save' ] 
~~ def decode_obj ( obj , force = False ) : 
if isinstance ( obj , unicode ) : return obj 
elif isinstance ( obj , bytes ) : 
~~~ if force_encoding is not None : return obj . decode ( force_encoding ) 
if chardet : 
~~~ enc_guess = chardet . detect ( obj ) 
if enc_guess [ 'confidence' ] > 0.7 : 
~~~ return obj . decode ( enc_guess [ 'encoding' ] ) 
~~ ~~ return obj . decode ( 'utf-8' ) 
~~~ return obj if not force else repr ( obj ) 
~~ ~~ def cgetter ( self , fcget : typing . Optional [ typing . Callable [ [ typing . Any ] , typing . Any ] ] ) -> "AdvancedProperty" : 
self . __fcget = fcget 
~~ def instance_method ( self , imeth : typing . Optional [ typing . Callable [ ... , typing . Any ] ] ) -> "SeparateClassMethod" : 
self . __instance_method = imeth 
~~ def class_method ( self , cmeth : typing . Optional [ typing . Callable [ ... , typing . Any ] ] ) -> "SeparateClassMethod" : 
self . __class_method = cmeth 
~~ def __traceback ( self ) -> str : 
if not self . log_traceback : 
~~~ return "" 
~~ exc_info = sys . exc_info ( ) 
stack = traceback . extract_stack ( ) 
exc_tb = traceback . extract_tb ( exc_info [ 2 ] ) 
exc_line : typing . List [ str ] = traceback . format_exception_only ( * exc_info [ : 2 ] ) 
return tb_text 
~~ def __get_obj_source ( self , instance : typing . Any , owner : typing . Optional [ type ] = None ) -> str : 
if self . log_object_repr : 
~~~ return f"{instance!r}" 
~~ def _get_logger_for_instance ( self , instance : typing . Any ) -> logging . Logger : 
~~~ return self . logger 
~~ elif hasattr ( instance , "logger" ) and isinstance ( instance . logger , logging . Logger ) : 
~~~ return instance . logger 
~~ elif hasattr ( instance , "log" ) and isinstance ( instance . log , logging . Logger ) : 
~~~ return instance . log 
~~ return _LOGGER 
~~ def logger ( self , logger : typing . Union [ logging . Logger , str , None ] ) -> None : 
if logger is None or isinstance ( logger , logging . Logger ) : 
~~~ self . __logger = logger 
~~~ self . __logger = logging . getLogger ( logger ) 
~~ ~~ def get_simple_vars_from_src ( src ) : 
ast_data = ( ast . Str , ast . Num , ast . List , ast . Set , ast . Dict , ast . Tuple , ast . Bytes , ast . NameConstant ) 
tree = ast . parse ( src ) 
result = collections . OrderedDict ( ) 
for node in ast . iter_child_nodes ( tree ) : 
~~~ if isinstance ( node . value , ast_data ) : 
~~~ value = ast . literal_eval ( node . value ) 
~~ for tgt in node . targets : 
~~~ if isinstance ( tgt , ast . Name ) and isinstance ( tgt . ctx , ast . Store ) : 
~~~ result [ tgt . id ] = value 
~~~ build_ext . build_ext . run ( self ) 
build_dir = os . path . abspath ( self . build_lib ) 
root_dir = os . path . abspath ( os . path . join ( __file__ , ".." ) ) 
target_dir = build_dir if not self . inplace else root_dir 
src_file = os . path . join ( "advanced_descriptors" , "__init__.py" ) 
src = os . path . join ( root_dir , src_file ) 
dst = os . path . join ( target_dir , src_file ) 
if src != dst : 
~~~ shutil . copyfile ( src , dst ) 
~~ ~~ except ( 
distutils . errors . DistutilsPlatformError , 
FileNotFoundError , 
~~~ raise BuildFailed ( ) 
~~ ~~ def _call_api ( self , method , params = None ) : 
url = self . url . format ( method = method ) 
~~~ params = { 'token' : self . token } 
~~~ params [ 'token' ] = self . token 
response = requests . get ( url , params = params ) . json ( ) 
if self . verify : 
~~~ if not response [ 'ok' ] : 
raise Exception ( msg . format ( url = url , response = response ) ) 
~~ def channels ( self ) : 
if not self . _channels : 
~~~ self . _channels = self . _call_api ( 'channels.list' ) [ 'channels' ] 
~~ return self . _channels 
~~ def users ( self ) : 
if not self . _users : 
~~~ self . _users = self . _call_api ( 'users.list' ) [ 'members' ] 
~~ return self . _users 
~~ def channel_from_name ( self , name ) : 
~~~ channel = [ channel for channel in self . channels 
if channel [ 'name' ] == name ] [ 0 ] 
~~~ raise ValueError ( \ . format ( name ) ) 
~~ return channel 
~~ def make_message ( self , text , channel ) : 
~~~ channel_id = self . slack . channel_from_name ( channel ) [ 'id' ] 
~~~ channel_id = channel 
~~ return pack ( { 
'text' : text , 
'type' : 'message' , 
'channel' : channel_id , 
'id' : self . message_id , 
~~ def translate ( self , message ) : 
~~~ user_id = message . pop ( 'user' ) 
user = self . slack . user_from_id ( user_id ) 
message [ u'user' ] = user [ 'name' ] 
~~ except ( KeyError , IndexError , ValueError ) : 
~~~ if type ( message [ 'channel' ] ) == str : 
~~~ channel_id = message . pop ( 'channel' ) 
~~~ channel_id = message . pop ( 'channel' ) [ 'id' ] 
self . slack . reload_channels ( ) 
~~ channel = self . slack . channel_from_id ( channel_id ) 
message [ u'channel' ] = channel [ 'name' ] 
~~ return message 
~~ def onMessage ( self , payload , isBinary ) : 
msg = self . translate ( unpack ( payload ) ) 
if 'type' in msg : 
~~~ channel_name = 'slack.{}' . format ( msg [ 'type' ] ) 
channels . Channel ( channel_name ) . send ( { 'text' : pack ( msg ) } ) 
~~ ~~ def sendSlack ( self , message ) : 
channel = message . get ( 'channel' , 'general' ) 
self . sendMessage ( self . make_message ( message [ 'text' ] , channel ) ) 
~~ def read_channel ( self ) : 
channel , message = self . protocol . channel_layer . receive_many ( [ u'slack.send' ] , block = False ) 
delay = 0.1 
if channel : 
~~~ self . protocols [ 0 ] . sendSlack ( message ) 
~~ reactor . callLater ( delay , self . read_channel ) 
slack = SlackAPI ( token = self . token ) 
rtm = slack . rtm_start ( ) 
factory = SlackClientFactory ( rtm [ 'url' ] ) 
factory . protocol = SlackClientProtocol 
factory . protocol . slack = slack 
factory . protocol . channel_layer = self . channel_layer 
factory . channel_name = self . channel_name 
factory . run ( ) 
~~ def run ( self , args ) : 
args = self . parser . parse_args ( args ) 
if not args . token : 
~~ sys . path . insert ( 0 , "." ) 
module_path , object_path = args . channel_layer . split ( ':' , 1 ) 
channel_layer = importlib . import_module ( module_path ) 
for part in object_path . split ( '.' ) : 
~~~ channel_layer = getattr ( channel_layer , part ) 
~~ Client ( 
channel_layer = channel_layer , 
token = args . token , 
) . run ( ) 
~~ def _set_affiliation ( self , v , load = False ) : 
if hasattr ( v , "_utype" ) : 
~~~ v = v . _utype ( v ) 
~~~ t = YANGDynClass ( 
v , 
base = RestrictedClassType ( 
base_type = unicode , 
restriction_type = "dict_key" , 
restriction_arg = { 
u"napalm-star-wars:EMPIRE" : { 
"@namespace" : u"https://napalm-yang.readthedocs.io/napalm-star-wars" , 
"@module" : u"napalm-star-wars" , 
u"EMPIRE" : { 
u"napalm-star-wars:REBEL_ALLIANCE" : { 
u"REBEL_ALLIANCE" : { 
is_leaf = True , 
yang_name = "affiliation" , 
parent = self , 
path_helper = self . _path_helper , 
extmethods = self . _extmethods , 
register_paths = True , 
namespace = "https://napalm-yang.readthedocs.io/napalm-star-wars" , 
defining_module = "napalm-star-wars" , 
yang_type = "identityref" , 
is_config = True , 
"defined-type" : "napalm-star-wars:identityref" , 
~~ self . __affiliation = t 
if hasattr ( self , "_set" ) : 
~~~ self . _set ( ) 
~~ ~~ def dict_diff ( prv , nxt ) : 
keys = set ( prv . keys ( ) + nxt . keys ( ) ) 
result = { } 
~~~ if prv . get ( k ) != nxt . get ( k ) : 
~~~ result [ k ] = ( prv . get ( k ) , nxt . get ( k ) ) 
~~ def colorize ( msg , color ) : 
if DONT_COLORIZE : 
~~~ return "{}{}{}" . format ( COLORS [ color ] , msg , COLORS [ "endc" ] ) 
~~ ~~ def v2_playbook_on_task_start ( self , task , ** kwargs ) : 
self . last_task_name = task . get_name ( ) 
self . printed_last_task = False 
~~ def v2_runner_on_ok ( self , result , ** kwargs ) : 
failed = "failed" in result . _result 
unreachable = "unreachable" in result . _result 
"print_action" in result . _task . tags 
or failed 
or unreachable 
or self . _display . verbosity > 1 
~~~ self . _print_task ( ) 
self . last_skipped = False 
msg = unicode ( result . _result . get ( "msg" , "" ) ) or unicode ( 
result . _result . get ( "reason" , "" ) 
) or unicode ( 
result . _result . get ( "message" , "" ) 
stderr = [ 
result . _result . get ( "exception" , None ) , 
result . _result . get ( "module_stderr" , None ) , 
stderr = "\\n" . join ( [ e for e in stderr if e ] ) . strip ( ) 
self . _print_host_or_item ( 
result . _host , 
result . _result . get ( "changed" , False ) , 
msg , 
result . _result . get ( "diff" , None ) , 
is_host = True , 
error = failed or unreachable , 
stdout = result . _result . get ( "module_stdout" , None ) , 
stderr = stderr . strip ( ) , 
if "results" in result . _result : 
~~~ for r in result . _result [ "results" ] : 
~~~ failed = "failed" in r 
stderr = [ r . get ( "exception" , None ) , r . get ( "module_stderr" , None ) ] 
r [ "item" ] , 
r . get ( "changed" , False ) , 
unicode ( r . get ( "msg" , "" ) ) , 
r . get ( "diff" , None ) , 
is_host = False , 
error = failed , 
stdout = r . get ( "module_stdout" , None ) , 
~~~ self . last_skipped = True 
print ( "." , end = "" ) 
~~ ~~ def v2_playbook_on_stats ( self , stats ) : 
self . _print_task ( "STATS" ) 
hosts = sorted ( stats . processed . keys ( ) ) 
for host in hosts : 
~~~ s = stats . summarize ( host ) 
if s [ "failures" ] or s [ "unreachable" ] : 
~~~ color = "failed" 
~~ elif s [ "changed" ] : 
~~~ color = "changed" 
~~~ color = "ok" 
host , s [ "ok" ] , s [ "changed" ] , s [ "failures" ] , s [ "unreachable" ] 
print ( colorize ( msg , color ) ) 
~~ ~~ def v2_runner_on_skipped ( self , result , ** kwargs ) : 
if self . _display . verbosity > 1 : 
line_length = 120 
colorize ( result . _host . name , "not_so_bold" ) , 
spaces , 
colorize ( "skipped" , "skipped" ) , 
reason = result . _result . get ( "skipped_reason" , "" ) or result . _result . get ( 
"skip_reason" , "" 
if len ( reason ) < 50 : 
print ( self . _indent_text ( reason , 8 ) ) 
print ( reason ) 
~~ ~~ ~~ def parse_indented_config ( config , current_indent = 0 , previous_indent = 0 , nested = False ) : 
parsed = OrderedDict ( ) 
~~~ if not config : 
~~ line = config . pop ( 0 ) 
if line . lstrip ( ) . startswith ( "!" ) : 
~~ last = line . lstrip ( ) 
leading_spaces = len ( line ) - len ( last ) 
if leading_spaces > current_indent : 
~~~ current = parse_indented_config ( 
config , leading_spaces , current_indent , True 
_attach_data_to_path ( parsed , last , current , nested ) 
~~ elif leading_spaces < current_indent : 
~~~ config . insert ( 0 , line ) 
~~~ if not nested : 
~~ ~~ ~~ return parsed 
prefix = netaddr . IPNetwork ( value ) 
return "{}{}{}" . format ( prefix . ip , sep , prefix . netmask ) 
~~ def _set_keepalive_interval ( self , v , load = False ) : 
base = RestrictedPrecisionDecimalType ( precision = 2 ) , 
default = Decimal ( 30 ) , 
yang_name = "keepalive-interval" , 
namespace = "http://openconfig.net/yang/network-instance" , 
defining_module = "openconfig-network-instance" , 
yang_type = "decimal64" , 
is_config = False , 
"defined-type" : "decimal64" , 
~~ self . __keepalive_interval = t 
~~ ~~ def check_empty ( default = "" ) : 
def real_decorator ( func ) : 
~~~ @ wraps ( func ) 
def wrapper ( value , * args , ** kwargs ) : 
~~~ if not value : 
~~~ return func ( value , * args , ** kwargs ) 
~~ return real_decorator 
~~ def add_model ( self , model , force = False ) : 
if isinstance ( model , str ) : 
~~~ self . _load_model ( model ) 
~~~ model = model ( ) 
~~ if model . _yang_name not in [ a [ 0 ] for a in SUPPORTED_MODELS ] and not force : 
~~ for k , v in model : 
~~~ self . _elements [ k ] = v 
setattr ( self , k , v ) 
~~ ~~ def get ( self , filter = False ) : 
for k , v in self . elements ( ) . items ( ) : 
~~~ intermediate = v . get ( filter = filter ) 
if intermediate : 
~~~ result [ k ] = intermediate 
~~ def load_dict ( self , data , overwrite = False , auto_load_model = True ) : 
for k , v in data . items ( ) : 
~~~ if k not in self . _elements . keys ( ) and not auto_load_model : 
~~ elif k not in self . _elements . keys ( ) and auto_load_model : 
~~~ self . _load_model ( k ) 
~~ attr = getattr ( self , k ) 
_load_dict ( attr , v ) 
~~ ~~ def to_dict ( self , filter = True ) : 
for k , v in self : 
~~~ r = _to_dict ( v , filter ) 
if r : 
~~~ result [ k ] = r 
~~ def parse_config ( self , device = None , profile = None , native = None , attrs = None ) : 
if attrs is None : 
~~~ attrs = self . elements ( ) . values ( ) 
~~ for v in attrs : 
~~~ parser = Parser ( 
v , device = device , profile = profile , native = native , is_config = True 
parser . parse ( ) 
~~ ~~ def parse_state ( self , device = None , profile = None , native = None , attrs = None ) : 
v , device = device , profile = profile , native = native , is_config = False 
~~ ~~ def translate_config ( self , profile , merge = None , replace = None ) : 
~~~ other_merge = getattr ( merge , k ) if merge else None 
other_replace = getattr ( replace , k ) if replace else None 
translator = Translator ( 
v , profile , merge = other_merge , replace = other_replace 
result . append ( translator . translate ( ) ) 
~~ return "\\n" . join ( result ) 
~~ def load_filters ( ) : 
all_filters = { } 
for m in JINJA_FILTERS : 
~~~ if hasattr ( m , "filters" ) : 
~~~ all_filters . update ( m . filters ( ) ) 
~~ ~~ return all_filters 
~~ def _parse_list_nested_recursive ( 
cls , data , path , iterators , list_vars , cur_vars = None 
cur_vars = dict ( cur_vars ) if cur_vars else { } 
~~~ p = path [ 0 ] 
path = path [ 1 : ] 
~~~ for _ in data : 
~~~ list_vars . append ( cur_vars ) 
~~ iterators . append ( data ) 
~~ if p . startswith ( "?" ) : 
~~~ for x in data : 
~~~ key , var_path = p . split ( "." ) 
cur_vars . update ( { key . lstrip ( "?" ) : x . xpath ( var_path ) [ 0 ] . text } ) 
cls . _parse_list_nested_recursive ( 
x , path , iterators , list_vars , cur_vars 
~~~ x = data . xpath ( p ) 
cls . _parse_list_nested_recursive ( x , path , iterators , list_vars , cur_vars ) 
~~ ~~ def _flatten_dictionary ( obj , path , key_name ) : 
if ">" in key_name : 
~~~ key_name , group_key = key_name . split ( ">" ) 
~~~ group_key = None 
~~ for k , v in obj . items ( ) : 
~~~ if path : 
~~~ if k == path [ 0 ] : 
~~~ path . pop ( 0 ) 
~~ ~~ if k . startswith ( "#" ) : 
~~ r = _resolve_path ( v , list ( path ) ) 
if isinstance ( r , dict ) : 
~~~ r = [ r ] 
~~ for e in r : 
~~~ if group_key : 
~~~ e [ group_key ] = { kk : vv for kk , vv in v . items ( ) if kk not in path } 
~~ e [ key_name ] = k 
result . append ( e ) 
~~ def _set_trunk_vlans ( self , v , load = False ) : 
base = TypedListType ( 
allowed_type = [ 
RestrictedClassType ( 
base_type = RestrictedClassType ( 
base_type = int , 
restriction_dict = { "range" : [ "0..65535" ] } , 
int_size = 16 , 
restriction_dict = { "range" : [ "1..4094" ] } , 
base_type = six . text_type , 
restriction_dict = { 
"pattern" : "(409[0-4]|40[0-8][0-9]|[1-3][0-9]{3}|[1-9][0-9]{1,2}|[1-9])\\\\.\\\\.(409[0-4]|40[0-8][0-9]|[1-3][0-9]{3}|[1-9][0-9]{1,2}|[1-9])" 
"pattern" : "(409[0-4]|40[0-8][0-9]|[1-3][0-9]{3}|[1-9][0-9]{1,2}|[1-9])\\\\.((409[0-4]|40[0-8][0-9]|[1-3][0-9]{3}|[1-9][0-9]{1,2}|[1-9])|\\\\*)" 
"pattern" : "(409[0-4]|40[0-8][0-9]|[1-3][0-9]{3}|[1-9][0-9]{1,2}|[1-9])\\\\.\\\\.(409[0-4]|40[0-8][0-9]|[1-3][0-9]{3}|[1-9][0-9]{1,2}|[1-9])\\\\.((409[0-4]|40[0-8][0-9]|[1-3][0-9]{3}|[1-9][0-9]{1,2}|[1-9])|\\\\*)" 
"pattern" : "(\\\\*|(409[0-4]|40[0-8][0-9]|[1-3][0-9]{3}|[1-9][0-9]{1,2}|[1-9]))\\\\.(409[0-4]|40[0-8][0-9]|[1-3][0-9]{3}|[1-9][0-9]{1,2}|[1-9])\\\\.\\\\.(409[0-4]|40[0-8][0-9]|[1-3][0-9]{3}|[1-9][0-9]{1,2}|[1-9])" 
is_leaf = False , 
yang_name = "trunk-vlans" , 
namespace = "http://openconfig.net/yang/vlan" , 
defining_module = "openconfig-vlan" , 
yang_type = "union" , 
"defined-type" : "openconfig-vlan:union" , 
~~ self . __trunk_vlans = t 
~~ ~~ def find_yang_file ( profile , filename , path ) : 
module_dir = os . path . dirname ( __file__ ) 
full_path = os . path . join ( module_dir , "mappings" , profile , path , filename ) 
if os . path . exists ( full_path ) : 
~~~ return full_path 
~~~ msg = "Couldn\ . format ( full_path ) 
logger . error ( msg ) 
raise IOError ( msg ) 
~~ ~~ def model_to_dict ( model , mode = "" , show_defaults = False ) : 
def is_mode ( obj , mode ) : 
~~~ if mode == "" : 
~~ elif mode == "config" : 
~~~ return obj . _yang_name == "config" or obj . _is_config 
~~ elif mode == "state" : 
~~~ return obj . _yang_name == "state" or not obj . _is_config 
~~ ~~ def get_key ( key , model , parent_defining_module , show_defaults ) : 
~~~ if not show_defaults : 
~~ if parent_defining_module != model . _defining_module : 
~~~ key = "{}:{}" . format ( model . _defining_module , key ) 
~~ return key 
~~ if model . _yang_type in ( "container" , "list" ) : 
~~~ cls = model if model . _yang_type in ( "container" , ) else model . _contained_class ( ) 
for k , v in cls : 
~~~ r = model_to_dict ( v , mode = mode , show_defaults = show_defaults ) 
~~~ result [ get_key ( k , v , model . _defining_module , show_defaults ) ] = r 
~~~ if show_defaults : 
~~~ if model . _default is False : 
~~~ if model . _yang_type != "boolean" : 
~~ ~~ return model . _default 
~~ return model . _yang_type if is_mode ( model , mode ) else None 
~~ ~~ def diff ( f , s ) : 
if isinstance ( f , base . Root ) or f . _yang_type in ( "container" , None ) : 
~~~ result = _diff_root ( f , s ) 
~~ elif f . _yang_type in ( "list" , ) : 
~~~ result = _diff_list ( f , s ) 
~~~ result = { } 
first = "{}" . format ( f ) 
second = "{}" . format ( s ) 
if first != second : 
~~~ result = { "first" : first , "second" : second } 
~~ def make_i2c_rdwr_data ( messages ) : 
msg_data_type = i2c_msg * len ( messages ) 
msg_data = msg_data_type ( ) 
for i , message in enumerate ( messages ) : 
~~~ msg_data [ i ] . addr = message [ 0 ] & 0x7F 
msg_data [ i ] . flags = message [ 1 ] 
msg_data [ i ] . len = message [ 2 ] 
msg_data [ i ] . buf = message [ 3 ] 
~~ data = i2c_rdwr_ioctl_data ( ) 
data . msgs = msg_data 
data . nmsgs = len ( messages ) 
~~ def open ( self , bus ) : 
if self . _device is not None : 
~~~ self . close ( ) 
~~ self . _device = open ( '/dev/i2c-{0}' . format ( bus ) , 'r+b' , buffering = 0 ) 
~~ def read_byte ( self , addr ) : 
self . _select_device ( addr ) 
return ord ( self . _device . read ( 1 ) ) 
~~ def read_bytes ( self , addr , number ) : 
return self . _device . read ( number ) 
~~ def read_byte_data ( self , addr , cmd ) : 
reg = c_uint8 ( cmd ) 
result = c_uint8 ( ) 
request = make_i2c_rdwr_data ( [ 
ioctl ( self . _device . fileno ( ) , I2C_RDWR , request ) 
return result . value 
~~ def read_word_data ( self , addr , cmd ) : 
result = c_uint16 ( ) 
~~ def read_i2c_block_data ( self , addr , cmd , length = 32 ) : 
result = create_string_buffer ( length ) 
return bytearray ( result . raw ) 
~~ def write_quick ( self , addr ) : 
~~ def write_byte ( self , addr , val ) : 
data = bytearray ( 1 ) 
data [ 0 ] = val & 0xFF 
self . _device . write ( data ) 
~~ def write_bytes ( self , addr , buf ) : 
self . _device . write ( buf ) 
~~ def write_byte_data ( self , addr , cmd , val ) : 
data = bytearray ( 2 ) 
data [ 0 ] = cmd & 0xFF 
data [ 1 ] = val & 0xFF 
~~ def write_word_data ( self , addr , cmd , val ) : 
data = struct . pack ( '=BH' , cmd & 0xFF , val & 0xFFFF ) 
~~ def write_block_data ( self , addr , cmd , vals ) : 
data = bytearray ( len ( vals ) + 1 ) 
data [ 0 ] = len ( vals ) & 0xFF 
data [ 1 : ] = vals [ 0 : ] 
self . write_i2c_block_data ( addr , cmd , data ) 
~~ def write_i2c_block_data ( self , addr , cmd , vals ) : 
~~ def process_call ( self , addr , cmd , val ) : 
data = create_string_buffer ( struct . pack ( '=BH' , cmd , val ) ) 
~~ def home_mode_set_state ( self , state , ** kwargs ) : 
if state not in ( HOME_MODE_ON , HOME_MODE_OFF ) : 
~~ api = self . _api_info [ 'home_mode' ] 
payload = dict ( { 
'api' : api [ 'name' ] , 
'method' : 'Switch' , 
'version' : api [ 'version' ] , 
'on' : state , 
'_sid' : self . _sid , 
} , ** kwargs ) 
response = self . _get_json_with_retry ( api [ 'url' ] , payload ) 
if response [ 'success' ] : 
~~ def home_mode_status ( self , ** kwargs ) : 
api = self . _api_info [ 'home_mode' ] 
'method' : 'GetInfo' , 
'_sid' : self . _sid 
return response [ 'data' ] [ 'on' ] 
~~ def camera_list ( self , ** kwargs ) : 
api = self . _api_info [ 'camera' ] 
'method' : 'List' , 
cameras = [ ] 
for data in response [ 'data' ] [ 'cameras' ] : 
~~~ cameras . append ( Camera ( data , self . _video_stream_url ) ) 
~~ return cameras 
~~ def camera_info ( self , camera_ids , ** kwargs ) : 
~~ def camera_snapshot ( self , camera_id , ** kwargs ) : 
'method' : 'GetSnapshot' , 
'cameraId' : camera_id , 
response = self . _get ( api [ 'url' ] , payload ) 
return response . content 
~~ def camera_disable ( self , camera_id , ** kwargs ) : 
'method' : 'Disable' , 
'version' : 9 , 
'idList' : camera_id , 
print ( api [ 'url' ] ) 
print ( payload ) 
return response [ 'success' ] 
~~ def camera_event_motion_enum ( self , camera_id , ** kwargs ) : 
api = self . _api_info [ 'camera_event' ] 
'method' : 'MotionEnum' , 
'camId' : camera_id , 
return MotionSetting ( camera_id , response [ 'data' ] [ 'MDParam' ] ) 
~~ def camera_event_md_param_save ( self , camera_id , ** kwargs ) : 
'method' : 'MDParamSave' , 
return response [ 'data' ] [ 'camId' ] 
~~ def update ( self ) : 
cameras = self . _api . camera_list ( ) 
self . _cameras_by_id = { v . camera_id : v for i , v in enumerate ( cameras ) } 
motion_settings = [ ] 
for camera_id in self . _cameras_by_id . keys ( ) : 
~~~ motion_setting = self . _api . camera_event_motion_enum ( camera_id ) 
motion_settings . append ( motion_setting ) 
~~ self . _motion_settings_by_id = { 
v . camera_id : v for i , v in enumerate ( motion_settings ) } 
~~ def set_home_mode ( self , state ) : 
state_parameter = HOME_MODE_OFF 
if state : 
~~~ state_parameter = HOME_MODE_ON 
~~ return self . _api . home_mode_set_state ( state_parameter ) 
~~ def replace_ext ( file_path , new_ext ) : 
if not new_ext . startswith ( os . extsep ) : 
~~~ new_ext = os . extsep + new_ext 
~~ index = file_path . rfind ( os . extsep ) 
return file_path [ : index ] + new_ext 
~~ def is_last_li ( li , meta_data , current_numId ) : 
if not is_li ( li , meta_data ) : 
~~ w_namespace = get_namespace ( li , 'w' ) 
next_el = li 
~~~ if next_el is None : 
~~ next_el = next_el . getnext ( ) 
if not is_li ( next_el , meta_data ) : 
~~ new_numId = get_numId ( next_el , w_namespace ) 
if current_numId != new_numId : 
~~ ~~ def get_single_list_nodes_data ( li , meta_data ) : 
yield li 
w_namespace = get_namespace ( li , 'w' ) 
current_numId = get_numId ( li , w_namespace ) 
starting_ilvl = get_ilvl ( li , w_namespace ) 
el = li 
~~~ el = el . getnext ( ) 
if el is None : 
~~ if not has_text ( el ) : 
~~ if _is_top_level_upper_roman ( el , meta_data ) : 
~~ if ( 
is_li ( el , meta_data ) and 
( starting_ilvl > get_ilvl ( el , w_namespace ) ) ) : 
~~ new_numId = get_numId ( el , w_namespace ) 
if new_numId is None or new_numId == - 1 : 
~~~ yield el 
~~ if current_numId != new_numId : 
~~ if is_last_li ( el , meta_data , current_numId ) : 
~~ yield el 
~~ ~~ def get_ilvl ( li , w_namespace ) : 
ilvls = li . xpath ( './/w:ilvl' , namespaces = li . nsmap ) 
if len ( ilvls ) == 0 : 
~~~ return - 1 
~~ return int ( ilvls [ 0 ] . get ( '%sval' % w_namespace ) ) 
~~ def get_numId ( li , w_namespace ) : 
numIds = li . xpath ( './/w:numId' , namespaces = li . nsmap ) 
if len ( numIds ) == 0 : 
~~ return numIds [ 0 ] . get ( '%sval' % w_namespace ) 
~~ def create_list ( list_type ) : 
list_types = { 
'bullet' : 'ul' , 
el = etree . Element ( list_types . get ( list_type , 'ol' ) ) 
list_type_conversions = { 
'decimal' : DEFAULT_LIST_NUMBERING_STYLE , 
'decimalZero' : 'decimal-leading-zero' , 
'upperRoman' : 'upper-roman' , 
'lowerRoman' : 'lower-roman' , 
'upperLetter' : 'upper-alpha' , 
'lowerLetter' : 'lower-alpha' , 
'ordinal' : DEFAULT_LIST_NUMBERING_STYLE , 
'cardinalText' : DEFAULT_LIST_NUMBERING_STYLE , 
'ordinalText' : DEFAULT_LIST_NUMBERING_STYLE , 
if list_type != 'bullet' : 
~~~ el . set ( 
'data-list-type' , 
list_type_conversions . get ( list_type , DEFAULT_LIST_NUMBERING_STYLE ) , 
~~ return el 
~~ def get_v_merge ( tc ) : 
if tc is None : 
~~ v_merges = tc . xpath ( './/w:vMerge' , namespaces = tc . nsmap ) 
if len ( v_merges ) != 1 : 
~~ v_merge = v_merges [ 0 ] 
return v_merge 
~~ def get_grid_span ( tc ) : 
w_namespace = get_namespace ( tc , 'w' ) 
grid_spans = tc . xpath ( './/w:gridSpan' , namespaces = tc . nsmap ) 
if len ( grid_spans ) != 1 : 
~~~ return 1 
~~ grid_span = grid_spans [ 0 ] 
return int ( grid_span . get ( '%sval' % w_namespace ) ) 
~~ def get_td_at_index ( tr , index ) : 
current = 0 
for td in tr . xpath ( './/w:tc' , namespaces = tr . nsmap ) : 
~~~ if index == current : 
~~~ return td 
~~ current += get_grid_span ( td ) 
~~ ~~ def style_is_false ( style ) : 
if style is None : 
~~ w_namespace = get_namespace ( style , 'w' ) 
return style . get ( '%sval' % w_namespace ) != 'false' 
~~ def is_bold ( r ) : 
w_namespace = get_namespace ( r , 'w' ) 
rpr = r . find ( '%srPr' % w_namespace ) 
if rpr is None : 
~~ bold = rpr . find ( '%sb' % w_namespace ) 
return style_is_false ( bold ) 
~~ def is_italics ( r ) : 
~~ italics = rpr . find ( '%si' % w_namespace ) 
return style_is_false ( italics ) 
~~ def is_underlined ( r ) : 
~~ underline = rpr . find ( '%su' % w_namespace ) 
return style_is_false ( underline ) 
~~ def is_title ( p ) : 
w_namespace = get_namespace ( p , 'w' ) 
styles = p . xpath ( './/w:pStyle' , namespaces = p . nsmap ) 
if len ( styles ) == 0 : 
~~ style = styles [ 0 ] 
return style . get ( '%sval' % w_namespace ) == 'Title' 
~~ def get_text_run_content_data ( r ) : 
valid_elements = ( 
'%st' % w_namespace , 
'%sdrawing' % w_namespace , 
'%spict' % w_namespace , 
'%sbr' % w_namespace , 
for el in r : 
~~~ if el . tag in valid_elements : 
~~ ~~ ~~ def whole_line_styled ( p ) : 
r_tags = p . xpath ( './/w:r' , namespaces = p . nsmap ) 
tags_are_bold = [ 
is_bold ( r ) or is_underlined ( r ) for r in r_tags 
tags_are_italics = [ 
is_italics ( r ) for r in r_tags 
return all ( tags_are_bold ) , all ( tags_are_italics ) 
~~ def get_numbering_info ( tree ) : 
if tree is None : 
~~ w_namespace = get_namespace ( tree , 'w' ) 
num_ids = { } 
result = defaultdict ( dict ) 
for list_type in tree . findall ( '%snum' % w_namespace ) : 
~~~ list_id = list_type . get ( '%snumId' % w_namespace ) 
abstract_number = list_type . find ( '%sabstractNumId' % w_namespace ) 
num_ids [ abstract_number . get ( '%sval' % w_namespace ) ] = list_id 
~~ for abstract_number in tree . findall ( '%sabstractNum' % w_namespace ) : 
~~~ abstract_num_id = abstract_number . get ( '%sabstractNumId' % w_namespace ) 
if abstract_num_id not in num_ids : 
~~ for lvl in abstract_number . findall ( '%slvl' % w_namespace ) : 
~~~ ilvl = int ( lvl . get ( '%silvl' % w_namespace ) ) 
lvl_format = lvl . find ( '%snumFmt' % w_namespace ) 
list_style = lvl_format . get ( '%sval' % w_namespace ) 
result [ num_ids [ abstract_num_id ] ] [ ilvl ] = list_style 
~~ def get_style_dict ( tree ) : 
for el in tree : 
~~~ style_id = el . get ( '%sstyleId' % w_namespace ) 
el_result = { 
'header' : False , 
'font_size' : None , 
'based_on' : None , 
name = el . find ( '%sname' % w_namespace ) 
~~ value = name . get ( '%sval' % w_namespace ) . lower ( ) 
if value in headers : 
~~~ el_result [ 'header' ] = headers [ value ] 
~~ rpr = el . find ( '%srPr' % w_namespace ) 
~~ size = rpr . find ( '%ssz' % w_namespace ) 
if size is None : 
~~~ el_result [ 'font_size' ] = None 
~~~ el_result [ 'font_size' ] = size . get ( '%sval' % w_namespace ) 
~~ based_on = el . find ( '%sbasedOn' % w_namespace ) 
if based_on is None : 
~~~ el_result [ 'based_on' ] = None 
~~~ el_result [ 'based_on' ] = based_on . get ( '%sval' % w_namespace ) 
~~ result [ style_id ] = el_result 
~~ def get_relationship_info ( tree , media , image_sizes ) : 
for el in tree . iter ( ) : 
~~~ el_id = el . get ( 'Id' ) 
if el_id is None : 
~~ target = el . get ( 'Target' ) 
if any ( 
target . lower ( ) . endswith ( ext ) for 
ext in IMAGE_EXTENSIONS_TO_SKIP ) : 
~~ if target in media : 
~~~ image_size = image_sizes . get ( el_id ) 
target = convert_image ( media [ target ] , image_size ) 
~~ result [ el_id ] = cgi . escape ( target ) 
~~ def _get_document_data ( f , image_handler = None ) : 
if image_handler is None : 
~~~ def image_handler ( image_id , relationship_dict ) : 
~~~ return relationship_dict . get ( image_id ) 
~~ ~~ document_xml = None 
numbering_xml = None 
relationship_xml = None 
styles_xml = None 
parser = etree . XMLParser ( strip_cdata = False ) 
path , _ = os . path . split ( f . filename ) 
media = { } 
image_sizes = { } 
for item in f . infolist ( ) : 
~~~ if item . filename == 'word/document.xml' : 
~~~ xml = f . read ( item . filename ) 
document_xml = etree . fromstring ( xml , parser ) 
~~ elif item . filename == 'word/numbering.xml' : 
numbering_xml = etree . fromstring ( xml , parser ) 
~~ elif item . filename == 'word/styles.xml' : 
styles_xml = etree . fromstring ( xml , parser ) 
~~ elif item . filename == 'word/_rels/document.xml.rels' : 
~~~ relationship_xml = etree . fromstring ( xml , parser ) 
~~ except XMLSyntaxError : 
~~~ relationship_xml = etree . fromstring ( '<xml></xml>' , parser ) 
~~ ~~ if item . filename . startswith ( 'word/media/' ) : 
~~~ media [ item . filename [ len ( 'word/' ) : ] ] = f . extract ( 
item . filename , 
path , 
~~ ~~ f . close ( ) 
numbering_dict = get_numbering_info ( numbering_xml ) 
image_sizes = get_image_sizes ( document_xml ) 
relationship_dict = get_relationship_info ( 
relationship_xml , 
media , 
image_sizes 
styles_dict = get_style_dict ( styles_xml ) 
font_sizes_dict = defaultdict ( int ) 
if DETECT_FONT_SIZE : 
~~~ font_sizes_dict = get_font_sizes_dict ( document_xml , styles_dict ) 
~~ meta_data = MetaData ( 
numbering_dict = numbering_dict , 
relationship_dict = relationship_dict , 
styles_dict = styles_dict , 
font_sizes_dict = font_sizes_dict , 
image_handler = image_handler , 
image_sizes = image_sizes , 
return document_xml , meta_data 
~~ def get_ordered_list_type ( meta_data , numId , ilvl ) : 
numbering_dict = meta_data . numbering_dict 
if numId not in numbering_dict : 
~~~ return DEFAULT_LIST_NUMBERING_STYLE 
~~ if ilvl not in numbering_dict [ numId ] : 
~~ return meta_data . numbering_dict [ numId ] [ ilvl ] 
~~ def build_list ( li_nodes , meta_data ) : 
ol_dict = { } 
current_ilvl = - 1 
current_numId = - 1 
current_ol = None 
root_ol = None 
visited_nodes = [ ] 
list_contents = [ ] 
def _build_li ( list_contents ) : 
return etree . XML ( '<li>%s</li>' % data ) 
~~ def _build_non_li_content ( el , meta_data ) : 
~~~ w_namespace = get_namespace ( el , 'w' ) 
if el . tag == '%stbl' % w_namespace : 
~~~ new_el , visited_nodes = build_table ( el , meta_data ) 
return etree . tostring ( new_el ) , visited_nodes 
~~ elif el . tag == '%sp' % w_namespace : 
~~~ return get_element_content ( el , meta_data ) , [ el ] 
~~ if has_text ( el ) : 
~~ ~~ def _merge_lists ( ilvl , current_ilvl , ol_dict , current_ol ) : 
~~~ for i in reversed ( range ( ilvl , current_ilvl ) ) : 
~~~ if i not in ol_dict : 
~~ if ol_dict [ i ] is not current_ol : 
~~~ if ol_dict [ i ] is current_ol : 
~~ ol_dict [ i ] [ - 1 ] . append ( current_ol ) 
current_ol = ol_dict [ i ] 
~~ ~~ for key in list ( ol_dict ) : 
~~~ if key > ilvl : 
~~~ del ol_dict [ key ] 
~~ ~~ return current_ol 
~~ for li_node in li_nodes : 
~~~ w_namespace = get_namespace ( li_node , 'w' ) 
if not is_li ( li_node , meta_data ) : 
~~~ new_el , el_visited_nodes = _build_non_li_content ( 
li_node , 
meta_data , 
list_contents . append ( new_el ) 
visited_nodes . extend ( el_visited_nodes ) 
~~ if list_contents : 
~~~ li_el = _build_li ( list_contents ) 
current_ol . append ( li_el ) 
~~ list_contents . append ( get_element_content ( 
ilvl = get_ilvl ( li_node , w_namespace ) 
numId = get_numId ( li_node , w_namespace ) 
list_type = get_ordered_list_type ( meta_data , numId , ilvl ) 
if ( ilvl > current_ilvl ) or ( numId != current_numId ) : 
~~~ ol_dict [ ilvl ] = create_list ( list_type ) 
current_ol = ol_dict [ ilvl ] 
current_ilvl = ilvl 
current_numId = numId 
~~~ current_ol = _merge_lists ( 
ilvl = ilvl , 
current_ilvl = current_ilvl , 
ol_dict = ol_dict , 
current_ol = current_ol , 
~~ if root_ol is None : 
~~~ root_ol = current_ol 
~~ if ilvl in ol_dict : 
~~~ current_ol = ol_dict [ ilvl ] 
~~~ if current_ol is not root_ol : 
~~~ root_ol [ - 1 ] . append ( current_ol ) 
current_ol = create_list ( list_type ) 
~~ ~~ visited_nodes . extend ( list ( li_node . iter ( ) ) ) 
~~ current_ol = _merge_lists ( 
ilvl = 0 , 
return root_ol , visited_nodes 
~~ def build_tr ( tr , meta_data , row_spans ) : 
tr_el = etree . Element ( 'tr' ) 
w_namespace = get_namespace ( tr , 'w' ) 
for el in tr : 
~~~ if el in visited_nodes : 
~~ visited_nodes . append ( el ) 
if el . tag == '%stc' % w_namespace : 
~~~ v_merge = get_v_merge ( el ) 
v_merge is not None and 
v_merge . get ( '%sval' % w_namespace ) != 'restart' ) : 
~~ texts = [ ] 
for td_content in el : 
~~~ if td_content in visited_nodes : 
~~ if is_li ( td_content , meta_data ) : 
~~~ li_nodes = get_single_list_nodes_data ( 
td_content , 
list_el , list_visited_nodes = build_list ( 
li_nodes , 
visited_nodes . extend ( list_visited_nodes ) 
texts . append ( etree . tostring ( list_el ) ) 
~~ elif td_content . tag == '%stbl' % w_namespace : 
~~~ table_el , table_visited_nodes = build_table ( 
visited_nodes . extend ( table_visited_nodes ) 
texts . append ( etree . tostring ( table_el ) ) 
~~ elif td_content . tag == '%stcPr' % w_namespace : 
~~~ visited_nodes . append ( td_content ) 
~~~ text = get_element_content ( 
is_td = True , 
texts . append ( text ) 
td_el = etree . XML ( '<td>%s</td>' % data ) 
colspan = get_grid_span ( el ) 
if colspan > 1 : 
~~~ td_el . set ( 'colspan' , '%d' % colspan ) 
~~ v_merge = get_v_merge ( el ) 
v_merge . get ( '%sval' % w_namespace ) == 'restart' ) : 
~~~ rowspan = next ( row_spans ) 
td_el . set ( 'rowspan' , '%d' % rowspan ) 
~~ tr_el . append ( td_el ) 
~~ ~~ return tr_el 
~~ def build_table ( table , meta_data ) : 
table_el = etree . Element ( 'table' ) 
w_namespace = get_namespace ( table , 'w' ) 
row_spans = get_rowspan_data ( table ) 
for el in table : 
~~~ if el . tag == '%str' % w_namespace : 
~~~ tr_el = build_tr ( 
el , 
row_spans , 
table_el . append ( tr_el ) 
~~ ~~ visited_nodes = list ( table . iter ( ) ) 
return table_el , visited_nodes 
~~ def get_t_tag_content ( 
t , parent , remove_bold , remove_italics , meta_data ) : 
if t is None or t . text is None : 
~~~ return '' 
~~ text = cgi . escape ( t . text ) 
el_is_bold = not remove_bold and ( 
is_bold ( parent ) or 
is_underlined ( parent ) 
el_is_italics = not remove_italics and is_italics ( parent ) 
if el_is_bold : 
~~~ text = '<strong>%s</strong>' % text 
~~ if el_is_italics : 
~~~ text = '<em>%s</em>' % text 
~~ return text 
~~ def get_element_content ( 
p , 
is_td = False , 
remove_italics = False , 
remove_bold = False , 
if not is_td and is_header ( p , meta_data ) : 
~~~ remove_bold , remove_italics = whole_line_styled ( p ) 
~~ p_text = '' 
if len ( p ) == 0 : 
~~ content_tags = ( 
'%sr' % w_namespace , 
'%shyperlink' % w_namespace , 
'%sins' % w_namespace , 
'%ssmartTag' % w_namespace , 
elements_with_content = [ ] 
for child in p : 
~~~ if child is None : 
~~ if child . tag in content_tags : 
~~~ elements_with_content . append ( child ) 
~~ ~~ for el in elements_with_content : 
~~~ if el . tag in ( '%sins' % w_namespace , '%ssmartTag' % w_namespace ) : 
~~~ p_text += get_element_content ( 
remove_bold = remove_bold , 
remove_italics = remove_italics , 
~~ elif el . tag == '%shyperlink' % w_namespace : 
~~~ p_text += build_hyperlink ( el , meta_data ) 
~~ elif el . tag == '%sr' % w_namespace : 
~~~ p_text += get_text_run_content ( 
~~~ raise SyntaxNotSupported ( 
\ % el . tag 
~~ ~~ return p_text 
~~ def _strip_tag ( tree , tag ) : 
~~~ if el . tag == tag : 
~~~ el . getparent ( ) . remove ( el ) 
~~ ~~ ~~ def convert ( file_path , image_handler = None , fall_back = None , converter = None ) : 
file_base , extension = os . path . splitext ( os . path . basename ( file_path ) ) 
if extension == '.html' or extension == '.htm' : 
~~~ return read_html_file ( file_path ) 
~~ docx_path = replace_ext ( file_path , '.docx' ) 
if extension == '.docx' : 
~~~ docx_path = file_path 
~~~ if converter is None : 
~~ converter ( docx_path , file_path ) 
if not os . path . isfile ( docx_path ) : 
~~~ if fall_back is None : 
~~~ return fall_back ( file_path ) 
~~ ~~ ~~ try : 
~~~ zf = get_zip_file_handler ( docx_path ) 
~~ except BadZipfile : 
~~ tree , meta_data = _get_document_data ( zf , image_handler ) 
return create_html ( tree , meta_data ) 
~~ def get_cumulative_data ( self ) : 
sets = map ( itemgetter ( 'data' ) , self . data ) 
if not sets : 
~~ sum = sets . pop ( 0 ) 
yield sum 
while sets : 
~~~ sum = map ( add , sets . pop ( 0 ) ) 
~~ ~~ def get_single_axis_values ( self , axis , dataset ) : 
data_index = getattr ( self , '%s_data_index' % axis ) 
return [ p [ data_index ] for p in dataset [ 'data' ] ] 
~~ def __draw_constant_line ( self , value_label_style ) : 
value , label , style = value_label_style 
start = self . transform_output_coordinates ( ( 0 , value ) ) [ 1 ] 
stop = self . graph_width 
path = etree . SubElement ( self . graph , 'path' , { 
'class' : 'constantLine' } ) 
if style : 
~~~ path . set ( 'style' , style ) 
~~ text = etree . SubElement ( self . graph , 'text' , { 
'x' : str ( 2 ) , 
'y' : str ( start - 2 ) , 
text . text = label 
~~ def load_transform_parameters ( self ) : 
x_min , x_max , x_div = self . x_range ( ) 
y_min , y_max , y_div = self . y_range ( ) 
x_step = ( float ( self . graph_width ) - self . font_size * 2 ) / ( x_max - x_min ) 
y_step = ( float ( self . graph_height ) - self . font_size * 2 ) / ( y_max - y_min ) 
self . __transform_parameters = dict ( locals ( ) ) 
del self . __transform_parameters [ 'self' ] 
~~ def reverse_mapping ( mapping ) : 
keys , values = zip ( * mapping . items ( ) ) 
return dict ( zip ( values , keys ) ) 
~~ def flatten_mapping ( mapping ) : 
key : value 
for keys , value in mapping . items ( ) 
for key in always_iterable ( keys ) 
~~ def float_range ( start = 0 , stop = None , step = 1 ) : 
start = float ( start ) 
while start < stop : 
~~~ yield start 
start += step 
~~ ~~ def add_data ( self , data_descriptor ) : 
pairs = itertools . zip_longest ( self . data , data_descriptor [ 'data' ] ) 
self . data = list ( itertools . starmap ( robust_add , pairs ) ) 
~~ def add_defs ( self , defs ) : 
etree . SubElement ( 
defs , 
'filter' , 
id = 'dropshadow' , 
width = '1.2' , 
height = '1.2' , 
'feGaussianBlur' , 
stdDeviation = '4' , 
result = 'blur' , 
~~ def add_data ( self , conf ) : 
self . validate_data ( conf ) 
self . process_data ( conf ) 
self . data . append ( conf ) 
~~ def burn ( self ) : 
if not self . data : 
~~ if hasattr ( self , 'calculations' ) : 
~~~ self . calculations ( ) 
~~ self . start_svg ( ) 
self . calculate_graph_dimensions ( ) 
self . foreground = etree . Element ( "g" ) 
self . draw_graph ( ) 
self . draw_titles ( ) 
self . draw_legend ( ) 
self . draw_data ( ) 
self . graph . append ( self . foreground ) 
self . render_inline_styles ( ) 
return self . render ( self . root ) 
~~ def calculate_left_margin ( self ) : 
bl = 7 
if self . rotate_y_labels : 
~~~ max_y_label_height_px = self . y_label_font_size 
~~~ label_lengths = map ( len , self . get_y_labels ( ) ) 
max_y_label_len = max ( label_lengths ) 
max_y_label_height_px = 0.6 * max_y_label_len * self . y_label_font_size 
~~ if self . show_y_labels : 
~~~ bl += max_y_label_height_px 
~~ if self . stagger_y_labels : 
~~~ bl += max_y_label_height_px + 10 
~~ if self . show_y_title : 
~~~ bl += self . y_title_font_size + 5 
~~ self . border_left = bl 
~~ def calculate_right_margin ( self ) : 
br = 7 
if self . key and self . key_position == 'right' : 
~~~ max_key_len = max ( map ( len , self . keys ( ) ) ) 
br += max_key_len * self . key_font_size * 0.6 
br += self . KEY_BOX_SIZE 
~~ self . border_right = br 
~~ def calculate_top_margin ( self ) : 
self . border_top = 5 
if self . show_graph_title : 
~~~ self . border_top += self . title_font_size 
~~ self . border_top += 5 
if self . show_graph_subtitle : 
~~~ self . border_top += self . subtitle_font_size 
~~ ~~ def add_popup ( self , x , y , label ) : 
txt_width = len ( label ) * self . font_size * 0.6 + 10 
tx = x + [ 5 , - 5 ] [ int ( x + txt_width > self . width ) ] 
anchor = [ 'start' , 'end' ] [ x + txt_width > self . width ] 
id = 'label-%s' % self . _w3c_name ( label ) 
attrs = { 
'x' : str ( tx ) , 
'y' : str ( y - self . font_size ) , 
'visibility' : 'hidden' , 
'style' : style , 
'text' : label , 
'id' : id , 
etree . SubElement ( self . foreground , 'text' , attrs ) 
vis_tmpl = ( 
"document.getElementById(\ 
'cx' : str ( x ) , 
'cy' : str ( y ) , 
'r' : str ( 10 ) , 
'onmouseover' : vis_tmpl . format ( val = 'visible' , id = id ) , 
'onmouseout' : vis_tmpl . format ( val = 'hidden' , id = id ) , 
etree . SubElement ( self . foreground , 'circle' , attrs ) 
~~ def calculate_bottom_margin ( self ) : 
bb = 7 
if self . key and self . key_position == 'bottom' : 
~~~ bb += len ( self . data ) * ( self . font_size + 5 ) 
bb += 10 
~~ if self . show_x_labels : 
~~~ max_x_label_height_px = self . x_label_font_size 
if self . rotate_x_labels : 
~~~ label_lengths = map ( len , self . get_x_labels ( ) ) 
max_x_label_len = functools . reduce ( max , label_lengths ) 
max_x_label_height_px *= 0.6 * max_x_label_len 
~~ bb += max_x_label_height_px 
if self . stagger_x_labels : 
~~~ bb += max_x_label_height_px + 10 
~~ ~~ if self . show_x_title : 
~~~ bb += self . x_title_font_size + 5 
~~ self . border_bottom = bb 
~~ def draw_graph ( self ) : 
self . graph = etree . SubElement ( self . root , 'g' , transform = transform ) 
etree . SubElement ( self . graph , 'rect' , { 
'x' : '0' , 
'y' : '0' , 
'width' : str ( self . graph_width ) , 
'height' : str ( self . graph_height ) , 
'class' : 'graphBackground' 
etree . SubElement ( self . graph , 'path' , { 
'class' : 'axis' , 
'id' : 'xAxis' 
'id' : 'yAxis' 
self . draw_x_labels ( ) 
self . draw_y_labels ( ) 
~~ def make_datapoint_text ( self , x , y , value , style = None ) : 
if not self . show_data_values : 
~~ e = etree . SubElement ( self . foreground , 'text' , { 
'x' : str ( x ) , 
'y' : str ( y ) , 
'class' : 'dataPointLabel' , 
e . text = str ( value ) 
e = etree . SubElement ( self . foreground , 'text' , { 
'class' : 'dataPointLabel' } ) 
~~~ e . set ( 'style' , style ) 
~~ ~~ def draw_x_labels ( self ) : 
if self . show_x_labels : 
~~~ labels = self . get_x_labels ( ) 
count = len ( labels ) 
labels = enumerate ( iter ( labels ) ) 
start = int ( not self . step_include_first_x_label ) 
labels = itertools . islice ( labels , start , None , self . step_x_labels ) 
list ( map ( self . draw_x_label , labels ) ) 
self . draw_x_guidelines ( self . field_width ( ) , count ) 
~~ ~~ def draw_y_labels ( self ) : 
if not self . show_y_labels : 
~~ labels = self . get_y_labels ( ) 
start = int ( not self . step_include_first_y_label ) 
labels = itertools . islice ( labels , start , None , self . step_y_labels ) 
list ( map ( self . draw_y_label , labels ) ) 
self . draw_y_guidelines ( self . field_height ( ) , count ) 
~~ def draw_x_guidelines ( self , label_height , count ) : 
if not self . show_x_guidelines : 
~~ for count in range ( 1 , count ) : 
start = label_height * count , 
stop = self . graph_height , 
path = { 'd' : move , 'class' : 'guideLines' } 
etree . SubElement ( self . graph , 'path' , path ) 
~~ ~~ def draw_y_guidelines ( self , label_height , count ) : 
if not self . show_y_guidelines : 
start = self . graph_height - label_height * count , 
stop = self . graph_width , 
~~ ~~ def draw_titles ( self ) : 
~~~ self . draw_graph_title ( ) 
~~ if self . show_graph_subtitle : 
~~~ self . draw_graph_subtitle ( ) 
~~ if self . show_x_title : 
~~~ self . draw_x_title ( ) 
~~~ self . draw_y_title ( ) 
~~ ~~ def render_inline_styles ( self ) : 
if not self . css_inline : 
~~ styles = self . parse_css ( ) 
for node in self . root . xpath ( '//*[@class]' ) : 
~~~ cl = '.' + node . attrib [ 'class' ] 
if cl not in styles : 
~~ style = styles [ cl ] 
if 'style' in node . attrib : 
~~~ style += node . attrib [ 'style' ] 
~~ node . attrib [ 'style' ] = style 
~~ ~~ def parse_css ( self ) : 
cssutils . ser . prefs . useMinified ( ) 
pairs = ( 
( r . selectorText , r . style . cssText ) 
for r in self . get_stylesheet ( ) 
if not isinstance ( r , cssutils . css . CSSComment ) 
return dict ( pairs ) 
~~ def start_svg ( self ) : 
SVG_NAMESPACE = 'http://www.w3.org/2000/svg' 
SVG = '{%s}' % SVG_NAMESPACE 
NSMAP = { 
None : SVG_NAMESPACE , 
'xlink' : 'http://www.w3.org/1999/xlink' , 
'a3' : 'http://ns.adobe.com/AdobeSVGViewerExtensions/3.0/' , 
root_attrs = self . _get_root_attributes ( ) 
self . root = etree . Element ( SVG + "svg" , attrib = root_attrs , nsmap = NSMAP ) 
if hasattr ( self , 'style_sheet_href' ) : 
~~~ pi = etree . ProcessingInstruction ( 
'xml-stylesheet' , 
\ % self . style_sheet_href 
self . root . addprevious ( pi ) 
~~ comment_strings = ( 
list ( map ( self . root . append , map ( etree . Comment , comment_strings ) ) ) 
defs = etree . SubElement ( self . root , 'defs' ) 
self . add_defs ( defs ) 
if not hasattr ( self , 'style_sheet_href' ) and not self . css_inline : 
~~~ self . root . append ( etree . Comment ( 
style = etree . SubElement ( defs , 'style' , type = 'text/css' ) 
style . text = self . get_stylesheet ( ) . cssText 
etree . SubElement ( self . root , 'rect' , { 
'width' : str ( self . width ) , 
'height' : str ( self . height ) , 
'class' : 'svgBackground' } ) 
~~ def get_stylesheet_resources ( self ) : 
class_vars = class_dict ( self ) 
loader = functools . partial ( 
self . load_resource_stylesheet , 
subs = class_vars ) 
sheets = list ( map ( loader , self . stylesheet_names ) ) 
return sheets 
~~ def executor ( self , max_workers = 1 ) : 
cls = self . __class__ 
if cls . _executor is None : 
~~~ cls . _executor = ThreadPoolExecutor ( max_workers ) 
~~ return cls . _executor 
~~ def client ( self ) : 
if cls . _client is None : 
if self . tls_config : 
~~~ kwargs [ 'tls' ] = docker . tls . TLSConfig ( ** self . tls_config ) 
~~ kwargs . update ( kwargs_from_env ( ) ) 
client = docker . APIClient ( version = 'auto' , ** kwargs ) 
cls . _client = client 
~~ return cls . _client 
~~ def tls_client ( self ) : 
if self . tls_cert and self . tls_key : 
~~~ return ( self . tls_cert , self . tls_key ) 
~~ def service_name ( self ) : 
if hasattr ( self , "server_name" ) and self . server_name : 
~~~ server_name = self . server_name 
~~~ server_name = 1 
~~ return "{}-{}-{}" . format ( self . service_prefix , 
self . service_owner , 
server_name 
~~ def _docker ( self , method , * args , ** kwargs ) : 
m = getattr ( self . client , method ) 
return m ( * args , ** kwargs ) 
~~ def docker ( self , method , * args , ** kwargs ) : 
return self . executor . submit ( self . _docker , method , * args , ** kwargs ) 
~~ def poll ( self ) : 
service = yield self . get_service ( ) 
if not service : 
return 0 
~~ task_filter = { 'service' : service [ 'Spec' ] [ 'Name' ] } 
tasks = yield self . docker ( 
'tasks' , task_filter 
running_task = None 
for task in tasks : 
~~~ task_state = task [ 'Status' ] [ 'State' ] 
self . log . debug ( 
task [ 'ID' ] [ : 7 ] , 
self . service_id [ : 7 ] , 
pformat ( task_state ) , 
if task_state == 'running' : 
~~~ running_task = task 
~~ ~~ if running_task is not None : 
if self . use_user_options : 
~~~ user_options = self . user_options 
~~~ user_options = { } 
if service is None : 
~~~ if 'name' in user_options : 
~~~ self . server_name = user_options [ 'name' ] 
~~ if hasattr ( self , 'container_spec' ) and self . container_spec is not None : 
~~~ container_spec = dict ( ** self . container_spec ) 
~~ elif user_options == { } : 
~~ container_spec . update ( user_options . get ( 'container_spec' , { } ) ) 
container_spec [ 'mounts' ] = [ ] 
for mount in self . container_spec [ 'mounts' ] : 
~~~ m = dict ( ** mount ) 
if 'source' in m : 
~~~ m [ 'source' ] = m [ 'source' ] . format ( 
username = self . service_owner ) 
~~ if 'driver_config' in m : 
~~~ device = m [ 'driver_config' ] [ 'options' ] [ 'device' ] . format ( 
username = self . service_owner 
m [ 'driver_config' ] [ 'options' ] [ 'device' ] = device 
m [ 'driver_config' ] = docker . types . DriverConfig ( 
** m [ 'driver_config' ] ) 
~~ container_spec [ 'mounts' ] . append ( docker . types . Mount ( ** m ) ) 
~~ container_spec [ 'env' ] = self . get_env ( ) 
if hasattr ( self , 'resource_spec' ) : 
~~~ resource_spec = self . resource_spec 
~~ resource_spec . update ( user_options . get ( 'resource_spec' , { } ) ) 
if hasattr ( self , 'networks' ) : 
~~~ networks = self . networks 
~~ if user_options . get ( 'networks' ) is not None : 
~~~ networks = user_options . get ( 'networks' ) 
~~ if hasattr ( self , 'placement' ) : 
~~~ placement = self . placement 
~~ if user_options . get ( 'placement' ) is not None : 
~~~ placement = user_options . get ( 'placement' ) 
~~ image = container_spec [ 'Image' ] 
del container_spec [ 'Image' ] 
container_spec = docker . types . ContainerSpec ( 
image , ** container_spec ) 
resources = docker . types . Resources ( ** resource_spec ) 
task_spec = { 'container_spec' : container_spec , 
'resources' : resources , 
'placement' : placement 
task_tmpl = docker . types . TaskTemplate ( ** task_spec ) 
resp = yield self . docker ( 'create_service' , 
task_tmpl , 
name = self . service_name , 
networks = networks ) 
self . service_id = resp [ 'ID' ] 
self . log . info ( 
self . service_name , self . service_id [ : 7 ] , image ) 
~~~ self . log . info ( 
self . service_name , self . service_id [ : 7 ] ) 
envs = service [ 'Spec' ] [ 'TaskTemplate' ] [ 'ContainerSpec' ] [ 'Env' ] 
for line in envs : 
~~~ if line . startswith ( 'JPY_API_TOKEN=' ) : 
~~~ self . api_token = line . split ( '=' , 1 ) [ 1 ] 
~~ ~~ ~~ ip = self . service_name 
port = self . service_port 
return ( ip , port ) 
~~ def stop ( self , now = False ) : 
yield self . docker ( 'remove_service' , self . service_id [ : 7 ] ) 
self . clear_state ( ) 
~~ def notebook_show ( obj , doc , comm ) : 
target = obj . ref [ 'id' ] 
load_mime = 'application/vnd.holoviews_load.v0+json' 
exec_mime = 'application/vnd.holoviews_exec.v0+json' 
bokeh_script , bokeh_div , _ = bokeh . embed . notebook . notebook_content ( obj , comm . id ) 
publish_display_data ( data = { 'text/html' : encode_utf8 ( bokeh_div ) } ) 
JS = '\\n' . join ( [ PYVIZ_PROXY , JupyterCommManager . js_manager ] ) 
publish_display_data ( data = { load_mime : JS , 'application/javascript' : JS } ) 
msg_handler = bokeh_msg_handler . format ( plot_id = target ) 
comm_js = comm . js_template . format ( plot_id = target , comm_id = comm . id , msg_handler = msg_handler ) 
bokeh_js = '\\n' . join ( [ comm_js , bokeh_script ] ) 
publish_display_data ( data = { exec_mime : '' , 'text/html' : '' , 
'application/javascript' : bokeh_js } , 
metadata = { exec_mime : { 'id' : target } } ) 
~~ def process_hv_plots ( widgets , plots ) : 
bokeh_plots = [ ] 
for plot in plots : 
~~~ if hasattr ( plot , '_update_callbacks' ) : 
~~~ for subplot in plot . traverse ( lambda x : x ) : 
~~~ subplot . comm = widgets . server_comm 
for cb in subplot . callbacks : 
~~~ for c in cb . callbacks : 
~~~ c . code = c . code . replace ( plot . id , widgets . plot_id ) 
~~ ~~ ~~ plot = plot . state 
~~ bokeh_plots . append ( plot ) 
~~ return bokeh_plots 
~~ def _get_customjs ( self , change , p_name ) : 
fetch_data = data_template . format ( change = change , p_name = p_name ) 
self_callback = JS_CALLBACK . format ( comm_id = self . comm . id , 
timeout = self . timeout , 
debounce = self . debounce , 
plot_id = self . plot_id ) 
js_callback = CustomJS ( code = '\\n' . join ( [ fetch_data , 
self_callback ] ) ) 
return js_callback 
~~ def widget ( self , param_name ) : 
if param_name not in self . _widgets : 
~~~ self . _widgets [ param_name ] = self . _make_widget ( param_name ) 
~~ return self . _widgets [ param_name ] 
~~ def widgets ( self ) : 
params = self . parameterized . params ( ) . items ( ) 
key_fn = lambda x : x [ 1 ] . precedence if x [ 1 ] . precedence is not None else self . p . default_precedence 
sorted_precedence = sorted ( params , key = key_fn ) 
outputs = [ k for k , p in sorted_precedence if isinstance ( p , _View ) ] 
filtered = [ ( k , p ) for ( k , p ) in sorted_precedence 
if ( ( p . precedence is None ) or ( p . precedence >= self . p . display_threshold ) ) 
and k not in outputs ] 
groups = itertools . groupby ( filtered , key = key_fn ) 
sorted_groups = [ sorted ( grp ) for ( k , grp ) in groups ] 
ordered_params = [ el [ 0 ] for group in sorted_groups for el in group ] 
ordered_params . pop ( ordered_params . index ( 'name' ) ) 
widgets = [ Div ( text = '<b>{0}</b>' . format ( self . parameterized . name ) ) ] 
def format_name ( pname ) : 
~~~ p = self . parameterized . params ( pname ) 
name = "" if issubclass ( type ( p ) , param . Action ) else pname 
return Div ( text = name ) 
~~ if self . p . show_labels : 
~~~ widgets += [ self . widget ( pname ) for pname in ordered_params ] 
~~ if self . p . button and not ( self . p . callback is None and self . p . next_n == 0 ) : 
~~~ display_button = Button ( label = self . p . button_text ) 
def click_cb ( ) : 
~~~ self . execute ( self . _changed ) 
~~~ self . _changed . clear ( ) 
~~ self . _changed . clear ( ) 
~~ display_button . on_click ( click_cb ) 
widgets . append ( display_button ) 
~~ outputs = [ self . widget ( pname ) for pname in outputs ] 
return widgets , outputs 
~~ def render_function ( obj , view ) : 
~~~ import holoviews as hv 
~~~ hv = None 
~~ if hv and isinstance ( obj , hv . core . Dimensioned ) : 
~~~ renderer = hv . renderer ( 'bokeh' ) 
if not view . _notebook : 
~~~ renderer = renderer . instance ( mode = 'server' ) 
~~ plot = renderer . get_plot ( obj , doc = view . _document ) 
if view . _notebook : 
~~~ plot . comm = view . _comm 
~~ plot . document = view . _document 
return plot . state 
~~ return obj 
~~ def TextWidget ( * args , ** kw ) : 
kw [ 'value' ] = str ( kw [ 'value' ] ) 
kw . pop ( 'options' , None ) 
return TextInput ( * args , ** kw ) 
~~ def named_objs ( objlist ) : 
objs = [ ] 
for k , obj in objlist : 
~~~ if hasattr ( k , '__name__' ) : 
~~~ k = k . __name__ 
~~~ k = as_unicode ( k ) 
~~ objs . append ( ( k , obj ) ) 
~~ return objs 
~~ def get_method_owner ( meth ) : 
if inspect . ismethod ( meth ) : 
~~~ if sys . version_info < ( 3 , 0 ) : 
~~~ return meth . im_class if meth . im_self is None else meth . im_self 
~~~ return meth . __self__ 
~~ ~~ ~~ def _assign_auth_values ( self , http_auth ) : 
if not http_auth : 
~~ elif isinstance ( http_auth , ( tuple , list ) ) : 
~~~ self . _auth_user , self . _auth_password = http_auth 
~~ elif isinstance ( http_auth , str ) : 
~~~ self . _auth_user , self . _auth_password = http_auth . split ( ':' ) 
~~ ~~ def ping ( self , params = None ) : 
~~~ self . transport . perform_request ( 'HEAD' , '/' , params = params ) 
~~ except TransportError : 
~~~ raise gen . Return ( False ) 
~~ raise gen . Return ( True ) 
~~ def info ( self , params = None ) : 
_ , data = yield self . transport . perform_request ( 'GET' , '/' , 
params = params ) 
raise gen . Return ( data ) 
~~ def health ( self , params = None ) : 
status , data = yield self . transport . perform_request ( 
"GET" , "/_cluster/health" , params = params ) 
raise gen . Return ( ( status , data ) ) 
~~ def create ( self , index , doc_type , body , id = None , params = None ) : 
result = yield self . index ( index , doc_type , body , id = id , params = params , 
op_type = 'create' ) 
raise gen . Return ( result ) 
~~ def index ( self , index , doc_type , body , id = None , params = None ) : 
_ , data = yield self . transport . perform_request ( 
'PUT' if id else 'POST' , _make_path ( index , doc_type , id ) , 
params = params , body = body ) 
~~ def exists ( self , index , id , doc_type = '_all' , params = None ) : 
~~~ self . transport . perform_request ( 
'HEAD' , _make_path ( index , doc_type , id ) , params = params ) 
~~ except exceptions . NotFoundError : 
~~~ return gen . Return ( False ) 
~~ def get_alias ( self , index = None , name = None , params = None ) : 
_ , result = yield self . transport . perform_request ( 
'GET' , _make_path ( index , '_alias' , name ) , params = params ) 
~~ def search ( self , index = None , doc_type = None , body = None , params = None ) : 
if 'from_' in params : 
~~~ params [ 'from' ] = params . pop ( 'from_' ) 
~~ if doc_type and not index : 
~~~ index = '_all' 
~~ _ , data = yield self . transport . perform_request ( 'GET' , 
_make_path ( index , 
doc_type , 
'_search' ) , 
body = body ) 
~~ def scroll ( self , scroll_id , scroll , params = None ) : 
"scroll" : scroll , 
"scroll_id" : scroll_id 
if params : 
~~~ if "scroll" in params . keys ( ) : 
~~~ params . pop ( "scroll" ) 
~~ if "scroll_id" in params . keys ( ) : 
~~~ params . pop ( "scroll_id" ) 
~~ ~~ _ , data = yield self . transport . perform_request ( 'POST' , 
_make_path ( '_search' , 
'scroll' ) , 
~~ def clear_scroll ( self , scroll_id , params = None ) : 
if not isinstance ( scroll_id , list ) : 
~~~ scroll_id = [ scroll_id ] 
if params and "scroll_id" in params . keys ( ) : 
~~ _ , data = yield self . transport . perform_request ( 'DELETE' , 
~~ def get_mapping ( self , index = None , doc_type = None , params = None ) : 
_ , data = yield self . transport . perform_request ( 'GET' , 
'_mapping' , 
doc_type ) , 
~~ def suggest ( self , index = None , body = None , params = None ) : 
_ , data = yield self . transport . perform_request ( 'POST' , 
'_suggest' ) , 
~~ def with_apps ( * apps ) : 
apps_set = set ( settings . INSTALLED_APPS ) 
apps_set . update ( apps ) 
return override_settings ( INSTALLED_APPS = list ( apps_set ) ) 
~~ def without_apps ( * apps ) : 
apps_list = [ a for a in settings . INSTALLED_APPS if a not in apps ] 
return override_settings ( INSTALLED_APPS = apps_list ) 
~~ def get_global_settings ( self ) : 
return dict ( ( key , getattr ( global_settings , key ) ) for key in dir ( global_settings ) 
if key . isupper ( ) ) 
~~ def get_organisation_information ( self , query_params = None ) : 
return self . fetch_json ( 
uri_path = self . base_uri , 
query_params = query_params or { } 
~~ def get_boards ( self , ** query_params ) : 
boards = self . get_boards_json ( self . base_uri , query_params = query_params ) 
boards_list = [ ] 
for board_json in boards : 
~~~ boards_list . append ( self . create_board ( board_json ) ) 
~~ return boards_list 
~~ def get_members ( self , ** query_params ) : 
members = self . get_members_json ( self . base_uri , 
query_params = query_params ) 
members_list = [ ] 
for member_json in members : 
~~~ members_list . append ( self . create_member ( member_json ) ) 
~~ return members_list 
~~ def update_organisation ( self , query_params = None ) : 
organisation_json = self . fetch_json ( 
http_method = 'PUT' , 
return self . create_organisation ( organisation_json ) 
~~ def remove_member ( self , member_id ) : 
uri_path = self . base_uri + '/members/%s' % member_id , 
http_method = 'DELETE' 
~~ def add_member_by_id ( self , member_id , membership_type = 'normal' ) : 
query_params = { 
'type' : membership_type 
~~ def add_member ( self , email , fullname , membership_type = 'normal' ) : 
uri_path = self . base_uri + '/members' , 
'email' : email , 
'fullName' : fullname , 
~~ def get_list_information ( self , query_params = None ) : 
~~ def add_card ( self , query_params = None ) : 
card_json = self . fetch_json ( 
uri_path = self . base_uri + '/cards' , 
http_method = 'POST' , 
return self . create_card ( card_json ) 
~~ def get_label_information ( self , query_params = None ) : 
~~ def get_items ( self , query_params = None ) : 
uri_path = self . base_uri + '/checkItems' , 
~~ def _update_label_name ( self , name ) : 
label_json = self . fetch_json ( 
query_params = { 'name' : name } 
return self . create_label ( label_json ) 
~~ def _update_label_dict ( self , query_params = { } ) : 
query_params = query_params 
~~ def get_authorisation_url ( self , application_name , token_expire = '1day' ) : 
'name' : application_name , 
'expiration' : token_expire , 
'response_type' : 'token' , 
'scope' : 'read,write' 
authorisation_url = self . build_uri ( 
path = '/authorize' , 
query_params = self . add_authorisation ( query_params ) 
'token:\\n' , authorisation_url ) 
return authorisation_url 
~~ def get_card_information ( self , query_params = None ) : 
~~ def get_board ( self , ** query_params ) : 
board_json = self . get_board_json ( self . base_uri , 
return self . create_board ( board_json ) 
~~ def get_list ( self , ** query_params ) : 
list_json = self . get_list_json ( self . base_uri , 
return self . create_list ( list_json ) 
~~ def get_checklists ( self , ** query_params ) : 
checklists = self . get_checklist_json ( self . base_uri , 
checklists_list = [ ] 
for checklist_json in checklists : 
~~~ checklists_list . append ( self . create_checklist ( checklist_json ) ) 
~~ return checklists_list 
~~ def add_comment ( self , comment_text ) : 
uri_path = self . base_uri + '/actions/comments' , 
query_params = { 'text' : comment_text } 
~~ def add_attachment ( self , filename , open_file ) : 
fields = { 
'api_key' : self . client . api_key , 
'token' : self . client . user_auth_token 
content_type , body = self . encode_multipart_formdata ( 
fields = fields , 
file_values = open_file 
uri_path = self . base_uri + '/attachments' , 
headers = { 'Content-Type' : content_type } , 
~~ def add_checklist ( self , query_params = None ) : 
checklist_json = self . fetch_json ( 
uri_path = self . base_uri + '/checklists' , 
return self . create_checklist ( checklist_json ) 
~~ def _add_label_from_dict ( self , query_params = None ) : 
uri_path = self . base_uri + '/labels' , 
~~ def _add_label_from_class ( self , label = None ) : 
uri_path = self . base_uri + '/idLabels' , 
query_params = { 'value' : label . id } 
~~ def add_member ( self , member_id ) : 
members = self . fetch_json ( 
uri_path = self . base_uri + '/idMembers' , 
query_params = { 'value' : member_id } 
~~ def encode_multipart_formdata ( self , fields , filename , file_values ) : 
boundary = '----------Trello_Boundary_$' 
crlf = '\\r\\n' 
data = [ ] 
for key in fields : 
~~~ data . append ( '--' + boundary ) 
data . append ( \ % key ) 
data . append ( '' ) 
data . append ( fields [ key ] ) 
~~ data . append ( '--' + boundary ) 
data . append ( 
\ % 
data . append ( file_values ) 
data . append ( '--' + boundary + '--' ) 
data = [ str ( segment ) for segment in data ] 
body = crlf . join ( data ) 
return content_type , body 
~~ def get_member_information ( self , query_params = None ) : 
~~ def get_cards ( self , ** query_params ) : 
cards = self . get_cards_json ( self . base_uri , query_params = query_params ) 
cards_list = [ ] 
for card_json in cards : 
~~~ cards_list . append ( self . create_card ( card_json ) ) 
~~ return cards_list 
~~ def get_organisations ( self , ** query_params ) : 
organisations = self . get_organisations_json ( self . base_uri , 
organisations_list = [ ] 
for organisation_json in organisations : 
~~~ organisations_list . append ( 
self . create_organisation ( organisation_json ) ) 
~~ return organisations_list 
~~ def create_new_board ( self , query_params = None ) : 
board_json = self . fetch_json ( 
uri_path = '/boards' , 
~~ def singledispatchmethod ( method ) : 
dispatcher = singledispatch ( method ) 
def wrapper ( * args , ** kw ) : 
~~~ return dispatcher . dispatch ( args [ 1 ] . __class__ ) ( * args , ** kw ) 
~~ wrapper . register = dispatcher . register 
update_wrapper ( wrapper , dispatcher ) 
return wrapper 
~~ def create_checklist_item ( self , card_id , checklist_id , checklistitem_json , ** kwargs ) : 
return self . client . create_checklist_item ( card_id , checklist_id , checklistitem_json , ** kwargs ) 
~~ def get_board_information ( self , query_params = None ) : 
uri_path = '/boards/' + self . id , 
~~ def get_lists ( self , ** query_params ) : 
lists = self . get_lists_json ( self . base_uri , query_params = query_params ) 
lists_list = [ ] 
for list_json in lists : 
~~~ lists_list . append ( self . create_list ( list_json ) ) 
~~ return lists_list 
~~ def get_labels ( self , ** query_params ) : 
labels = self . get_labels_json ( self . base_uri , query_params = query_params ) 
labels_list = [ ] 
for label_json in labels : 
~~~ labels_list . append ( self . create_label ( label_json ) ) 
~~ return labels_list 
~~ def get_card ( self , card_id , ** query_params ) : 
uri_path = self . base_uri + '/cards/' + card_id 
~~ def get_checklists ( self ) : 
checklists = self . getChecklistsJson ( self . base_uri ) 
~~~ checklists_list . append ( self . createChecklist ( checklist_json ) ) 
~~ def get_organisation ( self , ** query_params ) : 
organisation_json = self . get_organisations_json ( 
self . base_uri , query_params = query_params ) 
~~ def update_board ( self , query_params = None ) : 
~~ def add_list ( self , query_params = None ) : 
list_json = self . fetch_json ( 
uri_path = self . base_uri + '/lists' , 
~~ def add_label ( self , query_params = None ) : 
return self . create_label ( list_json ) 
~~ def get_checklist_information ( self , query_params = None ) : 
~~ def get_card ( self ) : 
card_id = self . get_checklist_information ( ) . get ( 'idCard' , None ) 
if card_id : 
~~~ return self . client . get_card ( card_id ) 
~~ ~~ def get_item_objects ( self , query_params = None ) : 
card = self . get_card ( ) 
checklistitems_list = [ ] 
for checklistitem_json in self . get_items ( query_params ) : 
~~~ checklistitems_list . append ( self . create_checklist_item ( card . id , self . id , checklistitem_json ) ) 
~~ return checklistitems_list 
~~ def update_checklist ( self , name ) : 
~~ def add_item ( self , query_params = None ) : 
~~ def remove_item ( self , item_id ) : 
uri_path = self . base_uri + '/checkItems/' + item_id , 
~~ def update_name ( self , name ) : 
checklistitem_json = self . fetch_json ( 
uri_path = self . base_uri + '/name' , 
query_params = { 'value' : name } 
return self . create_checklist_item ( self . idCard , self . idChecklist , checklistitem_json ) 
~~ def update_state ( self , state ) : 
uri_path = self . base_uri + '/state' , 
query_params = { 'value' : 'complete' if state else 'incomplete' } 
~~ def add_authorisation ( self , query_params ) : 
query_params [ 'key' ] = self . api_key 
if self . user_auth_token : 
~~~ query_params [ 'token' ] = self . user_auth_token 
~~ return query_params 
~~ def check_errors ( self , uri , response ) : 
if response . status == 401 : 
~~~ raise trolly . Unauthorised ( uri , response ) 
~~ if response . status != 200 : 
~~~ raise trolly . ResourceUnavailable ( uri , response ) 
~~ ~~ def build_uri ( self , path , query_params ) : 
url = 'https://api.trello.com/1' + self . clean_path ( path ) 
url += '?' + urlencode ( query_params ) 
return url 
~~ def fetch_json ( self , uri_path , http_method = 'GET' , query_params = None , 
body = None , headers = None ) : 
headers = headers or { } 
uri = self . build_uri ( uri_path , query_params ) 
allowed_methods = ( "POST" , "PUT" , "DELETE" ) 
if http_method in allowed_methods and 'Content-Type' not in headers : 
~~~ headers [ 'Content-Type' ] = 'application/json' 
~~ headers [ 'Accept' ] = 'application/json' 
response , content = self . client . request ( 
method = http_method , 
headers = headers 
self . check_errors ( uri , response ) 
return json . loads ( content . decode ( 'utf-8' ) ) 
~~ def create_organisation ( self , organisation_json ) : 
return trolly . organisation . Organisation ( 
trello_client = self , 
organisation_id = organisation_json [ 'id' ] , 
name = organisation_json [ 'name' ] , 
data = organisation_json , 
~~ def create_board ( self , board_json ) : 
return trolly . board . Board ( 
board_id = board_json [ 'id' ] , 
name = board_json [ 'name' ] , 
data = board_json , 
~~ def create_label ( self , label_json ) : 
return trolly . label . Label ( 
label_id = label_json [ 'id' ] , 
name = label_json [ 'name' ] , 
data = label_json , 
~~ def create_list ( self , list_json ) : 
return trolly . list . List ( 
list_id = list_json [ 'id' ] , 
name = list_json [ 'name' ] , 
data = list_json , 
~~ def create_card ( self , card_json ) : 
return trolly . card . Card ( 
card_id = card_json [ 'id' ] , 
name = card_json [ 'name' ] , 
data = card_json , 
~~ def create_checklist ( self , checklist_json ) : 
return trolly . checklist . Checklist ( 
checklist_id = checklist_json [ 'id' ] , 
name = checklist_json [ 'name' ] , 
data = checklist_json , 
~~ def create_checklist_item ( self , card_id , checklist_id , checklistitem_json ) : 
return trolly . checklist . ChecklistItem ( 
card_id = card_id , 
checklist_id = checklist_id , 
checklistitem_id = checklistitem_json [ 'id' ] . encode ( 'utf-8' ) , 
name = checklistitem_json [ 'name' ] . encode ( 'utf-8' ) , 
state = checklistitem_json [ 'state' ] . encode ( 'utf-8' ) 
~~ def create_member ( self , member_json ) : 
return trolly . member . Member ( 
member_id = member_json [ 'id' ] , 
name = member_json [ 'fullName' ] , 
data = member_json , 
~~ def get_organisation ( self , id , name = None ) : 
return self . create_organisation ( dict ( id = id , name = name ) ) 
~~ def get_board ( self , id , name = None ) : 
return self . create_board ( dict ( id = id , name = name ) ) 
~~ def get_list ( self , id , name = None ) : 
return self . create_list ( dict ( id = id , name = name ) ) 
~~ def get_card ( self , id , name = None ) : 
return self . create_card ( dict ( id = id , name = name ) ) 
~~ def get_checklist ( self , id , name = None ) : 
return self . create_checklist ( dict ( id = id , name = name ) ) 
~~ def get_member ( self , id = 'me' , name = None ) : 
return self . create_member ( dict ( id = id , fullName = name ) ) 
~~ def to_raw_text_markupless ( text , keep_whitespace = False , normalize_ascii = True ) : 
return sent_tokenize ( 
remove_dates ( _remove_urls ( text ) ) , 
keep_whitespace , 
normalize_ascii 
~~ def to_raw_text ( text , keep_whitespace = False , normalize_ascii = True ) : 
out = text 
out = _remove_urls ( text ) 
out = _remove_mvar ( out ) 
out = _remove_squiggly_bracket ( out ) 
out = _remove_table ( out ) 
out = _remove_brackets ( out ) 
out = remove_remaining_double_brackets ( out ) 
out = remove_markup ( out ) 
out = remove_wikipedia_link . sub ( anchor_replacer , out ) 
out = remove_bullets_nbsps . sub ( empty_space , out ) 
out = remove_dates ( out ) 
out = remove_math_sections ( out ) 
out = remove_html ( out ) 
out = sent_tokenize ( out , keep_whitespace , normalize_ascii ) 
return out 
~~ def to_raw_text_pairings ( text , keep_whitespace = False , normalize_ascii = True ) : 
for sentence in sent_tokenize ( out , keep_whitespace , normalize_ascii ) : 
~~~ yield sentence 
~~ ~~ def detect_sentence_boundaries ( tokens ) : 
tokenized = group_quoted_tokens ( tokens ) 
words = [ ] 
sentences = [ ] 
for i in range ( len ( tokenized ) ) : 
~~~ end_sentence = False 
if isinstance ( tokenized [ i ] , list ) : 
~~~ if len ( words ) == 0 : 
~~~ if is_end_symbol ( tokenized [ i ] [ - 2 ] . rstrip ( ) ) : 
~~~ end_sentence = True 
~~~ if ( tokenized [ i ] [ 0 ] [ 0 ] == \ and 
is_end_symbol ( tokenized [ i ] [ - 2 ] . rstrip ( ) ) and 
not tokenized [ i ] [ 1 ] [ 0 ] . isupper ( ) ) : 
~~ ~~ words . extend ( tokenized [ i ] ) 
~~~ stripped_tokenized = tokenized [ i ] . rstrip ( ) 
if is_end_symbol ( stripped_tokenized ) : 
~~~ words . append ( tokenized [ i ] ) 
not_last_word = i + 1 != len ( tokenized ) 
next_word_lowercase = ( 
not_last_word and 
tokenized [ i + 1 ] [ 0 ] . islower ( ) 
next_word_continue_punct = ( 
tokenized [ i + 1 ] [ 0 ] in CONTINUE_PUNCT_SYMBOLS 
end_sentence = not ( 
next_word_lowercase or 
next_word_continue_punct 
~~ ~~ if end_sentence : 
~~~ sentences . append ( words ) 
~~ ~~ if len ( words ) > 0 : 
~~ if len ( sentences ) > 0 and sentences [ - 1 ] [ - 1 ] : 
~~~ alpha_word_piece = word_with_alpha_and_period . match ( sentences [ - 1 ] [ - 1 ] ) 
if alpha_word_piece : 
~~~ sentences [ - 1 ] [ - 1 ] = alpha_word_piece . group ( 1 ) 
sentences [ - 1 ] . append ( alpha_word_piece . group ( 2 ) ) 
~~ ~~ return sentences 
~~ def sent_tokenize ( text , keep_whitespace = False , normalize_ascii = True ) : 
sentences = detect_sentence_boundaries ( 
tokenize ( 
text , 
if not keep_whitespace : 
~~~ sentences = remove_whitespace ( sentences ) 
~~ return sentences 
~~ def protect_shorthand ( text , split_locations ) : 
word_matches = list ( re . finditer ( word_with_period , text ) ) 
total_words = len ( word_matches ) 
for i , match in enumerate ( word_matches ) : 
~~~ match_start = match . start ( ) 
match_end = match . end ( ) 
for char_pos in range ( match_start , match_end ) : 
~~~ if split_locations [ char_pos ] == SHOULD_SPLIT and match_end - char_pos > 1 : 
~~~ match_start = char_pos 
~~ ~~ word = text [ match_start : match_end ] 
if not word . endswith ( '.' ) : 
~~~ if ( not word [ 0 ] . isdigit ( ) and 
split_locations [ match_start ] == UNDECIDED ) : 
~~~ split_locations [ match_start ] = SHOULD_SPLIT 
~~ period_pos = match_end - 1 
word_is_in_abbr = word [ : - 1 ] . lower ( ) in ABBR 
is_abbr_like = ( 
word_is_in_abbr or 
one_letter_long_or_repeating . match ( word [ : - 1 ] ) is not None 
is_digit = False if is_abbr_like else word [ : - 1 ] . isdigit ( ) 
is_last_word = i == ( total_words - 1 ) 
is_ending = is_last_word and ( match_end == len ( text ) or text [ match_end : ] . isspace ( ) ) 
is_not_ending = not is_ending 
abbreviation_and_not_end = ( 
len ( word ) > 1 and 
is_abbr_like and 
is_not_ending 
if abbreviation_and_not_end and ( 
( not is_last_word and word_matches [ i + 1 ] . group ( 0 ) [ 0 ] . islower ( ) ) or 
( not is_last_word and word_matches [ i + 1 ] . group ( 0 ) in PUNCT_SYMBOLS ) or 
word [ 0 ] . isupper ( ) or 
len ( word ) == 2 ) : 
~~~ if split_locations [ period_pos ] == SHOULD_SPLIT and period_pos + 1 < len ( split_locations ) : 
~~~ split_locations [ period_pos + 1 ] = SHOULD_SPLIT 
~~ split_locations [ period_pos ] = SHOULD_NOT_SPLIT 
~~ elif ( is_digit and 
len ( word [ : - 1 ] ) <= 2 and 
not is_last_word and 
word_matches [ i + 1 ] . group ( 0 ) . lower ( ) in MONTHS ) : 
~~ elif split_locations [ period_pos ] == UNDECIDED : 
~~~ split_locations [ period_pos ] = SHOULD_SPLIT 
~~ ~~ ~~ def split_with_locations ( text , locations ) : 
for pos , decision in enumerate ( locations ) : 
~~~ if decision == SHOULD_SPLIT : 
~~~ if start != pos : 
~~~ yield text [ start : pos ] 
~~ start = pos 
~~ ~~ if start != len ( text ) : 
~~~ yield text [ start : ] 
~~ ~~ def mark_regex ( regex , text , split_locations ) : 
for match in regex . finditer ( text ) : 
~~~ end_match = match . end ( ) 
if end_match < len ( split_locations ) : 
~~~ split_locations [ end_match ] = SHOULD_SPLIT 
~~ ~~ ~~ def mark_begin_end_regex ( regex , text , split_locations ) : 
begin_match = match . start ( ) 
for i in range ( begin_match + 1 , end_match ) : 
~~~ split_locations [ i ] = SHOULD_NOT_SPLIT 
~~ if end_match < len ( split_locations ) : 
~~~ if split_locations [ end_match ] == UNDECIDED : 
~~ ~~ if split_locations [ begin_match ] == UNDECIDED : 
~~~ split_locations [ begin_match ] = SHOULD_SPLIT 
~~ ~~ ~~ def tokenize ( text , normalize_ascii = True ) : 
if no_punctuation . match ( text ) : 
~~~ return [ text ] 
~~ if normalize_ascii : 
~~~ text = text . replace ( u"" , "oe" ) . replace ( u"" , "ae" ) 
text = repeated_dash_converter . sub ( "-" , text ) 
~~ split_locations = [ UNDECIDED ] * len ( text ) 
regexes = ( 
pure_whitespace , 
left_quote_shifter , 
left_quote_converter , 
left_single_quote_converter , 
remaining_quote_converter , 
english_nots , 
english_contractions , 
english_specific_appendages , 
french_appendages 
for regex in regexes : 
~~~ mark_regex ( regex , text , split_locations ) 
~~ begin_end_regexes = ( 
multi_single_quote_finder , 
right_single_quote_converter , 
simple_dash_finder if normalize_ascii else advanced_dash_finder , 
numerical_expression , 
url_file_finder , 
shifted_ellipses , 
shifted_standard_punctuation 
for regex in begin_end_regexes : 
~~~ mark_begin_end_regex ( regex , text , split_locations ) 
~~ protect_shorthand ( text , split_locations ) 
if normalize_ascii : 
~~~ text = dash_converter . sub ( "-" , text ) 
~~ return list ( split_with_locations ( text , split_locations ) ) 
~~ def get_url ( self , method = None , ** kwargs ) : 
kwargs . setdefault ( 'v' , self . __version ) 
if self . __token is not None : 
~~~ kwargs . setdefault ( 'access_token' , self . __token ) 
~~ return 'https://api.vk.com/method/{}?{}' . format ( 
method or self . __method , urlencode ( kwargs ) 
~~ def request ( self , method , ** kwargs ) : 
~~ return requests . get ( self . get_url ( method , ** kwargs ) ) . json ( ) 
~~ def authentication ( login , password ) : 
session = requests . Session ( ) 
response = session . get ( 'https://m.vk.com' ) 
url = re . search ( r\ , response . text ) . group ( 1 ) 
data = { 'email' : login , 'pass' : password } 
response = session . post ( url , data = data ) 
~~ def oauth ( login , password , app_id = 4729418 , scope = 2097151 ) : 
session = authentication ( login , password ) 
'client_id' : app_id , 
'scope' : scope , 
'display' : 'mobile' , 
response = session . post ( 'https://oauth.vk.com/authorize' , data = data ) 
if 'access_token' not in response . url : 
~~~ url = re . search ( r\ , response . text ) . group ( 1 ) 
response = session . get ( url ) 
~~~ return re . search ( r'access_token=([^\\&]+)' , response . url ) . group ( 1 ) 
~~ ~~ def autologin ( function , timeout = TIMEOUT ) : 
@ wraps ( function ) 
async def wrapper ( self , * args , ** kwargs ) : 
~~~ async with async_timeout . timeout ( timeout ) : 
~~~ return await function ( self , * args , ** kwargs ) 
~~ ~~ except ( asyncio . TimeoutError , ClientError , Error ) : 
~~ _LOGGER . debug ( "autologin" ) 
~~~ await self . login ( ) 
return await function ( self , * args , ** kwargs ) 
~~~ raise Error ( str ( function ) ) 
~~ async def get_information ( ) : 
jar = aiohttp . CookieJar ( unsafe = True ) 
websession = aiohttp . ClientSession ( cookie_jar = jar ) 
modem = eternalegypt . Modem ( hostname = sys . argv [ 1 ] , websession = websession ) 
await modem . login ( password = sys . argv [ 2 ] ) 
result = await modem . information ( ) 
for sms in result . sms : 
~~~ pprint . pprint ( sms ) 
~~ await modem . logout ( ) 
await websession . close ( ) 
~~ async def send_message ( ) : 
await modem . sms ( phone = sys . argv [ 3 ] , message = sys . argv [ 4 ] ) 
await modem . logout ( ) 
~~~ modem = eternalegypt . Modem ( hostname = sys . argv [ 1 ] , websession = websession ) 
~~ except eternalegypt . Error : 
~~ await websession . close ( ) 
~~ async def set_failover_mode ( mode ) : 
await modem . set_failover_mode ( mode ) 
~~ def nbviewer_link ( url ) : 
if six . PY2 : 
~~~ from urlparse import urlparse as urlsplit 
~~~ from urllib . parse import urlsplit 
~~ info = urlsplit ( url ) 
domain = info . netloc 
url_type = 'github' if domain == 'github.com' else 'url' 
return 'https://nbviewer.jupyter.org/%s%s' % ( url_type , info . path ) 
~~ def thumbnail_div ( self ) : 
return self . THUMBNAIL_TEMPLATE . format ( 
snippet = self . get_description ( ) [ 1 ] , thumbnail = self . thumb_file , 
ref_name = self . reference ) 
~~ def code_div ( self ) : 
code_example = self . code_example 
if code_example is None : 
~~ return self . CODE_TEMPLATE . format ( 
snippet = self . get_description ( ) [ 1 ] , code = code_example , 
~~ def code_example ( self ) : 
if self . _code_example is not None : 
~~~ return self . _code_example 
~~ return getattr ( self . nb . metadata , 'code_example' , None ) 
~~ def supplementary_files ( self ) : 
if self . _supplementary_files is not None : 
~~~ return self . _supplementary_files 
~~ return getattr ( self . nb . metadata , 'supplementary_files' , None ) 
~~ def other_supplementary_files ( self ) : 
if self . _other_supplementary_files is not None : 
~~~ return self . _other_supplementary_files 
~~ return getattr ( self . nb . metadata , 'other_supplementary_files' , None ) 
~~ def url ( self ) : 
if self . _url is not None : 
~~~ url = self . _url 
~~~ url = getattr ( self . nb . metadata , 'url' , None ) 
~~ if url is not None : 
~~~ return nbviewer_link ( url ) 
~~ ~~ def get_out_file ( self , ending = 'rst' ) : 
return os . path . splitext ( self . outfile ) [ 0 ] + os . path . extsep + ending 
~~ def process_notebook ( self , disable_warnings = True ) : 
infile = self . infile 
outfile = self . outfile 
in_dir = os . path . dirname ( infile ) + os . path . sep 
odir = os . path . dirname ( outfile ) + os . path . sep 
create_dirs ( os . path . join ( odir , 'images' ) ) 
ep = nbconvert . preprocessors . ExecutePreprocessor ( 
timeout = 300 ) 
cp = nbconvert . preprocessors . ClearOutputPreprocessor ( 
self . nb = nb = nbformat . read ( infile , nbformat . current_nbformat ) 
if disable_warnings : 
~~~ for i , cell in enumerate ( nb . cells ) : 
~~~ if cell [ 'cell_type' ] == 'code' : 
~~~ cell = cell . copy ( ) 
~~ ~~ cell = cell . copy ( ) 
nb . cells . insert ( i , cell ) 
~~ if self . preprocess : 
~~~ t = dt . datetime . now ( ) 
~~~ ep . preprocess ( nb , { 'metadata' : { 'path' : in_dir } } ) 
~~ except nbconvert . preprocessors . execute . CellExecutionError : 
~~~ logger . critical ( 
( dt . datetime . now ( ) - t ) . seconds ) 
~~ if disable_warnings : 
~~~ nb . cells . pop ( i ) 
~~ ~~ self . py_file = self . get_out_file ( 'py' ) 
if self . remove_tags : 
~~~ tp = nbconvert . preprocessors . TagRemovePreprocessor ( timeout = 300 ) 
for key , val in self . tag_options . items ( ) : 
~~~ setattr ( tp , key , set ( val ) ) 
~~ nb4rst = deepcopy ( nb ) 
tp . preprocess ( nb4rst , { 'metadata' : { 'path' : in_dir } } ) 
~~~ nb4rst = nb 
~~ self . create_rst ( nb4rst , in_dir , odir ) 
if self . clear : 
~~~ cp . preprocess ( nb , { 'metadata' : { 'path' : in_dir } } ) 
~~ nbformat . write ( nb , outfile ) 
self . create_py ( nb ) 
~~ def create_rst ( self , nb , in_dir , odir ) : 
raw_rst , resources = nbconvert . export_by_name ( 'rst' , nb ) 
rst_content = '' 
i0 = 0 
m = None 
bokeh_str = '' 
if 'bokeh' in raw_rst and self . insert_bokeh : 
~~~ bokeh_str += self . BOKEH_TEMPLATE . format ( 
version = self . insert_bokeh ) 
~~ if 'bokeh' in raw_rst and self . insert_bokeh_widgets : 
~~~ bokeh_str += self . BOKEH_WIDGETS_TEMPLATE . format ( 
version = self . insert_bokeh_widgets ) 
~~ for m in code_blocks . finditer ( raw_rst ) : 
~~~ lines = m . group ( ) . splitlines ( True ) 
header , content = lines [ 0 ] , '' . join ( lines [ 1 : ] ) 
no_magics = magic_patt . sub ( '\\g<1>' , content ) 
if no_magics . strip ( ) : 
~~~ rst_content += ( 
raw_rst [ i0 : m . start ( ) ] + bokeh_str + header + no_magics ) 
i0 = m . end ( ) 
~~~ rst_content += raw_rst [ i0 : m . start ( ) ] 
~~ ~~ if m is not None : 
~~~ rst_content += bokeh_str + raw_rst [ m . end ( ) : ] 
~~~ rst_content = raw_rst 
url = self . url 
if url is not None : 
~~~ rst_content += self . CODE_DOWNLOAD_NBVIEWER . format ( 
pyfile = os . path . basename ( self . py_file ) , 
nbfile = os . path . basename ( self . outfile ) , 
url = url ) 
~~~ rst_content += self . CODE_DOWNLOAD . format ( 
nbfile = os . path . basename ( self . outfile ) ) 
~~ supplementary_files = self . supplementary_files 
other_supplementary_files = self . other_supplementary_files 
if supplementary_files or other_supplementary_files : 
~~~ for f in ( supplementary_files or [ ] ) + ( 
other_supplementary_files or [ ] ) : 
~~~ if not os . path . exists ( os . path . join ( odir , f ) ) : 
~~~ copyfile ( os . path . join ( in_dir , f ) , os . path . join ( odir , f ) ) 
~~ ~~ ~~ if supplementary_files : 
~~~ rst_content += self . data_download ( supplementary_files ) 
~~ rst_file = self . get_out_file ( ) 
outputs = sorted ( resources [ 'outputs' ] , key = rst_content . find ) 
base = os . path . join ( 'images' , os . path . splitext ( 
os . path . basename ( self . infile ) ) [ 0 ] + '_%i.png' ) 
out_map = { os . path . basename ( original ) : base % i 
for i , original in enumerate ( outputs ) } 
for original , final in six . iteritems ( out_map ) : 
~~~ rst_content = rst_content . replace ( original , final ) 
~~ with open ( rst_file , 'w' ) as f : 
~~~ f . write ( rst_content . rstrip ( ) + '\\n' ) 
~~ pictures = [ ] 
for original in outputs : 
~~~ fname = os . path . join ( odir , out_map [ os . path . basename ( original ) ] ) 
pictures . append ( fname ) 
~~~ f = open ( fname , 'w+b' ) 
~~~ f = open ( fname , 'w' ) 
~~ f . write ( resources [ 'outputs' ] [ original ] ) 
f . close ( ) 
~~ self . pictures = pictures 
~~ def create_py ( self , nb , force = False ) : 
if list ( map ( int , re . findall ( '\\d+' , nbconvert . __version__ ) ) ) >= [ 4 , 2 ] : 
~~~ py_file = os . path . basename ( self . py_file ) 
~~~ py_file = self . py_file 
~~~ level = logger . logger . level 
~~~ level = logger . level 
~~ spr . call ( [ 'jupyter' , 'nbconvert' , '--to=python' , 
'--output=' + py_file , '--log-level=%s' % level , 
self . outfile ] ) 
with open ( self . py_file ) as f : 
~~~ py_content = f . read ( ) 
py_content , flags = re . MULTILINE ) 
with open ( self . py_file , 'w' ) as f : 
~~~ f . write ( py_content ) 
~~ ~~ def data_download ( self , files ) : 
if len ( files ) > 1 : 
~~~ return self . DATA_DOWNLOAD % ( 
~~ return self . DATA_DOWNLOAD % ':download:`%s`' % files [ 0 ] 
~~ def create_thumb ( self ) : 
thumbnail_figure = self . copy_thumbnail_figure ( ) 
if thumbnail_figure is not None : 
~~~ if isinstance ( thumbnail_figure , six . string_types ) : 
~~~ pic = thumbnail_figure 
~~~ pic = self . pictures [ thumbnail_figure ] 
~~ self . save_thumbnail ( pic ) 
~~~ for pic in self . pictures [ : : - 1 ] : 
~~~ if pic . endswith ( 'png' ) : 
~~~ self . save_thumbnail ( pic ) 
~~ ~~ ~~ ~~ def get_description ( self ) : 
def split_header ( s , get_header = True ) : 
~~~ s = s . lstrip ( ) . rstrip ( ) 
parts = s . splitlines ( ) 
if parts [ 0 ] . startswith ( '#' ) : 
~~~ if get_header : 
~~~ header = re . sub ( '#+\\s*' , '' , parts . pop ( 0 ) ) 
if not parts : 
~~~ return header , '' 
~~~ header = '' 
~~ rest = '\\n' . join ( parts ) . lstrip ( ) . split ( '\\n\\n' ) 
return header , desc 
~~~ if parts [ 0 ] . startswith ( ( '=' , '-' ) ) : 
~~~ parts = parts [ 1 : ] 
~~ header = parts . pop ( 0 ) 
if parts and parts [ 0 ] . startswith ( ( '=' , '-' ) ) : 
~~~ parts . pop ( 0 ) 
~~ if not parts : 
~~ ~~ first_cell = self . nb [ 'cells' ] [ 0 ] 
if not first_cell [ 'cell_type' ] == 'markdown' : 
~~~ return '' , '' 
~~ header , desc = split_header ( first_cell [ 'source' ] ) 
if not desc and len ( self . nb [ 'cells' ] ) > 1 : 
~~~ second_cell = self . nb [ 'cells' ] [ 1 ] 
if second_cell [ 'cell_type' ] == 'markdown' : 
~~~ _ , desc = split_header ( second_cell [ 'source' ] , False ) 
~~ ~~ return header , desc 
~~ def scale_image ( self , in_fname , out_fname , max_width , max_height ) : 
~~~ from PIL import Image 
~~~ import Image 
~~ img = Image . open ( in_fname ) 
width_in , height_in = img . size 
scale_w = max_width / float ( width_in ) 
scale_h = max_height / float ( height_in ) 
if height_in * scale_w <= max_height : 
~~~ scale = scale_w 
~~~ scale = scale_h 
~~ if scale >= 1.0 and in_fname == out_fname : 
~~ width_sc = int ( round ( scale * width_in ) ) 
height_sc = int ( round ( scale * height_in ) ) 
img . thumbnail ( ( width_sc , height_sc ) , Image . ANTIALIAS ) 
thumb = Image . new ( 'RGB' , ( max_width , max_height ) , ( 255 , 255 , 255 ) ) 
pos_insert = ( 
( max_width - width_sc ) // 2 , ( max_height - height_sc ) // 2 ) 
thumb . paste ( img , pos_insert ) 
thumb . save ( out_fname ) 
~~ def save_thumbnail ( self , image_path ) : 
thumb_dir = os . path . join ( os . path . dirname ( image_path ) , 'thumb' ) 
create_dirs ( thumb_dir ) 
thumb_file = os . path . join ( thumb_dir , 
'%s_thumb.png' % self . reference ) 
if os . path . exists ( image_path ) : 
self . scale_image ( image_path , thumb_file , 400 , 280 ) 
~~ self . thumb_file = thumb_file 
~~ def copy_thumbnail_figure ( self ) : 
ret = None 
if self . _thumbnail_figure is not None : 
~~~ if not isstring ( self . _thumbnail_figure ) : 
~~~ ret = self . _thumbnail_figure 
~~~ ret = osp . join ( osp . dirname ( self . outfile ) , 
osp . basename ( self . _thumbnail_figure ) ) 
copyfile ( self . _thumbnail_figure , ret ) 
return ret 
~~ ~~ elif hasattr ( self . nb . metadata , 'thumbnail_figure' ) : 
~~~ if not isstring ( self . nb . metadata . thumbnail_figure ) : 
~~~ ret = self . nb . metadata . thumbnail_figure 
~~~ ret = osp . join ( osp . dirname ( self . outfile ) , 'images' , 
osp . basename ( self . nb . metadata . thumbnail_figure ) ) 
copyfile ( osp . join ( osp . dirname ( self . infile ) , 
self . nb . metadata . thumbnail_figure ) , 
ret ) 
~~ ~~ return ret 
~~ def process_directories ( self ) : 
for i , ( base_dir , target_dir , paths ) in enumerate ( zip ( 
self . in_dir , self . out_dir , map ( os . walk , self . in_dir ) ) ) : 
~~~ self . _in_dir_count = i 
self . recursive_processing ( base_dir , target_dir , paths ) 
~~ ~~ def recursive_processing ( self , base_dir , target_dir , it ) : 
~~~ file_dir , dirs , files = next ( it ) 
~~~ return '' , [ ] 
~~ readme_files = { 'README.md' , 'README.rst' , 'README.txt' } 
if readme_files . intersection ( files ) : 
~~~ foutdir = file_dir . replace ( base_dir , target_dir ) 
create_dirs ( foutdir ) 
this_nbps = [ 
NotebookProcessor ( 
infile = f , 
outfile = os . path . join ( foutdir , os . path . basename ( f ) ) , 
disable_warnings = self . disable_warnings , 
preprocess = ( 
( self . preprocess is True or f in self . preprocess ) and 
not ( self . dont_preprocess is True or 
f in self . dont_preprocess ) ) , 
clear = ( ( self . clear is True or f in self . clear ) and not 
( self . dont_clear is True or f in self . dont_clear ) ) , 
code_example = self . code_examples . get ( f ) , 
supplementary_files = self . supplementary_files . get ( f ) , 
other_supplementary_files = self . osf . get ( f ) , 
thumbnail_figure = self . thumbnail_figures . get ( f ) , 
url = self . get_url ( f . replace ( base_dir , '' ) ) , 
** self . _nbp_kws ) 
for f in map ( lambda f : os . path . join ( file_dir , f ) , 
filter ( self . pattern . match , files ) ) ] 
readme_file = next ( iter ( readme_files . intersection ( files ) ) ) 
~~ labels = OrderedDict ( ) 
this_label = 'gallery_' + foutdir . replace ( os . path . sep , '_' ) 
if this_label . endswith ( '_' ) : 
~~~ this_label = this_label [ : - 1 ] 
~~ for d in dirs : 
~~~ label , nbps = self . recursive_processing ( 
base_dir , target_dir , it ) 
if label : 
~~~ labels [ label ] = nbps 
with open ( os . path . join ( file_dir , readme_file ) ) as f : 
~~~ s += f . read ( ) . rstrip ( ) + '\\n\\n' 
nbp . get_out_file ( ) ) ) [ 0 ] for nbp in this_nbps ) 
for d in dirs : 
~~~ findex = os . path . join ( d , 'index.rst' ) 
if os . path . exists ( os . path . join ( foutdir , findex ) ) : 
~~ ~~ s += '\\n' 
for nbp in this_nbps : 
~~~ code_div = nbp . code_div 
if code_div is not None : 
~~~ s += code_div + '\\n' 
~~~ s += nbp . thumbnail_div + '\\n' 
for label , nbps in labels . items ( ) : 
label ) 
for nbp in nbps : 
~~ s += '\\n' 
with open ( os . path . join ( foutdir , 'index.rst' ) , 'w' ) as f : 
~~~ f . write ( s ) 
~~ return this_label , list ( chain ( this_nbps , * labels . values ( ) ) ) 
~~ def from_sphinx ( cls , app ) : 
app . config . html_static_path . append ( os . path . join ( 
os . path . dirname ( __file__ ) , '_static' ) ) 
config = app . config . example_gallery_config 
insert_bokeh = config . get ( 'insert_bokeh' ) 
if insert_bokeh : 
~~~ if not isstring ( insert_bokeh ) : 
~~~ import bokeh 
insert_bokeh = bokeh . __version__ 
~~ app . add_stylesheet ( 
NotebookProcessor . BOKEH_STYLE_SHEET . format ( 
version = insert_bokeh ) ) 
app . add_javascript ( 
NotebookProcessor . BOKEH_JS . format ( version = insert_bokeh ) ) 
~~ insert_bokeh_widgets = config . get ( 'insert_bokeh_widgets' ) 
if insert_bokeh_widgets : 
~~~ if not isstring ( insert_bokeh_widgets ) : 
insert_bokeh_widgets = bokeh . __version__ 
NotebookProcessor . BOKEH_WIDGETS_STYLE_SHEET . format ( 
version = insert_bokeh_widgets ) ) 
NotebookProcessor . BOKEH_WIDGETS_JS . format ( 
~~ if not app . config . process_examples : 
~~ cls ( ** app . config . example_gallery_config ) . process_directories ( ) 
~~ def get_url ( self , nbfile ) : 
urls = self . urls 
if isinstance ( urls , dict ) : 
~~~ return urls . get ( nbfile ) 
~~ elif isstring ( urls ) : 
~~~ if not urls . endswith ( '/' ) : 
~~~ urls += '/' 
~~ return urls + nbfile 
~~ ~~ def received ( self , src , body ) : 
self . _msgid += 1 
message = IncomingMessage ( src , body , self . _msgid ) 
self . _traffic . append ( message ) 
self . _receive_message ( message ) 
return message 
~~ def subscribe ( self , number , callback ) : 
self . _subscribers [ digits_only ( number ) ] = callback 
~~ def states ( self ) : 
ret = set ( ) 
if self . accepted : 
~~~ ret . add ( 'accepted' ) 
~~ if self . delivered : 
~~~ ret . add ( 'delivered' ) 
~~ if self . expired : 
~~~ ret . add ( 'expired' ) 
~~ if self . error : 
~~~ ret . add ( 'error' ) 
~~ def add_provider ( self , name , Provider , ** config ) : 
provider = Provider ( self , name , ** config ) 
self . _providers [ name ] = provider 
if self . default_provider is None : 
~~~ self . default_provider = name 
~~ return provider 
~~ def send ( self , message ) : 
if message . provider is not None : 
provider = self . get_provider ( message . provider ) 
~~~ provider_name = self . router ( message , * message . routing_values ) or self . _default_provider 
~~ provider = self . get_provider ( provider_name ) 
~~ message . provider = provider . name 
message = provider . send ( message ) 
self . onSend ( message ) 
~~ def receiver_blueprint_for ( self , name ) : 
provider = self . get_provider ( name ) 
bp = provider . make_receiver_blueprint ( ) 
@ bp . before_request 
def init_g ( ) : 
~~~ g . provider = provider 
~~ return bp 
~~ def receiver_blueprints ( self ) : 
blueprints = { } 
for name in self . _providers : 
~~~ blueprints [ name ] = self . receiver_blueprint_for ( name ) 
~~ ~~ return blueprints 
~~ def receiver_blueprints_register ( self , app , prefix = '/' ) : 
for name , bp in self . receiver_blueprints ( ) . items ( ) : 
~~~ app . register_blueprint ( 
bp , 
url_prefix = '{prefix}{name}' . format ( 
prefix = '/' + prefix . strip ( '/' ) + '/' if prefix else '/' , 
name = name 
~~ def _receive_message ( self , message ) : 
message . provider = self . name 
self . gateway . onReceive ( message ) 
~~ def _receive_status ( self , status ) : 
status . provider = self . name 
self . gateway . onStatus ( status ) 
return status 
~~ def im ( ) : 
req = jsonex_loads ( request . get_data ( ) ) 
message = g . provider . _receive_message ( req [ 'message' ] ) 
return { 'message' : message } 
~~ def status ( ) : 
status = g . provider . _receive_status ( req [ 'status' ] ) 
return { 'status' : status } 
~~ def jsonex_loads ( s ) : 
return json . loads ( s . decode ( 'utf-8' ) , cls = JsonExDecoder , classes = classes , exceptions = exceptions ) 
~~ def jsonex_api ( f ) : 
~~~ code , res = 200 , f ( * args , ** kwargs ) 
~~ except HTTPException as e : 
~~~ code , res = e . code , { 'error' : e } 
~~~ code , res = 500 , { 'error' : e } 
~~ response = make_response ( jsonex_dumps ( res ) , code ) 
response . headers [ 'Content-Type' ] = 'application/json' 
~~ def _parse_authentication ( url ) : 
u = url 
if url in _parse_authentication . _memoize : 
~~~ u , h = _parse_authentication . _memoize [ url ] 
~~~ p = urlsplit ( url , 'http' ) 
if p . username and p . password : 
u = urlunsplit ( ( p . scheme , p . netloc . split ( '@' , 1 ) [ 1 ] , p . path , p . query , p . fragment ) ) 
~~ _parse_authentication . _memoize [ url ] = ( u , h ) 
~~ return u , h 
~~ def jsonex_request ( url , data , headers = None ) : 
url , headers = _parse_authentication ( url ) 
headers [ 'Content-Type' ] = 'application/json' 
~~~ req = Request ( url , headers = headers ) 
response = urlopen ( req , jsonex_dumps ( data ) ) 
res_str = response . read ( ) 
res = jsonex_loads ( res_str ) 
~~ except HTTPError as e : 
~~~ if 'Content-Type' in e . headers and e . headers [ 'Content-Type' ] == 'application/json' : 
~~~ res = jsonex_loads ( e . read ( ) ) 
~~~ raise exc . ServerError ( \ . format ( url , e ) ) 
~~ ~~ except URLError as e : 
~~~ raise exc . ConnectionError ( \ . format ( url , e ) ) 
res = jsonex_request ( self . server_url + '/im' . lstrip ( '/' ) , { 'message' : message } ) 
for k , v in msg . __dict__ . items ( ) : 
~~~ setattr ( message , k , v ) 
~~ def _forward_object_to_client ( self , client , obj ) : 
url , name = ( '/im' , 'message' ) if isinstance ( obj , IncomingMessage ) else ( '/status' , 'status' ) 
res = jsonex_request ( client . rstrip ( '/' ) + '/' + url . lstrip ( '/' ) , { name : obj } ) 
return res [ name ] 
~~ def forward ( self , obj ) : 
clients = self . choose_clients ( obj ) 
if Parallel : 
~~~ pll = Parallel ( self . _forward_object_to_client ) 
for client in clients : 
~~~ pll ( client , obj ) 
~~ results , errors = pll . join ( ) 
if errors : 
~~~ raise errors [ 0 ] 
~~~ for client in clients : 
~~~ self . _forward_object_to_client ( client , obj ) 
~~ ~~ ~~ def get_balance ( self , address : str , erc20_address : str ) -> int : 
return get_erc20_contract ( self . w3 , erc20_address ) . functions . balanceOf ( address ) . call ( ) 
~~ def get_info ( self , erc20_address : str ) -> Erc20_Info : 
erc20 = get_example_erc20_contract ( self . w3 , erc20_address ) 
name = erc20 . functions . name ( ) . call ( ) 
symbol = erc20 . functions . symbol ( ) . call ( ) 
decimals = erc20 . functions . decimals ( ) . call ( ) 
return Erc20_Info ( name , symbol , decimals ) 
~~ def get_transfer_history ( self , from_block : int , to_block : Optional [ int ] = None , 
from_address : Optional [ str ] = None , to_address : Optional [ str ] = None , 
token_address : Optional [ str ] = None ) -> List [ Dict [ str , any ] ] : 
erc20 = get_erc20_contract ( self . w3 ) 
argument_filters = { } 
if from_address : 
~~~ argument_filters [ 'from' ] = from_address 
~~ if to_address : 
~~~ argument_filters [ 'to' ] = to_address 
~~ return erc20 . events . Transfer . createFilter ( fromBlock = from_block , 
toBlock = to_block , 
address = token_address , 
argument_filters = argument_filters ) . get_all_entries ( ) 
~~ def send_tokens ( self , to : str , amount : int , erc20_address : str , private_key : str ) -> bytes : 
erc20 = get_erc20_contract ( self . w3 , erc20_address ) 
account = Account . privateKeyToAccount ( private_key ) 
tx = erc20 . functions . transfer ( to , amount ) . buildTransaction ( { 'from' : account . address } ) 
return self . ethereum_client . send_unsigned_transaction ( tx , private_key = private_key ) 
~~ def trace_filter ( self , from_block : int = 1 , to_block : Optional [ int ] = None , 
from_address : Optional [ List [ str ] ] = None , to_address : Optional [ List [ str ] ] = None , 
after : Optional [ int ] = None , count : Optional [ int ] = None ) -> List [ Dict [ str , any ] ] : 
if from_block : 
~~~ parameters [ 'fromBlock' ] = '0x%x' % from_block 
~~ if to_block : 
~~~ parameters [ 'toBlock' ] = '0x%x' % to_block 
~~ if from_address : 
~~~ parameters [ 'fromAddress' ] = from_address 
~~~ parameters [ 'toAddress' ] = to_address 
~~ if after : 
~~~ parameters [ 'after' ] = after 
~~~ parameters [ 'count' ] = count 
~~~ return self . _decode_traces ( self . slow_w3 . parity . traceFilter ( parameters ) ) 
~~ except ParityTraceDecodeException as exc : 
return self . _decode_traces ( self . slow_w3 . parity . traceFilter ( parameters ) ) 
~~ ~~ def get_slow_provider ( self , timeout : int ) : 
if isinstance ( self . w3_provider , AutoProvider ) : 
~~~ return HTTPProvider ( endpoint_uri = 'http://localhost:8545' , 
request_kwargs = { 'timeout' : timeout } ) 
~~ elif isinstance ( self . w3_provider , HTTPProvider ) : 
~~~ return HTTPProvider ( endpoint_uri = self . w3_provider . endpoint_uri , 
~~~ return self . w3_provider 
~~ ~~ def send_unsigned_transaction ( self , tx : Dict [ str , any ] , private_key : Optional [ str ] = None , 
public_key : Optional [ str ] = None , retry : bool = False , 
block_identifier : Optional [ str ] = None ) -> bytes : 
if private_key : 
~~~ address = self . private_key_to_address ( private_key ) 
~~ elif public_key : 
~~~ address = public_key 
~~ if tx . get ( 'nonce' ) is None : 
~~~ tx [ 'nonce' ] = self . get_nonce_for_account ( address , block_identifier = block_identifier ) 
~~ number_errors = 5 
while number_errors >= 0 : 
~~~ if private_key : 
~~~ signed_tx = self . w3 . eth . account . signTransaction ( tx , private_key = private_key ) 
~~~ return self . send_raw_transaction ( signed_tx . rawTransaction ) 
~~ except TransactionAlreadyImported as e : 
~~~ tx_hash = signed_tx . hash 
return tx_hash 
~~ ~~ elif public_key : 
~~~ tx [ 'from' ] = address 
return self . send_transaction ( tx ) 
~~ ~~ except ReplacementTransactionUnderpriced as e : 
~~~ if not retry or not number_errors : 
address , tx [ 'nonce' ] ) 
tx [ 'nonce' ] += 1 
~~ except InvalidNonce as e : 
tx [ 'nonce' ] = self . get_nonce_for_account ( address , block_identifier = block_identifier ) 
number_errors -= 1 
~~ ~~ ~~ def send_eth_to ( self , private_key : str , to : str , gas_price : int , value : int , gas : int = 22000 , 
retry : bool = False , block_identifier = None , max_eth_to_send : int = 0 ) -> bytes : 
assert check_checksum ( to ) 
if max_eth_to_send and value > self . w3 . toWei ( max_eth_to_send , 'ether' ) : 
~~ tx = { 
'to' : to , 
'value' : value , 
'gas' : gas , 
'gasPrice' : gas_price , 
return self . send_unsigned_transaction ( tx , private_key = private_key , retry = retry , 
block_identifier = block_identifier ) 
~~ def check_tx_with_confirmations ( self , tx_hash : str , confirmations : int ) -> bool : 
tx_receipt = self . w3 . eth . getTransactionReceipt ( tx_hash ) 
if not tx_receipt or tx_receipt [ 'blockNumber' ] is None : 
~~~ return ( self . w3 . eth . blockNumber - tx_receipt [ 'blockNumber' ] ) >= confirmations 
~~ ~~ def get_signing_address ( hash : Union [ bytes , str ] , v : int , r : int , s : int ) -> str : 
encoded_64_address = ecrecover_to_pub ( hash , v , r , s ) 
address_bytes = sha3 ( encoded_64_address ) [ - 20 : ] 
return checksum_encode ( address_bytes ) 
~~ def generate_address_2 ( from_ : Union [ str , bytes ] , salt : Union [ str , bytes ] , init_code : [ str , bytes ] ) -> str : 
from_ = HexBytes ( from_ ) 
salt = HexBytes ( salt ) 
init_code = HexBytes ( init_code ) 
init_code_hash = Web3 . sha3 ( init_code ) 
contract_address = Web3 . sha3 ( HexBytes ( 'ff' ) + from_ + salt + init_code_hash ) 
return Web3 . toChecksumAddress ( contract_address [ 12 : ] ) 
~~ def get_safe_contract ( w3 : Web3 , address = None ) : 
return w3 . eth . contract ( address , 
abi = GNOSIS_SAFE_INTERFACE [ 'abi' ] , 
bytecode = GNOSIS_SAFE_INTERFACE [ 'bytecode' ] ) 
~~ def get_old_safe_contract ( w3 : Web3 , address = None ) : 
abi = OLD_GNOSIS_SAFE_INTERFACE [ 'abi' ] , 
bytecode = OLD_GNOSIS_SAFE_INTERFACE [ 'bytecode' ] ) 
~~ def get_paying_proxy_contract ( w3 : Web3 , address = None ) : 
abi = PAYING_PROXY_INTERFACE [ 'abi' ] , 
bytecode = PAYING_PROXY_INTERFACE [ 'bytecode' ] ) 
~~ def get_erc20_contract ( w3 : Web3 , address = None ) : 
abi = ERC20_INTERFACE [ 'abi' ] , 
bytecode = ERC20_INTERFACE [ 'bytecode' ] ) 
~~ def signature_split ( signatures : bytes , pos : int ) -> Tuple [ int , int , int ] : 
signature_pos = 65 * pos 
v = signatures [ 64 + signature_pos ] 
r = int . from_bytes ( signatures [ signature_pos : 32 + signature_pos ] , 'big' ) 
s = int . from_bytes ( signatures [ 32 + signature_pos : 64 + signature_pos ] , 'big' ) 
return v , r , s 
~~ def signature_to_bytes ( vrs : Tuple [ int , int , int ] ) -> bytes : 
byte_order = 'big' 
v , r , s = vrs 
return ( r . to_bytes ( 32 , byteorder = byte_order ) + 
s . to_bytes ( 32 , byteorder = byte_order ) + 
v . to_bytes ( 1 , byteorder = byte_order ) ) 
~~ def signatures_to_bytes ( signatures : List [ Tuple [ int , int , int ] ] ) -> bytes : 
return b'' . join ( [ signature_to_bytes ( vrs ) for vrs in signatures ] ) 
~~ def find_valid_random_signature ( s : int ) -> Tuple [ int , int ] : 
for _ in range ( 10000 ) : 
~~~ r = int ( os . urandom ( 31 ) . hex ( ) , 16 ) 
v = ( r % 2 ) + 27 
if r < secpk1n : 
~~~ tx = Transaction ( 0 , 1 , 21000 , b'' , 0 , b'' , v = v , r = r , s = s ) 
~~~ tx . sender 
return v , r 
~~ except ( InvalidTransaction , ValueError ) : 
~~ def _build_proxy_contract_creation_constructor ( self , 
master_copy : str , 
initializer : bytes , 
funder : str , 
payment_token : str , 
payment : int ) -> ContractConstructor : 
if not funder or funder == NULL_ADDRESS : 
~~~ funder = NULL_ADDRESS 
payment = 0 
~~ return get_paying_proxy_contract ( self . w3 ) . constructor ( 
master_copy , 
initializer , 
funder , 
payment_token , 
payment ) 
~~ def _build_proxy_contract_creation_tx ( self , 
payment : int , 
gas : int , 
gas_price : int , 
nonce : int = 0 ) : 
return self . _build_proxy_contract_creation_constructor ( 
payment 
) . buildTransaction ( { 
'nonce' : nonce , 
~~ def _build_contract_creation_tx_with_valid_signature ( self , tx_dict : Dict [ str , None ] , s : int ) -> Transaction : 
zero_address = HexBytes ( '0x' + '0' * 40 ) 
f_address = HexBytes ( '0x' + 'f' * 40 ) 
nonce = tx_dict [ 'nonce' ] 
gas_price = tx_dict [ 'gasPrice' ] 
gas = tx_dict [ 'gas' ] 
value = tx_dict [ 'value' ] 
data = tx_dict [ 'data' ] 
for _ in range ( 100 ) : 
~~~ v , r = self . find_valid_random_signature ( s ) 
contract_creation_tx = Transaction ( nonce , gas_price , gas , to , value , HexBytes ( data ) , v = v , r = r , s = s ) 
sender_address = contract_creation_tx . sender 
contract_address = contract_creation_tx . creates 
if sender_address in ( zero_address , f_address ) or contract_address in ( zero_address , f_address ) : 
~~~ raise InvalidTransaction 
~~ return contract_creation_tx 
~~ except InvalidTransaction : 
~~ def _estimate_gas ( self , 
payment_token : str ) -> int : 
gas : int = self . _build_proxy_contract_creation_constructor ( 
0 ) . estimateGas ( ) 
if payment_token == NULL_ADDRESS : 
~~~ gas += self . w3 . eth . estimateGas ( { 'to' : funder , 'value' : 1 } ) 
~~~ gas += get_erc20_contract ( self . w3 , 
payment_token ) . functions . transfer ( funder , 1 ) . estimateGas ( { 'from' : 
payment_token } ) 
~~~ raise InvalidERC20Token from exc 
~~ ~~ return gas 
~~ def _sign_web3_transaction ( tx : Dict [ str , any ] , v : int , r : int , s : int ) -> ( bytes , HexBytes ) : 
unsigned_transaction = serializable_unsigned_transaction_from_dict ( tx ) 
rlp_encoded_transaction = encode_transaction ( unsigned_transaction , vrs = ( v , r , s ) ) 
return rlp_encoded_transaction , unsigned_transaction . hash ( ) 
~~ def check_proxy_code ( self , address ) -> bool : 
deployed_proxy_code = self . w3 . eth . getCode ( address ) 
proxy_code_fns = ( get_paying_proxy_deployed_bytecode , 
get_proxy_factory_contract ( self . w3 , 
self . proxy_factory_address ) . functions . proxyRuntimeCode ( ) . call ) 
for proxy_code_fn in proxy_code_fns : 
~~~ if deployed_proxy_code == proxy_code_fn ( ) : 
~~ def check_funds_for_tx_gas ( self , safe_address : str , safe_tx_gas : int , data_gas : int , gas_price : int , 
gas_token : str ) -> bool : 
if gas_token == NULL_ADDRESS : 
~~~ balance = self . ethereum_client . get_balance ( safe_address ) 
~~~ balance = self . ethereum_client . erc20 . get_balance ( safe_address , gas_token ) 
~~ return balance >= ( safe_tx_gas + data_gas ) * gas_price 
~~ def deploy_master_contract ( self , deployer_account = None , deployer_private_key = None ) -> str : 
assert deployer_account or deployer_private_key 
deployer_address = deployer_account or self . ethereum_client . private_key_to_address ( deployer_private_key ) 
safe_contract = self . get_contract ( ) 
tx = safe_contract . constructor ( ) . buildTransaction ( { 'from' : deployer_address } ) 
tx_hash = self . ethereum_client . send_unsigned_transaction ( tx , private_key = deployer_private_key , 
public_key = deployer_account ) 
tx_receipt = self . ethereum_client . get_transaction_receipt ( tx_hash , timeout = 60 ) 
assert tx_receipt . status 
contract_address = tx_receipt . contractAddress 
master_safe = self . get_contract ( contract_address ) 
tx = master_safe . functions . setup ( 
[ "0x0000000000000000000000000000000000000002" , "0x0000000000000000000000000000000000000003" ] , 
) . buildTransaction ( { 'from' : deployer_address } ) 
return contract_address 
~~ def deploy_paying_proxy_contract ( self , initializer = b'' , deployer_account = None , deployer_private_key = None ) -> str : 
safe_proxy_contract = get_paying_proxy_contract ( self . w3 ) 
tx = safe_proxy_contract . constructor ( self . master_copy_address , initializer , 
NULL_ADDRESS , 
NULL_ADDRESS , 0 ) . buildTransaction ( { 'from' : deployer_address } ) 
return tx_receipt . contractAddress 
~~ def deploy_proxy_contract ( self , initializer = b'' , deployer_account = None , deployer_private_key = None ) -> str : 
proxy_factory_contract = get_proxy_factory_contract ( self . w3 , self . proxy_factory_address ) 
create_proxy_fn = proxy_factory_contract . functions . createProxy ( self . master_copy_address , initializer ) 
contract_address = create_proxy_fn . call ( ) 
tx = create_proxy_fn . buildTransaction ( { 'from' : deployer_address } ) 
tx_receipt = self . ethereum_client . get_transaction_receipt ( tx_hash , timeout = 120 ) 
~~ def deploy_proxy_contract_with_nonce ( self , salt_nonce : int , initializer : bytes , gas : int , gas_price : int , 
deployer_private_key = None ) -> Tuple [ bytes , Dict [ str , any ] , str ] : 
assert deployer_private_key 
create_proxy_fn = proxy_factory_contract . functions . createProxyWithNonce ( self . master_copy_address , initializer , 
salt_nonce ) 
deployer_account = Account . privateKeyToAccount ( deployer_private_key ) 
nonce = self . ethereum_client . get_nonce_for_account ( deployer_account . address , 'pending' ) 
tx = create_proxy_fn . buildTransaction ( { 'from' : deployer_account . address , 'gasPrice' : gas_price , 
'nonce' : nonce , 'gas' : gas + 50000 } ) 
signed_tx = deployer_account . signTransaction ( tx ) 
tx_hash = self . ethereum_client . send_raw_transaction ( signed_tx . rawTransaction ) 
return tx_hash , tx , contract_address 
~~ def deploy_proxy_factory_contract ( self , deployer_account = None , deployer_private_key = None ) -> str : 
proxy_factory_contract = get_proxy_factory_contract ( self . w3 ) 
tx = proxy_factory_contract . constructor ( ) . buildTransaction ( { 'from' : deployer_address } ) 
~~ def estimate_tx_gas_with_safe ( self , safe_address : str , to : str , value : int , data : bytes , operation : int , 
block_identifier = 'pending' ) -> int : 
data = data or b'' 
def parse_revert_data ( result : bytes ) -> int : 
~~~ gas_estimation_offset = 4 + 32 + 32 
estimated_gas = result [ gas_estimation_offset : ] 
if len ( estimated_gas ) != 32 : 
safe_address , result . hex ( ) , tx ) 
~~ return int ( estimated_gas . hex ( ) , 16 ) 
~~~ tx = self . get_contract ( safe_address ) . functions . requiredTxGas ( 
to , 
data , 
operation 
'from' : safe_address , 
'gas' : int ( 1e7 ) , 
'gasPrice' : 0 , 
result : HexBytes = self . w3 . eth . call ( tx , block_identifier = block_identifier ) 
return parse_revert_data ( result ) 
error_dict = exc . args [ 0 ] 
data = error_dict . get ( 'data' ) 
if not data : 
~~ key = list ( data . keys ( ) ) [ 0 ] 
result = data [ key ] [ 'return' ] 
if result == '0x0' : 
estimated_gas_hex = result [ 138 : ] 
assert len ( estimated_gas_hex ) == 64 
estimated_gas = int ( estimated_gas_hex , 16 ) 
return estimated_gas 
~~ ~~ ~~ def estimate_tx_gas_with_web3 ( self , safe_address : str , to : str , value : int , data : bytes ) -> int : 
return self . ethereum_client . estimate_gas ( safe_address , to , value , data , block_identifier = 'pending' ) 
~~ def estimate_tx_gas ( self , safe_address : str , to : str , value : int , data : bytes , operation : int ) -> int : 
proxy_gas = 1000 
old_call_gas = 35000 
safe_gas_estimation = ( self . estimate_tx_gas_with_safe ( safe_address , to , value , data , operation ) 
+ proxy_gas + old_call_gas ) 
if SafeOperation ( operation ) == SafeOperation . CALL : 
~~~ web3_gas_estimation = ( self . estimate_tx_gas_with_web3 ( safe_address , to , value , data ) 
~~~ web3_gas_estimation = 0 
~~ return max ( safe_gas_estimation , web3_gas_estimation ) 
~~~ return safe_gas_estimation 
~~ ~~ def estimate_tx_operational_gas ( self , safe_address : str , data_bytes_length : int ) : 
threshold = self . retrieve_threshold ( safe_address ) 
return 15000 + data_bytes_length // 32 * 100 + 5000 * threshold 
~~ def send_multisig_tx ( self , 
safe_address : str , 
to : str , 
value : int , 
data : bytes , 
operation : int , 
safe_tx_gas : int , 
data_gas : int , 
gas_token : str , 
refund_receiver : str , 
signatures : bytes , 
tx_sender_private_key : str , 
tx_gas = None , 
tx_gas_price = None , 
block_identifier = 'pending' ) -> Tuple [ bytes , Dict [ str , any ] ] : 
safe_tx = self . build_multisig_tx ( safe_address , 
operation , 
safe_tx_gas , 
data_gas , 
gas_price , 
gas_token , 
refund_receiver , 
signatures ) 
tx_sender_address = Account . privateKeyToAccount ( tx_sender_private_key ) . address 
safe_tx . call ( tx_sender_address = tx_sender_address ) 
return safe_tx . execute ( tx_sender_private_key = tx_sender_private_key , 
tx_gas = tx_gas , 
tx_gas_price = tx_gas_price , 
~~ def build ( self , owners : List [ str ] , threshold : int , salt_nonce : int , 
gas_price : int , payment_receiver : Optional [ str ] = None , 
payment_token : Optional [ str ] = None , 
payment_token_eth_value : float = 1.0 , fixed_creation_cost : Optional [ int ] = None ) : 
assert 0 < threshold <= len ( owners ) 
payment_receiver = payment_receiver or NULL_ADDRESS 
payment_token = payment_token or NULL_ADDRESS 
assert Web3 . isChecksumAddress ( payment_receiver ) 
assert Web3 . isChecksumAddress ( payment_token ) 
safe_setup_data : bytes = self . _get_initial_setup_safe_data ( owners , threshold , payment_token = payment_token , 
payment_receiver = payment_receiver ) 
magic_gas : int = self . _calculate_gas ( owners , safe_setup_data , payment_token ) 
estimated_gas : int = self . _estimate_gas ( safe_setup_data , 
salt_nonce , payment_token , payment_receiver ) 
gas = max ( magic_gas , estimated_gas ) 
payment = self . _calculate_refund_payment ( gas , 
fixed_creation_cost , 
payment_token_eth_value ) 
payment = payment , payment_receiver = payment_receiver ) 
safe_address = self . calculate_create2_address ( safe_setup_data , salt_nonce ) 
return SafeCreate2Tx ( salt_nonce , owners , threshold , self . master_copy_address , self . proxy_factory_address , 
payment_receiver , payment_token , payment , gas , gas_price , payment_token_eth_value , 
fixed_creation_cost , safe_address , safe_setup_data ) 
~~ def _calculate_gas ( owners : List [ str ] , safe_setup_data : bytes , payment_token : str ) -> int : 
if payment_token != NULL_ADDRESS : 
~~~ payment_token_gas = 55000 
~~~ payment_token_gas = 0 
return base_gas + data_gas + payment_token_gas + len ( owners ) * gas_per_owner 
~~ def _estimate_gas ( self , initializer : bytes , salt_nonce : int , 
payment_token : str , payment_receiver : str ) -> int : 
gas : int = self . proxy_factory_contract . functions . createProxyWithNonce ( self . master_copy_address , 
initializer , salt_nonce ) . estimateGas ( ) 
payment : int = 1 
~~~ gas += self . w3 . eth . estimateGas ( { 'to' : payment_receiver , 'value' : payment } ) 
~~~ gas += 55000 
~~ return gas 
~~ def w3_tx ( self ) : 
safe_contract = get_safe_contract ( self . w3 , address = self . safe_address ) 
return safe_contract . functions . execTransaction ( 
self . to , 
self . value , 
self . data , 
self . operation , 
self . safe_tx_gas , 
self . data_gas , 
self . gas_price , 
self . gas_token , 
self . refund_receiver , 
self . signatures ) 
~~ def call ( self , tx_sender_address : Optional [ str ] = None , tx_gas : Optional [ int ] = None , 
if tx_sender_address : 
~~~ parameters [ 'from' ] = tx_sender_address 
~~ if tx_gas : 
~~~ parameters [ 'gas' ] = tx_gas 
~~~ success = self . w3_tx . call ( parameters , block_identifier = block_identifier ) 
if not success : 
~~ return success 
~~~ return self . _parse_vm_exception ( str ( exc ) ) 
return self . _parse_vm_exception ( str ( result ) ) 
~~ ~~ ~~ def execute ( self , 
tx_gas : Optional [ int ] = None , 
tx_gas_price : Optional [ int ] = None , 
tx_nonce : Optional [ int ] = None , 
tx_gas = tx_gas or ( self . safe_tx_gas + self . data_gas ) * 2 
tx_parameters = { 
'from' : tx_sender_address , 
'gas' : tx_gas , 
'gasPrice' : tx_gas_price , 
if tx_nonce is not None : 
~~~ tx_parameters [ 'nonce' ] = tx_nonce 
~~ self . tx = self . w3_tx . buildTransaction ( tx_parameters ) 
self . tx_hash = self . ethereum_client . send_unsigned_transaction ( self . tx , 
private_key = tx_sender_private_key , 
retry = True , 
return self . tx_hash , self . tx 
~~ async def write ( self , towrite : bytes , await_blocking = False ) : 
await self . _write ( towrite ) 
if await_blocking : 
~~~ return await self . flush ( ) 
~~ ~~ async def read ( self , num_bytes = 0 ) -> bytes : 
if num_bytes < 1 : 
~~~ num_bytes = self . in_waiting or 1 
~~ return await self . _read ( num_bytes ) 
~~ async def _read ( self , num_bytes ) -> bytes : 
~~~ if self . in_waiting < num_bytes : 
~~~ await asyncio . sleep ( self . _asyncio_sleep_time ) 
~~~ inbytes = self . _serial_instance . read ( num_bytes ) 
if not inbytes : 
~~~ return inbytes 
~~ ~~ ~~ ~~ async def readline ( self ) -> bytes : 
~~~ line = self . _serial_instance . readline ( ) 
if not line : 
~~ ~~ ~~ def send ( self , message ) : 
if message . has_bad_headers ( self . mail . default_sender ) : 
~~~ raise BadHeaderError 
~~ if message . date is None : 
~~~ message . date = time . time ( ) 
~~ sender = message . sender or self . mail . default_sender 
if self . host : 
~~~ self . host . sendmail ( sanitize_address ( sender ) if sender is not None else None , 
message . send_to , 
message . as_string ( self . mail . default_sender ) , 
message . mail_options , 
message . rcpt_options ) 
~~ email_dispatched . send ( message , mail = self . mail ) 
self . num_emails += 1 
if self . num_emails == self . mail . max_emails : 
~~~ self . num_emails = 0 
~~~ self . host . quit ( ) 
self . host = self . configure_host ( ) 
~~ ~~ ~~ def _mimetext ( self , text , subtype = 'plain' ) : 
charset = self . charset or 'utf-8' 
return MIMEText ( text , _subtype = subtype , _charset = charset ) 
~~ def as_string ( self , default_from = None ) : 
encoding = self . charset or 'utf-8' 
attachments = self . attachments or [ ] 
if len ( attachments ) == 0 and not self . html : 
~~~ msg = self . _mimetext ( self . body ) 
~~ elif len ( attachments ) > 0 and not self . html : 
~~~ msg = MIMEMultipart ( ) 
msg . attach ( self . _mimetext ( self . body ) ) 
alternative = MIMEMultipart ( 'alternative' ) 
alternative . attach ( self . _mimetext ( self . body , 'plain' ) ) 
alternative . attach ( self . _mimetext ( self . html , 'html' ) ) 
msg . attach ( alternative ) 
~~ if self . charset : 
~~~ msg [ 'Subject' ] = Header ( self . subject , encoding ) 
~~~ msg [ 'Subject' ] = self . subject 
~~ sender = self . sender or default_from 
if sender is not None : 
~~~ msg [ 'From' ] = sanitize_address ( sender , encoding ) 
msg [ 'Date' ] = formatdate ( self . date , localtime = True ) 
msg [ 'Message-ID' ] = self . msgId 
if self . cc : 
~~ if self . reply_to : 
~~~ msg [ 'Reply-To' ] = sanitize_address ( self . reply_to , encoding ) 
~~ if self . extra_headers : 
~~~ for k , v in self . extra_headers . items ( ) : 
~~~ msg [ k ] = v 
~~ ~~ for attachment in attachments : 
~~~ f = MIMEBase ( * attachment . content_type . split ( '/' ) ) 
f . set_payload ( attachment . data ) 
encode_base64 ( f ) 
~~~ attachment . filename and attachment . filename . encode ( 'ascii' ) 
~~ except UnicodeEncodeError : 
~~~ filename = attachment . filename 
if not PY3 : 
~~~ filename = filename . encode ( 'utf8' ) 
~~ f . add_header ( 'Content-Disposition' , attachment . disposition , 
filename = ( 'UTF8' , '' , filename ) ) 
~~~ f . add_header ( 'Content-Disposition' , '%s;filename=%s' % 
( attachment . disposition , attachment . filename ) ) 
~~ for key , value in attachment . headers : 
~~~ f . add_header ( key , value ) 
~~ msg . attach ( f ) 
~~ return msg . as_string ( ) 
~~ def has_bad_headers ( self , default_from = None ) : 
sender = self . sender or default_from 
reply_to = self . reply_to or '' 
for val in [ self . subject , sender , reply_to ] + self . recipients : 
~~~ for c in '\\r\\n' : 
~~~ if c in val : 
~~ def attach ( self , 
filename = None , 
content_type = None , 
disposition = None , 
headers = None ) : 
self . attachments . append ( 
Attachment ( filename , content_type , data , disposition , headers ) ) 
~~ def record_messages ( self ) : 
if not email_dispatched : 
~~ outbox = [ ] 
def _record ( message , mail ) : 
~~~ outbox . append ( message ) 
~~ email_dispatched . connect ( _record ) 
~~~ yield outbox 
~~~ email_dispatched . disconnect ( _record ) 
~~ ~~ def ng ( self , wavelength ) : 
return self . n ( wavelength ) - ( wavelength * 1.e-9 ) * self . nDer1 ( wavelength ) 
~~ def gvd ( self , wavelength ) : 
g = ( wavelength * 1.e-9 ) ** 3. / ( 2. * spc . pi * spc . c ** 2. ) * self . nDer2 ( wavelength ) 
return g 
~~ def _cauchy_equation ( wavelength , coefficients ) : 
n = 0. 
for i , c in enumerate ( coefficients ) : 
~~~ exponent = 2 * i 
n += c / wavelength ** exponent 
~~ return n 
bc = BackendUpdate ( ) 
bc . initialize ( ) 
logger . debug ( "~~~~~~~~~~~~~~~~~~~~~~~~~~~~" ) 
success = False 
if bc . item_type and bc . action == 'list' : 
~~~ success = bc . get_resource_list ( bc . item_type , bc . item ) 
~~ if bc . item_type and bc . action == 'get' : 
~~~ if bc . list : 
~~~ if not bc . item : 
exit ( 64 ) 
~~ success = bc . get_resource ( bc . item_type , bc . item ) 
~~ ~~ if bc . action in [ 'add' , 'update' ] : 
~~~ success = bc . create_update_resource ( bc . item_type , bc . item , bc . action == 'update' ) 
~~ if bc . action == 'delete' : 
~~~ success = bc . delete_resource ( bc . item_type , bc . item ) 
~~ if not success : 
if not bc . verbose : 
~~ exit ( 2 ) 
~~ exit ( 0 ) 
~~ def initialize ( self ) : 
~~~ logger . info ( "Authenticating..." ) 
self . backend = Backend ( self . backend_url ) 
self . backend . login ( self . username , self . password ) 
~~ if self . backend . token is None : 
print ( "~~~~~~~~~~~~~~~~~~~~~~~~~~" ) 
exit ( 1 ) 
~~ logger . info ( "Authenticated." ) 
users = self . backend . get_all ( 'user' , { 'where' : json . dumps ( { 'name' : self . username } ) } ) 
self . logged_in_user = users [ '_items' ] [ 0 ] 
self . default_realm = self . logged_in_user [ '_realm' ] 
self . realm_all = None 
realms = self . backend . get_all ( 'realm' ) 
for r in realms [ '_items' ] : 
~~~ if r [ 'name' ] == 'All' and r [ '_level' ] == 0 : 
~~~ self . realm_all = r [ '_id' ] 
~~ if r [ '_id' ] == self . default_realm : 
~~ ~~ self . tp_always = None 
self . tp_never = None 
timeperiods = self . backend . get_all ( 'timeperiod' ) 
for tp in timeperiods [ '_items' ] : 
~~~ if tp [ 'name' ] == '24x7' : 
~~~ self . tp_always = tp [ '_id' ] 
~~ if tp [ 'name' ] . lower ( ) == 'none' or tp [ 'name' ] . lower ( ) == 'never' : 
~~~ self . tp_never = tp [ '_id' ] 
dump = json . dumps ( data , indent = 4 , 
path = os . path . join ( self . folder or os . getcwd ( ) , filename ) 
~~~ dfile = open ( path , "wt" ) 
dfile . write ( dump ) 
dfile . close ( ) 
return path 
~~ def get_resource_list ( self , resource_name , name = '' ) : 
if resource_name in [ 'host' , 'service' , 'user' ] : 
~~~ params = { 'where' : json . dumps ( { '_is_template' : self . model } ) } 
~~ if resource_name == 'service' and name and '/' in name : 
~~~ splitted_name = name . split ( '/' ) 
response2 = self . backend . get ( 
'host' , params = { 'where' : json . dumps ( { 'name' : splitted_name [ 0 ] , 
'_is_template' : self . model } ) } ) 
if response2 [ '_items' ] : 
~~~ host = response2 [ '_items' ] [ 0 ] 
splitted_name [ 0 ] , splitted_name [ 1 ] ) 
~~ params = { 'where' : json . dumps ( { 'host' : host [ '_id' ] } ) } 
~~ if self . embedded and resource_name in self . embedded_resources : 
~~~ params . update ( { 'embedded' : json . dumps ( self . embedded_resources [ resource_name ] ) } ) 
~~ rsp = self . backend . get_all ( resource_name , params = params ) 
if rsp [ '_items' ] and rsp [ '_status' ] == 'OK' : 
~~~ response = rsp [ '_items' ] 
if not self . dry_run : 
for item in response : 
~~~ for field in list ( item ) : 
~~~ if field in [ '_created' , '_updated' , '_etag' , '_links' , '_status' ] : 
~~~ item . pop ( field ) 
~~ if self . embedded and resource_name in self . embedded_resources and field in self . embedded_resources [ resource_name ] : 
~~~ embedded_items = item [ field ] 
if not isinstance ( item [ field ] , list ) : 
~~~ embedded_items = [ item [ field ] ] 
~~ for embedded_item in embedded_items : 
~~~ if not embedded_item : 
~~ for embedded_field in list ( embedded_item ) : 
~~~ if embedded_field . startswith ( '_' ) : 
~~~ embedded_item . pop ( embedded_field ) 
~~ ~~ ~~ ~~ ~~ ~~ filename = self . file_dump ( response , 'alignak-%s-list-%ss.json' 
% ( 'model' if self . model else 'object' , 
resource_name ) ) 
filename = self . file_dump ( [ ] , 'alignak-%s-list-%ss.json' 
~~ ~~ def get_resource ( self , resource_name , name ) : 
services_list = False 
if resource_name == 'host' and '/' in name : 
services_list = True 
name = splitted_name [ 0 ] 
~~ params = { 'where' : json . dumps ( { 'name' : name } ) } 
~~~ params = { 'where' : json . dumps ( { 'name' : name , '_is_template' : self . model } ) } 
~~ if resource_name == 'service' and '/' in name : 
'host' , params = { 'where' : json . dumps ( { 'name' : splitted_name [ 0 ] } ) } ) 
~~ params = { 'where' : json . dumps ( { 'name' : splitted_name [ 1 ] , 
'host' : host [ '_id' ] , 
'_is_template' : self . model } ) } 
~~ response = self . backend . get ( resource_name , params = params ) 
if response [ '_items' ] : 
~~~ response = response [ '_items' ] [ 0 ] 
if services_list : 
~~~ params = { 'where' : json . dumps ( { 'host' : response [ '_id' ] } ) } 
if self . embedded and 'service' in self . embedded_resources : 
~~~ params . update ( 
{ 'embedded' : json . dumps ( self . embedded_resources [ 'service' ] ) } ) 
~~ response2 = self . backend . get ( 'service' , params = params ) 
~~~ response [ '_services' ] = response2 [ '_items' ] 
len ( response2 [ '_items' ] ) , splitted_name [ 0 ] ) 
~~ ~~ if not self . dry_run : 
for field in list ( response ) : 
~~~ response . pop ( field ) 
embedded_items = response [ field ] 
if not isinstance ( response [ field ] , list ) : 
~~~ embedded_items = [ response [ field ] ] 
~~ ~~ ~~ ~~ ~~ dump = json . dumps ( response , indent = 4 , 
if not self . quiet : 
~~~ print ( dump ) 
~~~ name = splitted_name [ 0 ] + '_' + splitted_name [ 1 ] 
~~ filename = self . file_dump ( response , 
'alignak-object-dump-%s-%s.json' 
% ( resource_name , name ) ) 
~~~ if resource_name == 'service' and '/' in name : 
resource_name , name ) 
~~ ~~ def delete_resource ( self , resource_name , name ) : 
~~~ if not self . dry_run : 
~~~ headers = { 
'Content-Type' : 'application/json' 
self . backend . delete ( resource_name , headers ) 
~~~ response = { '_id' : '_fake' , '_etag' : '_fake' } 
~~~ params = { 'where' : json . dumps ( { 'name' : name } ) } 
name = splitted_name [ 0 ] + '_' + splitted_name [ 1 ] 
~~ if splitted_name [ 1 ] == '*' : 
~~~ params = { 'where' : json . dumps ( { 'host' : host [ '_id' ] } ) } 
~~~ params = { 'where' : json . dumps ( { 'name' : splitted_name [ 1 ] , 
'host' : host [ '_id' ] } ) } 
~~ ~~ response = self . backend . get_all ( resource_name , params = params ) 
for item in response [ '_items' ] : 
'Content-Type' : 'application/json' , 
'If-Match' : item [ '_etag' ] 
self . backend . delete ( resource_name + '/' + item [ '_id' ] , headers ) 
resource_name , item [ '_id' ] ) 
~~ def create_update_resource ( self , resource_name , name , update = False ) : 
if self . data is None : 
~~~ self . data = { } 
~~ json_data = None 
if self . data : 
~~~ if self . data == 'stdin' : 
~~~ input_file = sys . stdin 
~~~ path = os . path . join ( self . folder or os . getcwd ( ) , self . data ) 
input_file = open ( path ) 
~~ json_data = json . load ( input_file ) 
if input_file is not sys . stdin : 
~~~ input_file . close ( ) 
~~ ~~ except IOError : 
~~ ~~ if name is None and json_data is None : 
~~ used_templates = [ ] 
if self . templates is not None : 
resource_name , self . templates ) 
for template in self . templates : 
~~~ response = self . backend . get ( 
resource_name , params = { 'where' : json . dumps ( { 'name' : template , 
'_is_template' : True } ) } ) 
~~~ used_templates . append ( response [ '_items' ] [ 0 ] [ '_id' ] ) 
resource_name , template , response [ '_items' ] [ 0 ] [ '_id' ] ) 
~~~ if json_data is None : 
~~~ json_data = { 'name' : name } 
~~ if not isinstance ( json_data , list ) : 
~~~ json_data = [ json_data ] 
for json_item in json_data : 
if resource_name not in [ 'history' , 'userrestrictrole' , 'logcheckresult' ] and name is None and ( 'name' not in json_item or not json_item [ 'name' ] ) : 
~~ item_name = name 
if 'name' in json_item : 
~~~ item_name = json_item [ 'name' ] 
~~ params = { 'name' : item_name } 
if resource_name == 'service' and 'host' in json_item : 
~~~ host_search = { 'name' : json_item [ 'host' ] } 
if '_is_template' in json_item : 
~~~ host_search . update ( { '_is_template' : json_item [ '_is_template' ] } ) 
resp_host = self . backend . get ( 
'host' , params = { 'where' : json . dumps ( host_search ) } ) 
if resp_host [ '_items' ] : 
~~~ host = resp_host [ '_items' ] [ 0 ] 
json_item [ 'host' ] , item_name ) 
~~ params = { 'name' : item_name , 'host' : host [ '_id' ] } 
~~ if resource_name == 'service' and '/' in item_name : 
~~~ splitted_name = item_name . split ( '/' ) 
host_search = { 'name' : splitted_name [ 0 ] } 
~~ resp_host = self . backend . get ( 
splitted_name [ 0 ] , item_name ) 
~~ item_name = splitted_name [ 1 ] 
params = { 'name' : item_name , 'host' : host [ '_id' ] } 
~~ if '_is_template' in json_item : 
~~~ params . update ( { '_is_template' : json_item [ '_is_template' ] } ) 
~~ params = { 'where' : json . dumps ( params ) } 
resource_name , item_name , params ) 
response = self . backend . get ( resource_name , params = params ) 
~~~ found_item = response [ '_items' ] [ 0 ] 
found_id = found_item [ '_id' ] 
found_etag = found_item [ '_etag' ] 
if not update : 
"exists!" , resource_name , item_name ) 
"exist!" , resource_name , item_name ) 
~~ ~~ ~~ item_data = { } 
if self . include_read_data : 
~~~ item_data = found_item 
~~ item_data . update ( json_item ) 
item_data [ 'name' ] = item_name 
if used_templates : 
~~~ item_data . update ( { '_templates' : used_templates , 
'_templates_with_services' : True } ) 
~~ for field in item_data . copy ( ) : 
if field in [ '_created' , '_updated' , '_etag' , '_links' , '_status' ] : 
~~~ item_data . pop ( field ) 
~~ if field in [ '_overall_state_id' ] : 
~~ if field not in [ 'realm' , '_realm' , '_templates' , 
'command' , 'host' , 'service' , 
'escalation_period' , 'maintenance_period' , 
'snapshot_period' , 'check_period' , 'dependency_period' , 
'notification_period' , 'host_notification_period' , 
'escalation_period' , 'service_notification_period' , 
'host_notification_commands' , 'service_notification_commands' , 
'service_dependencies' , 'users' , 'usergroups' , 
'check_command' , 'event_handler' , 'grafana' , 'statsd' ] : 
~~ field_values = item_data [ field ] 
if not isinstance ( item_data [ field ] , list ) : 
~~~ field_values = [ item_data [ field ] ] 
~~ found = None 
for value in field_values : 
~~~ int ( value , 16 ) 
~~~ found = value 
~~~ if found is None : 
~~~ found = [ ] 
~~ found . append ( value ) 
~~ ~~ except TypeError : 
~~~ field_params = { 'where' : json . dumps ( { 'name' : value } ) } 
if field in [ 'escalation_period' , 'maintenance_period' , 
'snapshot_period' , 'check_period' , 
'dependency_period' , 'notification_period' , 
'host_notification_period' , 
'service_notification_period' ] : 
~~~ response2 = self . backend . get ( 'timeperiod' , params = field_params ) 
~~ elif field in [ '_realm' ] : 
~~~ response2 = self . backend . get ( 'realm' , params = field_params ) 
~~ elif field in [ 'service_dependencies' ] : 
~~~ response2 = self . backend . get ( 'service' , params = field_params ) 
~~ elif field in [ 'users' ] : 
~~~ response2 = self . backend . get ( 'user' , params = field_params ) 
~~ elif field in [ 'usergroups' ] : 
~~~ response2 = self . backend . get ( 'usergroup' , params = field_params ) 
~~ elif field in [ 'check_command' , 'event_handler' , 
'service_notification_commands' , 
'host_notification_commands' ] : 
~~~ response2 = self . backend . get ( 'command' , params = field_params ) 
~~ elif field in [ '_templates' ] : 
~~~ field_params = { 'where' : json . dumps ( { 'name' : value , 
'_is_template' : True } ) } 
response2 = self . backend . get ( resource_name , params = field_params ) 
~~~ response2 = self . backend . get ( field , params = field_params ) 
~~ if response2 [ '_items' ] : 
~~~ response2 = response2 [ '_items' ] [ 0 ] 
field , value ) 
~~~ found = response2 [ '_id' ] 
~~ found . append ( response2 [ '_id' ] ) 
~~ ~~ ~~ ~~ if found is None : 
item_data . pop ( field ) 
~~~ item_data [ field ] = found 
~~ ~~ if resource_name not in [ 'realm' ] and '_realm' not in item_data : 
item_data . update ( { '_realm' : self . default_realm } ) 
~~ if resource_name in [ 'realm' ] and '_realm' not in item_data : 
item_data . update ( { '_parent' : self . default_realm } ) 
~~ if '_id' in item_data : 
~~~ item_data . pop ( '_id' ) 
~~ if not update : 
~~~ if not item_data [ 'name' ] : 
~~~ item_data . pop ( 'name' ) 
~~~ response = self . backend . post ( resource_name , item_data , headers = None ) 
~~ except BackendException as exp : 
~~~ self . item = item_name 
~~~ response = { '_status' : 'OK' , '_id' : '_fake' , '_etag' : '_fake' } 
~~~ if not name : 
~~~ headers = { 'Content-Type' : 'application/json' , 'If-Match' : found_etag } 
response = self . backend . patch ( resource_name + '/' + found_id , 
item_data , headers = headers , 
inception = True ) 
~~ ~~ if response [ '_status' ] == 'ERR' : 
~~ ~~ count = count + 1 
~~ if count == len ( json_data ) : 
~~ def get_response ( self , method , endpoint , headers = None , json = None , params = None , data = None ) : 
url = self . get_url ( endpoint ) 
~~~ response = self . session . request ( method = method , url = url , headers = headers , json = json , 
params = params , data = data , proxies = self . proxies , 
timeout = self . timeout ) 
~~ except RequestException as e : 
~~~ response = { "_status" : "ERR" , 
"_error" : { "message" : e , "code" : BACKEND_ERROR } , 
"_issues" : { "message" : e , "code" : BACKEND_ERROR } } 
raise BackendException ( code = BACKEND_ERROR , 
message = e , 
~~~ return response 
~~ ~~ def decode ( response ) : 
~~~ response . raise_for_status ( ) 
~~ except requests . HTTPError as e : 
~~~ raise BackendException ( code = response . status_code , 
~~~ resp_json = response . json ( ) 
error = resp_json . get ( '_error' , None ) 
if error : 
~~~ raise BackendException ( code = error [ 'code' ] , 
message = error [ 'message' ] , 
~~ return resp_json 
~~ ~~ def set_token ( self , token ) : 
if token : 
~~~ auth = HTTPBasicAuth ( token , '' ) 
self . _token = token 
self . session . auth = auth 
~~~ self . _token = None 
self . authenticated = False 
self . session . auth = None 
~~ ~~ def login ( self , username , password , generate = 'enabled' , proxies = None ) : 
if not username or not password : 
~~ if proxies : 
~~~ for key in proxies . keys ( ) : 
~~~ assert key in PROXY_PROTOCOLS 
~~ ~~ ~~ self . proxies = proxies 
endpoint = 'login' 
json = { u'username' : username , u'password' : password } 
if generate == 'force' : 
~~~ json [ 'action' ] = 'generate' 
~~ response = self . get_response ( method = 'POST' , endpoint = endpoint , json = json ) 
if response . status_code == 401 : 
self . set_token ( token = None ) 
~~ resp = self . decode ( response = response ) 
if 'token' in resp : 
~~~ self . set_token ( token = resp [ 'token' ] ) 
~~~ self . set_token ( token = None ) 
return self . login ( username , password , 'force' ) 
~~ def get_domains ( self ) : 
resp = self . get ( '' ) 
if "_links" in resp : 
~~~ _links = resp [ "_links" ] 
if "child" in _links : 
~~~ return _links [ "child" ] 
~~ ~~ return { } 
~~ def get_all ( self , endpoint , params = None ) : 
~~~ params = { 'max_results' : BACKEND_PAGINATION_LIMIT } 
~~ elif params and 'max_results' not in params : 
~~~ params [ 'max_results' ] = BACKEND_PAGINATION_LIMIT 
~~ last_page = False 
items = [ ] 
if self . processes == 1 : 
~~~ while not last_page : 
~~~ resp = self . get ( endpoint = endpoint , params = params ) 
if 'next' in resp [ '_links' ] : 
~~~ params [ 'page' ] = int ( resp [ '_meta' ] [ 'page' ] ) + 1 
params [ 'max_results' ] = int ( resp [ '_meta' ] [ 'max_results' ] ) 
~~~ last_page = True 
~~ items . extend ( resp [ '_items' ] ) 
~~~ def get_pages ( endpoint , params , pages , out_q ) : 
multi_items = [ ] 
for page in pages : 
~~~ params [ 'page' ] = page 
resp = self . get ( endpoint , params ) 
multi_items . extend ( resp [ '_items' ] ) 
~~ out_q . put ( multi_items ) 
~~ resp = self . get ( endpoint , params ) 
number_pages = int ( math . ceil ( 
float ( resp [ '_meta' ] [ 'total' ] ) / float ( resp [ '_meta' ] [ 'max_results' ] ) ) ) 
out_q = multiprocessing . Queue ( ) 
chunksize = int ( math . ceil ( number_pages / float ( self . processes ) ) ) 
procs = [ ] 
for i in range ( self . processes ) : 
~~~ begin = i * chunksize 
end = begin + chunksize 
if end > number_pages : 
~~~ end = number_pages 
~~ begin += 1 
end += 1 
p = multiprocessing . Process ( target = get_pages , 
args = ( endpoint , params , range ( begin , end ) , out_q ) ) 
procs . append ( p ) 
p . start ( ) 
~~ for i in range ( self . processes ) : 
~~~ items . extend ( out_q . get ( ) ) 
~~ for p in procs : 
~~~ p . join ( ) 
~~ ~~ return { 
'_items' : items , 
'_status' : 'OK' 
~~ def patch ( self , endpoint , data , headers = None , inception = False ) : 
if not headers : 
~~ response = self . get_response ( method = 'PATCH' , endpoint = endpoint , json = data , headers = headers ) 
if response . status_code == 200 : 
~~~ return self . decode ( response = response ) 
~~ if response . status_code == 412 : 
~~~ if inception : 
~~~ resp = self . get ( endpoint ) 
headers = { 'If-Match' : resp [ '_etag' ] } 
return self . patch ( endpoint , data = data , headers = headers , inception = False ) 
~~ raise BackendException ( response . status_code , response . content ) 
~~~ raise BackendException ( response . status_code , response . content ) 
~~ ~~ def delete ( self , endpoint , headers ) : 
response = self . get_response ( method = 'DELETE' , endpoint = endpoint , headers = headers ) 
~~~ resp = self . decode ( response = response ) 
~~ resp = { "_status" : "OK" } 
return resp 
~~ def samefile ( path1 , path2 ) : 
info1 = fs . getfileinfo ( path1 ) 
info2 = fs . getfileinfo ( path2 ) 
return ( info1 . dwVolumeSerialNumber == info2 . dwVolumeSerialNumber and 
info1 . nFileIndexHigh == info2 . nFileIndexHigh and 
info1 . nFileIndexLow == info2 . nFileIndexLow ) 
~~ def new_junction_reparse_buffer ( path = None ) : 
~~~ substnamebufferchars = 8000 
~~~ substnamebufferchars = len ( path ) + 1 
~~ class REPARSE_DATA_BUFFER ( ctypes . Structure ) : 
~~~ _fields_ = [ ( "ReparseTag" , ctypes . c_ulong ) , 
( "ReparseDataLength" , ctypes . c_ushort ) , 
( "Reserved" , ctypes . c_ushort ) , 
( "SubstituteNameOffset" , ctypes . c_ushort ) , 
( "SubstituteNameLength" , ctypes . c_ushort ) , 
( "PrintNameOffset" , ctypes . c_ushort ) , 
( "PrintNameLength" , ctypes . c_ushort ) , 
( "SubstituteNameBuffer" , ctypes . c_wchar * substnamebufferchars ) , 
( "PrintNameBuffer" , ctypes . c_wchar * 1 ) ] 
~~ numpathbytes = ( substnamebufferchars - 1 ) * sizeof ( ctypes . c_wchar ) 
buffersize = ( numpathbytes + ( sizeof ( ctypes . c_wchar ) * 2 ) + 
( sizeof ( ctypes . c_ushort ) * 4 ) ) 
~~~ buffer = REPARSE_DATA_BUFFER ( ) 
buffer . ReparseTag = IO_REPARSE_TAG_MOUNT_POINT 
~~~ buffer = REPARSE_DATA_BUFFER ( 
IO_REPARSE_TAG_MOUNT_POINT , 
buffersize , 
0 , numpathbytes , 
numpathbytes + 2 , 0 , 
"" ) 
~~ return ( buffer , buffersize + REPARSE_DATA_BUFFER . SubstituteNameOffset . offset ) 
~~ def create ( source , link_name ) : 
if not os . path . isdir ( source ) : 
~~ if os . path . exists ( link_name ) : 
~~ link_name = os . path . abspath ( link_name ) 
os . mkdir ( link_name ) 
hlink = CreateFile ( link_name , fs . GENERIC_WRITE , 
fs . FILE_SHARE_READ | fs . FILE_SHARE_WRITE , None , fs . OPEN_EXISTING , 
fs . FILE_FLAG_OPEN_REPARSE_POINT | fs . FILE_FLAG_BACKUP_SEMANTICS , 
None ) 
~~~ if hlink == fs . INVALID_HANDLE_VALUE : 
~~~ raise WinError ( ) 
~~ srcvolpath = unparsed_convert ( source ) 
( junctioninfo , infolen ) = new_junction_reparse_buffer ( srcvolpath ) 
dummy = DWORD ( 0 ) 
res = DeviceIoControl ( 
hlink , 
FSCTL_SET_REPARSE_POINT , 
byref ( junctioninfo ) , 
infolen , 
None , 
byref ( dummy ) , 
if res == 0 : 
~~ success = True 
~~~ if hlink != fs . INVALID_HANDLE_VALUE : 
~~~ CloseHandle ( hlink ) 
~~~ os . rmdir ( link_name ) 
~~ ~~ ~~ def getvolumeinfo ( path ) : 
volpath = ctypes . create_unicode_buffer ( len ( path ) + 2 ) 
rv = GetVolumePathName ( path , volpath , len ( volpath ) ) 
if rv == 0 : 
~~ fsnamebuf = ctypes . create_unicode_buffer ( MAX_PATH + 1 ) 
fsflags = DWORD ( 0 ) 
rv = GetVolumeInformation ( volpath , None , 0 , None , None , byref ( fsflags ) , 
fsnamebuf , len ( fsnamebuf ) ) 
~~ return ( fsnamebuf . value , fsflags . value ) 
~~ def get_variant_phenotypes_with_suggested_changes ( variant_id_list ) : 
variants = civic . get_variants_by_ids ( variant_id_list ) 
evidence = list ( ) 
for variant in variants : 
~~~ evidence . extend ( variant . evidence ) 
~~ for e in evidence : 
~~~ suggested_changes_url = f'https://civicdb.org/api/evidence_items/{e.id}/suggested_changes' 
resp = requests . get ( suggested_changes_url ) 
resp . raise_for_status ( ) 
suggested_changes = dict ( ) 
for suggested_change in resp . json ( ) : 
~~~ pheno_changes = suggested_change [ 'suggested_changes' ] . get ( 'phenotype_ids' , None ) 
if pheno_changes is None : 
~~ a , b = pheno_changes 
added = set ( b ) - set ( a ) 
deleted = set ( a ) - set ( b ) 
rid = suggested_change [ 'id' ] 
suggested_changes [ rid ] = { 'added' : added , 'deleted' : deleted } 
~~ yield e , { 'suggested_changes' : suggested_changes , 'current' : set ( [ x . id for x in e . phenotypes ] ) } 
~~ ~~ def get_variant_phenotypes_with_suggested_changes_merged ( variant_id_list ) : 
for evidence , phenotype_status in get_variant_phenotypes_with_suggested_changes ( variant_id_list ) : 
~~~ final = phenotype_status [ 'current' ] 
for rid in sorted ( phenotype_status [ 'suggested_changes' ] ) : 
~~~ changes = phenotype_status [ 'suggested_changes' ] [ rid ] 
final = final - changes [ 'deleted' ] 
final = final | changes [ 'added' ] 
~~ if final : 
~~~ yield evidence , final 
~~ ~~ ~~ def search_variants_by_coordinates ( coordinate_query , search_mode = 'any' ) : 
get_all_variants ( ) 
ct = COORDINATE_TABLE 
start_idx = COORDINATE_TABLE_START 
stop_idx = COORDINATE_TABLE_STOP 
chr_idx = COORDINATE_TABLE_CHR 
start = int ( coordinate_query . start ) 
stop = int ( coordinate_query . stop ) 
chromosome = str ( coordinate_query . chr ) 
left_idx = chr_idx . searchsorted ( chromosome ) 
right_idx = chr_idx . searchsorted ( chromosome , side = 'right' ) 
chr_ct_idx = chr_idx [ left_idx : right_idx ] . index 
right_idx = start_idx . searchsorted ( stop , side = 'right' ) 
start_ct_idx = start_idx [ : right_idx ] . index 
left_idx = stop_idx . searchsorted ( start ) 
stop_ct_idx = stop_idx [ left_idx : ] . index 
match_idx = chr_ct_idx & start_ct_idx & stop_ct_idx 
m_df = ct . loc [ match_idx , ] 
if search_mode == 'any' : 
~~~ var_digests = m_df . v_hash . to_list ( ) 
return [ CACHE [ v ] for v in var_digests ] 
~~ elif search_mode == 'include_smaller' : 
~~~ match_idx = ( start <= m_df . start ) & ( stop >= m_df . stop ) 
~~ elif search_mode == 'include_larger' : 
~~~ match_idx = ( start >= m_df . start ) & ( stop <= m_df . stop ) 
~~ elif search_mode == 'exact' : 
~~~ match_idx = ( start == m_df . stop ) & ( stop == m_df . start ) 
if coordinate_query . alt : 
~~~ match_idx = match_idx & ( coordinate_query . alt == m_df . alt ) 
~~ var_digests = m_df . loc [ match_idx , ] . v_hash . to_list ( ) 
~~ def bulk_search_variants_by_coordinates ( sorted_queries , search_mode = 'any' ) : 
def is_sorted ( prev_q , current_q ) : 
~~~ if prev_q [ 'chr' ] < current_q [ 'chr' ] : 
~~ if prev_q [ 'chr' ] > current_q [ 'chr' ] : 
~~ if prev_q [ 'start' ] < current_q [ 'start' ] : 
~~ if prev_q [ 'start' ] > current_q [ 'start' ] : 
~~ if prev_q [ 'stop' ] < current_q [ 'stop' ] : 
~~ if prev_q [ 'stop' ] > current_q [ 'stop' ] : 
~~ ct_pointer = 0 
query_pointer = 0 
last_query_pointer = - 1 
match_start = None 
ct = MODULE . COORDINATE_TABLE 
matches = defaultdict ( list ) 
Match = namedtuple ( 'Match' , ct . columns ) 
while query_pointer < len ( sorted_queries ) and ct_pointer < len ( ct ) : 
~~~ if last_query_pointer != query_pointer : 
~~~ q = sorted_queries [ query_pointer ] 
if match_start is not None : 
~~~ ct_pointer = match_start 
~~ last_query_pointer = query_pointer 
~~ c = ct . iloc [ ct_pointer ] 
q_chr = str ( q . chr ) 
c_chr = c . chr 
if q_chr < c_chr : 
~~~ query_pointer += 1 
~~ if q_chr > c_chr : 
~~~ ct_pointer += 1 
~~ q_start = int ( q . start ) 
c_start = c . start 
q_stop = int ( q . stop ) 
c_stop = c . stop 
if q_start > c_stop : 
~~ if q_stop < c_start : 
~~ if search_mode == 'any' : 
~~~ matches [ q ] . append ( c . to_dict ( ) ) 
~~ elif search_mode == 'exact' and q_start == c_start and q_stop == c_stop : 
~~~ q_alt = q . alt 
c_alt = c . alt 
if not ( q_alt and c_alt and q_alt != c_alt ) : 
~~~ matches [ q ] . append ( Match ( ** c . to_dict ( ) ) ) 
~~ ~~ elif search_mode == 'include_smaller' : 
~~~ raise NotImplementedError 
~~ if match_start is None : 
~~~ match_start = ct_pointer 
~~ ct_pointer += 1 
~~ return dict ( matches ) 
~~ def update ( self , allow_partial = True , force = False , ** kwargs ) : 
if kwargs : 
~~~ self . __init__ ( partial = allow_partial , force = force , ** kwargs ) 
return not self . _partial 
~~ if not force and CACHE . get ( hash ( self ) ) : 
~~~ cached = CACHE [ hash ( self ) ] 
for field in self . _SIMPLE_FIELDS | self . _COMPLEX_FIELDS : 
~~~ v = getattr ( cached , field ) 
setattr ( self , field , v ) 
~~ self . _partial = False 
~~ resp_dict = element_lookup_by_id ( self . type , self . id ) 
self . __init__ ( partial = False , ** resp_dict ) 
~~ def ekm_log ( logstr , priority = 3 ) : 
if priority <= ekmmeters_log_level : 
~~~ dt = datetime . datetime 
~~ def initPort ( self ) : 
~~~ self . m_ser = serial . Serial ( port = self . m_ttyport , 
baudrate = self . m_baudrate , 
timeout = 0 , 
parity = serial . PARITY_EVEN , 
stopbits = serial . STOPBITS_ONE , 
bytesize = serial . SEVENBITS , 
rtscts = False ) 
time . sleep ( self . m_init_wait ) 
~~~ ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) 
~~ def write ( self , output ) : 
view_str = output . encode ( 'ascii' , 'ignore' ) 
if ( len ( view_str ) > 0 ) : 
~~~ self . m_ser . write ( view_str ) 
self . m_ser . flush ( ) 
self . m_ser . reset_input_buffer ( ) 
time . sleep ( self . m_force_wait ) 
~~ def setPollingValues ( self , max_waits , wait_sleep ) : 
self . m_max_waits = max_waits 
self . m_wait_sleep = wait_sleep 
~~ def getResponse ( self , context = "" ) : 
while ( waits < self . m_max_waits ) : 
~~~ bytes_to_read = self . m_ser . inWaiting ( ) 
if bytes_to_read > 0 : 
~~~ next_chunk = str ( self . m_ser . read ( bytes_to_read ) ) . encode ( 'ascii' , 'ignore' ) 
response_str += next_chunk 
if ( len ( response_str ) == 255 ) : 
~~~ time . sleep ( self . m_force_wait ) 
return response_str 
~~ if ( len ( response_str ) == 1 ) and ( response_str . encode ( 'hex' ) == '06' ) : 
~~~ waits += 1 
~~ ~~ response_str = "" 
~~ return response_str 
~~ def combineAB ( self ) : 
v4definition_meter = V4Meter ( ) 
v4definition_meter . makeAB ( ) 
defv4 = v4definition_meter . getReadBuffer ( ) 
v3definition_meter = V3Meter ( ) 
v3definition_meter . makeReturnFormat ( ) 
defv3 = v3definition_meter . getReadBuffer ( ) 
for fld in defv3 : 
~~~ if fld not in self . m_all_fields : 
~~~ compare_fld = fld . upper ( ) 
if not "RESERVED" in compare_fld and not "CRC" in compare_fld : 
~~~ self . m_all_fields [ fld ] = defv3 [ fld ] 
~~ ~~ ~~ for fld in defv4 : 
~~~ self . m_all_fields [ fld ] = defv4 [ fld ] 
~~ ~~ ~~ pass 
~~ def mapTypeToSql ( fld_type = FieldType . NoType , fld_len = 0 ) : 
if fld_type == FieldType . Float : 
~~~ return "FLOAT" 
~~ elif fld_type == FieldType . String : 
~~~ return "VARCHAR(" + str ( fld_len ) + ")" 
~~ elif fld_type == FieldType . Int : 
~~~ return "INT" 
~~ elif fld_type == FieldType . Hex : 
~~~ return "VARCHAR(" + str ( fld_len * 2 ) + ")" 
~~ elif fld_type == FieldType . PowerFactor : 
return "VARCHAR(255)" 
~~ ~~ def fillCreate ( self , qry_str ) : 
for fld in self . m_all_fields : 
~~~ fld_type = self . m_all_fields [ fld ] [ MeterData . TypeValue ] 
fld_len = self . m_all_fields [ fld ] [ MeterData . SizeValue ] 
qry_spec = self . mapTypeToSql ( fld_type , fld_len ) 
if count > 0 : 
return qry_str 
~~ def sqlCreate ( self ) : 
qry_str = self . fillCreate ( qry_str ) 
ekm_log ( qry_str , 4 ) 
~~ def sqlInsert ( def_buf , raw_a , raw_b ) : 
for fld in def_buf : 
~~~ if count > 0 : 
~~ qry_str = qry_str + fld 
"Raw_A,\\n\\t" + 
~~ fld_type = def_buf [ fld ] [ MeterData . TypeValue ] 
fld_str_content = def_buf [ fld ] [ MeterData . StringValue ] 
delim = "" 
if ( fld_type == FieldType . Hex ) or ( fld_type == FieldType . String ) or ( fld_type == FieldType . PowerFactor ) : 
~~~ delim = "\ 
~~ qry_str = qry_str + delim + fld_str_content + delim 
~~ time_val = int ( time . time ( ) * 1000 ) 
qry_str = ( qry_str + ",\\n\\t" + str ( time_val ) + ",\\n\\t\ + 
binascii . b2a_hex ( raw_a ) + "\ + ",\\n\\t\ + 
binascii . b2a_hex ( raw_b ) + "\ ) 
~~ def dbInsert ( self , def_buf , raw_a , raw_b ) : 
self . dbExec ( self . sqlInsert ( def_buf , raw_a , raw_b ) ) 
~~ def dbExec ( self , query_str ) : 
~~~ connection = sqlite3 . connect ( self . m_connection_string ) 
cursor = connection . cursor ( ) 
cursor . execute ( query_str ) 
connection . commit ( ) 
cursor . close ( ) 
connection . close ( ) 
~~ def dict_factory ( self , cursor , row ) : 
d = { } 
for idx , col in enumerate ( cursor . description ) : 
~~~ val = row [ idx ] 
name = col [ 0 ] 
if name == Field . Time_Stamp : 
~~~ d [ col [ 0 ] ] = str ( val ) 
~~ if name not in self . m_all_fields : 
~~ if ( str ( val ) != "None" ) and ( ( val > 0 ) or ( val < 0 ) ) : 
~~~ d [ name ] = str ( val ) 
~~ ~~ return d 
~~ def raw_dict_factory ( cursor , row ) : 
if name == Field . Time_Stamp or name == Field . Meter_Address : 
~~ if name == "Raw_A" or name == "Raw_B" : 
~~ def renderJsonReadsSince ( self , timestamp , meter ) : 
result = "" 
connection . row_factory = self . dict_factory 
select_cursor = connection . cursor ( ) 
reads = select_cursor . fetchall ( ) 
result = json . dumps ( reads , indent = 4 ) 
~~ def setContext ( self , context_str ) : 
if ( len ( self . m_context ) == 0 ) and ( len ( context_str ) >= 7 ) : 
~~~ if context_str [ 0 : 7 ] != "request" : 
~~ ~~ self . m_context = context_str 
~~ def calc_crc16 ( buf ) : 
crc_table = [ 0x0000 , 0xc0c1 , 0xc181 , 0x0140 , 0xc301 , 0x03c0 , 0x0280 , 0xc241 , 
0xc601 , 0x06c0 , 0x0780 , 0xc741 , 0x0500 , 0xc5c1 , 0xc481 , 0x0440 , 
0xcc01 , 0x0cc0 , 0x0d80 , 0xcd41 , 0x0f00 , 0xcfc1 , 0xce81 , 0x0e40 , 
0x0a00 , 0xcac1 , 0xcb81 , 0x0b40 , 0xc901 , 0x09c0 , 0x0880 , 0xc841 , 
0xd801 , 0x18c0 , 0x1980 , 0xd941 , 0x1b00 , 0xdbc1 , 0xda81 , 0x1a40 , 
0x1e00 , 0xdec1 , 0xdf81 , 0x1f40 , 0xdd01 , 0x1dc0 , 0x1c80 , 0xdc41 , 
0x1400 , 0xd4c1 , 0xd581 , 0x1540 , 0xd701 , 0x17c0 , 0x1680 , 0xd641 , 
0xd201 , 0x12c0 , 0x1380 , 0xd341 , 0x1100 , 0xd1c1 , 0xd081 , 0x1040 , 
0xf001 , 0x30c0 , 0x3180 , 0xf141 , 0x3300 , 0xf3c1 , 0xf281 , 0x3240 , 
0x3600 , 0xf6c1 , 0xf781 , 0x3740 , 0xf501 , 0x35c0 , 0x3480 , 0xf441 , 
0x3c00 , 0xfcc1 , 0xfd81 , 0x3d40 , 0xff01 , 0x3fc0 , 0x3e80 , 0xfe41 , 
0xfa01 , 0x3ac0 , 0x3b80 , 0xfb41 , 0x3900 , 0xf9c1 , 0xf881 , 0x3840 , 
0x2800 , 0xe8c1 , 0xe981 , 0x2940 , 0xeb01 , 0x2bc0 , 0x2a80 , 0xea41 , 
0xee01 , 0x2ec0 , 0x2f80 , 0xef41 , 0x2d00 , 0xedc1 , 0xec81 , 0x2c40 , 
0xe401 , 0x24c0 , 0x2580 , 0xe541 , 0x2700 , 0xe7c1 , 0xe681 , 0x2640 , 
0x2200 , 0xe2c1 , 0xe381 , 0x2340 , 0xe101 , 0x21c0 , 0x2080 , 0xe041 , 
0xa001 , 0x60c0 , 0x6180 , 0xa141 , 0x6300 , 0xa3c1 , 0xa281 , 0x6240 , 
0x6600 , 0xa6c1 , 0xa781 , 0x6740 , 0xa501 , 0x65c0 , 0x6480 , 0xa441 , 
0x6c00 , 0xacc1 , 0xad81 , 0x6d40 , 0xaf01 , 0x6fc0 , 0x6e80 , 0xae41 , 
0xaa01 , 0x6ac0 , 0x6b80 , 0xab41 , 0x6900 , 0xa9c1 , 0xa881 , 0x6840 , 
0x7800 , 0xb8c1 , 0xb981 , 0x7940 , 0xbb01 , 0x7bc0 , 0x7a80 , 0xba41 , 
0xbe01 , 0x7ec0 , 0x7f80 , 0xbf41 , 0x7d00 , 0xbdc1 , 0xbc81 , 0x7c40 , 
0xb401 , 0x74c0 , 0x7580 , 0xb541 , 0x7700 , 0xb7c1 , 0xb681 , 0x7640 , 
0x7200 , 0xb2c1 , 0xb381 , 0x7340 , 0xb101 , 0x71c0 , 0x7080 , 0xb041 , 
0x5000 , 0x90c1 , 0x9181 , 0x5140 , 0x9301 , 0x53c0 , 0x5280 , 0x9241 , 
0x9601 , 0x56c0 , 0x5780 , 0x9741 , 0x5500 , 0x95c1 , 0x9481 , 0x5440 , 
0x9c01 , 0x5cc0 , 0x5d80 , 0x9d41 , 0x5f00 , 0x9fc1 , 0x9e81 , 0x5e40 , 
0x5a00 , 0x9ac1 , 0x9b81 , 0x5b40 , 0x9901 , 0x59c0 , 0x5880 , 0x9841 , 
0x8801 , 0x48c0 , 0x4980 , 0x8941 , 0x4b00 , 0x8bc1 , 0x8a81 , 0x4a40 , 
0x4e00 , 0x8ec1 , 0x8f81 , 0x4f40 , 0x8d01 , 0x4dc0 , 0x4c80 , 0x8c41 , 
0x4400 , 0x84c1 , 0x8581 , 0x4540 , 0x8701 , 0x47c0 , 0x4680 , 0x8641 , 
0x8201 , 0x42c0 , 0x4380 , 0x8341 , 0x4100 , 0x81c1 , 0x8081 , 0x4040 ] 
crc = 0xffff 
for c in buf : 
~~~ index = ( crc ^ ord ( c ) ) & 0xff 
crct = crc_table [ index ] 
crc = ( crc >> 8 ) ^ crct 
~~ crc = ( crc << 8 ) | ( crc >> 8 ) 
crc &= 0x7F7F 
return "%04x" % crc 
~~ def calcPF ( pf ) : 
pf_y = pf [ : 1 ] 
pf_x = pf [ 1 : ] 
result = 100 
if pf_y == CosTheta . CapacitiveLead : 
~~~ result = 200 - int ( pf_x ) 
~~ elif pf_y == CosTheta . InductiveLag : 
~~~ result = int ( pf_x ) 
~~ def setMaxDemandPeriod ( self , period , password = "00000000" ) : 
result = False 
self . setContext ( "setMaxDemandPeriod" ) 
~~~ if period < 1 or period > 3 : 
self . setContext ( "" ) 
~~ if not self . request ( False ) : 
~~~ if not self . serialCmdPwdAuth ( password ) : 
~~~ req_str = "015731023030353028" + binascii . hexlify ( str ( period ) ) . zfill ( 2 ) + "2903" 
req_str += self . calc_crc16 ( req_str [ 2 : ] . decode ( "hex" ) ) 
self . m_serial_port . write ( req_str . decode ( "hex" ) ) 
if self . m_serial_port . getResponse ( self . getContext ( ) ) . encode ( "hex" ) == "06" : 
result = True 
~~ ~~ ~~ self . serialPostEnd ( ) 
~~ self . setContext ( "" ) 
~~ def setMeterPassword ( self , new_pwd , pwd = "00000000" ) : 
self . setContext ( "setMeterPassword" ) 
~~~ if len ( new_pwd ) != 8 or len ( pwd ) != 8 : 
~~~ if not self . serialCmdPwdAuth ( pwd ) : 
~~~ req_pwd = binascii . hexlify ( new_pwd . zfill ( 8 ) ) 
req_str = "015731023030323028" + req_pwd + "2903" 
~~ def unpackStruct ( self , data , def_buf ) : 
struct_str = "=" 
~~~ if not def_buf [ fld ] [ MeterData . CalculatedFlag ] : 
~~~ struct_str = struct_str + str ( def_buf [ fld ] [ MeterData . SizeValue ] ) + "s" 
~~ ~~ if len ( data ) == 255 : 
~~~ contents = struct . unpack ( struct_str , str ( data ) ) 
contents = ( ) 
~~ return contents 
~~ def convertData ( self , contents , def_buf , kwh_scale = ScaleKWH . EmptyScale ) : 
log_str = "" 
if kwh_scale == ScaleKWH . EmptyScale : 
~~~ scale_offset = int ( def_buf . keys ( ) . index ( Field . kWh_Scale ) ) 
self . m_kwh_precision = kwh_scale = int ( contents [ scale_offset ] ) 
~~ for fld in def_buf : 
~~~ if def_buf [ fld ] [ MeterData . CalculatedFlag ] : 
~~ if len ( contents ) == 0 : 
~~~ raw_data = contents [ count ] 
fld_type = def_buf [ fld ] [ MeterData . TypeValue ] 
fld_scale = def_buf [ fld ] [ MeterData . ScaleValue ] 
~~~ float_data = float ( str ( raw_data ) ) 
divisor = 1 
if fld_scale == ScaleType . KWH : 
~~~ divisor = 1 
if kwh_scale == ScaleKWH . Scale10 : 
~~~ divisor = 10 
~~ elif kwh_scale == ScaleKWH . Scale100 : 
~~~ divisor = 100 
~~ elif ( kwh_scale != ScaleKWH . NoScale ) and ( kwh_scale != ScaleKWH . EmptyScale ) : 
~~ ~~ elif fld_scale == ScaleType . Div10 : 
~~ elif fld_scale == ScaleType . Div100 : 
~~ elif fld_scale != ScaleType . No : 
~~ float_data /= divisor 
float_data_str = str ( float_data ) 
def_buf [ fld ] [ MeterData . StringValue ] = float_data_str 
def_buf [ fld ] [ MeterData . NativeValue ] = float_data 
~~~ hex_data = raw_data . encode ( 'hex' ) 
def_buf [ fld ] [ MeterData . StringValue ] = hex_data 
def_buf [ fld ] [ MeterData . NativeValue ] = hex_data 
~~~ integer_data = int ( raw_data ) 
integer_data_str = str ( integer_data ) 
if len ( integer_data_str ) == 0 : 
~~~ integer_data_str = str ( 0 ) 
~~ def_buf [ fld ] [ MeterData . StringValue ] = integer_data_str 
def_buf [ fld ] [ MeterData . NativeValue ] = integer_data 
~~~ string_data = str ( raw_data ) 
def_buf [ fld ] [ MeterData . StringValue ] = string_data 
def_buf [ fld ] [ MeterData . NativeValue ] = string_data 
~~~ def_buf [ fld ] [ MeterData . StringValue ] = str ( raw_data ) 
def_buf [ fld ] [ MeterData . NativeValue ] = str ( raw_data ) 
~~ log_str = log_str + \ + fld + \ + def_buf [ fld ] [ MeterData . StringValue ] + \ 
ekm_log ( traceback . format_exc ( sys . exc_info ( ) ) ) 
~~ def jsonRender ( self , def_buf ) : 
~~~ ret_dict = SerialBlock ( ) 
ret_dict [ Field . Meter_Address ] = self . getMeterAddress ( ) 
~~~ ret_dict [ str ( fld ) ] = def_buf [ fld ] [ MeterData . StringValue ] 
~~ ~~ ~~ except : 
return "" 
~~ return json . dumps ( ret_dict , indent = 4 ) 
~~ def crcMeterRead ( self , raw_read , def_buf ) : 
~~~ if len ( raw_read ) == 0 : 
~~ sent_crc = self . calc_crc16 ( raw_read [ 1 : - 2 ] ) 
ekm_log ( logstr ) 
if int ( def_buf [ "crc16" ] [ MeterData . StringValue ] , 16 ) == int ( sent_crc , 16 ) : 
~~ ~~ except struct . error : 
~~~ ekm_log ( str ( sys . exc_info ( ) ) ) 
for frame in traceback . extract_tb ( sys . exc_info ( ) [ 2 ] ) : 
~~~ fname , lineno , fn , text = frame 
~~ def splitEkmDate ( dateint ) : 
date_str = str ( dateint ) 
dt = namedtuple ( 'EkmDate' , [ 'yy' , 'mm' , 'dd' , 'weekday' , 'hh' , 'minutes' , 'ss' ] ) 
if len ( date_str ) != 14 : 
~~~ dt . yy = dt . mm = dt . dd = dt . weekday = dt . hh = dt . minutes = dt . ss = 0 
return dt 
~~ dt . yy = int ( date_str [ 0 : 2 ] ) 
dt . mm = int ( date_str [ 2 : 4 ] ) 
dt . dd = int ( date_str [ 4 : 6 ] ) 
dt . weekday = int ( date_str [ 6 : 8 ] ) 
dt . hh = int ( date_str [ 8 : 10 ] ) 
dt . minutes = int ( date_str [ 10 : 12 ] ) 
dt . ss = int ( date_str [ 12 : 14 ] ) 
~~ def unregisterObserver ( self , observer ) : 
if observer in self . m_observers : 
~~~ self . m_observers . remove ( observer ) 
~~ def initSchd_1_to_4 ( self ) : 
self . m_schd_1_to_4 [ "reserved_40" ] = [ 6 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
self . m_schd_1_to_4 [ "Schedule_1_Period_1_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_1_Period_1_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_1_Period_1_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_1_Period_2_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_1_Period_2_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_1_Period_2_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_1_Period_3_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_1_Period_3_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_1_Period_3_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_1_Period_4_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_1_Period_4_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_1_Period_4_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "reserved_41" ] = [ 24 , FieldType . Hex , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_2_Period_1_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_2_Period_1_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_2_Period_1_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_2_Period_2_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_2_Period_2_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_2_Period_2_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_2_Period_3_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_2_Period_3_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_2_Period_3_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_2_Period_4_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_2_Period_4_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_2_Period_4_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "reserved_42" ] = [ 24 , FieldType . Hex , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_3_Period_1_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_3_Period_1_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_3_Period_1_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_3_Period_2_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_3_Period_2_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_3_Period_2_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_3_Period_3_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_3_Period_3_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_3_Period_3_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_3_Period_4_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_3_Period_4_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_3_Period_4_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "reserved_43" ] = [ 24 , FieldType . Hex , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_4_Period_1_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_4_Period_1_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_4_Period_1_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_4_Period_2_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_4_Period_2_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_4_Period_2_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_4_Period_3_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_4_Period_3_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_4_Period_3_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_4_Period_4_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_4_Period_4_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "Schedule_4_Period_4_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "reserved_44" ] = [ 79 , FieldType . Hex , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_1_to_4 [ "crc16" ] = [ 2 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
~~ def initSchd_5_to_6 ( self ) : 
self . m_schd_5_to_6 [ "reserved_30" ] = [ 6 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
self . m_schd_5_to_6 [ "Schedule_5_Period_1_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_5_Period_1_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_5_Period_1_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_5_Period_2_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_5_Period_2_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_5_Period_2_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_5_Period_3_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_5_Period_3_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_5_Period_3_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_5_Period_4_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_5_Period_4_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_5_Period_4_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "reserved_31" ] = [ 24 , FieldType . Hex , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_6_Period_1_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_6_Period_1_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_6_Period_1_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_6_Period_2_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_6_Period_2_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_6_Period_2_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_6_Period_3_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_6_Period_3_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_6_Period_3_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_6_Period_4_Hour" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_6_Period_4_Min" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "Schedule_6_Period_4_Tariff" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "reserved_32" ] = [ 24 , FieldType . Hex , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "reserved_33" ] = [ 24 , FieldType . Hex , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "reserved_34" ] = [ 24 , FieldType . Hex , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "reserved_35" ] = [ 24 , FieldType . Hex , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "reserved_36" ] = [ 79 , FieldType . Hex , ScaleType . No , "" , 0 , False , True ] 
self . m_schd_5_to_6 [ "crc16" ] = [ 2 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
~~ def getSchedulesBuffer ( self , period_group ) : 
empty_return = SerialBlock ( ) 
if period_group == ReadSchedules . Schedules_1_To_4 : 
~~~ return self . m_schd_1_to_4 
~~ elif period_group == ReadSchedules . Schedules_5_To_6 : 
~~~ return self . m_schd_5_to_6 
~~~ return empty_return 
~~ ~~ def initHldyDates ( self ) : 
self . m_hldy [ "reserved_20" ] = [ 6 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
self . m_hldy [ "Holiday_1_Mon" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_1_Day" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_2_Mon" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_2_Day" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_3_Mon" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_3_Day" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_4_Mon" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_4_Day" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_5_Mon" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_5_Day" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_6_Mon" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_6_Day" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_7_Mon" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_7_Day" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_8_Mon" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_8_Day" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_9_Mon" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_9_Day" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_10_Mon" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_10_Day" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_11_Mon" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_11_Day" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_12_Mon" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_12_Day" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_13_Mon" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_13_Day" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_14_Mon" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_14_Day" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_15_Mon" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_15_Day" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_16_Mon" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_16_Day" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_17_Mon" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_17_Day" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_18_Mon" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_18_Day" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_19_Mon" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_19_Day" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_20_Mon" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_20_Day" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Weekend_Schd" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "Holiday_Schd" ] = [ 2 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_hldy [ "reserved_21" ] = [ 163 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
self . m_hldy [ "crc16" ] = [ 2 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
~~ def initMons ( self ) : 
self . m_mons [ "reserved_echo_cmd" ] = [ 6 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
self . m_mons [ "Month_1_Tot" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_1_Tariff_1" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_1_Tariff_2" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_1_Tariff_3" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_1_Tariff_4" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_2_Tot" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_2_Tariff_1" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_2_Tariff_2" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_2_Tariff_3" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_2_Tariff_4" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_3_Tot" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_3_Tariff_1" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_3_Tariff_2" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_3_Tariff_3" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_3_Tariff_4" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_4_Tot" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_4_Tariff_1" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_4_Tariff_2" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_4_Tariff_3" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_4_Tariff_4" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_5_Tot" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_5_Tariff_1" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_5_Tariff_2" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_5_Tariff_3" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_5_Tariff_4" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_6_Tot" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_6_Tariff_1" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_6_Tariff_2" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_6_Tariff_3" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "Month_6_Tariff_4" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_mons [ "reserved_1" ] = [ 7 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
self . m_mons [ "crc16" ] = [ 2 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
~~ def initRevMons ( self ) : 
self . m_rev_mons [ "reserved_echo_cmd" ] = [ 6 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_1_Tot" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_1_Tariff_1" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_1_Tariff_2" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_1_Tariff_3" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_1_Tariff_4" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_2_Tot" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_2_Tariff_1" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_2_Tariff_2" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_2_Tariff_3" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_2_Tariff_4" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_3_Tot" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_3_Tariff_1" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_3_Tariff_2" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_3_Tariff_3" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_3_Tariff_4" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_4_Tot" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_4_Tariff_1" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_4_Tariff_2" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_4_Tariff_3" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_4_Tariff_4" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_5_Tot" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_5_Tariff_1" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_5_Tariff_2" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_5_Tariff_3" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_5_Tariff_4" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_6_Tot" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_6_Tariff_1" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_6_Tariff_2" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_6_Tariff_3" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "Month_6_Tariff_4" ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_rev_mons [ "reserved_1" ] = [ 7 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
self . m_rev_mons [ "crc16" ] = [ 2 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
~~ def getMonthsBuffer ( self , direction ) : 
if direction == ReadMonths . kWhReverse : 
~~~ return self . m_rev_mons 
~~ return self . m_mons 
~~ def setTime ( self , yy , mm , dd , hh , minutes , ss , password = "00000000" ) : 
self . setContext ( "setTime" ) 
~~~ if mm < 1 or mm > 12 : 
~~ if dd < 1 or dd > 31 : 
~~ if hh < 0 or hh > 23 : 
~~ if minutes < 0 or minutes > 59 : 
~~ if ss < 0 or ss > 59 : 
~~ if len ( password ) != 8 : 
~~~ dt_buf = datetime . datetime ( int ( yy ) , int ( mm ) , int ( dd ) , int ( hh ) , int ( minutes ) , int ( ss ) ) 
dayofweek = dt_buf . date ( ) . isoweekday ( ) 
req_str = "015731023030363028" 
req_str += binascii . hexlify ( str ( yy ) [ - 2 : ] ) 
req_str += binascii . hexlify ( str ( mm ) . zfill ( 2 ) ) 
req_str += binascii . hexlify ( str ( dd ) . zfill ( 2 ) ) 
req_str += binascii . hexlify ( str ( dayofweek ) . zfill ( 2 ) ) 
req_str += binascii . hexlify ( str ( hh ) . zfill ( 2 ) ) 
req_str += binascii . hexlify ( str ( minutes ) . zfill ( 2 ) ) 
req_str += binascii . hexlify ( str ( ss ) . zfill ( 2 ) ) 
req_str += "2903" 
~~ def setCTRatio ( self , new_ct , password = "00000000" ) : 
ret = False 
self . setContext ( "setCTRatio" ) 
~~~ self . clearCmdMsg ( ) 
if ( ( new_ct != CTRatio . Amps_100 ) and ( new_ct != CTRatio . Amps_200 ) and 
( new_ct != CTRatio . Amps_400 ) and ( new_ct != CTRatio . Amps_600 ) and 
( new_ct != CTRatio . Amps_800 ) and ( new_ct != CTRatio . Amps_1000 ) and 
( new_ct != CTRatio . Amps_1200 ) and ( new_ct != CTRatio . Amps_1500 ) and 
( new_ct != CTRatio . Amps_2000 ) and ( new_ct != CTRatio . Amps_3000 ) and 
( new_ct != CTRatio . Amps_4000 ) and ( new_ct != CTRatio . Amps_5000 ) ) : 
~~~ req_str = "015731023030443028" + binascii . hexlify ( str ( new_ct ) . zfill ( 4 ) ) + "2903" 
ret = True 
~~ def assignSchedule ( self , schedule , period , hour , minute , tariff ) : 
if ( ( schedule not in range ( Extents . Schedules ) ) or 
( period not in range ( Extents . Tariffs ) ) or 
( hour < 0 ) or ( hour > 23 ) or ( minute < 0 ) or 
( minute > 59 ) or ( tariff < 0 ) ) : 
~~ period += 1 
idx_min = "Min_" + str ( period ) 
idx_hour = "Hour_" + str ( period ) 
idx_rate = "Tariff_" + str ( period ) 
if idx_min not in self . m_schedule_params : 
~~ if idx_hour not in self . m_schedule_params : 
~~ if idx_rate not in self . m_schedule_params : 
~~ self . m_schedule_params [ idx_rate ] = tariff 
self . m_schedule_params [ idx_hour ] = hour 
self . m_schedule_params [ idx_min ] = minute 
self . m_schedule_params [ 'Schedule' ] = schedule 
~~ def assignSeasonSchedule ( self , season , month , day , schedule ) : 
season += 1 
schedule += 1 
if ( ( season < 1 ) or ( season > Extents . Seasons ) or ( schedule < 1 ) or 
( schedule > Extents . Schedules ) or ( month > 12 ) or ( month < 0 ) or 
( day < 0 ) or ( day > 31 ) ) : 
~~ idx_mon = "Season_" + str ( season ) + "_Start_Day" 
idx_day = "Season_" + str ( season ) + "_Start_Month" 
idx_schedule = "Season_" + str ( season ) + "_Schedule" 
if idx_mon not in self . m_seasons_sched_params : 
~~ if idx_day not in self . m_seasons_sched_params : 
~~ if idx_schedule not in self . m_seasons_sched_params : 
~~ self . m_seasons_sched_params [ idx_mon ] = month 
self . m_seasons_sched_params [ idx_day ] = day 
self . m_seasons_sched_params [ idx_schedule ] = schedule 
~~ def setSeasonSchedules ( self , cmd_dict = None , password = "00000000" ) : 
self . setContext ( "setSeasonSchedules" ) 
if not cmd_dict : 
~~~ cmd_dict = self . m_seasons_sched_params 
~~~ if not self . request ( False ) : 
~~~ req_table = "" 
req_table += binascii . hexlify ( str ( cmd_dict [ "Season_1_Start_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Season_1_Start_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Season_1_Schedule" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Season_2_Start_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Season_2_Start_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Season_2_Schedule" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Season_3_Start_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Season_3_Start_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Season_3_Schedule" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Season_4_Start_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Season_4_Start_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Season_4_Schedule" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( 0 ) . zfill ( 24 ) ) 
req_str = "015731023030383028" + req_table + "2903" 
~~ def assignHolidayDate ( self , holiday , month , day ) : 
holiday += 1 
if ( month > 12 ) or ( month < 0 ) or ( day > 31 ) or ( day < 0 ) or ( holiday < 1 ) or ( holiday > Extents . Holidays ) : 
~~ day_str = "Holiday_" + str ( holiday ) + "_Day" 
mon_str = "Holiday_" + str ( holiday ) + "_Month" 
if day_str not in self . m_holiday_date_params : 
~~ if mon_str not in self . m_holiday_date_params : 
~~ self . m_holiday_date_params [ day_str ] = day 
self . m_holiday_date_params [ mon_str ] = month 
~~ def setHolidayDates ( self , cmd_dict = None , password = "00000000" ) : 
self . setContext ( "setHolidayDates" ) 
~~~ cmd_dict = self . m_holiday_date_params 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_1_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_1_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_2_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_2_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_3_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_3_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_4_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_4_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_5_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_5_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_6_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_6_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_7_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_7_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_8_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_8_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_9_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_9_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_10_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_10_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_11_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_11_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_12_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_12_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_13_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_13_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_14_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_14_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_15_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_15_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_16_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_16_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_17_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_17_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_18_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_18_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_19_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_19_Day" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_20_Month" ] ) . zfill ( 2 ) ) 
req_table += binascii . hexlify ( str ( cmd_dict [ "Holiday_20_Day" ] ) . zfill ( 2 ) ) 
req_str = "015731023030423028" + req_table + "2903" 
~~ def setWeekendHolidaySchedules ( self , new_wknd , new_hldy , password = "00000000" ) : 
self . setContext ( "setWeekendHolidaySchedules" ) 
~~~ req_wkd = binascii . hexlify ( str ( new_wknd ) . zfill ( 2 ) ) 
req_hldy = binascii . hexlify ( str ( new_hldy ) . zfill ( 2 ) ) 
req_str = "015731023030433028" + req_wkd + req_hldy + "2903" 
~~ def readSchedules ( self , tableset ) : 
self . setContext ( "readSchedules" ) 
~~~ req_table = binascii . hexlify ( str ( tableset ) . zfill ( 1 ) ) 
req_str = "01523102303037" + req_table + "282903" 
self . request ( False ) 
req_crc = self . calc_crc16 ( req_str [ 2 : ] . decode ( "hex" ) ) 
req_str += req_crc 
raw_ret = self . m_serial_port . getResponse ( self . getContext ( ) ) 
self . serialPostEnd ( ) 
return_crc = self . calc_crc16 ( raw_ret [ 1 : - 2 ] ) 
if tableset == ReadSchedules . Schedules_1_To_4 : 
~~~ unpacked_read = self . unpackStruct ( raw_ret , self . m_schd_1_to_4 ) 
self . convertData ( unpacked_read , self . m_schd_1_to_4 , self . m_kwh_precision ) 
if str ( return_crc ) == str ( self . m_schd_1_to_4 [ "crc16" ] [ MeterData . StringValue ] ) : 
~~ ~~ elif tableset == ReadSchedules . Schedules_5_To_6 : 
~~~ unpacked_read = self . unpackStruct ( raw_ret , self . m_schd_5_to_6 ) 
self . convertData ( unpacked_read , self . m_schd_5_to_6 , self . m_kwh_precision ) 
if str ( return_crc ) == str ( self . m_schd_5_to_6 [ "crc16" ] [ MeterData . StringValue ] ) : 
~~ def extractSchedule ( self , schedule , period ) : 
ret = namedtuple ( "ret" , [ "Hour" , "Min" , "Tariff" , "Period" , "Schedule" ] ) 
work_table = self . m_schd_1_to_4 
if Schedules . Schedule_5 <= schedule <= Schedules . Schedule_6 : 
~~~ work_table = self . m_schd_5_to_6 
ret . Period = str ( period ) 
ret . Schedule = str ( schedule ) 
if ( schedule < 1 ) or ( schedule > Extents . Schedules ) or ( period < 0 ) or ( period > Extents . Periods ) : 
ret . Hour = ret . Min = ret . Tariff = str ( 0 ) 
~~ idxhr = "Schedule_" + str ( schedule ) + "_Period_" + str ( period ) + "_Hour" 
idxmin = "Schedule_" + str ( schedule ) + "_Period_" + str ( period ) + "_Min" 
idxrate = "Schedule_" + str ( schedule ) + "_Period_" + str ( period ) + "_Tariff" 
if idxhr not in work_table : 
~~ if idxmin not in work_table : 
~~ if idxrate not in work_table : 
~~ ret . Hour = work_table [ idxhr ] [ MeterData . StringValue ] 
ret . Min = work_table [ idxmin ] [ MeterData . StringValue ] . zfill ( 2 ) 
ret . Tariff = work_table [ idxrate ] [ MeterData . StringValue ] 
~~ def readMonthTariffs ( self , months_type ) : 
self . setContext ( "readMonthTariffs" ) 
~~~ req_type = binascii . hexlify ( str ( months_type ) . zfill ( 1 ) ) 
req_str = "01523102303031" + req_type + "282903" 
work_table = self . m_mons 
if months_type == ReadMonths . kWhReverse : 
~~~ work_table = self . m_rev_mons 
~~ self . request ( False ) 
unpacked_read = self . unpackStruct ( raw_ret , work_table ) 
self . convertData ( unpacked_read , work_table , self . m_kwh_precision ) 
if str ( return_crc ) == str ( work_table [ "crc16" ] [ MeterData . StringValue ] ) : 
~~ def extractMonthTariff ( self , month ) : 
ret = namedtuple ( "ret" , [ "Month" , Field . kWh_Tariff_1 , Field . kWh_Tariff_2 , Field . kWh_Tariff_3 , 
Field . kWh_Tariff_4 , Field . kWh_Tot , Field . Rev_kWh_Tariff_1 , 
Field . Rev_kWh_Tariff_2 , Field . Rev_kWh_Tariff_3 , 
Field . Rev_kWh_Tariff_4 , Field . Rev_kWh_Tot ] ) 
month += 1 
ret . Month = str ( month ) 
if ( month < 1 ) or ( month > Extents . Months ) : 
~~~ ret . kWh_Tariff_1 = ret . kWh_Tariff_2 = ret . kWh_Tariff_3 = ret . kWh_Tariff_4 = str ( 0 ) 
ret . Rev_kWh_Tariff_1 = ret . Rev_kWh_Tariff_2 = ret . Rev_kWh_Tariff_3 = ret . Rev_kWh_Tariff_4 = str ( 0 ) 
ret . kWh_Tot = ret . Rev_kWh_Tot = str ( 0 ) 
~~ base_str = "Month_" + str ( month ) + "_" 
ret . kWh_Tariff_1 = self . m_mons [ base_str + "Tariff_1" ] [ MeterData . StringValue ] 
ret . kWh_Tariff_2 = self . m_mons [ base_str + "Tariff_2" ] [ MeterData . StringValue ] 
ret . kWh_Tariff_3 = self . m_mons [ base_str + "Tariff_3" ] [ MeterData . StringValue ] 
ret . kWh_Tariff_4 = self . m_mons [ base_str + "Tariff_4" ] [ MeterData . StringValue ] 
ret . kWh_Tot = self . m_mons [ base_str + "Tot" ] [ MeterData . StringValue ] 
ret . Rev_kWh_Tariff_1 = self . m_rev_mons [ base_str + "Tariff_1" ] [ MeterData . StringValue ] 
ret . Rev_kWh_Tariff_2 = self . m_rev_mons [ base_str + "Tariff_2" ] [ MeterData . StringValue ] 
ret . Rev_kWh_Tariff_3 = self . m_rev_mons [ base_str + "Tariff_3" ] [ MeterData . StringValue ] 
ret . Rev_kWh_Tariff_4 = self . m_rev_mons [ base_str + "Tariff_4" ] [ MeterData . StringValue ] 
ret . Rev_kWh_Tot = self . m_rev_mons [ base_str + "Tot" ] [ MeterData . StringValue ] 
~~ def readHolidayDates ( self ) : 
self . setContext ( "readHolidayDates" ) 
~~~ req_str = "0152310230304230282903" 
unpacked_read = self . unpackStruct ( raw_ret , self . m_hldy ) 
self . convertData ( unpacked_read , self . m_hldy , self . m_kwh_precision ) 
if str ( return_crc ) == str ( self . m_hldy [ "crc16" ] [ MeterData . StringValue ] ) : 
~~ def extractHolidayDate ( self , setting_holiday ) : 
ret = namedtuple ( "result" , [ "Holiday" , "Month" , "Day" ] ) 
setting_holiday += 1 
ret . Holiday = str ( setting_holiday ) 
if ( setting_holiday < 1 ) or ( setting_holiday > Extents . Holidays ) : 
ret . Holiday = ret . Month = ret . Day = str ( 0 ) 
~~ idxday = "Holiday_" + str ( setting_holiday ) + "_Day" 
idxmon = "Holiday_" + str ( setting_holiday ) + "_Mon" 
if idxmon not in self . m_hldy : 
~~~ ret . Holiday = ret . Month = ret . Day = str ( 0 ) 
~~ if idxday not in self . m_hldy : 
~~ ret . Day = self . m_hldy [ idxday ] [ MeterData . StringValue ] 
ret . Month = self . m_hldy [ idxmon ] [ MeterData . StringValue ] 
~~ def extractHolidayWeekendSchedules ( self ) : 
result = namedtuple ( "result" , [ "Weekend" , "Holiday" ] ) 
result . Weekend = self . m_hldy [ "Weekend_Schd" ] [ MeterData . StringValue ] 
result . Holiday = self . m_hldy [ "Holiday_Schd" ] [ MeterData . StringValue ] 
~~ def readSettings ( self ) : 
success = ( self . readHolidayDates ( ) and 
self . readMonthTariffs ( ReadMonths . kWh ) and 
self . readMonthTariffs ( ReadMonths . kWhReverse ) and 
self . readSchedules ( ReadSchedules . Schedules_1_To_4 ) and 
self . readSchedules ( ReadSchedules . Schedules_5_To_6 ) ) 
return success 
~~ def writeCmdMsg ( self , msg ) : 
self . m_command_msg = msg 
~~ def serialCmdPwdAuth ( self , password_str ) : 
~~~ req_start = "0150310228" + binascii . hexlify ( password_str ) + "2903" 
req_crc = self . calc_crc16 ( req_start [ 2 : ] . decode ( "hex" ) ) 
req_str = req_start + req_crc 
~~ def initWorkFormat ( self ) : 
self . m_blk_a [ "reserved_10" ] = [ 1 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ Field . Model ] = [ 2 , FieldType . Hex , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_a [ Field . Firmware ] = [ 1 , FieldType . Hex , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_a [ Field . Meter_Address ] = [ 12 , FieldType . String , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_a [ Field . kWh_Tot ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_a [ Field . kWh_Tariff_1 ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_a [ Field . kWh_Tariff_2 ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_a [ Field . kWh_Tariff_3 ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_a [ Field . kWh_Tariff_4 ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_a [ Field . Rev_kWh_Tot ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_a [ Field . Rev_kWh_Tariff_1 ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_a [ Field . Rev_kWh_Tariff_2 ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_a [ Field . Rev_kWh_Tariff_3 ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_a [ Field . Rev_kWh_Tariff_4 ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_a [ Field . RMS_Volts_Ln_1 ] = [ 4 , FieldType . Float , ScaleType . Div10 , "" , 0 , False , False ] 
self . m_blk_a [ Field . RMS_Volts_Ln_2 ] = [ 4 , FieldType . Float , ScaleType . Div10 , "" , 0 , False , False ] 
self . m_blk_a [ Field . RMS_Volts_Ln_3 ] = [ 4 , FieldType . Float , ScaleType . Div10 , "" , 0 , False , False ] 
self . m_blk_a [ Field . Amps_Ln_1 ] = [ 5 , FieldType . Float , ScaleType . Div10 , "" , 0 , False , False ] 
self . m_blk_a [ Field . Amps_Ln_2 ] = [ 5 , FieldType . Float , ScaleType . Div10 , "" , 0 , False , False ] 
self . m_blk_a [ Field . Amps_Ln_3 ] = [ 5 , FieldType . Float , ScaleType . Div10 , "" , 0 , False , False ] 
self . m_blk_a [ Field . RMS_Watts_Ln_1 ] = [ 7 , FieldType . Int , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ Field . RMS_Watts_Ln_2 ] = [ 7 , FieldType . Int , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ Field . RMS_Watts_Ln_3 ] = [ 7 , FieldType . Int , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ Field . RMS_Watts_Tot ] = [ 7 , FieldType . Int , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ Field . Cos_Theta_Ln_1 ] = [ 4 , FieldType . PowerFactor , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ Field . Cos_Theta_Ln_2 ] = [ 4 , FieldType . PowerFactor , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ Field . Cos_Theta_Ln_3 ] = [ 4 , FieldType . PowerFactor , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ Field . Max_Demand ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , True ] 
self . m_blk_a [ Field . Max_Demand_Period ] = [ 1 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_a [ Field . Meter_Time ] = [ 14 , FieldType . String , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ Field . CT_Ratio ] = [ 4 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_a [ Field . Pulse_Cnt_1 ] = [ 8 , FieldType . Int , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ Field . Pulse_Cnt_2 ] = [ 8 , FieldType . Int , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ Field . Pulse_Cnt_3 ] = [ 8 , FieldType . Int , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ Field . Pulse_Ratio_1 ] = [ 4 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_a [ Field . Pulse_Ratio_2 ] = [ 4 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_a [ Field . Pulse_Ratio_3 ] = [ 4 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_a [ Field . State_Inputs ] = [ 3 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_a [ "reserved_11" ] = [ 19 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ Field . Status_A ] = [ 1 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ "reserved_12" ] = [ 4 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ "crc16" ] = [ 2 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ Field . Power_Factor_Ln_1 ] = [ 4 , FieldType . Int , ScaleType . No , "0" , 0 , True , False ] 
self . m_blk_a [ Field . Power_Factor_Ln_2 ] = [ 4 , FieldType . Int , ScaleType . No , "0" , 0 , True , False ] 
self . m_blk_a [ Field . Power_Factor_Ln_3 ] = [ 4 , FieldType . Int , ScaleType . No , "0" , 0 , True , False ] 
~~ def request ( self , send_terminator = False ) : 
self . m_a_crc = False 
start_context = self . getContext ( ) 
self . setContext ( "request[v3A]" ) 
~~~ self . m_serial_port . write ( "2f3f" . decode ( "hex" ) + 
self . m_meter_address + 
"210d0a" . decode ( "hex" ) ) 
self . m_raw_read_a = self . m_serial_port . getResponse ( self . getContext ( ) ) 
unpacked_read_a = self . unpackStruct ( self . m_raw_read_a , self . m_blk_a ) 
self . convertData ( unpacked_read_a , self . m_blk_a , 1 ) 
self . m_a_crc = self . crcMeterRead ( self . m_raw_read_a , self . m_blk_a ) 
if send_terminator : 
~~~ self . serialPostEnd ( ) 
~~ self . calculateFields ( ) 
self . makeReturnFormat ( ) 
~~ self . setContext ( start_context ) 
return self . m_a_crc 
~~ def makeReturnFormat ( self ) : 
for fld in self . m_blk_a : 
~~~ self . m_req [ fld ] = self . m_blk_a [ fld ] 
~~ def insert ( self , meter_db ) : 
if meter_db : 
~~~ meter_db . dbInsert ( self . m_req , self . m_raw_read_a , self . m_raw_read_b ) 
~~ def updateObservers ( self ) : 
for observer in self . m_observers : 
~~~ observer . update ( self . m_req ) 
~~ ~~ ~~ def getField ( self , fld_name ) : 
if fld_name in self . m_req : 
~~~ result = self . m_req [ fld_name ] [ MeterData . StringValue ] 
~~ def initFormatA ( self ) : 
self . m_blk_a [ "reserved_1" ] = [ 1 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ Field . Reactive_Energy_Tot ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_a [ Field . kWh_Ln_1 ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_a [ Field . kWh_Ln_2 ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_a [ Field . kWh_Ln_3 ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_a [ Field . Rev_kWh_Ln_1 ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_a [ Field . Rev_kWh_Ln_2 ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_a [ Field . Rev_kWh_Ln_3 ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_a [ Field . Resettable_kWh_Tot ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_a [ Field . Resettable_Rev_kWh_Tot ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_a [ Field . Reactive_Pwr_Ln_1 ] = [ 7 , FieldType . Int , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ Field . Reactive_Pwr_Ln_2 ] = [ 7 , FieldType . Int , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ Field . Reactive_Pwr_Ln_3 ] = [ 7 , FieldType . Int , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ Field . Reactive_Pwr_Tot ] = [ 7 , FieldType . Int , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ Field . Line_Freq ] = [ 4 , FieldType . Float , ScaleType . Div100 , "" , 0 , False , False ] 
self . m_blk_a [ Field . State_Inputs ] = [ 1 , FieldType . Int , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ Field . State_Watts_Dir ] = [ 1 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_a [ Field . State_Out ] = [ 1 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_a [ Field . kWh_Scale ] = [ 1 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_a [ "reserved_2" ] = [ 2 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ "reserved_3" ] = [ 2 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_a [ "reserved_4" ] = [ 4 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
~~ def initFormatB ( self ) : 
self . m_blk_b [ "reserved_5" ] = [ 1 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_b [ Field . Model ] = [ 2 , FieldType . Hex , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_b [ Field . Firmware ] = [ 1 , FieldType . Hex , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_b [ Field . Meter_Address ] = [ 12 , FieldType . String , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_b [ Field . kWh_Tariff_1 ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_b [ Field . kWh_Tariff_2 ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_b [ Field . kWh_Tariff_3 ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_b [ Field . kWh_Tariff_4 ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_b [ Field . Rev_kWh_Tariff_1 ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_b [ Field . Rev_kWh_Tariff_2 ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_b [ Field . Rev_kWh_Tariff_3 ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_b [ Field . Rev_kWh_Tariff_4 ] = [ 8 , FieldType . Float , ScaleType . KWH , "" , 0 , False , False ] 
self . m_blk_b [ Field . RMS_Volts_Ln_1 ] = [ 4 , FieldType . Float , ScaleType . Div10 , "" , 0 , False , False ] 
self . m_blk_b [ Field . RMS_Volts_Ln_2 ] = [ 4 , FieldType . Float , ScaleType . Div10 , "" , 0 , False , False ] 
self . m_blk_b [ Field . RMS_Volts_Ln_3 ] = [ 4 , FieldType . Float , ScaleType . Div10 , "" , 0 , False , False ] 
self . m_blk_b [ Field . Amps_Ln_1 ] = [ 5 , FieldType . Float , ScaleType . Div10 , "" , 0 , False , False ] 
self . m_blk_b [ Field . Amps_Ln_2 ] = [ 5 , FieldType . Float , ScaleType . Div10 , "" , 0 , False , False ] 
self . m_blk_b [ Field . Amps_Ln_3 ] = [ 5 , FieldType . Float , ScaleType . Div10 , "" , 0 , False , False ] 
self . m_blk_b [ Field . RMS_Watts_Ln_1 ] = [ 7 , FieldType . Int , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_b [ Field . RMS_Watts_Ln_2 ] = [ 7 , FieldType . Int , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_b [ Field . RMS_Watts_Ln_3 ] = [ 7 , FieldType . Int , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_b [ Field . RMS_Watts_Tot ] = [ 7 , FieldType . Int , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_b [ Field . Cos_Theta_Ln_1 ] = [ 4 , FieldType . PowerFactor , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_b [ Field . Cos_Theta_Ln_2 ] = [ 4 , FieldType . PowerFactor , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_b [ Field . Cos_Theta_Ln_3 ] = [ 4 , FieldType . PowerFactor , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_b [ Field . RMS_Watts_Max_Demand ] = [ 8 , FieldType . Float , ScaleType . Div10 , "" , 0 , False , False ] 
self . m_blk_b [ Field . Max_Demand_Period ] = [ 1 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_b [ Field . Pulse_Ratio_1 ] = [ 4 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_b [ Field . Pulse_Ratio_2 ] = [ 4 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_b [ Field . Pulse_Ratio_3 ] = [ 4 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_b [ Field . CT_Ratio ] = [ 4 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_b [ Field . Max_Demand_Interval_Reset ] = [ 1 , FieldType . Int , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_b [ Field . Pulse_Output_Ratio ] = [ 4 , FieldType . Int , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_b [ "reserved_7" ] = [ 53 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_b [ Field . Status_A ] = [ 1 , FieldType . Hex , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_b [ Field . Status_B ] = [ 1 , FieldType . Hex , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_b [ Field . Status_C ] = [ 1 , FieldType . Hex , ScaleType . No , "" , 0 , False , True ] 
self . m_blk_b [ Field . Meter_Time ] = [ 14 , FieldType . String , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_b [ "reserved_8" ] = [ 2 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_b [ "reserved_9" ] = [ 4 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_b [ "crc16" ] = [ 2 , FieldType . Hex , ScaleType . No , "" , 0 , False , False ] 
self . m_blk_b [ Field . Net_Calc_Watts_Ln_1 ] = [ 7 , FieldType . Int , ScaleType . No , "0" , 0 , True , False ] 
self . m_blk_b [ Field . Net_Calc_Watts_Ln_2 ] = [ 7 , FieldType . Int , ScaleType . No , "0" , 0 , True , False ] 
self . m_blk_b [ Field . Net_Calc_Watts_Ln_3 ] = [ 7 , FieldType . Int , ScaleType . No , "0" , 0 , True , False ] 
self . m_blk_b [ Field . Net_Calc_Watts_Tot ] = [ 7 , FieldType . Int , ScaleType . No , "0" , 0 , True , False ] 
self . m_blk_b [ Field . Power_Factor_Ln_1 ] = [ 4 , FieldType . Int , ScaleType . No , "0" , 0 , True , False ] 
self . m_blk_b [ Field . Power_Factor_Ln_2 ] = [ 4 , FieldType . Int , ScaleType . No , "0" , 0 , True , False ] 
self . m_blk_b [ Field . Power_Factor_Ln_3 ] = [ 4 , FieldType . Int , ScaleType . No , "0" , 0 , True , False ] 
~~ def initLcdLookup ( self ) : 
self . m_lcd_lookup [ "kWh_Tot" ] = LCDItems . kWh_Tot 
self . m_lcd_lookup [ "Rev_kWh_Tot" ] = LCDItems . Rev_kWh_Tot 
self . m_lcd_lookup [ "RMS_Volts_Ln_1" ] = LCDItems . RMS_Volts_Ln_1 
self . m_lcd_lookup [ "RMS_Volts_Ln_2" ] = LCDItems . RMS_Volts_Ln_2 
self . m_lcd_lookup [ "RMS_Volts_Ln_3" ] = LCDItems . RMS_Volts_Ln_3 
self . m_lcd_lookup [ "Amps_Ln_1" ] = LCDItems . Amps_Ln_1 
self . m_lcd_lookup [ "Amps_Ln_2" ] = LCDItems . Amps_Ln_2 
self . m_lcd_lookup [ "Amps_Ln_3" ] = LCDItems . Amps_Ln_3 
self . m_lcd_lookup [ "RMS_Watts_Ln_1" ] = LCDItems . RMS_Watts_Ln_1 
self . m_lcd_lookup [ "RMS_Watts_Ln_2" ] = LCDItems . RMS_Watts_Ln_2 
self . m_lcd_lookup [ "RMS_Watts_Ln_3" ] = LCDItems . RMS_Watts_Ln_3 
self . m_lcd_lookup [ "RMS_Watts_Tot" ] = LCDItems . RMS_Watts_Tot 
self . m_lcd_lookup [ "Power_Factor_Ln_1" ] = LCDItems . Power_Factor_Ln_1 
self . m_lcd_lookup [ "Power_Factor_Ln_2" ] = LCDItems . Power_Factor_Ln_2 
self . m_lcd_lookup [ "Power_Factor_Ln_3" ] = LCDItems . Power_Factor_Ln_3 
self . m_lcd_lookup [ "kWh_Tariff_1" ] = LCDItems . kWh_Tariff_1 
self . m_lcd_lookup [ "kWh_Tariff_2" ] = LCDItems . kWh_Tariff_2 
self . m_lcd_lookup [ "kWh_Tariff_3" ] = LCDItems . kWh_Tariff_3 
self . m_lcd_lookup [ "kWh_Tariff_4" ] = LCDItems . kWh_Tariff_4 
self . m_lcd_lookup [ "Rev_kWh_Tariff_1" ] = LCDItems . Rev_kWh_Tariff_1 
self . m_lcd_lookup [ "Rev_kWh_Tariff_2" ] = LCDItems . Rev_kWh_Tariff_2 
self . m_lcd_lookup [ "Rev_kWh_Tariff_3" ] = LCDItems . Rev_kWh_Tariff_3 
self . m_lcd_lookup [ "Rev_kWh_Tariff_4" ] = LCDItems . Rev_kWh_Tariff_4 
self . m_lcd_lookup [ "Reactive_Pwr_Ln_1" ] = LCDItems . Reactive_Pwr_Ln_1 
self . m_lcd_lookup [ "Reactive_Pwr_Ln_2" ] = LCDItems . Reactive_Pwr_Ln_2 
self . m_lcd_lookup [ "Reactive_Pwr_Ln_3" ] = LCDItems . Reactive_Pwr_Ln_3 
self . m_lcd_lookup [ "Reactive_Pwr_Tot" ] = LCDItems . Reactive_Pwr_Tot 
self . m_lcd_lookup [ "Line_Freq" ] = LCDItems . Line_Freq 
self . m_lcd_lookup [ "Pulse_Cnt_1" ] = LCDItems . Pulse_Cnt_1 
self . m_lcd_lookup [ "Pulse_Cnt_2" ] = LCDItems . Pulse_Cnt_2 
self . m_lcd_lookup [ "Pulse_Cnt_3" ] = LCDItems . Pulse_Cnt_3 
self . m_lcd_lookup [ "kWh_Ln_1" ] = LCDItems . kWh_Ln_1 
self . m_lcd_lookup [ "Rev_kWh_Ln_1" ] = LCDItems . Rev_kWh_Ln_1 
self . m_lcd_lookup [ "kWh_Ln_2" ] = LCDItems . kWh_Ln_2 
self . m_lcd_lookup [ "Rev_kWh_Ln_2" ] = LCDItems . Rev_kWh_Ln_2 
self . m_lcd_lookup [ "kWh_Ln_3" ] = LCDItems . kWh_Ln_3 
self . m_lcd_lookup [ "Rev_kWh_Ln_3" ] = LCDItems . Rev_kWh_Ln_3 
self . m_lcd_lookup [ "Reactive_Energy_Tot" ] = LCDItems . Reactive_Energy_Tot 
self . m_lcd_lookup [ "Max_Demand_Rst" ] = LCDItems . Max_Demand_Rst 
self . m_lcd_lookup [ "Rev_kWh_Rst" ] = LCDItems . Rev_kWh_Rst 
self . m_lcd_lookup [ "State_Inputs" ] = LCDItems . State_Inputs 
self . m_lcd_lookup [ "Max_Demand" ] = LCDItems . Max_Demand 
~~~ retA = self . requestA ( ) 
retB = self . requestB ( ) 
if retA and retB : 
~~~ self . makeAB ( ) 
self . calculateFields ( ) 
self . updateObservers ( ) 
~~ def requestA ( self ) : 
work_context = self . getContext ( ) 
self . setContext ( "request[v4A]" ) 
self . m_serial_port . write ( "2f3f" . decode ( "hex" ) + self . m_meter_address + "3030210d0a" . decode ( "hex" ) ) 
self . convertData ( unpacked_read_a , self . m_blk_a ) 
self . m_kwh_precision = int ( self . m_blk_a [ Field . kWh_Scale ] [ MeterData . NativeValue ] ) 
self . setContext ( work_context ) 
~~ def requestB ( self ) : 
self . setContext ( "request[v4B]" ) 
self . m_serial_port . write ( "2f3f" . decode ( "hex" ) + self . m_meter_address + "3031210d0a" . decode ( "hex" ) ) 
self . m_raw_read_b = self . m_serial_port . getResponse ( self . getContext ( ) ) 
unpacked_read_b = self . unpackStruct ( self . m_raw_read_b , self . m_blk_b ) 
self . convertData ( unpacked_read_b , self . m_blk_b , self . m_kwh_precision ) 
self . m_b_crc = self . crcMeterRead ( self . m_raw_read_b , self . m_blk_b ) 
return self . m_b_crc 
~~ def makeAB ( self ) : 
~~ ~~ for fld in self . m_blk_b : 
~~~ self . m_req [ fld ] = self . m_blk_b [ fld ] 
~~ def calculateFields ( self ) : 
pf1 = self . m_blk_b [ Field . Cos_Theta_Ln_1 ] [ MeterData . StringValue ] 
pf2 = self . m_blk_b [ Field . Cos_Theta_Ln_2 ] [ MeterData . StringValue ] 
pf3 = self . m_blk_b [ Field . Cos_Theta_Ln_3 ] [ MeterData . StringValue ] 
pf1_int = self . calcPF ( pf1 ) 
pf2_int = self . calcPF ( pf2 ) 
pf3_int = self . calcPF ( pf3 ) 
self . m_blk_b [ Field . Power_Factor_Ln_1 ] [ MeterData . StringValue ] = str ( pf1_int ) 
self . m_blk_b [ Field . Power_Factor_Ln_2 ] [ MeterData . StringValue ] = str ( pf2_int ) 
self . m_blk_b [ Field . Power_Factor_Ln_3 ] [ MeterData . StringValue ] = str ( pf3_int ) 
self . m_blk_b [ Field . Power_Factor_Ln_1 ] [ MeterData . NativeValue ] = pf1_int 
self . m_blk_b [ Field . Power_Factor_Ln_2 ] [ MeterData . NativeValue ] = pf2_int 
self . m_blk_b [ Field . Power_Factor_Ln_3 ] [ MeterData . NativeValue ] = pf2_int 
rms_watts_1 = self . m_blk_b [ Field . RMS_Watts_Ln_1 ] [ MeterData . NativeValue ] 
rms_watts_2 = self . m_blk_b [ Field . RMS_Watts_Ln_2 ] [ MeterData . NativeValue ] 
rms_watts_3 = self . m_blk_b [ Field . RMS_Watts_Ln_3 ] [ MeterData . NativeValue ] 
sign_rms_watts_1 = 1 
sign_rms_watts_2 = 1 
sign_rms_watts_3 = 1 
direction_byte = self . m_blk_a [ Field . State_Watts_Dir ] [ MeterData . NativeValue ] 
if direction_byte == DirectionFlag . ForwardForwardForward : 
~~ if direction_byte == DirectionFlag . ForwardForwardReverse : 
~~~ sign_rms_watts_3 = - 1 
~~ if direction_byte == DirectionFlag . ForwardReverseForward : 
~~~ sign_rms_watts_2 = - 1 
~~ if direction_byte == DirectionFlag . ReverseForwardForward : 
~~~ sign_rms_watts_1 = - 1 
~~ if direction_byte == DirectionFlag . ForwardReverseReverse : 
sign_rms_watts_3 = - 1 
~~ if direction_byte == DirectionFlag . ReverseForwardReverse : 
~~ if direction_byte == DirectionFlag . ReverseReverseForward : 
sign_rms_watts_2 = - 1 
~~ if direction_byte == DirectionFlag . ReverseReverseReverse : 
~~ net_watts_1 = rms_watts_1 * sign_rms_watts_1 
net_watts_2 = rms_watts_2 * sign_rms_watts_2 
net_watts_3 = rms_watts_3 * sign_rms_watts_3 
net_watts_tot = net_watts_1 + net_watts_2 + net_watts_3 
self . m_blk_b [ Field . Net_Calc_Watts_Ln_1 ] [ MeterData . NativeValue ] = net_watts_1 
self . m_blk_b [ Field . Net_Calc_Watts_Ln_2 ] [ MeterData . NativeValue ] = net_watts_2 
self . m_blk_b [ Field . Net_Calc_Watts_Ln_3 ] [ MeterData . NativeValue ] = net_watts_3 
self . m_blk_b [ Field . Net_Calc_Watts_Tot ] [ MeterData . NativeValue ] = net_watts_tot 
self . m_blk_b [ Field . Net_Calc_Watts_Ln_1 ] [ MeterData . StringValue ] = str ( net_watts_1 ) 
self . m_blk_b [ Field . Net_Calc_Watts_Ln_2 ] [ MeterData . StringValue ] = str ( net_watts_2 ) 
self . m_blk_b [ Field . Net_Calc_Watts_Ln_3 ] [ MeterData . StringValue ] = str ( net_watts_3 ) 
self . m_blk_b [ Field . Net_Calc_Watts_Tot ] [ MeterData . StringValue ] = str ( net_watts_tot ) 
~~ def setLCDCmd ( self , display_list , password = "00000000" ) : 
~~~ self . initLcd ( ) 
item_cnt = len ( display_list ) 
if ( item_cnt > 45 ) or ( item_cnt <= 0 ) : 
~~ for display_item in display_list : 
~~~ self . addLcdItem ( int ( display_item ) ) 
~~ result = self . setLCD ( password ) 
~~ def setRelay ( self , seconds , relay , status , password = "00000000" ) : 
self . setContext ( "setRelay" ) 
if len ( password ) != 8 : 
~~ if seconds < 0 or seconds > 9999 : 
~~ if not self . requestA ( ) : 
~~~ req_str = "" 
req_str = ( "01573102303038" + 
binascii . hexlify ( str ( relay ) ) . zfill ( 2 ) + 
"28" + 
binascii . hexlify ( str ( status ) ) . zfill ( 2 ) + 
binascii . hexlify ( str ( seconds ) . zfill ( 4 ) ) + "2903" ) 
~~ def serialPostEnd ( self ) : 
~~~ self . m_serial_port . write ( "0142300375" . decode ( "hex" ) ) 
~~ def setPulseInputRatio ( self , line_in , new_cnst , password = "00000000" ) : 
self . setContext ( "setPulseInputRatio" ) 
~~~ if not self . requestA ( ) : 
~~~ req_const = binascii . hexlify ( str ( new_cnst ) . zfill ( 4 ) ) 
line_const = binascii . hexlify ( str ( line_in - 1 ) ) 
req_str = "01573102303041" + line_const + "28" + req_const + "2903" 
~~ def setZeroResettableKWH ( self , password = "00000000" ) : 
self . setContext ( "setZeroResettableKWH" ) 
~~~ req_str = "0157310230304433282903" 
~~ def setLCD ( self , password = "00000000" ) : 
self . setContext ( "setLCD" ) 
~~ if not self . request ( ) : 
fill_len = 40 - len ( self . m_lcd_items ) 
for lcdid in self . m_lcd_items : 
~~~ append_val = binascii . hexlify ( str ( lcdid ) . zfill ( 2 ) ) 
req_table += append_val 
~~ for i in range ( 0 , fill_len ) : 
~~~ append_val = binascii . hexlify ( str ( 0 ) . zfill ( 2 ) ) 
~~ req_str = "015731023030443228" + req_table + "2903" 
~~ def apply_argument_parser ( argumentsParser , options = None ) : 
if options is not None : 
~~~ args = argumentsParser . parse_args ( options ) 
~~~ args = argumentsParser . parse_args ( ) 
~~ return args 
~~ def freeze_from_checkpoint ( input_checkpoint , output_file_path , output_node_names ) : 
check_input_checkpoint ( input_checkpoint ) 
output_node_names = output_node_names_string_as_list ( output_node_names ) 
with tf . Session ( ) as sess : 
~~~ restore_from_checkpoint ( sess , input_checkpoint ) 
freeze_graph . freeze_graph_with_def_protos ( input_graph_def = sess . graph_def , input_saver_def = None , 
input_checkpoint = input_checkpoint , 
output_node_names = ',' . join ( output_node_names ) , 
restore_op_name = 'save/restore_all' , 
filename_tensor_name = 'save/Const:0' , output_graph = output_file_path , 
clear_devices = True , initializer_nodes = '' ) 
~~ ~~ def freeze ( sess , output_file_path , output_node_names ) : 
with TemporaryDirectory ( ) as temp_dir_name : 
~~~ checkpoint_path = os . path . join ( temp_dir_name , 'model.ckpt' ) 
tf . train . Saver ( ) . save ( sess , checkpoint_path ) 
freeze_from_checkpoint ( checkpoint_path , output_file_path , output_node_names ) 
~~ ~~ def save_graph_only ( sess , output_file_path , output_node_names , as_text = False ) : 
for node in sess . graph_def . node : 
~~~ node . device = '' 
~~ graph_def = graph_util . extract_sub_graph ( sess . graph_def , output_node_names ) 
output_dir , output_filename = os . path . split ( output_file_path ) 
graph_io . write_graph ( graph_def , output_dir , output_filename , as_text = as_text ) 
~~ def save_graph_only_from_checkpoint ( input_checkpoint , output_file_path , output_node_names , as_text = False ) : 
save_graph_only ( sess , output_file_path , output_node_names , as_text = as_text ) 
~~ ~~ def save_weights ( sess , output_path , conv_var_names = None , conv_transpose_var_names = None ) : 
if not conv_var_names : 
~~~ conv_var_names = [ ] 
~~ if not conv_transpose_var_names : 
~~~ conv_transpose_var_names = [ ] 
~~ for var in tf . trainable_variables ( ) : 
~~~ filename = '{}-{}' . format ( output_path , var . name . replace ( ':' , '-' ) . replace ( '/' , '-' ) ) 
if var . name in conv_var_names : 
~~~ var = tf . transpose ( var , perm = [ 3 , 0 , 1 , 2 ] ) 
~~ elif var . name in conv_transpose_var_names : 
~~~ var = tf . transpose ( var , perm = [ 3 , 1 , 0 , 2 ] ) 
~~ value = sess . run ( var ) 
with open ( filename , 'w' ) as file_ : 
~~~ value . tofile ( file_ ) 
~~ ~~ ~~ def save_weights_from_checkpoint ( input_checkpoint , output_path , conv_var_names = None , conv_transpose_var_names = None ) : 
save_weights ( sess , output_path , conv_var_names = conv_var_names , 
conv_transpose_var_names = conv_transpose_var_names ) 
~~ ~~ def restore_from_checkpoint ( sess , input_checkpoint ) : 
saver = tf . train . import_meta_graph ( '{}.meta' . format ( input_checkpoint ) ) 
saver . restore ( sess , input_checkpoint ) 
return saver 
~~ def caffe_to_tensorflow_session ( caffe_def_path , caffemodel_path , inputs , graph_name = 'Graph' , 
conversion_out_dir_path = None , use_padding_same = False ) : 
~~~ from caffeflow import convert 
~~ with ( dummy_context_mgr ( conversion_out_dir_path ) or util . TemporaryDirectory ( ) ) as dir_path : 
~~~ params_values_output_path = os . path . join ( dir_path , 'params_values.npy' ) 
network_output_path = os . path . join ( dir_path , 'network.py' ) 
convert . convert ( caffe_def_path , caffemodel_path , params_values_output_path , network_output_path , False , 
use_padding_same = use_padding_same ) 
network_module = imp . load_source ( 'module.name' , network_output_path ) 
network_class = getattr ( network_module , graph_name ) 
network = network_class ( inputs ) 
sess = tf . Session ( ) 
network . load ( params_values_output_path , sess ) 
return sess 
~~ ~~ def freeze ( caffe_def_path , caffemodel_path , inputs , output_file_path , output_node_names , graph_name = 'Graph' , 
conversion_out_dir_path = None , checkpoint_out_path = None , use_padding_same = False ) : 
with caffe_to_tensorflow_session ( caffe_def_path , caffemodel_path , inputs , graph_name = graph_name , 
conversion_out_dir_path = conversion_out_dir_path , 
use_padding_same = use_padding_same ) as sess : 
~~~ saver = tf . train . Saver ( ) 
with ( dummy_context_mgr ( checkpoint_out_path ) or util . TemporaryDirectory ( ) ) as temp_dir_path : 
~~~ checkpoint_path = checkpoint_out_path or os . path . join ( temp_dir_path , 'pose.ckpt' ) 
saver . save ( sess , checkpoint_path ) 
output_node_names = util . output_node_names_string_as_list ( output_node_names ) 
tf_freeze . freeze_from_checkpoint ( checkpoint_path , output_file_path , output_node_names ) 
~~ ~~ ~~ def save_graph_only ( caffe_def_path , caffemodel_path , inputs , output_file_path , output_node_names , graph_name = 'Graph' , 
use_padding_same = False ) : 
~~~ tf_freeze . save_graph_only ( sess , output_file_path , output_node_names ) 
~~ ~~ def save_weights ( caffe_def_path , caffemodel_path , inputs , output_path , graph_name = 'Graph' , conv_var_names = None , 
conv_transpose_var_names = None , use_padding_same = False ) : 
~~~ tf_freeze . save_weights ( sess , output_path , conv_var_names = conv_var_names , 
~~ ~~ def make_rows ( num_columns , seq ) : 
num_rows , partial = divmod ( len ( seq ) , num_columns ) 
if partial : 
~~~ num_rows += 1 
~~~ result = more_itertools . grouper ( seq , num_rows ) 
~~~ result = more_itertools . grouper ( num_rows , seq ) 
~~ return zip ( * result ) 
~~ def bisect ( seq , func = bool ) : 
queues = GroupbySaved ( seq , func ) 
return queues . get_first_n_queues ( 2 ) 
~~ def grouper_nofill_str ( n , iterable ) : 
res = more_itertools . chunked ( iterable , n ) 
if isinstance ( iterable , six . string_types ) : 
~~~ res = ( '' . join ( item ) for item in res ) 
~~ def flatten ( subject , test = None ) : 
DeprecationWarning , 
stacklevel = 2 ) 
return list ( more_itertools . collapse ( subject , base_type = ( bytes , ) ) ) 
~~ def every_other ( iterable ) : 
items = iter ( iterable ) 
~~~ yield next ( items ) 
next ( items ) 
~~ ~~ ~~ def remove_duplicates ( iterable , key = None ) : 
return itertools . chain . from_iterable ( six . moves . map ( 
every_other , six . moves . map ( 
operator . itemgetter ( 1 ) , 
itertools . groupby ( iterable , key ) 
) ) ) 
~~ def peek ( iterable ) : 
peeker , original = itertools . tee ( iterable ) 
return next ( peeker ) , original 
~~ def takewhile_peek ( predicate , iterable ) : 
~~~ if not predicate ( iterable . peek ( ) ) : 
~~ yield next ( iterable ) 
~~ ~~ ~~ def nwise ( iter , n ) : 
iterset = [ iter ] 
while len ( iterset ) < n : 
~~~ iterset [ - 1 : ] = itertools . tee ( iterset [ - 1 ] ) 
next ( iterset [ - 1 ] , None ) 
~~ return six . moves . zip ( * iterset ) 
~~ def window ( iter , pre_size = 1 , post_size = 1 ) : 
pre_iter , iter = itertools . tee ( iter ) 
pre_iter = itertools . chain ( ( None , ) * pre_size , pre_iter ) 
pre_iter = nwise ( pre_iter , pre_size ) 
post_iter , iter = itertools . tee ( iter ) 
post_iter = itertools . chain ( post_iter , ( None , ) * post_size ) 
post_iter = nwise ( post_iter , post_size ) 
next ( post_iter , None ) 
return six . moves . zip ( pre_iter , iter , post_iter ) 
~~ def partition_items ( count , bin_size ) : 
num_bins = int ( math . ceil ( count / float ( bin_size ) ) ) 
bins = [ 0 ] * num_bins 
for i in range ( count ) : 
~~~ bins [ i % num_bins ] += 1 
~~ return bins 
~~ def balanced_rows ( n , iterable , fillvalue = None ) : 
iterable , iterable_copy = itertools . tee ( iterable ) 
count = len ( tuple ( iterable_copy ) ) 
for allocation in partition_items ( count , n ) : 
~~~ row = itertools . islice ( iterable , allocation ) 
if allocation < n : 
~~~ row = itertools . chain ( row , [ fillvalue ] ) 
~~ yield tuple ( row ) 
~~ ~~ def always_iterable ( item ) : 
base_types = six . text_type , bytes , collections . abc . Mapping 
return more_itertools . always_iterable ( item , base_type = base_types ) 
~~ def suppress_exceptions ( callables , * exceptions ) : 
if not exceptions : 
~~~ exceptions = Exception , 
~~ for callable in callables : 
~~~ yield callable ( ) 
~~ except exceptions : 
~~ ~~ ~~ def duplicates ( * iterables , ** kwargs ) : 
key = kwargs . pop ( 'key' , lambda x : x ) 
assert not kwargs 
zipped = more_itertools . collate ( * iterables , key = key ) 
grouped = itertools . groupby ( zipped , key = key ) 
groups = ( 
tuple ( g ) 
for k , g in grouped 
def has_dupes ( group ) : 
~~~ return len ( group ) > 1 
~~ return filter ( has_dupes , groups ) 
~~ def assert_ordered ( iterable , key = lambda x : x , comp = operator . le ) : 
err_tmpl = ( 
for pair in more_itertools . pairwise ( iterable ) : 
~~~ keyed = tuple ( map ( key , pair ) ) 
assert comp ( * keyed ) , err_tmpl . format ( ** locals ( ) ) 
yield pair [ 0 ] 
~~ yield pair [ 1 ] 
~~ def collate_revs ( old , new , key = lambda x : x , merge = lambda old , new : new ) : 
missing = object ( ) 
def maybe_merge ( * items ) : 
def not_missing ( ob ) : 
~~~ return ob is not missing 
~~ return functools . reduce ( merge , filter ( not_missing , items ) ) 
~~ new_items = collections . OrderedDict ( 
( key ( el ) , el ) 
for el in new 
old_items = collections . OrderedDict ( 
for el in old 
for old_key , old_item in _mutable_iter ( old_items ) : 
~~~ if old_key not in new_items : 
~~~ yield old_item 
~~ before , match_new , new_items = _swap_on_miss ( 
partition_dict ( new_items , old_key ) ) 
for new_key , new_item in before . items ( ) : 
~~~ yield maybe_merge ( new_item , old_items . pop ( new_key , missing ) ) 
~~ yield merge ( old_item , match_new ) 
~~ for item in new_items . values ( ) : 
~~ ~~ def _mutable_iter ( dict ) : 
while dict : 
~~~ prev_key = next ( iter ( dict ) ) 
yield prev_key , dict . pop ( prev_key ) 
~~ ~~ def _swap_on_miss ( partition_result ) : 
before , item , after = partition_result 
return ( before , item , after ) if item else ( after , item , before ) 
~~ def partition_dict ( items , key ) : 
def unmatched ( pair ) : 
~~~ test_key , item , = pair 
return test_key != key 
~~ items_iter = iter ( items . items ( ) ) 
item = items . get ( key ) 
left = collections . OrderedDict ( itertools . takewhile ( unmatched , items_iter ) ) 
right = collections . OrderedDict ( items_iter ) 
return left , item , right 
~~ def get_first_n_queues ( self , n ) : 
~~~ while len ( self . queues ) < n : 
~~~ self . __fetch__ ( ) 
~~ ~~ except StopIteration : 
~~ values = list ( self . queues . values ( ) ) 
missing = n - len ( values ) 
values . extend ( iter ( [ ] ) for n in range ( missing ) ) 
return values 
~~ def reset ( self ) : 
self . __iterator , self . __saved = itertools . tee ( self . __saved ) 
~~ def descendant ( self , chain_path ) : 
public_child = self . hdkeychain 
chain_step_bytes = 4 
max_bits_per_step = 2 ** 31 
chain_steps = [ 
int ( chain_path [ i : i + chain_step_bytes * 2 ] , 16 ) % max_bits_per_step 
for i in range ( 0 , len ( chain_path ) , chain_step_bytes * 2 ) 
for step in chain_steps : 
~~~ public_child = public_child . get_child ( step ) 
~~ return PublicKeychain ( public_child ) 
~~ def bip32_serialize ( rawtuple ) : 
vbytes , depth , fingerprint , i , chaincode , key = rawtuple 
i = encode ( i , 256 , 4 ) 
chaincode = encode ( hash_to_int ( chaincode ) , 256 , 32 ) 
keydata = b'\\x00' + key [ : - 1 ] if vbytes in PRIVATE else key 
bindata = vbytes + from_int_to_byte ( depth % 256 ) + fingerprint + i + chaincode + keydata 
return changebase ( bindata + bin_dbl_sha256 ( bindata ) [ : 4 ] , 256 , 58 ) 
~~ def bip32_deserialize ( data ) : 
dbin = changebase ( data , 58 , 256 ) 
if bin_dbl_sha256 ( dbin [ : - 4 ] ) [ : 4 ] != dbin [ - 4 : ] : 
~~ vbytes = dbin [ 0 : 4 ] 
depth = from_byte_to_int ( dbin [ 4 ] ) 
fingerprint = dbin [ 5 : 9 ] 
i = decode ( dbin [ 9 : 13 ] , 256 ) 
chaincode = dbin [ 13 : 45 ] 
key = dbin [ 46 : 78 ] + b'\\x01' if vbytes in PRIVATE else dbin [ 45 : 78 ] 
return ( vbytes , depth , fingerprint , i , chaincode , key ) 
~~ def generate ( ctx , url , * args , ** kwargs ) : 
file_previews = ctx . obj [ 'file_previews' ] 
metadata = kwargs [ 'metadata' ] 
width = kwargs [ 'width' ] 
height = kwargs [ 'height' ] 
output_format = kwargs [ 'format' ] 
if metadata : 
~~~ options [ 'metadata' ] = metadata . split ( ',' ) 
~~ if width : 
~~~ options . setdefault ( 'size' , { } ) 
options [ 'size' ] [ 'width' ] = width 
~~ if height : 
options [ 'size' ] [ 'height' ] = height 
~~ if output_format : 
~~~ options [ 'format' ] = output_format 
~~ results = file_previews . generate ( url , ** options ) 
click . echo ( results ) 
~~ def retrieve ( ctx , preview_id , * args , ** kwargs ) : 
results = file_previews . retrieve ( preview_id ) 
~~ def get_monoprice ( port_url ) : 
lock = RLock ( ) 
def synchronized ( func ) : 
~~~ with lock : 
~~~ return func ( * args , ** kwargs ) 
~~ class MonopriceSync ( Monoprice ) : 
~~~ def __init__ ( self , port_url ) : 
~~~ self . _port = serial . serial_for_url ( port_url , do_not_open = True ) 
self . _port . baudrate = 9600 
self . _port . stopbits = serial . STOPBITS_ONE 
self . _port . bytesize = serial . EIGHTBITS 
self . _port . parity = serial . PARITY_NONE 
self . _port . timeout = TIMEOUT 
self . _port . write_timeout = TIMEOUT 
self . _port . open ( ) 
~~ def _process_request ( self , request : bytes , skip = 0 ) : 
_LOGGER . debug ( \ , request ) 
self . _port . reset_output_buffer ( ) 
self . _port . reset_input_buffer ( ) 
self . _port . write ( request ) 
self . _port . flush ( ) 
result = bytearray ( ) 
~~~ c = self . _port . read ( 1 ) 
~~~ raise serial . SerialTimeoutException ( 
~~ result += c 
if len ( result ) > skip and result [ - LEN_EOL : ] == EOL : 
~~ ~~ ret = bytes ( result ) 
_LOGGER . debug ( \ , ret ) 
return ret . decode ( 'ascii' ) 
~~ @ synchronized 
def zone_status ( self , zone : int ) : 
~~~ return ZoneStatus . from_string ( self . _process_request ( _format_zone_status_request ( zone ) , skip = 6 ) ) 
def set_power ( self , zone : int , power : bool ) : 
~~~ self . _process_request ( _format_set_power ( zone , power ) ) 
def set_mute ( self , zone : int , mute : bool ) : 
~~~ self . _process_request ( _format_set_mute ( zone , mute ) ) 
def set_volume ( self , zone : int , volume : int ) : 
~~~ self . _process_request ( _format_set_volume ( zone , volume ) ) 
def set_treble ( self , zone : int , treble : int ) : 
~~~ self . _process_request ( _format_set_treble ( zone , treble ) ) 
def set_bass ( self , zone : int , bass : int ) : 
~~~ self . _process_request ( _format_set_bass ( zone , bass ) ) 
def set_balance ( self , zone : int , balance : int ) : 
~~~ self . _process_request ( _format_set_balance ( zone , balance ) ) 
def set_source ( self , zone : int , source : int ) : 
~~~ self . _process_request ( _format_set_source ( zone , source ) ) 
def restore_zone ( self , status : ZoneStatus ) : 
~~~ self . set_power ( status . zone , status . power ) 
self . set_mute ( status . zone , status . mute ) 
self . set_volume ( status . zone , status . volume ) 
self . set_treble ( status . zone , status . treble ) 
self . set_bass ( status . zone , status . bass ) 
self . set_balance ( status . zone , status . balance ) 
self . set_source ( status . zone , status . source ) 
~~ ~~ return MonopriceSync ( port_url ) 
~~ def get_async_monoprice ( port_url , loop ) : 
lock = asyncio . Lock ( ) 
def locked_coro ( coro ) : 
~~~ @ asyncio . coroutine 
@ wraps ( coro ) 
~~~ with ( yield from lock ) : 
~~~ return ( yield from coro ( * args , ** kwargs ) ) 
~~ class MonopriceAsync ( Monoprice ) : 
~~~ def __init__ ( self , monoprice_protocol ) : 
~~~ self . _protocol = monoprice_protocol 
~~ @ locked_coro 
@ asyncio . coroutine 
~~~ string = yield from self . _protocol . send ( _format_zone_status_request ( zone ) , skip = 6 ) 
return ZoneStatus . from_string ( string ) 
~~~ yield from self . _protocol . send ( _format_set_power ( zone , power ) ) 
~~~ yield from self . _protocol . send ( _format_set_mute ( zone , mute ) ) 
~~~ yield from self . _protocol . send ( _format_set_volume ( zone , volume ) ) 
~~~ yield from self . _protocol . send ( _format_set_treble ( zone , treble ) ) 
~~~ yield from self . _protocol . send ( _format_set_bass ( zone , bass ) ) 
~~~ yield from self . _protocol . send ( _format_set_balance ( zone , balance ) ) 
~~~ yield from self . _protocol . send ( _format_set_source ( zone , source ) ) 
~~~ yield from self . _protocol . send ( _format_set_power ( status . zone , status . power ) ) 
yield from self . _protocol . send ( _format_set_mute ( status . zone , status . mute ) ) 
yield from self . _protocol . send ( _format_set_volume ( status . zone , status . volume ) ) 
yield from self . _protocol . send ( _format_set_treble ( status . zone , status . treble ) ) 
yield from self . _protocol . send ( _format_set_bass ( status . zone , status . bass ) ) 
yield from self . _protocol . send ( _format_set_balance ( status . zone , status . balance ) ) 
yield from self . _protocol . send ( _format_set_source ( status . zone , status . source ) ) 
~~ ~~ class MonopriceProtocol ( asyncio . Protocol ) : 
~~~ def __init__ ( self , loop ) : 
~~~ super ( ) . __init__ ( ) 
self . _loop = loop 
self . _lock = asyncio . Lock ( ) 
self . _transport = None 
self . _connected = asyncio . Event ( loop = loop ) 
self . q = asyncio . Queue ( loop = loop ) 
~~ def connection_made ( self , transport ) : 
~~~ self . _transport = transport 
self . _connected . set ( ) 
~~ def data_received ( self , data ) : 
~~~ asyncio . ensure_future ( self . q . put ( data ) , loop = self . _loop ) 
~~ @ asyncio . coroutine 
def send ( self , request : bytes , skip = 0 ) : 
~~~ yield from self . _connected . wait ( ) 
with ( yield from self . _lock ) : 
~~~ self . _transport . serial . reset_output_buffer ( ) 
self . _transport . serial . reset_input_buffer ( ) 
while not self . q . empty ( ) : 
~~~ self . q . get_nowait ( ) 
~~ self . _transport . write ( request ) 
~~~ result += yield from asyncio . wait_for ( self . q . get ( ) , TIMEOUT , loop = self . _loop ) 
~~~ ret = bytes ( result ) 
~~ ~~ ~~ except asyncio . TimeoutError : 
~~ ~~ ~~ ~~ _ , protocol = yield from create_serial_connection ( loop , functools . partial ( MonopriceProtocol , loop ) , 
port_url , baudrate = 9600 ) 
return MonopriceAsync ( protocol ) 
~~ def from_reply ( cls , state ) : 
if state in ( SASLState . FAILURE , SASLState . SUCCESS , 
SASLState . CHALLENGE ) : 
~~~ return state 
~~ if state in ( "failure" , "success" , "challenge" ) : 
~~~ return SASLState ( state ) 
~~ ~~ def initiate ( self , mechanism , payload = None ) : 
if self . _state != SASLState . INITIAL : 
~~~ next_state , payload = yield from self . interface . initiate ( 
mechanism , 
payload = payload ) 
~~ except SASLFailure : 
~~~ self . _state = SASLState . FAILURE 
~~ next_state = SASLState . from_reply ( next_state ) 
self . _state = next_state 
return next_state , payload 
~~ def response ( self , payload ) : 
if self . _state == SASLState . SUCCESS_SIMULATE_CHALLENGE : 
~~~ if payload != b"" : 
raise SASLFailure ( 
~~ self . _state = SASLState . SUCCESS 
return SASLState . SUCCESS , None 
~~ if self . _state != SASLState . CHALLENGE : 
~~~ raise RuntimeError ( 
~~~ next_state , payload = yield from self . interface . respond ( payload ) 
if next_state == SASLState . SUCCESS and payload is not None : 
~~~ self . _state = SASLState . SUCCESS_SIMULATE_CHALLENGE 
return SASLState . CHALLENGE , payload 
~~ self . _state = next_state 
~~ def abort ( self ) : 
if self . _state == SASLState . INITIAL : 
~~ if self . _state == SASLState . SUCCESS_SIMULATE_CHALLENGE : 
~~~ return ( yield from self . interface . abort ( ) ) 
~~ ~~ def _saslprep_do_mapping ( chars ) : 
while i < len ( chars ) : 
~~~ c = chars [ i ] 
if stringprep . in_table_c12 ( c ) : 
~~~ chars [ i ] = "\\u0020" 
~~ elif stringprep . in_table_b1 ( c ) : 
~~~ del chars [ i ] 
~~ ~~ def trace ( string ) : 
check_prohibited_output ( 
string , 
stringprep . in_table_c21 , 
stringprep . in_table_c22 , 
stringprep . in_table_c3 , 
stringprep . in_table_c4 , 
stringprep . in_table_c5 , 
stringprep . in_table_c6 , 
stringprep . in_table_c8 , 
stringprep . in_table_c9 , 
check_bidi ( string ) 
return string 
~~ def xor_bytes ( a , b ) : 
assert len ( a ) == len ( b ) 
return bytes ( map ( operator . xor , a , b ) ) 
~~ def build_payment_parameters ( amount : Money , client_ref : str ) -> PaymentParameters : 
merchant_id = web_merchant_id 
amount , currency = money_to_amount_and_currency ( amount ) 
refno = client_ref 
sign = sign_web ( merchant_id , amount , currency , refno ) 
parameters = PaymentParameters ( 
merchant_id = merchant_id , 
amount = amount , 
currency = currency , 
refno = refno , 
sign = sign , 
use_alias = False , 
logger . info ( 'build-payment-parameters' , parameters = parameters ) 
return parameters 
~~ def build_register_credit_card_parameters ( client_ref : str ) -> PaymentParameters : 
amount = 0 
use_alias = True , 
logger . info ( 'building-payment-parameters' , parameters = parameters ) 
~~ def pay_with_alias ( amount : Money , alias_registration_id : str , client_ref : str ) -> Payment : 
if amount . amount <= 0 : 
~~ alias_registration = AliasRegistration . objects . get ( pk = alias_registration_id ) 
logger . info ( 'paying-with-alias' , amount = amount , client_ref = client_ref , 
alias_registration = alias_registration ) 
request_xml = build_pay_with_alias_request_xml ( amount , client_ref , alias_registration ) 
logger . info ( 'sending-pay-with-alias-request' , url = datatrans_authorize_url , data = request_xml ) 
response = requests . post ( 
url = datatrans_authorize_url , 
headers = { 'Content-Type' : 'application/xml' } , 
data = request_xml ) 
logger . info ( 'processing-pay-with-alias-response' , response = response . content ) 
charge_response = parse_pay_with_alias_response_xml ( response . content ) 
charge_response . save ( ) 
charge_response . send_signal ( ) 
return charge_response 
~~ def parse_notification_xml ( xml : str ) -> Union [ AliasRegistration , Payment ] : 
body = fromstring ( xml ) . find ( 'body' ) 
transaction = body . find ( 'transaction' ) 
_user_parameters = transaction . find ( 'userParameters' ) 
def get_named_parameter ( name ) : 
~~~ return _user_parameters . find ( "parameter[@name=\ + name + "\ ) 
~~ def success ( ) : 
~~~ return transaction . get ( 'status' ) == 'success' 
~~ def parse_success ( ) : 
~~~ computed_signature = sign_web ( body . get ( 'merchantId' ) , transaction . find ( 'amount' ) . text , 
transaction . find ( 'currency' ) . text , 
transaction . find ( 'uppTransactionId' ) . text ) 
sign2 = get_named_parameter ( 'sign2' ) . text 
if computed_signature != sign2 : 
~~ success = transaction . find ( 'success' ) 
d = dict ( 
response_code = success . find ( 'responseCode' ) . text , 
response_message = success . find ( 'responseMessage' ) . text , 
authorization_code = success . find ( 'authorizationCode' ) . text , 
acquirer_authorization_code = success . find ( 'acqAuthorizationCode' ) . text , 
return { k : v for k , v in d . items ( ) if v is not None } 
~~ def parse_error ( ) : 
~~~ error = transaction . find ( 'error' ) 
error_code = error . find ( 'errorCode' ) . text , 
error_message = error . find ( 'errorMessage' ) . text , 
error_detail = error . find ( 'errorDetail' ) . text ) 
acquirer_error_code = get_named_parameter ( 'acqErrorCode' ) 
if acquirer_error_code is not None : 
~~~ d [ 'acquirer_error_code' ] = acquirer_error_code . text 
~~ return { k : v for k , v in d . items ( ) if v is not None } 
~~ def parse_common_attributes ( ) : 
~~~ d = dict ( 
transaction_id = transaction . find ( 'uppTransactionId' ) . text , 
merchant_id = body . get ( 'merchantId' ) , 
client_ref = transaction . get ( 'refno' ) , 
amount = parse_money ( transaction ) ) 
payment_method = transaction . find ( 'pmethod' ) 
if payment_method is not None : 
~~~ d [ 'payment_method' ] = payment_method . text 
~~ request_type = transaction . find ( 'reqtype' ) 
if request_type is not None : 
~~~ d [ 'request_type' ] = request_type . text 
~~ credit_card_country = get_named_parameter ( 'returnCustomerCountry' ) 
if credit_card_country is not None : 
~~~ d [ 'credit_card_country' ] = credit_card_country . text 
~~ expiry_month = get_named_parameter ( 'expm' ) 
if expiry_month is not None : 
~~~ d [ 'expiry_month' ] = int ( expiry_month . text ) 
~~ expiry_year = get_named_parameter ( 'expy' ) 
if expiry_year is not None : 
~~~ d [ 'expiry_year' ] = int ( expiry_year . text ) 
~~ use_alias_parameter = get_named_parameter ( 'useAlias' ) 
if use_alias_parameter is not None and use_alias_parameter . text == 'true' : 
~~~ d = dict ( parse_common_attributes ( ) ) 
masked_card_number = get_named_parameter ( 'maskedCC' ) 
if masked_card_number is not None : 
~~~ d [ 'masked_card_number' ] = masked_card_number . text 
~~ card_alias = get_named_parameter ( 'aliasCC' ) 
if card_alias is not None : 
~~~ d [ 'card_alias' ] = card_alias . text 
~~ if success ( ) : 
~~~ d [ 'success' ] = True 
d . update ( parse_success ( ) ) 
~~~ d [ 'success' ] = False 
d . update ( parse_error ( ) ) 
~~ return AliasRegistration ( ** d ) 
~~~ if success ( ) : 
~~~ d = dict ( success = True ) 
cardno = get_named_parameter ( 'cardno' ) 
if cardno is not None : 
~~~ d [ 'masked_card_number' ] = cardno . text 
~~ d . update ( parse_common_attributes ( ) ) 
return Payment ( ** d ) 
~~~ d = dict ( success = False ) 
d . update ( parse_common_attributes ( ) ) 
~~ ~~ ~~ def refund ( amount : Money , payment_id : str ) -> Refund : 
~~ payment = Payment . objects . get ( pk = payment_id ) 
if not payment . success : 
~~ if payment . amount . currency != amount . currency : 
~~ if amount . amount > payment . amount . amount : 
~~ logger . info ( 'refunding-payment' , amount = str ( amount ) , 
payment = dict ( amount = str ( payment . amount ) , transaction_id = payment . transaction_id , 
masked_card_number = payment . masked_card_number ) ) 
client_ref = '{}-r' . format ( payment . client_ref ) 
request_xml = build_refund_request_xml ( amount = amount , 
original_transaction_id = payment . transaction_id , 
client_ref = client_ref , 
merchant_id = payment . merchant_id ) 
logger . info ( 'sending-refund-request' , url = datatrans_processor_url , data = request_xml ) 
url = datatrans_processor_url , 
logger . info ( 'processing-refund-response' , response = response . content ) 
refund_response = parse_refund_response_xml ( response . content ) 
refund_response . save ( ) 
refund_response . send_signal ( ) 
return refund_response 
~~ def call ( args , stdout = None , stderr = None , stdin = None , daemonize = False , 
preexec_fn = None , shell = False , cwd = None , env = None ) : 
stream = lambda s , m : s is None and os . open ( os . devnull , m ) or s 
stdout = stream ( stdout , os . O_WRONLY ) 
stderr = stream ( stderr , os . O_WRONLY ) 
stdin = stream ( stdin , os . O_RDONLY ) 
shared_pid = Value ( 'i' , 0 ) 
pid = os . fork ( ) 
if pid > 0 : 
~~~ os . waitpid ( pid , 0 ) 
child_pid = shared_pid . value 
del shared_pid 
if daemonize : 
~~~ sys . exit ( 0 ) 
~~ return child_pid 
~~~ os . setsid ( ) 
proc = subprocess . Popen ( args , stdout = stdout , stderr = stderr , stdin = stdin , close_fds = True , 
preexec_fn = preexec_fn , shell = shell , cwd = cwd , env = env ) 
shared_pid . value = proc . pid 
os . _exit ( 0 ) 
~~ ~~ def _get_max_fd ( self ) : 
limits = resource . getrlimit ( resource . RLIMIT_NOFILE ) 
result = limits [ 1 ] 
if result == resource . RLIM_INFINITY : 
~~~ result = maxfd 
~~ def _close_fd ( self , fd ) : 
~~~ os . close ( fd ) 
~~ except OSError , exc : 
~~~ if exc . errno != errno . EBADF : 
raise Error ( msg ) 
~~ ~~ ~~ def _close_open_fds ( self ) : 
maxfd = self . _get_max_fd ( ) 
for fd in reversed ( range ( maxfd ) ) : 
~~~ if fd not in self . exclude_fds : 
~~~ self . _close_fd ( fd ) 
~~ ~~ ~~ def _redirect ( self , stream , target ) : 
~~~ target_fd = os . open ( os . devnull , os . O_RDWR ) 
~~~ target_fd = target . fileno ( ) 
~~ os . dup2 ( target_fd , stream . fileno ( ) ) 
~~ def set_form_widgets_attrs ( form , attrs ) : 
for _ , field in form . fields . items ( ) : 
~~~ attrs_ = dict ( attrs ) 
for name , val in attrs . items ( ) : 
~~~ if hasattr ( val , '__call__' ) : 
~~~ attrs_ [ name ] = val ( field ) 
~~ ~~ field . widget . attrs = field . widget . build_attrs ( attrs_ ) 
~~ ~~ def get_model_class_from_string ( model_path ) : 
~~~ app_name , model_name = model_path . split ( '.' ) 
~~ if apps_get_model is None : 
~~~ model = get_model ( app_name , model_name ) 
~~~ model = apps_get_model ( app_name , model_name ) 
~~ except ( LookupError , ValueError ) : 
~~~ model = None 
~~ ~~ if model is None : 
~~ return model 
~~ def get_site_url ( request = None ) : 
env = partial ( environ . get ) 
settings_ = partial ( getattr , settings ) 
domain = None 
scheme = None 
for src in ( env , settings_ ) : 
~~~ if url is None : 
~~~ url = src ( 'SITE_URL' , None ) 
~~ if domain is None : 
~~~ domain = src ( 'SITE_DOMAIN' , None ) 
~~ if scheme is None : 
~~~ scheme = src ( 'SITE_PROTO' , src ( 'SITE_SCHEME' , None ) ) 
~~ ~~ if domain is None and url is not None : 
~~~ scheme , domain = url . split ( '://' ) [ : 2 ] 
~~~ site = get_current_site ( request or DomainGetter ( domain ) ) 
domain = site . domain 
~~ if scheme is None and request : 
~~~ scheme = request . scheme 
~~~ domain = 'undefined-domain.local' 
~~~ scheme = 'http' 
~~ domain = domain . rstrip ( '/' ) 
return '%s://%s' % ( scheme , domain ) 
~~ def import_app_module ( app_name , module_name ) : 
name_split = app_name . split ( '.' ) 
~~~ app_name = '.' . join ( name_split [ : - 2 ] ) 
~~ module = import_module ( app_name ) 
~~~ sub_module = import_module ( '%s.%s' % ( app_name , module_name ) ) 
return sub_module 
~~ ~~ def import_project_modules ( module_name ) : 
from django . conf import settings 
submodules = [ ] 
for app in settings . INSTALLED_APPS : 
~~~ module = import_app_module ( app , module_name ) 
if module is not None : 
~~~ submodules . append ( module ) 
~~ ~~ return submodules 
~~ def include_ ( parser , token ) : 
bits = token . split_contents ( ) 
dynamic = False 
if len ( bits ) >= 2 : 
~~~ dynamic = '{{' in bits [ 1 ] 
if dynamic : 
~~~ fallback = None 
bits_new = [ ] 
for bit in bits : 
~~~ if fallback is True : 
~~~ fallback = bit 
~~ if bit == 'fallback' : 
~~~ fallback = True 
~~~ bits_new . append ( bit ) 
~~ ~~ if fallback : 
~~~ fallback = parser . compile_filter ( construct_relative_path_ ( parser , fallback ) ) 
~~ ~~ token . contents = token . contents . replace ( 'include_' , 'include' ) 
include_node = do_include ( parser , token ) 
~~~ include_node = DynamicIncludeNode ( 
include_node . template , 
extra_context = include_node . extra_context , 
isolated_context = include_node . isolated_context , 
fallback = fallback or None , 
~~ return include_node 
~~ def gravatar_get_url ( obj , size = 65 , default = 'identicon' ) : 
return get_gravatar_url ( obj , size = size , default = default ) 
~~ def gravatar_get_img ( obj , size = 65 , default = 'identicon' ) : 
url = get_gravatar_url ( obj , size = size , default = default ) 
if url : 
~~~ return safe ( \ % url ) 
~~ def register_task ( self , task_def ) : 
r = self . session . post ( 
self . task_url , 
data = task_def , 
headers = { 'Content-Type' : 'application/json' , 'Accept' : 'application/json' } 
task_dict = json . loads ( task_def ) 
if r . status_code == 200 : 
~~ ~~ def delete_task ( self , task_name ) : 
response = self . session . delete ( '%s/%s' % ( self . task_url , task_name ) ) 
~~ elif response . status_code == 400 : 
~~ ~~ def get_input_string_port ( self , port_name , default = None ) : 
if self . __string_input_ports : 
~~~ return self . __string_input_ports . get ( port_name , default ) 
~~ def set_output_string_port ( self , port_name , value ) : 
if not self . __string_output_ports : 
~~~ self . __string_output_ports = { } 
~~ self . __string_output_ports [ port_name ] = value 
~~ def finalize ( self , success_or_fail , message = '' ) : 
if self . __string_output_ports : 
~~~ with open ( os . path . join ( self . output_path , 'ports.json' ) , 'w' ) as opf : 
~~~ json . dump ( self . __string_output_ports , opf , indent = 4 ) 
~~ with open ( os . path . join ( self . base_path , 'status.json' ) , 'w' ) as sf : 
~~~ json . dump ( { 'status' : success_or_fail , 'reason' : message } , sf , indent = 4 ) 
~~ ~~ def list_files ( self , extensions = None ) : 
if self . type . lower ( ) != 'directory' : 
~~ filesystem_location = self . path 
for root , dirs , files in os . walk ( filesystem_location ) : 
~~~ if extensions is None : 
~~~ return [ os . path . join ( root , f ) for f in files ] 
~~ elif not isinstance ( extensions , list ) : 
~~~ extensions = [ extensions ] 
~~ subset_files = [ ] 
for f in files : 
~~~ for extension in extensions : 
~~~ if f . lower ( ) . endswith ( extension . lower ( ) ) : 
~~~ subset_files . append ( os . path . join ( root , f ) ) 
~~ ~~ ~~ return subset_files 
~~ ~~ def is_valid_filesys ( path ) : 
if os . path . isabs ( path ) and os . path . isdir ( path ) and not os . path . isfile ( path ) : 
~~~ raise LocalPortValidationError ( 
~~ ~~ def is_valid_s3_url ( url ) : 
if url . startswith ( 'source:' ) : 
~~ scheme , netloc , path , _ , _ , _ = urlparse ( url ) 
port_except = RemotePortValidationError ( 
if len ( scheme ) < 2 : 
~~~ raise port_except 
~~ if 's3' in scheme or 's3' in netloc or 's3' in path : 
~~ ~~ def invoke ( self ) : 
for key in self . FUNCTION_KEYS . keys ( ) : 
~~~ if self . _arguments [ key ] is True : 
~~~ self . FUNCTION_KEYS [ key ] ( ) 
~~ ~~ ~~ def _register_anonymous_task ( self ) : 
is_overwrite = self . _arguments . get ( '--overwrite' ) 
task_name = "CloudHarness_Anonymous_Task" 
task_srv = TaskService ( ) 
if is_overwrite : 
~~~ code , message = task_srv . delete_task ( task_name ) 
if code not in [ 200 , 400 ] : 
~~~ raise TaskRegistryError ( message ) 
~~ ~~ task_def_file = os . path . join ( 
os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) , 
'gbdx_task_template' , 'task_definition.json' 
with open ( task_def_file , 'r' ) as f : 
~~~ code , message = task_srv . register_task ( f . read ( ) ) 
if code == 200 : 
~~~ print ( message ) 
~~ elif code == 409 : 
~~ ~~ ~~ def _create_app ( self ) : 
template_path = os . path . join ( 
os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) , self . TEMPLATE_FOLDER , self . TEMPLATE_FILENAME 
new_dir = self . _arguments [ '<dir_name>' ] 
override_destination = self . _arguments . get ( '--destination' , None ) 
if override_destination is not None : 
~~~ if override_destination == '' : 
~~ if os . path . isabs ( override_destination ) and os . path . isdir ( override_destination ) : 
~~~ new_dir = os . path . join ( override_destination , new_dir ) 
~~~ override_path = os . path . join ( os . getcwd ( ) , override_destination ) 
if not os . path . isabs ( override_path ) or not os . path . isdir ( override_path ) : 
~~ new_dir = os . path . join ( override_path , new_dir ) 
~~~ if os . path . isabs ( new_dir ) or os . path . sep in new_dir : 
~~ new_dir = os . path . join ( os . getcwd ( ) , new_dir ) 
~~ os . makedirs ( new_dir ) 
new_file_path = os . path . join ( new_dir , self . DEFAULT_NEW_APP_FILENAME ) 
shutil . copyfile ( template_path , new_file_path ) 
~~ def _run_app ( self ) : 
is_remote_run = self . _arguments . get ( '--remote' ) 
filename = self . _arguments . get ( '<file_name>' ) 
upload_ports = self . _arguments . get ( '--upload' ) 
download_ports = self . _arguments . get ( '--download' ) 
is_verbose = self . _arguments . get ( '--verbose' ) 
is_dry_run = self . _arguments . get ( '--dry-run' ) 
~~ if not isinstance ( filename , str ) and issubclass ( filename , TaskTemplate ) : 
~~~ template_class = filename 
template_file = inspect . getfile ( template_class ) 
config_file = self . _write_config_file ( template_file ) 
~~~ template_file = self . _get_template_abs_path ( filename ) 
if not os . path . isfile ( template_file ) : 
~~ config_file = self . _write_config_file ( template_file ) 
template_class = self . _get_class ( template_file ) 
~~ with template_class ( ) as template : 
~~~ task = template . task 
task . source_bundle . value = os . path . join ( os . path . dirname ( template_file ) , 'tmp_%s' % str ( uuid . uuid4 ( ) ) ) 
task . run_name = '{task_name}_src' . format ( task_name = task . name ) 
src_bundle_dir = task . source_bundle . value 
self . _archive_source ( os . path . dirname ( src_bundle_dir ) , src_bundle_dir ) 
port_service = PortService ( task ) 
if upload_ports : 
~~~ port_service . upload_input_ports ( ) 
~~~ port_service . upload_input_ports ( port_list = [ self . SOURCE_BUNDLE_PORT ] ) 
~~ shutil . rmtree ( src_bundle_dir ) 
os . remove ( config_file ) 
task = port_service . task 
task . is_valid ( remote = True ) 
workflow = Workflow ( task ) 
if is_verbose : 
~~~ temp_wf = workflow . json 
printer ( temp_wf ) 
~~ if is_dry_run : 
~~~ return task 
~~~ workflow . execute ( ) 
printer ( workflow . id ) 
~~~ printer ( e . message ) 
~~ is_done = workflow . monitor_run ( ) 
if not is_done : 
~~ if download_ports : 
~~~ template . check_and_create_outputs ( ) 
template . task . is_valid ( ) 
~~~ printer ( template . task . json ( ) ) 
all_ports = template . task . ports [ 0 ] + template . task . ports [ 1 ] 
printer ( [ port . __str__ ( ) for port in all_ports ] ) 
~~~ template . invoke ( ) 
~~ if template . reason is None or template . reason == '' : 
~~ ~~ ~~ ~~ def _write_config_file ( template_file ) : 
config_filename = '.cloud_harness_config.json' 
config_path = os . path . dirname ( template_file ) 
filename = os . path . split ( template_file ) [ 1 ] 
if filename . endswith ( '.pyc' ) : 
~~~ filename = filename [ : - 1 ] 
~~ config_file = os . path . join ( config_path , config_filename ) 
with open ( config_file , 'w' ) as f : 
~~~ f . write ( json . dumps ( { 'task_filename' : filename } ) ) 
~~ return config_file 
~~ def _get_class ( template_file ) : 
with warnings . catch_warnings ( ) : 
~~~ warnings . filterwarnings ( "ignore" , category = RuntimeWarning ) 
template_module = imp . load_source ( 'module.name' , template_file ) 
~~ for name , data in inspect . getmembers ( template_module , inspect . isclass ) : 
~~~ if issubclass ( data , TaskTemplate ) and data . __name__ != TaskTemplate . __name__ : 
~~ ~~ ~~ def _get_template_abs_path ( filename ) : 
if os . path . isabs ( filename ) and os . path . isfile ( filename ) : 
~~~ return filename 
~~~ return os . path . join ( os . getcwd ( ) , filename ) 
~~ ~~ def upload ( self , source_files , s3_folder = None ) : 
if s3_folder is None : 
~~~ folder = self . prefix 
~~~ folder = '%s/%s' % ( self . prefix , s3_folder ) 
~~ if isinstance ( source_files , list ) : 
~~~ for file_tuple in source_files : 
~~~ self . __upload_file ( file_tuple , folder ) 
~~ ~~ elif isinstance ( source_files , tuple ) : 
~~~ self . __upload_file ( source_files , folder ) 
if not os . path . isdir ( local_port_path ) : 
~~ if not isinstance ( key_names , list ) : 
~~~ key_names = [ key_names ] 
~~ for key_name in key_names : 
~~~ is_folder = key_name . endswith ( '/' ) 
key_name = key_name . lstrip ( '/' ) . rstrip ( '/' ) 
key_parts = key_name . split ( '/' ) 
if key_parts [ 0 ] == self . prefix : 
~~~ path = os . path . join ( local_port_path , * key_parts [ 1 : ] ) 
if not is_folder : 
~~~ folder_path = os . path . join ( local_port_path , * key_parts [ 1 : - 1 ] ) 
~~ get_key_name = key_name 
~~~ path = os . path . join ( local_port_path , * key_parts ) 
~~~ folder_path = os . path . join ( local_port_path , * key_parts [ : - 1 ] ) 
~~ get_key_name = '%s/%s' % ( self . prefix , key_name ) 
~~ if is_folder and not os . path . isdir ( path ) : 
~~~ os . makedirs ( path ) 
~~~ if not os . path . isdir ( folder_path ) : 
~~~ os . makedirs ( folder_path ) 
~~ self . __download_file ( path , get_key_name ) 
~~ ~~ ~~ def list ( self , s3_folder = '' , full_key_data = False ) : 
if not s3_folder . startswith ( '/' ) : 
~~~ s3_folder = '/' + s3_folder 
~~ s3_prefix = self . prefix + s3_folder 
bucket_data = self . client . list_objects ( Bucket = self . bucket , Prefix = s3_prefix ) 
if full_key_data : 
~~~ return bucket_data [ 'Contents' ] 
~~~ return [ k [ 'Key' ] for k in bucket_data [ 'Contents' ] ] 
~~ ~~ def _build_worklfow_json ( self ) : 
wf_json = { 'tasks' : [ ] , 'name' : 'cloud-harness_%s' % str ( uuid . uuid4 ( ) ) } 
task_def = json . loads ( self . task_template . json ( ) ) 
d = { 
"name" : task_def [ 'name' ] , 
"outputs" : [ ] , 
"inputs" : [ ] , 
"taskType" : task_def [ 'taskType' ] 
for port in self . task_template . input_ports : 
~~~ port_value = port . value 
if port_value is False : 
~~~ port_value = 'false' 
~~ if port_value is True : 
~~~ port_value = 'true' 
~~ d [ 'inputs' ] . append ( { 
"name" : port . _name , 
"value" : port_value 
~~ for port in self . task_template . output_ports : 
~~~ d [ 'outputs' ] . append ( { 
"name" : port . _name 
~~ wf_json [ 'tasks' ] . append ( d ) 
for port in self . task_template . output_ports : 
~~~ if hasattr ( port , 'stageToS3' ) and port . stageToS3 : 
~~~ save_location = '{customer_storage}/{run_name}/{port}' . format ( 
customer_storage = self . storage . location , 
run_name = self . task_template . run_name , 
port = port . name 
new_task = dict ( ** self . STAGE_TO_S3 ) 
new_task [ 'inputs' ] = [ 
{ 'name' : 'data' , 'source' : '%s:%s' % ( task_def [ 'name' ] , port . _name ) } , 
{ 'name' : 'destination' , 'value' : save_location } 
wf_json [ 'tasks' ] . append ( new_task ) 
~~ ~~ return wf_json 
~~ def execute ( self , override_wf_json = None ) : 
r = self . gbdx . post ( 
self . URL , 
json = self . json if override_wf_json is None else override_wf_json 
~~~ r . raise_for_status ( ) 
self . id = None 
~~ self . id = r . json ( ) [ 'id' ] 
self . _refresh_status ( ) 
spinner = itertools . cycle ( [ '-' , '/' , '|' , '\\\\' ] ) 
while not self . complete : 
~~~ for i in xrange ( 300 ) : 
~~~ sys . stdout . write ( spinner . next ( ) ) 
sys . stdout . flush ( ) 
sys . stdout . write ( '\\b' ) 
time . sleep ( 0.03 ) 
~~ ~~ if self . succeeded : 
~~ ~~ def finalize ( self , success_or_fail , message = '' ) : 
if not self . __remote_run : 
~~~ return json . dumps ( { 'status' : success_or_fail , 'reason' : message } , indent = 4 ) 
~~~ super ( TaskTemplate , self ) . finalize ( success_or_fail , message ) 
~~ ~~ def check_and_create_outputs ( self ) : 
if self . task is None : 
~~ for output_port in self . task . output_ports : 
~~~ if output_port . type == 'directory' : 
~~~ is_file = os . path . isabs ( output_port . value ) and not os . path . isfile ( output_port . value ) 
is_remote = output_port . is_valid_s3_url ( output_port . value ) 
~~ except LocalPortValidationError : 
~~~ is_file = False 
is_remote = None 
~~ except RemotePortValidationError : 
~~~ is_remote = False 
( output_port . name , is_file , is_remote ) ) 
if is_file and not is_remote : 
~~~ os . makedirs ( output_port . value ) 
~~~ self . logit . exception ( e ) 
~~ ~~ ~~ ~~ ~~ ~~ def upload_input_ports ( self , port_list = None , exclude_list = None ) : 
input_ports = self . _task . input_ports 
for port in input_ports : 
~~~ if port_list and port . name not in port_list : 
~~ if exclude_list and port . name in exclude_list : 
~~ if not port . value or not os . path . isabs ( port . value ) or not os . path . isdir ( port . value ) : 
~~ prefix = '{run_name}/{port}' . format ( 
run_name = self . _task . run_name , 
port_files = self . _get_port_files ( port . value , prefix ) 
port . value = '%s/%s' % ( self . s3_root , prefix ) 
if len ( port_files ) == 0 : 
~~~ self . storage . upload ( port_files ) 
~~ ~~ ~~ def _get_port_files ( local_path , prefix ) : 
source_files = [ ] 
for root , dirs , files in os . walk ( local_path , topdown = False ) : 
~~~ for name in files : 
~~~ fname = os . path . join ( root , name ) 
key_name = '%s/%s' % ( prefix , fname [ len ( local_path ) + 1 : ] ) 
source_files . append ( ( fname , key_name ) ) 
~~ ~~ return source_files 
~~ def archive ( folder , dry_run = False ) : 
for f in folder : 
~~~ if not os . path . exists ( f ) : 
~~ ~~ _archive_safe ( folder , PROJ_ARCHIVE , dry_run = dry_run ) 
~~ def _mkdir ( p ) : 
isdir = os . path . isdir 
stack = [ os . path . abspath ( p ) ] 
while not isdir ( stack [ - 1 ] ) : 
~~~ parent_dir = os . path . dirname ( stack [ - 1 ] ) 
stack . append ( parent_dir ) 
~~ while stack : 
~~~ p = stack . pop ( ) 
if not isdir ( p ) : 
~~~ os . mkdir ( p ) 
~~ ~~ ~~ def list ( pattern = ( ) ) : 
globs = [ '*{0}*' . format ( p ) for p in pattern ] + [ '*' ] 
matches = [ ] 
offset = len ( PROJ_ARCHIVE ) + 1 
for suffix in globs : 
~~~ glob_pattern = os . path . join ( PROJ_ARCHIVE , '*' , '*' , suffix ) 
matches . append ( set ( 
f [ offset : ] for f in glob . glob ( glob_pattern ) 
~~ matches = reduce ( lambda x , y : x . intersection ( y ) , 
matches ) 
for m in sorted ( matches ) : 
~~~ print ( m ) 
~~ ~~ def restore ( folder ) : 
if os . path . isdir ( folder ) : 
~~ pattern = os . path . join ( PROJ_ARCHIVE , '*' , '*' , folder ) 
matches = glob . glob ( pattern ) 
if not matches : 
~~ if len ( matches ) > 1 : 
~~ source = sorted ( matches ) [ - 1 ] 
print ( source , '-->' , folder ) 
shutil . move ( source , '.' ) 
~~ def new ( cls , access_token , environment = 'prod' ) : 
api_client = ApiClient . new ( access_token , environment ) 
return cls ( api_client ) 
~~ def list ( self , path ) : 
self . __validate_storage_path ( path ) 
entity = self . api_client . get_entity_by_query ( path = path ) 
if entity [ 'entity_type' ] not in self . __BROWSABLE_TYPES : 
~~~ raise StorageArgumentException ( \ 
'listed' . format ( entity [ 'entity_type' ] ) ) 
~~ entity_uuid = entity [ 'uuid' ] 
file_names = [ ] 
more_pages = True 
page_number = 1 
while more_pages : 
~~~ response = self . api_client . list_folder_content ( 
entity_uuid , page = page_number , ordering = 'name' ) 
more_pages = response [ 'next' ] is not None 
page_number += 1 
for child in response [ 'results' ] : 
~~~ pattern = '/{name}' if child [ 'entity_type' ] == 'folder' else '{name}' 
file_names . append ( pattern . format ( name = child [ 'name' ] ) ) 
~~ ~~ return file_names 
~~ def download_file ( self , path , target_path ) : 
if entity [ 'entity_type' ] != 'file' : 
~~ signed_url = self . api_client . get_signed_url ( entity [ 'uuid' ] ) 
response = self . api_client . download_signed_url ( signed_url ) 
with open ( target_path , "wb" ) as output : 
~~~ for chunk in response . iter_content ( chunk_size = 1024 ) : 
~~~ output . write ( chunk ) 
~~ ~~ ~~ def exists ( self , path ) : 
~~~ metadata = self . api_client . get_entity_by_query ( path = path ) 
~~ except StorageNotFoundException : 
~~ return metadata and 'uuid' in metadata 
~~ def get_parent ( self , path ) : 
self . __validate_storage_path ( path , projects_allowed = False ) 
path_steps = [ step for step in path . split ( '/' ) if step ] 
del path_steps [ - 1 ] 
parent_path = '/{0}' . format ( '/' . join ( path_steps ) ) 
return self . api_client . get_entity_by_query ( path = parent_path ) 
~~ def mkdir ( self , path ) : 
parent_metadata = self . get_parent ( path ) 
self . api_client . create_folder ( path . split ( '/' ) [ - 1 ] , parent_metadata [ 'uuid' ] ) 
~~ def upload_file ( self , local_file , dest_path , mimetype ) : 
self . __validate_storage_path ( dest_path ) 
if dest_path . endswith ( '/' ) : 
~~ if local_file . endswith ( os . path . sep ) : 
~~ new_file = self . api_client . create_file ( 
name = dest_path . split ( '/' ) . pop ( ) , 
content_type = mimetype , 
parent = self . get_parent ( dest_path ) [ 'uuid' ] 
etag = self . api_client . upload_file_content ( new_file [ 'uuid' ] , source = local_file ) 
new_file [ 'etag' ] = etag 
return new_file 
~~ def delete ( self , path ) : 
if entity [ 'entity_type' ] in self . __BROWSABLE_TYPES : 
~~~ contents = self . api_client . list_folder_content ( entity [ 'uuid' ] ) 
if contents [ 'count' ] > 0 : 
~~~ raise StorageArgumentException ( 
~~ self . api_client . delete_folder ( entity [ 'uuid' ] ) 
~~ elif entity [ 'entity_type' ] == 'file' : 
~~~ self . api_client . delete_file ( entity [ 'uuid' ] ) 
~~ ~~ def __validate_storage_path ( cls , path , projects_allowed = True ) : 
if not path or not isinstance ( path , str ) or path [ 0 ] != '/' or path == '/' : 
~~ if not projects_allowed and len ( [ elem for elem in path . split ( '/' ) if elem ] ) == 1 : 
~~ ~~ def is_valid ( self , remote = False ) : 
if len ( self . input_ports ) < 1 : 
~~ if remote : 
~~~ ports = [ 
port for port in self . input_ports if port . type == 'directory' 
for port in ports : 
~~~ port . is_valid_s3_url ( port . value ) 
~~~ all_ports = self . ports [ 0 ] + self . ports [ 1 ] 
ports = [ 
port for port in all_ports if port . type == 'directory' and port . name != 'source_bundle' 
~~~ port . is_valid_filesys ( port . value ) 
storage_client = StorageClient . new ( access_token , environment = environment ) ) 
request = RequestBuilder . request ( environment ) . to_service ( cls . SERVICE_NAME , cls . SERVICE_VERSION ) . throw ( 
StorageForbiddenException , 
if resp . status_code == 403 else None 
) . throw ( 
StorageNotFoundException , 
if resp . status_code == 404 else None 
StorageException , 
if not resp . ok else None 
authenticated_request = request . with_token ( access_token ) 
return cls ( request , authenticated_request ) 
~~ def _prep_params ( params ) : 
return { k : v for ( k , v ) in params . items ( ) if v is not None and k != 'self' } 
~~ def get_entity_details ( self , entity_id ) : 
if not is_valid_uuid ( entity_id ) : 
~~ return self . _authenticated_request . to_endpoint ( 'entity/{}/' . format ( entity_id ) ) . return_body ( ) . get ( ) 
~~ def get_entity_by_query ( self , uuid = None , path = None , metadata = None ) : 
if not ( uuid or path or metadata ) : 
~~ if uuid and not is_valid_uuid ( uuid ) : 
~~ params = locals ( ) . copy ( ) 
~~~ if not isinstance ( metadata , dict ) : 
~~ key , value = next ( iter ( metadata . items ( ) ) ) 
params [ key ] = value 
del params [ 'metadata' ] 
~~ params = self . _prep_params ( params ) 
return self . _authenticated_request . to_endpoint ( 'entity/' ) . with_params ( params ) . return_body ( ) . get ( ) 
~~ def set_metadata ( self , entity_type , entity_id , metadata ) : 
~~ if not isinstance ( metadata , dict ) : 
'dictionary' ) 
~~ return self . _authenticated_request . to_endpoint ( '{}/{}/metadata/' . format ( entity_type , entity_id ) ) . with_json_body ( metadata ) . return_body ( ) . post ( ) 
~~ def get_metadata ( self , entity_type , entity_id ) : 
~~ return self . _authenticated_request . to_endpoint ( '{}/{}/metadata/' . format ( entity_type , entity_id ) ) . return_body ( ) . get ( ) 
~~ def update_metadata ( self , entity_type , entity_id , metadata ) : 
~~ return self . _authenticated_request . to_endpoint ( '{}/{}/metadata/' . format ( entity_type , entity_id ) ) . with_json_body ( metadata ) . return_body ( ) . put ( ) 
~~ def delete_metadata ( self , entity_type , entity_id , metadata_keys ) : 
~~ if not isinstance ( metadata_keys , list ) : 
~~ return self . _authenticated_request . to_endpoint ( '{}/{}/metadata/' . format ( entity_type , entity_id ) ) . with_json_body ( { 'keys' : metadata_keys } ) . return_body ( ) . delete ( ) 
~~ def list_projects ( self , hpc = None , access = None , name = None , collab_id = None , 
page_size = DEFAULT_PAGE_SIZE , page = None , ordering = None ) : 
return self . _authenticated_request . to_endpoint ( 'project/' ) . with_params ( self . _prep_params ( locals ( ) ) ) . return_body ( ) . get ( ) 
~~ def get_project_details ( self , project_id ) : 
if not is_valid_uuid ( project_id ) : 
~~ return self . _authenticated_request . to_endpoint ( 'project/{}/' . format ( project_id ) ) . return_body ( ) . get ( ) 
~~ def create_project ( self , collab_id ) : 
return self . _authenticated_request . to_endpoint ( 'project/' ) . with_json_body ( self . _prep_params ( locals ( ) ) ) . return_body ( ) . post ( ) 
~~ def delete_project ( self , project ) : 
if not is_valid_uuid ( project ) : 
~~ self . _authenticated_request . to_endpoint ( 'project/{}/' . format ( project ) ) . delete ( ) 
~~ def create_folder ( self , name , parent ) : 
if not is_valid_uuid ( parent ) : 
~~ return self . _authenticated_request . to_endpoint ( 'folder/' ) . with_json_body ( self . _prep_params ( locals ( ) ) ) . return_body ( ) . post ( ) 
~~ def get_folder_details ( self , folder ) : 
if not is_valid_uuid ( folder ) : 
~~ return self . _authenticated_request . to_endpoint ( 'folder/{}/' . format ( folder ) ) . return_body ( ) . get ( ) 
~~ def list_folder_content ( self , folder , name = None , entity_type = None , 
content_type = None , page_size = DEFAULT_PAGE_SIZE , 
page = None , ordering = None ) : 
~~ params = self . _prep_params ( locals ( ) ) 
return self . _authenticated_request . to_endpoint ( 'folder/{}/children/' . format ( folder ) ) . with_params ( params ) . return_body ( ) . get ( ) 
~~ def delete_folder ( self , folder ) : 
~~ self . _authenticated_request . to_endpoint ( 'folder/{}/' . format ( folder ) ) . delete ( ) 
~~ def upload_file_content ( self , file_id , etag = None , source = None , content = None ) : 
if not is_valid_uuid ( file_id ) : 
~~ if not ( source or content ) or ( source and content ) : 
~~ resp = self . _authenticated_request . to_endpoint ( 'file/{}/content/upload/' . format ( file_id ) ) . with_body ( content or open ( source , 'rb' ) ) . with_headers ( { 'If-Match' : etag } if etag else { } ) . post ( ) 
if 'ETag' not in resp . headers : 
~~ return resp . headers [ 'ETag' ] 
~~ def copy_file_content ( self , file_id , source_file ) : 
~~ if not is_valid_uuid ( source_file ) : 
~~ self . _authenticated_request . to_endpoint ( 'file/{}/content/' . format ( file_id ) ) . with_headers ( { 'X-Copy-From' : source_file } ) . put ( ) 
~~ def download_file_content ( self , file_id , etag = None ) : 
~~ headers = { 'Accept' : '*/*' } 
if etag : 
~~~ headers [ 'If-None-Match' ] = etag 
~~ resp = self . _authenticated_request . to_endpoint ( 'file/{}/content/' . format ( file_id ) ) . with_headers ( headers ) . get ( ) 
if resp . status_code == 304 : 
~~~ return ( None , None ) 
~~ if 'ETag' not in resp . headers : 
~~ return ( resp . headers [ 'ETag' ] , resp . content ) 
~~ def get_signed_url ( self , file_id ) : 
~~ return self . _authenticated_request . to_endpoint ( 'file/{}/content/secure_link/' . format ( file_id ) ) . return_body ( ) . get ( ) [ 'signed_url' ] 
~~ def delete_file ( self , file_id ) : 
~~ self . _authenticated_request . to_endpoint ( 'file/{}/' . format ( file_id ) ) . delete ( ) 
~~ def to_service ( self , service , version ) : 
service_url = self . _service_locator . get_service_url ( service , version ) 
return self . __copy_and_set ( 'service_url' , self . __strip_trailing_slashes ( service_url ) ) 
~~ def with_headers ( self , headers ) : 
copy = headers . copy ( ) 
copy . update ( self . _headers ) 
return self . __copy_and_set ( 'headers' , copy ) 
~~ def with_params ( self , params ) : 
copy = params . copy ( ) 
copy . update ( self . _params ) 
return self . __copy_and_set ( 'params' , copy ) 
~~ def throw ( self , exception_class , should_throw ) : 
return self . __copy_and_set ( 'throws' , self . _throws + [ ( exception_class , should_throw ) ] ) 
~~ def run_command ( cmd ) : 
cmd , 
stdin = subprocess . PIPE , 
stderr = subprocess . STDOUT , 
outdata , _ = process . communicate ( ) 
return outdata 
~~ ~~ def extract_source ( bundle_path , source_path ) : 
with tarfile . open ( bundle_path , 'r:gz' ) as tf : 
~~~ tf . extractall ( path = source_path ) 
~~ def printer ( data ) : 
if not isinstance ( data , str ) : 
~~~ output = json . dumps ( 
sort_keys = True , 
indent = 4 , 
~~ elif hasattr ( data , 'json' ) : 
~~~ output = data . json ( ) 
~~~ output = data 
~~ sys . stdout . write ( output ) 
sys . stdout . write ( '\\n' ) 
~~ def map_job ( job , func , inputs , * args ) : 
num_partitions = 100 
partition_size = len ( inputs ) / num_partitions 
if partition_size > 1 : 
~~~ for partition in partitions ( inputs , partition_size ) : 
~~~ job . addChildJobFn ( map_job , func , partition , * args ) 
~~~ for sample in inputs : 
~~~ job . addChildJobFn ( func , sample , * args ) 
~~ ~~ ~~ def gatk_genotype_gvcfs ( job , 
gvcfs , 
ref , fai , ref_dict , 
annotations = None , 
emit_threshold = 10.0 , call_threshold = 30.0 , 
unsafe_mode = False ) : 
inputs = { 'genome.fa' : ref , 
'genome.fa.fai' : fai , 
'genome.dict' : ref_dict } 
inputs . update ( gvcfs ) 
work_dir = job . fileStore . getLocalTempDir ( ) 
for name , file_store_id in inputs . iteritems ( ) : 
~~~ job . fileStore . readGlobalFile ( file_store_id , os . path . join ( work_dir , name ) ) 
~~ command = [ '-T' , 'GenotypeGVCFs' , 
'-R' , '/data/genome.fa' , 
'--out' , 'genotyped.vcf' , 
'-stand_emit_conf' , str ( emit_threshold ) , 
'-stand_call_conf' , str ( call_threshold ) ] 
if annotations : 
~~~ for annotation in annotations : 
~~~ command . extend ( [ '-A' , annotation ] ) 
~~ ~~ for uuid in gvcfs . keys ( ) : 
~~~ command . extend ( [ '--variant' , os . path . join ( '/data' , uuid ) ] ) 
~~ if unsafe_mode : 
~~~ command . extend ( [ '-U' , 'ALLOW_SEQ_DICT_INCOMPATIBILITY' ] ) 
'Annotations:\\n{annotations}\\n\\n' 
'Samples:\\n{samples}\\n' . format ( emit_threshold = emit_threshold , 
call_threshold = call_threshold , 
annotations = '\\n' . join ( annotations ) if annotations else '' , 
samples = '\\n' . join ( gvcfs . keys ( ) ) ) ) 
docker_parameters = [ '--rm' , 'log-driver' , 'none' , 
dockerCall ( job = job , workDir = work_dir , 
parameters = command , 
tool = 'quay.io/ucsc_cgl/gatk:3.5--dba6dae49156168a909c43330350c6161dc7ecc2' , 
dockerParameters = docker_parameters ) 
return job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'genotyped.vcf' ) ) 
~~ def run_oncotator ( job , vcf_id , oncotator_db ) : 
inputs = { 'input.vcf' : vcf_id , 
'oncotator_db' : oncotator_db } 
~~~ inputs [ name ] = job . fileStore . readGlobalFile ( file_store_id , os . path . join ( work_dir , name ) ) 
~~ if tarfile . is_tarfile ( inputs [ 'oncotator_db' ] ) : 
~~~ tar = tarfile . open ( inputs [ 'oncotator_db' ] ) 
tar . extractall ( path = work_dir ) 
inputs [ 'oncotator_db' ] = tar . getmembers ( ) [ 0 ] . name 
tar . close ( ) 
~~ command = [ '-i' , 'VCF' , 
'-o' , 'VCF' , 
'--db-dir' , inputs [ 'oncotator_db' ] , 
'input.vcf' , 
'annotated.vcf' , 
tool = 'jpfeil/oncotator:1.9--8fffc356981862d50cfacd711b753700b886b605' , 
return job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'annotated.vcf' ) ) 
~~ def rfxcom ( device ) : 
if device is None : 
~~~ device = app . config . get ( 'DEVICE' ) 
~~ if device is None : 
~~ rfxcom_collect ( device ) 
~~ def create_user ( username ) : 
user = User ( username = username , password = password ) 
db . session . add ( user ) 
db . session . commit ( ) 
~~ def tarball_files ( tar_name , file_paths , output_dir = '.' , prefix = '' ) : 
with tarfile . open ( os . path . join ( output_dir , tar_name ) , 'w:gz' ) as f_out : 
~~~ for file_path in file_paths : 
~~~ if not file_path . startswith ( '/' ) : 
~~ arcname = prefix + os . path . basename ( file_path ) 
f_out . add ( file_path , arcname = arcname ) 
~~ ~~ ~~ def __forall_files ( file_paths , output_dir , op ) : 
for file_path in file_paths : 
~~ dest = os . path . join ( output_dir , os . path . basename ( file_path ) ) 
op ( file_path , dest ) 
~~ ~~ def copy_file_job ( job , name , file_id , output_dir ) : 
fpath = job . fileStore . readGlobalFile ( file_id , os . path . join ( work_dir , name ) ) 
copy_files ( [ fpath ] , output_dir ) 
~~ def consolidate_tarballs_job ( job , fname_to_id ) : 
tar_paths = [ ] 
for fname , file_store_id in fname_to_id . iteritems ( ) : 
~~~ p = job . fileStore . readGlobalFile ( file_store_id , os . path . join ( work_dir , fname + '.tar.gz' ) ) 
tar_paths . append ( ( p , fname ) ) 
~~ output_name = 'foo.tar.gz' 
out_tar = os . path . join ( work_dir , output_name ) 
with tarfile . open ( os . path . join ( work_dir , out_tar ) , 'w:gz' ) as f_out : 
~~~ for tar , fname in tar_paths : 
~~~ with tarfile . open ( tar , 'r' ) as f_in : 
~~~ for tarinfo in f_in : 
~~~ with closing ( f_in . extractfile ( tarinfo ) ) as f_in_file : 
~~~ tarinfo . name = os . path . join ( output_name , fname , os . path . basename ( tarinfo . name ) ) 
f_out . addfile ( tarinfo , fileobj = f_in_file ) 
~~ ~~ ~~ ~~ ~~ return job . fileStore . writeGlobalFile ( out_tar ) 
~~ def _make_parameters ( master_ip , default_parameters , memory , arguments , override_parameters ) : 
require ( ( override_parameters is not None or memory is not None ) and 
( override_parameters is None or memory is None ) , 
parameters = [ ] 
if memory is not None : 
~~~ parameters = [ "--master" , "spark://%s:%s" % ( master_ip , SPARK_MASTER_PORT ) , 
"--conf" , "spark.driver.memory=%sg" % memory , 
"--conf" , "spark.executor.memory=%sg" % memory , 
"--conf" , ( "spark.hadoop.fs.default.name=hdfs://%s:%s" % ( master_ip , HDFS_MASTER_PORT ) ) ] 
~~~ parameters . extend ( override_parameters ) 
~~ parameters . extend ( default_parameters ) 
parameters . append ( '--' ) 
parameters . extend ( arguments ) 
~~ def call_conductor ( job , master_ip , src , dst , memory = None , override_parameters = None ) : 
arguments = [ "-C" , src , dst ] 
docker_parameters = [ '--log-driver' , 'none' , master_ip . docker_parameters ( [ "--net=host" ] ) ] 
dockerCall ( job = job , 
tool = "quay.io/ucsc_cgl/conductor" , 
parameters = _make_parameters ( master_ip , 
memory , 
arguments , 
override_parameters ) , 
~~ def call_adam ( job , master_ip , arguments , 
memory = None , 
override_parameters = None , 
run_local = False , 
native_adam_path = None ) : 
if run_local : 
~~~ master = [ "--master" , "local[*]" ] 
~~~ master = [ "--master" , 
( "spark://%s:%s" % ( master_ip , SPARK_MASTER_PORT ) ) , 
"--conf" , ( "spark.hadoop.fs.default.name=hdfs://%s:%s" % ( master_ip , HDFS_MASTER_PORT ) ) , ] 
~~ default_params = ( master + [ 
"--conf" , "spark.driver.maxResultSize=0" , 
"--conf" , "spark.storage.memoryFraction=0.3" , 
"--conf" , "spark.storage.unrollFraction=0.1" , 
"--conf" , "spark.network.timeout=300s" ] ) 
if native_adam_path is None : 
~~~ docker_parameters = [ '--log-driver' , 'none' , master_ip . docker_parameters ( [ "--net=host" ] ) ] 
tool = "quay.io/ucsc_cgl/adam:962-ehf--6e7085f8cac4b9a927dc9fb06b48007957256b80" , 
dockerParameters = docker_parameters , 
default_params , 
override_parameters ) ) 
~~~ check_call ( [ os . path . join ( native_adam_path , "bin/adam-submit" ) ] + 
default_params + 
arguments ) 
~~ ~~ def docker_parameters ( self , docker_parameters = None ) : 
if self != self . actual : 
~~~ add_host_option = '--add-host=spark-master:' + self . actual 
if docker_parameters is None : 
~~~ docker_parameters = [ add_host_option ] 
~~~ docker_parameters . append ( add_host_option ) 
~~ ~~ return docker_parameters 
~~ def run_mutect ( job , normal_bam , normal_bai , tumor_bam , tumor_bai , ref , ref_dict , fai , cosmic , dbsnp ) : 
file_ids = [ normal_bam , normal_bai , tumor_bam , tumor_bai , ref , fai , ref_dict , cosmic , dbsnp ] 
file_names = [ 'normal.bam' , 'normal.bai' , 'tumor.bam' , 'tumor.bai' , 'ref.fasta' , 
'ref.fasta.fai' , 'ref.dict' , 'cosmic.vcf' , 'dbsnp.vcf' ] 
for file_store_id , name in zip ( file_ids , file_names ) : 
~~ parameters = [ '--analysis_type' , 'MuTect' , 
'--reference_sequence' , 'ref.fasta' , 
'--cosmic' , '/data/cosmic.vcf' , 
'--dbsnp' , '/data/dbsnp.vcf' , 
'--input_file:normal' , '/data/normal.bam' , 
'--input_file:tumor' , '/data/tumor.bam' , 
'--out' , 'mutect.out' , 
'--coverage_file' , 'mutect.cov' , 
'--vcf' , 'mutect.vcf' ] 
dockerCall ( job = job , workDir = work_dir , parameters = parameters , 
tool = 'quay.io/ucsc_cgl/mutect:1.1.7--e8bf09459cf0aecb9f55ee689c2b2d194754cbd3' ) 
output_file_names = [ 'mutect.vcf' , 'mutect.cov' , 'mutect.out' ] 
output_file_paths = [ os . path . join ( work_dir , x ) for x in output_file_names ] 
tarball_files ( 'mutect.tar.gz' , file_paths = output_file_paths , output_dir = work_dir ) 
return job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'mutect.tar.gz' ) ) 
~~ def run_pindel ( job , normal_bam , normal_bai , tumor_bam , tumor_bai , ref , fai ) : 
file_ids = [ normal_bam , normal_bai , tumor_bam , tumor_bai , ref , fai ] 
file_names = [ 'normal.bam' , 'normal.bai' , 'tumor.bam' , 'tumor.bai' , 'ref.fasta' , 'ref.fasta.fai' ] 
~~ with open ( os . path . join ( work_dir , 'pindel-config.txt' ) , 'w' ) as f : 
~~~ for bam in [ 'normal' , 'tumor' ] : 
~~ ~~ parameters = [ '-f' , '/data/ref.fasta' , 
'-i' , '/data/pindel-config.txt' , 
'--number_of_threads' , str ( job . cores ) , 
'--minimum_support_for_event' , '3' , 
'--report_long_insertions' , 'true' , 
'--report_breakpoints' , 'true' , 
'-o' , 'pindel' ] 
dockerCall ( job = job , tool = 'quay.io/ucsc_cgl/pindel:0.2.5b6--4e8d1b31d4028f464b3409c6558fb9dfcad73f88' , 
workDir = work_dir , parameters = parameters ) 
output_files = glob ( os . path . join ( work_dir , 'pindel*' ) ) 
tarball_files ( 'pindel.tar.gz' , file_paths = output_files , output_dir = work_dir ) 
return job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'pindel.tar.gz' ) ) 
~~ def run_bwa_index ( job , ref_id ) : 
job . fileStore . readGlobalFile ( ref_id , os . path . join ( work_dir , 'ref.fa' ) ) 
command = [ 'index' , '/data/ref.fa' ] 
dockerCall ( job = job , workDir = work_dir , parameters = command , 
tool = 'quay.io/ucsc_cgl/bwa:0.7.12--256539928ea162949d8a65ca5c79a72ef557ce7c' ) 
for output in [ 'ref.fa.amb' , 'ref.fa.ann' , 'ref.fa.bwt' , 'ref.fa.pac' , 'ref.fa.sa' ] : 
~~~ ids [ output . split ( '.' ) [ - 1 ] ] = ( job . fileStore . writeGlobalFile ( os . path . join ( work_dir , output ) ) ) 
~~ return ids [ 'amb' ] , ids [ 'ann' ] , ids [ 'bwt' ] , ids [ 'pac' ] , ids [ 'sa' ] 
~~ def read ( * paths ) : 
filename = os . path . join ( * paths ) 
with codecs . open ( filename , mode = 'r' , encoding = 'utf-8' ) as handle : 
~~~ return handle . read ( ) 
~~ ~~ def download_url ( job , url , work_dir = '.' , name = None , s3_key_path = None , cghub_key_path = None ) : 
file_path = os . path . join ( work_dir , name ) if name else os . path . join ( work_dir , os . path . basename ( url ) ) 
if cghub_key_path : 
~~~ _download_with_genetorrent ( job , url , file_path , cghub_key_path ) 
~~ elif urlparse ( url ) . scheme == 's3' : 
~~~ _s3am_with_retry ( job , num_cores = 1 , file_path = file_path , s3_url = url , mode = 'download' , s3_key_path = s3_key_path ) 
~~ elif urlparse ( url ) . scheme == 'file' : 
~~~ shutil . copy ( urlparse ( url ) . path , file_path ) 
~~~ subprocess . check_call ( [ 'curl' , '-fs' , '--retry' , '5' , '--create-dir' , url , '-o' , file_path ] ) 
~~ assert os . path . exists ( file_path ) 
return file_path 
~~ def download_url_job ( job , url , name = None , s3_key_path = None , cghub_key_path = None ) : 
fpath = download_url ( job = job , url = url , work_dir = work_dir , name = name , 
s3_key_path = s3_key_path , cghub_key_path = cghub_key_path ) 
return job . fileStore . writeGlobalFile ( fpath ) 
~~ def s3am_upload ( job , fpath , s3_dir , num_cores = 1 , s3_key_path = None ) : 
s3_dir = os . path . join ( s3_dir , os . path . basename ( fpath ) ) 
_s3am_with_retry ( job = job , num_cores = num_cores , file_path = fpath , 
s3_url = s3_dir , mode = 'upload' , s3_key_path = s3_key_path ) 
~~ def s3am_upload_job ( job , file_id , file_name , s3_dir , s3_key_path = None ) : 
fpath = job . fileStore . readGlobalFile ( file_id , os . path . join ( work_dir , file_name ) ) 
s3am_upload ( job = job , fpath = fpath , s3_dir = s3_dir , num_cores = job . cores , s3_key_path = s3_key_path ) 
~~ def _s3am_with_retry ( job , num_cores , file_path , s3_url , mode = 'upload' , s3_key_path = None ) : 
container_key_file = None 
base_boto = '.boto' 
base_aws = '.aws/credentials' 
docker_home_dir = '/root' 
credentials_to_mount = { os . path . join ( os . path . expanduser ( "~" ) , path ) : os . path . join ( docker_home_dir , path ) 
for path in [ base_aws , base_boto ] 
if os . path . exists ( os . path . join ( os . path . expanduser ( "~" ) , path ) ) } 
require ( os . path . isabs ( file_path ) , "\ ) 
dir_path , file_name = file_path . rsplit ( '/' , 1 ) 
container_dir_path = '/data' + dir_path 
container_file = os . path . join ( container_dir_path , file_name ) 
mounts = { dir_path : container_dir_path } 
if s3_key_path : 
~~~ require ( os . path . isabs ( s3_key_path ) , "\ ) 
key_dir_path , key_name = s3_key_path . rsplit ( '/' , 1 ) 
container_key_dir_path = '/data' + key_dir_path 
container_key_file = os . path . join ( container_key_dir_path , key_name ) 
mounts [ key_dir_path ] = container_key_dir_path 
~~ for k , v in credentials_to_mount . iteritems ( ) : 
~~~ mounts [ k ] = v 
~~ arguments = [ ] 
url_arguments = [ ] 
if mode == 'upload' : 
~~~ arguments . extend ( [ 'upload' , '--force' , '--upload-slots=%s' % num_cores , '--exists=overwrite' ] ) 
url_arguments . extend ( [ 'file://' + container_file , s3_url ] ) 
~~ elif mode == 'download' : 
~~~ arguments . extend ( [ 'download' , '--file-exists=overwrite' , '--download-exists=discard' ] ) 
url_arguments . extend ( [ s3_url , 'file://' + container_file ] ) 
~~ if s3_key_path : 
~~~ arguments . extend ( [ '--sse-key-is-master' , '--sse-key-file' , container_key_file ] ) 
~~ arguments . extend ( [ '--part-size=50M' , '--download-slots=%s' % num_cores ] ) 
arguments . extend ( url_arguments ) 
env = { } 
if 'AWS_PROFILE' in os . environ : 
~~~ env [ 'AWS_PROFILE' ] = os . environ [ 'AWS_PROFILE' ] 
~~ docker_parameters = [ '--rm' , '--log-driver' , 'none' ] 
if mounts : 
~~~ for k , v in mounts . iteritems ( ) : 
~~~ docker_parameters . extend ( [ '-v' , k + ':' + v ] ) 
~~ ~~ if env : 
~~~ for e , v in env . iteritems ( ) : 
~~~ docker_parameters . extend ( [ '-e' , '{}={}' . format ( e , v ) ] ) 
~~ ~~ retry_count = 3 
for i in xrange ( retry_count ) : 
~~~ dockerCall ( job = job , tool = 'quay.io/ucsc_cgl/s3am:2.0--fed932897e7fd40f4ec878362e5dd6afe15caaf0' , 
parameters = arguments , dockerParameters = docker_parameters ) 
~~ except subprocess . CalledProcessError : 
( mode , retry_count , arguments ) ) 
~~ def labels ( ontology , output , ols_base ) : 
for label in get_labels ( ontology = ontology , ols_base = ols_base ) : 
~~~ click . echo ( label , file = output ) 
~~ ~~ def tree ( ontology , output , ols_base ) : 
for parent , child in get_hierarchy ( ontology = ontology , ols_base = ols_base ) : 
~~~ click . echo ( '{}\\t{}' . format ( parent , child ) , file = output ) 
~~ ~~ def get_mean_insert_size ( work_dir , bam_name ) : 
process = subprocess . Popen ( args = cmd , shell = True , stdout = subprocess . PIPE ) 
b_sum = 0.0 
b_count = 0.0 
~~~ line = process . stdout . readline ( ) 
~~ tmp = line . split ( "\\t" ) 
if abs ( long ( tmp [ 8 ] ) ) < 10000 : 
~~~ b_sum += abs ( long ( tmp [ 8 ] ) ) 
b_count += 1 
~~ ~~ process . wait ( ) 
~~~ mean = b_sum / b_count 
~~~ mean = 150 
return int ( mean ) 
~~ def partitions ( l , partition_size ) : 
for i in xrange ( 0 , len ( l ) , partition_size ) : 
~~~ yield l [ i : i + partition_size ] 
~~ ~~ def required_length ( nmin , nmax ) : 
class RequiredLength ( argparse . Action ) : 
~~~ def __call__ ( self , parser , args , values , option_string = None ) : 
~~~ if not nmin <= len ( values ) <= nmax : 
~~~ msg = \ . format ( 
f = self . dest , nmin = nmin , nmax = nmax ) 
raise argparse . ArgumentTypeError ( msg ) 
~~ setattr ( args , self . dest , values ) 
~~ ~~ return RequiredLength 
~~ def current_docker_container_id ( ) : 
~~~ with open ( '/proc/1/cgroup' , 'r' ) as readable : 
~~~ raw = readable . read ( ) 
~~ ids = set ( re . compile ( '[0-9a-f]{12,}' ) . findall ( raw ) ) 
assert len ( ids ) == 1 
return ids . pop ( ) 
raise NotInsideContainerError ( ) 
~~ ~~ def run_star ( job , r1_id , r2_id , star_index_url , wiggle = False , sort = True ) : 
download_url ( job , url = star_index_url , name = 'starIndex.tar.gz' , work_dir = work_dir ) 
subprocess . check_call ( [ 'tar' , '-xvf' , os . path . join ( work_dir , 'starIndex.tar.gz' ) , '-C' , work_dir ] ) 
os . remove ( os . path . join ( work_dir , 'starIndex.tar.gz' ) ) 
star_index = os . path . join ( '/data' , os . listdir ( work_dir ) [ 0 ] ) if len ( os . listdir ( work_dir ) ) == 1 else '/data' 
parameters = [ '--runThreadN' , str ( job . cores ) , 
'--genomeDir' , star_index , 
'--outFileNamePrefix' , 'rna' , 
'--outSAMunmapped' , 'Within' , 
'--quantMode' , 'TranscriptomeSAM' , 
'--outSAMattributes' , 'NH' , 'HI' , 'AS' , 'NM' , 'MD' , 
'--outFilterType' , 'BySJout' , 
'--outFilterMultimapNmax' , '20' , 
'--outFilterMismatchNmax' , '999' , 
'--outFilterMismatchNoverReadLmax' , '0.04' , 
'--alignIntronMin' , '20' , 
'--alignIntronMax' , '1000000' , 
'--alignMatesGapMax' , '1000000' , 
'--alignSJoverhangMin' , '8' , 
'--alignSJDBoverhangMin' , '1' , 
'--sjdbScore' , '1' , 
'--limitBAMsortRAM' , '49268954168' ] 
~~~ parameters . extend ( [ '--outSAMtype' , 'BAM' , 'SortedByCoordinate' ] ) 
aligned_bam = 'rnaAligned.sortedByCoord.out.bam' 
~~~ parameters . extend ( [ '--outSAMtype' , 'BAM' , 'Unsorted' ] ) 
aligned_bam = 'rnaAligned.out.bam' 
~~ if wiggle : 
~~~ parameters . extend ( [ '--outWigType' , 'bedGraph' , 
'--outWigStrand' , 'Unstranded' , 
'--outWigReferencesPrefix' , 'chr' ] ) 
~~ if r1_id and r2_id : 
~~~ job . fileStore . readGlobalFile ( r1_id , os . path . join ( work_dir , 'R1.fastq' ) ) 
job . fileStore . readGlobalFile ( r2_id , os . path . join ( work_dir , 'R2.fastq' ) ) 
parameters . extend ( [ '--readFilesIn' , '/data/R1.fastq' , '/data/R2.fastq' ] ) 
parameters . extend ( [ '--readFilesIn' , '/data/R1.fastq' ] ) 
~~ dockerCall ( job = job , tool = 'quay.io/ucsc_cgl/star:2.4.2a--bcbd5122b69ff6ac4ef61958e47bde94001cfe80' , 
aligned_bam_path = os . path . join ( work_dir , aligned_bam ) 
~~ aligned_id = job . fileStore . writeGlobalFile ( aligned_bam_path ) 
transcriptome_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'rnaAligned.toTranscriptome.out.bam' ) ) 
log_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'rnaLog.final.out' ) ) 
sj_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'rnaSJ.out.tab' ) ) 
if wiggle : 
~~~ wiggle_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'rnaSignal.UniqueMultiple.str1.out.bg' ) ) 
return transcriptome_id , aligned_id , wiggle_id , log_id , sj_id 
~~~ return transcriptome_id , aligned_id , log_id , sj_id 
~~ ~~ def run_bwakit ( job , config , sort = True , trim = False , mark_secondary = False ) : 
rg = None 
inputs = { 'ref.fa' : config . ref , 
'ref.fa.fai' : config . fai , 
'ref.fa.amb' : config . amb , 
'ref.fa.ann' : config . ann , 
'ref.fa.bwt' : config . bwt , 
'ref.fa.pac' : config . pac , 
'ref.fa.sa' : config . sa } 
samples = [ ] 
realignment = False 
if getattr ( config , 'r1' , None ) : 
~~~ inputs [ 'input.1.fq.gz' ] = config . r1 
samples . append ( 'input.1.fq.gz' ) 
~~ if getattr ( config , 'r2' , None ) : 
~~~ inputs [ 'input.2.fq.gz' ] = config . r2 
samples . append ( 'input.2.fq.gz' ) 
~~ if getattr ( config , 'bam' , None ) : 
~~~ inputs [ 'input.bam' ] = config . bam 
samples . append ( 'input.bam' ) 
realignment = True 
~~ if getattr ( config , 'sam' , None ) : 
~~~ inputs [ 'input.sam' ] = config . sam 
samples . append ( 'input.sam' ) 
~~ if getattr ( config , 'alt' , None ) : 
~~~ inputs [ 'ref.fa.alt' ] = config . alt 
~~ for name , fileStoreID in inputs . iteritems ( ) : 
~~~ job . fileStore . readGlobalFile ( fileStoreID , os . path . join ( work_dir , name ) ) 
~~ if getattr ( config , 'rg_line' , None ) : 
~~~ rg = config . rg_line 
~~ elif all ( getattr ( config , elem , None ) for elem in [ 'library' , 'platform' , 'program_unit' , 'uuid' ] ) : 
rg_attributes = [ config . library , config . platform , config . program_unit , config . uuid ] 
for tag , info in zip ( [ 'LB' , 'PL' , 'PU' , 'SM' ] , rg_attributes ) : 
~~~ rg += '\\\\t{0}:{1}' . format ( tag , info ) 
~~ ~~ elif realignment : 
~~~ rg = None 
~~ opt_args = [ ] 
~~~ opt_args . append ( '-s' ) 
~~ if trim : 
~~~ opt_args . append ( '-a' ) 
~~ if mark_secondary : 
~~~ opt_args . append ( '-M' ) 
~~ parameters = [ '-t' , str ( job . cores ) ] + opt_args + [ '-o' , '/data/aligned' , '/data/ref.fa' ] 
if rg is not None : 
~~~ parameters = [ '-R' , rg ] + parameters 
~~ for sample in samples : 
~~~ parameters . append ( '/data/{}' . format ( sample ) ) 
~~ dockerCall ( job = job , tool = 'quay.io/ucsc_cgl/bwakit:0.7.12--c85ccff267d5021b75bb1c9ccf5f4b79f91835cc' , 
parameters = parameters , workDir = work_dir ) 
return job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'aligned.aln.bam' ) ) 
~~ def get_labels ( ontology , ols_base = None ) : 
client = OlsClient ( ols_base = ols_base ) 
return client . iter_labels ( ontology ) 
~~ def get_metadata ( ontology , ols_base = None ) : 
return client . get_ontology ( ontology ) 
~~ def get_hierarchy ( ontology , ols_base = None ) : 
return client . iter_hierarchy ( ontology ) 
~~ def run ( cls , name , desc ) : 
wrapper = cls ( name , desc ) 
mount_path = wrapper . _get_mount_path ( ) 
arg_parser = wrapper . _create_argument_parser ( ) 
wrapper . _extend_argument_parser ( arg_parser ) 
empty_config = wrapper . __get_empty_config ( ) 
config_yaml = ruamel . yaml . load ( empty_config ) 
wrapper . __populate_parser_from_config ( arg_parser , config_yaml ) 
args = arg_parser . parse_args ( ) 
for k , v in vars ( args ) . items ( ) : 
~~~ k = k . replace ( '_' , '-' ) 
if k in config_yaml : 
~~~ config_yaml [ k ] = v 
~~ ~~ config_path = wrapper . _get_config_path ( ) 
with open ( config_path , 'w' ) as writable : 
~~~ ruamel . yaml . dump ( config_yaml , stream = writable ) 
~~ workdir_path = os . path . join ( mount_path , 'Toil-' + wrapper . _name ) 
if os . path . exists ( workdir_path ) : 
~~~ if args . restart : 
~~~ os . makedirs ( workdir_path ) 
~~ command = wrapper . _create_pipeline_command ( args , workdir_path , config_path ) 
wrapper . _extend_pipeline_command ( command , args ) 
~~~ subprocess . check_call ( command ) 
~~~ print ( e , file = sys . stderr ) 
~~~ stat = os . stat ( mount_path ) 
chown_command = [ 'chown' , '-R' , '%s:%s' % ( stat . st_uid , stat . st_gid ) , mount_path ] 
subprocess . check_call ( chown_command ) 
if args . no_clean : 
~~~ log . info ( \ , workdir_path ) 
shutil . rmtree ( workdir_path ) 
~~ ~~ ~~ def __populate_parser_from_config ( self , arg_parser , config_data , prefix = '' ) : 
for k , v in config_data . items ( ) : 
~~~ k = prefix + '.' + k if prefix else k 
if isinstance ( v , dict ) : 
~~~ self . __populate_parser_from_config ( arg_parser , v , prefix = k ) 
~~~ self . _add_option ( arg_parser , name = k , default = v ) 
~~ ~~ ~~ def __get_empty_config ( self ) : 
self . _generate_config ( ) 
path = self . _get_config_path ( ) 
with open ( path , 'r' ) as readable : 
~~~ contents = readable . read ( ) 
~~ os . remove ( path ) 
return contents 
~~ def _get_mount_path ( self ) : 
if self . _mount_path is None : 
~~~ name = current_docker_container_id ( ) 
if dockerd_is_reachable ( ) : 
~~~ blob = json . loads ( subprocess . check_output ( [ 'docker' , 'inspect' , name ] ) ) 
mounts = blob [ 0 ] [ 'Mounts' ] 
sock_mnt = [ x [ 'Source' ] == x [ 'Destination' ] 
for x in mounts if 'docker.sock' in x [ 'Source' ] ] 
require ( len ( sock_mnt ) == 1 , 
if len ( mounts ) == 2 : 
~~~ require ( all ( x [ 'Source' ] == x [ 'Destination' ] for x in mounts ) , 
'socket.' ) 
work_mount = [ x [ 'Source' ] for x in mounts if 'docker.sock' not in x [ 'Source' ] ] 
~~~ mirror_mounts = [ x [ 'Source' ] for x in mounts if x [ 'Source' ] == x [ 'Destination' ] ] 
work_mount = [ x for x in mirror_mounts if 'docker.sock' not in x ] 
'documentation.' ) 
~~ self . _mount_path = work_mount [ 0 ] 
~~ ~~ return self . _mount_path 
~~ def _add_option ( self , arg_parser , name , * args , ** kwargs ) : 
arg_parser . add_argument ( '--' + name , * args , ** kwargs ) 
~~ def _create_argument_parser ( self ) : 
parser = argparse . ArgumentParser ( description = self . _desc , 
formatter_class = argparse . RawTextHelpFormatter ) 
parser . add_argument ( '--no-clean' , action = 'store_true' , 
parser . add_argument ( '--restart' , action = 'store_true' , 
parser . add_argument ( '--cores' , type = int , default = None , 
return parser 
~~ def _create_pipeline_command ( self , args , workdir_path , config_path ) : 
return ( [ self . _name , 'run' , os . path . join ( workdir_path , 'jobStore' ) , 
'--config' , config_path , 
'--workDir' , workdir_path , '--retryCount' , '1' ] 
+ ( [ '--restart' ] if args . restart else [ ] ) ) 
~~ def run_cutadapt ( job , r1_id , r2_id , fwd_3pr_adapter , rev_3pr_adapter ) : 
if r2_id : 
~~ parameters = [ '-a' , fwd_3pr_adapter , 
'-m' , '35' ] 
if r1_id and r2_id : 
parameters . extend ( [ '-A' , rev_3pr_adapter , 
'-o' , '/data/R1_cutadapt.fastq' , 
'-p' , '/data/R2_cutadapt.fastq' , 
'/data/R1.fastq' , '/data/R2.fastq' ] ) 
parameters . extend ( [ '-o' , '/data/R1_cutadapt.fastq' , '/data/R1.fastq' ] ) 
~~ dockerCall ( job = job , tool = 'quay.io/ucsc_cgl/cutadapt:1.9--6bd44edd2b8f8f17e25c5a268fedaab65fa851d2' , 
~~~ r1_cut_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'R1_cutadapt.fastq' ) ) 
r2_cut_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'R2_cutadapt.fastq' ) ) 
r2_cut_id = None 
~~ return r1_cut_id , r2_cut_id 
~~ def run_samtools_faidx ( job , ref_id ) : 
job . fileStore . readGlobalFile ( ref_id , os . path . join ( work_dir , 'ref.fasta' ) ) 
command = [ 'faidx' , 'ref.fasta' ] 
tool = 'quay.io/ucsc_cgl/samtools:0.1.19--dd5ac549b95eb3e5d166a5e310417ef13651994e' ) 
return job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'ref.fasta.fai' ) ) 
~~ def run_samtools_index ( job , bam ) : 
job . fileStore . readGlobalFile ( bam , os . path . join ( work_dir , 'sample.bam' ) ) 
parameters = [ 'index' , '/data/sample.bam' ] 
return job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'sample.bam.bai' ) ) 
~~ def run_sambamba_markdup ( job , bam ) : 
job . fileStore . readGlobalFile ( bam , os . path . join ( work_dir , 'input.bam' ) ) 
command = [ '/usr/local/bin/sambamba' , 
'markdup' , 
'-t' , str ( int ( job . cores ) ) , 
'/data/input.bam' , 
'/data/output.bam' ] 
start_time = time . time ( ) 
tool = 'quay.io/biocontainers/sambamba:0.6.6--0' ) 
end_time = time . time ( ) 
return job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'output.bam' ) ) 
~~ def run_samblaster ( job , sam ) : 
job . fileStore . readGlobalFile ( sam , os . path . join ( work_dir , 'input.sam' ) ) 
command = [ '/usr/local/bin/samblaster' , 
'-i' , '/data/input.sam' , 
'-o' , '/data/output.sam' , 
'--ignoreUnmated' ] 
tool = 'quay.io/biocontainers/samblaster:0.1.24--0' ) 
_log_runtime ( job , start_time , end_time , "SAMBLASTER" ) 
return job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'output.sam' ) ) 
~~ def picard_mark_duplicates ( job , bam , bai , validation_stringency = 'LENIENT' ) : 
job . fileStore . readGlobalFile ( bam , os . path . join ( work_dir , 'sorted.bam' ) ) 
job . fileStore . readGlobalFile ( bai , os . path . join ( work_dir , 'sorted.bai' ) ) 
command = [ 'MarkDuplicates' , 
'INPUT=sorted.bam' , 
'OUTPUT=mkdups.bam' , 
'METRICS_FILE=metrics.txt' , 
'ASSUME_SORTED=true' , 
'CREATE_INDEX=true' , 
'VALIDATION_STRINGENCY=%s' % validation_stringency . upper ( ) ] 
docker_parameters = [ '--rm' , 
'--log-driver' , 'none' , 
'-v' , '{}:/data' . format ( work_dir ) ] 
tool = 'quay.io/ucsc_cgl/picardtools:1.95--dd5ac549b95eb3e5d166a5e310417ef13651994e' , 
bam = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'mkdups.bam' ) ) 
bai = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'mkdups.bai' ) ) 
return bam , bai 
~~ def run_picard_sort ( job , bam , sort_by_name = False ) : 
command = [ 'SortSam' , 
'O=/data/output.bam' , 
'I=/data/input.bam' ] 
if sort_by_name : 
~~~ command . append ( 'SO=queryname' ) 
~~~ command . append ( 'SO=coordinate' ) 
~~ start_time = time . time ( ) 
~~ def run_gatk_preprocessing ( job , bam , bai , ref , ref_dict , fai , g1k , mills , dbsnp , realign = False , unsafe = False ) : 
mdups_disk = PromisedRequirement ( lambda bam_ , bai_ : 2 * ( bam_ . size + bai_ . size ) , bam , bai ) 
mdups = job . wrapJobFn ( picard_mark_duplicates , 
bam , 
bai , 
cores = job . cores , 
disk = mdups_disk , 
memory = job . memory ) 
bqsr_input_bam = mdups . rv ( 0 ) 
bqsr_input_bai = mdups . rv ( 1 ) 
genome_ref_size = ref . size + ref_dict . size + fai . size 
if realign : 
~~~ indel_ref_size = mills . size + g1k . size + genome_ref_size 
realigner_target_disk = PromisedRequirement ( lambda bam_ , bai_ , ref_size : 
bam_ . size + bai_ . size + 2 * ref_size , 
mdups . rv ( 0 ) , 
mdups . rv ( 1 ) , 
indel_ref_size ) 
realigner_target = job . wrapJobFn ( run_realigner_target_creator , 
ref , ref_dict , fai , 
g1k , mills , 
unsafe = unsafe , 
disk = realigner_target_disk , 
indel_realign_disk = PromisedRequirement ( lambda bam_ , bai_ , intervals , ref_size : 
2 * ( bam_ . size + bai_ . size ) + intervals . size + ref_size , 
realigner_target . rv ( ) , 
indel_realign = job . wrapJobFn ( run_indel_realignment , 
disk = indel_realign_disk , 
mdups . addChild ( realigner_target ) 
realigner_target . addChild ( indel_realign ) 
bqsr_input_bam = indel_realign . rv ( 0 ) 
bqsr_input_bai = indel_realign . rv ( 1 ) 
~~ bqsr_ref_size = dbsnp . size + mills . size + genome_ref_size 
base_recal_disk = PromisedRequirement ( lambda bam_ , bai_ , ref_size : 
bqsr_input_bam , 
bqsr_input_bai , 
bqsr_ref_size ) 
base_recal = job . wrapJobFn ( run_base_recalibration , 
dbsnp , mills , 
disk = base_recal_disk , 
recalibrate_reads_disk = PromisedRequirement ( lambda bam_ , bai_ , recal , ref_size : 
2 * ( bam_ . size + bai_ . size ) + recal . size + ref_size , 
base_recal . rv ( ) , 
genome_ref_size ) 
recalibrate_reads = job . wrapJobFn ( apply_bqsr_recalibration , 
disk = recalibrate_reads_disk , 
job . addChild ( mdups ) 
mdups . addFollowOn ( base_recal ) 
base_recal . addChild ( recalibrate_reads ) 
return recalibrate_reads . rv ( 0 ) , recalibrate_reads . rv ( 1 ) 
~~ def run_base_recalibration ( job , bam , bai , ref , ref_dict , fai , dbsnp , mills , unsafe = False ) : 
inputs = { 'ref.fasta' : ref , 
'ref.fasta.fai' : fai , 
'ref.dict' : ref_dict , 
'input.bam' : bam , 
'input.bai' : bai , 
'dbsnp.vcf' : dbsnp , 
'mills.vcf' : mills } 
~~ parameters = [ '-T' , 'BaseRecalibrator' , 
'-nct' , str ( int ( job . cores ) ) , 
'-R' , '/data/ref.fasta' , 
'-I' , '/data/input.bam' , 
'-knownSites' , '/data/dbsnp.vcf' , 
'-knownSites' , '/data/mills.vcf' , 
'-o' , '/data/recal_data.table' ] 
if unsafe : 
~~~ parameters . extend ( [ '-U' , 'ALLOW_SEQ_DICT_INCOMPATIBILITY' ] ) 
~~ docker_parameters = [ '--rm' , 
dockerCall ( job = job , tool = 'quay.io/ucsc_cgl/gatk:3.5--dba6dae49156168a909c43330350c6161dc7ecc2' , 
workDir = work_dir , 
parameters = parameters , 
return job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'recal_data.table' ) ) 
~~ def run_kallisto ( job , r1_id , r2_id , kallisto_index_url ) : 
download_url ( job , url = kallisto_index_url , name = 'kallisto_hg38.idx' , work_dir = work_dir ) 
parameters = [ 'quant' , 
'-i' , '/data/kallisto_hg38.idx' , 
'-t' , str ( job . cores ) , 
'-o' , '/data/' , 
'-b' , '100' , 
'--fusion' ] 
parameters . extend ( [ '/data/R1.fastq' , '/data/R2.fastq' ] ) 
parameters . extend ( [ '--single' , '-l' , '200' , '-s' , '15' , '/data/R1.fastq' ] ) 
~~ dockerCall ( job = job , tool = 'quay.io/ucsc_cgl/kallisto:0.42.4--35ac87df5b21a8e8e8d159f26864ac1e1db8cf86' , 
output_files = [ os . path . join ( work_dir , x ) for x in [ 'run_info.json' , 'abundance.tsv' , 'abundance.h5' , 'fusion.txt' ] ] 
tarball_files ( tar_name = 'kallisto.tar.gz' , file_paths = output_files , output_dir = work_dir ) 
return job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'kallisto.tar.gz' ) ) 
~~ def run_rsem ( job , bam_id , rsem_ref_url , paired = True ) : 
download_url ( job , url = rsem_ref_url , name = 'rsem_ref.tar.gz' , work_dir = work_dir ) 
subprocess . check_call ( [ 'tar' , '-xvf' , os . path . join ( work_dir , 'rsem_ref.tar.gz' ) , '-C' , work_dir ] ) 
os . remove ( os . path . join ( work_dir , 'rsem_ref.tar.gz' ) ) 
rsem_files = [ ] 
for root , directories , files in os . walk ( work_dir ) : 
~~~ rsem_files . extend ( [ os . path . join ( root , x ) for x in files ] ) 
~~ ref_prefix = [ os . path . basename ( os . path . splitext ( x ) [ 0 ] ) for x in rsem_files if 'grp' in x ] [ 0 ] 
ref_folder = os . path . join ( '/data' , os . listdir ( work_dir ) [ 0 ] ) if len ( os . listdir ( work_dir ) ) == 1 else '/data' 
job . fileStore . readGlobalFile ( bam_id , os . path . join ( work_dir , 'transcriptome.bam' ) ) 
output_prefix = 'rsem' 
parameters = [ '--quiet' , 
'--no-qualities' , 
'-p' , str ( job . cores ) , 
'--forward-prob' , '0.5' , 
'--seed-length' , '25' , 
'--fragment-length-mean' , '-1.0' , 
'--bam' , '/data/transcriptome.bam' , 
os . path . join ( ref_folder , ref_prefix ) , 
output_prefix ] 
if paired : 
~~~ parameters = [ '--paired-end' ] + parameters 
~~ dockerCall ( job = job , tool = 'quay.io/ucsc_cgl/rsem:1.2.25--d4275175cc8df36967db460b06337a14f40d2f21' , 
gene_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , output_prefix + '.genes.results' ) ) 
isoform_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , output_prefix + '.isoforms.results' ) ) 
return gene_id , isoform_id 
~~ def run_rsem_postprocess ( job , rsem_gene_id , rsem_isoform_id ) : 
genes = job . fileStore . readGlobalFile ( rsem_gene_id , os . path . join ( work_dir , 'rsem_genes.results' ) ) 
iso = job . fileStore . readGlobalFile ( rsem_isoform_id , os . path . join ( work_dir , 'rsem_isoforms.results' ) ) 
command = [ '-g' , 'rsem_genes.results' , '-i' , 'rsem_isoforms.results' ] 
dockerCall ( job = job , tool = 'quay.io/ucsc_cgl/gencode_hugo_mapping:1.0--cb4865d02f9199462e66410f515c4dabbd061e4d' , 
parameters = command , workDir = work_dir ) 
hugo_files = [ os . path . join ( work_dir , x ) for x in [ 'rsem_genes.hugo.results' , 'rsem_isoforms.hugo.results' ] ] 
tarball_files ( 'rsem.tar.gz' , file_paths = [ os . path . join ( work_dir , x ) for x in [ genes , iso ] ] , output_dir = work_dir ) 
tarball_files ( 'rsem_hugo.tar.gz' , file_paths = [ os . path . join ( work_dir , x ) for x in hugo_files ] , output_dir = work_dir ) 
rsem_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'rsem.tar.gz' ) ) 
hugo_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'rsem_hugo.tar.gz' ) ) 
return rsem_id , hugo_id 
~~ def fit ( 
df , 
similarity_type = "jaccard" , 
time_decay_coefficient = 30 , 
time_now = None , 
timedecay_formula = False , 
threshold = 1 , 
assert threshold > 0 
df . createOrReplaceTempView ( "{prefix}df_train_input" . format ( ** self . header ) ) 
if timedecay_formula : 
~~~ query = self . f ( 
time_now = time_now , 
time_decay_coefficient = time_decay_coefficient , 
df = self . spark . sql ( query ) 
~~~ if self . header [ 'col_timestamp' ] . lower ( ) in [ s . name . lower ( ) for s in df . schema ] : 
~~ ~~ df . createOrReplaceTempView ( self . f ( "{prefix}df_train" ) ) 
query = self . f ( 
threshold = threshold , 
item_cooccurrence = self . spark . sql ( query ) 
item_cooccurrence . write . mode ( "overwrite" ) . saveAsTable ( 
self . f ( "{prefix}item_cooccurrence" ) 
if similarity_type == SIM_LIFT or similarity_type == SIM_JACCARD : 
~~~ item_marginal = self . spark . sql ( 
self . f ( 
item_marginal . createOrReplaceTempView ( self . f ( "{prefix}item_marginal" ) ) 
~~ if similarity_type == SIM_COOCCUR : 
~~~ self . item_similarity = item_cooccurrence 
~~ elif similarity_type == SIM_JACCARD : 
self . item_similarity = self . spark . sql ( query ) 
~~ elif similarity_type == SIM_LIFT : 
self . item_similarity . write . mode ( "overwrite" ) . saveAsTable ( 
self . f ( "{prefix}item_similarity_upper" ) 
self . f ( "{prefix}item_similarity" ) 
self . item_similarity = self . spark . table ( self . f ( "{prefix}item_similarity" ) ) 
~~ def get_user_affinity ( self , test ) : 
test . createOrReplaceTempView ( self . f ( "{prefix}df_test" ) ) 
df_test_users = self . spark . sql ( query ) 
df_test_users . write . mode ( "overwrite" ) . saveAsTable ( 
self . f ( "{prefix}df_test_users" ) 
return self . spark . sql ( query ) 
~~ def recommend_k_items_slow ( self , test , top_k = 10 , remove_seen = True ) : 
if remove_seen : 
~~ self . get_user_affinity ( test ) . write . mode ( "overwrite" ) . saveAsTable ( self . f ( "{prefix}user_affinity" ) ) 
top_k = top_k , 
~~ def gatk_select_variants ( job , mode , vcf_id , ref_fasta , ref_fai , ref_dict ) : 
inputs = { 'genome.fa' : ref_fasta , 
'genome.fa.fai' : ref_fai , 
'genome.dict' : ref_dict , 
'input.vcf' : vcf_id } 
~~ command = [ '-T' , 'SelectVariants' , 
'-R' , 'genome.fa' , 
'-V' , 'input.vcf' , 
'-o' , 'output.vcf' , 
'-selectType' , mode ] 
return job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'output.vcf' ) ) 
~~ def gatk_variant_filtration ( job , vcf_id , filter_name , filter_expression , ref_fasta , ref_fai , ref_dict ) : 
~~ command = [ '-T' , 'VariantFiltration' , 
'--filterExpression' , filter_expression , 
'-o' , 'filtered_variants.vcf' ] 
'{expression}' . format ( name = filter_name , expression = filter_expression ) ) 
malformed_header = os . path . join ( work_dir , 'filtered_variants.vcf' ) 
fixed_header = os . path . join ( work_dir , 'fixed_header.vcf' ) 
filter_regex = re . escape ( \ % filter_expression ) 
with open ( malformed_header , 'r' ) as f , open ( fixed_header , 'w' ) as g : 
~~~ g . write ( re . sub ( filter_regex , filter_expression , line ) ) 
~~ ~~ return job . fileStore . writeGlobalFile ( fixed_header ) 
~~ def gatk_variant_recalibrator ( job , 
mode , 
vcf , 
ref_fasta , ref_fai , ref_dict , 
annotations , 
hapmap = None , omni = None , phase = None , dbsnp = None , mills = None , 
max_gaussians = 4 , 
mode = mode . upper ( ) 
'input.vcf' : vcf } 
command = [ '-T' , 'VariantRecalibrator' , 
'-input' , 'input.vcf' , 
'-tranche' , '100.0' , 
'-tranche' , '99.9' , 
'-tranche' , '99.0' , 
'-tranche' , '90.0' , 
'--maxGaussians' , str ( max_gaussians ) , 
'-recalFile' , 'output.recal' , 
'-tranchesFile' , 'output.tranches' , 
'-rscriptFile' , 'output.plots.R' ] 
if mode == 'SNP' : 
~~~ command . extend ( 
[ '-resource:hapmap,known=false,training=true,truth=true,prior=15.0' , 'hapmap.vcf' , 
'-resource:omni,known=false,training=true,truth=true,prior=12.0' , 'omni.vcf' , 
'-resource:dbsnp,known=true,training=false,truth=false,prior=2.0' , 'dbsnp.vcf' , 
'-resource:1000G,known=false,training=true,truth=false,prior=10.0' , '1000G.vcf' , 
'-mode' , 'SNP' ] ) 
inputs [ 'hapmap.vcf' ] = hapmap 
inputs [ 'omni.vcf' ] = omni 
inputs [ 'dbsnp.vcf' ] = dbsnp 
inputs [ '1000G.vcf' ] = phase 
~~ elif mode == 'INDEL' : 
[ '-resource:mills,known=false,training=true,truth=true,prior=12.0' , 'mills.vcf' , 
'-mode' , 'INDEL' ] ) 
inputs [ 'mills.vcf' ] = mills 
~~ for annotation in annotations : 
~~~ command . extend ( [ '-an' , annotation ] ) 
~~ work_dir = job . fileStore . getLocalTempDir ( ) 
'{annotations}' . format ( mode = mode , annotations = '\\n' . join ( annotations ) ) ) 
recal_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'output.recal' ) ) 
tranches_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'output.tranches' ) ) 
plots_id = job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'output.plots.R' ) ) 
return recal_id , tranches_id , plots_id 
~~ def gatk_apply_variant_recalibration ( job , 
recal_table , tranches , 
ts_filter_level = 99.0 , 
'input.vcf' : vcf , 
'recal' : recal_table , 
'tranches' : tranches } 
~~ mode = mode . upper ( ) 
command = [ '-T' , 'ApplyRecalibration' , 
'-mode' , mode , 
'-o' , 'vqsr.vcf' , 
'-ts_filter_level' , str ( ts_filter_level ) , 
'-recalFile' , 'recal' , 
'-tranchesFile' , 'tranches' ] 
if unsafe_mode : 
sensitivity = ts_filter_level ) ) 
return job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'vqsr.vcf' ) ) 
~~ def gatk_combine_variants ( job , vcfs , ref_fasta , ref_fai , ref_dict , merge_option = 'UNIQUIFY' ) : 
inputs . update ( vcfs ) 
~~ command = [ '-T' , 'CombineVariants' , 
'-o' , '/data/merged.vcf' , 
'--genotypemergeoption' , merge_option ] 
for uuid , vcf_id in vcfs . iteritems ( ) : 
~~ docker_parameters = [ '--rm' , 'log-driver' , 'none' , 
return job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'merged.vcf' ) ) 
~~ def bam_quickcheck ( bam_path ) : 
directory , bam_name = os . path . split ( bam_path ) 
exit_code = subprocess . call ( [ 'docker' , 'run' , '-v' , directory + ':/data' , 
'quay.io/ucsc_cgl/samtools:1.3--256539928ea162949d8a65ca5c79a72ef557ce7c' , 
'quickcheck' , '-vv' , '/data/' + bam_name ] ) 
if exit_code != 0 : 
~~ def load_handlers ( handler_mapping ) : 
handlers = { } 
for packet_type , handler in handler_mapping . items ( ) : 
~~~ if packet_type == '*' : 
~~~ Packet = packet_type 
~~ elif isinstance ( packet_type , str ) : 
~~~ Packet = importer ( packet_type ) 
~~ if isinstance ( handler , str ) : 
~~~ Handler = importer ( handler ) 
~~~ Handler = handler 
~~ if Packet in handlers : 
~~~ raise HandlerConfigError ( 
~~ handlers [ Packet ] = Handler 
~~ return handlers 
~~ def write_config ( configuration ) : 
with open ( CONFIG_PATH , 'w' ) as f : 
~~~ json . dump ( configuration , f , indent = 2 , sort_keys = True ) 
~~ ~~ def get_config ( ) : 
if not os . path . exists ( CONFIG_PATH ) : 
~~~ write_config ( { } ) 
~~ with open ( CONFIG_PATH ) as f : 
~~~ return json . load ( f ) 
~~ ~~ def get_ontology ( self , ontology ) : 
url = self . ontology_metadata_fmt . format ( ontology = ontology ) 
response = requests . get ( url ) 
return response . json ( ) 
~~ def get_term ( self , ontology , iri ) : 
url = self . ontology_term_fmt . format ( ontology , iri ) 
~~ def search ( self , name , query_fields = None ) : 
params = { 'q' : name } 
if query_fields is not None : 
~~~ params [ 'queryFields' ] = '{{{}}}' . format ( ',' . join ( query_fields ) ) 
~~ response = requests . get ( self . ontology_search , params = params ) 
~~ def suggest ( self , name , ontology = None ) : 
if ontology : 
~~~ params [ 'ontology' ] = ',' . join ( ontology ) 
~~ response = requests . get ( self . ontology_suggest , params = params ) 
~~ def _iter_terms_helper ( url , size = None , sleep = None ) : 
~~~ size = 500 
~~ elif size > 500 : 
~~ t = time . time ( ) 
response = requests . get ( url , params = { 'size' : size } ) . json ( ) 
links = response [ '_links' ] 
for response_term in _iterate_response_terms ( response ) : 
~~~ yield response_term 
~~ t = time . time ( ) - t 
log . info ( 
response [ 'page' ] [ 'number' ] + 1 , 
response [ 'page' ] [ 'totalPages' ] , 
t 
while 'next' in links : 
~~~ if sleep : 
~~~ time . sleep ( sleep ) 
response = requests . get ( links [ 'next' ] [ 'href' ] , params = { 'size' : size } ) . json ( ) 
~~ log . info ( 
response [ 'page' ] [ 'number' ] , 
time . time ( ) - t 
~~ ~~ def iter_terms ( self , ontology , size = None , sleep = None ) : 
url = self . ontology_terms_fmt . format ( ontology = ontology ) 
for term in self . _iter_terms_helper ( url , size = size , sleep = sleep ) : 
~~ ~~ def iter_descendants ( self , ontology , iri , size = None , sleep = None ) : 
url = self . ontology_term_descendants_fmt . format ( ontology = ontology , iri = iri ) 
~~ ~~ def iter_descendants_labels ( self , ontology , iri , size = None , sleep = None ) : 
for label in _help_iterate_labels ( self . iter_descendants ( ontology , iri , size = size , sleep = sleep ) ) : 
~~~ yield label 
~~ ~~ def iter_labels ( self , ontology , size = None , sleep = None ) : 
for label in _help_iterate_labels ( self . iter_terms ( ontology = ontology , size = size , sleep = sleep ) ) : 
~~ ~~ def iter_hierarchy ( self , ontology , size = None , sleep = None ) : 
for term in self . iter_terms ( ontology = ontology , size = size , sleep = sleep ) : 
~~~ hierarchy_children_link = term [ '_links' ] [ HIERARCHICAL_CHILDREN ] [ 'href' ] 
~~ response = requests . get ( hierarchy_children_link ) . json ( ) 
for child_term in response [ '_embedded' ] [ 'terms' ] : 
~~~ yield term [ 'label' ] , child_term [ 'label' ] 
~~ ~~ ~~ def run_fastqc ( job , r1_id , r2_id ) : 
job . fileStore . readGlobalFile ( r1_id , os . path . join ( work_dir , 'R1.fastq' ) ) 
parameters = [ '/data/R1.fastq' ] 
output_names = [ 'R1_fastqc.html' , 'R1_fastqc.zip' ] 
~~~ job . fileStore . readGlobalFile ( r2_id , os . path . join ( work_dir , 'R2.fastq' ) ) 
parameters . extend ( [ '-t' , '2' , '/data/R2.fastq' ] ) 
output_names . extend ( [ 'R2_fastqc.html' , 'R2_fastqc.zip' ] ) 
~~ dockerCall ( job = job , tool = 'quay.io/ucsc_cgl/fastqc:0.11.5--be13567d00cd4c586edf8ae47d991815c8c72a49' , 
output_files = [ os . path . join ( work_dir , x ) for x in output_names ] 
tarball_files ( tar_name = 'fastqc.tar.gz' , file_paths = output_files , output_dir = work_dir ) 
return job . fileStore . writeGlobalFile ( os . path . join ( work_dir , 'fastqc.tar.gz' ) ) 
~~ def create_app ( config = None ) : 
from home . config import TEMPLATE_FOLDER , STATIC_FOLDER 
app = Flask ( __name__ , static_folder = STATIC_FOLDER , 
template_folder = TEMPLATE_FOLDER ) 
app . config . from_object ( 'home.config' ) 
if 'HOME_SETTINGS' in environ : 
~~~ app . config . from_envvar ( 'HOME_SETTINGS' ) 
~~ app . config . from_object ( config ) 
from home . dash . web import web 
from home . dash . api import api 
app . register_blueprint ( web ) 
app . register_blueprint ( api , url_prefix = '/api' ) 
login_manager . init_app ( app ) 
from home . dash . models import User 
@ login_manager . user_loader 
def load_user ( user_id ) : 
~~~ return User . query . get ( int ( user_id ) ) 
~~ Migrate ( app , db , directory = app . config [ 'MIGRATE_DIRECTORY' ] ) 
admin = Admin ( app ) 
from home . dash . admin import setup_admin 
setup_admin ( admin ) 
db . init_app ( app ) 
~~ def spawn_spark_cluster ( job , 
numWorkers , 
cores = None , 
disk = None , 
overrideLeaderIP = None ) : 
if numWorkers < 1 : 
~~ leaderService = SparkService ( cores = cores , 
memory = memory , 
disk = disk , 
overrideLeaderIP = overrideLeaderIP ) 
leaderIP = job . addService ( leaderService ) 
for i in range ( numWorkers ) : 
~~~ job . addService ( WorkerService ( leaderIP , 
cores = cores , 
memory = memory ) , 
parentService = leaderService ) 
~~ return leaderIP 
~~ def start ( self , job ) : 
if self . hostname is None : 
~~~ self . hostname = subprocess . check_output ( [ "hostname" , "-f" , ] ) [ : - 1 ] 
self . sparkContainerID = dockerCheckOutput ( job = job , 
defer = STOP , 
workDir = os . getcwd ( ) , 
tool = "quay.io/ucsc_cgl/apache-spark-master:1.5.2" , 
dockerParameters = [ "--net=host" , 
"-d" , 
"-v" , "/mnt/ephemeral/:/ephemeral/:rw" , 
"-e" , "SPARK_MASTER_IP=" + self . hostname , 
"-e" , "SPARK_LOCAL_DIRS=/ephemeral/spark/local" , 
"-e" , "SPARK_WORKER_DIR=/ephemeral/spark/work" ] , 
parameters = [ self . hostname ] ) [ : - 1 ] 
self . hdfsContainerID = dockerCheckOutput ( job = job , 
tool = "quay.io/ucsc_cgl/apache-hadoop-master:2.6.2" , 
"-d" ] , 
return self . hostname 
tool = "quay.io/ucsc_cgl/apache-spark-worker:1.5.2" , 
"-e" , 
"\\"SPARK_MASTER_IP=" + self . masterIP + ":" + _SPARK_MASTER_PORT + "\\"" , 
parameters = [ self . masterIP + ":" + _SPARK_MASTER_PORT ] ) [ : - 1 ] 
self . __start_datanode ( job ) 
hdfs_down = True 
retries = 0 
while hdfs_down and ( retries < 5 ) : 
time . sleep ( 30 ) 
clusterID = "" 
~~~ clusterID = subprocess . check_output ( [ "docker" , 
"exec" , 
self . hdfsContainerID , 
"grep" , 
"clusterID" , 
"-R" , 
"/opt/apache-hadoop/logs" ] ) 
~~ if "Incompatible" in clusterID : 
retries += 1 
subprocess . check_call ( [ "docker" , 
"rm" , 
"-rf" , 
"/ephemeral/hdfs" ] ) 
"kill" , 
self . hdfsContainerID ] ) 
hdfs_down = False 
~~ ~~ if retries >= 5 : 
~~ def __start_datanode ( self , job ) : 
tool = "quay.io/ucsc_cgl/apache-hadoop-worker:2.6.2" , 
"-v" , "/mnt/ephemeral/:/ephemeral/:rw" ] , 
parameters = [ self . masterIP ] ) [ : - 1 ] 
~~ def stop ( self , fileStore ) : 
subprocess . call ( [ "docker" , "exec" , self . sparkContainerID , "rm" , "-r" , "/ephemeral/spark" ] ) 
subprocess . call ( [ "docker" , "stop" , self . sparkContainerID ] ) 
subprocess . call ( [ "docker" , "rm" , self . sparkContainerID ] ) 
subprocess . call ( [ "docker" , "exec" , self . hdfsContainerID , "rm" , "-r" , "/ephemeral/hdfs" ] ) 
subprocess . call ( [ "docker" , "stop" , self . hdfsContainerID ] ) 
subprocess . call ( [ "docker" , "rm" , self . hdfsContainerID ] ) 
~~ def check ( self ) : 
status = _checkContainerStatus ( self . sparkContainerID , 
sparkNoun = 'worker' , 
hdfsNoun = 'datanode' ) 
~~ def minimize_best_n ( Members ) : 
return ( list ( reversed ( sorted ( 
Members , key = lambda Member : Member . fitness_score 
) ) ) ) 
~~ def fitness ( self ) : 
if len ( self . __members ) != 0 : 
~~~ if self . __num_processes > 1 : 
~~~ members = [ m . get ( ) for m in self . __members ] 
~~~ members = self . __members 
~~ return sum ( m . fitness_score for m in members ) / len ( members ) 
~~ ~~ def ave_cost_fn_val ( self ) : 
~~ return sum ( m . cost_fn_val for m in members ) / len ( members ) 
~~ ~~ def med_cost_fn_val ( self ) : 
~~ return median ( [ m . cost_fn_val for m in members ] ) 
~~ ~~ def parameters ( self ) : 
~~ params = { } 
for p in self . __parameters : 
~~~ params [ p . name ] = sum ( 
m . parameters [ p . name ] for m in members 
) / len ( members ) 
~~ return params 
~~ ~~ def members ( self ) : 
if self . __num_processes > 1 : 
~~~ return [ m . get ( ) for m in self . __members ] 
~~~ return self . __members 
~~ ~~ def add_parameter ( self , name , min_val , max_val ) : 
self . __parameters . append ( Parameter ( name , min_val , max_val ) ) 
~~ def generate_population ( self ) : 
~~~ process_pool = Pool ( processes = self . __num_processes ) 
~~ self . __members = [ ] 
for _ in range ( self . __pop_size ) : 
~~~ feed_dict = { } 
for param in self . __parameters : 
~~~ feed_dict [ param . name ] = self . __random_param_val ( 
param . min_val , 
param . max_val , 
param . dtype 
~~ if self . __num_processes > 1 : 
~~~ self . __members . append ( process_pool . apply_async ( 
self . _start_process , 
[ self . __cost_fn , feed_dict , self . __cost_fn_args ] ) 
~~~ self . __members . append ( 
Member ( 
feed_dict , 
self . __cost_fn ( feed_dict , self . __cost_fn_args ) 
~~ ~~ if self . __num_processes > 1 : 
~~~ process_pool . close ( ) 
process_pool . join ( ) 
~~ self . __determine_best_member ( ) 
~~ def next_generation ( self , mut_rate = 0 , max_mut_amt = 0 , log_base = 10 ) : 
members = [ m . get ( ) for m in self . __members ] 
~~ if len ( members ) == 0 : 
~~~ raise Exception ( 
~~ selected_members = self . __select_fn ( members ) 
reproduction_probs = list ( reversed ( logspace ( 0.0 , 1.0 , 
num = len ( selected_members ) , base = log_base ) ) ) 
reproduction_probs = reproduction_probs / sum ( reproduction_probs ) 
self . __members = [ ] 
~~~ parent_1 = nrandom . choice ( selected_members , p = reproduction_probs ) 
parent_2 = nrandom . choice ( selected_members , p = reproduction_probs ) 
feed_dict = { } 
~~~ which_parent = uniform ( 0 , 1 ) 
if which_parent < 0.5 : 
~~~ feed_dict [ param . name ] = parent_1 . parameters [ param . name ] 
~~~ feed_dict [ param . name ] = parent_2 . parameters [ param . name ] 
~~ feed_dict [ param . name ] = self . __mutate_parameter ( 
feed_dict [ param . name ] , param , mut_rate , max_mut_amt 
~~ def __mutate_parameter ( value , param , mut_rate , max_mut_amt ) : 
if uniform ( 0 , 1 ) < mut_rate : 
~~~ mut_amt = uniform ( 0 , max_mut_amt ) 
op = choice ( ( add , sub ) ) 
new_val = op ( value , param . dtype ( 
( param . max_val - param . min_val ) * mut_amt 
if new_val > param . max_val : 
~~~ return param . max_val 
~~ elif new_val < param . min_val : 
~~~ return param . min_val 
~~ ~~ def __determine_best_member ( self ) : 
~~ if self . __best_fitness is None : 
~~~ self . __best_fitness = members [ 0 ] . fitness_score 
self . __best_cost_fn_val = members [ 0 ] . cost_fn_val 
self . __best_parameters = { } 
~~~ self . __best_parameters [ p . name ] = members [ 0 ] . parameters [ p . name ] 
~~ ~~ for m_id , member in enumerate ( members ) : 
~~~ if member . fitness_score > self . __best_fitness : 
~~~ self . __best_fitness = member . fitness_score 
self . __best_cost_fn_val = member . cost_fn_val 
~~~ self . __best_parameters [ p . name ] = member . parameters [ p . name ] 
~~ ~~ ~~ ~~ def throws_exception ( callable , * exceptions ) : 
with context . ExceptionTrap ( ) : 
~~~ with context . ExceptionTrap ( * exceptions ) as exc : 
~~~ callable ( ) 
~~ ~~ return bool ( exc ) 
~~ def classpath ( v ) : 
if isinstance ( v , type ) : 
~~~ ret = strclass ( v ) 
~~~ ret = strclass ( v . __class__ ) 
~~ def loghandler_members ( ) : 
Members = namedtuple ( "Members" , [ "name" , "handler" , "member_name" , "member" ] ) 
log_manager = logging . Logger . manager 
loggers = [ ] 
ignore = set ( [ modname ( ) ] ) 
if log_manager . root : 
~~~ loggers = list ( log_manager . loggerDict . items ( ) ) 
loggers . append ( ( "root" , log_manager . root ) ) 
~~ for logger_name , logger in loggers : 
~~~ if logger_name in ignore : continue 
for handler in getattr ( logger , "handlers" , [ ] ) : 
~~~ members = inspect . getmembers ( handler ) 
for member_name , member in members : 
~~~ yield Members ( logger_name , handler , member_name , member ) 
~~ ~~ ~~ ~~ def get_counts ( ) : 
counts = { } 
ks = [ 
( 'PYT_TEST_CLASS_COUNT' , "classes" ) , 
( 'PYT_TEST_COUNT' , "tests" ) , 
( 'PYT_TEST_MODULE_COUNT' , "modules" ) , 
for ek , cn in ks : 
~~~ counts [ cn ] = int ( os . environ . get ( ek , 0 ) ) 
~~ return counts 
~~ def is_single_class ( ) : 
counts = get_counts ( ) 
if counts [ "classes" ] < 1 and counts [ "modules" ] < 1 : 
~~~ ret = counts [ "tests" ] > 0 
~~~ ret = counts [ "classes" ] <= 1 and counts [ "modules" ] <= 1 
~~ def is_single_module ( ) : 
if counts [ "modules" ] == 1 : 
~~~ ret = True 
~~ elif counts [ "modules" ] < 1 : 
~~~ ret = is_single_class ( ) 
~~ def sub_symbols ( pattern , code , symbol ) : 
return pattern . replace ( '' , code ) . replace ( '' , symbol ) 
~~ def amount_converter ( obj ) : 
if isinstance ( obj , Decimal ) : 
~~ elif isinstance ( obj , ( str , int , float ) ) : 
~~~ return Decimal ( str ( obj ) ) 
~~ ~~ def coerce ( value ) : 
with contextlib2 . suppress ( Exception ) : 
~~~ loaded = json . loads ( value ) 
assert isinstance ( loaded , numbers . Number ) 
return loaded 
~~ def ensure_fresh_rates ( func ) : 
def wrapper ( self , * args , ** kwargs ) : 
~~~ if self . last_updated + timedelta ( minutes = 5 ) < zulu . now ( ) : 
~~~ self . refresh ( ) 
~~ def quotation ( self , origin , target ) : 
a = self . rate ( origin ) 
b = self . rate ( target ) 
if a and b : 
~~~ return Decimal ( b ) / Decimal ( a ) 
~~ def format_currency ( number , currency , format = None , 
locale = LC_NUMERIC , currency_digits = True , 
format_type = 'standard' , decimal_quantization = True ) : 
locale = Locale . parse ( locale ) 
if format : 
~~~ pattern = parse_pattern ( format ) 
~~~ p = locale . currency_formats [ format_type ] 
pattern = NumberPattern ( 
p . pattern , p . prefix , p . suffix , p . grouping , p . int_prec , 
p . frac_prec , p . exp_prec , p . exp_plus ) 
~~~ raise UnknownCurrencyFormatError ( 
~~ ~~ return pattern . apply ( 
number , locale , currency = currency , currency_digits = currency_digits , 
decimal_quantization = decimal_quantization ) 
~~ def parse_pattern ( pattern ) : 
if isinstance ( pattern , NumberPattern ) : 
~~~ return pattern 
~~ def _match_number ( pattern ) : 
~~~ rv = number_re . search ( pattern ) 
if rv is None : 
~~ return rv . groups ( ) 
~~ pos_pattern = pattern 
if ';' in pattern : 
~~~ pos_pattern , neg_pattern = pattern . split ( ';' , 1 ) 
pos_prefix , number , pos_suffix = _match_number ( pos_pattern ) 
neg_prefix , _ , neg_suffix = _match_number ( neg_pattern ) 
~~~ pos_prefix , number , pos_suffix = _match_number ( pos_pattern ) 
neg_prefix = '-' + pos_prefix 
neg_suffix = pos_suffix 
~~ if 'E' in number : 
~~~ number , exp = number . split ( 'E' , 1 ) 
~~~ exp = None 
~~ if '@' in number : 
~~~ if '.' in number and '0' in number : 
~~ ~~ if '.' in number : 
~~~ integer , fraction = number . rsplit ( '.' , 1 ) 
~~~ integer = number 
fraction = '' 
~~ def parse_precision ( p ) : 
min = max = 0 
for c in p : 
~~~ if c in '@0' : 
~~~ min += 1 
max += 1 
~~ elif c == '#' : 
~~~ max += 1 
~~ elif c == ',' : 
~~ ~~ return min , max 
~~ int_prec = parse_precision ( integer ) 
frac_prec = parse_precision ( fraction ) 
if exp : 
~~~ exp_plus = exp . startswith ( '+' ) 
exp = exp . lstrip ( '+' ) 
exp_prec = parse_precision ( exp ) 
~~~ exp_plus = None 
exp_prec = None 
~~ grouping = babel . numbers . parse_grouping ( integer ) 
return NumberPattern ( pattern , ( pos_prefix , neg_prefix ) , 
( pos_suffix , neg_suffix ) , grouping , 
int_prec , frac_prec , 
exp_prec , exp_plus ) 
~~ def get_decimal_quantum ( precision ) : 
assert isinstance ( precision , ( int , decimal . Decimal ) ) 
return decimal . Decimal ( 10 ) ** ( - precision ) 
~~ def get_decimal_precision ( number ) : 
assert isinstance ( number , decimal . Decimal ) 
decimal_tuple = number . normalize ( ) . as_tuple ( ) 
if decimal_tuple . exponent >= 0 : 
~~ return abs ( decimal_tuple . exponent ) 
~~ def apply ( 
self , value , locale , currency = None , currency_digits = True , 
decimal_quantization = True ) : 
if not isinstance ( value , decimal . Decimal ) : 
~~~ value = decimal . Decimal ( str ( value ) ) 
~~ value = value . scaleb ( self . scale ) 
is_negative = int ( value . is_signed ( ) ) 
value = abs ( value ) . normalize ( ) 
if self . exp_prec : 
~~~ value , exp , exp_sign = self . scientific_notation_elements ( 
value , locale ) 
~~ frac_prec = self . frac_prec 
if currency and currency_digits : 
~~~ frac_prec = ( babel . numbers . get_currency_precision ( currency ) , ) * 2 
~~ if not decimal_quantization or ( self . exp_prec and frac_prec == ( 0 , 0 ) ) : 
~~~ frac_prec = ( frac_prec [ 0 ] , max ( [ frac_prec [ 1 ] , 
get_decimal_precision ( value ) ] ) ) 
~~ if self . exp_prec : 
~~~ number = '' . join ( [ 
self . _quantize_value ( value , locale , frac_prec ) , 
babel . numbers . get_exponential_symbol ( locale ) , 
exp_sign , 
self . _format_int ( 
str ( exp ) , self . exp_prec [ 0 ] , self . exp_prec [ 1 ] , locale ) ] ) 
~~ elif '@' in self . pattern : 
~~~ text = self . _format_significant ( value , 
self . int_prec [ 0 ] , 
self . int_prec [ 1 ] ) 
a , sep , b = text . partition ( "." ) 
number = self . _format_int ( a , 0 , 1000 , locale ) 
if sep : 
~~~ number += babel . numbers . get_decimal_symbol ( locale ) + b 
~~~ number = self . _quantize_value ( value , locale , frac_prec ) 
~~ retval = '' . join ( [ 
self . prefix [ is_negative ] , 
number , 
self . suffix [ is_negative ] ] ) 
if u'' in retval : 
~~~ retval = retval . replace ( u'' , 
babel . numbers . get_currency_name ( 
currency , value , locale ) ) 
retval = retval . replace ( u'' , currency . upper ( ) ) 
retval = retval . replace ( u'' , babel . numbers . get_currency_symbol ( 
currency , locale ) ) 
~~ return retval 
~~ def scientific_notation_elements ( self , value , locale ) : 
exp = value . adjusted ( ) 
value = value * get_decimal_quantum ( exp ) 
assert value . adjusted ( ) == 0 
lead_shift = max ( [ 1 , min ( self . int_prec ) ] ) - 1 
exp = exp - lead_shift 
value = value * get_decimal_quantum ( - lead_shift ) 
exp_sign = '' 
if exp < 0 : 
~~~ exp_sign = babel . numbers . get_minus_sign_symbol ( locale ) 
~~ elif self . exp_plus : 
~~~ exp_sign = babel . numbers . get_plus_sign_symbol ( locale ) 
~~ exp = abs ( exp ) 
return value , exp , exp_sign 
~~ def do_dice_roll ( ) : 
options = get_options ( ) 
dice = Dice ( options . sides ) 
rolls = [ dice . roll ( ) for n in range ( options . number ) ] 
for roll in rolls : 
~~~ print ( 'rolled' , roll ) 
~~ if options . number > 1 : 
~~~ print ( 'total' , sum ( rolls ) ) 
~~ ~~ def price_converter ( obj ) : 
if isinstance ( obj , str ) : 
~~~ obj = PriceClass . parse ( obj ) 
~~ def price ( * args , ** kwargs ) : 
kwargs . setdefault ( 'converter' , price_converter ) 
if 'validator' in kwargs : 
~~~ validator = kwargs . pop ( 'validator' ) 
if not isinstance ( validator , ( tuple , list ) ) : 
~~~ validator = [ validator ] 
~~~ validator = [ ] 
~~ validator . append ( instance_of ( PriceClass ) ) 
return attr . ib ( validator = validator , * args , ** kwargs ) 
if isinstance ( obj , models . Model ) : 
~~~ return self . encode ( model_to_dict ( obj ) ) 
~~ elif isinstance ( obj , models . query . QuerySet ) : 
~~~ return serializers . serialize ( 'json' , obj ) 
~~~ return super ( JsonResponseEncoder , self ) . default ( obj ) 
~~ ~~ def api_accepts ( fields ) : 
def decorator ( func ) : 
def wrapped_func ( request , * args , ** kwargs ) : 
~~~ if request . method not in [ 'GET' , 'POST' ] : 
~~~ return func ( request , * args , ** kwargs ) 
~~ form_class = type ( 'ApiForm' , ( forms . Form , ) , fields . copy ( ) ) 
form = form_class ( getattr ( request , request . method ) ) 
if not form . is_valid ( ) : 
~~~ if settings . DEBUG : 
~~~ return JsonResponseBadRequest ( 
~~~ logger . warn ( 
request . path , 
dict ( form . errors ) 
return func ( request , * args , ** kwargs ) 
~~ ~~ for ( field_name , field_instance ) in fields . items ( ) : 
~~~ if isinstance ( field_instance , models . Model ) : 
~~~ field_type = type ( field_instance ) 
field_id = '%s-id' % field_name 
if field_id not in request . REQUEST : 
~~ field_pk = int ( request . REQUEST [ field_id ] ) 
~~~ field_value = field_type . objects . get ( pk = field_pk ) 
~~ except field_type . DoesNotExist : 
~~~ return JsonResponseNotFound ( 
field_type , field_pk 
~~ form . cleaned_data [ field_name ] = field_value 
~~ ~~ validated_request = ValidatedRequest ( request , form ) 
return func ( validated_request , * args , ** kwargs ) 
~~ return wrapped_func 
~~ def api_returns ( return_values ) : 
~~~ return_value = func ( request , * args , ** kwargs ) 
if not isinstance ( return_value , JsonResponse ) : 
~~ ~~ accepted_return_codes = return_values . keys ( ) 
accepted_return_codes . append ( 500 ) 
if return_value . status_code not in accepted_return_codes : 
( return_value . status_code , accepted_return_codes ) 
return_value . status_code , 
accepted_return_codes , 
~~ ~~ return return_value 
~~ def api ( accept_return_dict ) : 
~~~ @ api_accepts ( accept_return_dict [ 'accepts' ] ) 
@ api_returns ( accept_return_dict [ 'returns' ] ) 
def apid_fnc ( request , * args , ** kwargs ) : 
~~ return apid_fnc ( request , * args , ** kwargs ) 
~~ def validate_json_request ( required_fields ) : 
~~~ request_dict = json . loads ( request . raw_post_data ) 
~~ for k in required_fields : 
~~~ if k not in request_dict : 
~~ ~~ return func ( request , request_dict , * args , ** kwargs ) 
~~ def set_possible ( self ) : 
possible = [ ] 
name = self . name 
name_f = self . name . lower ( ) 
filepath = "" 
if name_f . endswith ( ".py" ) or ".py:" in name_f : 
~~~ bits = name . split ( ":" , 1 ) 
filepath = bits [ 0 ] 
name = bits [ 1 ] if len ( bits ) > 1 else "" 
~~ ~~ bits = name . split ( '.' ) 
basedir = self . basedir 
method_prefix = self . method_prefix 
if re . search ( r'^\\*?[A-Z]' , bits [ - 1 ] ) : 
possible . append ( PathFinder ( basedir , method_prefix , ** { 
'class_name' : bits [ - 1 ] , 
'module_name' : bits [ - 2 ] if len ( bits ) > 1 else '' , 
'prefix' : os . sep . join ( bits [ 0 : - 2 ] ) , 
'filepath' : filepath , 
} ) ) 
~~ elif len ( bits ) > 1 and re . search ( r'^\\*?[A-Z]' , bits [ - 2 ] ) : 
'class_name' : bits [ - 2 ] , 
'method_name' : bits [ - 1 ] , 
'module_name' : bits [ - 3 ] if len ( bits ) > 2 else '' , 
'prefix' : os . sep . join ( bits [ 0 : - 3 ] ) , 
~~~ if self . name : 
~~~ if filepath : 
~~~ if len ( bits ) : 
~~~ possible . append ( PathFinder ( basedir , method_prefix , ** { 
'method_name' : bits [ 0 ] , 
'module_name' : bits [ - 1 ] , 
'prefix' : os . sep . join ( bits [ 0 : - 1 ] ) , 
'prefix' : os . sep . join ( bits ) , 
~~~ possible . append ( PathFinder ( basedir , method_prefix , filepath = filepath ) ) 
self . possible = possible 
~~ def modules ( self ) : 
sys . path . insert ( 0 , self . basedir ) 
for p in self . paths ( ) : 
~~~ module_name = self . module_path ( p ) 
m = importlib . import_module ( module_name ) 
yield m 
logger . warning ( e , exc_info = True ) 
error_info = getattr ( self , 'error_info' , None ) 
if not error_info : 
~~~ exc_info = sys . exc_info ( ) 
self . error_info = exc_info 
~~ ~~ sys . path . pop ( 0 ) 
~~ def classes ( self ) : 
for module in self . modules ( ) : 
~~~ cs = inspect . getmembers ( module , inspect . isclass ) 
class_name = getattr ( self , 'class_name' , '' ) 
class_regex = '' 
if class_name : 
~~~ if class_name . startswith ( "*" ) : 
~~~ class_name = class_name . strip ( "*" ) 
class_regex = re . compile ( r'.*?{}' . format ( class_name ) , re . I ) 
~~~ class_regex = re . compile ( r'^{}' . format ( class_name ) , re . I ) 
~~ ~~ for c_name , c in cs : 
~~~ can_yield = True 
if class_regex and not class_regex . match ( c_name ) : 
~~~ can_yield = False 
~~ if can_yield and issubclass ( c , unittest . TestCase ) : 
yield c 
~~ ~~ ~~ ~~ ~~ def method_names ( self ) : 
for c in self . classes ( ) : 
~~~ ms = inspect . getmembers ( c , lambda f : inspect . ismethod ( f ) or inspect . isfunction ( f ) ) 
method_name = getattr ( self , 'method_name' , '' ) 
method_regex = '' 
if method_name : 
~~~ if method_name . startswith ( self . method_prefix ) : 
~~~ method_regex = re . compile ( r'^{}' . format ( method_name ) , flags = re . I ) 
~~~ if method_name . startswith ( "*" ) : 
~~~ method_name = method_name . strip ( "*" ) 
method_regex = re . compile ( 
r'^{}[_]{{0,1}}.*?{}' . format ( self . method_prefix , method_name ) , 
flags = re . I 
~~~ method_regex = re . compile ( 
r'^{}[_]{{0,1}}{}' . format ( self . method_prefix , method_name ) , 
~~ ~~ ~~ for m_name , m in ms : 
~~~ if not m_name . startswith ( self . method_prefix ) : continue 
can_yield = True 
if method_regex and not method_regex . match ( m_name ) : 
~~ if can_yield : 
yield c , m_name 
~~ ~~ ~~ ~~ def _find_basename ( self , name , basenames , is_prefix = False ) : 
ret = "" 
fileroots = [ ( os . path . splitext ( n ) [ 0 ] , n ) for n in basenames ] 
glob = False 
if name . startswith ( "*" ) : 
~~~ glob = True 
~~ name = name . strip ( "*" ) 
for fileroot , basename in fileroots : 
~~~ if name in fileroot or fileroot in name : 
~~~ for pf in self . module_postfixes : 
~~~ logger . debug ( 
basename , 
pf 
if glob : 
~~~ if name in fileroot and fileroot . endswith ( pf ) : 
~~~ ret = basename 
~~~ if fileroot . startswith ( name ) and fileroot . endswith ( pf ) : 
~~ ~~ ~~ if not ret : 
~~~ for pf in self . module_prefixes : 
~~~ n = pf + name 
~~~ if fileroot . startswith ( pf ) and name in fileroot : 
~~~ if fileroot . startswith ( n ) : 
~~ ~~ ~~ ~~ if not ret : 
~~~ if is_prefix : 
if basename . startswith ( name ) or ( glob and name in basename ) : 
~~~ if name in basename and self . _is_module_path ( basename ) : 
~~~ if basename . startswith ( name ) and self . _is_module_path ( basename ) : 
~~ ~~ ~~ ~~ ~~ if ret : 
~~ ~~ ~~ return ret 
~~ def _find_prefix_path ( self , basedir , prefix ) : 
for ret in self . _find_prefix_paths ( basedir , prefix ) : 
~~ if not ret : 
~~ def _is_module_path ( self , path ) : 
basename = os . path . basename ( path ) 
fileroot = os . path . splitext ( basename ) [ 0 ] 
for pf in self . module_postfixes : 
~~~ if fileroot . endswith ( pf ) : 
~~ ~~ if not ret : 
~~~ if fileroot . startswith ( pf ) : 
~~ def walk ( self , basedir ) : 
system_d = SitePackagesDir ( ) 
filter_system_d = system_d and os . path . commonprefix ( [ system_d , basedir ] ) != system_d 
for root , dirs , files in os . walk ( basedir , topdown = True ) : 
~~~ dirs [ : ] = [ d for d in dirs if d [ 0 ] != '.' and d [ 0 ] != "_" ] 
if filter_system_d : 
~~~ dirs [ : ] = [ d for d in dirs if not d . startswith ( system_d ) ] 
~~ yield root , dirs , files 
~~ ~~ def paths ( self ) : 
module_name = getattr ( self , 'module_name' , '' ) 
module_prefix = getattr ( self , 'prefix' , '' ) 
filepath = getattr ( self , 'filepath' , '' ) 
if filepath : 
~~~ if os . path . isabs ( filepath ) : 
~~~ yield filepath 
~~~ yield os . path . join ( self . basedir , filepath ) 
~~~ if module_prefix : 
~~~ basedirs = self . _find_prefix_paths ( self . basedir , module_prefix ) 
~~~ basedirs = [ self . basedir ] 
~~ for basedir in basedirs : 
~~~ if module_name : 
~~~ path = self . _find_module_path ( basedir , module_name ) 
~~~ path = basedir 
~~ if os . path . isfile ( path ) : 
yield path 
~~~ seen_paths = set ( ) 
for root , dirs , files in self . walk ( path ) : 
~~~ for basename in files : 
~~~ if basename . startswith ( "__init__" ) : 
~~~ if self . _is_module_path ( root ) : 
~~~ filepath = os . path . join ( root , basename ) 
if filepath not in seen_paths : 
seen_paths . add ( filepath ) 
yield filepath 
~~~ fileroot = os . path . splitext ( basename ) [ 0 ] 
~~ ~~ ~~ for pf in self . module_prefixes : 
~~ ~~ ~~ ~~ ~~ ~~ ~~ ~~ except IOError as e : 
~~~ logger . warning ( e , exc_info = True ) 
~~ ~~ ~~ ~~ def module_path ( self , filepath ) : 
possible_modbits = re . split ( '[\\\\/]' , filepath . strip ( '\\\\/' ) ) 
basename = possible_modbits [ - 1 ] 
prefixes = possible_modbits [ 0 : - 1 ] 
modpath = [ ] 
discarded = [ ] 
for i in range ( len ( prefixes ) ) : 
~~~ path_args = [ "/" ] 
path_args . extend ( prefixes [ 0 : i + 1 ] ) 
path_args . append ( '__init__.py' ) 
prefix_module = os . path . join ( * path_args ) 
if os . path . isfile ( prefix_module ) : 
~~~ modpath = prefixes [ i : ] 
~~~ discarded = path_args [ 0 : - 1 ] 
~~ ~~ modpath . append ( basename ) 
module_name = '.' . join ( modpath ) 
module_name = re . sub ( r'(?:\\.__init__)?\\.py$' , '' , module_name , flags = re . I ) 
return module_name 
~~ def Popen_nonblocking ( * args , ** kwargs ) : 
kwargs . setdefault ( 'close_fds' , 'posix' in sys . builtin_module_names ) 
kwargs . setdefault ( 'bufsize' , 1 ) 
proc = subprocess . Popen ( * args , ** kwargs ) 
if proc . stdout : 
~~~ q = queue . Queue ( ) 
t = threading . Thread ( 
target = enqueue_lines , 
args = ( proc . stdout , q ) ) 
proc . stdout = q 
t . daemon = True 
t . start ( ) 
~~ if proc . stderr : 
args = ( proc . stderr , q ) ) 
proc . stderr = q 
~~ return proc 
~~ def currencyFormat ( _context , code , symbol , format , 
currency_digits = True , decimal_quantization = True , 
name = '' ) : 
_context . action ( 
discriminator = ( 'currency' , name , code ) , 
callable = _register_currency , 
args = ( name , code , symbol , format , currency_digits , 
decimal_quantization ) 
~~ def exchange ( _context , component , backend , base , name = '' ) : 
discriminator = ( 'currency' , 'exchange' , component ) , 
callable = _register_exchange , 
args = ( name , component , backend , base ) 
~~ def atomize ( f , lock = None ) : 
lock = lock or threading . RLock ( ) 
def exec_atomic ( * args , ** kwargs ) : 
~~~ lock . acquire ( ) 
~~ ~~ return exec_atomic 
~~ def _support ( self , caller ) : 
markdown_content = caller ( ) 
html_content = markdown . markdown ( 
markdown_content , 
extensions = [ 
"markdown.extensions.fenced_code" , 
CodeHiliteExtension ( css_class = "highlight" ) , 
"markdown.extensions.tables" , 
return html_content 
~~ def _code_support ( self , language , caller ) : 
code = caller ( ) 
lines = code . splitlines ( ) 
first_nonempty_line_index = 0 
while not lines [ first_nonempty_line_index ] : 
~~~ first_nonempty_line_index += 1 
~~ len_to_trim = len ( lines [ first_nonempty_line_index ] ) - len ( 
lines [ first_nonempty_line_index ] . lstrip ( ) 
lines = [ x [ len_to_trim : ] for x in lines ] 
code = "\\n" . join ( lines ) 
~~~ lexer = get_lexer_by_name ( language , stripall = True ) 
~~~ lexer = guess_lexer ( code ) 
~~ highlighted = highlight ( code , lexer , HtmlFormatter ( ) ) 
return highlighted 
~~ def render_grid_file ( context , f ) : 
identifier = str ( f . _id ) , 
filename = f . filename , 
length = f . length , 
mimetype = f . content_type 
~~ response . conditional_response = True 
if context . request . if_range . match_response ( response ) : 
~~ def start ( self , context ) : 
if __debug__ : 
uri = redact_uri ( self . uri ) , 
config = self . config , 
alias = self . alias , 
~~ engine = self . engine = create_engine ( self . uri , ** self . config ) 
self . Session = scoped_session ( sessionmaker ( bind = engine ) ) 
engine . connect ( ) . close ( ) 
context . db [ self . alias ] = engine 
~~ def get_time_units_and_multiplier ( seconds ) : 
for cutoff , units , multiplier in units_table : 
~~~ if seconds < cutoff : 
~~ ~~ return units , multiplier 
~~ def format_duration ( seconds ) : 
units , divider = get_time_units_and_multiplier ( seconds ) 
seconds *= divider 
self . config [ 'alias' ] = self . alias 
safe_config = dict ( self . config ) 
del safe_config [ 'host' ] 
uri = redact_uri ( self . config [ 'host' ] ) , 
self . connection = connect ( ** self . config ) 
~~ def prepare ( self , context ) : 
context . db [ self . alias ] = MongoEngineProxy ( self . connection ) 
~~ def highlight_info ( ctx , style ) : 
click . echo ( list ( pygments . styles . get_all_styles ( ) ) ) 
click . echo ( ) 
click . secho ( 
f\ , fg = "green" 
click . echo ( pygments . formatters . HtmlFormatter ( style = style ) . get_style_defs ( ) ) 
~~ def _connect ( self , context ) : 
uri = redact_uri ( self . uri , self . protect ) , 
~~ self . connection = context . db [ self . alias ] = self . _connector ( self . uri , ** self . config ) 
~~ def _handle_event ( self , event , * args , ** kw ) : 
for engine in self . engines . values ( ) : 
~~~ if hasattr ( engine , event ) : 
~~~ getattr ( engine , event ) ( * args , ** kw ) 
~~ ~~ ~~ def run ( self ) : 
while not self . stopper . is_set ( ) : 
~~~ item = self . in_queue . get ( timeout = 5 ) 
~~ except queue . Empty : 
~~~ result = self . func ( item ) 
~~~ self . out_queue . put ( result ) 
~~ ~~ ~~ def _content_type_matches ( candidate , pattern ) : 
def _wildcard_compare ( type_spec , type_pattern ) : 
~~~ return type_pattern == '*' or type_spec == type_pattern 
~~ return ( 
_wildcard_compare ( candidate . content_type , pattern . content_type ) and 
_wildcard_compare ( candidate . content_subtype , pattern . content_subtype ) 
~~ def select_content_type ( requested , available ) : 
class Match ( object ) : 
WILDCARD , PARTIAL , FULL_TYPE , = 2 , 1 , 0 
def __init__ ( self , candidate , pattern ) : 
~~~ self . candidate = candidate 
self . pattern = pattern 
if pattern . content_type == pattern . content_subtype == '*' : 
~~~ self . match_type = self . WILDCARD 
~~ elif pattern . content_subtype == '*' : 
~~~ self . match_type = self . PARTIAL 
~~~ self . match_type = self . FULL_TYPE 
~~ self . parameter_distance = len ( self . candidate . parameters ) 
for key , value in candidate . parameters . items ( ) : 
~~~ if key in pattern . parameters : 
~~~ if pattern . parameters [ key ] == value : 
~~~ self . parameter_distance -= 1 
~~~ self . parameter_distance += 1 
~~ ~~ ~~ ~~ ~~ def extract_quality ( obj ) : 
~~~ return getattr ( obj , 'quality' , 1.0 ) 
~~ matches = [ ] 
for pattern in sorted ( requested , key = extract_quality , reverse = True ) : 
~~~ for candidate in sorted ( available ) : 
~~~ if _content_type_matches ( candidate , pattern ) : 
~~~ if extract_quality ( pattern ) == 0.0 : 
~~ return candidate , pattern 
~~ matches . append ( Match ( candidate , pattern ) ) 
~~ ~~ ~~ if not matches : 
~~~ raise errors . NoMatch 
~~ matches = sorted ( matches , 
key = attrgetter ( 'match_type' , 'parameter_distance' ) ) 
return matches [ 0 ] . candidate , matches [ 0 ] . pattern 
~~ def rewrite_url ( input_url , ** kwargs ) : 
scheme , netloc , path , query , fragment = parse . urlsplit ( input_url ) 
if 'scheme' in kwargs : 
~~~ scheme = kwargs [ 'scheme' ] 
~~ ident , host_n_port = parse . splituser ( netloc ) 
user , password = parse . splitpasswd ( ident ) if ident else ( None , None ) 
if 'user' in kwargs : 
~~~ user = kwargs [ 'user' ] 
~~ elif user is not None : 
~~~ user = parse . unquote_to_bytes ( user ) . decode ( 'utf-8' ) 
~~ if 'password' in kwargs : 
~~~ password = kwargs [ 'password' ] 
~~ elif password is not None : 
~~~ password = parse . unquote_to_bytes ( password ) . decode ( 'utf-8' ) 
~~ ident = _create_url_identifier ( user , password ) 
host , port = parse . splitnport ( host_n_port , defport = None ) 
if 'host' in kwargs : 
~~~ host = kwargs [ 'host' ] 
if host is not None : 
~~~ host = _normalize_host ( 
host , 
enable_long_host = kwargs . get ( 'enable_long_host' , False ) , 
encode_with_idna = kwargs . get ( 'encode_with_idna' , None ) , 
scheme = scheme , 
~~ ~~ if 'port' in kwargs : 
~~~ port = kwargs [ 'port' ] 
if port is not None : 
~~~ port = int ( kwargs [ 'port' ] ) 
if port < 0 : 
~~ ~~ ~~ if host is None or host == '' : 
~~~ host_n_port = None 
~~ elif port is None : 
~~~ host_n_port = host 
~~~ host_n_port = '{0}:{1}' . format ( host , port ) 
~~ if 'path' in kwargs : 
~~~ path = kwargs [ 'path' ] 
~~~ path = '/' 
~~~ path = parse . quote ( path . encode ( 'utf-8' ) , safe = PATH_SAFE_CHARS ) 
~~ ~~ netloc = '{0}@{1}' . format ( ident , host_n_port ) if ident else host_n_port 
if 'query' in kwargs : 
~~~ new_query = kwargs [ 'query' ] 
if new_query is None : 
~~~ query = None 
~~~ params = [ ] 
~~~ for param in sorted ( new_query . keys ( ) ) : 
~~~ params . append ( ( param , new_query [ param ] ) ) 
~~~ params = [ ( param , value ) for param , value in new_query ] 
~~ ~~ if params : 
~~~ query = parse . urlencode ( params ) 
~~~ query = new_query 
~~ ~~ ~~ if 'fragment' in kwargs : 
~~~ fragment = kwargs [ 'fragment' ] 
if fragment is not None : 
~~~ fragment = parse . quote ( fragment . encode ( 'utf-8' ) , 
safe = FRAGMENT_SAFE_CHARS ) 
~~ ~~ if scheme is None : 
~~~ scheme = '' 
~~ return parse . urlunsplit ( ( scheme , netloc , path , query , fragment ) ) 
~~ def remove_url_auth ( url ) : 
parts = parse . urlsplit ( url ) 
return RemoveUrlAuthResult ( auth = ( parts . username or None , parts . password ) , 
url = rewrite_url ( url , user = None , password = None ) ) 
~~ def _create_url_identifier ( user , password ) : 
if user is not None : 
~~~ user = parse . quote ( user . encode ( 'utf-8' ) , safe = USERINFO_SAFE_CHARS ) 
if password : 
~~~ password = parse . quote ( password . encode ( 'utf-8' ) , 
safe = USERINFO_SAFE_CHARS ) 
return '{0}:{1}' . format ( user , password ) 
~~ return user 
~~ def _normalize_host ( host , enable_long_host = False , encode_with_idna = None , 
scheme = None ) : 
if encode_with_idna is not None : 
~~~ enable_idna = encode_with_idna 
~~~ enable_idna = scheme . lower ( ) in IDNA_SCHEMES if scheme else False 
~~ if enable_idna : 
~~~ host = '.' . join ( segment . encode ( 'idna' ) . decode ( ) 
for segment in host . split ( '.' ) ) 
~~ except UnicodeError as exc : 
~~~ host = parse . quote ( host . encode ( 'utf-8' ) , safe = HOST_SAFE_CHARS ) 
~~ if len ( host ) > 255 and not enable_long_host : 
~~ return host 
~~ def transform ( self , X ) : 
noise = self . _noise_func ( * self . _args , size = X . shape ) 
results = X + noise 
self . relative_noise_size_ = self . relative_noise_size ( X , results ) 
return results 
~~ def relative_noise_size ( self , data , noise ) : 
return np . mean ( [ 
sci_dist . cosine ( u / la . norm ( u ) , v / la . norm ( v ) ) 
for u , v in zip ( noise , data ) 
~~ def general ( * dargs , ** dkwargs ) : 
invoked = bool ( len ( dargs ) == 1 and not dkwargs and callable ( dargs [ 0 ] ) ) 
if invoked : 
~~~ func = dargs [ 0 ] 
~~ def wrapper ( func ) : 
~~~ @ hand . _click . command ( 
name = func . __name__ . lower ( ) , 
help = func . __doc__ ) 
def _wrapper ( * args , ** kwargs ) : 
~~~ log_level = dkwargs . pop ( 'log_level' , logging . INFO ) 
log . setLevel ( log_level ) 
func ( * args , ** kwargs ) 
~~ return _wrapper 
~~ return wrapper if not invoked else wrapper ( func ) 
~~ def fit ( self , X , y ) : 
self . _avgs = average_by_label ( X , y , self . reference_label ) 
~~ def transform ( self , X , y = None ) : 
return map_dict_list ( 
X , 
key_func = lambda k , v : self . names [ k . lower ( ) ] , 
if_func = lambda k , v : k . lower ( ) in self . names ) 
~~ def list_dataset_uris ( cls , base_uri , config_path ) : 
storage_account_name = generous_parse_uri ( base_uri ) . netloc 
blobservice = get_blob_service ( storage_account_name , config_path ) 
containers = blobservice . list_containers ( include_metadata = True ) 
uri_list = [ ] 
for c in containers : 
~~~ admin_metadata = c . metadata 
uri = cls . generate_uri ( 
admin_metadata [ 'name' ] , 
admin_metadata [ 'uuid' ] , 
base_uri 
uri_list . append ( uri ) 
~~ return uri_list 
~~ def list_overlay_names ( self ) : 
overlay_names = [ ] 
for blob in self . _blobservice . list_blobs ( 
self . uuid , 
prefix = self . overlays_key_prefix 
~~~ overlay_file = blob . name . rsplit ( '/' , 1 ) [ - 1 ] 
overlay_name , ext = overlay_file . split ( '.' ) 
overlay_names . append ( overlay_name ) 
~~ return overlay_names 
~~ def add_item_metadata ( self , handle , key , value ) : 
identifier = generate_identifier ( handle ) 
metadata_blob_suffix = "{}.{}.json" . format ( identifier , key ) 
metadata_blob_name = self . fragments_key_prefix + metadata_blob_suffix 
self . _blobservice . create_blob_from_text ( 
metadata_blob_name , 
json . dumps ( value ) 
self . _blobservice . set_blob_metadata ( 
container_name = self . uuid , 
blob_name = metadata_blob_name , 
metadata = { 
"type" : "item_metadata" 
~~ def put_text ( self , key , contents ) : 
key , 
contents 
~~ def get_item_abspath ( self , identifier ) : 
admin_metadata = self . get_admin_metadata ( ) 
uuid = admin_metadata [ "uuid" ] 
dataset_cache_abspath = os . path . join ( self . _azure_cache_abspath , uuid ) 
mkdir_parents ( dataset_cache_abspath ) 
metadata = self . _blobservice . get_blob_metadata ( 
identifier 
relpath = metadata [ 'relpath' ] 
_ , ext = os . path . splitext ( relpath ) 
local_item_abspath = os . path . join ( 
dataset_cache_abspath , 
identifier + ext 
if not os . path . isfile ( local_item_abspath ) : 
~~~ tmp_local_item_abspath = local_item_abspath + ".tmp" 
self . _blobservice . get_blob_to_path ( 
identifier , 
tmp_local_item_abspath 
os . rename ( tmp_local_item_abspath , local_item_abspath ) 
~~ return local_item_abspath 
~~ def iter_item_handles ( self ) : 
blob_generator = self . _blobservice . list_blobs ( 
include = 'metadata' 
for blob in blob_generator : 
~~~ if 'type' in blob . metadata : 
~~~ if blob . metadata [ 'type' ] == 'item' : 
~~~ handle = blob . metadata [ 'relpath' ] 
yield handle 
~~ ~~ ~~ ~~ def get_item_metadata ( self , handle ) : 
metadata = { } 
prefix = self . fragments_key_prefix + '{}' . format ( identifier ) 
include = 'metadata' , 
prefix = prefix 
~~~ metadata_key = blob . name . split ( '.' ) [ - 2 ] 
value_as_string = self . get_text ( blob . name ) 
value = json . loads ( value_as_string ) 
metadata [ metadata_key ] = value 
~~ return metadata 
~~ def get_git_version ( ) : 
def _minimal_ext_cmd ( cmd ) : 
~~~ env = { } 
for k in [ 'SYSTEMROOT' , 'PATH' ] : 
~~~ v = os . environ . get ( k ) 
~~~ env [ k ] = v 
~~ ~~ env [ 'LANGUAGE' ] = 'C' 
env [ 'LANG' ] = 'C' 
env [ 'LC_ALL' ] = 'C' 
with open ( os . devnull , 'w' ) as err_out : 
~~~ out = subprocess . Popen ( cmd , 
env = env ) . communicate ( ) [ 0 ] 
~~ return out 
~~~ git_dir = os . path . dirname ( os . path . realpath ( __file__ ) ) 
out = _minimal_ext_cmd ( [ 'git' , '-C' , git_dir , 'rev-parse' , 'HEAD' ] ) 
GIT_REVISION = out . strip ( ) . decode ( 'ascii' ) 
~~~ GIT_REVISION = 'Unknown' 
~~ return GIT_REVISION 
~~ def load_hands ( ) : 
mgr = stevedore . ExtensionManager ( 
namespace = env . plugin_namespace , 
invoke_on_load = True ) 
def register_hand ( ext ) : 
~~~ _hand = ext . obj . register ( ) 
if hasattr ( hand , _hand . __name__ ) : 
~~~ raise HandDuplicationOfNameError ( _hand . __name__ ) 
~~ hand [ _hand . __name__ ] = _hand 
_pkg , _ver = ext . obj . version ( ) 
env . version [ _pkg ] = _ver 
~~~ mgr . map ( register_hand ) 
~~ except stevedore . exception . NoMatches : 
~~ return hand 
~~ def partial_fit ( self , X , y ) : 
X , y = filter_by_label ( X , y , self . reference_label ) 
super ( ) . partial_fit ( X , y ) 
return [ { 
new_feature : self . _fisher_pval ( x , old_features ) 
for new_feature , old_features in self . feature_groups . items ( ) 
if len ( set ( x . keys ( ) ) & set ( old_features ) ) 
} for x in X ] 
~~ def _filtered_values ( self , x : dict , feature_set : list = None ) : 
feature_set = feature_set or x 
n = sum ( self . filter_func ( x [ i ] ) for i in feature_set if i in x ) 
return [ len ( feature_set ) - n , n ] 
~~ def consistency ( self , desired_version = None , include_package = False , 
strictness = None ) : 
keys_to_check = list ( self . versions . keys ( ) ) 
if not include_package and 'package' in keys_to_check : 
~~~ keys_to_check . remove ( 'package' ) 
~~ if desired_version is None : 
~~~ desired_version = self . versions [ 'setup.py' ] 
~~~ desired_version = self . versions [ keys_to_check [ 0 ] ] 
~~ ~~ if strictness is None : 
~~~ strictness = self . strictness 
~~ desired = self . _version ( desired_version , strictness ) 
error_keys = [ ] 
for key in keys_to_check : 
~~~ test = self . _version ( self . versions [ key ] , strictness ) 
if test != desired : 
~~~ error_keys += [ key ] 
~~ ~~ msg = "" 
for key in error_keys : 
d = str ( desired ) , 
v = str ( self . versions [ key ] ) , 
k = str ( key ) 
~~ return msg 
~~ def setup_is_release ( setup , expected = True ) : 
~~~ is_release = setup . IS_RELEASE 
~~~ if is_release and expected : 
~~ elif not is_release and not expected : 
+ str ( is_release ) + ".\\n" ) 
~~ ~~ ~~ def add_to_queue ( self , url ) : 
if self . connection_handler . current_music is None : 
~~ elif self . connection_handler . current_storage is None : 
~~~ self . queues [ 'download' ] . put ( url ) 
~~ ~~ def use_music_service ( self , service_name , api_key = None ) : 
self . connection_handler . use_music_service ( service_name , api_key = api_key ) 
~~ def use_storage_service ( self , service_name , custom_path = None ) : 
self . connection_handler . use_storage_service ( service_name , custom_path = custom_path ) 
~~ def start_workers ( self , workers_per_task = 1 ) : 
if not self . workers : 
~~~ for _ in range ( workers_per_task ) : 
~~~ self . workers . append ( Worker ( self . _download , self . queues [ 'download' ] , self . queues [ 'convert' ] , self . stopper ) ) 
self . workers . append ( Worker ( self . _convert , self . queues [ 'convert' ] , self . queues [ 'upload' ] , self . stopper ) ) 
self . workers . append ( Worker ( self . _upload , self . queues [ 'upload' ] , self . queues [ 'delete' ] , self . stopper ) ) 
self . workers . append ( Worker ( self . _delete , self . queues [ 'delete' ] , self . queues [ 'done' ] , self . stopper ) ) 
~~ self . signal_handler = SignalHandler ( self . workers , self . stopper ) 
signal . signal ( signal . SIGINT , self . signal_handler ) 
for worker in self . workers : 
~~~ worker . start ( ) 
~~ ~~ ~~ def set ( self , k , v ) : 
k = k . lstrip ( '/' ) 
url = '{}/{}' . format ( self . endpoint , k ) 
r = requests . put ( url , data = str ( v ) ) 
if r . status_code != 200 or r . json ( ) is not True : 
~~ ~~ def get ( self , k , wait = False , wait_index = False , timeout = '5m' ) : 
~~~ params [ 'index' ] = wait_index 
params [ 'wait' ] = timeout 
~~ r = requests . get ( url , params = params ) 
if r . status_code == 404 : 
~~~ return base64 . b64decode ( r . json ( ) [ 0 ] [ 'Value' ] ) 
~~ ~~ def recurse ( self , k , wait = False , wait_index = None , timeout = '5m' ) : 
params [ 'recurse' ] = 'true' 
~~~ params [ 'wait' ] = timeout 
if not wait_index : 
~~~ params [ 'index' ] = self . index ( k , recursive = True ) 
~~ ~~ r = requests . get ( url , params = params ) 
~~ entries = { } 
for e in r . json ( ) : 
~~~ if e [ 'Value' ] : 
~~~ entries [ e [ 'Key' ] ] = base64 . b64decode ( e [ 'Value' ] ) 
~~~ entries [ e [ 'Key' ] ] = '' 
~~ ~~ return entries 
~~ def index ( self , k , recursive = False ) : 
if recursive : 
~~~ params [ 'recurse' ] = '' 
return r . headers [ 'X-Consul-Index' ] 
~~ def delete ( self , k , recursive = False ) : 
~~ r = requests . delete ( url , params = params ) 
~~ ~~ def plot_heatmap ( X , y , top_n = 10 , metric = 'correlation' , method = 'complete' ) : 
sns . set ( color_codes = True ) 
df = feature_importance_report ( X , y ) 
df_sns = pd . DataFrame ( ) . from_records ( X ) [ df [ : top_n ] . index ] . T 
df_sns . columns = y 
color_mapping = dict ( zip ( set ( y ) , sns . mpl_palette ( "Set2" , len ( set ( y ) ) ) ) ) 
return sns . clustermap ( df_sns , figsize = ( 22 , 22 ) , z_score = 0 , 
metric = metric , method = method , 
col_colors = [ color_mapping [ i ] for i in y ] ) 
~~ def get_setup_version ( ) : 
ver = '.' . join ( map ( str , VERSION [ : 3 ] ) ) 
if not VERSION [ 3 ] : 
~~~ return ver 
~~ hyphen = '' 
suffix = hyphen . join ( map ( str , VERSION [ - 2 : ] ) ) 
if VERSION [ 3 ] in [ VERSION_SUFFIX_DEV , VERSION_SUFFIX_POST ] : 
~~~ hyphen = '.' 
~~ ver = hyphen . join ( [ ver , suffix ] ) 
return ver 
~~ def get_cli_version ( ) : 
directory = os . path . dirname ( os . path . abspath ( __file__ ) ) 
version_path = os . path . join ( directory , 'VERSION' ) 
if os . path . exists ( version_path ) : 
~~~ with open ( version_path ) as f : 
~~~ ver = f . read ( ) 
~~ return ver 
~~ return get_setup_version ( ) 
inverser_tranformer = self . dict_vectorizer_ 
if self . feature_selection : 
~~~ inverser_tranformer = self . clone_dict_vectorizer_ 
~~ return inverser_tranformer . inverse_transform ( 
self . transformer . transform ( 
self . dict_vectorizer_ . transform ( X ) ) ) 
~~ def use_music_service ( self , service_name , api_key ) : 
~~~ self . current_music = self . music_services [ service_name ] 
~~~ if service_name == 'youtube' : 
~~~ self . music_services [ 'youtube' ] = Youtube ( ) 
self . current_music = self . music_services [ 'youtube' ] 
~~ elif service_name == 'soundcloud' : 
~~~ self . music_services [ 'soundcloud' ] = Soundcloud ( api_key = api_key ) 
self . current_music = self . music_services [ 'soundcloud' ] 
~~ ~~ ~~ def use_storage_service ( self , service_name , custom_path ) : 
~~~ self . current_storage = self . storage_services [ service_name ] 
self . current_storage . connect ( ) 
~~ elif service_name == 'dropbox' : 
~~ elif service_name == 'local' : 
~~~ self . storage_services [ 'local' ] = LocalStorage ( custom_path = custom_path ) 
self . current_storage = self . storage_services [ 'local' ] 
~~ ~~ ~~ def from_csv ( self , label_column = 'labels' ) : 
df = pd . read_csv ( self . path , header = 0 ) 
X = df . loc [ : , df . columns != label_column ] . to_dict ( 'records' ) 
X = map_dict_list ( X , if_func = lambda k , v : v and math . isfinite ( v ) ) 
y = list ( df [ label_column ] . values ) 
return X , y 
~~ def from_json ( self ) : 
with gzip . open ( '%s.gz' % self . path , 
'rt' ) if self . gz else open ( self . path ) as file : 
~~~ return list ( map ( list , zip ( * json . load ( file ) ) ) ) [ : : - 1 ] 
~~ ~~ def to_json ( self , X , y ) : 
with gzip . open ( '%s.gz' % self . path , 'wt' ) if self . gz else open ( 
self . path , 'w' ) as file : 
~~~ json . dump ( list ( zip ( y , X ) ) , file ) 
~~ ~~ def filter_by_label ( X , y , ref_label , reverse = False ) : 
check_reference_label ( y , ref_label ) 
return list ( zip ( * filter ( lambda t : ( not reverse ) == ( t [ 1 ] == ref_label ) , 
zip ( X , y ) ) ) ) 
~~ def average_by_label ( X , y , ref_label ) : 
return defaultdict ( float , 
pd . DataFrame . from_records ( 
filter_by_label ( X , y , ref_label ) [ 0 ] 
) . mean ( ) . to_dict ( ) ) 
~~ def map_dict ( d , key_func = None , value_func = None , if_func = None ) : 
key_func = key_func or ( lambda k , v : k ) 
value_func = value_func or ( lambda k , v : v ) 
if_func = if_func or ( lambda k , v : True ) 
key_func ( * k_v ) : value_func ( * k_v ) 
for k_v in d . items ( ) if if_func ( * k_v ) 
~~ def map_dict_list ( ds , key_func = None , value_func = None , if_func = None ) : 
return [ map_dict ( d , key_func , value_func , if_func ) for d in ds ] 
~~ def check_reference_label ( y , ref_label ) : 
set_y = set ( y ) 
if ref_label not in set_y : 
~~ ~~ def feature_importance_report ( X , 
y , 
threshold = 0.001 , 
correcting_multiple_hypotesis = True , 
method = 'fdr_bh' , 
alpha = 0.1 , 
sort_by = 'pval' ) : 
df = variance_threshold_on_df ( 
pd . DataFrame . from_records ( X ) , threshold = threshold ) 
F , pvals = f_classif ( df . values , y ) 
if correcting_multiple_hypotesis : 
~~~ _ , pvals , _ , _ = multipletests ( pvals , alpha = alpha , method = method ) 
~~ df [ 'labels' ] = y 
df_mean = df . groupby ( 'labels' ) . mean ( ) . T 
df_mean [ 'F' ] = F 
df_mean [ 'pval' ] = pvals 
return df_mean . sort_values ( sort_by , ascending = True ) 
~~ def multi ( dispatch_fn , default = None ) : 
def _inner ( * args , ** kwargs ) : 
~~~ dispatch_value = dispatch_fn ( * args , ** kwargs ) 
f = _inner . __multi__ . get ( dispatch_value , _inner . __multi_default__ ) 
if f is None : 
~~ return f ( * args , ** kwargs ) 
~~ _inner . __multi__ = { } 
_inner . __multi_default__ = default 
_inner . __dispatch_fn__ = dispatch_fn 
return _inner 
~~ def method ( dispatch_fn , dispatch_key = None ) : 
def apply_decorator ( fn ) : 
~~~ if dispatch_key is None : 
~~~ dispatch_fn . __multi_default__ = fn 
~~~ dispatch_fn . __multi__ [ dispatch_key ] = fn 
~~ return fn 
~~ return apply_decorator 
~~ def convert_to_mp3 ( file_name , delete_queue ) : 
file = os . path . splitext ( file_name ) 
if file [ 1 ] == '.mp3' : 
return file_name 
~~ new_file_name = file [ 0 ] + '.mp3' 
ff = FFmpeg ( 
inputs = { file_name : None } , 
outputs = { new_file_name : None } 
start_time = time ( ) 
~~~ ff . run ( stdout = subprocess . DEVNULL , stderr = subprocess . DEVNULL ) 
~~ except FFRuntimeError : 
~~~ os . remove ( new_file_name ) 
ff . run ( stdout = subprocess . DEVNULL , stderr = subprocess . DEVNULL ) 
~~ end_time = time ( ) 
delete_queue . put ( file_name ) 
return new_file_name 
~~ def delete_local_file ( file_name ) : 
~~~ os . remove ( file_name ) 
~~ ~~ def cli ( * args , ** kwargs ) : 
env . update ( kwargs ) 
~~ def _is_package ( path ) : 
def _exists ( s ) : 
~~~ return os . path . exists ( os . path . join ( path , s ) ) 
os . path . isdir ( path ) and 
( _exists ( '__init__.py' ) or _exists ( '__init__.pyc' ) ) 
~~ def find_handfile ( names = None ) : 
names = names or [ env . handfile ] 
if not names [ 0 ] . endswith ( '.py' ) : 
~~~ names += [ names [ 0 ] + '.py' ] 
~~ if os . path . dirname ( names [ 0 ] ) : 
~~~ for name in names : 
~~~ expanded = os . path . expanduser ( name ) 
if os . path . exists ( expanded ) : 
~~~ if name . endswith ( '.py' ) or _is_package ( expanded ) : 
~~~ return os . path . abspath ( expanded ) 
~~~ path = '.' 
while os . path . split ( os . path . abspath ( path ) ) [ 1 ] : 
~~~ joined = os . path . join ( path , name ) 
if os . path . exists ( joined ) : 
~~~ if name . endswith ( '.py' ) or _is_package ( joined ) : 
~~~ return os . path . abspath ( joined ) 
~~ ~~ ~~ path = os . path . join ( '..' , path ) 
~~ def get_commands_from_module ( imported ) : 
imported_vars = vars ( imported ) 
if "__all__" in imported_vars : 
~~~ imported_vars = [ 
( name , imported_vars [ name ] ) for name in 
imported_vars if name in imported_vars [ "__all__" ] ] 
~~~ imported_vars = imported_vars . items ( ) 
~~ cmd_dict = extract_commands ( imported_vars ) 
return imported . __doc__ , cmd_dict 
~~ def extract_commands ( imported_vars ) : 
commands = dict ( ) 
for tup in imported_vars : 
~~~ name , obj = tup 
if is_command_object ( obj ) : 
~~~ commands . setdefault ( name , obj ) 
~~ ~~ return commands 
~~ def load_handfile ( path , importer = None ) : 
if importer is None : 
~~~ importer = __import__ 
~~ directory , handfile = os . path . split ( path ) 
added_to_path = False 
index = None 
if directory not in sys . path : 
~~~ sys . path . insert ( 0 , directory ) 
added_to_path = True 
~~~ i = sys . path . index ( directory ) 
if i != 0 : 
~~~ index = i 
sys . path . insert ( 0 , directory ) 
del sys . path [ i + 1 ] 
~~ ~~ sys_byte_code_bak = sys . dont_write_bytecode 
sys . dont_write_bytecode = True 
imported = importer ( os . path . splitext ( handfile ) [ 0 ] ) 
sys . dont_write_bytecode = sys_byte_code_bak 
if added_to_path : 
~~~ del sys . path [ 0 ] 
~~ if index is not None : 
~~~ sys . path . insert ( index + 1 , directory ) 
del sys . path [ 0 ] 
~~ docstring , commands = get_commands_from_module ( imported ) 
return docstring , commands 
~~ def reasonable_desired_version ( self , desired_version , allow_equal = False , 
allow_patch_skip = False ) : 
~~~ desired_version = desired_version . base_version 
~~ ( new_major , new_minor , new_patch ) = map ( int , desired_version . split ( '.' ) ) 
tag_versions = self . _versions_from_tags ( ) 
if not tag_versions : 
~~ max_version = max ( self . _versions_from_tags ( ) ) . base_version 
( old_major , old_minor , old_patch ) = map ( int , str ( max_version ) . split ( '.' ) ) 
v_desired = vers . Version ( desired_version ) 
v_max = vers . Version ( max_version ) 
if allow_equal and v_desired == v_max : 
~~ if v_desired < v_max : 
+ update_str + "\\n" ) 
~~ bad_update = skipped_version ( ( old_major , old_minor , old_patch ) , 
( new_major , new_minor , new_patch ) , 
allow_patch_skip ) 
msg = "" 
if bad_update : 
+ update_str + "?\\n" ) 
~~ def parse_accept ( header_value ) : 
next_explicit_q = decimal . ExtendedContext . next_plus ( decimal . Decimal ( '5.0' ) ) 
headers = [ parse_content_type ( header ) 
for header in parse_list ( header_value ) ] 
~~~ q = header . parameters . pop ( 'q' , None ) 
if q is None : 
~~~ q = '1.0' 
~~ elif float ( q ) == 1.0 : 
~~~ q = float ( next_explicit_q ) 
next_explicit_q = next_explicit_q . next_minus ( ) 
~~ header . quality = float ( q ) 
~~ def ordering ( left , right ) : 
if left . quality != right . quality : 
~~~ return right . quality - left . quality 
~~ if left == right : 
~~ if left > right : 
~~ return 1 
~~ return sorted ( headers , key = functools . cmp_to_key ( ordering ) ) 
~~ def parse_cache_control ( header_value ) : 
directives = { } 
for segment in parse_list ( header_value ) : 
~~~ name , sep , value = segment . partition ( '=' ) 
if sep != '=' : 
~~~ directives [ name ] = None 
~~ elif sep and value : 
~~~ value = _dequote ( value . strip ( ) ) 
~~~ directives [ name ] = int ( value ) 
~~~ directives [ name ] = value 
~~ ~~ ~~ for name in _CACHE_CONTROL_BOOL_DIRECTIVES : 
~~~ if directives . get ( name , '' ) is None : 
~~~ directives [ name ] = True 
~~ ~~ return directives 
~~ def parse_content_type ( content_type , normalize_parameter_values = True ) : 
parts = _remove_comments ( content_type ) . split ( ';' ) 
content_type , content_subtype = parts . pop ( 0 ) . split ( '/' ) 
if '+' in content_subtype : 
~~~ content_subtype , content_suffix = content_subtype . split ( '+' ) 
~~~ content_suffix = None 
~~ parameters = _parse_parameter_list ( 
parts , normalize_parameter_values = normalize_parameter_values ) 
return datastructures . ContentType ( content_type , content_subtype , 
dict ( parameters ) , 
content_suffix ) 
~~ def parse_forwarded ( header_value , only_standard_parameters = False ) : 
for entry in parse_list ( header_value ) : 
~~~ param_tuples = _parse_parameter_list ( entry . split ( ';' ) , 
normalize_parameter_names = True , 
normalize_parameter_values = False ) 
if only_standard_parameters : 
~~~ for name , _ in param_tuples : 
~~~ if name not in ( 'for' , 'proto' , 'by' , 'host' ) : 
~~~ raise errors . StrictHeaderParsingFailure ( 'Forwarded' , 
header_value ) 
~~ ~~ ~~ result . append ( dict ( param_tuples ) ) 
~~ def parse_link ( header_value , strict = True ) : 
sanitized = _remove_comments ( header_value ) 
links = [ ] 
def parse_links ( buf ) : 
quoted = re . findall ( \ , buf ) 
for segment in quoted : 
~~~ left , match , right = buf . partition ( segment ) 
match = match . replace ( ',' , '\\000' ) 
match = match . replace ( ';' , '\\001' ) 
buf = '' . join ( [ left , match , right ] ) 
~~ while buf : 
~~~ matched = re . match ( r'<(?P<link>[^>]*)>\\s*(?P<params>.*)' , buf ) 
if matched : 
~~~ groups = matched . groupdict ( ) 
params , _ , buf = groups [ 'params' ] . partition ( ',' ) 
if params and not params . startswith ( ';' ) : 
~~~ raise errors . MalformedLinkValue ( 
~~ yield ( groups [ 'link' ] . strip ( ) , 
[ p . replace ( '\\001' , ';' ) . strip ( ) 
for p in params [ 1 : ] . split ( ';' ) if p ] ) 
buf = buf . strip ( ) 
~~ ~~ ~~ for target , param_list in parse_links ( sanitized ) : 
~~~ parser = _helpers . ParameterParser ( strict = strict ) 
for name , value in _parse_parameter_list ( param_list ) : 
~~~ parser . add_value ( name , value ) 
~~ links . append ( datastructures . LinkHeader ( target = target , 
parameters = parser . values ) ) 
~~ return links 
~~ def parse_list ( value ) : 
segments = _QUOTED_SEGMENT_RE . findall ( value ) 
for segment in segments : 
~~~ left , match , right = value . partition ( segment ) 
value = '' . join ( [ left , match . replace ( ',' , '\\000' ) , right ] ) 
~~ return [ _dequote ( x . strip ( ) ) . replace ( '\\000' , ',' ) 
for x in value . split ( ',' ) ] 
~~ def _parse_parameter_list ( parameter_list , 
normalized_parameter_values = _DEF_PARAM_VALUE , 
normalize_parameter_names = False , 
normalize_parameter_values = True ) : 
DeprecationWarning ) 
normalize_parameter_values = normalized_parameter_values 
~~ parameters = [ ] 
for param in parameter_list : 
~~~ param = param . strip ( ) 
if param : 
~~~ name , value = param . split ( '=' ) 
if normalize_parameter_names : 
~~~ name = name . lower ( ) 
~~ if normalize_parameter_values : 
~~ parameters . append ( ( name , _dequote ( value . strip ( ) ) ) ) 
~~ ~~ return parameters 
~~ def _parse_qualified_list ( value ) : 
found_wildcard = False 
values , rejected_values = [ ] , [ ] 
parsed = parse_list ( value ) 
default = float ( len ( parsed ) + 1 ) 
highest = default + 1.0 
for raw_str in parsed : 
if charset == '*' : 
~~~ found_wildcard = True 
~~ params = dict ( _parse_parameter_list ( parameter_str . split ( ';' ) ) ) 
quality = float ( params . pop ( 'q' , default ) ) 
if quality < 0.001 : 
~~~ rejected_values . append ( charset ) 
~~ elif quality == 1.0 : 
~~~ values . append ( ( highest + default , charset ) ) 
~~~ values . append ( ( quality , charset ) ) 
~~ default -= 1.0 
~~ parsed = [ value [ 1 ] for value in sorted ( values , reverse = True ) ] 
if found_wildcard : 
~~~ parsed . append ( '*' ) 
~~ parsed . extend ( rejected_values ) 
return parsed 
~~ def parse_link_header ( header_value , strict = True ) : 
warnings . warn ( "deprecated" , DeprecationWarning ) 
return parse_link ( header_value , strict ) 
~~ def add_value ( self , name , value ) : 
~~~ if self . _rfc_values [ name ] is None : 
~~~ self . _rfc_values [ name ] = value 
~~ elif self . strict : 
~~~ if name in ( 'media' , 'type' ) : 
~~ if self . strict and name in ( 'title' , 'title*' ) : 
~~ self . _values . append ( ( name , value ) ) 
~~ def values ( self ) : 
values = self . _values [ : ] 
if self . strict : 
~~~ if self . _rfc_values [ 'title*' ] : 
~~~ values . append ( ( 'title*' , self . _rfc_values [ 'title*' ] ) ) 
if self . _rfc_values [ 'title' ] : 
~~~ values . append ( ( 'title' , self . _rfc_values [ 'title*' ] ) ) 
~~ ~~ elif self . _rfc_values [ 'title' ] : 
~~~ values . append ( ( 'title' , self . _rfc_values [ 'title' ] ) ) 
~~ ~~ return values 
~~ def download ( self , url ) : 
~~~ yt = YouTube ( url ) 
~~ except RegexMatchError : 
~~~ stream = yt . streams . first ( ) 
stream . download ( ) 
end_time = time ( ) 
return stream . default_filename 
~~ ~~ def download ( self , url ) : 
~~~ track = self . client . get ( '/resolve' , url = url ) 
~~ r = requests . get ( self . client . get ( track . stream_url , allow_redirects = False ) . location , stream = True ) 
total_size = int ( r . headers [ 'content-length' ] ) 
chunk_size = 1000000 
file_name = track . title + '.mp3' 
with open ( file_name , 'wb' ) as f : 
~~~ for data in tqdm ( r . iter_content ( chunk_size ) , desc = track . title , total = total_size / chunk_size , unit = 'MB' , file = sys . stdout ) : 
~~~ f . write ( data ) 
SCOPES = 'https://www.googleapis.com/auth/drive' 
store = file . Storage ( 'drive_credentials.json' ) 
creds = store . get ( ) 
if not creds or creds . invalid : 
~~~ flow = client . flow_from_clientsecrets ( 'client_secret.json' , SCOPES ) 
~~ except InvalidClientSecretsError : 
~~ creds = tools . run_flow ( flow , store ) 
~~ self . connection = build ( 'drive' , 'v3' , http = creds . authorize ( Http ( ) ) ) 
response = self . connection . files ( ) . list ( q = "name=\ ) . execute ( ) 
~~~ folder_id = response . get ( 'files' , [ ] ) [ 0 ] [ 'id' ] 
folder_metadata = { 'name' : 'Music' , 'mimeType' : 'application/vnd.google-apps.folder' } 
folder = self . connection . files ( ) . create ( body = folder_metadata , fields = 'id' ) . execute ( ) 
~~ ~~ def upload ( self , file_name ) : 
folder_id = response . get ( 'files' , [ ] ) [ 0 ] [ 'id' ] 
file_metadata = { 'name' : file_name , 'parents' : [ folder_id ] } 
media = MediaFileUpload ( file_name , mimetype = 'audio/mpeg' ) 
self . connection . files ( ) . create ( body = file_metadata , media_body = media , fields = 'id' ) . execute ( ) 
if self . music_folder is None : 
~~~ music_folder = os . path . join ( os . path . expanduser ( '~' ) , 'Music' ) 
if not os . path . exists ( music_folder ) : 
~~~ os . makedirs ( music_folder ) 
~~ self . music_folder = music_folder 
os . rename ( file_name , os . path . join ( self . music_folder , file_name ) ) 
~~ def pfreduce ( func , iterable , initial = None ) : 
~~~ first_item = next ( iterator ) 
if initial : 
~~~ value = func ( initial , first_item ) 
~~~ value = first_item 
~~~ return initial 
~~ for item in iterator : 
~~~ value = func ( value , item ) 
~~ def pfcollect ( iterable , n = None ) : 
if n : 
~~~ return list ( itertools . islice ( iterable , n ) ) 
~~~ return list ( iterable ) 
~~ ~~ def pfprint ( item , end = '\\n' , file = None ) : 
if file is None : 
~~~ file = sys . stdout 
~~ print ( item , end = end , file = file ) 
~~ def pfprint_all ( iterable , end = '\\n' , file = None ) : 
for item in iterable : 
~~~ pfprint ( item , end = end , file = file ) 
~~ ~~ def __sig_from_func ( self , func ) : 
if isinstance ( func , types . MethodType ) : 
~~~ argspec = getfullargspec ( func . __func__ ) 
self . pargl = argspec [ 0 ] [ 1 : ] 
~~~ argspec = getfullargspec ( func ) 
self . pargl = argspec [ 0 ] [ : ] 
~~ if argspec [ 3 ] is not None : 
~~~ def_offset = len ( self . pargl ) - len ( argspec [ 3 ] ) 
self . def_argv = dict ( ( self . pargl [ def_offset + i ] , argspec [ 3 ] [ i ] ) for i in range ( len ( argspec [ 3 ] ) ) ) 
~~~ self . def_argv = { } 
~~ self . var_pargs = argspec [ 1 ] is not None 
self . var_kargs = argspec [ 2 ] is not None 
self . kargl = argspec [ 4 ] 
if argspec [ 5 ] is not None : 
~~~ self . def_argv . update ( argspec [ 5 ] ) 
~~ ~~ def __sig_from_partial ( self , inst ) : 
self . pargl = list ( inst . pargl ) 
self . kargl = list ( inst . kargl ) 
self . def_argv = inst . def_argv . copy ( ) 
self . var_pargs = inst . var_pargs 
self . var_kargs = inst . var_kargs 
~~ def make_copy ( klass , inst , func = None , argv = None , extra_argv = None , copy_sig = True ) : 
dest = klass ( func or inst . func ) 
dest . argv = ( argv or inst . argv ) . copy ( ) 
dest . extra_argv = list ( extra_argv if extra_argv else inst . extra_argv ) 
if copy_sig : 
~~~ dest . __sig_from_partial ( inst ) 
~~ return dest 
~~ def __new_argv ( self , * new_pargs , ** new_kargs ) : 
new_argv = self . argv . copy ( ) 
new_extra_argv = list ( self . extra_argv ) 
for v in new_pargs : 
~~~ arg_name = None 
for name in self . pargl : 
~~~ if not name in new_argv : 
~~~ arg_name = name 
~~ ~~ if arg_name : 
~~~ new_argv [ arg_name ] = v 
~~ elif self . var_pargs : 
~~~ new_extra_argv . append ( v ) 
~~~ num_prev_pargs = len ( [ name for name in self . pargl if name in self . argv ] ) 
len ( self . pargl ) , 
num_prev_pargs + len ( new_pargs ) ) ) 
~~ ~~ for k , v in new_kargs . items ( ) : 
~~~ if not ( self . var_kargs or ( k in self . pargl ) or ( k in self . kargl ) ) : 
~~ new_argv [ k ] = v 
~~ return ( new_argv , new_extra_argv ) 
~~ def ignore_certain_metainf_files ( filename ) : 
ignore = ( "META-INF/manifest.mf" , 
"META-INF/*.sf" , 
"META-INF/*.rsa" , 
"META-INF/*.dsa" , 
"META-INF/ids.json" ) 
for glob in ignore : 
~~~ if fnmatch . fnmatchcase ( filename . upper ( ) , glob . upper ( ) ) : 
~~ def file_key ( filename ) : 
prio = 4 
if filename == 'install.rdf' : 
~~~ prio = 1 
~~ elif filename in [ "chrome.manifest" , "icon.png" , "icon64.png" ] : 
~~~ prio = 2 
~~ elif filename in [ "MPL" , "GPL" , "LGPL" , "COPYING" , 
"LICENSE" , "license.txt" ] : 
~~~ prio = 5 
~~ return ( prio , os . path . split ( filename . lower ( ) ) ) 
~~ def genesis_signing_lockset ( genesis , privkey ) : 
v = VoteBlock ( 0 , 0 , genesis . hash ) 
v . sign ( privkey ) 
ls = LockSet ( num_eligible_votes = 1 ) 
ls . add ( v ) 
assert ls . has_quorum 
return ls 
~~ def sign ( self , privkey ) : 
if self . v : 
~~ if privkey in ( 0 , '' , '\\x00' * 32 ) : 
~~ rawhash = sha3 ( rlp . encode ( self , self . __class__ . exclude ( [ 'v' , 'r' , 's' ] ) ) ) 
if len ( privkey ) == 64 : 
~~~ privkey = encode_privkey ( privkey , 'bin' ) 
~~ pk = PrivateKey ( privkey , raw = True ) 
signature = pk . ecdsa_recoverable_serialize ( pk . ecdsa_sign_recoverable ( rawhash , raw = True ) ) 
signature = signature [ 0 ] + chr ( signature [ 1 ] ) 
self . v = ord ( signature [ 64 ] ) + 27 
self . r = big_endian_to_int ( signature [ 0 : 32 ] ) 
self . s = big_endian_to_int ( signature [ 32 : 64 ] ) 
self . _sender = None 
~~ def hash ( self ) : 
if self . sender is None : 
~~~ raise MissingSignatureError ( ) 
~~ class HashSerializable ( rlp . Serializable ) : 
~~~ fields = [ ( field , sedes ) for field , sedes in self . fields 
if field not in ( 'v' , 'r' , 's' ) ] + [ ( '_sender' , binary ) ] 
_sedes = None 
~~ return sha3 ( rlp . encode ( self , HashSerializable ) ) 
~~ def hr ( self ) : 
h = set ( [ ( v . height , v . round ) for v in self . votes ] ) 
assert len ( h ) == 1 , len ( h ) 
return h . pop ( ) 
~~ def has_quorum ( self ) : 
assert self . is_valid 
bhs = self . blockhashes ( ) 
if bhs and bhs [ 0 ] [ 1 ] > 2 / 3. * self . num_eligible_votes : 
~~~ return bhs [ 0 ] [ 0 ] 
~~ ~~ def has_noquorum ( self ) : 
if not bhs or bhs [ 0 ] [ 1 ] <= 1 / 3. * self . num_eligible_votes : 
~~~ assert not self . has_quorum_possible 
if not self . is_valid : 
~~ test = ( self . has_quorum , self . has_quorum_possible , self . has_noquorum ) 
assert 1 == len ( [ x for x in test if x is not None ] ) 
~~ def to_block ( self , env , parent = None ) : 
return Block ( self . header , self . transaction_list , self . uncles , env = env , parent = parent ) 
~~ def validate_votes ( self , validators_H , validators_prevH ) : 
assert self . sender 
def check ( lockset , validators ) : 
~~~ if not lockset . num_eligible_votes == len ( validators ) : 
~~ for v in lockset : 
~~~ if v . sender not in validators : 
~~ ~~ ~~ if self . round_lockset : 
~~~ check ( self . round_lockset , validators_H ) 
~~ check ( self . signing_lockset , validators_prevH ) 
~~ def validate_votes ( self , validators_H ) : 
if not self . round_lockset . num_eligible_votes == len ( validators_H ) : 
~~ for v in self . round_lockset : 
~~~ if v . sender not in validators_H : 
~~ ~~ ~~ def transfer ( ctx , _to = 'address' , _value = 'uint256' , returns = STATUS ) : 
if ctx . accounts [ ctx . msg_sender ] >= _value : 
~~~ ctx . accounts [ ctx . msg_sender ] -= _value 
ctx . accounts [ _to ] += _value 
ctx . Transfer ( ctx . msg_sender , _to , _value ) 
return OK 
~~~ return INSUFFICIENTFUNDS 
~~ ~~ def transferFrom ( ctx , _from = 'address' , _to = 'address' , _value = 'uint256' , returns = STATUS ) : 
auth = ctx . allowances [ _from ] [ ctx . msg_sender ] 
if ctx . accounts [ _from ] >= _value and auth >= _value : 
~~~ ctx . allowances [ _from ] [ ctx . msg_sender ] -= _value 
ctx . accounts [ _from ] -= _value 
ctx . Transfer ( _from , _to , _value ) 
~~ ~~ def approve ( ctx , _spender = 'address' , _value = 'uint256' , returns = STATUS ) : 
ctx . allowances [ ctx . msg_sender ] [ _spender ] += _value 
ctx . Approval ( ctx . msg_sender , _spender , _value ) 
~~ def issue_funds ( ctx , amount = 'uint256' , rtgs_hash = 'bytes32' , returns = STATUS ) : 
ctx . accounts [ ctx . msg_sender ] += amount 
ctx . issued_amounts [ ctx . msg_sender ] += amount 
ctx . Issuance ( ctx . msg_sender , rtgs_hash , amount ) 
~~ def last_lock ( self ) : 
rs = list ( self . rounds ) 
~~~ if self . rounds [ r ] . lock is not None : 
~~~ return self . rounds [ r ] . lock 
~~ ~~ ~~ def last_voted_blockproposal ( self ) : 
for r in self . rounds : 
~~~ if isinstance ( self . rounds [ r ] . proposal , BlockProposal ) : 
~~~ assert isinstance ( self . rounds [ r ] . lock , Vote ) 
if self . rounds [ r ] . proposal . blockhash == self . rounds [ r ] . lock . blockhash : 
~~~ return self . rounds [ r ] . proposal 
~~ ~~ ~~ ~~ def last_valid_lockset ( self ) : 
~~~ ls = self . rounds [ r ] . lockset 
if ls . is_valid : 
~~~ return ls 
~~ def get_timeout ( self ) : 
if self . timeout_time is not None or self . proposal : 
~~ now = self . cm . chainservice . now 
round_timeout = ConsensusManager . round_timeout 
round_timeout_factor = ConsensusManager . round_timeout_factor 
delay = round_timeout * round_timeout_factor ** self . round 
self . timeout_time = now + delay 
return delay 
~~ def request ( self ) : 
missing = self . missing 
self . cm . log ( 'sync.request' , missing = len ( missing ) , requested = len ( self . requested ) , 
received = len ( self . received ) ) 
if self . requested : 
~~ if len ( self . received ) + self . max_getproposals_count >= self . max_queued : 
~~ if not missing : 
~~~ self . cm . log ( 'insync' ) 
~~ self . cm . log ( 'collecting' ) 
blocknumbers = [ ] 
for h in missing : 
~~~ if h not in self . received and h not in self . requested : 
~~~ blocknumbers . append ( h ) 
self . requested . add ( h ) 
if len ( blocknumbers ) == self . max_getproposals_count : 
~~ ~~ ~~ self . cm . log ( 'collected' , num = len ( blocknumbers ) ) 
if not blocknumbers : 
~~ self . cm . log ( 'requesting' , num = len ( blocknumbers ) , 
requesting_range = ( blocknumbers [ 0 ] , blocknumbers [ - 1 ] ) ) 
self . last_active_protocol . send_getblockproposals ( * blocknumbers ) 
self . cm . chainservice . setup_alarm ( self . timeout , self . on_alarm , blocknumbers ) 
~~ def on_proposal ( self , proposal , proto ) : 
assert isinstance ( proto , HDCProtocol ) 
assert isinstance ( proposal , Proposal ) 
if proposal . height >= self . cm . height : 
~~~ assert proposal . lockset . is_valid 
self . last_active_protocol = proto 
~~ ~~ def wait_next_block_factory ( app , timeout = None ) : 
chain = app . services . chain 
new_block_evt = gevent . event . Event ( ) 
def _on_new_block ( app ) : 
new_block_evt . set ( ) 
~~ chain . on_new_head_cbs . append ( _on_new_block ) 
def wait_next_block ( ) : 
~~~ bn = chain . chain . head . number 
new_block_evt . wait ( timeout ) 
new_block_evt . clear ( ) 
if chain . chain . head . number > bn : 
~~ elif chain . chain . head . number == bn : 
~~ ~~ return wait_next_block 
~~ def mk_privkeys ( num ) : 
privkeys = [ ] 
assert num <= num_colors 
for i in range ( num ) : 
~~~ j = 0 
~~~ k = sha3 ( str ( j ) ) 
a = privtoaddr ( k ) 
an = big_endian_to_int ( a ) 
if an % num_colors == i : 
~~ j += 1 
~~ privkeys . append ( k ) 
~~ return privkeys 
~~ def delay ( self , sender , receiver , packet , add_delay = 0 ) : 
bw = min ( sender . ul_bandwidth , receiver . dl_bandwidth ) 
delay = sender . base_latency + receiver . base_latency 
delay += len ( packet ) / bw 
delay += add_delay 
~~ def deliver ( self , sender , receiver , packet ) : 
to = ConsensusManager . round_timeout 
assert to > 0 
super ( SlowTransport , self ) . deliver ( sender , receiver , packet , add_delay = to ) 
~~ def abi_encode_args ( method , args ) : 
assert issubclass ( method . im_class , NativeABIContract ) , method . im_class 
m_abi = method . im_class . _get_method_abi ( method ) 
return zpad ( encode_int ( m_abi [ 'id' ] ) , 4 ) + abi . encode_abi ( m_abi [ 'arg_types' ] , args ) 
~~ def chain_nac_proxy ( chain , sender , contract_address , value = 0 ) : 
klass = registry [ contract_address ] . im_self 
assert issubclass ( klass , NativeABIContract ) 
def mk_method ( method ) : 
~~~ def m ( s , * args ) : 
~~~ data = abi_encode_args ( method , args ) 
block = chain . head_candidate 
output = test_call ( block , sender , contract_address , data ) 
if output is not None : 
~~~ return abi_decode_return_vals ( method , output ) 
~~ ~~ return m 
~~ class cproxy ( object ) : 
~~ for m in klass . _abi_methods ( ) : 
~~~ setattr ( cproxy , m . __func__ . func_name , mk_method ( m ) ) 
~~ return cproxy ( ) 
~~ def address_to_native_contract_class ( self , address ) : 
assert isinstance ( address , bytes ) and len ( address ) == 20 
assert self . is_instance_address ( address ) 
nca = self . native_contract_address_prefix + address [ - 4 : ] 
return self . native_contracts [ nca ] 
~~ def register ( self , contract ) : 
assert issubclass ( contract , NativeContractBase ) 
assert len ( contract . address ) == 20 
assert contract . address . startswith ( self . native_contract_address_prefix ) 
if self . native_contracts . get ( contract . address ) == contract . _on_msg : 
self . native_contracts [ contract . address ] = contract . _on_msg 
~~ def validators_from_config ( validators ) : 
for validator in validators : 
~~~ if len ( validator ) == 40 : 
~~~ validator = validator . decode ( 'hex' ) 
~~ result . append ( validator ) 
~~ def update ( self , data ) : 
if data not in self . filter : 
~~~ self . filter . append ( data ) 
if len ( self . filter ) > self . max_items : 
~~~ self . filter . pop ( 0 ) 
~~~ self . filter . append ( self . filter . pop ( 0 ) ) 
~~ ~~ def add_transaction ( self , tx , origin = None , force_broadcast = False ) : 
self . consensus_manager . log ( 
'add_transaction' , blk = self . chain . head_candidate , lock = self . proposal_lock ) 
log . debug ( 'add_transaction' , lock = self . proposal_lock ) 
block = self . proposal_lock . block 
self . proposal_lock . acquire ( ) 
assert not hasattr ( self . chain . head_candidate , 'should_be_locked' ) 
success = super ( ChainService , self ) . add_transaction ( tx , origin , force_broadcast ) 
~~~ self . proposal_lock . release ( if_block = block ) 
~~ def on_receive_transactions ( self , proto , transactions ) : 
log . debug ( '----------------------------------' ) 
log . debug ( 'remote_transactions_received' , count = len ( transactions ) , remote_id = proto ) 
def _add_txs ( ) : 
~~~ for tx in transactions : 
~~~ self . add_transaction ( tx , origin = proto ) 
~~ ~~ gevent . spawn ( _add_txs ) 
~~ def img_from_vgg ( x ) : 
x = x . transpose ( ( 1 , 2 , 0 ) ) 
x [ : , : , 0 ] += 103.939 
x [ : , : , 1 ] += 116.779 
x [ : , : , 2 ] += 123.68 
return x 
~~ def img_to_vgg ( x ) : 
x [ : , : , 0 ] -= 103.939 
x [ : , : , 1 ] -= 116.779 
x [ : , : , 2 ] -= 123.68 
x = x . transpose ( ( 2 , 0 , 1 ) ) 
~~ def get_f_layer ( self , layer_name ) : 
inputs = [ self . net_input ] 
if self . learning_phase is not None : 
~~~ inputs . append ( K . learning_phase ( ) ) 
~~ return K . function ( inputs , [ self . get_layer_output ( layer_name ) ] ) 
~~ def get_layer_output ( self , name ) : 
if not name in self . _f_layer_outputs : 
~~~ layer = self . net . get_layer ( name ) 
self . _f_layer_outputs [ name ] = layer . output 
~~ return self . _f_layer_outputs [ name ] 
~~ def get_features ( self , x , layers ) : 
if not layers : 
~~ inputs = [ self . net . input ] 
~~~ inputs . append ( self . learning_phase ) 
~~ f = K . function ( inputs , [ self . get_layer_output ( layer_name ) for layer_name in layers ] ) 
feature_outputs = f ( [ x ] ) 
features = dict ( zip ( layers , feature_outputs ) ) 
return features 
~~ def start ( cls , now , number , ** options ) : 
return ( cls . mask ( now , ** options ) - 
timedelta ( ** { cls . __name__ . lower ( ) : number - 1 } ) ) 
~~ def filter ( cls , datetimes , number , now = None , ** options ) : 
if not isinstance ( number , int ) or number < 0 : 
~~ datetimes = tuple ( datetimes ) 
tzinfo = None 
if datetimes and datetimes [ 0 ] . tzinfo is not None : 
~~~ tzinfo = UTC ( ) 
~~ if now is None : 
~~~ now = datetime . now ( tzinfo ) 
~~ if not hasattr ( now , 'second' ) : 
~~~ now = datetime . combine ( now , time ( 23 , 59 , 59 , 999999 , tzinfo = tzinfo ) ) 
~~ future = set ( dt for dt in datetimes if dt > now ) 
if number == 0 : 
~~~ return future 
~~ start = cls . start ( now , number , ** options ) 
valid = ( dt for dt in datetimes if start <= dt <= now ) 
kept = { } 
for dt in sorted ( valid ) : 
~~~ kept . setdefault ( cls . mask ( dt , ** options ) , dt ) 
~~ return set ( kept . values ( ) ) | future 
~~ def mask ( cls , dt , ** options ) : 
return dt . replace ( hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) 
~~ def start ( cls , now , number , firstweekday = calendar . SATURDAY , ** options ) : 
week = cls . mask ( now , firstweekday = firstweekday , ** options ) 
days = ( number - 1 ) * cls . DAYS_IN_WEEK 
return week - timedelta ( days = days ) 
~~ def mask ( cls , dt , firstweekday = calendar . SATURDAY , ** options ) : 
correction = ( dt . weekday ( ) - firstweekday ) % cls . DAYS_IN_WEEK 
week = dt - timedelta ( days = correction ) 
return week . replace ( hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) 
year = now . year 
month = now . month - number + 1 
if month < 0 : 
~~~ year = year + ( month // cls . MONTHS_IN_YEAR ) 
month = month % cls . MONTHS_IN_YEAR 
~~ if month == 0 : 
~~~ year = year - 1 
month = 12 
~~ return cls . mask ( now , ** options ) . replace ( year = year , month = month ) 
return cls . mask ( now ) . replace ( year = ( now . year - number + 1 ) ) 
~~ def to_keep ( datetimes , 
years = 0 , months = 0 , weeks = 0 , days = 0 , 
hours = 0 , minutes = 0 , seconds = 0 , 
firstweekday = SATURDAY , now = None ) : 
datetimes = set ( datetimes ) 
return ( filters . Years . filter ( datetimes , number = years , now = now ) | 
filters . Months . filter ( datetimes , number = months , now = now ) | 
filters . Weeks . filter ( datetimes , number = weeks , 
firstweekday = firstweekday , now = now ) | 
filters . Days . filter ( datetimes , number = days , now = now ) | 
filters . Hours . filter ( datetimes , number = hours , now = now ) | 
filters . Minutes . filter ( datetimes , number = minutes , now = now ) | 
filters . Seconds . filter ( datetimes , number = seconds , now = now ) ) 
~~ def to_delete ( datetimes , 
return datetimes - to_keep ( datetimes , 
years = years , months = months , 
weeks = weeks , days = days , 
hours = hours , minutes = minutes , seconds = seconds , 
firstweekday = firstweekday , now = now ) 
~~ def dates_to_keep ( dates , 
years = 0 , months = 0 , weeks = 0 , days = 0 , firstweekday = SATURDAY , 
now = None ) : 
datetimes = to_keep ( ( datetime . combine ( d , time ( ) ) for d in dates ) , 
years = years , months = months , weeks = weeks , days = days , 
return set ( dt . date ( ) for dt in datetimes ) 
~~ def dates_to_delete ( dates , 
dates = set ( dates ) 
return dates - dates_to_keep ( dates , 
~~ def permission_required ( perm , login_url = None , raise_exception = False ) : 
def check_perms ( user ) : 
~~~ if not getattr ( settings , 'DASHBOARD_REQUIRE_LOGIN' , 
app_settings . REQUIRE_LOGIN ) : 
~~ if user . has_perm ( perm ) : 
~~~ raise PermissionDenied 
~~ return user_passes_test ( check_perms , login_url = login_url ) 
~~ def get_context_data ( self , ** kwargs ) : 
ctx = super ( RenderWidgetMixin , self ) . get_context_data ( ** kwargs ) 
ctx . update ( { 
'is_rendered' : True , 
'widget' : self . widget , 
ctx . update ( self . widget . get_context_data ( ) ) 
return ctx 
~~ def get_widgets_sorted ( self ) : 
for widget_name , widget in self . get_widgets ( ) . items ( ) : 
~~~ result . append ( ( widget_name , widget , widget . position ) ) 
~~ result . sort ( key = lambda x : x [ 2 ] ) 
~~ def get_widgets_that_need_update ( self ) : 
~~~ if widget . should_update ( ) : 
~~~ result . append ( widget ) 
~~ def register_widget ( self , widget_cls , ** widget_kwargs ) : 
if not issubclass ( widget_cls , DashboardWidgetBase ) : 
~~ widget = widget_cls ( ** widget_kwargs ) 
widget_name = widget . get_name ( ) 
if widget_name in self . widgets : 
~~~ raise WidgetAlreadyRegistered ( 
'registered.' . format ( widget_cls , widget_name ) ) 
~~ self . widgets [ widget_name ] = widget 
~~ def unregister_widget ( self , widget_cls ) : 
if widget_cls . __name__ in self . widgets : 
~~~ del self . widgets [ widget_cls ( ) . get_name ( ) ] 
~~ ~~ def get_last_update ( self ) : 
instance , created = models . DashboardWidgetLastUpdate . objects . get_or_create ( 
widget_name = self . get_name ( ) ) 
~~ def get_setting ( self , setting_name , default = None ) : 
~~~ setting = models . DashboardWidgetSettings . objects . get ( 
widget_name = self . get_name ( ) , 
setting_name = setting_name ) 
~~ except models . DashboardWidgetSettings . DoesNotExist : 
~~~ setting = default 
~~ return setting 
~~ def save_setting ( self , setting_name , value ) : 
setting = self . get_setting ( setting_name ) 
if setting is None : 
~~~ setting = models . DashboardWidgetSettings . objects . create ( 
setting_name = setting_name , 
value = value ) 
~~ setting . value = value 
setting . save ( ) 
return setting 
~~ def should_update ( self ) : 
last_update = self . get_last_update ( ) 
time_since = now ( ) - last_update . last_update 
if time_since . seconds < self . update_interval : 
~~ def getCityDetails ( self , ** kwargs ) : 
available_keys = [ "q" , "lat" , "lon" , "city_ids" , "count" ] 
for key in available_keys : 
~~~ if key in kwargs : 
~~~ params [ key ] = kwargs [ key ] 
~~ ~~ cities = self . api . get ( "/cities" , params ) 
return cities 
~~ def getCollectionsViaCityId ( self , city_id , ** kwargs ) : 
params = { "city_id" : city_id } 
optional_params = [ "lat" , "lon" , "count" ] 
for key in optional_params : 
~~ ~~ collections = self . api . get ( "/collections" , params ) 
return collections 
~~ def getEstablishments ( self , city_id , ** kwargs ) : 
optional_params = [ "lat" , "lon" ] 
~~ ~~ establishments = self . api . get ( "/establishments" , params ) 
return establishments 
~~ def getByGeocode ( self , lat , lon ) : 
params = { "lat" : lat , "lon" : lon } 
response = self . api . get ( "/geocode" , params ) 
~~ def getLocationDetails ( self , entity_id , entity_type ) : 
params = { "entity_id" : entity_id , "entity_type" : entity_type } 
location_details = self . api . get ( "/location_details" , params ) 
return location_details 
~~ def getLocations ( self , query , ** kwargs ) : 
params = { "query" : query } 
~~ ~~ locations = self . api . get ( "/locations" , params ) 
return locations 
~~ def getDailyMenu ( self , restaurant_id ) : 
params = { "res_id" : restaurant_id } 
daily_menu = self . api . get ( "/dailymenu" , params ) 
return daily_menu 
~~ def getRestaurantDetails ( self , restaurant_id ) : 
restaurant_details = self . api . get ( "/restaurant" , params ) 
return restaurant_details 
~~ def getRestaurantReviews ( self , restaurant_id , ** kwargs ) : 
optional_params = [ "start" , "count" ] 
~~ ~~ reviews = self . api . get ( "/reviews" , params ) 
return reviews 
~~ def search ( self , ** kwargs ) : 
available_params = [ 
"entity_id" , "entity_type" , "q" , "start" , 
"count" , "lat" , "lon" , "radius" , "cuisines" , 
"establishment_type" , "collection_id" , 
"category" , "sort" , "order" ] 
for key in available_params : 
~~ ~~ results = self . api . get ( "/search" , params ) 
~~ def sanitize_turbo ( html , allowed_tags = TURBO_ALLOWED_TAGS , allowed_attrs = TURBO_ALLOWED_ATTRS ) : 
return clean ( html , tags = allowed_tags , attributes = allowed_attrs , strip = True ) 
~~ def configure_analytics_yandex ( self , ident , params = None ) : 
params = params or { } 
'type' : 'Yandex' , 
'id' : ident , 
~~~ data [ 'params' ] = '%s' % params 
~~ self . analytics . append ( data ) 
~~ def tag_list ( self , tags ) : 
return [ 
for tag in self . model . objects . all ( ) 
~~ def gcd ( self , lon1 , lat1 , lon2 , lat2 ) : 
lon1 , lat1 , lon2 , lat2 = map ( math . radians , [ lon1 , lat1 , lon2 , lat2 ] ) 
dlon = lon2 - lon1 
dlat = lat2 - lat1 
a = math . sin ( dlat / 2 ) ** 2 + math . cos ( lat1 ) * math . cos ( lat2 ) * math . sin ( dlon / 2 ) ** 2 
c = 2 * math . asin ( math . sqrt ( a ) ) 
dis = E . R * c 
return dis 
~~ def hash_md5 ( self ) : 
fp_plain = hashlib . md5 ( self . _decoded_key ) . hexdigest ( ) 
return "MD5:" + ':' . join ( a + b for a , b in zip ( fp_plain [ : : 2 ] , fp_plain [ 1 : : 2 ] ) ) 
~~ def hash_sha256 ( self ) : 
fp_plain = hashlib . sha256 ( self . _decoded_key ) . digest ( ) 
return ( b"SHA256:" + base64 . b64encode ( fp_plain ) . replace ( b"=" , b"" ) ) . decode ( "utf-8" ) 
~~ def hash_sha512 ( self ) : 
fp_plain = hashlib . sha512 ( self . _decoded_key ) . digest ( ) 
return ( b"SHA512:" + base64 . b64encode ( fp_plain ) . replace ( b"=" , b"" ) ) . decode ( "utf-8" ) 
~~ def _unpack_by_int ( self , data , current_position ) : 
~~~ requested_data_length = struct . unpack ( '>I' , data [ current_position : current_position + self . INT_LEN ] ) [ 0 ] 
~~ except struct . error : 
~~ current_position += self . INT_LEN 
remaining_data_length = len ( data [ current_position : ] ) 
if remaining_data_length < requested_data_length : 
~~~ raise MalformedDataError ( 
~~ next_data = data [ current_position : current_position + requested_data_length ] 
current_position += requested_data_length 
return current_position , next_data 
~~ def _parse_long ( cls , data ) : 
if sys . version < '3' : 
for byte in data : 
~~~ ret = ( ret << 8 ) + ord ( byte ) 
~~~ ret = 0 
~~~ ret = ( ret << 8 ) + byte 
~~ def decode_key ( cls , pubkey_content ) : 
~~~ decoded_key = base64 . b64decode ( pubkey_content . encode ( "ascii" ) ) 
~~ except ( TypeError , binascii . Error ) : 
~~ return decoded_key 
~~ def parse_options ( self , options ) : 
quote_open = False 
parsed_options = { } 
def parse_add_single_option ( opt ) : 
if "=" in opt : 
~~~ opt_name , opt_value = opt . split ( "=" , 1 ) 
opt_value = opt_value . replace ( \ , '' ) 
~~~ opt_name = opt 
opt_value = True 
~~ if self . strict_mode : 
~~~ for valid_opt_name , value_required in self . OPTIONS_SPEC : 
~~~ if opt_name . lower ( ) == valid_opt_name : 
~~~ if value_required and opt_value is True : 
~~ ~~ if opt_name not in parsed_options : 
~~~ parsed_options [ opt_name ] = [ ] 
~~ parsed_options [ opt_name ] . append ( opt_value ) 
~~ start_of_current_opt = 0 
for i , character in enumerate ( options ) : 
~~~ quote_open = not quote_open 
~~ if quote_open : 
~~ if character == "," : 
~~~ opt = options [ start_of_current_opt : i ] 
parse_add_single_option ( opt ) 
start_of_current_opt = i + 1 
~~ ~~ if start_of_current_opt + 1 != i : 
~~~ opt = options [ start_of_current_opt : ] 
~~ return parsed_options 
~~ def _process_ssh_rsa ( self , data ) : 
current_position , raw_e = self . _unpack_by_int ( data , 0 ) 
current_position , raw_n = self . _unpack_by_int ( data , current_position ) 
unpacked_e = self . _parse_long ( raw_e ) 
unpacked_n = self . _parse_long ( raw_n ) 
self . rsa = RSAPublicNumbers ( unpacked_e , unpacked_n ) . public_key ( default_backend ( ) ) 
self . bits = self . rsa . key_size 
if self . strict_mode : 
~~~ min_length = self . RSA_MIN_LENGTH_STRICT 
max_length = self . RSA_MAX_LENGTH_STRICT 
~~~ min_length = self . RSA_MIN_LENGTH_LOOSE 
max_length = self . RSA_MAX_LENGTH_LOOSE 
~~ if self . bits < min_length : 
~~~ raise TooShortKeyError ( 
~~ if self . bits > max_length : 
~~~ raise TooLongKeyError ( 
~~ return current_position 
~~ def _process_ssh_dss ( self , data ) : 
data_fields = { } 
current_position = 0 
for item in ( "p" , "q" , "g" , "y" ) : 
~~~ current_position , value = self . _unpack_by_int ( data , current_position ) 
data_fields [ item ] = self . _parse_long ( value ) 
~~ q_bits = self . _bits_in_number ( data_fields [ "q" ] ) 
p_bits = self . _bits_in_number ( data_fields [ "p" ] ) 
if q_bits != self . DSA_N_LENGTH : 
~~~ min_length = self . DSA_MIN_LENGTH_STRICT 
max_length = self . DSA_MAX_LENGTH_STRICT 
~~~ min_length = self . DSA_MIN_LENGTH_LOOSE 
max_length = self . DSA_MAX_LENGTH_LOOSE 
~~ if p_bits < min_length : 
~~ if p_bits > max_length : 
~~ dsa_parameters = DSAParameterNumbers ( data_fields [ "p" ] , data_fields [ "q" ] , data_fields [ "g" ] ) 
self . dsa = DSAPublicNumbers ( data_fields [ "y" ] , dsa_parameters ) . public_key ( default_backend ( ) ) 
self . bits = self . dsa . key_size 
return current_position 
~~ def _process_ecdsa_sha ( self , data ) : 
current_position , curve_information = self . _unpack_by_int ( data , 0 ) 
if curve_information not in self . ECDSA_CURVE_DATA : 
~~ curve , hash_algorithm = self . ECDSA_CURVE_DATA [ curve_information ] 
current_position , key_data = self . _unpack_by_int ( data , current_position ) 
~~~ ecdsa_key = ecdsa . VerifyingKey . from_string ( key_data [ 1 : ] , curve , hash_algorithm ) 
~~ self . bits = int ( curve_information . replace ( b"nistp" , b"" ) ) 
self . ecdsa = ecdsa_key 
~~ def _process_ed25516 ( self , data ) : 
current_position , verifying_key = self . _unpack_by_int ( data , 0 ) 
verifying_key_length = len ( verifying_key ) * 8 
verifying_key = self . _parse_long ( verifying_key ) 
if verifying_key < 0 : 
~~ self . bits = verifying_key_length 
if self . bits != 256 : 
~~ def parse ( self , keydata = None ) : 
if keydata is None : 
~~~ if self . keydata is None : 
~~ keydata = self . keydata 
~~~ self . reset ( ) 
self . keydata = keydata 
pubkey_content = "" . join ( [ line for line in keydata . split ( "\\n" ) if ":" not in line and "----" not in line ] ) 
~~~ key_parts = self . _split_key ( keydata ) 
key_type = key_parts [ 0 ] 
pubkey_content = key_parts [ 1 ] 
~~ self . _decoded_key = self . decode_key ( pubkey_content ) 
current_position , unpacked_key_type = self . _unpack_by_int ( self . _decoded_key , 0 ) 
if key_type is not None and key_type != unpacked_key_type . decode ( ) : 
~~ self . key_type = unpacked_key_type 
key_data_length = self . _process_key ( self . _decoded_key [ current_position : ] ) 
current_position = current_position + key_data_length 
if current_position != len ( self . _decoded_key ) : 
~~ if self . disallow_options and self . options : 
~~ ~~ def main ( properties = properties , options = options , ** custom_options ) : 
return init ( ** dict ( options , ** custom_options ) ) ( ** properties ) 
~~ def init ( 
dist = 'dist' , 
minver = None , 
maxver = None , 
use_markdown_readme = True , 
use_stdeb = False , 
use_distribute = False , 
if not minver == maxver == None : 
if not minver <= sys . version < ( maxver or 'Any' ) : 
~~~ sys . stderr . write ( 
sys . argv [ 0 ] , minver or 'any' , maxver or 'any' , sys . version . split ( ) [ 0 ] ) ) 
~~ ~~ if use_distribute : 
~~~ from distribute_setup import use_setuptools 
use_setuptools ( to_dir = dist ) 
from setuptools import setup 
~~~ from setuptools import setup 
~~~ from distutils . core import setup 
~~ ~~ if use_markdown_readme : 
~~~ import setuptools . command . sdist 
setuptools . command . sdist . READMES = tuple ( list ( getattr ( setuptools . command . sdist , 'READMES' , ( ) ) ) 
+ [ 'README.md' ] ) 
~~ ~~ if use_stdeb : 
~~~ import platform 
if 'debian' in platform . dist ( ) : 
~~~ import stdeb 
~~ ~~ ~~ return setup 
~~ def render ( self , name , value , attrs = None ) : 
if self . js_var_format is not None : 
~~~ js_var_bit = '' 
~~ output = [ super ( CodeMirrorTextarea , self ) . render ( name , value , attrs ) , 
( js_var_bit , \ % name , self . option_json ) ] 
return mark_safe ( '\\n' . join ( output ) ) 
~~ def iter_auth_hashes ( user , purpose , minutes_valid ) : 
now = timezone . now ( ) . replace ( microsecond = 0 , second = 0 ) 
for minute in range ( minutes_valid + 1 ) : 
~~~ yield hashlib . sha1 ( 
'%s:%s:%s:%s:%s' % ( 
now - datetime . timedelta ( minutes = minute ) , 
user . password , 
purpose , 
user . pk , 
settings . SECRET_KEY , 
) . hexdigest ( ) 
~~ ~~ def calc_expiry_time ( minutes_valid ) : 
timezone . now ( ) + datetime . timedelta ( minutes = minutes_valid + 1 ) 
) . replace ( second = 0 , microsecond = 0 ) 
~~ def get_user_token ( user , purpose , minutes_valid ) : 
token = '' . join ( 
dumps ( [ 
user . get_username ( ) , 
get_auth_hash ( user , purpose ) , 
] ) . encode ( 'base64' ) . split ( '\\n' ) 
'id' : get_meteor_id ( user ) , 
'token' : token , 
'tokenExpires' : calc_expiry_time ( minutes_valid ) , 
~~ def serialize ( self , obj , * args , ** kwargs ) : 
data = super ( Users , self ) . serialize ( obj , * args , ** kwargs ) 
profile = data . pop ( 'fields' ) 
profile . setdefault ( 'name' , obj . get_full_name ( ) ) 
fields = data [ 'fields' ] = { 
'username' : obj . get_username ( ) , 
'emails' : [ ] , 
'profile' : profile , 
'permissions' : sorted ( self . model . get_all_permissions ( obj ) ) , 
for sensitive in [ 
'password' , 
'user_permissions_ids' , 
'is_active' , 
'is_staff' , 
'is_superuser' , 
'groups_ids' , 
] : 
~~~ profile . pop ( sensitive , None ) 
~~~ fields [ 'createdAt' ] = profile . pop ( 'date_joined' ) 
~~~ date_joined = getattr ( 
obj , 'get_date_joined' , 
lambda : getattr ( obj , 'date_joined' , None ) 
if date_joined : 
~~~ fields [ 'createdAt' ] = date_joined 
~~~ email = profile . pop ( 'email' ) 
~~~ email = getattr ( 
obj , 'get_email' , 
lambda : getattr ( obj , 'email' , None ) 
~~ if email : 
~~~ fields [ 'emails' ] . append ( { 'address' : email , 'verified' : True } ) 
~~ def deserialize_profile ( profile , key_prefix = '' , pop = False ) : 
if pop : 
~~~ getter = profile . pop 
~~~ getter = profile . get 
~~ def prefixed ( name ) : 
return '%s%s' % ( key_prefix , name ) 
~~ for key in profile . keys ( ) : 
~~~ val = getter ( key ) 
if key == prefixed ( 'name' ) : 
~~~ result [ 'full_name' ] = val 
~~ def update ( self , selector , update , options = None ) : 
del options 
user = get_object ( 
self . model , selector [ '_id' ] , 
pk = this . user_id , 
profile_update = self . deserialize_profile ( 
update [ '$set' ] , key_prefix = 'profile.' , pop = True , 
if len ( update [ '$set' ] ) != 0 : 
~~ for key , val in profile_update . items ( ) : 
~~~ setattr ( user , key , val ) 
~~ user . save ( ) 
~~ def user_factory ( self ) : 
if this . user_id is None : 
~~ return self . user_model . objects . get ( pk = this . user_id ) 
~~ def update_subs ( new_user_id ) : 
for sub in Subscription . objects . filter ( connection = this . ws . connection ) : 
~~~ params = loads ( sub . params_ejson ) 
pub = API . get_pub_by_name ( sub . publication ) 
pre = collections . OrderedDict ( [ 
( col , query ) for col , query 
in API . sub_unique_objects ( sub , params , pub ) 
sub . user_id = new_user_id 
sub . save ( ) 
post = collections . OrderedDict ( [ 
for col_post , query in post . items ( ) : 
~~~ qs_pre = pre [ col_post ] 
query = query . exclude ( 
pk__in = qs_pre . order_by ( ) . values ( 'pk' ) , 
~~ for obj in query : 
~~~ this . ws . send ( col_post . obj_change_as_msg ( obj , ADDED ) ) 
~~ ~~ for col_pre , query in pre . items ( ) : 
~~~ qs_post = post [ col_pre ] 
pk__in = qs_post . order_by ( ) . values ( 'pk' ) , 
~~~ this . ws . send ( col_pre . obj_change_as_msg ( obj , REMOVED ) ) 
~~ ~~ ~~ ~~ def auth_failed ( ** credentials ) : 
if credentials : 
~~~ user_login_failed . send_robust ( 
sender = __name__ , 
credentials = auth . _clean_credentials ( credentials ) , 
~~ def validated_user ( cls , token , purpose , minutes_valid ) : 
~~~ username , auth_hash = loads ( token . decode ( 'base64' ) ) 
~~ except ( ValueError , Error ) : 
~~~ cls . auth_failed ( token = token ) 
~~~ user = cls . user_model . objects . get ( ** { 
cls . user_model . USERNAME_FIELD : username , 
'is_active' : True , 
user . backend = 'django.contrib.auth.backends.ModelBackend' 
~~ except cls . user_model . DoesNotExist : 
~~~ cls . auth_failed ( username = username , token = token ) 
~~ if auth_hash not in iter_auth_hashes ( user , purpose , minutes_valid ) : 
~~ def check_secure ( ) : 
if this . request . is_secure ( ) : 
~~ elif this . request . META [ 'REMOTE_ADDR' ] in [ 
'localhost' , 
'127.0.0.1' , 
~~ def get_username ( self , user ) : 
if isinstance ( user , basestring ) : 
~~~ return user 
~~ elif isinstance ( user , dict ) and len ( user ) == 1 : 
~~~ [ ( key , val ) ] = user . items ( ) 
if key == 'username' or ( key == self . user_model . USERNAME_FIELD ) : 
~~~ return val 
~~ elif key in ( 'email' , 'emails.address' ) : 
~~~ email_field = getattr ( self . user_model , 'EMAIL_FIELD' , 'email' ) 
if self . user_model . USERNAME_FIELD == email_field : 
~~ return self . user_model . objects . values_list ( 
self . user_model . USERNAME_FIELD , flat = True , 
) . get ( ** { email_field : val } ) 
~~ elif key in ( 'id' , 'pk' ) : 
~~~ return self . user_model . objects . values_list ( 
) . get ( 
pk = val , 
~~ ~~ def create_user ( self , params ) : 
receivers = create_user . send ( 
request = this . request , 
if len ( receivers ) == 0 : 
~~ user = receivers [ 0 ] [ 1 ] 
user = auth . authenticate ( 
username = user . get_username ( ) , password = params [ 'password' ] , 
self . do_login ( user ) 
return get_user_token ( 
user = user , purpose = HashPurpose . RESUME_LOGIN , 
minutes_valid = HASH_MINUTES_VALID [ HashPurpose . RESUME_LOGIN ] , 
~~ def do_login ( self , user ) : 
this . user_id = user . pk 
this . user_ddp_id = get_meteor_id ( user ) 
this . user_sub_id = meteor_random_id ( ) 
API . do_sub ( this . user_sub_id , 'LoggedInUser' , silent = True ) 
self . update_subs ( user . pk ) 
user_logged_in . send ( 
sender = user . __class__ , request = this . request , user = user , 
~~ def do_logout ( self ) : 
API . do_unsub ( this . user_sub_id , silent = True ) 
del this . user_sub_id 
self . update_subs ( None ) 
user_logged_out . send ( 
sender = self . user_model , request = this . request , user = this . user , 
this . user_id = None 
this . user_ddp_id = None 
~~ def login ( self , params ) : 
if 'password' in params : 
~~~ return self . login_with_password ( params ) 
~~ elif 'resume' in params : 
~~~ return self . login_with_resume_token ( params ) 
~~~ self . auth_failed ( ** params ) 
~~ ~~ def login_with_password ( self , params ) : 
self . check_secure ( ) 
username = self . get_username ( params [ 'user' ] ) 
password = self . get_password ( params [ 'password' ] ) 
user = auth . authenticate ( username = username , password = password ) 
~~~ if user . is_active : 
~~~ self . do_login ( user ) 
~~ ~~ self . auth_failed ( ) 
~~ def login_with_resume_token ( self , params ) : 
user = self . validated_user ( 
params [ 'resume' ] , purpose = HashPurpose . RESUME_LOGIN , 
~~ def change_password ( self , old_password , new_password ) : 
~~~ user = this . user 
~~ except self . user_model . DoesNotExist : 
~~~ self . auth_failed ( ) 
~~ user = auth . authenticate ( 
username = user . get_username ( ) , 
password = self . get_password ( old_password ) , 
if user is None : 
~~~ user . set_password ( self . get_password ( new_password ) ) 
user . save ( ) 
password_changed . send ( 
return { "passwordChanged" : True } 
~~ ~~ def forgot_password ( self , params ) : 
username = self . get_username ( params ) 
~~~ user = self . user_model . objects . get ( ** { 
self . user_model . USERNAME_FIELD : username , 
~~ minutes_valid = HASH_MINUTES_VALID [ HashPurpose . PASSWORD_RESET ] 
token = get_user_token ( 
user = user , purpose = HashPurpose . PASSWORD_RESET , 
minutes_valid = minutes_valid , 
forgot_password . send ( 
token = token , 
expiry_date = calc_expiry_time ( minutes_valid ) , 
~~ def reset_password ( self , token , new_password ) : 
token , purpose = HashPurpose . PASSWORD_RESET , 
minutes_valid = HASH_MINUTES_VALID [ HashPurpose . PASSWORD_RESET ] , 
user . set_password ( new_password ) 
return { "userId" : this . user_ddp_id } 
~~ def dict_merge ( lft , rgt ) : 
if not isinstance ( rgt , dict ) : 
~~~ return rgt 
~~ result = deepcopy ( lft ) 
for key , val in rgt . iteritems ( ) : 
~~~ if key in result and isinstance ( result [ key ] , dict ) : 
~~~ result [ key ] = dict_merge ( result [ key ] , val ) 
~~~ result [ key ] = deepcopy ( val ) 
~~ def read ( path , default = None , encoding = 'utf8' ) : 
~~~ with io . open ( path , mode = 'r' , encoding = encoding ) as contents : 
~~~ return contents . read ( ) 
~~~ if default is not None : 
~~ ~~ def get ( self , request , path ) : 
if path == 'meteor_runtime_config.js' : 
~~~ config = { 
'DDP_DEFAULT_CONNECTION_URL' : request . build_absolute_uri ( '/' ) , 
'PUBLIC_SETTINGS' : self . meteor_settings . get ( 'public' , { } ) , 
'ROOT_URL' : request . build_absolute_uri ( 
'%s/' % ( 
self . runtime_config . get ( 'ROOT_URL_PATH_PREFIX' , '' ) , 
'ROOT_URL_PATH_PREFIX' : '' , 
if config [ 'DDP_DEFAULT_CONNECTION_URL' ] . startswith ( 'http:' ) and settings . SECURE_SSL_REDIRECT : 
~~~ config [ 'DDP_DEFAULT_CONNECTION_URL' ] = 'https:%s' % ( 
config [ 'DDP_DEFAULT_CONNECTION_URL' ] . split ( ':' , 1 ) [ 1 ] , 
~~ config . update ( self . runtime_config ) 
return HttpResponse ( 
content_type = 'text/javascript' , 
~~~ file_path , content_type = self . url_map [ path ] 
with open ( file_path , 'r' ) as content : 
~~~ return HttpResponse ( 
content . read ( ) , 
content_type = content_type , 
~~~ return HttpResponse ( self . html ) 
~~ ~~ def get_meteor_id ( obj_or_model , obj_pk = None ) : 
if obj_or_model is None : 
~~ meta = obj_or_model . _meta 
model = meta . model 
if model is ObjectMapping : 
~~ if isinstance ( obj_or_model , model ) : 
~~~ if isinstance ( meta . pk , AleaIdField ) : 
~~~ return obj_or_model . pk 
~~ if obj_pk is None : 
~~~ obj_pk = str ( obj_or_model . pk ) 
~~ ~~ alea_unique_fields = [ 
field 
for field in meta . local_fields 
if isinstance ( field , AleaIdField ) and field . unique 
if len ( alea_unique_fields ) == 1 : 
~~~ aid = alea_unique_fields [ 0 ] . attname 
if isinstance ( obj_or_model , model ) : 
~~~ val = getattr ( obj_or_model , aid ) 
~~ elif obj_pk is None : 
~~~ val = None 
~~~ val = model . objects . values_list ( aid , flat = True ) . get ( 
pk = obj_pk , 
~~ if val : 
~~ ~~ if obj_pk is None : 
~~ content_type = ContentType . objects . get_for_model ( model ) 
~~~ return ObjectMapping . objects . values_list ( 
'meteor_id' , flat = True , 
object_id = obj_pk , 
~~ except ObjectDoesNotExist : 
~~~ return ObjectMapping . objects . create ( 
meteor_id = meteor_random_id ( '/collection/%s' % meta ) , 
) . meteor_id 
~~ ~~ def get_meteor_ids ( model , object_ids ) : 
meta = model . _meta 
result = collections . OrderedDict ( 
( str ( obj_pk ) , None ) 
for obj_pk 
in object_ids 
if isinstance ( meta . pk , AleaIdField ) : 
~~~ return collections . OrderedDict ( 
( obj_pk , obj_pk ) for obj_pk in object_ids 
~~ alea_unique_fields = [ 
if isinstance ( field , AleaIdField ) and field . unique and not field . null 
~~~ aid = alea_unique_fields [ 0 ] . name 
query = model . objects . filter ( 
pk__in = object_ids , 
) . values_list ( 'pk' , aid ) 
~~~ content_type = ContentType . objects . get_for_model ( model ) 
query = ObjectMapping . objects . filter ( 
object_id__in = list ( result ) 
) . values_list ( 'object_id' , 'meteor_id' ) 
~~ for obj_pk , meteor_id in query : 
~~~ result [ str ( obj_pk ) ] = meteor_id 
~~ for obj_pk , meteor_id in result . items ( ) : 
~~~ if meteor_id is None : 
~~~ result [ obj_pk ] = get_meteor_id ( model , obj_pk ) 
~~ def get_object_id ( model , meteor_id ) : 
if meteor_id is None : 
~~ meta = model . _meta 
~~ if isinstance ( meta . pk , AleaIdField ) : 
~~~ return meteor_id 
~~~ val = model . objects . values_list ( 
'pk' , flat = True , 
) . get ( ** { 
alea_unique_fields [ 0 ] . attname : meteor_id , 
if val : 
~~ ~~ content_type = ContentType . objects . get_for_model ( model ) 
return ObjectMapping . objects . filter ( 
meteor_id = meteor_id , 
) . values_list ( 'object_id' , flat = True ) . get ( ) 
~~ def get_object_ids ( model , meteor_ids ) : 
alea_unique_fields = [ 
( str ( meteor_id ) , None ) 
for meteor_id 
in meteor_ids 
query = model . objects . filter ( ** { 
'%s__in' % aid : meteor_ids , 
} ) . values_list ( aid , 'pk' ) 
meteor_id__in = meteor_ids , 
) . values_list ( 'meteor_id' , 'object_id' ) 
~~ for meteor_id , object_id in query : 
~~~ result [ meteor_id ] = object_id 
~~ def get_object ( model , meteor_id , * args , ** kwargs ) : 
~~~ return model . objects . filter ( * args , ** kwargs ) . get ( pk = meteor_id ) 
~~~ return model . objects . filter ( * args , ** kwargs ) . get ( ** { 
alea_unique_fields [ 0 ] . name : meteor_id , 
~~ return model . objects . filter ( * args , ** kwargs ) . get ( 
pk = get_object_id ( model , meteor_id ) , 
~~ def get_pk_value_on_save ( self , instance ) : 
value = super ( AleaIdField , self ) . get_pk_value_on_save ( instance ) 
~~~ value = self . get_seeded_value ( instance ) 
~~ def pre_save ( self , model_instance , add ) : 
value = super ( AleaIdField , self ) . pre_save ( model_instance , add ) 
if ( not value ) and self . default in ( meteor_random_id , NOT_PROVIDED ) : 
~~~ value = self . get_seeded_value ( model_instance ) 
setattr ( model_instance , self . attname , value ) 
~~ def set_default_forwards ( app_name , operation , apps , schema_editor ) : 
model = apps . get_model ( app_name , operation . model_name ) 
for obj_pk in model . objects . values_list ( 'pk' , flat = True ) : 
~~~ model . objects . filter ( pk = obj_pk ) . update ( ** { 
operation . name : get_meteor_id ( model , obj_pk ) , 
~~ ~~ def set_default_reverse ( app_name , operation , apps , schema_editor ) : 
~~~ get_meteor_id ( model , obj_pk ) 
~~ ~~ def truncate ( self , app_label , schema_editor , models ) : 
for model_name in models : 
~~~ model = '%s_%s' % ( app_label , model_name ) 
schema_editor . execute ( 
model . lower ( ) , 
~~ ~~ def database_forwards ( self , app_label , schema_editor , from_state , to_state ) : 
self . truncate ( app_label , schema_editor , self . truncate_forwards ) 
~~ def database_backwards ( self , app_label , schema_editor , from_state , to_state ) : 
self . truncate ( app_label , schema_editor , self . truncate_backwards ) 
~~ def initialize_options ( self ) : 
setuptools . command . build_py . build_py . initialize_options ( self ) 
self . meteor = 'meteor' 
self . meteor_debug = False 
self . build_lib = None 
self . package_dir = None 
self . meteor_builds = [ ] 
self . no_prune_npm = None 
self . inplace = True 
~~ def finalize_options ( self ) : 
self . set_undefined_options ( 
'build' , 
( 'build_lib' , 'build_lib' ) , 
'build_py' , 
( 'package_dir' , 'package_dir' ) , 
setuptools . command . build_py . build_py . finalize_options ( self ) 
for ( package , source , target , extra_args ) in self . meteor_builds : 
~~~ src_dir = self . get_package_dir ( package ) 
project_dir = self . path_to_dir ( src_dir , source ) 
target_dir = self . path_to_dir ( src_dir , target ) 
output_dir = self . path_to_dir ( 
os . path . abspath ( SETUP_DIR if self . inplace else self . build_lib ) , 
target_dir , 
cmdline = [ self . meteor , 'build' , '--directory' , output_dir ] 
no_prune_npm = self . no_prune_npm 
if extra_args [ : 1 ] == [ '--no-prune-npm' ] : 
~~~ no_prune_npm = True 
extra_args [ : 1 ] = [ ] 
~~ if self . meteor_debug and '--debug' not in cmdline : 
~~~ cmdline . append ( '--debug' ) 
~~ cmdline . extend ( extra_args ) 
subprocess . check_call ( cmdline , cwd = project_dir ) 
if not no_prune_npm : 
~~~ npm_build_dir = os . path . join ( 
output_dir , 'bundle' , 'programs' , 'server' , 'npm' , 
shutil . rmtree ( npm_build_dir ) 
~~ ~~ ~~ def path_to_dir ( * path_args ) : 
return os . path . join ( 
* list ( path_args [ : - 1 ] ) + path_args [ - 1 ] . split ( posixpath . sep ) 
~~ def seed ( self , values ) : 
if not values : 
~~~ seed_ids = [ int , str , random , self , values , self . __class__ ] 
random . shuffle ( seed_ids ) 
values = list ( map ( id , seed_ids ) ) + [ time . time ( ) , os . urandom ( 512 ) ] 
~~ mash = Mash ( ) 
self . c = 1 
for val in values : 
~~~ self . s0 -= mash ( val ) 
if self . s0 < 0 : 
~~~ self . s0 += 1 
~~ self . s1 -= mash ( val ) 
if self . s1 < 0 : 
~~~ self . s1 += 1 
~~ self . s2 -= mash ( val ) 
if self . s2 < 0 : 
~~~ self . s2 += 1 
~~ ~~ ~~ def state ( self ) : 
return { 'c' : self . c , 's0' : self . s0 , 's1' : self . s1 , 's2' : self . s2 } 
~~ def random_string ( self , length , alphabet ) : 
return '' . join ( 
self . choice ( alphabet ) for n in range ( length ) 
~~ def api_endpoint ( path_or_func = None , decorate = True ) : 
def maybe_decorated ( func ) : 
if decorate : 
~~~ for decorator in API_ENDPOINT_DECORATORS : 
~~~ func = decorator ( ) ( func ) 
~~ ~~ return func 
~~ if callable ( path_or_func ) : 
~~~ path_or_func . api_path = path_or_func . __name__ 
return maybe_decorated ( path_or_func ) 
~~~ def _api_endpoint ( func ) : 
if path_or_func is None : 
~~~ func . api_path = func . __name__ 
~~~ func . api_path = path_or_func 
~~ return maybe_decorated ( func ) 
~~ return _api_endpoint 
~~ ~~ def api_endpoints ( obj ) : 
for name in dir ( obj ) : 
~~~ attr = getattr ( obj , name ) 
api_path = getattr ( attr , 'api_path' , None ) 
if api_path : 
~~~ yield ( 
'%s%s' % ( obj . api_path_prefix , api_path ) , 
attr , 
~~ ~~ for api_provider in obj . api_providers : 
~~~ for api_path , attr in api_endpoints ( api_provider ) : 
~~~ yield ( api_path , attr ) 
~~ ~~ ~~ def api_path_map ( self ) : 
if self . _api_path_cache is None : 
~~~ self . _api_path_cache = { 
api_path : func 
for api_path , func 
in api_endpoints ( self ) 
~~ return self . _api_path_cache 
~~ def clear_api_path_map_cache ( self ) : 
self . _api_path_cache = None 
for api_provider in self . api_providers : 
~~~ if six . get_method_self ( 
api_provider . clear_api_path_map_cache , 
) is not None : 
~~~ api_provider . clear_api_path_map_cache ( ) 
~~ ~~ ~~ def safe_call ( func , * args , ** kwargs ) : 
~~~ return None , func ( * args , ** kwargs ) 
~~~ return traceback . format_exc ( ) , None 
~~ ~~ def dprint ( name , val ) : 
from pprint import pformat 
print ( 
pformat ( 
val , indent = 4 , width = 75 , 
) . split ( '\\n' ) 
~~ def validate_kwargs ( func , kwargs ) : 
func_name = func . __name__ 
argspec = inspect . getargspec ( func ) 
all_args = argspec . args [ : ] 
defaults = list ( argspec . defaults or [ ] ) 
if inspect . ismethod ( func ) and all_args [ : 1 ] == [ 'self' ] : 
~~~ all_args [ : 1 ] = [ ] 
~~ if defaults : 
~~~ required = all_args [ : - len ( defaults ) ] 
~~~ required = all_args [ : ] 
~~ trans = { 
arg : arg . endswith ( '_' ) and arg [ : - 1 ] or arg 
for arg 
in all_args 
for key in list ( kwargs ) : 
~~~ key_adj = '%s_' % key 
if key_adj in all_args : 
~~~ kwargs [ key_adj ] = kwargs . pop ( key ) 
~~ ~~ supplied = sorted ( kwargs ) 
missing = [ 
trans . get ( arg , arg ) for arg in required 
if arg not in supplied 
if missing : 
~~~ raise MeteorError ( 
400 , 
func . err , 
func_name , 
~~ extra = [ 
arg for arg in supplied 
if arg not in all_args 
if extra : 
~~ ~~ def on_open ( self ) : 
this . request = WSGIRequest ( self . ws . environ ) 
this . ws = self 
this . send = self . send 
this . reply = self . reply 
self . logger = self . ws . logger 
self . remote_ids = collections . defaultdict ( set ) 
self . _tx_buffer = { } 
self . _tx_buffer_id_gen = itertools . cycle ( irange ( sys . maxint ) ) 
self . _tx_next_id_gen = itertools . cycle ( irange ( sys . maxint ) ) 
self . _tx_next_id = next ( self . _tx_next_id_gen ) 
this . remote_addr = self . remote_addr = '{0[REMOTE_ADDR]}:{0[REMOTE_PORT]}' . format ( 
self . ws . environ , 
this . subs = { } 
self . send ( 'o' ) 
self . send ( \ ) 
~~ def on_close ( self , * args , ** kwargs ) : 
if self . connection is not None : 
~~~ del self . pgworker . connections [ self . connection . pk ] 
self . connection . delete ( ) 
self . connection = None 
~~ signals . request_finished . send ( sender = self . __class__ ) 
~~ def on_message ( self , message ) : 
if self . ws . closed : 
for data in self . ddp_frames_from_message ( message ) : 
~~~ self . process_ddp ( data ) 
signals . request_finished . send ( sender = self . __class__ ) 
~~ ~~ except geventwebsocket . WebSocketError : 
~~~ self . ws . close ( ) 
~~ ~~ def ddp_frames_from_message ( self , message ) : 
~~~ msgs = ejson . loads ( message ) 
~~~ self . reply ( 
raise StopIteration 
~~ if not isinstance ( msgs , list ) : 
~~ while msgs : 
~~~ raw = msgs . pop ( 0 ) 
~~~ data = ejson . loads ( raw ) 
~~~ data = None 
~~ if not isinstance ( data , dict ) : 
'error' , error = 400 , 
offendingMessage = raw , 
~~ yield data 
if msgs : 
~~~ gevent . sleep ( ) 
~~ ~~ ~~ def process_ddp ( self , data ) : 
msg_id = data . get ( 'id' , None ) 
~~~ msg = data . pop ( 'msg' ) 
offendingMessage = data , 
~~~ self . dispatch ( msg , data ) 
~~~ kwargs = { 
'msg' : { 'method' : 'result' } . get ( msg , 'error' ) , 
if msg_id is not None : 
~~~ kwargs [ 'id' ] = msg_id 
~~ if isinstance ( err , MeteorError ) : 
~~~ error = err . as_dict ( ) 
~~~ error = { 
'error' : 500 , 
~~ if kwargs [ 'msg' ] == 'error' : 
~~~ kwargs . update ( error ) 
~~~ kwargs [ 'error' ] = error 
~~ if not isinstance ( err , MeteorError ) : 
~~~ stack , _ = safe_call ( 
if stack is not None : 
~~~ traceback . print_exc ( file = sys . stderr ) 
sys . stderr . write ( 
sys . stderr . write ( stack ) 
~~ ~~ elif settings . DEBUG : 
dprint ( 'msg' , msg ) 
dprint ( 'data' , data ) 
error . setdefault ( 'details' , traceback . format_exc ( ) ) 
print ( error [ 'details' ] ) 
~~ self . reply ( ** kwargs ) 
if msg_id and msg == 'method' : 
~~~ self . reply ( 'updated' , methods = [ msg_id ] ) 
~~ ~~ ~~ def dispatch ( self , msg , kwargs ) : 
if self . connection is None and msg != 'connect' : 
~~ if msg == 'method' : 
~~~ if ( 
'method' not in kwargs 
) or ( 
'id' not in kwargs 
~~~ handler = getattr ( self , 'recv_%s' % msg ) 
~~ except ( AttributeError , UnicodeEncodeError ) : 
~~ validate_kwargs ( handler , kwargs ) 
handler ( ** kwargs ) 
~~ def send ( self , data , tx_id = None ) : 
if tx_id is None : 
~~~ tx_id = self . get_tx_id ( ) 
~~ self . _tx_buffer [ tx_id ] = data 
while self . _tx_next_id in self . _tx_buffer : 
~~~ data = self . _tx_buffer . pop ( self . _tx_next_id ) 
if self . _tx_buffer : 
~~ self . _tx_next_id = next ( self . _tx_next_id_gen ) 
if not isinstance ( data , basestring ) : 
~~~ msg = data . get ( 'msg' , None ) 
if msg in ( ADDED , CHANGED , REMOVED ) : 
~~~ ids = self . remote_ids [ data [ 'collection' ] ] 
meteor_id = data [ 'id' ] 
if msg == ADDED : 
~~~ if meteor_id in ids : 
~~~ msg = data [ 'msg' ] = CHANGED 
~~~ ids . add ( meteor_id ) 
~~ ~~ elif msg == CHANGED : 
~~~ if meteor_id not in ids : 
~~~ msg = data [ 'msg' ] = ADDED 
ids . add ( meteor_id ) 
~~ ~~ elif msg == REMOVED : 
~~~ ids . remove ( meteor_id ) 
~~ ~~ ~~ data = 'a%s' % ejson . dumps ( [ ejson . dumps ( data ) ] ) 
~~~ self . ws . send ( data ) 
~~ except geventwebsocket . WebSocketError : 
self . _tx_buffer . clear ( ) 
~~ ~~ num_waiting = len ( self . _tx_buffer ) 
if num_waiting > 10 : 
~~~ safe_call ( 
self . logger . warn , 
tx_id , self . _tx_next_id , num_waiting , self . _tx_buffer , 
~~ ~~ def recv_connect ( self , version = None , support = None , session = None ) : 
self . connection . connection_id , 
~~ elif None in ( version , support ) or version not in self . versions : 
~~~ self . reply ( 'failed' , version = self . versions [ 0 ] ) 
~~ elif version not in support : 
~~~ from dddp . models import Connection 
cur = connection . cursor ( ) 
( backend_pid , ) = cur . fetchone ( ) 
this . version = version 
this . support = support 
self . connection = Connection . objects . create ( 
server_addr = '%d:%s' % ( 
backend_pid , 
self . ws . handler . socket . getsockname ( ) , 
remote_addr = self . remote_addr , 
version = version , 
self . pgworker . connections [ self . connection . pk ] = self 
self . reply ( 'connected' , session = self . connection . connection_id ) 
~~ ~~ def recv_ping ( self , id_ = None ) : 
if id_ is None : 
~~~ self . reply ( 'pong' ) 
~~~ self . reply ( 'pong' , id = id_ ) 
~~ ~~ def recv_sub ( self , id_ , name , params ) : 
self . api . sub ( id_ , name , * params ) 
~~ def recv_unsub ( self , id_ = None ) : 
if id_ : 
~~~ self . api . unsub ( id_ ) 
~~~ self . reply ( 'nosub' ) 
~~ ~~ def recv_method ( self , method , params , id_ , randomSeed = None ) : 
if randomSeed is not None : 
~~~ this . random_streams . random_seed = randomSeed 
this . alea_random = alea . Alea ( randomSeed ) 
~~ self . api . method ( method , params , id_ ) 
self . reply ( 'updated' , methods = [ id_ ] ) 
~~ def ddpp_sockjs_info ( environ , start_response ) : 
import random 
import ejson 
start_response ( 
] + common_headers ( environ ) , 
yield ejson . dumps ( collections . OrderedDict ( [ 
( 'websocket' , True ) , 
( 'origins' , [ 
'*:*' , 
] ) , 
( 'cookie_needed' , False ) , 
( 'entropy' , random . getrandbits ( 32 ) ) , 
] ) ) 
~~ def addr ( val , default_port = 8000 , defualt_host = 'localhost' ) : 
import re 
import socket 
match = re . match ( r'\\A(?P<host>.*?)(:(?P<port>(\\d+|\\w+)))?\\Z' , val ) 
if match is None : 
~~~ raise argparse . ArgumentTypeError ( 
~~ host , port = match . group ( 'host' , 'port' ) 
if not host : 
~~~ host = defualt_host 
~~ if not port : 
~~~ port = default_port 
~~ elif port . isdigit ( ) : 
~~~ port = int ( port ) 
~~~ port = socket . getservbyname ( port ) 
~~ return Addr ( host , port ) 
~~ def serve ( listen , verbosity = 1 , debug_port = 0 , ** ssl_args ) : 
launcher = DDPLauncher ( debug = verbosity == 3 , verbosity = verbosity ) 
if debug_port : 
~~~ launcher . servers . append ( 
launcher . get_backdoor_server ( 'localhost:%d' % debug_port ) 
~~ launcher . add_web_servers ( listen , ** ssl_args ) 
sigmap = { 
val : name 
for name , val 
in vars ( signal ) . items ( ) 
if name . startswith ( 'SIG' ) 
def sighandler ( signum = None , frame = None ) : 
launcher . logger . info ( 
sigmap . get ( signum , signum ) , 
frame , 
launcher . stop ( ) 
~~ for signum in [ signal . SIGINT , signal . SIGQUIT ] : 
~~~ gevent . signal ( signum , sighandler ) 
~~ launcher . run ( ) 
parser = argparse . ArgumentParser ( description = __doc__ ) 
django . add_argument ( 
'--verbosity' , '-v' , metavar = 'VERBOSITY' , dest = 'verbosity' , type = int , 
default = 1 , 
'--debug-port' , metavar = 'DEBUG_PORT' , dest = 'debug_port' , type = int , 
default = 0 , 
'--settings' , metavar = 'SETTINGS' , dest = 'settings' , 
http . add_argument ( 
'listen' , metavar = 'address[:port]' , nargs = '*' , type = addr , 
ssl . add_argument ( '--ssl-version' , metavar = 'SSL_VERSION' , dest = 'ssl_version' , 
choices = [ '1' , '2' , '3' ] , default = '3' ) 
ssl . add_argument ( '--certfile' , metavar = 'FILE' , dest = 'certfile' , 
ssl . add_argument ( '--ciphers' , metavar = 'CIPHERS' , dest = 'ciphers' , 
ssl . add_argument ( '--ca-certs' , metavar = 'FILE' , dest = 'ca_certs' , 
ssl . add_argument ( '--keyfile' , metavar = 'FILE' , dest = 'keyfile' , 
namespace = parser . parse_args ( ) 
if namespace . settings : 
~~~ os . environ [ 'DJANGO_SETTINGS_MODULE' ] = namespace . settings 
~~ serve ( 
namespace . listen or [ Addr ( 'localhost' , 8000 ) ] , 
debug_port = namespace . debug_port , 
keyfile = namespace . keyfile , 
certfile = namespace . certfile , 
verbosity = namespace . verbosity , 
~~ def print ( self , msg , * args , ** kwargs ) : 
if self . verbosity >= 1 : 
~~~ print ( msg , * args , ** kwargs ) 
~~ ~~ def add_web_servers ( self , listen_addrs , debug = False , ** ssl_args ) : 
self . servers . extend ( 
self . get_web_server ( listen_addr , debug = debug , ** ssl_args ) 
for listen_addr in listen_addrs 
~~ def get_web_server ( self , listen_addr , debug = False , ** ssl_args ) : 
return geventwebsocket . WebSocketServer ( 
listen_addr , 
self . resource , 
debug = debug , 
** { key : val for key , val in ssl_args . items ( ) if val is not None } 
~~ def get_backdoor_server ( self , listen_addr , ** context ) : 
local_vars = { 
'launcher' : self , 
'servers' : self . servers , 
'pgworker' : self . pgworker , 
'stop' : self . stop , 
'api' : self . api , 
'resource' : self . resource , 
'settings' : settings , 
'wsgi_app' : self . wsgi_app , 
'wsgi_name' : self . wsgi_name , 
local_vars . update ( context ) 
return BackdoorServer ( 
~~ def stop ( self ) : 
self . _stop_event . set ( ) 
for server in self . servers + [ DDPLauncher . pgworker ] : 
server . stop ( ) 
~~ gevent . joinall ( self . threads + [ DDPLauncher . pgworker ] ) 
self . threads = [ ] 
~~ def start ( self ) : 
self . _stop_event . clear ( ) 
if self . verbosity > 1 : 
~~~ for api_path in sorted ( self . api . api_path_map ( ) ) : 
~~ ~~ self . pgworker . start ( ) 
for server in self . servers : 
~~~ thread = gevent . spawn ( server . serve_forever ) 
self . threads . append ( thread ) 
if thread . dead : 
~~~ self . stop ( ) 
thread . get ( ) 
~~ if isinstance ( server , geventwebsocket . WebSocketServer ) : 
~~~ self . print ( 
'https' if server . ssl_enabled else 'http' , 
server . server_host , 
server . server_port , 
~~ elif isinstance ( server , gevent . backdoor . BackdoorServer ) : 
self . start ( ) 
self . _stop_event . wait ( ) 
gevent . joinall ( self . threads + [ DDPLauncher . pgworker ] ) 
~~ def ready ( self ) : 
if not settings . DATABASES : 
~~ for ( alias , conf ) in settings . DATABASES . items ( ) : 
~~~ engine = conf [ 'ENGINE' ] 
if engine not in [ 
'django.db.backends.postgresql' , 
'django.db.backends.postgresql_psycopg2' , 
~~~ warnings . warn ( 
alias , engine , 
UserWarning , 
~~ ~~ self . api = autodiscover ( ) 
self . api . ready ( ) 
conn_params = self . connection . get_connection_params ( ) 
conn_params . update ( 
async = True , 
conn = None 
while conn is None : 
~~~ conn = psycopg2 . connect ( ** conn_params ) 
~~ except psycopg2 . OperationalError as err : 
~~~ msg = ( '%s' % err ) . strip ( ) 
msg_prefix = \ 
if not msg . startswith ( msg_prefix ) : 
~~ key = msg [ len ( msg_prefix ) : - 1 ] 
self . logger . warning ( 
self . connection . alias , 
key , conn_params . pop ( key ) , 
cur = conn . cursor ( ) 
cur . execute ( \ ) 
while not self . _stop_event . is_set ( ) : 
~~~ self . select_greenlet = gevent . spawn ( 
gevent . select . select , 
[ conn ] , [ ] , [ ] , timeout = None , 
self . select_greenlet . get ( ) 
~~ except gevent . GreenletExit : 
~~~ self . _stop_event . set ( ) 
~~~ self . select_greenlet = None 
~~ self . poll ( conn ) 
cur . close ( ) 
self . poll ( conn ) 
conn . close ( ) 
if self . select_greenlet is not None : 
~~~ self . select_greenlet . kill ( ) 
gevent . sleep ( ) 
~~ ~~ def poll ( self , conn ) : 
while 1 : 
~~~ state = conn . poll ( ) 
if state == psycopg2 . extensions . POLL_OK : 
~~~ while conn . notifies : 
~~~ notify = conn . notifies . pop ( ) 
self . logger . info ( 
notify . pid , notify . payload , 
hdr , chunk = notify . payload . split ( '|' , 1 ) 
header = ejson . loads ( hdr ) 
uuid = header [ 'uuid' ] 
size , chunks = self . chunks . setdefault ( uuid , [ 0 , { } ] ) 
if header [ 'fin' ] : 
~~~ size = self . chunks [ uuid ] [ 0 ] = header [ 'seq' ] 
~~ chunks [ header [ 'seq' ] ] = chunk 
if len ( chunks ) != size : 
~~ data = '' . join ( 
chunk for _ , chunk in sorted ( chunks . items ( ) ) 
data = ejson . loads ( data ) 
sender = data . pop ( '_sender' , None ) 
tx_id = data . pop ( '_tx_id' , None ) 
for connection_id in data . pop ( '_connection_ids' ) : 
~~~ websocket = self . connections [ connection_id ] 
~~ if connection_id == sender : 
~~~ websocket . send ( data , tx_id = tx_id ) 
~~~ websocket . send ( data ) 
~~ ~~ ~~ break 
~~ elif state == psycopg2 . extensions . POLL_WRITE : 
~~~ gevent . select . select ( [ ] , [ conn . fileno ( ) ] , [ ] ) 
~~ elif state == psycopg2 . extensions . POLL_READ : 
~~~ gevent . select . select ( [ conn . fileno ( ) ] , [ ] , [ ] ) 
~~ ~~ ~~ def greenify ( ) : 
if _GREEN : 
~~ _GREEN [ True ] = True 
from gevent . monkey import patch_all , saved 
if ( 'threading' in sys . modules ) and ( 'threading' not in saved ) : 
~~~ import warnings 
~~ patch_all ( ) 
~~~ import psycopg2 
del psycopg2 
~~~ from psycopg2cffi import compat 
compat . register ( ) 
~~ from psycogreen . gevent import patch_psycopg 
patch_psycopg ( ) 
~~ def meteor_random_id ( name = None , length = 17 ) : 
~~~ stream = THREAD_LOCAL . alea_random 
~~~ stream = THREAD_LOCAL . random_streams [ name ] 
~~ return stream . random_string ( length , METEOR_ID_CHARS ) 
~~ def autodiscover ( ) : 
from django . utils . module_loading import autodiscover_modules 
from dddp . api import API 
autodiscover_modules ( 'ddp' , register_to = API ) 
return API 
~~ def as_dict ( self , ** kwargs ) : 
error , reason , details , err_kwargs = self . args 
result = { 
key : val 
for key , val in { 
'error' : error , 'reason' : reason , 'details' : details , 
} . items ( ) 
if val is not None 
result . update ( err_kwargs ) 
result . update ( kwargs ) 
~~ def get ( self , name , factory , * factory_args , ** factory_kwargs ) : 
update_thread_local = getattr ( factory , 'update_thread_local' , True ) 
if ( not update_thread_local ) or ( name not in self . __dict__ ) : 
~~~ obj = factory ( * factory_args , ** factory_kwargs ) 
if update_thread_local : 
~~~ setattr ( self , name , obj ) 
~~ return getattr ( self , name ) 
~~ def emit ( self , record ) : 
if getattr ( this , 'subs' , { } ) . get ( LOGS_NAME , False ) : 
~~~ self . format ( record ) 
this . send ( { 
'msg' : ADDED , 
'collection' : LOGS_NAME , 
'id' : meteor_random_id ( '/collection/%s' % LOGS_NAME ) , 
'fields' : { 
attr : { 
'args' : lambda args : [ repr ( arg ) for arg in args ] , 
'created' : datetime . datetime . fromtimestamp , 
'exc_info' : stacklines_or_none , 
} . get ( 
) ( getattr ( record , attr , None ) ) 
for attr in ( 
'args' , 
'asctime' , 
'created' , 
'exc_info' , 
'filename' , 
'funcName' , 
'levelname' , 
'levelno' , 
'lineno' , 
'module' , 
'msecs' , 
'message' , 
'name' , 
'pathname' , 
'process' , 
'processName' , 
'relativeCreated' , 
'thread' , 
'threadName' , 
~~ ~~ def _access_control ( self , access_control , my_media_group = None ) : 
extension = None 
if access_control is AccessControl . Private : 
~~~ if my_media_group : 
~~~ my_media_group . private = gdata . media . Private ( ) 
~~ ~~ elif access_control is AccessControl . Unlisted : 
~~~ from gdata . media import YOUTUBE_NAMESPACE 
from atom import ExtensionElement 
"namespace" : YOUTUBE_NAMESPACE , 
"attributes" : { 'action' : 'list' , 'permission' : 'denied' } , 
extension = ( [ ExtensionElement ( 'accessControl' , ** kwargs ) ] ) 
~~ return extension 
~~ def fetch_feed_by_username ( self , username ) : 
youtube_url = 'http://gdata.youtube.com/feeds/api' 
uri = os . sep . join ( [ youtube_url , "users" , username , "uploads" ] ) 
return Api . yt_service . GetYouTubeVideoFeed ( uri ) 
~~ def authenticate ( self , email = None , password = None , source = None ) : 
from gdata . service import BadAuthentication 
Api . yt_service . email = email if email else settings . YOUTUBE_AUTH_EMAIL 
Api . yt_service . password = password if password else settings . YOUTUBE_AUTH_PASSWORD 
Api . yt_service . source = source if source else settings . YOUTUBE_CLIENT_ID 
~~~ Api . yt_service . ProgrammaticLogin ( ) 
self . authenticated = True 
~~ except BadAuthentication : 
~~ ~~ def upload_direct ( self , video_path , title , description = "" , keywords = "" , developer_tags = None , access_control = AccessControl . Public ) : 
my_media_group = gdata . media . Group ( 
title = gdata . media . Title ( text = title ) , 
description = gdata . media . Description ( description_type = 'plain' , 
text = description ) , 
keywords = gdata . media . Keywords ( text = keywords ) , 
category = [ gdata . media . Category ( 
text = 'Autos' , 
scheme = 'http://gdata.youtube.com/schemas/2007/categories.cat' , 
label = 'Autos' ) ] , 
extension = self . _access_control ( access_control , my_media_group ) 
video_entry = gdata . youtube . YouTubeVideoEntry ( media = my_media_group , extension_elements = extension ) 
if developer_tags : 
~~~ video_entry . AddDeveloperTags ( developer_tags ) 
~~ new_entry = Api . yt_service . InsertVideoEntry ( video_entry , video_path ) 
return new_entry 
~~ def upload ( self , title , description = "" , keywords = "" , developer_tags = None , access_control = AccessControl . Public ) : 
if not self . authenticated : 
~~ my_media_group = gdata . media . Group ( 
video_entry = gdata . youtube . YouTubeVideoEntry ( 
media = my_media_group , extension_elements = extension ) 
~~ response = Api . yt_service . GetFormUploadToken ( video_entry ) 
post_url = response [ 0 ] 
youtube_token = response [ 1 ] 
return { 'post_url' : post_url , 'youtube_token' : youtube_token } 
~~ def check_upload_status ( self , video_id ) : 
~~ entry = self . fetch_video ( video_id ) 
upload_status = Api . yt_service . CheckUploadStatus ( entry ) 
if upload_status is not None : 
~~~ video_upload_state = upload_status [ 0 ] 
detailed_message = upload_status [ 1 ] 
return { "upload_state" : video_upload_state , "detailed_message" : detailed_message } 
~~ ~~ def update_video ( self , video_id , title = "" , description = "" , keywords = "" , access_control = AccessControl . Unlisted ) : 
extension = self . _access_control ( access_control ) 
if extension : 
~~~ entry . extension_elements = extension 
~~ if title : 
~~~ entry . media . title . text = title 
~~ if description : 
~~~ entry . media . description . text = description 
~~ success = Api . yt_service . UpdateVideoEntry ( entry ) 
~~ def delete_video ( self , video_id ) : 
response = Api . yt_service . DeleteVideoEntry ( entry ) 
~~ def check_video_availability ( request , video_id ) : 
api = Api ( ) 
api . authenticate ( ) 
availability = api . check_upload_status ( video_id ) 
if availability is not True : 
~~~ data = { 'success' : False } 
~~~ data = { 'success' : True } 
~~ return HttpResponse ( json . dumps ( data ) , content_type = "application/json" ) 
~~ def video ( request , video_id ) : 
~~~ video = Video . objects . filter ( video_id = video_id ) . get ( ) 
state = availability [ "upload_state" ] 
if state == "failed" or state == "rejected" : 
~~~ return render_to_response ( 
"django_youtube/video_failed.html" , 
{ "video" : video , "video_id" : video_id , "message" : 
context_instance = RequestContext ( request ) 
"django_youtube/video_unavailable.html" , 
{ "video" : video , "video_id" : video_id , 
~~ ~~ video_params = _video_params ( request , video_id ) 
return render_to_response ( 
"django_youtube/video.html" , 
video_params , 
~~ def video_list ( request , username = None ) : 
if username is None and not request . user . is_authenticated ( ) : 
~~~ from django . http import Http404 
raise Http404 
~~ from django . contrib . auth . models import User 
user = User . objects . get ( username = username ) if username else request . user 
videos = Video . objects . filter ( user = user ) . all ( ) 
video_params = [ ] 
for video in videos : 
~~~ video_params . append ( _video_params ( request , video . video_id ) ) 
~~ return render_to_response ( 
"django_youtube/videos.html" , 
{ "video_params" : video_params } , 
~~ def direct_upload ( request ) : 
if request . method == "POST" : 
~~~ form = YoutubeDirectUploadForm ( request . POST , request . FILES ) 
if form . is_valid ( ) : 
~~~ uploaded_video = form . save ( ) 
swf_url = video_entry . GetSwfUrl ( ) 
youtube_url = video_entry . id . text 
url_parts = youtube_url . split ( "/" ) 
url_parts . reverse ( ) 
video_id = url_parts [ 0 ] 
video = Video ( ) 
video . user = request . user 
video . video_id = video_id 
video . youtube_url = youtube_url 
video . swf_url = swf_url 
video . save ( ) 
video_created . send ( sender = video , video = video ) 
uploaded_video . delete ( ) 
return_only_data = request . GET . get ( 'only_data' ) 
if return_only_data : 
~~~ return HttpResponse ( json . dumps ( { "video_id" : video_id } ) , content_type = "application/json" ) 
~~~ next_url = settings . YOUTUBE_UPLOAD_REDIRECT_URL 
~~~ next_url = reverse ( 
"django_youtube.views.video" , kwargs = { "video_id" : video_id } ) 
~~ return HttpResponseRedirect ( next_url ) 
0 ] , sys . exc_info ( ) [ 1 ] ) ) 
~~ ~~ form = YoutubeDirectUploadForm ( ) 
~~~ return HttpResponse ( json . dumps ( { "error" : 500 } ) , content_type = "application/json" ) 
"django_youtube/direct-upload.html" , 
{ "form" : form } , 
~~ ~~ def upload ( request ) : 
title = request . GET . get ( "title" , "%s\ % ( 
request . user . username , request . get_host ( ) ) ) 
description = request . GET . get ( "description" , "" ) 
keywords = request . GET . get ( "keywords" , "" ) 
~~~ api = Api ( ) 
data = api . upload ( title , description = description , keywords = keywords , 
access_control = AccessControl . Unlisted ) 
~~ except ApiError as e : 
~~~ messages . add_message ( request , messages . ERROR , e . message ) 
return HttpResponseRedirect ( "/" ) 
~~~ messages . add_message ( request , messages . ERROR , _ ( 
~~ form = YoutubeUploadForm ( initial = { "token" : data [ "youtube_token" ] } ) 
protocol = 'https' if request . is_secure ( ) else 'http' 
next_url = '%s://%s%s/' % ( protocol , request . get_host ( ) , reverse ( "django_youtube.views.upload_return" ) ) 
"django_youtube/upload.html" , 
{ "form" : form , "post_url" : data [ "post_url" ] , "next_url" : next_url } , 
~~ def upload_return ( request ) : 
status = request . GET . get ( "status" ) 
video_id = request . GET . get ( "id" ) 
if status == "200" and video_id : 
~~~ video = Video ( ) 
~~~ from django . contrib import messages 
messages . add_message ( 
return HttpResponseRedirect ( reverse ( "django_youtube.views.upload" ) ) 
~~ ~~ def remove ( request , video_id ) : 
~~~ next_url = settings . YOUTUBE_DELETE_REDIRECT_URL 
~~~ next_url = reverse ( "django_youtube.views.upload" ) 
~~~ Video . objects . get ( video_id = video_id ) . delete ( ) 
~~ def entry ( self ) : 
return api . fetch_video ( self . video_id ) 
~~ def save ( self , * args , ** kwargs ) : 
if not self . id : 
~~~ entry = self . entry ( ) 
self . title = entry . media . title . text 
self . description = entry . media . description . text 
self . keywords = entry . media . keywords . text 
self . youtube_url = entry . media . player . url 
self . swf_url = entry . GetSwfUrl ( ) 
if entry . media . private : 
~~~ self . access_control = AccessControl . Private 
~~~ self . access_control = AccessControl . Public 
~~ super ( Video , self ) . save ( * args , ** kwargs ) 
for thumbnail in entry . media . thumbnail : 
~~~ t = Thumbnail ( ) 
t . url = thumbnail . url 
t . video = self 
t . save ( ) 
api . update_video ( self . video_id , self . title , self . description , 
self . keywords , self . access_control ) 
~~ return super ( Video , self ) . save ( * args , ** kwargs ) 
~~ def delete ( self , * args , ** kwargs ) : 
api . delete_video ( self . video_id ) 
return super ( Video , self ) . delete ( * args , ** kwargs ) 
~~ def iter_subclasses ( cls , _seen = None ) : 
if not isinstance ( cls , type ) : 
~~ if _seen is None : 
~~~ _seen = set ( ) 
~~~ subs = cls . __subclasses__ ( ) 
~~~ subs = cls . __subclasses__ ( cls ) 
~~ for sub in subs : 
~~~ if sub in _seen : 
~~ _seen . add ( sub ) 
yield sub 
for sub in iter_subclasses ( sub , _seen ) : 
~~~ yield sub 
~~ ~~ ~~ def check_config_file ( msg ) : 
with jsonconfig . Config ( "messages" , indent = 4 ) as cfg : 
~~~ verify_profile_name ( msg , cfg ) 
retrieve_data_from_config ( msg , cfg ) 
if msg . _auth is None : 
~~~ retrieve_pwd_from_config ( msg , cfg ) 
~~ if msg . save : 
~~~ update_config_data ( msg , cfg ) 
update_config_pwd ( msg , cfg ) 
~~ ~~ ~~ def verify_profile_name ( msg , cfg ) : 
if msg . profile not in cfg . data : 
~~~ raise UnknownProfileError ( msg . profile ) 
~~ ~~ def retrieve_data_from_config ( msg , cfg ) : 
msg_type = msg . __class__ . __name__ . lower ( ) 
for attr in msg : 
~~~ if getattr ( msg , attr ) is None and attr in cfg . data [ msg . profile ] [ msg_type ] : 
~~~ setattr ( msg , attr , cfg . data [ msg . profile ] [ msg_type ] [ attr ] ) 
~~ ~~ ~~ def retrieve_pwd_from_config ( msg , cfg ) : 
key_fmt = msg . profile + "_" + msg_type 
if len ( pwd ) == 1 : 
~~~ msg . auth = pwd [ 0 ] 
~~~ msg . auth = tuple ( pwd ) 
~~ ~~ def update_config_data ( msg , cfg ) : 
~~~ if attr in cfg . data [ msg . profile ] and attr is not "auth" : 
~~~ cfg . data [ msg . profile ] [ attr ] = getattr ( msg , attr ) 
~~ ~~ ~~ def update_config_pwd ( msg , cfg ) : 
if isinstance ( msg . _auth , ( MutableSequence , tuple ) ) : 
~~~ cfg . pwd [ key_fmt ] = msg . _auth 
~~ ~~ def create_config_profile ( msg_type ) : 
msg_type = msg_type . lower ( ) 
if msg_type not in CONFIG . keys ( ) : 
~~~ raise UnsupportedMessageTypeError ( msg_type ) 
~~ display_required_items ( msg_type ) 
if get_user_ack ( ) : 
data = get_data_from_user ( msg_type ) 
auth = get_auth_from_user ( msg_type ) 
configure_profile ( msg_type , profile_name , data , auth ) 
~~ ~~ def display_required_items ( msg_type ) : 
for k , v in CONFIG [ msg_type ] [ "settings" ] . items ( ) : 
for k , v in CONFIG [ msg_type ] [ "auth" ] . items ( ) : 
~~ ~~ def get_data_from_user ( msg_type ) : 
~~ def get_auth_from_user ( msg_type ) : 
auth = [ ] 
~~ return OrderedDict ( auth ) 
~~ def configure_profile ( msg_type , profile_name , data , auth ) : 
~~~ write_data ( msg_type , profile_name , data , cfg ) 
write_auth ( msg_type , profile_name , auth , cfg ) 
~~ def write_data ( msg_type , profile_name , data , cfg ) : 
if profile_name not in cfg . data : 
~~~ cfg . data [ profile_name ] = { } 
~~ cfg . data [ profile_name ] [ msg_type ] = data 
~~ def write_auth ( msg_type , profile_name , auth , cfg ) : 
key_fmt = profile_name + "_" + msg_type 
pwd = [ ] 
~~~ pwd . append ( auth [ k ] ) 
~~ if len ( pwd ) > 1 : 
~~~ cfg . pwd [ key_fmt ] = pwd [ 0 ] 
~~ ~~ def _construct_message ( self ) : 
self . message [ "text" ] = "" 
if self . from_ : 
~~ if self . subject : 
~~ self . message [ "text" ] += self . body 
self . _add_attachments ( ) 
~~ def _add_attachments ( self ) : 
if self . attachments : 
~~~ if not isinstance ( self . attachments , list ) : 
~~~ self . attachments = [ self . attachments ] 
~~ self . message [ "attachments" ] = [ 
{ "image_url" : url , "author_name" : "" } for url in self . attachments 
if self . params : 
~~~ for attachment in self . message [ "attachments" ] : 
~~~ attachment . update ( self . params ) 
~~ ~~ ~~ ~~ def send ( self , encoding = "json" ) : 
self . _construct_message ( ) 
if self . verbose : 
"\\n--------------" 
~~ if encoding == "json" : 
~~~ resp = requests . post ( self . url , json = self . message ) 
~~ elif encoding == "url" : 
~~~ resp = requests . post ( self . url , data = self . message ) 
~~~ resp . raise_for_status ( ) 
if resp . history and resp . history [ 0 ] . status_code >= 300 : 
~~ elif "invalid_auth" in resp . text : 
~~ ~~ except ( requests . exceptions . HTTPError , MessageSendError ) as e : 
~~~ raise MessageSendError ( e ) 
~~ if self . verbose : 
timestamp ( ) , 
type ( self ) . __name__ , 
resp . status_code , 
~~ def _construct_message ( self ) : 
self . message = { "token" : self . _auth , "channel" : self . channel } 
super ( ) . _construct_message ( ) 
~~ def send ( msg_type , send_async = False , * args , ** kwargs ) : 
message = message_factory ( msg_type , * args , ** kwargs ) 
~~~ if send_async : 
~~~ message . send_async ( ) 
~~~ message . send ( ) 
~~ ~~ except MessageSendError as e : 
~~ ~~ def message_factory ( msg_type , msg_types = MESSAGE_TYPES , * args , ** kwargs ) : 
~~~ return msg_types [ msg_type . lower ( ) ] ( * args , ** kwargs ) 
~~ except ( UnknownProfileError , InvalidMessageInputError ) as e : 
~~~ raise UnsupportedMessageTypeError ( msg_type , msg_types ) 
~~ ~~ def credential_property ( cred ) : 
def getter ( instance ) : 
~~~ return "***obfuscated***" 
~~ def setter ( instance , value ) : 
~~~ private = "_" + cred 
instance . __dict__ [ private ] = value 
~~ return property ( fget = getter , fset = setter ) 
~~ def validate_property ( attr ) : 
~~~ return instance . __dict__ [ attr ] 
~~~ validate_input ( instance . __class__ . __name__ , attr , value ) 
instance . __dict__ [ attr ] = value 
~~ def validate_input ( msg_type , attr , value ) : 
~~~ valid = { 
"Email" : validate_email , 
"Twilio" : validate_twilio , 
"SlackWebhook" : validate_slackwebhook , 
"SlackPost" : validate_slackpost , 
"TelegramBot" : validate_telegrambot , 
"WhatsApp" : validate_whatsapp , 
} [ msg_type ] ( attr , value ) 
~~ ~~ def check_valid ( msg_type , attr , value , func , exec_info ) : 
~~~ if isinstance ( value , MutableSequence ) : 
~~~ for v in value : 
~~~ if not func ( v ) : 
~~~ raise InvalidMessageInputError ( msg_type , attr , value , exec_info ) 
~~~ if not func ( value ) : 
~~ ~~ ~~ ~~ def validate_twilio ( attr , value ) : 
if attr in ( "from_" , "to" ) : 
~~ elif attr in ( "attachments" ) : 
~~~ check_valid ( "Twilio" , attr , value , validus . isurl , "url" ) 
~~ ~~ def validate_slackpost ( attr , value ) : 
if attr in ( "channel" , "credentials" ) : 
~~~ if not isinstance ( value , str ) : 
~~~ raise InvalidMessageInputError ( "SlackPost" , attr , value , "string" ) 
~~ ~~ elif attr in ( "attachments" ) : 
~~~ check_valid ( "SlackPost" , attr , value , validus . isurl , "url" ) 
~~ ~~ def validate_whatsapp ( attr , value ) : 
~~~ if value is not None and "whatsapp:" in value : 
~~~ value = value . split ( "whatsapp:+" ) [ - 1 ] 
~~ check_valid ( 
"WhatsApp" , 
validus . isint , 
~~~ check_valid ( "WhatsApp" , attr , value , validus . isurl , "url" ) 
~~ ~~ def _send_coroutine ( ) : 
with PoolExecutor ( ) as executor : 
~~~ msg = yield 
future = executor . submit ( msg . send ) 
future . add_done_callback ( _exception_handler ) 
~~ ~~ ~~ def add_message ( self , msg ) : 
~~~ self . _coro . send ( msg ) 
~~~ raise UnsupportedMessageTypeError ( msg . __class__ . __name__ ) 
~~ ~~ def get_body_from_file ( kwds ) : 
if kwds [ "file" ] and os . path . isfile ( kwds [ "file" ] ) : 
~~~ kwds [ "body" ] = open ( kwds [ "file" ] , "r" ) . read ( ) 
kwds [ "file" ] = None 
~~ ~~ def trim_args ( kwds ) : 
reject_key = ( "type" , "types" , "configure" ) 
reject_val = ( None , ( ) ) 
k : v for k , v in kwds . items ( ) if k not in reject_key and v not in reject_val 
for k , v in kwargs . items ( ) : 
~~~ if k in ( "to" , "cc" , "bcc" , "attachments" ) : 
~~~ kwargs [ k ] = list ( kwargs [ k ] ) 
~~ def send_message ( msg_type , kwds ) : 
if kwds [ "file" ] : 
~~~ get_body_from_file ( kwds ) 
~~ kwargs = trim_args ( kwds ) 
send ( msg_type , send_async = False , ** kwargs ) 
~~ def get_chat_id ( self , username ) : 
if username is not None : 
~~~ chats = requests . get ( self . base_url + "/getUpdates" ) . json ( ) 
user = username . split ( "@" ) [ - 1 ] 
for chat in chats [ "result" ] : 
~~~ if chat [ "message" ] [ "from" ] [ "username" ] == user : 
~~~ return chat [ "message" ] [ "from" ] [ "id" ] 
~~ ~~ ~~ ~~ def _construct_message ( self ) : 
self . message [ "chat_id" ] = self . chat_id 
self . message . update ( self . params ) 
~~ def _send_content ( self , method = "/sendMessage" ) : 
url = self . base_url + method 
~~~ resp = requests . post ( url , json = self . message ) 
~~ except requests . exceptions . HTTPError as e : 
~~~ if method == "/sendMessage" : 
~~ elif method == "/sendDocument" : 
~~ print ( timestamp ( ) , content_type , "sent." ) 
~~ ~~ def send ( self ) : 
~~ self . _send_content ( "/sendMessage" ) 
~~~ if isinstance ( self . attachments , str ) : 
~~ for a in self . attachments : 
~~~ self . message [ "document" ] = a 
self . _send_content ( method = "/sendDocument" ) 
~~ ~~ if self . verbose : 
~~ def send ( self ) : 
url = ( 
"https://api.twilio.com/2010-04-01/Accounts/" 
+ self . _auth [ 0 ] 
+ "/Messages.json" 
"From" : self . from_ , 
"To" : self . to , 
"Body" : self . body , 
"MediaUrl" : self . attachments , 
~~~ resp = requests . post ( url , data = data , auth = ( self . _auth [ 0 ] , self . _auth [ 1 ] ) ) 
~~ except requests . exceptions . RequestException as e : 
~~~ exc = "{}\\n{}" . format ( e , resp . json ( ) [ "message" ] ) 
raise MessageSendError ( exc ) 
~~ self . sid = resp . json ( ) [ "sid" ] 
~~ def get_server ( address = None ) : 
if address : 
~~~ domain = address . split ( "@" ) [ 1 ] 
~~~ return SMTP_SERVERS [ domain ] 
~~~ return ( "smtp." + domain , 465 ) 
~~ ~~ return ( None , None ) 
~~ def _generate_email ( self ) : 
self . message = MIMEMultipart ( ) 
self . _add_header ( ) 
self . _add_body ( ) 
~~ def _add_header ( self ) : 
self . message [ "From" ] = self . from_ 
self . message [ "Subject" ] = self . subject 
if self . to : 
~~~ self . message [ "To" ] = self . list_to_string ( self . to ) 
~~ if self . cc : 
~~~ self . message [ "Cc" ] = self . list_to_string ( self . cc ) 
~~ if self . bcc : 
~~~ self . message [ "Bcc" ] = self . list_to_string ( self . bcc ) 
~~ ~~ def _add_body ( self ) : 
if self . body : 
~~~ b = MIMEText ( "text" , "plain" ) 
b . set_payload ( self . body ) 
self . message . attach ( b ) 
~~ ~~ def _add_attachments ( self ) : 
num_attached = 0 
~~ for item in self . attachments : 
~~~ doc = MIMEApplication ( open ( item , "rb" ) . read ( ) ) 
doc . add_header ( "Content-Disposition" , "attachment" , filename = item ) 
self . message . attach ( doc ) 
num_attached += 1 
~~ ~~ return num_attached 
~~ def _get_session ( self ) : 
if self . port in ( 465 , "465" ) : 
~~~ session = self . _get_ssl ( ) 
~~ elif self . port in ( 587 , "587" ) : 
~~~ session = self . _get_tls ( ) 
~~~ session . login ( self . from_ , self . _auth ) 
~~ except SMTPResponseException as e : 
~~~ raise MessageSendError ( e . smtp_error . decode ( "unicode_escape" ) ) 
~~ return session 
~~ def _get_ssl ( self ) : 
return smtplib . SMTP_SSL ( 
self . server , self . port , context = ssl . create_default_context ( ) 
~~ def _get_tls ( self ) : 
session = smtplib . SMTP ( self . server , self . port ) 
session . ehlo ( ) 
session . starttls ( context = ssl . create_default_context ( ) ) 
self . _generate_email ( ) 
~~ recipients = [ ] 
for i in ( self . to , self . cc , self . bcc ) : 
~~~ if i : 
~~~ if isinstance ( i , MutableSequence ) : 
~~~ recipients += i 
~~~ recipients . append ( i ) 
~~ ~~ ~~ session = self . _get_session ( ) 
~~ session . sendmail ( self . from_ , recipients , self . message . as_string ( ) ) 
session . quit ( ) 
~~ def init_logs ( ) : 
start_time = dt . fromtimestamp ( time . time ( ) ) . strftime ( '%Y%m%d_%H%M' ) 
logname = os . path . join ( os . path . expanduser ( "~" ) + "/nanoGUI_" + start_time + ".log" ) 
handlers = [ logging . FileHandler ( logname ) ] 
logging . basicConfig ( 
handlers = handlers , 
level = logging . INFO ) 
return logname 
~~ def validate_integer ( self , action , index , value_if_allowed , prior_value , 
text , validation_type , trigger_type , widget_name ) : 
if ( action == '1' ) : 
~~~ if text in '0123456789.-+' : 
~~~ int ( value_if_allowed ) 
~~ ~~ def alias_item ( self , alias ) : 
ident = self . alias [ alias ] 
return self . items [ ident ] 
~~ def freeze_dict ( dict_ ) : 
pairs = dict_ . items ( ) 
key_getter = operator . itemgetter ( 0 ) 
return tuple ( sorted ( pairs , key = key_getter ) ) 
~~ def join_html_attrs ( attrs ) : 
attrs = collections . OrderedDict ( freeze_dict ( attrs or { } ) ) 
return template , list ( attrs . values ( ) ) 
appcontext_pushed . connect ( self . initialize_bars , app ) 
app . add_template_global ( self , 'nav' ) 
~~ def initialize_bars ( self , sender = None , ** kwargs ) : 
for bar in self . bars . values ( ) : 
~~~ for initializer in bar . initializers : 
~~~ initializer ( self ) 
~~ ~~ ~~ def bind_bar ( self , sender = None , ** kwargs ) : 
bar = kwargs . pop ( 'bar' ) 
self . bars [ bar . name ] = bar 
~~ def args ( self ) : 
if self . _args is None : 
~~ if callable ( self . _args ) : 
~~~ return dict ( self . _args ( ) ) 
~~ return dict ( self . _args ) 
if self . is_internal : 
~~~ return url_for ( self . endpoint , ** self . args ) 
~~ return self . _url 
~~ def is_current ( self ) : 
if not self . is_internal : 
~~ has_same_endpoint = ( request . endpoint == self . endpoint ) 
has_same_args = ( request . view_args == self . args ) 
return has_same_endpoint and has_same_args 
~~ def validate ( metric_class ) : 
if not hasattr ( metric_class , 'label' ) : 
~~ if not hasattr ( metric_class , 'widget' ) : 
~~ ~~ def get_statistic_by_name ( stat_name ) : 
if stat_name == 'ALL' : 
~~~ return get_statistic_models ( ) 
~~ for stat in get_statistic_models ( ) : 
~~~ if stat . __name__ == stat_name : 
~~~ return stat 
~~ def calculate_statistics ( stat , frequencies ) : 
stats = ensure_list ( stat ) 
frequencies = ensure_list ( frequencies ) 
for stat in stats : 
~~~ for f in frequencies : 
stat . calculate ( f ) 
~~ ~~ ~~ def reset_statistics ( stat , frequencies , reset_cumulative , recalculate = False ) : 
for s in stats : 
~~~ if not s . cumulative or reset_cumulative : 
s . objects . filter ( frequency = f ) . delete ( ) 
~~ elif s . cumulative and not reset_cumulative : 
~~ ~~ ~~ if recalculate : 
calculate_statistics ( stats , frequencies ) 
~~ ~~ def autodiscover ( ) : 
from django . utils . importlib import import_module 
from django . utils . module_loading import module_has_submodule 
~~~ mod = import_module ( app ) 
~~~ import_module ( '%s.gadgets' % app ) 
~~~ if module_has_submodule ( mod , 'gadgets' ) : 
~~ ~~ ~~ ~~ def csv_dump ( request , uid ) : 
metric = Metric . objects . get ( uid = uid ) 
frequency = request . GET . get ( 'frequency' , settings . STATISTIC_FREQUENCY_DAILY ) 
response = HttpResponse ( mimetype = 'text/csv' ) 
writer = csv . writer ( response ) 
for stat in metric . statistics . filter ( frequency = frequency ) . order_by ( 'date_time' ) : 
~~~ writer . writerow ( [ stat . date_time . strftime ( settings . CSV_DATETIME_FORMAT ) , stat . count , stat . cumulative_count ] ) 
~~ def calculate ( cls , frequency = settings . STATISTIC_FREQUENCY_DAILY , verbose = settings . STATISTIC_CALCULATION_VERBOSE ) : 
~~ start_datetime = None 
end_datetime = None 
latest_stat = cls . get_latest ( frequency ) 
now = datetime . now ( ) 
if cls . cumulative : 
~~~ if frequency == settings . STATISTIC_FREQUENCY_HOURLY : 
~~ elif frequency == settings . STATISTIC_FREQUENCY_DAILY : 
~~~ start_datetime = today 
~~ elif frequency == settings . STATISTIC_FREQUENCY_WEEKLY : 
~~ elif frequency == settings . STATISTIC_FREQUENCY_MONTHLY : 
~~ stat , created = cls . objects . get_or_create ( date_time = start_datetime , frequency = frequency ) 
stat . cumulative_count = cls . get_cumulative ( ) 
stat . count = ( stat . cumulative_count - latest_stat . cumulative_count ) if latest_stat else stat . cumulative_count 
~~~ start_datetime = cls . get_start_datetime ( ) if latest_stat is None else latest_stat . date_time 
if frequency == settings . STATISTIC_FREQUENCY_HOURLY : 
end_datetime = start_datetime + timedelta ( hours = 1 ) 
end_datetime = start_datetime + timedelta ( days = 1 ) 
end_datetime = start_datetime + timedelta ( days = 7 ) 
~~ while start_datetime < now : 
~~~ count = cls . get_count ( start_datetime , end_datetime ) 
cumulative_count = 0 
if isinstance ( count , tuple ) : 
~~~ cumulative_count = count [ 1 ] 
count = count [ 0 ] 
~~~ cumulative_count = ( latest_stat . cumulative_count + count ) if latest_stat else count 
stat . count = count 
stat . cumulative_count = cumulative_count 
stat . save ( ) 
latest_stat = stat 
start_datetime = end_datetime 
~~~ end_datetime += timedelta ( hours = 1 ) 
~~~ end_datetime += timedelta ( days = 1 ) 
~~~ end_datetime += timedelta ( days = 7 ) 
~~ ~~ ~~ ~~ def handle ( self , * args , ** kwargs ) : 
frequency = kwargs [ 'frequency' ] 
frequencies = settings . STATISTIC_FREQUENCY_ALL if frequency == 'a' else ( frequency . split ( ',' ) if ',' in frequency else [ frequency ] ) 
if kwargs [ 'list' ] : 
~~~ maintenance . list_statistics ( ) 
~~ elif kwargs [ 'calculate' ] : 
~~~ maintenance . calculate_statistics ( maintenance . get_statistic_by_name ( kwargs [ 'calculate' ] ) , frequencies ) 
~~ elif kwargs [ 'reset' ] : 
~~~ maintenance . reset_statistics ( maintenance . get_statistic_by_name ( kwargs [ 'reset' ] ) , frequencies , kwargs [ 'reset_cumulative' ] ) 
~~ elif kwargs [ 'recalculate' ] : 
~~~ maintenance . reset_statistics ( maintenance . get_statistic_by_name ( kwargs [ 'recalculate' ] ) , frequencies , kwargs [ 'reset_cumulative' ] , True ) 
~~ ~~ def get_GET_array ( request , var_name , fail_silently = True ) : 
vals = request . GET . getlist ( var_name ) 
if not vals : 
~~~ if fail_silently : 
~~ ~~ return vals 
~~ def get_GET_bool ( request , var_name , default = True ) : 
val = request . GET . get ( var_name , default ) 
if isinstance ( val , str ) or isinstance ( val , unicode ) : 
~~~ val = True if val [ 0 ] == 't' else False 
~~ return val 
~~ def get_next_colour ( ) : 
colour = settings . GECKOBOARD_COLOURS [ get_next_colour . cur_colour ] 
get_next_colour . cur_colour += 1 
if get_next_colour . cur_colour >= len ( settings . GECKOBOARD_COLOURS ) : 
~~~ get_next_colour . cur_colour = 0 
~~ return colour 
~~ def get_gecko_params ( request , uid = None , days_back = 0 , cumulative = True , 
frequency = settings . STATISTIC_FREQUENCY_DAILY , min_val = 0 , max_val = 100 , 
chart_type = 'standard' , percentage = 'show' , sort = False ) : 
'days_back' : int ( request . GET . get ( 'daysback' , days_back ) ) , 
'uid' : request . GET . get ( 'uid' , uid ) , 
'uids' : get_GET_array ( request , 'uids[]' ) , 
'cumulative' : get_GET_bool ( request , 'cumulative' , cumulative ) , 
'frequency' : request . GET . get ( 'frequency' , frequency ) , 
'min' : request . GET . get ( 'min' , min_val ) , 
'max' : request . GET . get ( 'max' , max_val ) , 
'type' : request . GET . get ( 'type' , chart_type ) , 
'percentage' : request . GET . get ( 'percentage' , percentage ) , 
'sort' : get_GET_bool ( request , 'sort' , sort ) , 
~~ def geckoboard_number_widget ( request ) : 
params = get_gecko_params ( request , days_back = 7 ) 
metric = Metric . objects . get ( uid = params [ 'uid' ] ) 
~~~ latest_stat = metric . statistics . filter ( frequency = params [ 'frequency' ] ) . order_by ( '-date_time' ) [ 0 ] 
~~~ return ( 0 , 0 ) 
~~~ prev_stat = metric . statistics . filter ( frequency = params [ 'frequency' ] , 
date_time__lte = latest_stat . date_time - timedelta ( days = params [ 'days_back' ] ) ) . order_by ( '-date_time' ) [ 0 ] 
~~~ return ( latest_stat . cumulative_count , 0 ) if params [ 'cumulative' ] else ( latest_stat . count , 0 ) 
~~ return ( latest_stat . cumulative_count , prev_stat . cumulative_count ) if params [ 'cumulative' ] else ( latest_stat . count , prev_stat . count ) 
~~ def geckoboard_rag_widget ( request ) : 
params = get_gecko_params ( request ) 
print params [ 'uids' ] 
max_date = datetime . now ( ) - timedelta ( days = params [ 'days_back' ] ) 
metrics = Metric . objects . filter ( uid__in = params [ 'uids' ] ) 
results = [ ( metric . latest_count ( frequency = params [ 'frequency' ] , count = not params [ 'cumulative' ] , 
cumulative = params [ 'cumulative' ] , max_date = max_date ) , metric . title ) for metric in metrics ] 
return tuple ( results ) 
~~ def geckoboard_pie_chart ( request ) : 
params = get_gecko_params ( request , cumulative = True ) 
from_date = datetime . now ( ) - timedelta ( days = params [ 'days_back' ] ) 
cumulative = params [ 'cumulative' ] ) , metric . title , get_next_colour ( ) ) for metric in metrics ] 
~~ def geckoboard_line_chart ( request ) : 
params = get_gecko_params ( request , cumulative = False , days_back = 7 ) 
start_date = datetime . now ( ) - timedelta ( days = params [ 'days_back' ] ) 
stats = [ s for s in metric . statistics . filter ( frequency = params [ 'frequency' ] , 
date_time__gte = start_date ) . order_by ( 'date_time' ) ] 
if len ( stats ) == 0 : 
~~ dates = [ stats [ 0 ] . date_time ] 
if len ( stats ) >= 3 : 
~~~ mid = len ( stats ) / 2 
if not mid : 
~~~ mid = 1 
~~ dates . extend ( [ stats [ mid ] . date_time , stats [ - 1 ] . date_time ] ) 
~~ elif len ( stats ) == 2 : 
~~~ dates . extend ( [ stats [ - 1 ] . date_time ] ) 
[ s . count for s in stats ] , 
dates , 
metric . title , 
~~ def geckoboard_geckometer ( request ) : 
return ( metric . latest_count ( frequency = params [ 'frequency' ] , count = not params [ 'cumulative' ] , 
cumulative = params [ 'cumulative' ] ) , params [ 'min' ] , params [ 'max' ] ) 
~~ def geckoboard_funnel ( request , frequency = settings . STATISTIC_FREQUENCY_DAILY ) : 
items = [ ( metric . latest_count ( frequency = params [ 'frequency' ] , count = not params [ 'cumulative' ] , 
cumulative = params [ 'cumulative' ] ) , metric . title ) for metric in metrics ] 
'items' : items , 
'type' : params [ 'type' ] , 
'percentage' : params [ 'percentage' ] , 
'sort' : params [ 'sort' ] , 
~~ def get_active_stats ( self ) : 
stats = [ ] 
for gadget in self . _registry . values ( ) : 
~~~ for s in gadget . stats : 
~~~ if s not in stats : 
~~~ stats . append ( s ) 
~~ ~~ ~~ return stats 
~~ def register ( self , gadget ) : 
if gadget in self . _registry : 
~~~ raise AlreadyRegistered 
~~~ self . _registry . append ( gadget ) 
~~ ~~ def unregister ( self , gadgets ) : 
gadgets = maintenance . ensure_list ( gadgets ) 
for gadget in gadgets : 
~~~ while gadget in self . _registry : 
~~~ self . _registry . remove ( gadget ) 
~~ ~~ ~~ def get_context_data ( self , ** kwargs ) : 
'gadgets' : self . _registry , 
'columns' : self . columns , 
'rows' : self . rows , 
'column_ratio' : 100 - self . columns * 2 , 
'row_ratio' : 100 - self . rows * 2 , 
context . update ( kwargs ) 
return context 
~~ def error ( self , message , code = 1 ) : 
print >> sys . stderr , message 
sys . exit ( code ) 
~~ def get_model ( self , model_name ) : 
klass = None 
~~~ module_name , class_name = model_name . rsplit ( '.' , 1 ) 
mod = __import__ ( module_name , fromlist = [ class_name ] ) 
klass = getattr ( mod , class_name ) 
~~ except ImportError , e : 
~~ return klass 
~~ def string_input ( prompt = '' ) : 
v = sys . version [ 0 ] 
if v == '3' : 
~~~ return input ( prompt ) 
~~~ return raw_input ( prompt ) 
showopts = True , qopt = False ) : 
choice = None 
if showopts : 
~~ if qopt : 
~~ while not choice : 
~~ except SyntaxError : 
~~~ if options == [ ] : 
~~ ~~ if choice : 
~~~ if choice in options : 
~~~ return choice 
~~ elif qopt == True and choice == 'q' : 
~~~ choice = None 
if is_sure in ( 'Y' , 'y' , 'yes' ) : 
~~ ~~ elif options == [ ] : 
if options : 
~~ ~~ ~~ elif options == [ ] : 
maxlines = None , maxlength = None ) : 
lines = [ ] 
print ( prompt ) 
lnum = 1 
~~~ if maxlines : 
~~~ if lnum > maxlines : 
~~~ if maxlength : 
~~~ lines . append ( string_input ( '' ) [ : maxlength ] ) 
~~~ lines . append ( string_input ( '' ) ) 
~~ lnum += 1 
~~ ~~ ~~ ~~ except EOFError : 
~~~ return '\\n' . join ( lines ) 
maxitems = None , maxlength = None ) : 
inum = 1 
~~~ if maxitems : 
~~~ if inum > maxitems : 
~~ inum += 1 
~~~ return lines 
~~ ~~ def outfile_input ( extension = None ) : 
fileok = False 
while not fileok : 
~~~ if not filename . endswith ( extension ) : 
~~~ if extension . startswith ( '.' ) : 
~~~ filename = filename + extension 
~~~ filename = filename + '.' + extension 
~~ ~~ ~~ if os . path . isfile ( filename ) : 
options = [ 'y' , 'n' ] ) 
if choice == 'y' : 
~~~ nowtime = time . time ( ) 
with open ( filename , 'a' ) as f : 
~~~ os . utime ( filename , ( nowtime , nowtime ) ) 
~~ fileok = True 
~~ except IOError : 
~~ except PermissionError : 
~~~ choice = choice_input ( 
with open ( filename , 'w' ) as f : 
~~ ~~ ~~ ~~ return filename 
~~ def roster ( self , year ) : 
doc = self . get_year_doc ( year ) 
table = doc ( 'table#roster' ) 
df = sportsref . utils . parse_table ( table ) 
df [ 'years_experience' ] = ( df [ 'years_experience' ] 
. replace ( 'R' , 0 ) . replace ( '' , np . nan ) . astype ( float ) ) 
~~ def schedule ( self , year ) : 
doc = self . get_year_doc ( '{}_games' . format ( year ) ) 
table = doc ( 'table#games' ) 
~~ def date ( self ) : 
match = re . match ( r'(\\d{4})(\\d{2})(\\d{2})' , self . boxscore_id ) 
year , month , day = map ( int , match . groups ( ) ) 
return datetime . date ( year = year , month = month , day = day ) 
~~ def weekday ( self ) : 
days = [ 'Monday' , 'Tuesday' , 'Wednesday' , 'Thursday' , 'Friday' , 
'Saturday' , 'Sunday' ] 
date = self . date ( ) 
wd = date . weekday ( ) 
return days [ wd ] 
~~ def home ( self ) : 
doc = self . get_doc ( ) 
table = doc ( 'table.linescore' ) 
relURL = table ( 'tr' ) . eq ( 2 ) ( 'a' ) . eq ( 2 ) . attr [ 'href' ] 
home = sportsref . utils . rel_url_to_id ( relURL ) 
return home 
~~ def home_score ( self ) : 
home_score = table ( 'tr' ) . eq ( 2 ) ( 'td' ) [ - 1 ] . text_content ( ) 
return int ( home_score ) 
~~ def away_score ( self ) : 
away_score = table ( 'tr' ) . eq ( 1 ) ( 'td' ) [ - 1 ] . text_content ( ) 
return int ( away_score ) 
~~ def winner ( self ) : 
hmScore = self . home_score ( ) 
awScore = self . away_score ( ) 
if hmScore > awScore : 
~~~ return self . home ( ) 
~~ elif hmScore < awScore : 
~~~ return self . away ( ) 
~~ ~~ def week ( self ) : 
match = re . match ( 
r'/years/{}/week_(\\d+)\\.htm' . format ( self . season ( ) ) , raw 
if match : 
~~~ return int ( match . group ( 1 ) ) 
~~~ return 21 
~~ ~~ def season ( self ) : 
return date . year - 1 if date . month <= 3 else date . year 
~~ def starters ( self ) : 
a = doc ( 'table#vis_starters' ) 
h = doc ( 'table#home_starters' ) 
for h , table in enumerate ( ( a , h ) ) : 
~~~ team = self . home ( ) if h else self . away ( ) 
~~~ datum = { } 
datum [ 'player_id' ] = sportsref . utils . rel_url_to_id ( 
row ( 'a' ) [ 0 ] . attrib [ 'href' ] 
datum [ 'playerName' ] = row ( 'th' ) . text ( ) 
datum [ 'position' ] = row ( 'td' ) . text ( ) 
datum [ 'team' ] = team 
datum [ 'home' ] = ( h == 1 ) 
datum [ 'offense' ] = ( i <= 10 ) 
data . append ( datum ) 
~~ ~~ return pd . DataFrame ( data ) 
~~ def surface ( self ) : 
table = doc ( 'table#game_info' ) 
giTable = sportsref . utils . parse_info_table ( table ) 
return giTable . get ( 'surface' , np . nan ) 
~~ def over_under ( self ) : 
if 'over_under' in giTable : 
~~~ ou = giTable [ 'over_under' ] 
return float ( ou . split ( ) [ 0 ] ) 
~~ ~~ def coin_toss ( self ) : 
~~ ~~ def weather ( self ) : 
if 'weather' in giTable : 
~~~ regex = ( 
m = re . match ( regex , giTable [ 'weather' ] ) 
d = m . groupdict ( ) 
for k in d : 
~~~ d [ k ] = int ( d [ k ] ) 
~~ ~~ d [ 'windChill' ] = ( d [ 'windChill' ] if pd . notnull ( d [ 'windChill' ] ) 
else d [ 'temp' ] ) 
d [ 'windMPH' ] = d [ 'windMPH' ] if pd . notnull ( d [ 'windMPH' ] ) else 0 
~~~ return { 
'temp' : 70 , 'windChill' : 70 , 'relHumidity' : None , 'windMPH' : 0 
~~ ~~ def pbp ( self ) : 
table = doc ( 'table#pbp' ) 
df [ 'boxscore_id' ] = self . boxscore_id 
df [ 'home' ] = self . home ( ) 
df [ 'away' ] = self . away ( ) 
df [ 'season' ] = self . season ( ) 
df [ 'week' ] = self . week ( ) 
feats = sportsref . nfl . pbp . expand_details ( df ) 
df = sportsref . nfl . pbp . _add_team_columns ( feats ) 
df [ 'home_wpa' ] = df . home_wp . diff ( ) 
for col in ( 'home_wp' , 'pbp_score_hm' , 'pbp_score_aw' ) : 
~~~ if col in df . columns : 
~~~ df [ col ] = df [ col ] . shift ( 1 ) 
~~ ~~ df . loc [ 0 , [ 'pbp_score_hm' , 'pbp_score_aw' ] ] = 0 
df . home_wp . fillna ( method = 'ffill' , inplace = True ) 
firstPlaysOfGame = df [ df . secsElapsed == 0 ] . index 
line = self . line ( ) 
for i in firstPlaysOfGame : 
~~~ initwp = sportsref . nfl . winProb . initialWinProb ( line ) 
df . loc [ i , 'home_wp' ] = initwp 
df . loc [ i , 'home_wpa' ] = df . loc [ i + 1 , 'home_wp' ] - initwp 
~~ lastPlayIdx = df . index [ - 1 ] 
lastPlayWP = df . loc [ lastPlayIdx , 'home_wp' ] 
winner = self . winner ( ) 
finalWP = 50. if pd . isnull ( winner ) else ( winner == self . home ( ) ) * 100. 
df . loc [ lastPlayIdx , 'home_wpa' ] = finalWP - lastPlayWP 
timeouts = df [ df . isTimeout ] . index 
for to in timeouts : 
~~~ df . loc [ to , 'home_wpa' ] = 0. 
if to + 2 in df . index : 
~~~ wpa = df . loc [ to + 2 , 'home_wp' ] - df . loc [ to + 1 , 'home_wp' ] 
~~~ wpa = finalWP - df . loc [ to + 1 , 'home_wp' ] 
~~ df . loc [ to + 1 , 'home_wpa' ] = wpa 
~~ df = sportsref . nfl . pbp . _add_team_features ( df ) 
df [ 'distToGoal' ] = np . where ( df . isKickoff , 65 , df . distToGoal ) 
df . distToGoal . fillna ( method = 'bfill' , inplace = True ) 
~~ def ref_info ( self ) : 
table = doc ( 'table#officials' ) 
return sportsref . utils . parse_info_table ( table ) 
~~ def player_stats ( self ) : 
tableIDs = ( 'player_offense' , 'player_defense' , 'returns' , 'kicking' ) 
dfs = [ ] 
for tID in tableIDs : 
~~~ table = doc ( 'table#{}' . format ( tID ) ) 
dfs . append ( sportsref . utils . parse_table ( table ) ) 
~~ dfs = [ df for df in dfs if not df . empty ] 
df = reduce ( 
lambda x , y : pd . merge ( 
x , y , how = 'outer' , on = list ( set ( x . columns ) & set ( y . columns ) ) 
) , dfs 
) . reset_index ( drop = True ) 
~~ def snap_counts ( self ) : 
table_ids = ( 'vis_snap_counts' , 'home_snap_counts' ) 
tms = ( self . away ( ) , self . home ( ) ) 
df = pd . concat ( [ 
sportsref . utils . parse_table ( doc ( 'table#{}' . format ( table_id ) ) ) 
. assign ( is_home = bool ( i ) , team = tms [ i ] , opp = tms [ i * - 1 + 1 ] ) 
for i , table_id in enumerate ( table_ids ) 
if df . empty : 
~~ return df . set_index ( 'player_id' ) 
~~ def get_main_doc ( self ) : 
url = ( sportsref . nba . BASE_URL + 
'/leagues/NBA_{}.html' . format ( self . yr ) ) 
return pq ( sportsref . utils . get_html ( url ) ) 
~~ def get_sub_doc ( self , subpage ) : 
html = sportsref . utils . get_html ( self . _subpage_url ( subpage ) ) 
return pq ( html ) 
~~ def get_team_ids ( self ) : 
df = self . team_stats_per_game ( ) 
if not df . empty : 
~~~ return df . index . tolist ( ) 
return [ ] 
~~ ~~ def team_ids_to_names ( self ) : 
doc = self . get_main_doc ( ) 
table = doc ( 'table#team-stats-per_game' ) 
flattened = sportsref . utils . parse_table ( table , flatten = True ) 
unflattened = sportsref . utils . parse_table ( table , flatten = False ) 
team_ids = flattened [ 'team_id' ] 
team_names = unflattened [ 'team_name' ] 
if len ( team_names ) != len ( team_ids ) : 
~~ return dict ( zip ( team_ids , team_names ) ) 
~~ def team_names_to_ids ( self ) : 
d = self . team_ids_to_names ( ) 
return { v : k for k , v in d . items ( ) } 
~~ def schedule ( self , kind = 'R' ) : 
kind = kind . upper ( ) [ 0 ] 
for month in ( 'october' , 'november' , 'december' , 'january' , 'february' , 
'march' , 'april' , 'may' , 'june' ) : 
~~~ doc = self . get_sub_doc ( 'games-{}' . format ( month ) ) 
~~ table = doc ( 'table#schedule' ) 
dfs . append ( df ) 
~~ df = pd . concat ( dfs ) . reset_index ( drop = True ) 
~~~ sportsref . utils . get_html ( '{}/playoffs/NBA_{}.html' . format ( 
sportsref . nba . BASE_URL , self . yr ) 
is_past_season = True 
~~~ is_past_season = False 
~~ if is_past_season : 
~~~ team_per_game = self . team_stats_per_game ( ) 
n_reg_games = int ( team_per_game . g . sum ( ) // 2 ) 
~~~ n_reg_games = len ( df ) 
~~ if kind == 'P' : 
~~~ return df . iloc [ n_reg_games : ] 
~~~ return df . iloc [ : n_reg_games ] 
~~ ~~ def standings ( self ) : 
doc = self . get_sub_doc ( 'standings' ) 
east_table = doc ( 'table#divs_standings_E' ) 
east_df = pd . DataFrame ( sportsref . utils . parse_table ( east_table ) ) 
east_df . sort_values ( 'wins' , ascending = False , inplace = True ) 
east_df [ 'seed' ] = range ( 1 , len ( east_df ) + 1 ) 
east_df [ 'conference' ] = 'E' 
west_table = doc ( 'table#divs_standings_W' ) 
west_df = sportsref . utils . parse_table ( west_table ) 
west_df . sort_values ( 'wins' , ascending = False , inplace = True ) 
west_df [ 'seed' ] = range ( 1 , len ( west_df ) + 1 ) 
west_df [ 'conference' ] = 'W' 
full_df = pd . concat ( [ east_df , west_df ] , axis = 0 ) . reset_index ( drop = True ) 
full_df [ 'team_id' ] = full_df . team_id . str . extract ( r'(\\w+)\\W*\\(\\d+\\)' , expand = False ) 
full_df [ 'gb' ] = [ gb if isinstance ( gb , int ) or isinstance ( gb , float ) else 0 
for gb in full_df [ 'gb' ] ] 
full_df = full_df . drop ( 'has_class_full_table' , axis = 1 ) 
expanded_table = doc ( 'table#expanded_standings' ) 
expanded_df = sportsref . utils . parse_table ( expanded_table ) 
full_df = pd . merge ( full_df , expanded_df , on = 'team_id' ) 
return full_df 
~~ def _get_team_stats_table ( self , selector ) : 
table = doc ( selector ) 
df . set_index ( 'team_id' , inplace = True ) 
~~ def _get_player_stats_table ( self , identifier ) : 
doc = self . get_sub_doc ( identifier ) 
table = doc ( 'table#{}_stats' . format ( identifier ) ) 
~~ def roy_voting ( self ) : 
url = '{}/awards/awards_{}.html' . format ( sportsref . nba . BASE_URL , self . yr ) 
doc = pq ( sportsref . utils . get_html ( url ) ) 
table = doc ( 'table#roy' ) 
~~ def linescore ( self ) : 
table = doc ( 'table#line_score' ) 
columns = [ th . text ( ) for th in table ( 'tr.thead' ) . items ( 'th' ) ] 
columns [ 0 ] = 'team_id' 
data = [ 
[ sportsref . utils . flatten_links ( td ) for td in tr ( 'td' ) . items ( ) ] 
for tr in table ( 'tr.thead' ) . next_all ( 'tr' ) . items ( ) 
return pd . DataFrame ( data , index = [ 'away' , 'home' ] , 
columns = columns , dtype = 'float' ) 
~~ def season ( self ) : 
d = self . date ( ) 
if d . month >= 9 : 
~~~ return d . year + 1 
~~~ return d . year 
~~ ~~ def _get_player_stats ( self , table_id_fmt ) : 
tms = self . away ( ) , self . home ( ) 
tm_ids = [ table_id_fmt . format ( tm ) for tm in tms ] 
tables = [ doc ( 'table#{}' . format ( tm_id ) . lower ( ) ) for tm_id in tm_ids ] 
dfs = [ sportsref . utils . parse_table ( table ) for table in tables ] 
for i , ( tm , df ) in enumerate ( zip ( tms , dfs ) ) : 
~~~ no_time = df [ 'mp' ] == 0 
stat_cols = [ col for col , dtype in df . dtypes . items ( ) 
if dtype != 'object' ] 
df . loc [ no_time , stat_cols ] = 0 
df [ 'team_id' ] = tm 
df [ 'is_home' ] = i == 1 
df [ 'is_starter' ] = [ p < 5 for p in range ( df . shape [ 0 ] ) ] 
df . drop_duplicates ( subset = 'player_id' , keep = 'first' , inplace = True ) 
~~ return pd . concat ( dfs ) 
~~ def pbp ( self , dense_lineups = False , sparse_lineups = False ) : 
~~~ doc = self . get_subpage_doc ( 'pbp' ) 
. format ( self . boxscore_id ) 
~~ table = doc ( 'table#pbp' ) 
trs = [ 
tr for tr in table ( 'tr' ) . items ( ) 
rows = [ tr . children ( 'td' ) for tr in trs ] 
n_rows = len ( trs ) 
cur_qtr = 0 
bsid = self . boxscore_id 
for i in range ( n_rows ) : 
~~~ tr = trs [ i ] 
row = rows [ i ] 
p = { } 
if tr . attr [ 'id' ] and tr . attr [ 'id' ] . startswith ( 'q' ) : 
~~~ assert int ( tr . attr [ 'id' ] [ 1 : ] ) == cur_qtr + 1 
cur_qtr += 1 
~~ t_str = row . eq ( 0 ) . text ( ) 
t_regex = r'(\\d+):(\\d+)\\.(\\d+)' 
mins , secs , tenths = map ( int , re . match ( t_regex , t_str ) . groups ( ) ) 
endQ = ( 12 * 60 * min ( cur_qtr , 4 ) + 
5 * 60 * ( cur_qtr - 4 if cur_qtr > 4 else 0 ) ) 
secsElapsed = endQ - ( 60 * mins + secs + 0.1 * tenths ) 
p [ 'secs_elapsed' ] = secsElapsed 
p [ 'clock_time' ] = t_str 
p [ 'quarter' ] = cur_qtr 
if row . length == 2 : 
~~~ desc = row . eq ( 1 ) 
~~~ p [ 'is_jump_ball' ] = True 
jb_str = sportsref . utils . flatten_links ( desc ) 
p . update ( 
sportsref . nba . pbp . parse_play ( bsid , jb_str , None ) 
. format ( self . boxscore_id , cur_qtr , 
t_str , desc . text ( ) ) 
~~ ~~ elif row . length == 6 : 
~~~ aw_desc , hm_desc = row . eq ( 1 ) , row . eq ( 5 ) 
is_hm_play = bool ( hm_desc . text ( ) ) 
desc = hm_desc if is_hm_play else aw_desc 
desc = sportsref . utils . flatten_links ( desc ) 
new_p = sportsref . nba . pbp . parse_play ( bsid , desc , is_hm_play ) 
if not new_p : 
~~ elif isinstance ( new_p , list ) : 
~~~ orig_p = dict ( p ) 
p . update ( new_p [ 0 ] ) 
data . append ( p ) 
p = orig_p 
new_p = new_p [ 1 ] 
~~ elif new_p . get ( 'is_error' ) : 
~~~ print ( "can\ 
. format ( desc , self . boxscore_id ) ) 
~~ p . update ( new_p ) 
~~~ raise Exception ( ( "don\ 
. format ( row . length ) ) ) 
~~ data . append ( p ) 
~~ df = pd . DataFrame . from_records ( data ) 
df . sort_values ( 'secs_elapsed' , inplace = True , kind = 'mergesort' ) 
df = sportsref . nba . pbp . clean_features ( df ) 
away , home = self . away ( ) , self . home ( ) 
df [ 'home' ] = home 
df [ 'away' ] = away 
df [ 'year' ] = date . year 
df [ 'month' ] = date . month 
df [ 'day' ] = date . day 
def _clean_rebs ( df ) : 
~~~ df . reset_index ( drop = True , inplace = True ) 
no_reb_after = ( 
( df . fta_num < df . tot_fta ) | df . is_ftm | 
df . get ( 'is_tech_fta' , False ) 
) . shift ( 1 ) . fillna ( False ) 
no_reb_before = ( 
( df . fta_num == df . tot_fta ) 
) . shift ( - 1 ) . fillna ( False ) 
se_end_qtr = df . loc [ 
df . clock_time == '0:00.0' , 'secs_elapsed' 
] . unique ( ) 
no_reb_when = df . secs_elapsed . isin ( se_end_qtr ) 
drop_mask = ( 
( df . rebounder == 'Team' ) & 
( no_reb_after | no_reb_before | no_reb_when ) 
) . nonzero ( ) [ 0 ] 
df . drop ( drop_mask , axis = 0 , inplace = True ) 
df . reset_index ( drop = True , inplace = True ) 
~~ df = _clean_rebs ( df ) 
new_poss = ( df . off_team == df . home ) . diff ( ) . fillna ( False ) 
df [ 'poss_id' ] = np . cumsum ( new_poss ) + df . is_dreb 
poss_id_reb = np . cumsum ( new_poss | df . is_reb ) 
sort_cols = [ col for col in 
[ 'is_reb' , 'is_fga' , 'is_pf' , 'is_tech_foul' , 
'is_ejection' , 'is_tech_fta' , 'is_timeout' , 'is_pf_fta' , 
'fta_num' , 'is_viol' , 'is_to' , 'is_jump_ball' , 'is_sub' ] 
if col in df . columns ] 
asc_true = [ 'fta_num' ] 
ascend = [ ( col in asc_true ) for col in sort_cols ] 
for label , group in df . groupby ( [ df . secs_elapsed , poss_id_reb ] ) : 
~~~ if len ( group ) > 1 : 
~~~ df . loc [ group . index , : ] = group . sort_values ( 
sort_cols , ascending = ascend , kind = 'mergesort' 
) . values 
~~ ~~ df = _clean_rebs ( df ) 
df . loc [ df [ 'is_sub' ] , [ 'off_team' , 'def_team' , 'poss_id' ] ] = np . nan 
df . off_team . fillna ( method = 'bfill' , inplace = True ) 
df . def_team . fillna ( method = 'bfill' , inplace = True ) 
df . poss_id . fillna ( method = 'bfill' , inplace = True ) 
if 'is_jump_ball' in df . columns : 
~~~ df . loc [ df [ 'is_jump_ball' ] , [ 'off_team' , 'def_team' ] ] = np . nan 
~~ if 'is_tech_fta' in df . columns : 
~~~ tech_fta = df [ 'is_tech_fta' ] 
df . loc [ tech_fta , 'off_team' ] = df . loc [ tech_fta , 'fta_team' ] 
df . loc [ tech_fta , 'def_team' ] = np . where ( 
df . loc [ tech_fta , 'off_team' ] == home , away , home 
~~ df . drop ( 'fta_team' , axis = 1 , inplace = True ) 
for ( se , tm , pnum ) , group in df [ df . is_sub ] . groupby ( 
[ df . secs_elapsed , df . sub_team , poss_id_reb ] 
~~~ sub_in = set ( ) 
sub_out = set ( ) 
for i , row in group . iterrows ( ) : 
~~~ if row [ 'sub_in' ] in sub_out : 
~~~ sub_out . remove ( row [ 'sub_in' ] ) 
~~~ sub_in . add ( row [ 'sub_in' ] ) 
~~ if row [ 'sub_out' ] in sub_in : 
~~~ sub_in . remove ( row [ 'sub_out' ] ) 
~~~ sub_out . add ( row [ 'sub_out' ] ) 
~~ ~~ assert len ( sub_in ) == len ( sub_out ) 
n_subs = len ( sub_in ) 
for idx , p_in , p_out in zip ( 
group . index [ : n_subs ] , sub_in , sub_out 
~~~ assert df . loc [ idx , 'is_sub' ] 
df . loc [ idx , 'sub_in' ] = p_in 
df . loc [ idx , 'sub_out' ] = p_out 
df . loc [ idx , 'sub_team' ] = tm 
df . loc [ idx , 'detail' ] = ( 
~~ n_extra = len ( group ) - len ( sub_in ) 
if n_extra : 
~~~ extra_idxs = group . index [ - n_extra : ] 
df . drop ( extra_idxs , axis = 0 , inplace = True ) 
~~ ~~ ~~ df . reset_index ( drop = True , inplace = True ) 
df [ 'pts' ] = ( df [ 'is_ftm' ] + 2 * df [ 'is_fgm' ] + 
( df [ 'is_fgm' ] & df [ 'is_three' ] ) ) 
df [ 'hm_pts' ] = np . where ( df . off_team == df . home , df . pts , 0 ) 
df [ 'aw_pts' ] = np . where ( df . off_team == df . away , df . pts , 0 ) 
df [ 'hm_score' ] = np . cumsum ( df [ 'hm_pts' ] ) 
df [ 'aw_score' ] = np . cumsum ( df [ 'aw_pts' ] ) 
new_qtr = df . quarter . diff ( ) . shift ( - 1 ) . fillna ( False ) . astype ( bool ) 
and1 = ( df . is_fgm & df . is_pf . shift ( - 1 ) . fillna ( False ) & 
df . is_fta . shift ( - 2 ) . fillna ( False ) & 
~ df . secs_elapsed . diff ( ) . shift ( - 1 ) . fillna ( False ) . astype ( bool ) ) 
'@double_lane' ) 
df [ 'play_id' ] = np . cumsum ( new_play ) . shift ( 1 ) . fillna ( 0 ) 
df [ 'hm_off' ] = df . off_team == df . home 
if dense_lineups : 
~~~ df = pd . concat ( 
( df , sportsref . nba . pbp . get_dense_lineups ( df ) ) , axis = 1 
~~ if sparse_lineups : 
( df , sportsref . nba . pbp . get_sparse_lineups ( df ) ) , axis = 1 
~~ def switch_to_dir ( dirPath ) : 
~~~ @ funcutils . wraps ( func ) 
~~~ orig_cwd = os . getcwd ( ) 
os . chdir ( dirPath ) 
ret = func ( * args , ** kwargs ) 
os . chdir ( orig_cwd ) 
~~ def cache ( func ) : 
CACHE_DIR = appdirs . user_cache_dir ( 'sportsref' , getpass . getuser ( ) ) 
if not os . path . isdir ( CACHE_DIR ) : 
~~~ os . makedirs ( CACHE_DIR ) 
~~ @ funcutils . wraps ( func ) 
def wrapper ( url ) : 
~~~ file_hash = hashlib . md5 ( ) 
encoded_url = url . encode ( errors = 'replace' ) 
file_hash . update ( encoded_url ) 
file_hash = file_hash . hexdigest ( ) 
filename = '{}/{}' . format ( CACHE_DIR , file_hash ) 
sport_id = None 
for a_base_url , a_sport_id in sportsref . SITE_ABBREV . items ( ) : 
~~~ if url . startswith ( a_base_url ) : 
~~~ sport_id = a_sport_id 
~~ file_exists = os . path . isfile ( filename ) 
if sport_id and file_exists : 
~~~ cur_time = int ( time . time ( ) ) 
mod_time = int ( os . path . getmtime ( filename ) ) 
days_since_mod = datetime . timedelta ( seconds = ( cur_time - mod_time ) ) . days 
days_cache_valid = globals ( ) [ '_days_valid_{}' . format ( sport_id ) ] ( url ) 
cache_is_valid = days_since_mod < days_cache_valid 
~~~ cache_is_valid = False 
~~ allow_caching = sportsref . get_option ( 'cache' ) 
if file_exists and cache_is_valid and allow_caching : 
~~~ with codecs . open ( filename , 'r' , encoding = 'utf-8' , errors = 'replace' ) as f : 
~~~ text = f . read ( ) 
~~~ text = func ( url ) 
with codecs . open ( filename , 'w+' , encoding = 'utf-8' ) as f : 
~~~ f . write ( text ) 
~~ ~~ return text 
~~ def get_class_instance_key ( cls , args , kwargs ) : 
l = [ id ( cls ) ] 
~~~ l . append ( id ( arg ) ) 
~~ l . extend ( ( k , id ( v ) ) for k , v in kwargs . items ( ) ) 
return tuple ( sorted ( l ) ) 
~~ def memoize ( fun ) : 
@ funcutils . wraps ( fun ) 
~~~ do_memoization = sportsref . get_option ( 'memoize' ) 
if not do_memoization : 
~~~ return fun ( * args , ** kwargs ) 
~~ hash_args = tuple ( args ) 
hash_kwargs = frozenset ( sorted ( kwargs . items ( ) ) ) 
key = ( hash_args , hash_kwargs ) 
def _copy ( v ) : 
~~~ if isinstance ( v , pq ) : 
~~~ return v . clone ( ) 
~~~ return copy . deepcopy ( v ) 
~~~ ret = _copy ( cache [ key ] ) 
~~~ cache [ key ] = fun ( * args , ** kwargs ) 
ret = _copy ( cache [ key ] ) 
. format ( fun . __name__ , key ) ) 
~~ ~~ cache = { } 
~~ def age ( self , year , month = 2 , day = 1 ) : 
date_string = doc ( \ ) . attr ( 'data-birth' ) 
regex = r'(\\d{4})\\-(\\d{2})\\-(\\d{2})' 
date_args = map ( int , re . match ( regex , date_string ) . groups ( ) ) 
birth_date = datetime . date ( * date_args ) 
age_date = datetime . date ( year = year , month = month , day = day ) 
delta = age_date - birth_date 
age = delta . days / 365. 
return age 
~~ def height ( self ) : 
raw = doc ( \ ) . text ( ) 
~~~ feet , inches = map ( int , raw . split ( '-' ) ) 
return feet * 12 + inches 
~~ ~~ def weight ( self ) : 
~~~ weight = re . match ( r'(\\d+)lb' , raw ) . group ( 1 ) 
return int ( weight ) 
~~ ~~ def hand ( self ) : 
hand = re . search ( r'Shoots:\\s*(L|R)' , doc . text ( ) ) . group ( 1 ) 
return hand 
~~ def draft_pick ( self ) : 
draft_p_tag = next ( p for p in p_tags . items ( ) if p . text ( ) . lower ( ) . startswith ( 'draft' ) ) 
draft_pick = int ( re . search ( r'(\\d+)\\w{,3}\\s+?overall' , draft_p_tag . text ( ) ) . group ( 1 ) ) 
return draft_pick 
~~ ~~ def _get_stats_table ( self , table_id , kind = 'R' , summary = False ) : 
table_id = 'table#{}{}' . format ( 
'playoffs_' if kind == 'P' else '' , table_id ) 
table = doc ( table_id ) 
df = sportsref . utils . parse_table ( table , flatten = ( not summary ) , 
footer = summary ) 
~~ def stats_per_game ( self , kind = 'R' , summary = False ) : 
return self . _get_stats_table ( 'per_game' , kind = kind , summary = summary ) 
~~ def stats_totals ( self , kind = 'R' , summary = False ) : 
return self . _get_stats_table ( 'totals' , kind = kind , summary = summary ) 
~~ def stats_per36 ( self , kind = 'R' , summary = False ) : 
return self . _get_stats_table ( 'per_minute' , kind = kind , summary = summary ) 
~~ def stats_per100 ( self , kind = 'R' , summary = False ) : 
return self . _get_stats_table ( 'per_poss' , kind = kind , summary = summary ) 
~~ def stats_advanced ( self , kind = 'R' , summary = False ) : 
return self . _get_stats_table ( 'advanced' , kind = kind , summary = summary ) 
~~ def stats_shooting ( self , kind = 'R' , summary = False ) : 
return self . _get_stats_table ( 'shooting' , kind = kind , summary = summary ) 
~~ def stats_pbp ( self , kind = 'R' , summary = False ) : 
return self . _get_stats_table ( 'advanced_pbp' , kind = kind , 
summary = summary ) 
~~ def gamelog_basic ( self , year , kind = 'R' ) : 
doc = self . get_sub_doc ( 'gamelog/{}' . format ( year ) ) 
table = ( doc ( 'table#pgl_basic_playoffs' ) 
if kind == 'P' else doc ( 'table#pgl_basic' ) ) 
~~ def parse_play ( boxscore_id , details , is_hm ) : 
if not details or not isinstance ( details , basestring ) : 
~~ bs = sportsref . nba . BoxScore ( boxscore_id ) 
aw , hm = bs . away ( ) , bs . home ( ) 
season = sportsref . nba . Season ( bs . season ( ) ) 
p [ 'detail' ] = details 
p [ 'home' ] = hm 
p [ 'away' ] = aw 
p [ 'is_home_play' ] = is_hm 
shotRE = r'{0}{1}(?:{2}|{3})?' . format ( shotRE , distRE , assistRE , blockRE ) 
m = re . match ( shotRE , details , re . IGNORECASE ) 
~~~ p [ 'is_fga' ] = True 
p . update ( m . groupdict ( ) ) 
p [ 'shot_dist' ] = p [ 'shot_dist' ] if p [ 'shot_dist' ] is not None else 0 
p [ 'shot_dist' ] = int ( p [ 'shot_dist' ] ) 
p [ 'is_fgm' ] = p [ 'is_fgm' ] == 'makes' 
p [ 'is_three' ] = p [ 'is_three' ] == '3' 
p [ 'is_assist' ] = pd . notnull ( p . get ( 'assister' ) ) 
p [ 'is_block' ] = pd . notnull ( p . get ( 'blocker' ) ) 
shooter_home = p [ 'shooter' ] in hm_roster 
p [ 'off_team' ] = hm if shooter_home else aw 
p [ 'def_team' ] = aw if shooter_home else hm 
. format ( PLAYER_RE ) ) 
m = re . match ( jumpRE , details , re . IGNORECASE ) 
m = re . match ( rebRE , details , re . I ) 
~~~ p [ 'is_reb' ] = True 
p [ 'is_oreb' ] = p [ 'is_oreb' ] . lower ( ) == 'offensive' 
p [ 'is_dreb' ] = not p [ 'is_oreb' ] 
if p [ 'rebounder' ] == 'Team' : 
~~~ p [ 'reb_team' ] , other = ( hm , aw ) if is_hm else ( aw , hm ) 
~~~ reb_home = p [ 'rebounder' ] in hm_roster 
p [ 'reb_team' ] , other = ( hm , aw ) if reb_home else ( aw , hm ) 
~~ p [ 'off_team' ] = p [ 'reb_team' ] if p [ 'is_oreb' ] else other 
p [ 'def_team' ] = p [ 'reb_team' ] if p [ 'is_dreb' ] else other 
m = re . match ( ftRE , details , re . I ) 
~~~ p [ 'is_fta' ] = True 
p [ 'is_ftm' ] = p [ 'is_ftm' ] == 'makes' 
p [ 'is_tech_fta' ] = bool ( p [ 'is_tech_fta' ] ) 
p [ 'is_flag_fta' ] = bool ( p [ 'is_flag_fta' ] ) 
p [ 'is_clearpath_fta' ] = bool ( p [ 'is_clearpath_fta' ] ) 
p [ 'is_pf_fta' ] = not p [ 'is_tech_fta' ] 
if p [ 'tot_fta' ] : 
~~~ p [ 'tot_fta' ] = int ( p [ 'tot_fta' ] ) 
~~ if p [ 'fta_num' ] : 
~~~ p [ 'fta_num' ] = int ( p [ 'fta_num' ] ) 
~~ ft_home = p [ 'ft_shooter' ] in hm_roster 
p [ 'fta_team' ] = hm if ft_home else aw 
if not p [ 'is_tech_fta' ] : 
~~~ p [ 'off_team' ] = hm if ft_home else aw 
p [ 'def_team' ] = aw if ft_home else hm 
~~ return p 
r'(?P<sub_out>{0})' ) . format ( PLAYER_RE ) 
m = re . match ( subRE , details , re . I ) 
~~~ p [ 'is_sub' ] = True 
sub_home = p [ 'sub_in' ] in hm_roster or p [ 'sub_out' ] in hm_roster 
p [ 'sub_team' ] = hm if sub_home else aw 
r'(?P<stealer>{0}))?' ) . format ( PLAYER_RE ) 
r'\\((?:{})\\)' ) . format ( PLAYER_RE , toReasons ) 
m = re . match ( toRE , details , re . I ) 
~~~ p [ 'is_to' ] = True 
p [ 'to_type' ] = p [ 'to_type' ] . lower ( ) 
~~ p [ 'is_steal' ] = pd . notnull ( p [ 'stealer' ] ) 
p [ 'is_travel' ] = p [ 'to_type' ] == 'traveling' 
p [ 'is_carry' ] = p [ 'to_type' ] == 'palming' 
if p [ 'to_by' ] == 'Team' : 
~~~ p [ 'off_team' ] = hm if is_hm else aw 
p [ 'def_team' ] = aw if is_hm else hm 
~~~ to_home = p [ 'to_by' ] in hm_roster 
p [ 'off_team' ] = hm if to_home else aw 
p [ 'def_team' ] = aw if to_home else hm 
m = re . match ( shotFoulRE , details , re . I ) 
~~~ p [ 'is_pf' ] = True 
p [ 'is_shot_foul' ] = True 
p [ 'is_block_foul' ] = bool ( p [ 'is_block_foul' ] ) 
foul_on_home = p [ 'fouler' ] in hm_roster 
p [ 'off_team' ] = aw if foul_on_home else hm 
p [ 'def_team' ] = hm if foul_on_home else aw 
p [ 'foul_team' ] = p [ 'def_team' ] 
m = re . match ( offFoulRE , details , re . I ) 
p [ 'is_off_foul' ] = True 
p [ 'is_to' ] = True 
p [ 'is_charge' ] = bool ( p [ 'is_charge' ] ) 
p [ 'fouler' ] = p [ 'to_by' ] 
p [ 'off_team' ] = hm if foul_on_home else aw 
p [ 'def_team' ] = aw if foul_on_home else hm 
p [ 'foul_team' ] = p [ 'off_team' ] 
r'(?P<drew_foul>{0})\\))?' ) . format ( PLAYER_RE ) 
m = re . match ( foulRE , details , re . I ) 
p [ 'is_take_foul' ] = bool ( p [ 'is_take_foul' ] ) 
m = re . match ( looseBallRE , details , re . I ) 
p [ 'is_loose_ball_foul' ] = True 
foul_home = p [ 'fouler' ] in hm_roster 
p [ 'foul_team' ] = hm if foul_home else aw 
m = re . match ( awayFromBallRE , details , re . I ) 
p [ 'is_away_from_play_foul' ] = True 
p [ 'foul_team' ] = hm if foul_on_home else aw 
m = re . match ( inboundRE , details , re . I ) 
p [ 'is_inbound_foul' ] = True 
m = re . match ( flagrantRE , details , re . I ) 
p [ 'is_flagrant' ] = True 
m = re . match ( clearPathRE , details , re . I ) 
p [ 'is_clear_path_foul' ] = True 
m = re . match ( timeoutRE , details , re . I ) 
~~~ p [ 'is_timeout' ] = True 
isOfficialTO = p [ 'timeout_team' ] . lower ( ) == 'official' 
name_to_id = season . team_names_to_ids ( ) 
p [ 'timeout_team' ] = ( 
'Official' if isOfficialTO else 
name_to_id . get ( hm , name_to_id . get ( aw , p [ 'timeout_team' ] ) ) 
r'(?P<tech_fouler>{0}|Team)' ) . format ( PLAYER_RE ) 
m = re . match ( techRE , details , re . I ) 
~~~ p [ 'is_tech_foul' ] = True 
p [ 'is_hanging' ] = bool ( p [ 'is_hanging' ] ) 
p [ 'is_taunting' ] = bool ( p [ 'is_taunting' ] ) 
p [ 'is_ill_def' ] = bool ( p [ 'is_ill_def' ] ) 
p [ 'is_delay' ] = bool ( p [ 'is_delay' ] ) 
p [ 'is_unsport' ] = bool ( p [ 'is_unsport' ] ) 
foul_on_home = p [ 'tech_fouler' ] in hm_roster 
m = re . match ( ejectRE , details , re . I ) 
~~~ p [ 'is_ejection' ] = True 
if p [ 'ejectee' ] == 'Team' : 
~~~ p [ 'ejectee_team' ] = hm if is_hm else aw 
~~~ eject_home = p [ 'ejectee' ] in hm_roster 
p [ 'ejectee_team' ] = hm if eject_home else aw 
m = re . match ( def3TechRE , details , re . I ) 
p [ 'is_def_three_secs' ] = True 
r'\\((?P<viol_type>.*)\\)' ) . format ( PLAYER_RE ) 
m = re . match ( violRE , details , re . I ) 
~~~ p [ 'is_viol' ] = True 
if p [ 'viol_type' ] == 'kicked_ball' : 
p [ 'to_by' ] = p [ 'violator' ] 
~~ if p [ 'violator' ] == 'Team' : 
~~~ p [ 'viol_team' ] = hm if is_hm else aw 
~~~ viol_home = p [ 'violator' ] in hm_roster 
p [ 'viol_team' ] = hm if viol_home else aw 
~~ p [ 'is_error' ] = True 
~~ def clean_features ( df ) : 
df = pd . DataFrame ( df ) 
bool_vals = set ( [ True , False , None , np . nan ] ) 
sparse_cols = sparse_lineup_cols ( df ) 
for col in df : 
~~~ if set ( df [ col ] . unique ( ) [ : 5 ] ) <= bool_vals : 
~~~ df [ col ] = ( df [ col ] == True ) 
~~ elif col in sparse_cols : 
~~~ df [ col ] = df [ col ] . fillna ( 0 ) 
~~ ~~ df . loc [ df . is_tech_fta , [ 'fta_num' , 'tot_fta' ] ] = 1 
df . off_team . fillna ( method = 'ffill' , inplace = True ) 
df . def_team . fillna ( method = 'ffill' , inplace = True ) 
~~ def clean_multigame_features ( df ) : 
if df . index . value_counts ( ) . max ( ) > 1 : 
~~ df = clean_features ( df ) 
for col in ( 'play_id' , 'poss_id' ) : 
~~~ diffs = df [ col ] . diff ( ) . fillna ( 0 ) 
if ( diffs < 0 ) . any ( ) : 
~~~ new_col = np . cumsum ( diffs . astype ( bool ) ) 
~~ ~~ return df 
~~ def get_period_starters ( df ) : 
def players_from_play ( play ) : 
play [ 'clock_time' ] == '12:00.0' and 
( play . get ( 'is_tech_foul' ) or play . get ( 'is_tech_fta' ) ) 
~~~ return [ ] , [ ] 
~~ stats = sportsref . nba . BoxScore ( play [ 'boxscore_id' ] ) . basic_stats ( ) 
home_grouped = stats . groupby ( 'is_home' ) 
hm_roster = set ( home_grouped . player_id . get_group ( True ) . values ) 
aw_roster = set ( home_grouped . player_id . get_group ( False ) . values ) 
player_keys = [ 
'assister' , 'away_jumper' , 'blocker' , 'drew_foul' , 'fouler' , 
'ft_shooter' , 'gains_poss' , 'home_jumper' , 'rebounder' , 'shooter' , 
'stealer' , 'sub_in' , 'sub_out' , 'to_by' 
players = [ p for p in play [ player_keys ] if pd . notnull ( p ) ] 
aw_players = [ p for p in players if p in aw_roster ] 
hm_players = [ p for p in players if p in hm_roster ] 
return aw_players , hm_players 
~~ n_periods = df . quarter . nunique ( ) 
period_starters = [ ( set ( ) , set ( ) ) for _ in range ( n_periods ) ] 
for qtr , qtr_grp in df . groupby ( df . quarter ) : 
~~~ aw_starters , hm_starters = period_starters [ qtr - 1 ] 
exclude = set ( ) 
for label , time_grp in qtr_grp . groupby ( qtr_grp . secs_elapsed ) : 
~~~ sub_ins = set ( time_grp . sub_in . dropna ( ) . values ) 
exclude . update ( sub_ins - aw_starters - hm_starters ) 
for i , row in time_grp . iterrows ( ) : 
~~~ aw_players , hm_players = players_from_play ( row ) 
aw_starters . update ( aw_players ) 
hm_starters . update ( hm_players ) 
~~ hm_starters -= exclude 
aw_starters -= exclude 
if len ( hm_starters ) > 5 or len ( aw_starters ) > 5 : 
~~~ import ipdb 
ipdb . set_trace ( ) 
~~ if len ( hm_starters ) >= 5 and len ( aw_starters ) >= 5 : 
~~ ~~ if len ( hm_starters ) != 5 or len ( aw_starters ) != 5 : 
. format ( qtr , df . boxscore_id . iloc [ 0 ] ) ) 
~~ ~~ return period_starters 
~~ def get_sparse_lineups ( df ) : 
if ( set ( ALL_LINEUP_COLS ) - set ( df . columns ) ) : 
~~~ lineup_df = get_dense_lineups ( df ) 
~~~ lineup_df = df [ ALL_LINEUP_COLS ] 
~~ hm_lineups = lineup_df [ HM_LINEUP_COLS ] . values 
aw_lineups = lineup_df [ AW_LINEUP_COLS ] . values 
hm_df = pd . DataFrame ( [ 
{ '{}_in' . format ( player_id ) : 1 for player_id in lineup } 
for lineup in hm_lineups 
] , dtype = int ) 
aw_df = pd . DataFrame ( [ 
{ '{}_in' . format ( player_id ) : - 1 for player_id in lineup } 
for lineup in aw_lineups 
sparse_df = pd . concat ( ( hm_df , aw_df ) , axis = 1 ) . fillna ( 0 ) 
return sparse_df 
~~ def get_dense_lineups ( df ) : 
assert df [ 'boxscore_id' ] . nunique ( ) == 1 
def lineup_dict ( aw_lineup , hm_lineup ) : 
'{}_player{}' . format ( tm , i + 1 ) : player 
for tm , lineup in zip ( [ 'aw' , 'hm' ] , [ aw_lineup , hm_lineup ] ) 
for i , player in enumerate ( lineup ) 
~~ def handle_sub ( row , aw_lineup , hm_lineup ) : 
assert row [ 'is_sub' ] 
sub_lineup = hm_lineup if row [ 'sub_team' ] == row [ 'home' ] else aw_lineup 
~~~ idx = sub_lineup . index ( row [ 'sub_out' ] ) 
sub_lineup [ idx ] = row [ 'sub_in' ] 
row [ 'sub_in' ] in sub_lineup 
and row [ 'sub_out' ] not in sub_lineup 
~~~ return aw_lineup , hm_lineup 
. format ( row [ 'boxscore_id' ] , row [ 'quarter' ] , 
row [ 'clock_time' ] , row [ 'detail' ] ) ) 
~~ return aw_lineup , hm_lineup 
~~ per_starters = get_period_starters ( df ) 
aw_lineup , hm_lineup = [ ] , [ ] 
df = df . reset_index ( drop = True ) 
lineups = [ { } for _ in range ( df . shape [ 0 ] ) ] 
sub_or_per_start = df . is_sub | df . quarter . diff ( ) . astype ( bool ) 
for i , row in df . loc [ sub_or_per_start ] . iterrows ( ) : 
~~~ if row [ 'quarter' ] > cur_qtr : 
~~~ assert row [ 'quarter' ] == cur_qtr + 1 
if cur_qtr > 0 and not df . loc [ i - 1 , 'is_sub' ] : 
~~~ lineups [ i - 1 ] = lineup_dict ( aw_lineup , hm_lineup ) 
~~ cur_qtr += 1 
aw_lineup , hm_lineup = map ( list , per_starters [ cur_qtr - 1 ] ) 
lineups [ i ] = lineup_dict ( aw_lineup , hm_lineup ) 
if row [ 'is_sub' ] : 
~~~ aw_lineup , hm_lineup = handle_sub ( row , aw_lineup , hm_lineup ) 
~~~ lineups [ i ] = lineup_dict ( aw_lineup , hm_lineup ) 
~~ ~~ ~~ lineup_df = pd . DataFrame ( lineups ) 
if lineup_df . iloc [ - 1 ] . isnull ( ) . all ( ) : 
~~~ lineup_df . iloc [ - 1 ] = lineup_dict ( aw_lineup , hm_lineup ) 
~~ lineup_df = lineup_df . groupby ( df . quarter ) . fillna ( method = 'bfill' ) 
bool_mat = lineup_df . isnull ( ) 
mask = bool_mat . any ( axis = 1 ) 
if mask . any ( ) : 
~~~ bs = sportsref . nba . BoxScore ( df . boxscore_id [ 0 ] ) 
stats = sportsref . nba . BoxScore ( df . boxscore_id . iloc [ 0 ] ) . basic_stats ( ) 
true_mp = pd . Series ( 
. set_index ( 'player_id' ) . to_dict ( ) [ 'mp' ] 
) * 60 
calc_mp = pd . Series ( 
{ p : ( df . secs_elapsed . diff ( ) * 
[ p in row for row in lineup_df . values ] ) . sum ( ) 
diff = true_mp - calc_mp 
players_missing = diff . loc [ diff . abs ( ) >= 150 ] 
missing_df = pd . DataFrame ( 
{ 'secs' : players_missing . values , 
'is_home' : players_missing . index . isin ( hm_roster ) } , 
index = players_missing . index 
if missing_df . empty : 
~~~ for is_home , group in missing_df . groupby ( 'is_home' ) : 
~~~ player_id = group . index . item ( ) 
tm_cols = ( sportsref . nba . pbp . HM_LINEUP_COLS if is_home else 
sportsref . nba . pbp . AW_LINEUP_COLS ) 
row_mask = lineup_df [ tm_cols ] . isnull ( ) . any ( axis = 1 ) 
lineup_df . loc [ row_mask , tm_cols ] = ( 
lineup_df . loc [ row_mask , tm_cols ] . fillna ( player_id ) . values 
~~ ~~ ~~ return lineup_df 
~~ def GamePlayFinder ( ** kwargs ) : 
querystring = _kwargs_to_qs ( ** kwargs ) 
url = '{}?{}' . format ( GPF_URL , querystring ) 
if kwargs . get ( 'verbose' , False ) : 
~~~ print ( url ) 
~~ html = utils . get_html ( url ) 
doc = pq ( html ) 
table = doc ( 'table#all_plays' ) 
plays = utils . parse_table ( table ) 
if 'score' in plays . columns : 
~~~ oScore , dScore = zip ( * plays . score . apply ( lambda s : s . split ( '-' ) ) ) 
plays [ 'teamScore' ] = oScore 
plays [ 'oppScore' ] = dScore 
~~ if 'description' in plays . columns : 
~~~ plays = pbp . expand_details ( plays , detailCol = 'description' ) 
~~ return plays 
~~ def _kwargs_to_qs ( ** kwargs ) : 
inpOptDef = inputs_options_defaults ( ) 
name : dct [ 'value' ] 
for name , dct in inpOptDef . items ( ) 
~~~ if k . lower ( ) in ( 'pid' , 'playerid' ) : 
~~~ del kwargs [ k ] 
kwargs [ 'player_id' ] = v 
~~ if k == 'player_id' : 
~~~ if v . startswith ( '/players/' ) : 
~~~ kwargs [ k ] = utils . rel_url_to_id ( v ) 
~~ ~~ if isinstance ( v , bool ) : 
~~~ kwargs [ k ] = 'Y' if v else 'N' 
~~ if k . lower ( ) in ( 'tm' , 'team' ) : 
kwargs [ 'team_id' ] = v 
~~ if k . lower ( ) in ( 'yr_min' , 'yr_max' ) : 
if k . lower ( ) == 'yr_min' : 
~~~ kwargs [ 'year_min' ] = int ( v ) 
~~~ kwargs [ 'year_max' ] = int ( v ) 
~~ ~~ if k . lower ( ) in ( 'wk_min' , 'wk_max' ) : 
if k . lower ( ) == 'wk_min' : 
~~~ kwargs [ 'week_num_min' ] = int ( v ) 
~~~ kwargs [ 'week_num_max' ] = int ( v ) 
~~ ~~ if k . lower ( ) in ( 'yr' , 'year' , 'yrs' , 'years' ) : 
if isinstance ( v , collections . Iterable ) : 
~~~ lst = list ( v ) 
kwargs [ 'year_min' ] = min ( lst ) 
kwargs [ 'year_max' ] = max ( lst ) 
~~ elif isinstance ( v , basestring ) : 
~~~ v = list ( map ( int , v . split ( ',' ) ) ) 
kwargs [ 'year_min' ] = min ( v ) 
kwargs [ 'year_max' ] = max ( v ) 
~~~ kwargs [ 'year_min' ] = v 
kwargs [ 'year_max' ] = v 
~~ ~~ if k . lower ( ) in ( 'wk' , 'week' , 'wks' , 'weeks' ) : 
kwargs [ 'week_num_min' ] = min ( lst ) 
kwargs [ 'week_num_max' ] = max ( lst ) 
kwargs [ 'week_num_min' ] = min ( v ) 
kwargs [ 'week_num_max' ] = max ( v ) 
~~~ kwargs [ 'week_num_min' ] = v 
kwargs [ 'week_num_max' ] = v 
~~ ~~ if k == 'playoff_round' : 
~~~ kwargs [ 'game_type' ] = 'P' 
~~ if isinstance ( v , basestring ) : 
~~~ v = v . split ( ',' ) 
~~ if not isinstance ( v , collections . Iterable ) : 
~~~ v = [ v ] 
~~ ~~ for k in kwargs : 
~~~ if k in opts : 
~~~ opts [ k ] = [ ] 
~~ ~~ for k , v in kwargs . items ( ) : 
~~~ if isinstance ( v , basestring ) : 
~~ elif not isinstance ( v , collections . Iterable ) : 
~~ for val in v : 
~~~ opts [ k ] . append ( val ) 
~~ ~~ ~~ opts [ 'request' ] = [ 1 ] 
qs = '&' . join ( '{}={}' . format ( name , val ) 
for name , vals in sorted ( opts . items ( ) ) for val in vals ) 
return qs 
~~ def inputs_options_defaults ( ) : 
if os . path . isfile ( GPF_CONSTANTS_FILENAME ) : 
~~~ modtime = int ( os . path . getmtime ( GPF_CONSTANTS_FILENAME ) ) 
curtime = int ( time . time ( ) ) 
~~ if ( os . path . isfile ( GPF_CONSTANTS_FILENAME ) 
and curtime - modtime <= 7 * 24 * 60 * 60 ) : 
~~~ with open ( GPF_CONSTANTS_FILENAME , 'r' ) as const_f : 
~~~ def_dict = json . load ( const_f ) 
html = utils . get_html ( GPF_URL ) 
def_dict = { } 
~~~ name = inp . attrib [ 'name' ] 
if name not in def_dict : 
~~~ def_dict [ name ] = { 
'value' : set ( ) , 
'options' : set ( ) , 
'type' : inp . type 
~~ val = inp . attrib . get ( 'value' , '' ) 
if inp . type in ( 'checkbox' , 'radio' ) : 
~~~ if 'checked' in inp . attrib : 
~~~ def_dict [ name ] [ 'value' ] . add ( val ) 
~~ def_dict [ name ] [ 'options' ] . add ( val ) 
~~~ name = sel . attr [ 'name' ] 
'type' : 'select' 
~~ defaultOpt = sel ( 'option[selected]' ) 
if len ( defaultOpt ) : 
~~~ defaultOpt = defaultOpt [ 0 ] 
def_dict [ name ] [ 'value' ] . add ( defaultOpt . attrib . get ( 'value' , '' ) ) 
~~~ def_dict [ name ] [ 'value' ] . add ( 
sel ( 'option' ) [ 0 ] . attrib . get ( 'value' , '' ) 
~~ def_dict [ name ] [ 'options' ] = { 
opt . attrib [ 'value' ] for opt in sel ( 'option' ) 
if opt . attrib . get ( 'value' ) 
~~ def_dict [ 'include_kneels' ] [ 'value' ] = [ '0' ] 
def_dict . pop ( 'request' , None ) 
def_dict . pop ( 'use_favorites' , None ) 
with open ( GPF_CONSTANTS_FILENAME , 'w+' ) as f : 
~~~ for k in def_dict : 
~~~ def_dict [ k ] [ 'value' ] = sorted ( 
list ( def_dict [ k ] [ 'value' ] ) , key = int 
def_dict [ k ] [ 'options' ] = sorted ( 
list ( def_dict [ k ] [ 'options' ] ) , key = int 
~~~ def_dict [ k ] [ 'value' ] = sorted ( list ( def_dict [ k ] [ 'value' ] ) ) 
list ( def_dict [ k ] [ 'options' ] ) 
~~ ~~ json . dump ( def_dict , f ) 
~~ ~~ return def_dict 
~~ def expand_details ( df , detailCol = 'detail' ) : 
df = copy . deepcopy ( df ) 
df [ 'detail' ] = df [ detailCol ] 
dicts = [ sportsref . nfl . pbp . parse_play_details ( detail ) for detail in df [ 'detail' ] . values ] 
cols = { c for d in dicts if d for c in d . keys ( ) } 
blankEntry = { c : np . nan for c in cols } 
newDicts = [ d if d else blankEntry for d in dicts ] 
details = pd . DataFrame ( newDicts ) 
df = pd . merge ( df , details , left_index = True , right_index = True ) 
errors = [ i for i , d in enumerate ( dicts ) if d is None ] 
df [ 'isError' ] = False 
df . loc [ errors , 'isError' ] = True 
df . loc [ 0 , 'qtr_time_remain' ] = '15:00' 
df . qtr_time_remain . fillna ( method = 'bfill' , inplace = True ) 
df . qtr_time_remain . fillna ( 
pd . Series ( np . where ( df . quarter == 4 , '0:00' , '15:00' ) ) , inplace = True 
new_df = df . apply ( _clean_features , axis = 1 ) 
return new_df 
~~ def parse_play_details ( details ) : 
if not isinstance ( details , basestring ) : 
~~ rushOptRE = r'(?P<rushDir>{})' . format ( 
r'|' . join ( RUSH_OPTS . keys ( ) ) 
passOptRE = r'(?P<passLoc>{})' . format ( 
r'|' . join ( PASS_OPTS . keys ( ) ) 
playerRE = r"\\S{6,8}\\d{2}" 
struct = { } 
challengeRE = re . compile ( 
'(?P<callUpheld>upheld|overturned)\\.' , 
re . IGNORECASE 
match = challengeRE . search ( details ) 
~~~ struct [ 'isChallenge' ] = True 
struct . update ( match . groupdict ( ) ) 
if 'overturned' in details : 
~~~ overturnedIdx = details . index ( 'overturned.' ) 
newStart = overturnedIdx + len ( 'overturned.' ) 
details = details [ newStart : ] . strip ( ) 
~~~ struct [ 'isChallenge' ] = False 
~~ struct [ 'isLateral' ] = details . find ( 'lateral' ) != - 1 
rusherRE = r"(?P<rusher>{0})" . format ( playerRE ) 
. format ( playerRE ) ) 
fumbleRE = ( 
r"(?:" 
r"(?:(?P<fumbRecFieldSide>[a-z]+)?\\-?(?P<fumbRecYdLine>\\-?\\d+))?" 
r")?" 
penaltyRE = ( r"(?:.*?" 
r"(?P<penalty>[^\\(,]+)" 
rushREstr = ( 
) . format ( rusherRE , rushOptRE , rushYardsRE , tackleRE , fumbleRE , tdSafetyRE , 
penaltyRE ) 
rushRE = re . compile ( rushREstr , re . IGNORECASE ) 
passerRE = r"(?P<passer>{0})" . format ( playerRE ) 
playerRE ) 
+ r'(?:(?P<intFieldSide>[a-z]*)?\\-?(?P<intYdLine>\\-?\\d*))?' 
throwRE = r'(?:{}{}{}(?:(?:{}|{}){})?)' . format ( 
completeRE , passOptRE , targetedRE , passYardsRE , intRE , tackleRE 
passREstr = ( 
) . format ( passerRE , sackRE , throwRE , fumbleRE , tdSafetyRE , penaltyRE ) 
passRE = re . compile ( passREstr , re . IGNORECASE ) 
koKickerRE = r'(?P<koKicker>{0})' . format ( playerRE ) 
nextREs = [ ] 
nextREs . append ( 
nextRE = '' . join ( r'(?:{})?' . format ( nre ) for nre in nextREs ) 
kickoffREstr = r'{}{}{}{}{}{}{}' . format ( 
koKickerRE , koYardsRE , nextRE , 
tackleRE , fumbleRE , tdSafetyRE , penaltyRE 
kickoffRE = re . compile ( kickoffREstr , re . IGNORECASE ) 
timeoutRE = re . compile ( timeoutREstr , re . IGNORECASE ) 
fgKickerRE = r'(?P<fgKicker>{0})' . format ( playerRE ) 
fgBlockRE = ( 
r'(?P<fgBlocker>{0}))?' . format ( playerRE ) + 
fgREstr = r'{}{}{}{}{}' . format ( fgKickerRE , fgBaseRE , 
fgBlockRE , tdSafetyRE , penaltyRE ) 
fgRE = re . compile ( fgREstr , re . IGNORECASE ) 
punterRE = r'.*?(?P<punter>{0})' . format ( playerRE ) 
puntBlockRE = ( 
nextRE = r'(?:{})?' . format ( '|' . join ( nextREs ) ) 
puntREstr = r'{}(?:{}|{}){}{}{}{}{}' . format ( 
punterRE , puntBlockRE , puntYdsRE , nextRE , 
puntRE = re . compile ( puntREstr , re . IGNORECASE ) 
kneelRE = re . compile ( kneelREstr , re . IGNORECASE ) 
spikeRE = re . compile ( spikeREstr , re . IGNORECASE ) 
extraPointRE = re . compile ( extraPointREstr , re . IGNORECASE ) 
twoPointREstr = ( 
r'(?P<twoPointSuccess>succeeds|fails)' 
twoPointRE = re . compile ( twoPointREstr , re . IGNORECASE ) 
psPenaltyREstr = ( 
psPenaltyRE = re . compile ( psPenaltyREstr , re . IGNORECASE ) 
match = kickoffRE . search ( details ) 
~~~ struct [ 'isKickoff' ] = True 
return struct 
~~ match = timeoutRE . search ( details ) 
~~~ struct [ 'isTimeout' ] = True 
~~ match = fgRE . search ( details ) 
~~~ struct [ 'isFieldGoal' ] = True 
~~ match = puntRE . search ( details ) 
~~~ struct [ 'isPunt' ] = True 
~~ match = kneelRE . search ( details ) 
~~~ struct [ 'isKneel' ] = True 
~~ match = spikeRE . search ( details ) 
~~~ struct [ 'isSpike' ] = True 
~~ match = extraPointRE . search ( details ) 
~~~ struct [ 'isXP' ] = True 
~~ match = twoPointRE . search ( details ) 
~~~ struct [ 'isTwoPoint' ] = True 
struct [ 'twoPointSuccess' ] = match . group ( 'twoPointSuccess' ) 
realPlay = sportsref . nfl . pbp . parse_play_details ( 
match . group ( 'twoPoint' ) ) 
if realPlay : 
~~~ struct . update ( realPlay ) 
~~ return struct 
~~ match = passRE . search ( details ) 
~~~ struct [ 'isPass' ] = True 
~~ match = psPenaltyRE . search ( details ) 
~~~ struct [ 'isPresnapPenalty' ] = True 
~~ match = rushRE . search ( details ) 
~~~ struct [ 'isRun' ] = True 
~~ def _clean_features ( struct ) : 
struct = dict ( struct ) 
ptypes = [ 'isKickoff' , 'isTimeout' , 'isFieldGoal' , 'isPunt' , 'isKneel' , 
'isSpike' , 'isXP' , 'isTwoPoint' , 'isPresnapPenalty' , 'isPass' , 
'isRun' ] 
for pt in ptypes : 
~~~ struct [ pt ] = struct [ pt ] if pd . notnull ( struct . get ( pt ) ) else False 
~~ struct [ 'callUpheld' ] = struct . get ( 'callUpheld' ) == 'upheld' 
struct [ 'fgGood' ] = struct . get ( 'fgGood' ) == 'good' 
struct [ 'isBlocked' ] = struct . get ( 'isBlocked' ) == 'blocked' 
struct [ 'isComplete' ] = struct . get ( 'isComplete' ) == 'complete' 
struct [ 'isMuffedCatch' ] = pd . notnull ( struct . get ( 'isMuffedCatch' ) ) 
struct [ 'isNoPlay' ] = ( 
if struct . get ( 'detail' ) else False ) 
struct [ 'isOnside' ] = struct . get ( 'isOnside' ) == 'onside' 
struct [ 'isSack' ] = pd . notnull ( struct . get ( 'sackYds' ) ) 
( struct . get ( 'detail' ) and 
struct [ 'oob' ] = pd . notnull ( struct . get ( 'oob' ) ) 
struct [ 'passLoc' ] = PASS_OPTS . get ( struct . get ( 'passLoc' ) , np . nan ) 
if struct [ 'isPass' ] : 
~~~ pyds = struct [ 'passYds' ] 
struct [ 'passYds' ] = pyds if pd . notnull ( pyds ) else 0 
~~ if pd . notnull ( struct [ 'penalty' ] ) : 
~~~ struct [ 'penalty' ] = struct [ 'penalty' ] . strip ( ) 
~~ struct [ 'penDeclined' ] = struct . get ( 'penDeclined' ) == 'Declined' 
if struct [ 'quarter' ] == 'OT' : 
~~~ struct [ 'quarter' ] = 5 
~~ struct [ 'rushDir' ] = RUSH_OPTS . get ( struct . get ( 'rushDir' ) , np . nan ) 
if struct [ 'isRun' ] : 
~~~ ryds = struct [ 'rushYds' ] 
struct [ 'rushYds' ] = ryds if pd . notnull ( ryds ) else 0 
~~ year = struct . get ( 'season' , np . nan ) 
struct [ 'timeoutTeam' ] = sportsref . nfl . teams . team_ids ( year ) . get ( 
struct . get ( 'timeoutTeam' ) , np . nan 
struct [ 'twoPointSuccess' ] = struct . get ( 'twoPointSuccess' ) == 'succeeds' 
struct [ 'xpGood' ] = struct . get ( 'xpGood' ) == 'good' 
bool_vars = [ 
'fgGood' , 'isBlocked' , 'isChallenge' , 'isComplete' , 'isFairCatch' , 
'isFieldGoal' , 'isKickoff' , 'isKneel' , 'isLateral' , 'isNoPlay' , 
'isPass' , 'isPresnapPenalty' , 'isPunt' , 'isRun' , 'isSack' , 'isSafety' , 
'isSpike' , 'isTD' , 'isTimeout' , 'isTouchback' , 'isTwoPoint' , 'isXP' , 
'isMuffedCatch' , 'oob' , 'penDeclined' , 'twoPointSuccess' , 'xpGood' 
int_vars = [ 
'down' , 'fgBlockRetYds' , 'fgDist' , 'fumbRecYdLine' , 'fumbRetYds' , 
'intRetYds' , 'intYdLine' , 'koRetYds' , 'koYds' , 'muffRetYds' , 
'pbp_score_aw' , 'pbp_score_hm' , 'passYds' , 'penYds' , 'puntBlockRetYds' , 
'puntRetYds' , 'puntYds' , 'quarter' , 'rushYds' , 'sackYds' , 'timeoutNum' , 
'ydLine' , 'yds_to_go' 
float_vars = [ 
'exp_pts_after' , 'exp_pts_before' , 'home_wp' 
string_vars = [ 
'challenger' , 'detail' , 'fairCatcher' , 'fgBlockRecoverer' , 
'fgBlocker' , 'fgKicker' , 'fieldSide' , 'fumbForcer' , 
'fumbRecFieldSide' , 'fumbRecoverer' , 'fumbler' , 'intFieldSide' , 
'interceptor' , 'kneelQB' , 'koKicker' , 'koReturner' , 'muffRecoverer' , 
'muffedBy' , 'passLoc' , 'passer' , 'penOn' , 'penalty' , 
'puntBlockRecoverer' , 'puntBlocker' , 'puntReturner' , 'punter' , 
'qtr_time_remain' , 'rushDir' , 'rusher' , 'sacker1' , 'sacker2' , 
'spikeQB' , 'tackler1' , 'tackler2' , 'target' , 'timeoutTeam' , 
'xpKicker' 
for var in bool_vars : 
~~~ struct [ var ] = struct . get ( var ) is True 
~~ for var in int_vars : 
~~~ struct [ var ] = int ( struct . get ( var ) ) 
~~~ struct [ var ] = np . nan 
~~ ~~ for var in float_vars : 
~~~ struct [ var ] = float ( struct . get ( var ) ) 
~~ ~~ for var in string_vars : 
~~~ if var not in struct or pd . isnull ( struct [ var ] ) or var == '' : 
~~ ~~ if struct [ 'isXP' ] : 
~~~ struct [ 'fieldSide' ] = struct [ 'ydLine' ] = np . nan 
~~~ fieldSide , ydline = _loc_to_features ( struct . get ( 'location' ) ) 
struct [ 'fieldSide' ] = fieldSide 
struct [ 'ydLine' ] = ydline 
~~ if pd . notnull ( struct . get ( 'qtr_time_remain' ) ) : 
~~~ qtr = struct [ 'quarter' ] 
mins , secs = map ( int , struct [ 'qtr_time_remain' ] . split ( ':' ) ) 
struct [ 'secsElapsed' ] = qtr * 900 - mins * 60 - secs 
~~ struct [ 'isInt' ] = pd . notnull ( struct . get ( 'interceptor' ) ) 
struct [ 'isFumble' ] = pd . notnull ( struct . get ( 'fumbler' ) ) 
struct [ 'isPenalty' ] = pd . notnull ( struct . get ( 'penalty' ) ) 
struct [ 'team_epa' ] = struct [ 'exp_pts_after' ] - struct [ 'exp_pts_before' ] 
struct [ 'opp_epa' ] = struct [ 'exp_pts_before' ] - struct [ 'exp_pts_after' ] 
return pd . Series ( struct ) 
~~ def _loc_to_features ( loc ) : 
if loc : 
~~~ if isinstance ( loc , basestring ) : 
~~~ loc = loc . strip ( ) 
~~~ r = loc . split ( ) 
r [ 0 ] = r [ 0 ] . lower ( ) 
r [ 1 ] = int ( r [ 1 ] ) 
~~~ r = ( np . nan , int ( loc ) ) 
~~ ~~ elif isinstance ( loc , float ) : 
~~~ return ( np . nan , 50 ) 
~~~ r = ( np . nan , np . nan ) 
~~ return r 
~~ def _add_team_columns ( features ) : 
features = features . to_dict ( 'records' ) 
curTm = curOpp = None 
playAfterKickoff = False 
for row in features : 
~~~ if row [ 'isKickoff' ] or playAfterKickoff : 
~~~ curTm , curOpp = _team_and_opp ( row ) 
~~~ curTm , curOpp = _team_and_opp ( row , curTm , curOpp ) 
~~ row [ 'team' ] , row [ 'opp' ] = curTm , curOpp 
playAfterKickoff = row [ 'isKickoff' ] 
~~ features = pd . DataFrame ( features ) 
features . team . fillna ( method = 'bfill' , inplace = True ) 
features . opp . fillna ( method = 'bfill' , inplace = True ) 
features . team . fillna ( method = 'ffill' , inplace = True ) 
features . opp . fillna ( method = 'ffill' , inplace = True ) 
~~ def _team_and_opp ( struct , curTm = None , curOpp = None ) : 
if pd . isnull ( curTm ) : 
~~~ if struct [ 'isRun' ] : 
~~~ pID = struct [ 'rusher' ] 
~~ elif struct [ 'isPass' ] : 
~~~ pID = struct [ 'passer' ] 
~~ elif struct [ 'isFieldGoal' ] : 
~~~ pID = struct [ 'fgKicker' ] 
~~ elif struct [ 'isPunt' ] : 
~~~ pID = struct [ 'punter' ] 
~~ elif struct [ 'isXP' ] : 
~~~ pID = struct [ 'xpKicker' ] 
~~ elif struct [ 'isKickoff' ] : 
~~~ pID = struct [ 'koKicker' ] 
~~ elif struct [ 'isSpike' ] : 
~~~ pID = struct [ 'spikeQB' ] 
~~ elif struct [ 'isKneel' ] : 
~~~ pID = struct [ 'kneelQB' ] 
~~~ pID = None 
~~ curTm = curOpp = np . nan 
bs = sportsref . nfl . boxscores . BoxScore ( struct [ 'boxscore_id' ] ) 
if pID and len ( pID ) == 3 : 
~~~ curTm = pID 
curOpp = bs . away ( ) if bs . home ( ) == curTm else bs . home ( ) 
~~ elif pID : 
~~~ player = sportsref . nfl . Player ( pID ) 
gamelog = player . gamelog ( kind = 'B' ) 
curTm = gamelog . loc [ 
gamelog . boxscore_id == struct [ 'boxscore_id' ] , 'team_id' 
] . item ( ) 
curOpp = bs . home ( ) if bs . home ( ) != curTm else bs . away ( ) 
~~ return curTm , curOpp 
~~ if struct [ 'has_class_divider' ] : 
~~~ return curOpp , curTm 
~~~ return curTm , curOpp 
~~ ~~ def _add_team_features ( df ) : 
assert df . team . notnull ( ) . all ( ) 
homeOnOff = df [ 'team' ] == df [ 'home' ] 
df [ 'distToGoal' ] = np . where ( df [ 'team' ] != df [ 'fieldSide' ] , 
df [ 'ydLine' ] , 100 - df [ 'ydLine' ] ) 
df [ 'distToGoal' ] = np . where ( df [ 'isXP' ] | df [ 'isTwoPoint' ] , 
2 , df [ 'distToGoal' ] ) 
df [ 'team_wp' ] = np . where ( homeOnOff , df [ 'home_wp' ] , 100. - df [ 'home_wp' ] ) 
df [ 'opp_wp' ] = 100. - df [ 'team_wp' ] 
df [ 'team_wpa' ] = np . where ( homeOnOff , df [ 'home_wpa' ] , - df [ 'home_wpa' ] ) 
df [ 'opp_wpa' ] = - df [ 'team_wpa' ] 
bs_id = df [ 'boxscore_id' ] . values [ 0 ] 
bs = sportsref . nfl . boxscores . BoxScore ( bs_id ) 
df [ 'team_score' ] = np . where ( df [ 'team' ] == bs . home ( ) , 
df [ 'pbp_score_hm' ] , df [ 'pbp_score_aw' ] ) 
df [ 'opp_score' ] = np . where ( df [ 'team' ] == bs . home ( ) , 
df [ 'pbp_score_aw' ] , df [ 'pbp_score_hm' ] ) 
~~ def _get_player_stats_table ( self , subpage , table_id ) : 
doc = self . get_sub_doc ( subpage ) 
table = doc ( 'table#{}' . format ( table_id ) ) 
~~ def initialWinProb ( line ) : 
line = float ( line ) 
probWin = 1. - norm . cdf ( 0.5 , - line , 13.86 ) 
probTie = norm . cdf ( 0.5 , - line , 13.86 ) - norm . cdf ( - 0.5 , - line , 13.86 ) 
return 100. * ( probWin + 0.5 * probTie ) 
~~ def gamelog ( self , year = None , kind = 'R' ) : 
table = ( doc ( 'table#stats' ) if kind == 'R' else 
doc ( 'table#stats_playoffs' ) ) 
if year is not None : 
~~ def passing ( self , kind = 'R' ) : 
table = ( doc ( 'table#passing' ) if kind == 'R' else 
doc ( 'table#passing_playoffs' ) ) 
~~ def rushing_and_receiving ( self , kind = 'R' ) : 
table = ( doc ( 'table#rushing_and_receiving' ) if kind == 'R' 
else doc ( 'table#rushing_and_receiving_playoffs' ) ) 
if not table : 
~~~ table = ( doc ( 'table#receiving_and_rushing' ) if kind == 'R' 
else doc ( 'table#receiving_and_rushing_playoffs' ) ) 
~~ df = sportsref . utils . parse_table ( table ) 
~~ def _plays ( self , year , play_type , expand_details ) : 
url = self . _subpage_url ( '{}-plays' . format ( play_type ) , year ) 
if table : 
~~~ if expand_details : 
~~~ plays = sportsref . nfl . pbp . expand_details ( 
sportsref . utils . parse_table ( table ) , detailCol = 'description' 
return plays 
~~~ return sportsref . utils . parse_table ( table ) 
~~ ~~ def advanced_splits ( self , year = None ) : 
url = self . _subpage_url ( 'splits' , year ) 
table = doc ( 'table#advanced_splits' ) 
~~~ df . split_type . fillna ( method = 'ffill' , inplace = True ) 
~~ def _simple_year_award ( self , award_id ) : 
return list ( map ( int , sportsref . utils . parse_awards_table ( table ) ) ) 
~~ def team_names ( year ) : 
doc = pq ( sportsref . utils . get_html ( sportsref . nfl . BASE_URL + '/teams/' ) ) 
active_table = doc ( 'table#teams_active' ) 
active_df = sportsref . utils . parse_table ( active_table ) 
inactive_table = doc ( 'table#teams_inactive' ) 
inactive_df = sportsref . utils . parse_table ( inactive_table ) 
df = pd . concat ( ( active_df , inactive_df ) ) 
df = df . loc [ ~ df [ 'has_class_partial_table' ] ] 
ids = df . team_id . str [ : 3 ] . values 
names = [ _f for _f in names if _f ] 
names = [ lst [ 0 ] . text_content ( ) for lst in names ] 
series = pd . Series ( names , index = ids ) 
mask = ( ( df . year_min <= year ) & ( year <= df . year_max ) ) . values 
return series [ mask ] . to_dict ( ) 
~~ def team_ids ( year ) : 
names = team_names ( year ) 
return { v : k for k , v in names . items ( ) } 
~~ def name ( self ) : 
lastIdx = headerwords . index ( 'Franchise' ) 
teamwords = headerwords [ : lastIdx ] 
doc = self . get_year_doc ( '{}_roster' . format ( year ) ) 
roster_table = doc ( 'table#games_played_team' ) 
df = sportsref . utils . parse_table ( roster_table ) 
starter_table = doc ( 'table#starters' ) 
if not starter_table . empty : 
~~~ start_df = sportsref . utils . parse_table ( starter_table ) 
start_df = start_df . dropna ( axis = 0 , subset = [ 'position' ] ) 
starters = start_df . set_index ( 'position' ) . player_id 
df [ 'is_starter' ] = df . player_id . isin ( starters ) 
df [ 'starting_pos' ] = df . player_id . map ( 
lambda pid : ( starters [ starters == pid ] . index [ 0 ] 
if pid in starters . values else None ) 
~~ def boxscores ( self , year ) : 
~~~ return np . array ( [ ] ) 
~~ return df . boxscore_id . values 
~~ def _year_info_pq ( self , year , keyword ) : 
texts = [ p_tag . text_content ( ) . strip ( ) for p_tag in p_tags ] 
~~~ return next ( 
pq ( p_tag ) for p_tag , text in zip ( p_tags , texts ) 
if keyword . lower ( ) in text . lower ( ) 
~~~ if len ( texts ) : 
~~ ~~ ~~ def head_coaches_by_game ( self , year ) : 
coach_str = self . _year_info_pq ( year , 'Coach' ) . text ( ) 
coachAndTenure = [ ] 
m = True 
while m : 
~~~ m = re . search ( regex , coach_str ) 
coachID , wins , losses , ties = m . groups ( ) 
nextIndex = m . end ( 4 ) + 1 
coachStr = coachStr [ nextIndex : ] 
tenure = int ( wins ) + int ( losses ) + int ( ties ) 
coachAndTenure . append ( ( coachID , tenure ) ) 
~~ coachIDs = [ 
cID for cID , games in coachAndTenure for _ in range ( games ) 
return np . array ( coachIDs [ : : - 1 ] ) 
~~ def wins ( self , year ) : 
schedule = self . schedule ( year ) 
if schedule . empty : 
~~~ return np . nan 
~~~ return pd . DataFrame ( ) 
~~ df = df . loc [ df [ 'week_num' ] . notnull ( ) ] 
df [ 'week_num' ] = np . arange ( len ( df ) ) + 1 
df [ 'is_win' ] = df [ 'game_outcome' ] == 'W' 
df [ 'is_loss' ] = df [ 'game_outcome' ] == 'L' 
df [ 'is_tie' ] = df [ 'game_outcome' ] == 'T' 
df [ 'is_bye' ] = df [ 'game_outcome' ] . isnull ( ) 
df [ 'is_ot' ] = df [ 'overtime' ] . notnull ( ) 
~~ def srs ( self , year ) : 
~~~ srs_text = self . _year_info_pq ( year , 'SRS' ) . text ( ) 
~~ m = re . match ( r'SRS\\s*?:\\s*?(\\S+)' , srs_text ) 
~~~ return float ( m . group ( 1 ) ) 
~~ ~~ def sos ( self , year ) : 
~~~ sos_text = self . _year_info_pq ( year , 'SOS' ) . text ( ) 
~~ m = re . search ( r'SOS\\s*:\\s*(\\S+)' , sos_text ) 
~~ ~~ def off_coordinator ( self , year ) : 
if oc_anchor : 
~~~ return oc_anchor . attr [ 'href' ] 
~~ ~~ def def_coordinator ( self , year ) : 
if dc_anchor : 
~~~ return dc_anchor . attr [ 'href' ] 
~~ ~~ def stadium ( self , year ) : 
anchor = self . _year_info_pq ( year , 'Stadium' ) ( 'a' ) 
return sportsref . utils . rel_url_to_id ( anchor . attr [ 'href' ] ) 
~~ def off_scheme ( self , year ) : 
~~~ return m . group ( 1 ) 
~~ ~~ def def_alignment ( self , year ) : 
~~ ~~ def team_stats ( self , year ) : 
table = doc ( 'table#team_stats' ) 
~~~ return pd . Series ( ) 
~~ def opp_stats ( self , year ) : 
~~ def off_splits ( self , year ) : 
doc = self . get_year_doc ( '{}_splits' . format ( year ) ) 
tables = doc ( 'table.stats_table' ) 
dfs = [ sportsref . utils . parse_table ( table ) for table in tables . items ( ) ] 
dfs = [ 
df . assign ( split = df . columns [ 0 ] ) 
. rename ( columns = { df . columns [ 0 ] : 'split_value' } ) 
for df in dfs 
if not dfs : 
~~ return pd . concat ( dfs ) . reset_index ( drop = True ) 
~~ def get_html ( url ) : 
global last_request_time 
with throttle_process_lock : 
~~~ with throttle_thread_lock : 
~~~ wait_left = THROTTLE_DELAY - ( time . time ( ) - last_request_time . value ) 
if wait_left > 0 : 
~~~ time . sleep ( wait_left ) 
~~ response = requests . get ( url ) 
last_request_time . value = time . time ( ) 
~~ ~~ if 400 <= response . status_code < 500 : 
. format ( response . status_code , url ) 
~~ html = response . text 
html = html . replace ( '<!--' , '' ) . replace ( '-->' , '' ) 
return html 
~~ def parse_table ( table , flatten = True , footer = False ) : 
if not len ( table ) : 
~~ columns = [ c . attrib [ 'data-stat' ] 
[ flatten_links ( td ) if flatten else td . text ( ) 
for td in row . items ( 'th,td' ) ] 
for row in rows 
df = pd . DataFrame ( data , columns = columns , dtype = 'float' ) 
allClasses = set ( 
cls 
if row . attr [ 'class' ] 
for cls in row . attr [ 'class' ] . split ( ) 
for cls in allClasses : 
~~~ df [ 'has_class_' + cls ] = [ 
bool ( row . attr [ 'class' ] and 
cls in row . attr [ 'class' ] . split ( ) ) 
~~ df . drop ( [ 'ranker' , 'Xxx' , 'Yyy' , 'Zzz' ] , 
axis = 1 , inplace = True , errors = 'ignore' ) 
if 'year_id' in df . columns : 
~~~ df . rename ( columns = { 'year_id' : 'year' } , inplace = True ) 
if flatten : 
~~~ df . year = df . year . fillna ( method = 'ffill' ) 
df [ 'year' ] = df . year . map ( lambda s : str ( s ) [ : 4 ] ) . astype ( int ) 
~~ ~~ if 'pos' in df . columns : 
~~~ df . rename ( columns = { 'pos' : 'position' } , inplace = True ) 
~~ for bs_id_col in ( 'boxscore_word' , 'game_date' , 'box_score_text' ) : 
~~~ if bs_id_col in df . columns : 
~~~ df . rename ( columns = { bs_id_col : 'boxscore_id' } , inplace = True ) 
~~ ~~ df . replace ( re . compile ( r'[\\*\\+\\u2605]' , re . U ) , '' , inplace = True ) 
for col in df . columns : 
~~~ if hasattr ( df [ col ] , 'str' ) : 
~~~ df [ col ] = df [ col ] . str . strip ( ) 
~~ ~~ if 'player' in df . columns : 
~~~ if flatten : 
~~~ df . rename ( columns = { 'player' : 'player_id' } , inplace = True ) 
player_names = parse_table ( table , flatten = False ) [ 'player_name' ] 
df [ 'player_name' ] = player_names 
~~~ df . rename ( columns = { 'player' : 'player_name' } , inplace = True ) 
~~ ~~ for team_col in ( 'team' , 'team_name' ) : 
~~~ if team_col in df . columns : 
~~~ df = df . loc [ ~ df [ team_col ] . isin ( [ 'XXX' ] ) ] 
~~~ df . rename ( columns = { team_col : 'team_id' } , inplace = True ) 
~~ ~~ ~~ if 'season' in df . columns and flatten : 
~~~ df [ 'season' ] = df [ 'season' ] . astype ( int ) 
~~ if 'date_game' in df . columns and flatten : 
~~~ date_re = r'month=(?P<month>\\d+)&day=(?P<day>\\d+)&year=(?P<year>\\d+)' 
date_df = df [ 'date_game' ] . str . extract ( date_re , expand = True ) 
if date_df . notnull ( ) . all ( axis = 1 ) . any ( ) : 
~~~ df = pd . concat ( ( df , date_df ) , axis = 1 ) 
~~~ df . rename ( columns = { 'date_game' : 'boxscore_id' } , inplace = True ) 
~~ ~~ if 'game_location' in df . columns and flatten : 
~~~ df [ 'game_location' ] = df [ 'game_location' ] . isnull ( ) 
df . rename ( columns = { 'game_location' : 'is_home' } , inplace = True ) 
~~ if 'mp' in df . columns and df . dtypes [ 'mp' ] == object and flatten : 
~~~ mp_df = df [ 'mp' ] . str . extract ( 
r'(?P<m>\\d+):(?P<s>\\d+)' , expand = True ) . astype ( float ) 
no_match = mp_df . isnull ( ) . all ( axis = 1 ) 
if no_match . any ( ) : 
~~~ df . loc [ no_match , 'note' ] = df . loc [ no_match , 'mp' ] 
~~ df [ 'mp' ] = mp_df [ 'm' ] + mp_df [ 's' ] / 60 
~~ def convert_to_float ( val ) : 
~~~ m = re . search ( r'([-\\.\\d]+)\\%' , 
val if isinstance ( val , basestring ) else str ( val ) , re . U ) 
~~~ if m : 
~~~ return float ( m . group ( 1 ) ) / 100 if m else val 
~~ if m : 
~~~ return int ( m . group ( 1 ) ) + int ( m . group ( 2 ) ) / 60 
~~ m = re . search ( r'\\$[\\d,]+' , 
~~~ return float ( re . sub ( r'\\$|,' , '' , val ) ) 
~~~ if isinstance ( val , ( int , bool ) ) : 
~~~ return float ( val ) 
~~ ~~ if flatten : 
~~~ df = df . applymap ( convert_to_float ) 
~~ df = df . loc [ df . astype ( bool ) . any ( axis = 1 ) ] 
~~ 